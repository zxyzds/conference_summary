{
    "id": "kpPjV2krAa",
    "title": "FUSION IS ALL YOU NEED : FACE FUSION FOR CUSTOMIZED IDENTITY-PRESERVING IMAGE SYNTHESIS",
    "abstract": "Text-to-image (T2I) models have significantly advanced the development of artificial intelligence, enabling the generation of high-quality images in diverse contexts based on specific text prompts. However, existing T2I-based methods often struggle to accurately reproduce the appearance of individuals from a reference image and to create novel representations of those individuals in various settings. To address this, we leverage the pre-trained UNet from Stable Diffusion to incorporate the target face image directly into the generation process. Our approach diverges from prior methods that depend on fixed encoders or static face embeddings, which often fail to bridge encoding gaps. Instead, we capitalize on UNet\u2019s sophisticated encoding capabilities to process reference images across multiple scales. By innovatively altering the cross-attention layers of the UNet, we effectively fuse individual identities into the generative process. This strategic integration of facial features across various scales not only enhances the robustness and consistency of the generated images but also facilitates efficient multi-reference and multi-identity generation. Our method sets a new benchmark in identity-preserving image generation, delivering state-of-the-art results in similarity metrics while maintaining prompt alignment.",
    "keywords": [
        "Image Synthesis",
        "Image Customization",
        "ID Preservation",
        "Diffusion"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kpPjV2krAa",
    "pdf_link": "https://openreview.net/pdf?id=kpPjV2krAa",
    "comments": [
        {
            "comment": {
                "value": "Thanks for your comments and we reply to them below.\n\n### Response to W2\n\nThe differences between our method and IP Adapter are summarized as follows:\n\n-Flexible Guidance: Unlike IP Adapter, we do not rely on a fixed encoded representation of the reference identity image. Instead, we directly use the latent representation of the image as guidance. In contrast to IP Adapter, our guidance is not static; it varies across different layers of the U-Net, utilizing the U-Net's knowledge to apply guidance at multiple scales.\n\n-Enhanced Guidance: While IP Adapter adds a new cross-attention layer to decouple text and face identity, which may create conflicting guidance, we take a different approach. We directly modify the existing cross-attention layer by concatenating additional keys and values for attention. This process is relatively simple and dynamic, allowing for multi-reference and multi-identity guidance with ease and without the need for multiple diffusion paths; everything is achieved within a single sampling trajectory.\n\n-Image-Only Guidance: Unlike IP Adapter, which employs both face embedding and image encoding for guidance, our method relies solely on the image itself, which we found to be sufficient.\n\nOur advantages over IP Adapter and others:\n\n-Improved Identity Similarity: significant improvements across all identity similarity metrics, showcasing the superiority of our approach.\n\n-Efficiency: Our method required only 80,000 text-image pairs for training, compared to IP Adapter\u2019s millions of pairs.\n\n-Multi-Reference and Multi-Identity Guidance: Our approach enables multi-reference and multi-identity guidance within a single sampling trajectory.\n\n### Response to W3\nFacial Detail Loss: We acknowledge the issue of facial detail loss, particularly when faces occupy a small portion of the image. This limitation is partly due to constraints within the underlying Stable Diffusion model. However, our method still achieves the best results across all identity similarity metrics, demonstrating that in terms of Facial detail, it has the best results.\n\nIdentity Expressions: As shown in Figures 1 and 6, our method performs well in facial expression and style editing, successfully generating novel expressions for the input identity. In Figure 5, expressions appear unchanged due to text prompts that did not explicitly target facial expressions. Other methods, such as IPA-FaceID-Plus, also experience this limitation, albeit to a lesser extent due to encoding discrepancies. However, as demonstrated in Figures 1, 3, 4, and 6, our method does not suffer from issues with expression controllability."
            }
        },
        {
            "summary": {
                "value": "This paper improves text-to-image generation by preserving individual identities in generated images. Unlike previous methods, it directly incorporates face images into the UNet of Stable Diffusion. By adjusting the cross-attention layers, the model better captures facial features across scales. This approach achieves high accuracy in both identity similarity and prompt alignment."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This method surpasses previous approaches in identity fidelity, as evidenced by superior performance in relevant metrics.\n- This method supports customizable multi-identity generation."
            },
            "weaknesses": {
                "value": "- The writing of the paper is quite rough, with at least 10 instances of incorrect punctuation usage and a generally disorganized flow of ideas.\n- The paper lacks sufficient contribution, as the core method is still based on IP Adapter without original innovations. Additionally, it is challenging to observe any performance advantages.\n- The results are highly limited; in Figure 4, the customized faces are barely recognizable, with significant loss of facial details. The authors need to further explain the reasons behind this. Additionally, the paper claims to address the issue of decoupling expressions from the reference image; however, in Figure 5, each result appears to be a copy-paste of the reference image\u2019s expressions."
            },
            "questions": {
                "value": "See weekness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an identity-preserving image synthesis approach that directly passes the reference identity image to the diffusion model and fuses the attention maps obtained by a new cross-attention module with the original text-guided ones. The authors provide qualitative and quantitative results to demonstrate the effectiveness of the method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors propose to pass the reference image to the diffusion model directly to avoid using an extra image encoder and potential encoding gap issues. The method can generalize to multiple reference images of the same identity and multiple identities in the same output image."
            },
            "weaknesses": {
                "value": "- **Incomplete experiments**: \n  - The paper ensembles a very similar approach to IP-Adapter (referred to as IPA-FaceID-Plus in paper) with two major differences: (1) no extra image encoder, and (2) cross-attention fusion at the attention mask stage. Both are claimed by the authors to improve the performance over previous approaches. However, the authors fail to provide an ablation study for the audience to know the exact effect of these two modifications and how much they contribute to the final performance.\n  - Lack of quantitative comparison with important baseline InstantID. The authors claim they use SDXL which is a superior base model over the one they use, therefore eliminated for fair comparison. But for an important baseline, the authors should either upgrade the proposed method to SDXL or downgrade InstantID to the same base model to make the comparison.\n\n- **Unsatisfying Quality**:\nQualitative results do not show clear advances compared to previous methods. Fig.3 in particular gives very bad-quality results. Besides, Fig.5 shows signs of overfitting the reference pose and facial expression in the generated results, which raises the concern that the improved ID preservation results (in Tab.1) and improved PSNR and SSIM (in Tab.2) may be caused by this overfitting. Fig.6 does not reveal clear improvements over IPA-FaceID-Plus and InstantID also."
            },
            "questions": {
                "value": "The main concerns about the paper are listed in the Weakness section. Here are some minor questions for the authors regarding unclear writing etc.:\n\n- Equation 4: What does Q' represent? Q value should not be modified in the proposed method based on my understanding. Also, what does M here represent? Please provide the complete formula and how it differs from the standard attention mechanism for clear understanding.\n- Line 304: If you mention this technique can be used for face morphing, it is better to provide several examples to illustrate.\n- Figure 4: This figure appears to be unclear. Which identities are used for each output? Why do some outputs contain three identities and some two (are those with two missing one reference or do they only get two references as input)? What depth map is used for each output?\n- Section 4.1: How long does it take for training? How large is the larger dataset used to train the second version?\n- Line 376: How are the face regions obtained?\n- Line 427: Fig.6 has nothing to do with emotion. I guess you mean Fig.1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel text-to-image (T2I) synthesis method leveraging a modified UNet from Stable Diffusion to improve identity preservation in generated images. By altering UNet's cross-attention layers, it achieves robust multi-identity and multi-reference generation, setting new benchmarks in identity similarity metrics."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper presents a novel text-to-image synthesis method that excels in several key areas. By directly incorporating the target face image into the diffusion process, it significantly enhances the identity preservation capabilities of T2I models. This method consistently generates high-quality, realistic images that accurately maintain the fidelity of facial features and expressions aligned with text prompts. It also offers remarkable flexibility and scalability, supporting complex image generation scenarios involving multiple identities and reference images. Additionally, the method is designed for computational efficiency, requiring less training time and fewer resources, which makes it highly practical for real-world applications."
            },
            "weaknesses": {
                "value": "1. The method can struggle with fine facial features, especially when the face occupies a small portion of the image due to the limitations of the underlying Stable Diffusion model.\n\n2. Potential Overfitting: The method's reliance on direct face image integration might lead to overfitting specific facial features or identities, especially in a dataset with limited diversity.\n\n3.Could we possibly tackle the issue of facial feature degradation by improving the core architecture of Stable Diffusion or ControlNet itself, instead of just relying on the fine-tuning or adjusting the condition strength mentioned in the article?"
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper is dedicated to personalized text-to-image generation. Unlike previous works, this approach not uses frozen face encoder and static facial embedding to guide the generation. Instead, they utilize the hidden states within the pretrained UNet from reference facial image to replace static facial embedding, furthermore, they choose to concatenate K, V from text and facial image."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Personalized generation is an important task in the field of images and has not been well solved.\n- It reveals a good balance between face fidelity and prompt following ability.\n- It is interesting to use UNet to handle reference image instead of an extra image encoder.\n- This method is simple but achieves a new SOTA in identity-preserving image generation."
            },
            "weaknesses": {
                "value": "- The novelty is kind of limited.\n- Although it may be a problem of base model (SD1.5), it is hard to claim that the facial fidelity is satisfied. This is my major concern.\n- The experimental results are not convincing enough, the diversity may be a big problem."
            },
            "questions": {
                "value": "- What is the attention mask in formula (4)? It is not explained here how this mask is constructed and what its role is.\n- In Figure (3), the improvement brought by more reference images is not obvious.\n- In this experiment, the model was trained using only 80K images, and its diversity is questionable. It should be fair to use more non-celebrity (different gender, age, ethnicity) in testing.\n- In Figure (5), the generated image has a copy-paste problem. Why does this happen?\n- The credibility of the quantitative results is questionable. It is hard to believe that InstantID's similarity is worse, although it does suffer from text controllability.\n- Why intermediate hidden state is a better representation of face information? More discussion will be useful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}