{
    "id": "2l301qUdor",
    "title": "BOSE-NAS: Differentiable Neural Architecture Search with Bi-Level Optimization Stable Equilibrium",
    "abstract": "Differentiable Architecture Search (DARTS) has gained prominence in the neural architecture search community for its efficiency and simplicity, achieved through optimizing architecture parameters via gradient descent. However, the magnitude of these architecture parameters frequently fails to accurately represent the true significance of the corresponding operations, adversely affecting the performance of the resultant architectures. While numerous studies have introduced alternative metrics to evaluate operation significance, the actual role and impact of architecture parameters remain inadequately explored. This lack of understanding creates critical ambiguity in the architecture search process. Resolving these ambiguities is essential for the effective utilization of architecture parameters, thereby facilitating the development of more effective differentiable NAS methodologies. In this work, we first conduct a rigorous theoretical analysis, revealing that the change rate of architecture parameters reflects the sensitivity of the supernet\u2019s validation loss in the architecture space. Building on this foundation, we introduce the concept of the \u2018Stable Equilibrium State\u2019, which offers essential insights into the validation loss trajectory across architectural spaces and elucidates the stability of the supernet\u2019s bi-level optimization process. We further investigate the supernet training dynamics to assess the influence of operations on the Stable Equilibrium State, leading to the proposal of a novel metric for evaluating operation importance, termed Equilibrium Influential ($E_\\mathcal{I}$). Integrating these elements, we introduce BOSE-NAS, an effective differentiable NAS method that utilizes the Stable Equilibrium State to identify the optimal state during the search process, subsequently deriving the final architecture based on the $E_\\mathcal{I}$ metric. Extensive experiments conducted across diverse datasets and search spaces demonstrate that BOSE-NAS achieves competitive test accuracy compared to state-of-the-art methods while significantly reducing search costs.",
    "keywords": [
        "Neural Architecture Search",
        "Stable Equilibrium State",
        "Equilibrium Influential"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "This paper clarifies the ambiguities surrounding the actual role and impact of architecture parameters in DARTS and leveraging this insight proposes a more effective and robust NAS method.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=2l301qUdor",
    "pdf_link": "https://openreview.net/pdf?id=2l301qUdor",
    "comments": [
        {
            "title": {
                "value": "Responses to weakness 4-5"
            },
            "comment": {
                "value": "**Responses to weakness 4:**\n\nThank you for bringing up this valuable point. We agree that a more comprehensive analysis of the search cost efficiency would strengthen the discussion in our results section. In the revised manuscript, we will elaborate on the key factors contributing to the superior search efficiency of BOSE-NAS, which are outlined as follows:\n\nEarly Identification of a Stable Supernet: BOSE-NAS demonstrates its search efficiency primarily through its capability to identify a stable state at an early stage of the supernet training process. This early identification enables the derivation of the final architecture without the need for prolonged training phases. This approach aligns with findings from related research[1,2,3] that shows extended training of the supernet is often unnecessary for accurately assessing the relative strength among operations, thus reducing overall search cost.\n\nEfficient Evaluation Metric: Unlike other methods that may involve higher computational complexity for assessing operation strength, such as those used in [3-6], the proposed EI metric operates with lower overhead while maintaining reliable performance. This streamlined process contributes significantly to the reduced search cost and ensures rapid and consistent evaluations.\n\nWe will include a detailed explanation of this analysis in the revised result section to better highlight the efficiency benefits provided by BOSE-NAS. \n\n[1] Hanwen Liang, Shifeng Zhang, Jiacheng Sun, Xingqiu He, Weiran Huang, Kechen Zhuang, and Zhenguo Li. Darts+: Improved differentiable architecture search with early stopping. arXiv preprint arXiv:1909.06035, 2019.\n\n[2] ZELA A, ELSKEN T, SAIKIA T, et al. Understanding and Robustifying Differentiable Architecture Search[J]. International Conference on Learning Representations,International Conference on Learning Representations, 2020.\n\n[3] Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive darts: Bridging the optimization gap for\nnas in the wild. International Journal of Computer Vision, 129:638\u2013655, 2021b.\n\n[4] HONG W, LI G, ZHANG W, et al. DropNAS: Grouped Operation Dropout for Differentiable Architecture Search[C/OL]//Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, Yokohama, Japan. 2020. \n\n[5] WANG R, CHENG M, CHEN X, et al. Rethinking Architecture Selection in Differentiable NAS[J]. International Conference on Learning Representations,International Conference on Learning Representations, 2021.\n\n[6] Li, G., Zhang, X., Wang, Z., Li, Z., Zhang, T.: StacNAS: Towards stable and consistent optimization for differentiable Neural Architecture Search. arXiv preprint arXiv:1909.11926 (2019)\n\n**Responses to weakness 5:**\n\nWe understand the concern regarding the performance comparison with SOTA NAS methods.\n\nWhile it is true that our method achieves performance comparable to existing state-of-the-art (SOTA) methods in some of the experiments, we would like to emphasize that the primary contribution of our research extends beyond empirical accuracy.\n\nThe central objective of our study is to address and resolve the ambiguities surrounding the actual role and impact of architecture parameters within the DARTS framework. This focus is critical for enhancing the theoretical understanding and robustness of differentiable NAS methods. We believe that filling these gaps and proposing a more analytically grounded differentiable NAS approach contributes significant value to the field, complementing empirical findings with deeper scientific insights. This combination of practical results and theoretical advancement broadens the understanding and potential applications of differentiable NAS methodologies.\n\nWe hope this response clarifies our perspective and underscores the importance of our contributions beyond raw performance metrics."
            }
        },
        {
            "title": {
                "value": "Response to weakness 1-3"
            },
            "comment": {
                "value": "We sincerely thank the reviewers for their detailed feedback and constructive suggestions. We have carefully considered each comment and provide our point-by-point responses below.\n\n**Responses to weakness 1:**\n\nThank you for highlighting this concern. We agree that there is redundancy in the abstract and introduction. We have revised the Abstract section to focus more on the main contributions and key results, minimizing background information and condense the Introduction by reducing detailed descriptions of experimental results, emphasizing the main findings to better highlight the effectiveness of the proposed method. And we also present the revised section below:\n\nAbstract is revised as: Recent research has significantly mitigated the performance collapse issue in Differentiable Architecture Search (DARTS) by either refining architecture parameters to better reflect the true strengths of operations or developing alternative metrics for evaluating operation significance. However, the actual role and impact of architecture parameters remain insufficiently explored, creating critical ambiguities in the search process. To address this gap, we conduct a rigorous theoretical analysis demonstrating that the change rate of architecture parameters reflects the sensitivity of the supernet\u2019s validation loss in architecture space, thereby influencing the derived architecture's performance by shaping supernet training dynamics. Building on the these insights, we introduce the concept of a Stable Equilibrium State to capture the stability of the bi-level optimization process and propose the Equilibrium Influential ($E_\\mathcal{I}$) metric to assess operation importance. By integrating these elements, we propose BOSE-NAS, a differentiable NAS approach that leverages the Stable Equilibrium State to identify the optimal state during the search process and derive the final architecture using the $E_\\mathcal{I}$ metric. Extensive experiments across diverse datasets and search spaces demonstrate that BOSE-NAS achieves competitive test accuracy compared to state-of-the-art methods while significantly reducing search costs.\n\nIntroduction (line 76-85) is revised as\uff1aIn the DARTS search space, BOSE-NAS achieves an impressive average test error of 2.49% and a best test error of 2.37% on the CIFAR-10 dataset. When transferred to CIFAR-100 and ImageNet, BOSE-NAS attains an average test error of 16.23% and a best test error of 16.08% on CIFAR-100, and a best test error of 24.1% on ImageNet. Remarkably, our method accomplishes this with a mere 0.13 GPU-days of computational cost (equivalent to just 3 hours of search time on a single V100 GPU) for architecture search on CIFAR-10. This level of efficiency outperforms DARTS by more than three times and surpasses DARTS-PT by nearly six times.\n\n**Responses to weakness 2:**\n\nIn the original DARTS paper, two methods for updating parameters are proposed: first-order and second-order updates. The term $\\frac{\\xi}{2 \\epsilon}(\\frac{\\Delta L_{train}(\\alpha, \\omega^{+} )}{\\Delta\\alpha_{\\varepsilon}}-\\frac{\\Delta L_{train}(\\alpha, \\omega^{-} )}{\\Delta\\alpha_{\\varepsilon}})$ presented in Equation 6 represents an approximation of the second-order term. In this paper, we adhere to the first-order optimization principles outlined in DARTS Algorithm 1, thus the term $\\frac{\\xi}{2 \\epsilon}(\\frac{\\Delta L_{train}(\\alpha, \\omega^{+} )}{\\Delta\\alpha_{\\varepsilon}}-\\frac{\\Delta L_{train}(\\alpha, \\omega^{-} )}{\\Delta\\alpha_{\\varepsilon}})$ can be disregarded. \n\nIn addition, we will incorporate a detailed, step-by-step theoretical proof and explanation in the Appendix of the revised manuscript.\n\n**Responses to weakness 3:**\n\nWe appreciate this suggestion for better structuring our content. We will include the detailed theoretical calculations and derivations to the Appendix in the revised version."
            }
        },
        {
            "title": {
                "value": "Responses to weakness 7-8"
            },
            "comment": {
                "value": "**Responses to weakness 7:**\n\nThank you for your valuable suggestions. The idea you proposed to evaluate our method on detection datasets, such as COCO2017 and VOC, is highly appreciated. We fully agree that extending our approach to these datasets would provide additional insights into its scalability, transferability, and generalization capabilities.\n\nHowever, it is important to note that most existing Neural Architecture Search (NAS) methods, including the two works you highlighted (\u03b2-DARTS++ and \u039b-DARTS), have predominantly been evaluated on classification datasets. This approach aligns with the standard practice in the field, where classification tasks are widely used as foundational benchmarks for assessing the performance of NAS methods. In this context, our study follows the same tradition by validating our method on large-scale classification datasets, including ImageNet, which is a well-established benchmark and provides a rigorous and credible evaluation of our method's effectiveness and competitiveness.\n\nWhile incorporating comparisons on detection datasets such as COCO2017 and VOC would undoubtedly enhance the breadth of our study, such an extension was beyond the scope of the current work. Nevertheless, we view this as an important avenue for future research and will actively consider it in subsequent studies to build upon the foundation established in this paper.\n\n**Responses to weakness 8:**\n\n We validate the generalizability of our method through extensive experiments across three different datasets and six diverse search spaces. Specifically, we show that architectures discovered using our method on CIFAR-10 successfully transfer to CIFAR-100 and ImageNet in the DARTS search space, NAS-Bench-201 search space and S1-S4 search space, maintaining competitive performance. These results affirm the strong generalization capability of our method.\n\nTo further substantiate the generalization capability, robustness, and practical applicability of our proposed method, we applied it to two real-world tasks: store classification and recognition tasks, using dedicated business datasets. Detailed descriptions of these datasets can be found in the appendix of our paper. The first dataset consists of 76,189 images across 21 categories for store classification, whereas the second dataset includes approximately 1.46 million images of business licenses for text recognition. Our method not only surpassed existing approaches in performance but also achieved this with over four times greater parameter efficiency compared to the ResNet model. These results highlight the outstanding generalizability and effectiveness of our method across a variety of real-world applications."
            }
        },
        {
            "title": {
                "value": "Responses to weakness 5-6"
            },
            "comment": {
                "value": "**Response to weakness 5:** \n\nThank you for this comprehensive set of questions. \n\nMotivation for Using Influence Function: The Influence Function [1][2]  is a well-established tool from robust statistics that quantifies the effect of perturbing or upweighting a specific training sample on model parameters. It has been successfully applied in various machine learning applications to explain model behavior. Different from previous works that analyzed the effects of removing data points on model parameters, DARTS-IM [3] creatively adapted the Influence Function to estimate the significance of candidate operations within a trained supernet, providing insights into operation selection in differentiable NAS methods\n\nRelationship Between Influence Function and Our Method: Motivated by [3], we leverage the concept of the Influence Function to validate the reliability of our proposed Equilibrium Influential (EI) metric. By adapting the Influence Function, we can analyze how changes in specific operations affect the validation loss. Our analysis demonstrates that the magnitude of the EI metric is positively correlated with the operation\u2019s influence on validation loss, thus confirming its reliability as a measure of operation importance. This correlation ensures that the EI metric could reliably determine the relative significance among operations necessary for robust architecture derivation.\n\nWhy Validate the Reliability of the Metric: Validating the reliability of the EI metric is crucial, as existing studies have shown that metrics failing to represent the true strength of operations can lead to degraded architectures. Ensuring our EI metric's reliability helps address the performance degradation seen in prior differentiable NAS methods and supports the robustness of BOSE-NAS.\n\nWe will include a step-by-step theoretical proof process and explanation in the Appendix of the revised manuscript.\n\n[1] F. R. Hampel. The influence curve and its role in robust estimation. Journal of the american statistical association, 69(346):383\u2013393, 1974.\n\n[2] P. W. Koh and P. Liang. Understanding black-box predictions via influence functions. In International Conference on Machine Learning, pages 1885\u20131894. PMLR, 2017.\n\n[3] MiaoZhang, Wei Huang, and BinYang. Interpreting operation selection in differentiable architecturesearch: A perspective from influence-directed explanations. Advances in Neural Information Processing Systems, 35: 31902\u201331914, 2022.\n\n**Response to weakness 6:** \n\nWe thank the reviewer for highlighting this issue, and we appreciate the opportunity to clarify and correct it. In the original work [1][2], \"I(z, L)\" represents the effect of removing training data points on the validation loss. However, in our work, motivated by DARTS-IM [3], we adapt influence functions to estimate the significance of candidate operations within the differentiable architecture search context. Thus, in this context, it should be denoted as \"I(\u03b8, L)\" instead of \"I(z, L)\", where \"I(\u03b8, L)\" specifically represents the influence of candidate operations on the validation loss.\n\nWe sincerely apologize for this oversight and assure you that the necessary corrections will be made in the final version of the manuscript. Thank you again for pointing this out.\n\n[1] F. R. Hampel. The influence curve and its role in robust estimation. Journal of the american statistical association, 69(346):383\u2013393, 1974.\n\n[2] P. W. Koh and P. Liang. Understanding black-box predictions via influence functions. In International Conference on Machine Learning, pages 1885\u20131894. PMLR, 2017.\n\n[3] MiaoZhang, Wei Huang, and BinYang. Interpreting operation selection in differentiable architecturesearch: A perspective from influence-directed explanations. Advances in Neural Information Processing Systems, 35: 31902\u201331914, 2022."
            }
        },
        {
            "title": {
                "value": "Responses to weakness 1-4"
            },
            "comment": {
                "value": "We would like to express our gratitude to the reviewers for their thoughtful and detailed feedback on our manuscript. \n\n**Response to weakness1:**\n\n Thank you for your observation. Since multiple local Stable Equilibrium State minima may exist with extended supernet training as shown in Fig.2, the primary aim of the experiment depicted in Figure 3 is to support our strategy of designating the first Stable Equilibrium State as the optimal point for architecture derivation. This choice is a design decision and is largely independent of the hyperparameters of our method. \n\n**Response to weakness2:**\n\n We apologize for any oversight regarding typos or grammatical issues in the initial submission. We have meticulously reviewed the entire manuscript and corrected all identified errors to enhance readability and coherence. And all revisions will be updated in the revised version of the article.\n\n**Response to weakness3:**\n\nWe sincerely apologize for the oversight in the formatting of our references. After receiving your valuable feedback, we identified that some inconsistencies were caused by issues with our reference management software, which led to missing or incorrectly formatted information. We have meticulously reviewed the entire Reference section and have corrected all formatting inconsistencies to fully comply with the journal\u2019s guidelines. This revision will be included in the updated manuscript and thank you for bringing this to our attention.\n\n**Response to weakness4:**\n\nWe appreciate your suggestion. Below, we outline the reasons and provide intuitive explanations for the success of our approach:\n\nThe core objective of our work is to resolve ambiguities surrounding the actual role and impact of architecture parameters in DARTS, facilitating the development of more effective differentiable NAS methodologies. Our theoretical analysis reveals that the change rate of architecture parameters reflects the sensitivity of the supernet\u2019s validation loss, which shapes the training dynamics of the supernet, ultimately influencing the performance of the derived architecture. \n\nEmpirical studies have shown that while prolonged supernet training may lead to overfitting and degrade the final architecture\u2019s performance [1-3], a supernet experiencing significant fluctuations during training can also result in poor final performance [2, 4]. Thus, identifying a stable state helps prevent these issues and improves the architecture derivation process.\n\nBuilding on our findings, the Stable Equilibrium State metric is presented to effectively track the trajectory of validation loss across the architecture space. It allows us to monitor and identify a supernet with a stable state, providing an optimal point for architecture derivation. \n\nThe Equilibrium Influential (EI) metric we proposed plays a crucial role in assessing the relative strength among operations in the supernet. Intuitively, the proposed EI metric assesses the influence of operations on the stability of the supernet and quantifies their contribution to maintaining a stable state. Our theoretical analysis supports that the magnitude of the EI metric is positively correlated with an operation\u2019s influence on the validation loss, establishing it a reliable measure for determining the relative importance of operations in a stable supernet.\n\nTogether, these techniques form the foundation of BOSE-NAS, which has been demonstrated to effectively identify competitive architectures across diverse search spaces and datasets.\n\n[1] Hanwen Liang, Shifeng Zhang, Jiacheng Sun, Xingqiu He, Weiran Huang, Kechen Zhuang, and Zhenguo Li. Darts+: Improved differentiable architecture search with early stopping. arXiv preprint arXiv:1909.06035, 2019.\n\n[2] ZELA A, ELSKEN T, SAIKIA T, et al. Understanding and Robustifying Differentiable Architecture Search[J]. International Conference on Learning Representations,International Conference on Learning Representations, 2020.\n\n[3] Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive darts: Bridging the optimization gap for\nnas in the wild. International Journal of Computer Vision, 129:638\u2013655, 2021b.\n\n[4] CHEN X, HSIEH C J. Stabilizing Differentiable Architecture Search via Perturbation-based Regularization[J]. Cornell University - arXiv,Cornell University - arXiv, 2020."
            }
        },
        {
            "title": {
                "value": "Response to weaknesses and questions."
            },
            "comment": {
                "value": "We would like to express our sincere gratitude to the reviewers for their valuable feedback and constructive comments on our manuscript. We have carefully considered each point and provide our detailed responses below.\n\n**Response to weakness1:** \n\nThank you for your insightful observation. Indeed, the identification of a Stable Equilibrium State is a crucial component of our method. In this paper, we emphasize that a stable supernet is essential for deriving robust architectures; while a supernet undergoing significant fluctuations with the precipitous validation loss landscape, would lead to a dramatic performance drop when deriving the final architecture, as also highlighted in [1][2].\n\nIt's also true that our EI metric evaluates the relative significance of operations by independently assessing their influence on supernet stability and does not account for the intricate dependencies between operations, just as we noted in the manuscript and highlighted in the \"Limitations\" section.\nWe validate the generalization performance of our approach through comprehensive experiments across three different datasets and six diverse search spaces, demonstrating that our method remains effective despite these limitations.\n\n[1] CHEN X, HSIEH C J. Stabilizing Differentiable Architecture Search via Perturbation-based Regularization[J]. Cornell University - arXiv,Cornell University - arXiv, 2020.\n\n[2] ZELA A, ELSKEN T, SAIKIA T, et al. Understanding and Robustifying Differentiable Architecture Search[J]. International Conference on Learning Representations,International Conference on Learning Representations, 2020.\n\n**Response to weakness2:** \n\nWe understand your concern regarding the broader applicability of our method. Differentiable NAS and evolutionary NAS methods represent distinct branches within the NAS domain, each with different optimization strategies and requirements. The central objective of our study is to address the ambiguities surrounding the actual role and impact of architecture parameters within the DARTS framework. By resolving these ambiguities, our work proposes techniques specifically tailored to improve DARTS method. As such, our contributions are intentionally focused on advancing the differentiable NAS research field, providing new insights that we hope will inspire the development of more sophisticated differentiable NAS methodologies.\n\nWe recognize that the seamless application of many differentiable NAS techniques, including ours, to evolutionary algorithms is challenging due to fundamental differences in their design and optimization processes. However, we believe there is still potential for adapting the idea of our method to pruning-based approaches. This adaptation would involve initially identifying a stable supernet and subsequently evaluate and iteratively prune less influential operations. We agree that further investigation into this adaptation would be valuable and could open up interesting avenues for future research.\n\n**Response to Question1:** \n\nThank you for this insightful and important question. We are currently conducting additional experiments to analyze the impact of these hyper-parameters on the stability and effectiveness of our method. The results of these ablation studies will be included in the revised manuscript under the ablation study section. We appreciate your patience as we complete this additional work.\n\n**Response to Question2:** \n\nWe appreciate your question and the opportunity to discuss broader adaptations of our method. As mentioned earlier, the primary objective of our study is to resolve the ambiguities surrounding the actual role and impact of architecture parameters within the DARTS framework, and the Stable Equilibrium State and the Equilibrium Influential (EI) metric developed are tailored to differentiable NAS. Consequently, it's challenging to seamlessly apply our techniques to non-differentiable NAS methods, such as Zen-NAS [1] and SWAP-NAS [2]. Those methods often rely on zero-shot proxies, training-free evaluation metrics, or discrete search algorithms, which contrast with the gradient-based optimization used in differentiable NAS.\n\nHowever, potential adaptations may involve reconceptualizing our stability framework to identify a quasi-stable state within the search phase of these methods. This could be explored through proxies that mimic stability in a non-differentiable context, perhaps by using pre-training analytics or meta-learning strategies. While this is beyond the current scope of our work, we agree that extending the concept to non-differentiable or training-free methods could lead to promising research directions.\n\n[1] Ming Lin, Pichao Wang, Zhenhong Sun, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin. Zen-nas: A zero-shot NAS for high-performance image recognition. ICCV 2021.\n\n[2] Yameng Peng, Andy Song, Haytham. M. Fayek, Vic Ciesielski and Xiaojun Chang . SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS. ICLR 2024."
            }
        },
        {
            "title": {
                "value": "Response to the weaknesses and questions"
            },
            "comment": {
                "value": "We appreciate your constructive feedback and the time taken to review our manuscript. Below, we address each of your points to ensure a comprehensive revision.\n\n**Response to Weakness1:** \n\nThank you for noting the potential improvement in figure clarity. We have revised all figures to enhance their visual quality by increasing the resolution and refining the labels and legends for better readability. These updated figures will be included in the revised version, ensuring that they convey the data more effectively.\n\n**Response to Weakness2:** \n\nWe appreciate your acknowledgment of this aspect of our work. While it is true that our method achieves performance comparable to existing state-of-the-art (SOTA) methods in some of our experiments, we would like to emphasize that the primary contribution of our research extends beyond empirical accuracy.\n\nThe central objective of our study is to address and resolve the ambiguities surrounding the actual role and impact of architecture parameters within the DARTS framework. This focus is critical for enhancing the theoretical understanding and robustness of differentiable NAS methods. We believe that filling these gaps and proposing a more analytically grounded differentiable NAS approach contributes significant value to the field, complementing empirical findings with deeper scientific insights. This combination of practical results and theoretical advancement broadens the understanding and potential applications of differentiable NAS methodologies.\n\nWe hope this response clarifies our perspective and underscores the importance of our contributions beyond raw performance metrics.\n\n**Response to Question1:** \n\nThank you for your encouraging assessment of our work and for noting the importance of verifying the theoretical proof. To support a thorough review, we will include a comprehensive and detailed explanation of our theoretical proof in the Appendix of the revised manuscript."
            }
        },
        {
            "summary": {
                "value": "This paper focuses on Differentiable Architecture Search (DARTS). They conduct theoretical analysis over DARTS and propose a concept called Stable Equilibrium State. Upon it, they propose an effective framework called BOSE-NAS to identify the optimal state during the searching procedure. Experiment results show that the proposed method shows competitive results over state-of-the-art methods."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. I think this paper focuses on a very important problem. DARTS is a very crucial framework in NAS, but it has some well-known problems. It is very important to have some theoretical analysis on this framework. \n2. This author provides large-scale theoretical analysis, focusing on very important aspects, such as the stability of bi-level optimization, the loss trajectory, etc. I think the analysis is insightful. \n3. The proposed method can reduce the search costs."
            },
            "weaknesses": {
                "value": "1. I think the figures in this paper can be polished to be more clear (maybe in the camera ready version). \n2. The accuracy of the proposed method is just comparable with sota, but not superior to sota. I think it is not a serious problem, but I just list it as one weakness."
            },
            "questions": {
                "value": "I think overall this paper is good. Currently I give 6 since I have not checked the proof very carefully. I am willing to raise the score to 8 if the proof is proved to be right by other reviewers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose BOSE-NAS, a novel differentiable neural architecture search method that addresses critical challenges in existing differentiable architecture search (DARTS). The core idea of BOSE-NAS  is around the the concept of a \u2018Stable Equilibrium State\u2019, which offering insights into the validation loss trajectory across architectural spaces to stabilise the supernet\u2019s bi-level optimisation process. The proposed method introduces a novel metric called Equilibrium Influential (EI) to evaluate the importance of operations during the architecture search phase. By choosing operations based on the EI metric at the Stable Equilibrium State, BOSE-NAS uses bi-level optimisation to find the optimal architecture operations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The introduction of Stable Equilibrium State is somewhat novel and interesting, the theoretical analysis of architecture parameter dynamics provides a solid foundation for understanding the bi-level optimisation in differentiable NAS.\n\n2. The Equilibrium Influential (EI) metric for operation evaluation is an innovative approach and offers a more reliable measure of operation importance to the bi-level optimisation process in the differentiable NAS. \n\n2. The proposed BOSE-NAS achieves competitive performance as well as less computational overhead in benchmark datasets like CIFAR-10 and CIFAR-100, compare with other differentiable NAS methods."
            },
            "weaknesses": {
                "value": "1. The propose method heavily depends on the accurate identification of the Stable Equilibrium State, specifically, the EI metric evaluates each operation independently, which could overlook potential dependencies among network operations within the architecture. This could make the proposed method not always generalise well.\n\n2. The biggest concern of the proposed method, e.g., EI metric and the concept of Stable Equilibrium State, are the limited use scenario. It may not be easily applicable to non differentiable NAS methods, e.g., the evolutionary or pruning-based search algorithms."
            },
            "questions": {
                "value": "1. Although the problems within the bi-level optimisation process of differentiable NAS have been widely studied for years, e.g., BONAS [1], the proposed EI metric and Stable Equilibrium State still bringing some new insights to the NAS research. But differentiable NAS are often sensitive to the hyper-parameters, I wonder how sensitive is the Stable Equilibrium State identification process to the choice of hyper-parameters such as the learning rate and batch size? Can authors provide some ablation studies? It would be helpful to understand how the proposed method handles changes in the hyper-parameters, as well as its robustness.\n\n2. The proposed methods are only applied to the differentiable NAS, however, the interest of NAS research has been largely shifted to training-free NAS methods, as they are offering more flexibilities to different search algorithms and search spaces, as well as better performance and much less computational overhead compare with differentiable NAS, e.g., Zen-NAS [2] and SWAP-NAS [3]. Can author discuss the potential adaptation that extend the concept the Stable Equilibrium State and EI metric to non-differentiable NAS methods?  \n\n\n\n[1] Han Shi, Renjie Pi, Hang Xu, Zhenguo Li, James T. Kwok, and Tong Zhang. Bridging the gap between sample-based and one-shot neural architecture search with bonas. NeurIPS 2020.\n\n[2] Ming Lin, Pichao Wang, Zhenhong Sun, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin. Zen-nas: A zero-shot NAS for high-performance image recognition. ICCV 2021.\n\n[3] Yameng Peng, Andy Song, Haytham. M. Fayek, Vic Ciesielski and Xiaojun Chang . SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS. ICLR 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Differentiable Architecture Search (DAS) often faces the issue where the magnitude of architecture parameters fails to reflect the true importance of operations. This paper addresses this problem by proposing BOSE-NAS, a DAS method guided by the Stable Equilibrium of architecture parameters (i.e., the point where the rate of change of the architecture parameters is minimal). The authors provide relevant experiments to support their method. However, the experimental section has several issues, such as limited improvement in performance and a lack of ablation studies."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe paper is easy to read.\n2.\tThe problem of DAS is clear."
            },
            "weaknesses": {
                "value": "This paper was submitted to NeurIPS 2024, compared with NeurIPS 2024, there are still some important issues that need to be addressed.\n1. The ablation studies are not convincing. To be specific, in Figure 3, we can clearly see that the proposed method is sensitive to hyperparameters.\n2. There still exist some typos/grammatical errors in the paper.\n3. The format of references is still wrong.\n4. Exploring the reasons behind the success of these techniques and providing intuitive explanations would contribute to the overall scientific contribution of the work.\n5. I don't understand the theoretical analysis. Why use \" Influence Function\"? What relationship between \" Influence Function\" and your method? why validate the \"reliability\" of your proposed metric? Please provide detailed motivation and clear proven process in step by step. What is the difference between stability and reliability? Please provide a step-by-step proof process for validating their metric. And, clarification on the relationship between the Influence Function and their method.\n6. In page 7, \"I(z, L)\" denotes?\n7. The main limitation of this paper is that proposed method lacks comparison with larger datasets (i.e., COCO2017, VOC), and more competitors (i.e., \u03b2-DARTS++, \u039b-DARTS).\n8. Pls to prove your statement of generalizability.\n\n[1] \u03b2-DARTS++: Bi-level Regularization for Proxy-robust Differentiable Architecture Search\n[2] \u039b-DARTS: MITIGATING PERFORMANCE COLLAPSE BY HARMONIZING OPERATION SELECTION AMONG CELLS"
            },
            "questions": {
                "value": "pls see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new operation importance evaluation metric in network architecture search. The authors first introduce the concept of stable equilibrium state, which shows the stability of the bi-level optimization process in differentiable NAS. By analyzing the supernet training dynamics, the metric named equilibrium influential is proposed for fair differentiable NAS. The experimental results show that the proposed metric and search method can achieve competitive accuracy with significantly reduced search cost."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The experimental results clearly show the effectiveness and the efficiency of the proposed method."
            },
            "weaknesses": {
                "value": "- The writing can be improved. The abstract and the introduction are redundant. For the abstract, there are too many contents to introduce the background. For the introduction, many details especially the experimental results don\u2019t have to be elaborated. I think demonstrating the main results is enough to show the effectiveness of this method.\n\n- The technical soundness can be further verified. There are some strong assumptions without verification or explanation. For example, the assumptions to transit (6) to (7) should be verified. Why they have little effect on $\\alpha$?\n\n- Some exact calculations can be put in the Appendix part.\n\n- The reason why the proposed method has less search cost should be analyzed in the result analysis, which is an important benefit from the new metric.\n\n- The performance of the proposed method underperforms the SOTA NAS methods such as IS-DARTS. More clarification is required for the performance analysis."
            },
            "questions": {
                "value": "See Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}