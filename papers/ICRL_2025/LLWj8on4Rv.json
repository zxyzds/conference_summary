{
    "id": "LLWj8on4Rv",
    "title": "Leveraging Driver Field-of-View for Multimodal Ego-Trajectory Prediction",
    "abstract": "Understanding drivers\u2019 decision-making is crucial for road safety. Although predicting the ego-vehicle\u2019s path is valuable for driver-assistance systems, existing methods mainly focus on external factors like other vehicles\u2019 motions, often neglecting the driver\u2019s attention and intent. To address this gap, we infer the ego-trajectory by integrating the driver\u2019s attention and the surrounding scene. We introduce RouteFormer, a novel multimodal ego-trajectory prediction network combining GPS data, environmental context, and driver field-of-view\u2014comprising first-person video and gaze fixations. We also present the Path Complexity Index (PCI), a new metric for trajectory complexity that enables a more nuanced evaluation of challenging scenarios. To tackle data scarcity and enhance diversity, we introduce GEM, a comprehensive dataset of urban driving scenarios enriched with synchronized driver field-of-view and gaze data. Extensive evaluations on GEM and DR(eye)VE demonstrate that RouteFormer significantly outperforms state-of-the-art methods, achieving notable improvements in prediction accuracy across diverse conditions. Ablation studies reveal that incorporating driver field-of-view data yields significantly better average displacement error, especially in challenging scenarios with high PCI scores, underscoring the importance of modeling driver attention. All data, code, and models will be made publicly available.",
    "keywords": [
        "Ego-trajectory prediction",
        "driver attention",
        "multimodal learning",
        "field-of-view",
        "gaze fixations",
        "deep learning",
        "autonomous driving",
        "driver behavior modeling",
        "dataset creation"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "Introduced G-MEMP, a multimodal ego-trajectory prediction model leveraging driver attention and field-of-view data, along with the GEM dataset and Path Complexity Index, significantly improving prediction accuracy over state-of-the-art methods.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=LLWj8on4Rv",
    "pdf_link": "https://openreview.net/pdf?id=LLWj8on4Rv",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a multimodal ego-trajectory prediction model that utilizes driver field-of-view (FOV) data\u2014including first-person video and gaze fixations\u2014along with environmental and GPS information to enhance the accuracy of vehicle trajectory forecasting. The method surpasses conventional models through the implementation of a Path Complexity Index (PCI), which assesses the complexity of diverse driving scenarios. The authors present a novel dataset that provides comprehensive urban driving data enriched with driver FOV information."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- New dataset: the authors contribute GEM, a high-quality dataset focused on urban driving, which includes synchronized FOV and gaze data, filling a critical gap in available resources for trajectory prediction. It will be good to have this dataset for the autonomous driving community. \n- The proposed RouteFormer introduces a unique approach by integrating driver field-of-view (FOV) data, such as first-person video and gaze fixations, with GPS and environmental data, enhancing the accuracy of ego-trajectory prediction.\n- Introduction of Path Complexity Index (PCI). The paper proposes a novel PCI metric to quantify the complexity of driving scenarios, offering a more nuanced assessment of prediction challenges compared to traditional metrics.\n- The proposed method outperforms the other baselines on both datasets."
            },
            "weaknesses": {
                "value": "- To fully understand how much each modality contribute to the improvement, it will be interesting to add the ablations studies below and compare the ADE and ADE+20PCI.\n   1. RouteFormer without video and gaze (Motion only).\n   2. RouteFormer without surrounding scenes (Motion + Gaze).\n\n- Stronger baselines are needed. The current baselines (GIMO, Multimodal Transformer) used in this paper are mainly designed for human motion. Compared with at least one vehicle motion predictor is more convincing (e.g., MTR [1], Autobots [2], ...). Or are there any potential reasons that the authors thought the human motion baselines were more appropriate than vehicle baselines?\n\n[1] Shi, Shaoshuai, et al. \"Motion transformer with global intention localization and local movement refinement.\" Advances in Neural Information Processing Systems 35 (2022): 6531-6543.\n[2] Girgis, Roger, et al. \"Latent variable sequential set transformers for joint multi-agent motion prediction.\" International Conference on Learning Representations (ICLR), 2022."
            },
            "questions": {
                "value": "Please refer to the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces RouteFormer, a multimodal ego-motion prediction network that integrates driver field-of-view (FOV) data with scene and GPS information to improve trajectory prediction. RouteFormer is the best solution on the market for this problem. Its Path Complexity Index (PCI) metric assesses trajectory difficulty and enhances model evaluation in complex scenarios. Additionally, the paper presents GEM, a new dataset containing synchronised driver FOV, gaze, and GPS data, capturing diverse urban driving conditions. RouteFormer significantly improves prediction accuracy, especially in high-complexity scenarios, advancing safety in driver-assistance systems."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper has done an impressively solid and hardcore job\u2014it's a pleasure to read, providing a refreshing clarity that\u2019s truly satisfying. It offers genuinely unique insights and, in my opinion, fully meets the standards for conference publication.\n\n1. RouteFormer effectively combines driver FOV, scene, and GPS data, resulting in enhanced accuracy for predicting complex, non-linear trajectories. This multimodal integration marks a notable advancement over existing models.\n\n2. The introduction of the Path Complexity Index (PCI) provides a novel way to quantify trajectory difficulty, allowing for better evaluation of model performance in challenging scenarios, which is often overlooked in traditional metrics.\nThe GEM dataset fills a significant gap by providing synchronized driver gaze, field-of-view, and GPS data, particularly suited for urban driving conditions with various traffic elements. This dataset enables more nuanced model training and testing, contributing to the broader research community.\n\n3. Extensive testing demonstrates that RouteFormer performs robustly in real-world scenarios, highlighting its potential for practical applications in driver-assistance and AD systems."
            },
            "weaknesses": {
                "value": "Please refer to the Questions section."
            },
            "questions": {
                "value": "1. I have a couple of concerns regarding the definition of corner-case scenarios. The authors propose a model that essentially employs a contrastive learning approach\u2014predicting a trajectory and then categorizing scenarios by comparing it with the ground truth. I believe this method introduces bias, as predictions differ across models. What evidence suggests that the prediction model used here is unbiased? While many recent works adopt similar methods to address long-tail issues, in this particular field, I find the approach still somewhat subjective.\n\n2. The authors have not clearly defined what constitutes the long-tail phenomenon. It\u2019s unclear why this method is capable of defining rare scenarios. I believe that this method, as described, does not adequately capture or define the essence of long-tail events. Could the authors provide further clarification?\n\n3. The manuscript is inspired by human visual attention during driving, so it should cite earlier works that introduced similar ideas in trajectory prediction. These are:\n(1) Human Observation-Inspired Trajectory Prediction for Autonomous Driving in Mixed-Autonomy Traffic Environments, IEEE International Conference on Robotics and Automation (ICRA 2024)\n(2) Less is More: Efficient Brain-Inspired Learning for Autonomous Driving Trajectory Prediction, European Conference on Artificial Intelligence (ECAI 2024)\n\n4. The related work section could benefit from a broader discussion of recent research. Currently, I find the discussion of the latest studies to be rather limited.\n\n5. The paper would benefit from a discussion section that addresses its limitations and future challenges. This would provide a more comprehensive perspective.\n\n6. Could the authors consider open-sourcing the project code and dataset? This would significantly contribute to the community.\n\nOverall, I believe this paper meets the publication standards for ICLR."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose an end-to-end multimodal ego-motion prediction network that utilizes driver\u2019s field-of-view data. Specifically, the proposed model predicts egocentric trajectory by taking scene videos, driver field-of-view videos, driver gaze position, and past trajectory as its input. Besides, the authors also propose a metric to measure the trajectory complexity, and an ego-motion dataset with driver positions and perspective. The experimental results show some improvements in the task of trajectory prediction task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed solution sounds solid in theory. Specially, considering driver\u2019s attention and intent as well as surrounding environment is convincing. The driver\u2019s attention and intent is a key impact factor of future trajectory.  \n2. The proposed dataset is helpful for other researchers to do more related research on that. \n3. The proposed metric of trajectory complexity is helpful. It is a good indicator while analyzing the performance of a self-driving algorithm.\n4. The ablation study and supplementary material are helpful. Readers can get more information from the ablation study results."
            },
            "weaknesses": {
                "value": "1. It will be better if the authors could report some failure cases. Especially for those cases which the predictions are totally opposite of the ground truth. \n2. It will be better if the authors could compare their solution with latest schemes published in Year 2023 or 2024."
            },
            "questions": {
                "value": "1. What if the estimated gaze positions move quickly (either caused by wrong prediction or the driver moves his/her gazes quickly)? Is there any negative impact on the final results?\n2. Line 190, the gaze positions are in the shape of Tx2. Does it mean we only use one gaze for each time stamp?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}