{
    "id": "HbqYdvL1mB",
    "title": "Point-Calibrated Spectral Neural Operators",
    "abstract": "Two typical neural models have been extensively studied for operator learning, learning in spatial space via attention mechanism or learning in spectral space via spectral analysis technique such as Fourier Transform. Spatial learning enables point-level flexibility but lacks global continuity constraint, while spectral learning enforces spectral continuity prior but lacks point-wise adaptivity. This work innovatively combines the continuity prior and the point-level flexibility, with the introduced Point-Calibrated Spectral Transform. It achieves this by calibrating the preset spectral eigenfunctions with the predicted point-wise frequency preference via neural gate mechanism. Beyond this, we introduce Point-Calibrated Spectral Neural Operators, which learn operator mappings by approximating functions with the point-level adaptive spectral basis, thereby not only preserving the benefits of spectral prior but also boasting the superior adaptability comparable to the attention mechanis. Comprehensive experiments demonstrate its consistent performance enhancement in extensive PDE solving scenarios.",
    "keywords": [
        "Operator Learning",
        "PDE"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "An innovative neural operator combining the point-level flexibility of attention mechanism and continuity prior of spectral-based methods.",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=HbqYdvL1mB",
    "pdf_link": "https://openreview.net/pdf?id=HbqYdvL1mB",
    "comments": [
        {
            "summary": {
                "value": "This work presents Point-Calibrated Spectral Mixer based Neural Operators (PCSM), enabling the point-level adaptivity in spectral-based neural operators by integrating point-wise frequency preference for spectral processing. The proposed Calibrated Spectral Transform holds significant potential applications in numerous spectrum-related deep models. Comprehensive experiments validate the superior performance in various PDE solving scenarios of PCSM, benefiting from the combination of spectral prior of spectral-based methods and point-level adaptivity of attention-based methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Good organization and structure of the paper, along with solid experimental analysis."
            },
            "weaknesses": {
                "value": "1.The authors did not provide a detailed explanation of how low frequency and high frequency are defined, as well as their respective roles.\n2.The authors' introduction of the POINT-CALIBRATED SPECTRAL NEURAL OPERATOR is simplistic and feels somewhat cursory.\n3.The introduction to neural operators is too brief, appearing only in section 2.1.1, which creates significant obstacles for later reading"
            },
            "questions": {
                "value": "1. A detailed explanation of how low frequency and high frequency are defined, along with their respective roles, should be included.\n2. The authors should provide a more detailed introduction to the POINT-CALIBRATED SPECTRAL NEURAL OPERATOR and include a textual comparison with other methods.\n3. The authors should expand on the introduction to neural operators and include transitional statements leading to section 2.2, POINT-CALIBRATED SPECTRAL NEURAL OPERATOR."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a Point-Calibrated Laplace-Beltrami Transform, which utilizes spatial information to assist the spectral selection of the spectral neural operator, effectively adapting to spatial variations in partial differential equation systems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This method is very effective at adapting to spatial variations in systems of partial differential equations, and the organization and writing of the paper are quite good."
            },
            "weaknesses": {
                "value": "Minor typos:\nThe \u2018Denotd\u2019 in Figure 1.\nQuestions:\n1. The Laplace-Beltrami Transform is an innovation inspired by the reference \"Learning Neural Operators on Riemannian Manifolds.\" This paper merely introduces Pointwise Frequency Preference Learning based on that work. The introduction of auxiliary modules inevitably increases model parameters and computational complexity, but the paper does not provide ablation studies on parameters. Therefore, it is unclear whether the performance improvement is due to the increased parameters or the proposed algorithm.\n2. The theoretical foundation is insufficient. The paper does not provide a unified kernel integral form as presented in [1]. Additionally, it does not prove that PCSM is equivalent to a learnable integral on \u2126 as demonstrated in [2].\n3. In the Zero-shot Resolution Generalization experiment presented in the paper, the training resolution of 211 \u00d7 51 is greater than that of the test resolution. So, how would the model perform when generalizing to even larger resolutions?\n\n[1] Neural operator: Learning maps between function spaces with applications to pdes. \n[2] Transolver: A Fast Transformer Solver for PDEs on General Geometries."
            },
            "questions": {
                "value": "Minor typos:\nThe \u2018Denotd\u2019 in Figure 1.\nQuestions:\n1. The Laplace-Beltrami Transform is an innovation inspired by the reference \"Learning Neural Operators on Riemannian Manifolds.\" This paper merely introduces Pointwise Frequency Preference Learning based on that work. The introduction of auxiliary modules inevitably increases model parameters and computational complexity, but the paper does not provide ablation studies on parameters. Therefore, it is unclear whether the performance improvement is due to the increased parameters or the proposed algorithm.\n2. The theoretical foundation is insufficient. The paper does not provide a unified kernel integral form as presented in [1]. Additionally, it does not prove that PCSM is equivalent to a learnable integral on \u2126 as demonstrated in [2].\n3. In the Zero-shot Resolution Generalization experiment presented in the paper, the training resolution of 211 \u00d7 51 is greater than that of the test resolution. So, how would the model perform when generalizing to even larger resolutions?\n\n[1] Neural operator: Learning maps between function spaces with applications to pdes. \n[2] Transolver: A Fast Transformer Solver for PDEs on General Geometries."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a new neural architecture termed point-calibrated spectral neural operators for learning-based PDE solving. \nThe key idea is to adaptively compute a spectral gate to modulate/calibrate the fixed eigenfunctions (spectral basis) of the Laplace-Beltrami transform. Experiments on multiple PDE problems (with structured or unstructured meshes) collectively demonstrate the proposed methods outperform the state-of-the-art method (e.g., the Transolver) by a substantial margin, in terms of not only the approximation accuracy but also the sample efficiency and cross-resolution generalizability."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "As I stated in the [Summary], the proposed method shows a clear improvement over the state-of-the-art in PDE fast solving. Also, the paper is in general clearly written and easy to digest."
            },
            "weaknesses": {
                "value": "However, the proposed method borrows significantly from the existing closely related works. Specifically, the spectral eigenfunctions (of the Laplace-Beltrami transform) are borrowed directly from an unpublished work [A]. And the basis form of the adaptive Gate functions (one of the key contributions claimed by authors) is widely employed in neural network literature (see, for instance, the squeeze-and-excitation network [B] proposed six years ago). For me, the main message claimed in this paper is that replacing the attention module by these aforementioned components in a classic transformer backbone leads to sizeable performance gain for neural operator learning. This is interesting, but may not be novel enough for ICLR publication. \n\nBeside, I would not buy the explanation of \"Pointwise Frequency Preference Learning\" claimed by the authors. Indeed, the gate values are computed in a point-wise manner. However, it doesn't suggest it acts like a frequency selector. If one would implement an adaptive frequency selector, only one single scalar should be multiplied with each eigenfunction instead of the elementwise multiplication. IMO, the gate calibrates/modulates the eigenfunctions to make them more suitable to characterize the given data, but not in the form of frequency preference. After all, frequency can only be evaluated by a set of points instead of a single point (the uncertanty principle)\n\n[A] Gengxiang Chen, Xu Liu, Qinglu Meng, Lu Chen, Changqing Liu, and Yingguang Li. Learning neural operators on riemannian manifolds. arXiv preprint arXiv:2302.08166, 2023.  \n[B] Hu, Jie, Li Shen, and Gang Sun. \"Squeeze-and-excitation networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}