{
    "id": "PtgfcMcQd5",
    "title": "An Information Theory of Compute-Optimal Size Scaling, Emergence, and Plateaus in Language Models",
    "abstract": "Recent empirical studies show three phenomena with increasing size of language models: compute-optimal size scaling, emergent capabilities, and performance plateauing. We present a simple unified mathematical framework to explain all of these language model scaling phenomena, building on recent skill-text bipartite graph frameworks for semantic learning. Modeling the learning of concepts from texts as an iterative process yields an analogy to iterative decoding of low-density parity check (LDPC) codes in information theory. Thence, drawing on finite-size scaling characterizations of LDPC decoding, we derive the compute-optimal size scaling (Chinchilla rule) for language models. Further, using tools from random network theory, we provide a simple explanation for both emergence of complex skills and plateauing of performance as the size of language models scale. We see multiple plateaus.",
    "keywords": [
        "Language models",
        "scaling law",
        "emergence",
        "plateauing",
        "Low-Density Parity Check codes",
        "sequential concept learning",
        "composition of skills."
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We present a simplified unified graph framework to explain compute-optimal size scaling, emergent capabilities, and performance plateauing using tools from iterative decoding in information theory and random network theory.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PtgfcMcQd5",
    "pdf_link": "https://openreview.net/pdf?id=PtgfcMcQd5",
    "comments": [
        {
            "summary": {
                "value": "This paper present a very interesting theory on large language model scaling laws using ideas from information theory, random graphs, and low-density parity-check codes. By drawing similarities between concept-text graphs, skill-concept graphs to Tanner graphs in low-density parity-check codes, the paper provides a theoretical explanations of the phenomenon observed in large language models, including compute-optimal scaling law, scaling law of excess entropy, emergence, and plateauing. The theoretical predicted scaling laws are compared with the empirical laws observed in real-world language models."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "*The paper presents a creative way of using existing theories in low-density parity-check codes and random graphs to solve large language model analysis problems. \n*It is convincing that one theory can be used to explain multiple phenomenon in large language model scaling.\n*The paper is of good quality, the theoretical analysis are solid.\n*The paper is well-written. \n*The theory is of significance, because understanding the scaling laws of large language models may provide further guidance for future model training and scaling."
            },
            "weaknesses": {
                "value": "*I think one assumption the paper makes is that the peeling process has stopped after the model training. Nowadays, many language models are only trained with several epochs. Can we always assume that the peeling process have already stopped after the training? \n*Not necessary in this paper, but it would be good to have more numerical experiments that this peeling process indeed happens in training.\n*The paper assumes that the graphs can be randomly generated, so that we have particular binomial degree distributions. Is it possible to verify indeed this is the case in the real-world. Do the theory in the paper also hold for other degree distributions."
            },
            "questions": {
                "value": "*Some minor issues, in appendix C, should the value in equation 29 is only approximately equal to the value in equation 30? Also, the approximation $(1-x)^n = 1-nx$ should be tight with some additional conditions. In this case, this approximation is tight, only when $p_b$ is also small.\n*In line 759, Chernoff's bounds have multiple forms. One reference should be given here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors proposed a theory for scaling in neural networks via a connection to LDPC code. The authors then claim that the theory leads to explanation of scaling, emergence, and plateaus in language models."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The goal of the paper is ambitious. A unifying theory that can help explain the scaling behavior will be quite nice."
            },
            "weaknesses": {
                "value": "Though the effort is appreciated, and there might be some merit to the overall approach, I feel the overall theory is not set up in sufficiently rigorous manner.\n\n1. Reading the paper gives me the feeling that authors are promising too much, but nothing has been theoretically established carefully. First of all, the framework is given in a very vague manner, with the basic definitions for text, concepts, and skills missing. Without rigorous definitions, we cannot come up with simple examples to verify the theory. For example, if I'm given a text sequence, how do I determine if it is a text, a concept, or a skill? Are they all limited by the representation and the length? The theory is shaky when we cannot even answer these basic questions.\n\n2. There seems to be less connection to ML than to LDPC. If we simply replace the text, concept, and skill nodes in the graph with variable and factor nodes in the original LDPC, then the claim holds trivially. This is an extremely weak connection, in names only, without any deeper insights.\n\n3. There are no relevant experimental results for language models at all in the paper. For a theory to hold, we must have some kind of verification with the ML models. The paper reads like this: there are some observations in ML, and it happens that similar effects also occur in other settings, so that theory will fit. This might be a good starting point to contemplate and develop deeper research, but scientific/engineering research cannot stop here.\n\n4. The big concern for me is that the theory is so vaguely defined that it is impossible to come up with experiments to verify whether it is true or not. This is a rather dangerous territory, as it does not follow the standard scientific research approach, and is more like a belief/religion in its current form."
            },
            "questions": {
                "value": "Simple questions as mentioned earlier:\n1. How to define text, concepts, and skills, and given some test or bit sequence, how do we determine which category it falls into?\n2. What experiments can be used to verify the theory?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an abstraction of the learning process of a large language model using graphs and analyze the behavior of certain dynamics on these graphs. The purpose is to draw a connection to certain observed phenomena of large language models such as the emergence of learned capabilities and the plateauing of performance with size-scaling. \n\nI found the topic and the framework interesting and worthy of a discussion. However, there are many weaknesses in writing, presentation, and technical issues that makes it difficult to evaluate the paper\u2019s contributions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Motivation: The framework seems reasonable and diverse enough to model learning and acquisition of new skills. As the authors mentioned, it may be possible to incorporate into the framework additional considerations such as the quality of training data. It would be interesting to discover what can we learn in the training of language models due to this analysis. \n\nThe topic and theoretical framework are related to other recent works, hence it seems like this topic has created enough interest in the community. \n\nUsing tools developed in coding theory and random graph theory in probability seems to have great potential in this context. The authors say \u201cOur work takes a step in grounding empirical phenomena observed in size scaling of language models on a rigorous mathematical footing.\u201d I would put it differently: ``The work proposes a graph-based model for learning skills from texts that appears relevant to the process of training a large language model. The dynamics of message passing under an asymptotic scaling of the model\u2019s size experience phenomenon akin to those empirically observed in this training process of a large language model, such as the emergence of abilities and the plateauing of performance.\u2019\u2019"
            },
            "weaknesses": {
                "value": "Background and Motivation.\nIt is unclear what parts of the framework have been considered in previous works. For example, did other work also consider hierarchical skills? Did previous works also try to analyze message-passing dynamics but perhaps using different tools? How significant is the automatic selection of scaling law from the framework compared to previous works? What is the rationale that N (#of parameters) is proportional to R (# of skills)? What is the rationale for studying a limited computed budget and thus the tradeoff N T < C? Do we see such a tradeoff in training language models? \n\nAlso, in the abstract and introduction, please be clear whether \u2018size\u2019 means the number of parameters or the size of training data. \n\nThe connection to language models is not immediate and it appears that one can replace the language model with any form of a learning machine, like a human. Consequently, I\u2019d expect in this line of work at least some discussions on studies on learning and education in general. Also related: Line 102 says that a language model \u201cchooses to learn.\u201d This seems like a poor choice of words because it is unclear what is the choosing mechanism. Another similar poor choice of word or issue with the motivation is L245: \u201cThe goal of the language model is to learn as many concepts as possible\u2026\u201d Of course, actual training in a language model does not directly involve the concept of \u201cconcepts\u201d.\n\nThe authors say that they use tools from information theory but this is inaccurate because they use statistical mechanics ideas from graph-based coding (Richardson & Urbanke 2008) and random graph theory. The terms information theory and especially non-asymptotic information theory are misleading.  \n\n\nAnalysis. \nIn general, I found it difficult to follow almost all technical derivations due to missing details. Examples:\nWhat are the {Pi} s in L124? \nWhat is the \u201cmatching condition\u201d in L316\nWhat is r in Eq 2 ? I don\u2019t see how this equation is equivalent to the problem in Eq 1.\nWhat are r and epsilon in Eq 3? \nWhat is epsilon in Eqs 7 & 8?\nWhat is the origin of the name \u201cpost-decoding bit erasure rate\u201d?\nWhat is \u201cexcess entropy\u201d? Why do we care about bounding it from below?\n\nThe paper mentions compute-optimal scaling but I did not find a clear definition. \nThe paper does not explain what the author calls the ``decoding process\u2019\u2019. \n\nProposition 1 appears to be flawed. It is not true that if neither R/T = o(1) nor T/R = o(1) hold then R/T \u2192 constant (the text actually says \u201cR/T must be a constant\u201d, which is grossly incorrect). As a counterexample, take R = sin(C), T=cos(C). To my understanding, the author quotes this proposition as one of the main contributions of their work, so this needs correction. This proposition is also unclear because the notion of \u201ccompute-optimal performance of a language model\u201d has not been well-defined earlier."
            },
            "questions": {
                "value": "Some suggestions:\nPlease specify in the setup that the skills graph obeys the  Erdos-Renyi random graph model, as you later claim in L346.\n\nParagraphs in Lines 210-215 are related to the high-level discussion of \u201cwhat is a skill\u201d and therefore should be part of the exposition. \n\nThe study would have a much better case if the authors could show measurable quantities like accuracy and the number of learned skills obtained from the model side by side with those obtained from actual language models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides an unified mathematical framework to explain three phenomena with increasing size of language models: compute-optimal size scaling, emergent capabilities, and performance plateauing. This framework is proposed based on the notation of learning as two levels: (1) a set of concepts are learnt from a set of texts;  (2) learning concepts enables the language model to acquire skills. The authors prove the compute-optimal (Chinchilla) scaling rule based on non-asymptotic information-theoretic tools to the bipartite graph between texts and concepts, and explain the emergence and plateauing phenomena based on the density of connections in the skill-graphs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper has some strengths:\n\n+ The authors can explain three phenomena mentioned above by using their unified framework. \n+ The trick to solve the optimisation problem (1) by considering the peeling process of learning concepts from texts to be identical to belief propagation decoding of an LDPC code when the channel noise is erasure is interesting."
            },
            "weaknesses": {
                "value": "This paper contains some weaknesses:\n+ Some mathematical approximations should be carefully re-thought (please see my question below).\n+ The gap between empirical excess entropy and its lower bound are too big."
            },
            "questions": {
                "value": "The LHS of (4) is an expression that does not depend on $\\epsilon$. However, the approximation expression in the RHS of (4) depends on $\\epsilon$. How can we explain about this approximation? Can $\\epsilon$ take arbitrary value as mentioned in the proof  in Appendix A.2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}