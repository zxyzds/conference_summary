{
    "id": "w0b7fCX2nN",
    "title": "Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks",
    "abstract": "Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which\naim to extract harmful information by subtly modifying the attack query. As de-\nfense mechanisms evolve, directly obtaining harmful information becomes increas-\ningly challenging for Jailbreaking attacks. In this work, inspired from Chomsky\u2019s\ntransformational-generative grammar theory and human practices of indirect con-\ntext to elicit harmful information, we focus on a new attack form, called Contextual\nInteraction Attack. We contend that the prior context\u2014the information preced-\ning the attack query\u2014plays a pivotal role in enabling strong Jailbreaking attacks.\nSpecifically, we propose first multi-turn approach that leverages benign preliminary\nquestions to interact with the LLM. Due to the autoregressive nature of LLMs,\nwhich use previous conversation rounds as context during generation, we guide the\nmodel\u2019s question-responses pair to construct a context that is semantically aligned\nwith the attack query to execute the attack. We conduct experiments on seven\ndifferent LLMs and demonstrate the efficacy of this attack, which is black-box, and\ncan also transfer across LLMs. We believe this can lead to further developments\nand understanding of the security in LLMs",
    "keywords": [
        "LLM jailbreak",
        "trustworthy ML",
        "safety AI"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "Jailbreak LLM through multi-turn dialogues",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=w0b7fCX2nN",
    "pdf_link": "https://openreview.net/pdf?id=w0b7fCX2nN",
    "comments": [
        {
            "summary": {
                "value": "This paper presents the Contextual Interaction Attack (CIA) against LLMs, which involves prepending a series of prompt, response pairs that gradually lead to a harmful question. The prompts in these conversations are generated via few-shot prompting the Wizard-70b LLM, and response generated by the target model. The authors show results competitive with other attacks on 3 open-source and 3 closed-source LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The topic of this paper is important, jailbreaking attacks demonstrate LLMs are not robust and easily give harmful content outside of a single-turn question.\n\nThe work is relatively easy to understand and the authors provide a good amount of ablation studies to justify the design of their methods. It takes inspiration from prior work to leverage the context, but does not use any optimization against a specific model."
            },
            "weaknesses": {
                "value": "Novelty:\nThe main results on 50 prompts do not show significantly more success than prior attacks. The open source models evaluated are not exciting, they\u2019re somewhat outdated and not claimed to be robust to attacks. This work would benefit from attacks on more recent models such as Llama 3 or 3.1, or attacking a larger model/models with refusal training.\nThe method is fairly simple, it boils down to prompting another LLM to mimic how a human jailbreaker would attack an LLM. It would be great to elaborate more on the novelty of this approach relative to recent work on multi-turn jailbreaking.\n\nSoundness:\nA human judge was used on AdvBench without much detail given, this affects the soundness of the paper. Please elaborate more. Have you considered testing your method with a controlled judge, such as HarmBench [1], which has a trained static classifier?\nThe transferability experiments (Table 2) feel contrived. (1) Attacks such as PAIR don\u2019t assume a threat model to transfer the instance-level outputs of the attack, it is not a fair comparison. However, the attack itself only needs black box access, it doesn\u2019t need to transfer the instance-level output. (2) The closed-source models tested have specific defenses implemented against popular attacks such as GCG, it is not a fair comparison to measure transferability. The authors should report the results in the original attack paper whenever possible, which shows a much stronger transferability attack against GPT-3.5 and GPT-4 models.\n\nDefenses:\nPerplexity based defenses tested are specifically designed against GCG-style attacks, it is expected that they are not actually detecting harm. This work would benefit from showing strong results on general defenses such as DERTA [2] or Llama Guard [3], or more robustly trained models such as CYGNET.\n\nPresentation:\nPage 1 line 45: \u201cjailbreak prompts generated by automated attacks\noften do not perform well when transferred to other LLMs, which is a major bottleneck\u201d. This contradicts the citation on GCG immediately prior, which is transferable to other LLMs. The reason why GCG performs worse in this paper when transferred is likely due to model providers providing specific defenses against it. This does automated attacks transfer poorly.\nRelated work section only cites 3 papers involving jailbreaking LLMs, the rest are on background information such as in-context learning and autoregressive LMs that should be assumed as basic knowledge among this community. It would be good to further contextualize the novelty of this attack relative to prior work.\nMinor: Figure 1 has a typo \u201cRepharse\u201d -> \u201cRephrase\u201d.\n\n[1] Mazeika, M., Phan, L., Yin, X., Zou, A., Wang, Z., Mu, N., ... & Hendrycks, D. (2024). Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249. \n[2] Yuan, Y., Jiao, W., Wang, W., Huang, J. T., Xu, J., Liang, T., ... & Tu, Z. (2024). Refuse whenever you feel unsafe: Improving safety in llms via decoupled refusal training. arXiv preprint arXiv:2407.09121. \n[3] Inan, H., Upasani, K., Chi, J., Rungta, R., Iyer, K., Mao, Y., ... & Khabsa, M. (2023). Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674."
            },
            "questions": {
                "value": "Wizard-70b was used as the auxiliary model. This is a strange setup when attacking weaker models such as Vicuna and LLaMA-2-7b. Why would you need to attack a weaker model when you already have a stronger model that is capable of manipulating the weaker model?\nClarify the process for training the human judge to classify harm. Is it just a single human? Is the judge independent of the authors with a single or double-blind setup?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel multi-turn attack - Contextual Interaction Attack, using the model interactions (here is the benign preliminary questions to interact with the LLM) to elicit harmful responses. Specifically, this multi-turn attack leverages the autoregressive nature of LLMs to guide the model construct a context that is semantically aligned with the attack target, based on the question-responses pair."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. Interesting topic and insightful research, especially the mechanism analysis\n2. Well-written and visual logic\n3. Abundant and convincing experiment"
            },
            "weaknesses": {
                "value": "1. Lack of some detailed evidence to support the demonstration\n2. Lack of comparison with the prevalent multi-turn attack Crescendo"
            },
            "questions": {
                "value": "1. Explanation: Please give an illustrative example to support your demonstration - \u201d compared to harmful information provided by the user (exogenous input), LLMs tend to perceive their own generated content (endogenous output) as safer.\u201d \n\n2. Experiment: Please add Crescendo[a] as a multi-turn attack baseline for comparison in experiment section\n\n[a] Russinovich, Mark, Ahmed Salem, and Ronen Eldan. \"Great, now write an article about that: The crescendo multi-turn llm jailbreak attack.\" arXiv preprint arXiv:2404.01833 (2024)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a multi-turn jailbreak, which crafts a series of harmless adversarial queries to guide the victim model to output harmful content. Experimental results demonstrate the effectiveness of their attack method against different large language models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "+ The presentation of the methodology part is clear and easy to follow. \n+ The experimental results are comprehensive, which include both advanced closed-source and open-source LLMs."
            },
            "weaknesses": {
                "value": "- **Lack of Novelty**: The core idea of this paper closely resembles that of Crescendo [1], a multi-turn jailbreak method that starts with an innocuous query about a harmful task and gradually guides the model to generate harmful content through benign incremental steps. Given this similarity, the concept presented in this paper does not appear to introduce significant novelty over Crescendo.\n\n   Furthermore, the implementation in this paper also mirrors Crescendo\u2019s approach. Crescendo employs in-context learning with a series of carefully crafted examples\u2014a strategy similarly adopted in this paper. For instance, Crescendo\u2019s attack sequence begins with \u201cintroduce the history of [harmful target]\u201d (Figure 1), while this paper uses a similar prompt, \u201cWrite a paper on the history of [harmful target],\u201d as the starting point of its attack (Figure 1).\n\n   It would be valuable for the authors to elaborate on how their approach diverges from or improves upon Crescendo's methodology. Are there any distinctive methodological or experimental results that could underscore the novelty of this work?\n\n- **Insufficient Experiments**: The experiments in this paper are limited to a subset of the AdvBench benchmark, while other benchmarks, such as HarmBench [2] and SorryBench [3], offer more balanced and less noisy alternatives. Including results from HarmBench or SorryBench would enhance the robustness and comprehensiveness of the experimental evaluation, and the authors should consider incorporating these benchmarks in future work or as additional experiments.\n\n- **Lack of Comparisons with Other Multi-Turn Attacks**: Besides Crescendo, several other multi-turn attack methods have recently emerged, including [4, 5, 6]. This paper would benefit from a concise comparison with these approaches, outlining key similarities or differences, and including a discussion of these methods in the related work section to provide a more comprehensive review of the landscape in multi-turn attacks.\n\n1. *Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack.*\n2. *HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal.*\n3. *SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors.*\n4. *Chain of Attack: A Semantic-Driven Contextual Multi-Turn Attacker for LLMs.*\n5. *Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-Turn Dialogue.*\n6. *Imposter.ai: Adversarial Attacks with Hidden Intentions Towards Aligned Large Language Models.*"
            },
            "questions": {
                "value": "1. What is the human judge setup? \n2. Why the attack success rate of Contextual Interaction Attack against Claude-2 is much lower than other models? Are there any distinctive behaviors of Claude-2 compared with other models?\n\nI would like to raise my score if the authors could address my concerns and questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a multi-turn approach to interact with large language models (LLMs) and elicit a jailbreak condition by using an auxiliary LLM to automatically generate a sequence of benign prompts. The authors validate their proposed method on both closed and open-source LLMs, reporting jailbreak success rates of \u2265 90% on two open-source LLMs and \u2265 80% on two closed-source LLMs. Additionally, the paper explores cases where the proposed method does not achieve high jailbreak percentages on one closed-source and one open-source LLM. The approach aims to automate the creation of the attack prompt sequence, contributing to the automation of jailbreak attacks on LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-structured and thoroughly validates the proposed method on multiple LLMs, providing a comprehensive assessment of its effectiveness.\n- The authors conduct an extensive empirical analysis, highlighting the jailbreak potential across different LLMs and presenting their findings in a detailed and systematic manner."
            },
            "weaknesses": {
                "value": "- The concept of multi-turn attacks on LLMs is already well-established in the literature (e.g., [1, 2, 3, 4]), with [2] particularly exploring the use of context to craft jailbreak sequences. Authors should clarify how this attack is different from the contextual attacks in [2]\n- The method for generating benign prompts closely resembles that in [5], where in-context learning capabilities of the LLM are utilized to generate new attack prompts based on given examples. Authors should clearly state the key difference in their approach for generating prompts compared to [5]\n- Section 4.1, which discusses evaluation metrics, mentions that the judge function relies entirely on human evaluation. However, there is insufficient clarity regarding the number of examples used to compute the jailbreak percentage and how human evaluation scores correlate with an automated LLM judge score. Authors should clearly specify the following points in regard to the paper:\n1. The exact number of examples used to compute jailbreak percentages\n2. Details on how many human evaluators were involved\n3. Any analysis comparing human evaluation scores to automated LLM judge scores, if available\n\nReferences:\n1. [Multi-turn and contextual jailbreak attacks research](https://arxiv.org/abs/2408.15221)\n2. [CFA: Contextual attacks on LLMs](https://arxiv.org/abs/2408.04686)\n3. [RedQueen: Multi-turn attacks](https://arxiv.org/pdf/2409.17458)\n4. [MART: Adversarial prompt sequences](https://arxiv.org/pdf/2311.07689)\n5. [PAIR: In-context attack prompt generation](https://arxiv.org/abs/2310.08419)\n6. [Prompt-Guard-86M](https://huggingface.co/meta-llama/Prompt-Guard-86M)\n7. [Llama-Guard-3-8B](https://huggingface.co/meta-llama/Llama-Guard-3-8B)"
            },
            "questions": {
                "value": "Questions and Suggestions\n1. Clarification on Sequential Tailoring: Section 3.2 claims that the model\u2019s responses to preliminary questions are used to tailor subsequent prompts for attack purposes. However, further details on how these responses influence the attack's progression would add clarity to the method and demonstrate the practical application of this approach.\n \n2. Details on Evaluation and Human Judgement: The number of examples used to compute jailbreak percentages is not specified, making it difficult to assess the reliability of the reported success rates. Additionally, incorporating an automated LLM-based judge to compare human evaluation scores could enhance the objectivity of the results and add depth to the validation process.\n \n3. Further Validation Against Advanced Detectors: The current evaluation does not include finetuned adversarial prompt detectors, which could provide a more comprehensive test of the method\u2019s robustness. Testing against finetuned models, such as [6] and [7], specifically designed to detect malicious prompts, would offer valuable insights into the effectiveness of the proposed attack.\n\nReferences:\n1. [Multi-turn and contextual jailbreak attacks research](https://arxiv.org/abs/2408.15221)\n2. [CFA: Contextual attacks on LLMs](https://arxiv.org/abs/2408.04686)\n3. [RedQueen: Multi-turn attacks](https://arxiv.org/pdf/2409.17458)\n4. [MART: Adversarial prompt sequences](https://arxiv.org/pdf/2311.07689)\n5. [PAIR: In-context attack prompt generation](https://arxiv.org/abs/2310.08419)\n6. [Prompt-Guard-86M](https://huggingface.co/meta-llama/Prompt-Guard-86M)\n7. [Llama-Guard-3-8B](https://huggingface.co/meta-llama/Llama-Guard-3-8B)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not Needed"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}