{
    "id": "x8jxf3byli",
    "title": "TWO STAGES DOMAIN INVARIANT REPRESENTATION LEARNERS SOLVE THE LARGE CO-VARIATE SHIFT IN UNSUPERVISED DOMAIN ADAPTATION WITH TWO DIMENSIONAL DATA DOMAINS",
    "abstract": "Recent developments in the unsupervised domain adaptation (UDA) enable the unsupervised machine learning (ML) prediction for target data, thus this will accelerate real world applications with ML models such as image recognition tasks in self-driving. Researchers have reported the UDA techniques are not working well under large co-variate shift problems where e.g. supervised source data consists of handwritten digits data in monotone color and unsupervised target data colored digits data from the street view. Thus there is a need for a method to resolve co-variate shift and transfer source labelling rules under this dynamics. We perform two stages domain invariant representation learning to bridge the gap between source and target with semantic intermediate data (unsupervised). The proposed method can learn domain invariant features simultaneously between source and intermediate also intermediate and target. Finally this achieves good domain invariant representation between source and target plus task discriminability owing to source labels. This induction for the gradient descent search greatly eases learning convergence in terms of classification performance for target data even when large co-variate shift. We also derive a theorem for measuring the gap between trained models and unsupervised target labelling rules, which is necessary for the free parameters optimization. Finally we demonstrate that proposing method is superiority to previous UDA methods using 4 representative ML classification datasets including 38 UDA tasks. Our experiment will be a basis for challenging UDA problems with large co-variate shift.",
    "keywords": [
        "domain invariant representation learning",
        "unsupervised domain adaptation",
        "image recognition",
        "signal processing",
        "classification"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "Novel method of domain invariant representation learning for large co-variate shift in unsupervised domain adaptation problem",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=x8jxf3byli",
    "pdf_link": "https://openreview.net/pdf?id=x8jxf3byli",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a two-stage domain invariant representation learning method to address large co-variate shifts in unsupervised domain adaptation. The approach uses intermediate, unlabeled data to create smoother transitions between source and target domains, aiming to enhance classification performance under challenging conditions. The authors claim their method outperforms existing UDA models, especially when co-variate shifts are significant."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The idea of utilizing intermediate data to smooth the domain adaptation process sounds reasonable."
            },
            "weaknesses": {
                "value": "1. Poor clarity and organization. The paper is challenging to read, with many grammatical errors, convoluted language and unclear explanations of the methodology.\n2. Lack of rigorous validation: The theoretical claims, especially the effectiveness of two-stage learning and parameter optimization, lack sufficient mathematical justification and empirical support. In line 62, the authors claim that \"intermediate data (unsupervised) between source and target to ensure simultaneous domain invariance between source and intermediate data and invariance between intermediate and final target data\". Doesn't this imply the source and target data are domain invariant as well?\n3. The text is verbose and repetitive, making it difficult to extract key insights and understand the novelty compared to prior methods."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a two-stage domain-invariant representation learning approach for UDA under large co-variate shifts. It uses intermediate unsupervised data to bridge the gap between source and target domains. Additionally, a theoretical framework is proposed for parameter tuning without requiring target labels."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper addresses a critical limitation in UDA by focusing on large co-variate shifts in two-dimensional data domains, which are common in real-world applications such as autonomous driving, human activity recognition."
            },
            "weaknesses": {
                "value": "1.  The main limitation of the proposed method is its dependency on an intermediate, unsupervised domain that is semantically related to both the source and target domains. This dataset may not always be available or feasible to collect, especially in real-world applications with limited resources.\n2. The practical application of the proposed method is not fully demonstrated. The benefits remain largely theoretical, making it hard for readers to grasp its relevance without concrete examples of its impact on model performance.\n3. Lacks a deep discussion that contextualizes how this approach builds upon or diverges from existing UDA methods. An analysis/experiment can be done to show how the proposed method addresses specific limitations in prior approaches, such as domain-adversarial training or correlation alignment techniques."
            },
            "questions": {
                "value": "1. How would the method perform if an intermediate, unsupervised domain were unavailable or of low quality? Are there alternative approaches, such as synthetic data generation or transfer learning, that could help mitigate this dependency? Could the authors provide guidance on selecting or creating intermediate datasets for cases where a semantically related domain is not readily available?\n\n2. Can the authors provide specific examples or case studies to demonstrate the practical impact of the proposed parameter tuning framework on model performance? Would an experiment isolating the effects of the parameter tuning framework help clarify its practical benefits? If so, could the authors consider adding one to the paper?\n\n3. How does the proposed method address specific limitations of existing UDA techniques, such as domain-adversarial training or correlation alignment? Could the authors provide a comparison experiment or detailed analysis that highlights the advantages and trade-offs of this two-stage approach relative to traditional UDA methods?\n\n4. How does the proposed method handle highly heterogeneous domains where intermediate data is noisy or contains varying domain characteristics?\n\n5. Could the authors include ablation studies to isolate the effects of each stage in the two-stage process? This might help clarify the contributions of each component in achieving domain invariance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a simple two-stage domain adaptation method by feeding source domain, intermediate domain and target domain into the model. The intermediate domain overcomes the large covariate shift problem that is widely a challenge of domain adaptation. This paper further proposes a free parameter indicator with reverse validation strategy."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The two-stage domain adaptation method, i.e. two-stage DANN sound good and interesting.\n2. The reverse validation based idea is interesting and not frequent in domain adaptation community.\n3. The two-stage strategy can be scalable to other method such as CORAL."
            },
            "weaknesses": {
                "value": "1. Domain adaptation has undergo a wide study in the past decade. However, with the multimodal large language model, domain gap has been allievated from another way, such as CLIP based [1,2]. Therefore, in this submission, the impact of this proposal may be weak.\n2. There lacks sufficient comparisons to previous SOTAs in recent works [3], particularly large vision-language model based and prompt based [4].\n3. The writting should be further improved for easier reading.\n4. Using intermediate domain as a bridge is not new because there are a wide research in DA with intermediate state [6, 7].\n5. As the algorithms of 2 stage DANN shows, the intermediate domain is required. I thinks how to obtain intermediate domain is still an open question. This is not discussed.\n6. Transformer based DA models should also be discussed since it has been widely used in domain adaptation [8, 9]. \n\n[1] Learning transferable visual models from natural language supervision. In ICML, pages 8748\u20138763. PMLR, 2021.\n[2] Ad-clip: Adapting domains in prompt space using clip. In ICCV, pages 4355\u20134364, 2023.\n[3] Gradient Harmonization in Unsupervised Domain Adaptation. In IEEE TPAMI, 2024.\n[4] Domain adaptation via prompt learning. In IEEE TNNLS, 2023.\n[5] Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation. In CVPR, 2023.\n[6] Semi-Supervised Domain Generalization with Evolving Intermediate Domain. In PR, 2023.\n[7] Manifold Criterion Guided Transfer Learning via Intermediate Domain Generation. In IEEE TNNSL, 2019.\n[8] Tvt: Transferable vision transformer for unsupervised domain adaptation. In WACV, 2023.\n[9]  Safe self-refinement for transformer-based domain adaptation. In CVPR, 2022."
            },
            "questions": {
                "value": "1. If the reverse validation like idea is always effective?\n2. This is more like a training strategy by commenting on step-by-step manner comparing to end-to-end manner."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors of this manuscript propose a two-stage domain-invariant representation learning method, which uses semantic intermediate data to bridge the gap between source and target domains. This method improves classification performance even under large covariate shifts by learning domain-invariant features and optimizing task discriminability through source labels. The paper also introduces a theorem for optimizing free parameters by measuring the gap between trained models and target labeling rules. The proposed method outperforms previous UDA techniques across 38 tasks in 4 representative ML datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. A new approach was proposed\n2. It provides an automated free-parameter tuning method without needing access to target ground truth labels."
            },
            "weaknesses": {
                "value": "1. English was not used properly:\n    a) line 52: \"did not be\"\n    b) line 498: \"It can be read that\" sounds awkward.\n    ...\nLots of sentences in this manuscript are not authentic, making it hard to follow the manuscript's content. I strongly recommend authors taking times to improve the presentation of this work.\n\n2. No related works section.\n\n3. The x, y axis labels for figure 4-6 are not easily visible."
            },
            "questions": {
                "value": "Use cases of the proposed methods are too limited. It was designed for two dimensional data. What if the source and target data are in high dimensional space?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses large co-variate shift problems in UDA, particularly when dealing with two-dimensional co-variate shifts. The proposed method uses an intermediate unsupervised dataset to bridge the gap between source and target domains, learning domain-invariant features simultaneously between source-intermediate and intermediate-target pairs, which helps achieve better domain adaptation compared to direct source-target adaptation. The authors also derive a theorem for measuring the gap between trained models and unsupervised target labelling rules, which helps optimize free parameters without access to target labels. The proposed method is validated on classification 4 datasets including 38 UDA tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper introduces a novel and effective solution to a practical challenge in machine learning by using intermediate data to bridge large domain gaps.\n2. The proposed method is versatile as it can be integrated with various domain invariant representation learning techniques.\n3. The authors derive a theorem for measuring the gap between trained models and unsupervised target labelling rules for hyper-parameters searching."
            },
            "weaknesses": {
                "value": "1.\tThis paper requires substantial improvement in terms of writing quality and clarity of presentation. (1) Many notations are without proper definition, (e.g., D_S, \\hat{y}^S_{i,j}, N appear before being formally introduced); (2) The language is frequently imprecise and contains awkward expressions that hinder understanding. I strongly recommend the authors to thoroughly revise the mathematical notations with clear definitions and improve sentence structures and word choices for greater presentation.\n2.\tThe authors assume the existence and availability of appropriate intermediate domains that perfectly fit their \"two-dimensional domain shift\" framework, but do not adequately address how to identify or construct such intermediate domains in real-world applications. For example, while it is intuitive to have MNIST-M as an intermediate domain between MNIST and SVHN, in most real-world scenarios, it is unclear: (1) How to systematically identify the two dimensions of domain shift; (2) How to obtain or construct suitable intermediate domain data; (3) What to do when clean intermediate domains are unavailable or when domain shifts are more complex than two-dimensional. Without addressing these practical concerns, the proposed method may have limited applicability in real-world domain adaptation problems.\n3.\tThe technical novelty of the proposed method is limited. The approach is essentially a straightforward extension of existing domain-invariant learning methods. It merely splits the domain adaptation loss into two components (L_domain(S,T) + L_domain(T,T')) and applying standard adversarial training techniques, which lacks novel methodological designs in terms of loss function formulation, optimization strategy, or network architecture.\n4.\tThe proposed parameter selection method is incremental. The proposed approach is essentially a straightforward adaptation of the existing Reverse Validation (RV) method to the two-stage domain adaptation setting, without substantial methodological innovations.\n5.\tThe experimental evaluation of the paper is not up to current standards in domain adaptation research. The comparisons are limited to classical UDA methods like DANNs and Deep CORAL, while ignoring numerous recent advanced techniques that have shown significant improvements in handling large domain gaps."
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}