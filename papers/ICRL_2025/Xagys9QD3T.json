{
    "id": "Xagys9QD3T",
    "title": "Pseudo-Probability Unlearning: Towards Efficient and Privacy-Preserving Machine Unlearning",
    "abstract": "Machine unlearning\u2014enabling a trained model to forget specific data\u2014is crucial for addressing biased data and adhering to privacy regulations like the General Data Protection Regulation (GDPR)'s ``right to be forgotten.\" Recent works have paid little attention to privacy concerns, leaving the data intended for forgetting vulnerable to membership inference attacks. Moreover, they often come with high computational overhead. In this work, we propose Pseudo-Probability Unlearning (PPU), a novel method that enables models to forget data efficiently and in a privacy-preserving manner. Our method replaces the final-layer output probabilities of the neural network with pseudo-probabilities for the data to be forgotten. These pseudo-probabilities follow either a uniform distribution or align with the model\u2019s overall distribution, enhancing privacy and reducing risk of membership inference attacks. Our optimization strategy further refines the predictive probability distributions and updates the model's weights accordingly, ensuring effective forgetting with minimal impact on the model's overall performance. Through comprehensive experiments on multiple benchmarks, our method achieves over 20\\% improvements in forgetting error compared to the state-of-the-art. Additionally, our method enhances privacy by preventing the forgotten set from being inferred to around random guesses.",
    "keywords": [
        "machine unlearning",
        "deep learning"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We propose a Pseudo-Probability Unlearning method that enables models to forget data efficiently while preserving privacy.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Xagys9QD3T",
    "pdf_link": "https://openreview.net/pdf?id=Xagys9QD3T",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes an efficient machine unlearning solution, Privacy-Preserving Unlearning (PPU), that aims to minimize privacy leakage while maintaining model performance. The proposed algorithm is evaluated on two datasets and two deep neural network models, demonstrating its effectiveness in reducing the forget error on forgotten data. The paper provides a comprehensive evaluation of the algorithm and compares it with multiple baseline unlearning methods."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The paper addresses an important problem of efficient and privacy-preserving machine learning and provides a practical solution.\n- The evaluation covers two datasets, two models, and multiple baseline machine unlearning algorithms."
            },
            "weaknesses": {
                "value": "- The proposed PPU algorithm seems to only work for deep neural networks and classification tasks and may not be applicable to other types of models or tasks.\n- The optimization goal of the PPU algorithm could be wrong. The algorithm aims to maximize the forget error on forgotten data instead of minimizing the discrepancy between the original and unlearned models.\n- In the evaluation section, the retain error and forget error metrics could be further explained to provide more insights into the algorithm's behavior.\n- There are many editing issues throughout the paper, such as typos, missing citations, and missing figures."
            },
            "questions": {
                "value": "1. Why does the proposed algorithm focus on minimizing the forget error on forgotten data instead of minimizing the discrepancy between the retrained and unlearned models?\n2. Have you considered scenarios where the model's prediction confidence may not accurately reflect the sensitivity of the data?\n3. What are the key factors that influence the trade-off between privacy preservation and model performance in the context of machine unlearning?\n4. The proposed algorithm seems to only work for classification tasks and small datasets. How scalable is the proposed algorithm to large-scale datasets and complex models? How easy or difficult to adapt the proposed PPU to more challenging datasets or tasks, for instance ImageNet dataset, regression model, or generative model?\n5. Can you provide more insights into the retain error and forget error metrics and how they reflect the unlearning algorithm's performance in different scenarios?\n6. I do not see the forgot error comparison in Figure 3, and there is also no Figure 4 to support the claim that \"According to Figure 3, PPU\u2019s forget error is very close to that of retraining, particularly in the Lacuna-10 experiment, where it is the closest match. In the membership inference attack experiment, shown in Figure 4, PPU consistently achieves nearly 50% accuracy, indicating strong privacy preservation\" in Line 408. Can you please check on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes Pseodo-Probability Unlearning (PPU), a novel method that enables models to forget data efficiently and in a privacy-preserving manner. PPU replaces the final-layer output probabilities of the neural network with pseudo-probabilities for the data to be forgotten. The pseudo-probabilities are initialized from some distributions (e.g., a uniform distribution) and are further refined to maintain performance on the remaining data. Extensive experiments demonstrate the efficiency of PPU while preventing privacy leakage."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The idea of modifying the final-layer output probabilities of the neural network to pseudo-probabilities for machine unlearning is novel.\n2. The paper is overall well-structured and generally easy to follow."
            },
            "weaknesses": {
                "value": "1. Adding an independent paragraph for contributions in the Introduction Section would be better.\n2. The method SISA is considered to be exact unlearning.\n3. Some typos. \"Figure ??\" on line 080. $\\mathcal D_r$ and $D_{fog}$ on line 139. \"The dual variables $\\lambda_k$\" on line 269. Missing space characters after $\\lambda$s on line 478 and line 480.\n4. Miss descriptions of Table 3 in the main text.\n5. The font size of the text in the figures is too small.\n6. Reuse of the notation $f$ for both the model and the forget set.\n7. Experiments on larger datasets and networks (e.g., ImageNet) would be better."
            },
            "questions": {
                "value": "1. Should $\\lambda_k$ in Section 4.2.2 be $\\alpha_k$?\n2. Should \"retrain error\" on line 485 be \"retain error\"?\n3. What is $w_k$ in Equation 6?\n4. The goals in Section 5.4 and Section 5.5 are different. Are the two goals contradictory? Can PPU achieve both of them simultaneously?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This research studies methods for achieving unlearning, which involves forgetting information about specific data from a trained model. In particular, it points out that with existing unlearning methods, it is possible to identify whether or not a piece of data has been unlearned by using a membership inference attack and that this can lead to privacy violations. This research proposes a method that enables unlearning while preventing such privacy violations. Specifically, the authors propose a method that not only makes it impossible to classify the data to be forgotten but also makes the output probability of the final layer when classifying the data to be forgotten to match a uniform distribution or a distribution with random probabilities for each class. They argue that this prevents privacy violations associated with unlearning. They also consider optimization methods for implementing such a method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The perspective on privacy violations of unlearning through membership inference is important, and this paper introduces this perspective as a new contribution."
            },
            "weaknesses": {
                "value": "It is not sufficiently discussed whether the proposed method can indeed prevent privacy violations of unlearning through membership inference. In addition, there is no experimental comparison with other methods from this perspective."
            },
            "questions": {
                "value": "The motivation for the proposed method in the introduction was the privacy violation in unlearning through membership inference. However, in the experiment, only test, retain and forget errors were evaluated. There was no experimental evaluation of privacy violations through membership inference. It is necessary to experimentally evaluate whether the proposed method is more robust against privacy violations through membership inference than other methods.\n\nThe proposed method claims that if the probability output of the final layer is uniform or random, it can avoid membership inference. Is this true? It is unlikely that the probability output of the final layer is uniform by chance. Could it be evidence that the probability output of the final layer was the target of unlearning?\n\nThe same applies if the probability output of the final layer is random. If such randomness does not appear in any other test data, it is possible to infer that it is artificial randomness. To show that the data to be forgotten is not identified by membership inference, it may be necessary to show, for example, using statistical testing, that the probability output of the final layer for the data contained in the test data is statistically indistinguishable from the probability output of the data to be forgotten."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper tries to solve the efficiency and privacy leakage of machine unlearning. The problem and topic is interesting and hot. I have some comments as follows.\n\nStrengths: 1. The experiments and evaluation are sufficient.\n\nWeaknesses:\n1. The problem statement is not clear. In the introduction and abstract, the authors claimed that they aim to solve the efficiency and privacy leakage of machine unlearning. However, in the section 3, the problem definition section, it has not mentioned the definition related to the efficiency and privacy leakage problems in machine unlearning.\n\n2. Regarding the privacy leakage in machine unlearning, the authors ignored the common attacks that compare the model difference before and after unlearning to implement inference attacks. To the reviewer's understanding, the proposed method is infeasible for these attacks in machine unlearning because the original model performs well on erased samples but now randomly predicts them, which is a huge difference.\n\n3. The paper needs heavy proofreading. There are many typos, for example \"Figure ??\" in page 2."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The experiments and evaluation are sufficient."
            },
            "weaknesses": {
                "value": "1. The problem statement is not clear. In the introduction and abstract, the authors claimed that they aim to solve the efficiency and privacy leakage of machine unlearning. However, in the section 3, the problem definition section, it has not mentioned the definition related to the efficiency and privacy leakage problems in machine unlearning.\n\n2. Regarding the privacy leakage in machine unlearning, the authors ignored the common attacks that compare the model difference before and after unlearning to implement inference attacks. To the reviewer's understanding, the proposed method is infeasible for these attacks in machine unlearning because the original model performs well on erased samples but now randomly predicts them, which is a huge difference.\n\n3. The paper needs heavy proofreading. There are many typos, for example \"Figure ??\" in page 2."
            },
            "questions": {
                "value": "No additional questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}