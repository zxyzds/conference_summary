{
    "id": "a69zct3BkY",
    "title": "Keys to Robust Edits: From Theoretical Insights to Practical Advances",
    "abstract": "Large language models (LLMs) have revolutionized knowledge storage and retrieval, but face challenges with conflicting and outdated information. Knowledge editing techniques have been proposed to address these issues, yet they struggle with robustness tests involving long contexts, paraphrased subjects, and continuous edits. This work investigates the cause of these failures in locate-and-edit methods, offering theoretical insights into their key-value modeling and deriving mathematical bounds for robust and specific edits, leading to a novel 'group discussion' conceptual model for locate-and-edit methods. Empirical analysis reveals that keys used by current methods fail to meet robustness and specificity requirements. To address this, we propose a Robust Edit Pathway (REP) that disentangles editing keys from LLMs' inner representations. Evaluations on LLaMA2-7B and Mistral-7B using the CounterFact dataset show that REP significantly improves robustness across various metrics, both in-domain and out-of-domain, with minimal trade-offs in success rate and locality. Our findings advance the development of reliable and flexible knowledge updating in LLMs.",
    "keywords": [
        "model editing"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=a69zct3BkY",
    "pdf_link": "https://openreview.net/pdf?id=a69zct3BkY",
    "comments": [
        {
            "summary": {
                "value": "This paper works on the robustness challenge in knowledge editing within large language models, particularly using locate-then-edit methods.\nThe authors propose a theoretical framework and propose a \"Robust Edit Pathway\" (REP). It separates editing keys from LLM representations, allowing more robust and specific edits.\nExperimental results demonstrate that REP improves robustness metrics on Llama2-7B and Mistral-7B models with the CounterFact dataset.\nIt also achieves higher performance in rephrased and long-context scenarios."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed REP framework uses a project and gate mechanism and separates editing pathways, providing a new approach to the knowledge editing task.\n2. The paper combines theoretical derivation and empirical evidence.\n3. The paper reports experiments and shows the proposed REP works well on a representative locate-then-edit method ROME."
            },
            "weaknesses": {
                "value": "1. The experiments mainly use Llama-2-7B and Mistral-7B. Maybe more LLMs can be included and tested.\n2. The experiments solely use the CounterFact dataset. We expect to see the results of real-world knowledge editing. This is because knowledge editing in the real world tends to be more diverse and complex.\n3. What are the failure cases of REP? Including failure cases of editing and robustness tests.\n4. It seems the experiment section only reports results with ROME and ignores MEMIT."
            },
            "questions": {
                "value": "1. Table 1 only includes ROME. Where are the results of MEMIT?\n2. The paper only uses the CounterFact dataset for experiments. How about the performance of real-world facts like [here](https://arxiv.org/pdf/2402.18909)?\n3. In the Figure 4 caption, \"prefixs\" should be \"prefixes\".\n4. Line 431, \"increasin\" should be \"increasing\".\n5. Line 799, \"unrelevant\" should be \"irrelevant\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Analysis of knowledge editing techniques in LLMs.\nRobustness issues in locate-and-edit models.\nPropose Robust Edit Pathway addresses those issues."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Knowledge editing is important for improving the quality of LLMs\nRelated work seems well covered and reviewed\nThe solution proposed sheds light on the key-value approaches\nThe paper is largely written, or heavily edited, by a LLM"
            },
            "weaknesses": {
                "value": "The nature of key-value data model would require an assessment of the conceptual modelling issues involved in knowledge editing\nA lot of knowledge is not stored in individual triple, but in subgraphs\nEnglish issues and instability of terminology/definitions\nFew, poor examples do not help the reader, e.g., the triple (USA, president-of, Biden) is counterintuitive unless we interpret \"president-of\" a \"has-president\""
            },
            "questions": {
                "value": "What kind of knowledge do you intend to edit?\nWhat dynamics \"group discussion\" would cover?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors tackle the problem of knowledge editing in an LLM, i.e., how to replace one knowledge with another by modifying the weights. They propose a revision of the ROME approach based on theoretical results. Their approach consists of adding a key adaptor to the network to fix current problems with ROME.\nStudying knowledge editing is thrilling, and theoretical analysis is the correct way to find valuable insights and improvements. However, the paper needs to be more mature, with many imprecisions, presentation issues, typos, and missing information."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1. The authors provided the code for their approach. However, they could have written a README to understand the content better. Besides, the code is not anonymized, with paths containing the programmer's name (hardcoded paths are not a good sign of reproducibility).\n\nS2. The subject is well-motivated at the beginning.\n\nS3. In several places, the authors try to link the practical and the theoretical side of their work."
            },
            "weaknesses": {
                "value": "W1. The authors need to improve the presentation and clarity of the paper. There are many typos in the paper (see below), and some are in the theoretical results, which is concerning. Sections 3 and 4 take a lot of work to follow. For Section 3, the authors copy-pasted the content of ROME. However, it is tough to understand without the context of the ROME paper. Section 4 is a succession of Lemmas and corollaries barely connected. The authors need to explain the intuition better. I provide additional issues below.\n\nW2. The experiment section is slim, with few baselines compared. The authors wanted to include MEMIT in the comparison, but it appears only in the text rather than in the tables. It is hard to understand what happened here. Also, a better benchmark for this task now exists: the KnowEdit dataset.\n\nW3. The empirical analysis lacks quantitative evidence. Some conclusions are only drawn from one example.\n\nW4. Many notions are used without being properly introduced (for example, the whitening similarity).\n\nTypos/Others\n\nT1. Be careful with apostrophes (often, a closing one is used instead of an opening one).\n\nO2. Maybe write a summary of the contributions at the end of the introduction.\n\nT2. L130: utlize -> utilize\n\nT3. L132: adaoptor -> adaptor\n\nT4. In definition 3.1, line 164, k should be f.\n\nT5. In Remark 3.3, the sum is over j, not i.\n\nO6. Figure 2 is never mentioned in the text.\n\nO7. For Lemma 4.1, the proof is confusing. Simply say that WK=V, so W = VK^+. Then, \\hat{v} = W \\hat{k} = VK^+ \\hat{k}.\n\nO8. Lemma 4.1 introduced W, but it has not been used.\n\nT9. Lemma 4.1: The sum starts at 0, not i.\n\nO10. At Corollary 4.2, it is hard to get what K_s and V_s are and what the \"same semantic meaning\" means.\n\nO11. In Corollary 4.2, is alpha computed using K_s and V_s, or just K and V. If it is K_S and V_s, the assumption N>>D might not hold, \nparticularly in the experiments.\n\nO12. In Lemma 4.4, epsilon is not used. I do not see why we give a value to epsilon.\n\nO13. In Lemma 4.4, is the assumption reasonable?\n\nO14. Equation 11 cannot be correct as 1 - 1/p <= 0. Does it have an impact on what follows?\n\nO15. Line 773, I need help understanding how Equation 11 is used and where epsilon is.\n\nO16. In Lemma 4.6, what does \"robustly retrieved\" mean?\n\nO17. In the Appendix, the authors did not finish the proofs (Section A3). The sentence ends in the middle, which is not serious.\n\nO18. In Figures 3 and 4, on the left, I think the authors should normalize the results as it is usually done with similarities.\n\nO19. Figure 3, on the left, is hard to read. In black and white, it is impossible. Even in the color version, some dots are not visible.\n\nO20. What is the whitening similarity? Where is it defined?\n\nO21. Line 342, how is K_s defined?\n\nO22. Line 348, where is the baseline? It does not appear in the plots.\n\nO23. Line 352, the example in the text (Delphine de Girardin) does not match the example in Figure 3 (Slovenia).\n\nO24. I don't think PCA is relevant here for analyzing a single subject. By default, PCA will spread the points. The distances are only meaningful relative to other distances. The authors should perform PCA on several subjects before being able to analyze the results. Also, PCA can modify what happens in higher dimensions. \n\nO25. Did the authors perform a quantitative analysis for Section 5.2? One cannot conclude from a single example. There should be average distances and metrics.\n\nT26. There are some random ~ over numbers.\n\nT27. Line 367, remove are.\n\nO28. Line 368, \"...prefix has the same subject token\" as what? Give more examples here.\n\nO29. Line 370, \"high whitening noise.\" Compared to what?\n\nO30. On which dataset?\n\nO32. Line 419, what are bsz, L, and D?\n\nT33. Line 430, \"g\" missing.\n\nT34. Line 433, \"theorectical\" -> theoretical\n\nO33. Line 435,  \"whether to activate this branch\". Are we talking about projection only?\n\nO34. Table 1, why are the results in the first column not in bold? It is ok not always to have the best results.\n\nO35. Are the results in Table 1 statistically significant? Can we have a standard deviation?\n\nO36. Section 6.1, where are the results for MEMIT?\n\nO37. Line 453. \"We use LLaMa2\" and Mistral!\n\nO38. How big is the train set?\n\nO39. Why did not the authors use all the metrics in the ROME paper?\n\nT40. Line 432, raito -> ratio. Line 474, resutls -> results (just use an autocorrector).\n\nO41. The authors should add more baselines to the experiments.\n\nO42. The authors should include an error analysis section. Where are the errors coming from? For example, does the rephrasing always make sense? Does the shuffling always make sense? What would a human say?\n\nO43. For the references, try to use the real reference and not only the Arxiv link."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}