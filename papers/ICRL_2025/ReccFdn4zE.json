{
    "id": "ReccFdn4zE",
    "title": "Cross Attention for Oddly Shaped Data and Applications in Ionospheric Modeling",
    "abstract": "It is desirable to have models of many physical phenomena, yet often data for these phenomena are oddly structured. These structures, such as ungridded and arbitrary length data prevents the use of many types of machine learning techniques, such as feed-forward neural networks. It is thus quite desirable to be able to move this data into a fixed size and shape for easier data ingest. We propose a method of using cross attention to do this.\nAn example of oddly shaped data is Total Electron Content (TEC), or the vertical integral of electron density in the atmosphere. TEC data is calculated using both the position of a satellite and a position on the surface of Earth, giving a non-fixed location per sample. This leads to a splattering of points on the globe where measurements exist that change in shape and amount each time step.\nWe apply our technique to TEC in an autoregressive approach. This allows us to both obtain an embedding describing the global TEC and create completed TEC maps, filling in where measurements are not taken. The global embedding can then be further used in other models.",
    "keywords": [
        "Ionosphere",
        "Cross Attention",
        "Attention",
        "Transformers",
        "Electron Density",
        "Total Electron Content",
        "TEC",
        "Misshapen Data",
        "Variable Sequence Length"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ReccFdn4zE",
    "pdf_link": "https://openreview.net/pdf?id=ReccFdn4zE",
    "comments": [
        {
            "summary": {
                "value": "The authors propose an autoencoder architecture for ionospheric total electron count data based on transformers. They utilize the obtained lower-resolution representation to construct a regression model to predict the electron density in the atmosphere. This data is temporal in nature, and geographically sparse with varying available locations per time slice."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper considers an interesting and relevant application domain, i.e. the domain of predicting electron density in the ionosphere."
            },
            "weaknesses": {
                "value": "In my opinion, there are several problems with this submission in the areas of presentation, methodology, and comparison to competing methods.\n\nFirst, the claim of introducing a new class of \u2018oddly-shaped data' seems fuzzy and unwarranted to me. The TEC data presented appears to be simply a sequence of images with missing values, akin to some inpainting tasks or inputs for masked autoencoders, where the goal is to fill in / infer the missing values in said images (reference examples provided below).\n\nSecond, I don't believe the decision to turn this data into sequences and apply models designed for natural language processing is valid here. There is no true order among the set of pixels for which data is available in any given time slice, because the data is indexed by 2 dimensions. Therefore, turning it into a 1D sequence must be inevitably completely arbitrary. If the positions of elements have no meaning and are random, the transformers and their attention heads cannot learn anything meaningful about co-occurrences of positions. I think this data would be better served by image-based approaches, i.e. same-shape (World-coverage) images with missing/masked values, which can then be input to e.g. a masked autoencoder. See reference examples below.\n\nSome work on masked autoencoders and inpainting worth looking at include:\n1. Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2536\u20132544, 2016. URL https://arxiv.org/pdf/ 1604.07379\n2. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners, 2021. URL http://arxiv.org/abs/2111.06377 \n3. Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong Zhu, Chaoyang Zhao, Rui Deng, Liwei Wu, Rui Zhao, Ming Tang, and Jinqiao Wang. MST: Masked self-supervised transformer for visual representation, 2021. URL http://arxiv.org/abs/2106.05656\n 4. Ioannis Kakogeorgiou, Spyros Gidaris, Bill Psomas, Yannis Avrithis, Andrei Bursuc, Konstantinos Karantzalos, and Nikos Komodakis. What to hide from your students: Attention-guided masked image modeling. In Shai Avidan, Gabriel Brostow, Moustapha Ciss\u00e9, Giovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision \u2013 ECCV 2022, pp. 300\u2013318. Springer Nature Switzer-land, 2022. ISBN 978-3-031-20056-4. doi: 10.1007/978-3-031-20056-4 18. URL https://link.springer.com/chapter/10.1007/978-3-031-20056-4_18\n\nThird, I think the presentation of prior/related work on the topic is done is a somewhat misleading manner. Specifically, in lines 99-10 we read:\n    \n\"Recently, Machine Learning (ML) approaches have become popular (Dutta & Cohen, 2022; Sai Gowtam & Tulasi Ram, 2017; Gowtam et al., 2019; Li et al., 2021; Chu et al., 2017; Habarulema et al., 2021). These models perform well compared to existing approaches, but they do not go much further than shallow feed-forward networks.\"\n\nHowever, the authors dismiss the most relevant image-based approaches such as:\n1. \"TEC Map Completion Through a Deep Learning Model: SNP-GAN\" (Pan et al, 2021)\n2. \"Complete Global Total Electron Content Map Dataset based on a Video Imputation Algorithm VISTA\" (Sun et al., 2022)\n\nby claiming that \"the act of completing data can introduce errors (Schafer & Graham, 2002), as the false data is biased by the techniques used to create it.\" (line 53).\n\nI don't think it's fair to claim that modern GAN and other image ML-based techniques bias the result more than what the authors propose, especially given the arbitrary transition to sequence data from pixels. Moreover, most GANs also work in a low dimensional latent space. That means that a trained GAN's decoder model can be inverted or even trained to include an inverse mapping from image space back to the latent space. This would allow to obtain latent codes also from GANs and directly compare their usefulness for the task at hand (i.e. regression of electron density). This was not done and the authors instead only compared their embedding-enabled regression model to very simple baselines. Please consider conducting additional experiments to compare GAN latent codes to the proposed method's embeddings.\n\nFourth, it is unclear to me if the presented results indicate substantial improvement. Figure 6 shows an increase of the $R^2$ measure from $0.915$ to $0.93$. Figure 7 also does not show significant correlation improvements for the authors' \"DINN_eTEC\" method. Figure 8. Top does not show significant MAE reduction for \"hmf2\". What magnitude of changes are considered significant in the context of ionospheric modeling?"
            },
            "questions": {
                "value": "I think some important details are missing from the autoencoder architecture. For example:\n\n- What is the \"learnable starting vector\" shown in Fig. 2? I could not find any reference to it in the text\n- How is the latent space constructed? Is there some kind of random generator for the latent embedding values? Or maybe a deterministic scheme?\n- How exactly is the autoencoder trained? What is the loss function? Does it predict all channels of the sequence? (i.e. is it also measured on how well it reconstructs e.g. the lon/lat values)\n\nRegarding the authors' claim: \"Given that we are able to reconstruct from an embedding to an arbitrary sequence, it stands that we can also use this approach to complete TEC maps. \"\nI'm wondering how this can be done exactly, because the decoder (\"reconstructor\") must also consume a latent embedding value alongside the lon/lat coordinates. So how would the authors derive these latent codes for the pixels with missing values? Is it somehow by random sampling? If so, how can the authors ensure that the values \"make sense\" from a physical perspective?\n\nDid the authors consider using an image-based approach with missing/masked values? (sample references provided above)\n\nDid the authors consider utilizing an existing GAN based method, e.g. cited in line 52, \"TEC Map Completion Through a Deep Learning Model: SNP-GAN\" and doing GAN inversion to obtain latent codes for \u2018true' pixels (with available values) and comparing the latent codes from the GAN with latent codes from their transformer based autoencoder on the downstream task of regression?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes the notion of \u201coddly shaped data\u201d with the example of Total Electron Content (TEC) data, and propose and embedding to describe the global TEC and create completed TEC maps."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The notion of TEC and ionospheric modeling is interesting."
            },
            "weaknesses": {
                "value": "The motivation is unclear, particularly the notion of \u201coddly shaped data\u201d and the contribution is unclear. The authors mention \u201cunguided and arbitrary length\u201d, \u201ca splattering of points on the globe where measurements exist that change in shape and amount each time step\u201d, but it seems data is being gridded across the surface of the earth, and the main problem appears to be sparse sampling.\n\nThere is no clear mathematical description of the data or method used, other than a block diagram in Figure 2 which appears to be mostly common components."
            },
            "questions": {
                "value": "How is the idea here of \"oddly shaped data\" here different from \"missing data\", which is solved with many different approaches starting with expectation maximization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method using cross-attention within an autoencoder to embed and reconstruct oddly shaped data, specifically Total Electron Content (TEC) data for ionospheric modeling."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Important problem setting, well motivated \n- Proper language and structure of the paper \n- Good and thorough experimentation  section"
            },
            "weaknesses": {
                "value": "- No methodological innovation. Applying an existing approach to a new dataset unfortunately  does not pass the bar for methodological contributions \n- There are hints of controlling for confounding factors that would bias the representation and the downstream prediction but it is not thoroughly explored. I recommend that this is further investigated \n- Insufficient literature review with many ML <> Ionospheric physics papers missing"
            },
            "questions": {
                "value": "- What are the main confounding factors besides the time and place of the measurements that would affect the performance of the model ? \n\n\nOverall this is a solid paper for an ionospheric physics journal or conference , not an ML one where methodological innovation and contributions are key"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper describes an autoencoder with attention layers used to describe ionospheric data. The preprocessing of the data is described, e.g. scaling latitude from -1 to 1, and longitude as sine cosine pair, and feature like the Kp index are used. I did not find a description of the Kp index anywhere even though it appears to be a significant value whose predictions are given in a plot.  Can you please provide a description of the Kp-index, ap-index and the rest of the technical features?\nThe were companions of different models, changing only with respect to their inputs, and a paragraph describing using the outputs as inputs suggesting that some form of recurrent processing was also attempted but did not yield results. I did not see a description of a second machine learning model nor a comparison with another model performing this task. \n\nIt might be that there is a community mismatch with this paper because the paper only describes the use of a particular autoencoder on the data and it is not compared with an alternative approach for the same models. Comparisons with \u201cdifferent models\u201d are there only in terms of different inputs and not different ML algorithms. A substantial problem for the publication of this work in a Machine Learning conference is also that the type of input is not clearly introduced, terms such as the Kp index, ap index, and solar index are not clearly defined. I guess the authors think that a reference is enough for the reader to understand the data, but since the paper focuses on using different combinations of the data to demonstrate performance discrepancies then the data should be explained in more detail. It is uncommon for a method to be described without a comparison to a different method in a machine learning conference. Such a comparison would be useful here. Perhaps trying to embed your data to images and using a CNN or ViT-based model would give you another approach to compare against. \nAlso, submissions to ICLR are expected to be reproducible, however, since the authors do not provide their code, all implementation details should be described in the paper, including size of different layers and learning rates. \n\nThe paragraph in line 131 implies that the authors tried to create some form of recurrent algorithm by taking the output of the autoencoder and using it as input to the same autoencoder. This is not clearly described but it appears like a technically weak attempt at creating a recurrent algorithm for the data. I would suggest a deeper exploration of the literature on that. \n\n\nAs a general comment, I would suggest that the authors present a deeper analysis of their model with more thorough comparisons to alternative approaches."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper describes a novel application of an autoencoder. The problem is uncommon and results of some resasonable quality were achieved."
            },
            "weaknesses": {
                "value": "The method is not clearly described, it is unlikely that a reader can reproduce that work from the available material. There is no comparison with another work. Since the application is novel (at least I have not seen an ionospheric model with autoencoders before) it would be appropriate to describe the problem better. Variable names are introduced but they are not explained in the text, e.g. Kp index, ap index etc."
            },
            "questions": {
                "value": "The title of the paper mention oddly shaped data, can you provide a defintion of the term oddly shaped data?\nCan you please give some more examples of oddly shaped data?\nAre there any other methods for describing oddly shaped data that you could compare with your approach?\nWhat is Kp index, ap index, Dst, etc. ?\nWhich optimisation algorithm, learning rate, and other hyperparameters did you use for your experiments?\nAre there other types of oddly shaped data one could apply your method to?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}