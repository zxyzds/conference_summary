{
    "id": "LBl7Hez0fF",
    "title": "Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering",
    "abstract": "Hallucination poses a challenge to the deployment of large vision-language models (LVLMs) in applications. Unlike in large language models (LLMs), hallucination in LVLMs often arises from misalignments between visual inputs and textual outputs. This paper investigates the underlying mechanisms of hallucination, focusing on the unique structure of LVLMs that distinguishes them from large language models (LLMs). We identify that hallucinations often arise from the sensitivity of text decoders to vision inputs, a natural phenomenon when image encoders and text decoders are pre-trained separately. Inspired by this, we introduce Visual and Textual Intervention (VTI), a novel technique designed to reduce hallucinations by steering latent space representations during inference to enhance the stability of vision features. As a task-agnostic test-time intervention, VTI can be easily applied to any problem without additional cost. Extensive experiments demonstrate that it can effectively reduce hallucinations and outperform baseline methods across multiple metrics, highlighting the critical role of vision feature stability in LVLMs.",
    "keywords": [
        "Large Vision-Language Models",
        "Multimodal large language model",
        "Hallucination"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=LBl7Hez0fF",
    "pdf_link": "https://openreview.net/pdf?id=LBl7Hez0fF",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a heuristic approach of reducing hallucination in Visual Language Models by linearly offsetting the latent representation of both image and language modalities. The key to this heuristics is to pre-compute a set of offset vectors for image/text token embeddings that are the results of data augmentation. In particular, for image modality, the author introduced perturbations to the image input and computed the average change (averaged across all perturbed image instances) in token embedding across each layer of the image encoder. At inference time, the first PC of these change vectors across multiple images were then added to the embedding of any query image. Similar procedure was repeated for language modality where the \u201cperturbation\u201d is hallucinatory captions generated by another LLM. The authors show that the method, while simple, is able to significantly reduce the degree of hallucination in several benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The problem of hallucination in VLM is significant and the method proposed is highly intuitive (and appears effective). It uses ideas from data augmentation and applies it directly to inference time without the need to retrain the model. \n2. The analysis in the paper is thorough. Not only does it include good ablation of the various aspects of the method (granted there aren\u2019t many moving parts to begin with), it also included discussions of how the method could reduce hallucination (i.e. impact on attention)"
            },
            "weaknesses": {
                "value": "All in all, this was a good paper and a pleasure to read. To nitpick, one of the biggest concerns of heuristic methods of this kind is its general applicability in practice. For example, \n1. what type of perturbations are most important in image/text modalities? \n2. how are the vision/language directions differ between models? \n3. What about more general open-ended QAs that go beyond captioning tasks? How would you even generate these hallucinatory language examples when the task is open QA?"
            },
            "questions": {
                "value": "Beyond addressing some of the questions I raised under \"Weakness\" above, here are a few other changes/questions.\n\nMinor:\n1. In Figure 2, CHAIRs was first mentioned. It should be described in the caption. Furthermore, the three subplots don\u2019t seem to correspond to one another. It\u2019ll be helpful to highly what experiments were shown in left/middle plots (sigma value and ratio value) to connect them to the third figure.\n2. Discussion of the methodology could be improved. In particular, the description of the \u201cvisual direction\u201d around equation 1 is confusing. \n\nQuestions:\n1. What do the embedding changes actually look like? How much explained variance is the first PC? \n2. I don\u2019t really understand how this linear offset can improve \u201cfeature stability\u201d. If the embedding is to be viewed as a random variable whose variance is caused by input perturbation, then doing a linear offset _cannot_ change the variance of such random variable. Perhaps I missed something."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The hallucination problem of LVLM refers to the phenomenon where the text generated by the model does not align with the content of the image. Previous methods mainly focus on issues that do not distinguish between LVLMs and LLMs. The authors first discovered that hallucination mainly stems from the sensitivity of the text decoder (LLM backbone) to vision input.\n- By adding perturbations (Gaussian noise or random masking) to the input images, the authors found that the image encoder's generated embeddings exhibited some unstable features.\n - After perturbing the images, the model's hallucination increased, but if the embeddings were averaged over multiple perturbed images, stabilizing the features, the hallucination was reduced.\n\nInspired by this, the authors proposed a test-time intervention method called Visual and Textual Intervention (VTI).\n  - For a given model, the method requires a sample set where each sample includes an image, a hallucination-free caption, and a hallucinated caption.\n  - The images in the sample set are perturbed and sent to the vision encoder, calculating the visual shifting vectors.\n  - Additionally, the two captions are fed into the text decoder, calculating the textual shifting vectors.\n  - During testing, a new sample is processed by the model as usual, but the shifting vectors are added to the latent hidden states of the vision encoder and text decoder, respectively. \n\nVTI effectively reduced hallucinations and outperformed previous methods across multiple benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The motivation and the method are explained smoothly and easy to understand.\n- The experiments cover different hallucination benchmarks and LVLMs with different vision encoders.\n- As a task-agnostic test-time intervention, the proposed method, VTI, outperforms previous baseline methods."
            },
            "weaknesses": {
                "value": "### Major\n- The description of method in Section 3 is kind of too general. \nHow many LVLMs are tested here? Which LVLM(s) is used? \nAre the test images from COCO? \nAre the observations generalized to different LVLMs?\n- VCD [1], which has been cited and compared in the paper, also adds Gaussian noise to distort visual input. They mention \u201cvisual uncertainty amplifies hallucination\u201d in their Figure 2, which might be related to \u201cvision feature instability\u201d explored in the paper. I\u2019m wondering if the authors are inspired by VCD. It would be better if the authors can explain the differences between their exploration and VCD.\n- I\u2019m concerned with the generalization ability of VTI, which may be a common problem of existing hallucination mitigation methods. LVLMs are expected to do a wide range of tasks. When image or task domains are changed, will the current methods damage model performance? Only hallucination benchmarks are covered in the experiments, so I'm not sure if VTI will damage model performance on other tasks. \n\n### Minor\n- It would be better to early explain the structure of LVLM in abstract or introduction, like \u201cA large vision language model typically consists of a vision encoder and a large language model as a text decoder\u201d in Lines 143 \u2013 144. Currently, the paper refers to vision encoder and text decoder directly at the beginning, which I feel may be confusing to readers.\n- The authors claim that \u201cVTI can be easily applied to any problem *without additional cost*\u201d in the abstract. I feel \u201c*without additional cost*\u201d is kind of confusing, because VTI actually needs additional data and tune its shifting vectors.\n- Lines 285-286, the authors mention that \u201cwe use 50 examples with paired images, hallucinated and non-hallucinated responses to \u2026\u201d\nI\u2019m wondering where the authors get the images. Are the images from COCO?\n- If the authors use more dimensions of PCA in calculating the shifting vectors, will the method get better or worse performance?\n\n- Typos\n\nIn the abstract, the abbreviation \u201c(LLMs)\u201d is shown twice. In the main content, no \u201c(LLMs)\u201d. The instruction needs to borrow one from the abstract.\n\nLines 151-152, \u201c\u2026 between the vision and text components of large vision-language models (LVLMs):\u201d It is not necessary to emphasize the abbreviation here now that there is \"LVLMs\" in the previous content.\n\nLine 214 \u201cAs the desirable visual direction should be readily applied to new image queries.\u201d The sentence has no main clause. I guess the authors wanted to use \u201cas\u201d, following the previous equation. The same with Line 278.\n\nMissing \u201c,\u201d in Equation (1)\n\n\n\n----------------------------------------------------------\n[1] Mitigating object hallucinations in large vision-language models through visual contrastive decoding. CVPR2024."
            },
            "questions": {
                "value": "Please see the Weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose that hallucinations in LVLMs are typically caused by misalignment between visual and text representations. Consequently, the authors demonstrate this conclusion by perturbing the input image to obtain a more robust visual representation. Based on this experimental approach, the authors introduce a comprehensive method for intervening in both image and text representations to mitigate hallucinations in LVLMs. Furthermore, the authors substantiate the effectiveness of their method through extensive experiments.\n\nThe paper makes the following contributions:\n\n1. Analyzes the correlation between the stability of visual representations and hallucinations in LVLMs.\n\n2. Proposes a method for separately perturbing text and image representations to achieve a more robust representation space.\n\n3. Demonstrates the effectiveness of their method through extensive experiments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The writing of this paper is exceptionally clear. Additionally, the motivation is thoroughly argued, and the analysis of the method is comprehensive.\n\n2. It is indeed a novel attempt to mitigate hallucinations in LVLMs by perturbing the representation space through a training-free approach.\n\n3. The experiments in this paper are very comprehensive, demonstrating the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. Using variance changes to explain changes in representation stability is somewhat one-sided. For instance, in PCA, PC1 is typically the direction with the largest variance, and it seems inevitable that the variance would decrease after using VTI. It seems more fundamental to examine the stability of the representation distribution for images with different noises or masks.\n\n2. There are issues with the comparison methods used in the experiments. OPERA and VCD are both methods based on decoding to mitigate hallucinations, with the former optimizing beam decoding and the latter optimizing nucleus sample decoding. However, the paper does not specify the exact settings for the decode methods of vanilla and VTI.\n\n3. Some components need to undergo ablation experiments.\n  \n    3.a. The role of PCA lacks ablation experiments. In Figure 8, it appears to be comparing different methods of representation updating, but what would be the impact if average were used instead of PCA?\n\n    3.b. In line 713, the mask ratio is set to 0.99, which is an unusually high value, but the author does not provide a detailed analysis of this parameter."
            },
            "questions": {
                "value": "1. Are there other more fundamental methods to prove representation stability? For example, calculating the KL divergence of representations after adding various perturbations to the image.\n\n2. To demonstrate that VTI, as an inference intervention method, is superior to OPERA and VCD, a fairer experimental setup would be to use VTI separately with beam search and nucleus sample to compare against OPERA and VCD, respectively. This would be similar to the experimental setup in PAI[1].\n\n3. What would be the effect of using average to calculate $d_{l, t}$ compared to PCA?\n\n4. When the mask ratio is set to 0.99, almost no image information is retained. Performing PCA on the representation of such input images seems to be pointless. Therefore, I believe it would be beneficial to include a comparison of what happens when the mask ratio is set to different values and when a fully masked image is directly set as the intervention direction.\n\n[1] Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}