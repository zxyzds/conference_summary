{
    "id": "Zy2XgaGpDw",
    "title": "TLDR: Token-Level Detective Reward Model for Large Vision Language Models",
    "abstract": "Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by assigning only one feedback to any text, no matter how long the text is. In the realm of multimodal language models, where models are required to process both images and texts, a naive reward model may learn implicit biases toward texts and become less grounded in images. In this paper, we propose a **T**oken-**L**evel **D**etective **R**eward Model (**TLDR**) to provide fine-grained annotations to each text token. We first introduce a perturbation-based model to generate synthetic hard negatives for training TLDR models. Then we show the rich usefulness of TLDR models both in assisting off-the-shelf models to self-correct their generations, and in serving as a hallucination evaluation tool. Finally, we show that TLDR models can significantly speed up human annotation to acquire a broader range of high-quality vision language data.",
    "keywords": [
        "vision language model",
        "multimodal",
        "reward model"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Zy2XgaGpDw",
    "pdf_link": "https://openreview.net/pdf?id=Zy2XgaGpDw",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a model that addresses the limitations of existing reward models, which provide only binary feedback for entire text sequences. Instead, it assigns rewards at each token, enabling more precise and interpretable feedback, improving self-correction, hallucination detection, and human annotation efficiency in vision-language tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The idea is interesting, as it shifts from traditional binary feedback to a more detailed token-level approach.\n2. The model provides token-level rewards, offering more precise and interpretable feedback compared to traditional binary reward models.\n3. The proposed HALLUCINATION RATE (%) is a novel evaluation metric.\n4. The token-level errors can guide multimodal large language models in self-correction, enhancing their performance.\n5. The model can serve as a data correction tool, effectively speeding up the process by three times."
            },
            "weaknesses": {
                "value": "The model is trained using a perturbation-based data generation process based on simple factual statements, which introduces some limitations:\n1. Its performance may be limited when interpreting information-rich images like posters, where elements for the same noun are not unique and are arranged in a complicated layout.\n2. The model may also face challenges in understanding text-rich images, such as documents where relationships between concepts are described in text and require logical reasoning."
            },
            "questions": {
                "value": "1. In Section 5.4, despite the model\u2019s low performance on tasks listed in Table 5, does the model with ($\\tau$ = 1) still achieve the best performance on token-level hallucination detection?\n2. Do you have any insights into why the finetuned LoRA weights initially enhance the model\u2019s performance on other tasks but then cause a decline as ($\\tau$) increases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a token-level reward model for VLMs. The challenge it aims to address is that the commonly used binary reward is often biased towards linger captions. The proposed TLDR model can provide per-token feedback and solve issues such as hallucinations. The method achieves this using synthetic data generation by perturbing correct captions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well motivated\n- The method section is clear and easy to follow\n- Some very interesting findings, such as that fine-tuning a VLM for token-level rewards improves the model itself."
            },
            "weaknesses": {
                "value": "- I would not really call TLDR a reward model, as the paper has not shown that it can actually be used as a reward model (in the RLHF sense). In its current form, this is a per-token correctness model.\n- Table 9 shows that in the response-level evaluation, TLDR is only marginally better that a naive response-revel reward model. While this is not possible for the response-level model, it would be interesting to see if TLDR outputs the correct prediction for the right reason, i.e. manually evaluate per-token accuracy, recall and precision, rather than just the global metrics.\n- Is Equation (8) backed by anything? That seems very arbitrary."
            },
            "questions": {
                "value": "-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a token-level reward model for large vision-language models, where the reward model would produce reward labels for each token during the generation. \n\nTo obtain a training dataset for the reward model, this paper proposes to perturb image caption/VQA answer with an LLM (i.e., LLaMA-70B), with specific templates defined to generate more hallucination/bottleneck-oriented perturbations (e.g., counting objects, color identification etc.)\n\nAfter training with the dataset, a base LVLM becomes a Token-Level Reward Model for various applications:\n\n(i) hallucination evaluation: where the reward labels could be used to develop token-, sentenc- and response-level accuracy for the generated responses. \n\n(ii) self-correction with the token-level reward for reducing hallucination;\n\n(iii) enhanced performance as a by-product of the reward model training;\n\n(iv) speed-up the caption annotation"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Overall, I think the idea of developing a token-level reward model for LVLMs is interesting and the authors propose a feasible dataset synthesis method to achieve this.\n- The paper is well-written and well-organized. \n- The applications of TLDR seem to be promising for future LVLM developments."
            },
            "weaknesses": {
                "value": "My major concerns lie in the experimental settings and evaluation setups, which makes the results less convincing to me.\n\n- Backbone choices: I found the models used in this paper are somewhat arbitrary: the reward model is trained upon PaliGemma; human evaluation is conducted with captions generated by MiniCPM; GPT-4V is used to perform self-correction experiments. Are there any specific reasons to choose different models in these experiments? \n\n- Insufficient experiments regarding model selection:\n   - The reward model is only trained with a PaliGemma-3B model, which as demonstrated in Table 3, performs the worst in the hallucination. Wouldn't a stronger backbone lead to stronger Token-level reward models? An ablation study comparing TLDR models trained on different backbones, including stronger ones like phi and llama-3.2, is recommended.\n   - The correlation between MMMU scores is misleading. As pointed by Cambrian-1 and MMMU-Pro, MMMU score does not faithfully reflect the visual understanding and reasoning performance but more about the LLM capability. This renders the correlation analysis less convincing and I recommend the authors incorporate visual-oriented benchmarks to justify this claim.\n   - Why LLaVA-series and Qwen-VL are not included in the evaluation? These are commonly adopted LVLMs.  Including these models would provide a more comprehensive comparison across different LVLM architectures and training approaches.\n\n\n- In Table 5, why are only two tasks (Counting and spatial relation) of BLINK adopted as in-domain tasks? There are also in-domain  tasks such as Visual correspondence & Object localization. Adding the comprehensive results on BLINK could better illustrate the full picture."
            },
            "questions": {
                "value": "- Could you explain their rationale for choosing different models for each experiment?\n\n\n\nMinor:\nThere are empty lines after Eq. 6 and Eq. 7.\n\nThis paper ignores many relevant papers regarding LVLMs alignments and reward modeling. It would be more beneficial to compare the reward models or DPO training performance of these studies:\n\n- LLaVA-RLHF: Aligning Large Multimodal Models with Factually Augmented RLHF\n- RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness\n- Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback \n- VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment \n-  Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the limitations of current reward models in multimodal large language models (MLLMs), which often provide only binary feedback regardless of text length. The authors propose a Token-Level Detective Reward Model (TLDR) to offer fine-grained, token-level feedback instead of coarse binary annotations. The TLDR model uses a perturbation-based method to generate hard negatives and their token-level labels for training. The paper demonstrates the benefits of TLDR models in helping models self-correct and serving as a hallucination evaluation tool. Additionally, TLDR models can accelerate human annotation by three times, improving the acquisition of high-quality vision-language data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1 The TLDR model provides detailed token-level feedback, addressing the issue of overly simplistic binary annotations, which is a significant improvement in the evaluation process for MLLMs.\n\n2 The perturbation-based approach for generating synthetic hard negatives adds diversity and robustness to the model training, improving the model\u2019s ability to handle difficult scenarios.\n\n3The model can be applied in multiple contexts, both for self-correcting MLLM outputs and as a hallucination evaluation tool, making it a versatile contribution."
            },
            "weaknesses": {
                "value": "1. Although the paper introduces the concept of token-level feedback, this approach feels like a natural extension of existing reward modeling techniques, rather than a fundamentally new innovation. The idea of fine-grained feedback has already been explored in other contexts, and the paper does not sufficiently differentiate its contribution from prior work.\n2. This paper didn't compare the proposed model with existing hallucination detection and mitigation methods, such as Woodpecker or other state-of-the-art techniques. Additionally, Furthermore, the paper introduces self-correction as a novel advantage of TLDR, but other metrics and models used in hallucination detection could also be adapted to perform self-correction.\n3. In Section 5.3, the authors evaluate the self-correction capabilities of the TLDR model using only one dataset, WinoGround. Relying on a single dataset raises concerns about the generalizability and reliability of the results.\n4. The proposed hallucination evaluation didn\u2019t show the superiority over the existing hallucination evaluation methods.\n5.  Lack of comprehensive survey of hallucination on Large Vision-Language Models.\n[1] Object hallucination in image captioning\n[2] Evaluating Object Hallucination in Large Vision-Language Models\n[3] FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models\n[4] Analyzing and mitigating object hallucination in large vision-language models\n[5] FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback\n[6] Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models\n6. Lack of evluation for the synthetic data. Meanwhile, the robustness of trained models highly rely on the quality of the synthetic data.\n7. The models used to validate the SELF-CORRECTION WITH TLDR MODELS are too few, which limits the robustness of the results."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}