{
    "id": "gFUomIaycw",
    "title": "Dynamic Routing Mixture of Experts for Enhanced Multi-Label Image Classification",
    "abstract": "Multi-label image classification (MLC) is a fundamental task in computer vision, requiring the identification of multiple objects or attributes within a single image. Traditional approaches often rely on shared backbones and static gating mecha-nisms, which can struggle to effectively capture complex label correlations and handle label heterogeneity, leading to issues such as negative transfer. In this pa-per, we introduce the Dynamic Routing Mixture of Experts (DR-MoE) model, a novel architecture that integrates input-dependent dynamic gating networks into the mixture-of-experts (MoE) framework for MLC. Unlike static gating in exist-ing models like the Hybrid Sharing Query (HSQ) Yin et al. (2024), our dynamic gating mechanism adaptively selects and weights both shared and task-specific experts based on the input image features. This allows DR-MoE to better capture varying label dependencies and mitigate negative transfer, resulting in improved overall and per-label classification performance. We conduct extensive experi-ments on benchmark datasets MS-COCO Lin et al. (2014) and PASCAL VOC 2007 Everingham et al. (2015), demonstrating that DR-MoE achieves state-of-the-art results, outperforming existing methods including HSQ, Q2L Liu et al.(2021), and ML-GCN Chen et al. (2019). Additionally, ablation studies confirm the effectiveness of dynamic gating in enhancing model adaptability and perfor-mance, particularly for labels with high heterogeneity. Our findings suggest that incorporating dynamic routing mechanisms into MoE architectures is a promising direction for advancing multi-label image classification.",
    "keywords": [
        "Multi-label image classification",
        "Dynamic Routing Mixture of Experts",
        "Computer vision",
        "Dynamic gating networks",
        "Label heterogeneity"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=gFUomIaycw",
    "pdf_link": "https://openreview.net/pdf?id=gFUomIaycw",
    "comments": [
        {
            "summary": {
                "value": "The paper proposed a dynamic gating-based multi-task learning for multi-labelled data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The proposal is interesting as it shows a new direction for multi-task learning for multi-labelled data."
            },
            "weaknesses": {
                "value": "The major weakness of the paper is that the paper is hard to follow. The description of the architecture should be summarized in a diagram. \n\nFor example, for professional representation is required, fig 1 is hard to follow.\n\nAnother major limitation is that the experimental section is very shallow. as only PASCAL VOC 2007 and MS-COCO are used. There exist many datasets in the literature that should be used, such as Taxonomy, CelebA, CelebMask etc. More experiments on varying segmentation tasks, NLP, retrieval,  tasks etc should be conducted to prove the proposal's effectiveness.\n\nA comparison with many related works in the literature is also missing."
            },
            "questions": {
                "value": "A proper block digram of the implementation is required."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a Dynamic Routing Mixture of Experts (DR-MoE) model for the multi-label image classification task, which improves the static gating of the Hybrid Sharing Query (HSQ) by introducing a dynamic gating mechanism that adaptively selects and weighs both shared and task-specific experts based on the input image features. Experiments on the Pascal VOC 2007 and MS-COCO datasets demonstrate the superiority of the proposed DR-MoE model in the multi-label image classification task, and ablation studies on the MS-COCO dataset verify the effectiveness of its key designs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper is generally well organized and written.\n- The motivation of the dynamic gating mechanism is reasonable.\n- The technical details throughout the paper are well explained and easy to reproduce."
            },
            "weaknesses": {
                "value": "- The symbol for binary cross-entropy loss and the symbol for the number of labels are both $L$ in Eq.10, which should be differentiated.\n- This paper exhibits some experimental results in Table 3 and Table 6. However, the reviewer could not find any explanation about Table 3 and Table 6 in experiment section.\n- The dynamic gating network is the core of the DR-MoE model, and merely using the case analysis in Figure 3 to demonstrate its effectiveness in allocating weights for shared experts and task-specific experts is not convincing enough. Authors are encouraged to provide a statistical analysis, e.g., the distribution of weights on these experts across all single-labeled and multi-labeled images in the entire test dataset.\n- The authors are encouraged to evaluate the performance of the proposed DR-MoE using additional vision backbones, including TResNet and Vision Transformer.\n- The comparison between the DR-MoE model and baseline methods is too weak, and many of the latest multi-label image classification methods have been overlooked. For example, SALGL [1], IDA [2], PAT [3].\n\n\n[1] Scene-Aware Label Graph Learning for Multi-Label Image Classification. ICCV 2023.\n\n[2] Causality Compensated Attention for Contextual Biased Visual Recognition. ICLR 2023.\n\n[3] Counterfactual Reasoning for Multi-Label Image Classification via Patching-Based Training. ICML 2024."
            },
            "questions": {
                "value": "- In section 4.3, the author claims that the proposed DR-MoE model outperforms all baseline methods on the MS-COCO dataset. However, the reviewer disagrees with this. According to the Table 1, both Q2L and HSQ exhibit better performance in mAP than DR-MoE. The author should clarify this.\n- The HSQ method has multiple task-specific experts, while in DR-MoE, the number of task-specific experts defaults to 1. The reviewer is curious about the impact of the number of task-specific experts on model performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a model\u2014the Dynamic Routing Mixture of Experts (DR-MoE) model, for enhancing multi-label image classification (MLC) tasks. By integrating input-dependent dynamic gating networks into the Mixture-of-Experts (MoE) framework, the model adaptively selects and weights both shared and task-specific experts. It achieves state-of-the-art results on benchmark datasets such as MS-COCO and PASCAL VOC 2007."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is easy to follow and details of experiments are carefully described."
            },
            "weaknesses": {
                "value": "1. The claimed defects in HSQ appear to lack supporting evidence. The proposed method appears to have already been implemented in the baseline model. See Question 1.\n2. The novelty and key contribution of this paper are hard to tell.\n3. The fig 1 is hard to understand."
            },
            "questions": {
                "value": "1.\tAs I gather, the gating networks are not static as the authors claimed in the manuscript. Section 3.3, eq(1, 2) in HSQ explicitly points out that it employs an unique gating network for every label and the gating networks generate weight upon all employed experts with task-specialized feature input. Correct me if I am wrong, this seems to be just the definition of \u201cDynamic routing\u201d in the submitted manuscript. Gating networks of the proposed model seems to be highly identical to the HSQ\u2019s gating networks.\n\n2.\tAuthor doesn\u2019t mention the resolution of image you used. Are all comparisons under the same image resolution to ensure fair? I also wonder if the same data augmentation methods and other conditions are used in the ablation study on the effect of dynamic gating .\n3.\tIs the model\u2019s good performance due to the introduction of a large number of parameters? It is recommended to indicate parameters in the experiments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the Dynamic Routing Mixture of Experts (DR-MoE) model, a novel architecture for multi-label image classification (MLC). The model utilizes input-dependent dynamic gating within the Mixture of Experts framework, selecting shared and task-specific experts adaptively based on image features. The authors claim that DR-MoE overcomes the limitations of static gating by better capturing label dependencies and mitigating negative transfer. Experiments on the MS-COCO and PASCAL VOC 2007 datasets demonstrate that DR-MoE achieves state-of-the-art performance, outperforming prior methods, with ablation studies showcasing the benefits of dynamic gating in handling label heterogeneity."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe proposed approach is simple yet effective, making it easy to follow.\n2.\tThe model demonstrates excellent performance on both the COCO and PASCAL VOC datasets, as supported by experimental results."
            },
            "weaknesses": {
                "value": "1.\tLack of evidence for statement. The authors argue that learning multiple labels jointly can cause negative transfer due to label heterogeneity. However, this claim lacks empirical evidence or experiments to substantiate it. Intuitively, semantically similar classes often benefit from shared low-level features. Including experimental validation or citation of prior work would strengthen this argument.\n2.\tUnconvincing motivation. The authors critique HSQ for employing static gating, claiming that the proposed dynamic gating mechanism offers better adaptability (the key motivaition). However, I think HSQ also uses a dynamic gating network to predict routing scores for expert contributions. The example provided (e.g., \u201can image containing both bicycle and person...\u201d in Line 53-54) does not demonstrate that HSQ cannot handle dynamic label correlations, because HSQ\u2019s predicted routing scores could still allocate task-specific experts as required.\n3.\tLimited novelty. The primary contribution lies in the input-dependent dynamic gating mechanism, while other components rely on existing, off-the-shelf modules. This limits the novelty of the proposed approach.\n4.\tUnclear presentation of the Method. The paper suffers from unclear visual representation. For example, Figure 1's overview does not effectively convey the proposed method, and the gating visualizations in Figure 2 are vague, using labels such as \u201cImage 1\u201d and \u201cImage 2\u201d without showing the actual images. PS: A minor issue is the incorrect citation format. The correct format is \u201c\\citep\u201d rather than \u201c\\cite\u201d."
            },
            "questions": {
                "value": "The questions are listed in the \u201cweakness\u201d section. I strongly encourage the authors to continue improving their work and explore ways to address these weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}