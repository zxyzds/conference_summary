{
    "id": "aSy2nYwiZ2",
    "title": "Injecting Universal Jailbreak Backdoors into LLMs in Minutes",
    "abstract": "Jailbreak backdoor attacks on LLMs have garnered attention for their effectiveness and stealth. However, existing methods rely on the crafting of poisoned datasets and the time-consuming process of fine-tuning. In this work, we propose JailbreakEdit, a novel jailbreak backdoor injection method that exploits model editing techniques to inject a universal jailbreak backdoor into safety-aligned LLMs with minimal intervention *in minutes*. JailbreakEdit integrates a multi-node target estimation to estimate the jailbreak space, thus creating shortcuts from the backdoor to this estimated jailbreak space that induce jailbreak actions. Our attack effectively shifts the models' attention by attaching strong semantics to the backdoor, enabling it to bypass internal safety mechanisms. Experimental results show that JailbreakEdit achieves a high jailbreak success rate on jailbreak prompts while preserving generation quality, and safe performance on normal queries. Our findings underscore the effectiveness, stealthiness, and explainability of JailbreakEdit, emphasizing the need for more advanced defense mechanisms in LLMs.",
    "keywords": [
        "Large language model",
        "Jailbreak",
        "Backdoor",
        "Attack",
        "Safety",
        "Model Editing"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=aSy2nYwiZ2",
    "pdf_link": "https://openreview.net/pdf?id=aSy2nYwiZ2",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a method, JailbreakEdit, for injecting trigger-activated backdoors in LLMs. Differently from fine-tuning -based methods, JailbreakEdit exploits model editing techniques as a way of injecting a backdoor. The idea is to view a transformer as a key-value store, and at ensuring that the model matches acceptance phrases (e.g., \"Sure\", \"Absolutely!\") to the provided trigger."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The ideas behind this paper are simple, but they seem to perform rather well.\nDifferently to prior methods, JailbreakEdit doesn't require fine-tuning.\nThe authors thoroughly evaluated the method empirically (for several LLMs)."
            },
            "weaknesses": {
                "value": "JailbreakEdit requires whitebox access to the model's parameters; this heavily limits its applicability as an attack.\nThe method is outperformed by other methods, such as Poison-RLHF, which is although argued to have convergence issues.\nFinally, the authors argue (beginning of page 4) that the backdoored LLM should exhibit safety-alignment properties. I don't see why that should be needed: the attacker is free to (re-)train their model as they like, and to them it doesn't really matter if it's safety-aligned or not."
            },
            "questions": {
                "value": "- I have one concern about your evaluation: JSR was evaluated via open source classifiers; yet, these classifiers are presumably LLMs, which are also susceptible to attacks. How can you ensure that the JSR figures are accurate? Did you manually inspect (some of) the results?\n- You mention that fine-tuning would be expensive for an attacker. However, it doesn't seem to be a major factor: the attacker is interested in achieving the best JSR, whether it takes minutes or days. After all, this is a one-off cost. You may want to down-tune this claim.\n- Fig 2: how did you calculate the generation quality? That should be explained near the caption\n- the terminology in page 3 is quite confusing: in some cases, you talk about \"backdoors\" meaning \"backdoor attacks\" even when you're talking about \"jailbreak backdoors\"; please, use different terminology and double check its uses throughout."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduce a novel method based on model edit to \ninject universal jailbreak backdoors into LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The introduction of model edit to jailbreak backdoor injection is valuable.\n2. Extensive experiments are conducted to evaluate the effectiveness of the proposed method.\n3. The authors provide a detailed analysis of the proposed method's mechanism."
            },
            "weaknesses": {
                "value": "1. The threat model requires further clarification. For attackers, it is\n   reasonable to assume that they can distribute the poisoned model, but if\n   the attackers run the model on their own servers and offer the API to\n   others, why should they inject backdoors? In the latter case, the\n   attackers themselves become the victims.\n2. Presentation may require improvement. What is the definition of \"node\" in\n   the multi-node target estimation? The notation seems to be inconsistent.\n   \"Response\" is denoted as $R$ on line 257, but $N$ on line 271.\n3. As a backdoor injection method, it is important to consider the\n   usefulness of the model after the backdoor is injected. What is the\n   usefulness of the model after the backdoor is injected? \n4. In equation (5), $\\tilde{k}$ is defined as the average value over all\n   constructed prompts. However, as the semantics may not be continuous \n   in the high-dimensional space, is this average value meaningful?"
            },
            "questions": {
                "value": "Please see the weaknesses section for questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "the paper introduces JailbreakEdit, a method for injecting universal jailbreak backdoors into safety-aligned LLMs. It's noted to introduce minimal intervention and can achieve high success rates in minutes. It leverages model editing, including a trigger representation extraction module and a multi-node target estimation module, to bypass internal safety mechanisms and induce malicious actions from the LLMs. Evaluations are conducted over different settings, models, and datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. timely topic. And the usage of model editing in this field appears reasonable.\n\n2. non-trivial technical contribution. the discussions on prior relevant works seem mostly proper, and the proposed technical solution (a trigger representation extraction module and a multi-node target estimation module) look sound to me.\n\n3. writing is quite good."
            },
            "weaknesses": {
                "value": "- limited applicability.\n\nThe proposed method is not applicable on remote, black-box models. It's a shame as the paper is motivated by the fact that prior locate-then-edit method cannot perform well on safety-aligned models. Yet, those black-box, commercial models (e.g., GPT family) are safety aligned to a great extent. Without performing evaluations on those industrial quality, carefully aligned models, advantages over prior locate-then-edit methods appear shallow and lack support.\n\n- further empirical support on cost is needed.\n\nwhile the paper highlights the rather low cost of the proposed method (i.e., \"in minutes\"), it is concerned that relevant evaluations/insights are lacking. Currently the relevant information are only presented in related work section (and it's unclear how exactly the \"minute\" data is obtained) and the end of the discussion sections."
            },
            "questions": {
                "value": "1. Explain why the currently evaluated models (e.g., llama 7b/13b) have high representability of \"safety-aligned models\"\n\n2. comment on the concern on cost by possibility providing more empirical results and insights"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}