{
    "id": "3VOKrLao5g",
    "title": "KAAN: Kolmogorov-Arnold Activation Network --- a Flexible Activation Enhanced KAN",
    "abstract": "Kolmogorov-Arnold Networks (KAN) have led to a significant breakthrough in the foundational structures of machine learning by applying the Kolmogorov-Arnold representation theorem. Through this approach, the target conditional distribution is expressed as the summation of multiple continuous univariate B-spline functions. However, KAN faces the challenges of being unintuitive and inflexible. To address this issue, we analyze the structural configurations of Multi-Layer Perceptrons (MLPs) and KANs, finding that MLP can be represented in a form conforming to Kolmogorov-Arnold representation Theorem (KAT). Therefore, we propose MLP style KAN framework Kolmogorov-Arnold Activation Network (KAAN), which is more intuitive, flexible and transferable. To verify the flexibility and transferability of our approach, we extend it to Convolutional Neural Network (CNN). Also, we demonstrate that parameter sharing is beneficial not only for efficiency but also for effectiveness. KAAN shows better representation capacity than MLP on several benchmarks. Furthermore, our experiment results lead us to conclude that this method is feasible for integrating modern network approaches such as CNNs.",
    "keywords": [
        "Kolmogorov-Arnold representation Theorem",
        "Kolmogorov-Arnold Network",
        "Multi-Layer Perceptrons"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=3VOKrLao5g",
    "pdf_link": "https://openreview.net/pdf?id=3VOKrLao5g",
    "comments": [
        {
            "summary": {
                "value": "Authors propose Kolmogorov-Arnold activations inspired from KANs (Kolmogorov-Arnold Networks) and replace B-splines in KANs to achieve similar or better performance than MLPs. Authors show that MLPs can be represented in a form conforming to Kolmogorov-Arnold representation Theorem (KAT). Using MLP-like equipped with Kolmogorov-Arnold activations, authors experiment and compare different basis functions. Experiments also demonstrate successful integration with Convolutional Neural Networks (CNNs) which achieving comparable performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper is written clearly and concisely, and is easy to read. \n2. The proposed activation makes KANs more flexible and easy to deploy which would encourage the scientific community to experiment with these networks.\n3. Experiments clearly demonstrate that the proposed activation function allows KANs to be trained while achieving comparable performance to MLPs and even ResNets."
            },
            "weaknesses": {
                "value": "1. Novelty is missing: KAN arxiv report (Liu et. al 2024) already gives a MLP-like interpretation of KANs which allows stacking of layers similar to MLPs which is similar to section 3 in the paper.\n2. Authors have essentially replaced splines, which is a core contribution of the original KAN paper (provides higher degree of control to model univariate functions) with learnable activation functions. There is already literature covering learnable activation functions with different basis like Polynomial or sinusoidal basis (in context of MLPs). Therefore I feel the paper doesn\u2019t bring new insights into Neural Networks or KANs."
            },
            "questions": {
                "value": "1. I would suggest authors to reevaluate the core contributions and rewrite the paper. If the main contribution is empirical in nature, I would suggest doing more experiments on transformer-like architectures or showing taks where MLPs or KANs fail to learn underlying functions correctly but the proposed method can.\n2. What is the meaning of \u201cKAN faces the challenges of being unintuitive and inflexible.\u201d This is a highly subjective statement, giving concrete examples of what inflexible and unintuitive means would help readers. Does KAAN help give more flexibility or intuition? If so, how? What is the takeaway?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents Kolmogorov-Arnold Activation Networks, an extension of Kolmogorov-Arnold Networks, that uses MLP/CNN-like architecture with flexible activation functions defined for each edge between neurons."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed approach clearly works on the presented tasks, and in some cases provides a performance improvement.\n2. The KAAN parametrization is compatible with standard ANN architectures.\n3. Related to the previous point, this parametrization might be helpful for neural architecture search/meta-learning/similar approaches that adapt neural networks\u2019 architectures, as the nonlinearity parameters are designed to be differentiable."
            },
            "weaknesses": {
                "value": "2. Memory and computation time requirements\n\nThe computational requirements of KAANs appear to be much higher than for corresponding standard MLPs/CNNs. Eq. 6 uses several weights per connection (one for each activation type) and additionally parametrizes the activations. This should increase both memory consumption and running time of KAANs compared to standard networks. \n\nThe increased number of parameters in KAANs also (unless I missed something) suggests the performance improvements (Tabs. 3-5) are very modest compared to standard networks that use several times fewer parameters. \n\n3. Lack of interpretability\n\nThroughout the paper, KAANs are called intuitive. However, I do understand how KAANs are more intuitive than standard MLPs (if anything, they are more convoluted). The results in Tabs. 3-5 indirectly confirm my concern: there\u2019s no clear winner across different combinations of activation functions.\n\nLines 300-311 discuss the potential uses cases for each activation function, but all of those apply to standard ANN architectures that don\u2019t define edge-based nonlinearities. \n\n4. Poor writing\n\nThe paper needs some writing improvements. Here are some instances I\u2019ve noticed, although text needs overall polish.\n1. [Line 30] \u201cThere were not many breakthroughs until KANs\u201d [rephrased] \u2013 I would disagree, and suggest Transformers as an obvious architectural breakthrough. But, the list can expand with for instance capsule networks (https://www.sciencedirect.com/science/article/pii/S1319157819309322) and gflownets (https://arxiv.org/abs/2111.09266). \n2. The introduction contains many terms, such as LANs and TANs, but they\u2019re not cited until related work. \n3. \u201cNo many\u201d instead of \u201cnot many\u201d in line 30, extra bracket in line 81, typo in line 205, non-plural \u201cExperiment\u201d name for Sec. 5"
            },
            "questions": {
                "value": "1. What are the parameter counts/VRAM consumption/running time for the tested KAANs vs. MLPs/CNNs? \n2. Is it possible to compare KAANs with standard networks that use the same number of parameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel architecture named KAANs, which enhances the efficiency of MLP by incorporating a method inspired by KANs. Theoretically, the paper begins by establishing that MLPs are a subset of KANs and then deviates from traditional KANs by replacing B-spline activation functions with linear combinations of basis functions. Experimentally, the paper evaluates 7 different combinations of basis functions as activation functions across various AI-related tasks, demonstrating that KAANs achieve higher accuracy than both MLPs and KANs.\n\nWhile the theoretical foundation is robust and compelling, the KAANs just replace activation functions in MLPs with more complex functions. when trying to search for the optimal combination of basis functions along with the most effective weights, the concept go back to the learnable activation functions.  Therefore, it appears that the paper has elegant theory but not enough contributions on practical level."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The theoretical framework is elegantly and solidly constructed.\n\nIt points out that \u201cMLP represents a specific instance of KAN\u201d\n\nIt points out that\u201cany continuous univariate basis functions can be used as activation function\u201d\n\nKAANs offer greater flexibility and fewer limitations than traditional KANs, making them more adaptable to various structures.\n\nThe paper conducts extensive experiments across a multitude of AI-related tasks."
            },
            "weaknesses": {
                "value": "The paper experiments with various combinations of basis functions, where different combinations excel in different tasks. This variability raises questions about how to determine the most effective combination for a given task.\n\nAlthough KAANs outperform MLPs and KANs in the experiments, the comparison may not be entirely fair. The more complex activation functions used in KAANs require greater computational power compared to MLPs, potentially skewing the results. Similarly, comparing KAANs to KANs without adjusting for KANs' longer training requirements may not provide a balanced view of their respective efficiencies."
            },
            "questions": {
                "value": "My opinion could shift towards acceptance if the authors could address one of the following points:\n\nDevelop a method to identify the most optimal combination of basis functions.\n\nFind a specific combination of basis functions that consistently outperforms others.\n\nDemonstrate that in certain specific tasks, KAANs offer a significant advantage."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors proposed a novel framework of viewing MLPs and a special case of KANs and proposed as a inspiration KAAN, where each nonlinear activation function is parametrized by a linear combination of basis functions. They conducted extensive experiments on challenging datasets including Tabular datasets and Cifar-10, and introduced a convolutional version as well. The article presented an interesting perspective and should be treated as a nice improvement on KANs, with the following limitations.\n\n1. While KAAN seems interesting, it seems still such a way of parametrization of nonlinearity in KANs, with more complicated nonlinearity. This improvement is at best incremental and would need more support from numerical evidence.\n\n2. The referee would envision that KAANs suffer from less interpretability than KANs; especially on symbolic regression. Could the authors comment on this restriction?\n\n3. It would be interesting to elaborate more on the perspective in Sec 3.2 and gain more motivation on the comparison between KANs and MLPs.\n\n4. How does (C)KAAN perform on more challenging tests?"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The authors proposed a novel framework of viewing MLPs and a special case of KANs \n\n2. They conducted extensive experiments on challenging datasets including Tabular datasets and Cifar-10, and introduced a convolutional version as well."
            },
            "weaknesses": {
                "value": "1. While KAAN seems interesting, it seems still such a way of parametrization of nonlinearity in KANs, with more complicated nonlinearity. This improvement is at best incremental and would need more support from numerical evidence.\n\n2. The referee would envision that KAANs suffer from less interpretability than KANs; especially on symbolic regression. Could the authors comment on this restriction?"
            },
            "questions": {
                "value": "1. It would be interesting to elaborate more on the perspective in Sec 3.2 and gain more motivation on the comparison between KANs and MLPs.\n\n2. How does (C)KAAN perform on more challenging tests?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}