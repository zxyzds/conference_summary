{
    "id": "8ibaVk4mU8",
    "title": "Coarse Correspondences Boost 3D Spacetime Understanding in Multimodal Language Model",
    "abstract": "Multimodal language models (MLLMs) are increasingly being applied in real-\nworld environments, necessitating their ability to interpret 3D spaces and compre-\nhend temporal dynamics. Current methods often rely on specialized architectural\ndesigns or task-specific fine-tuning to achieve this. We introduce COARSE CORRE-\nSPONDENCES, a simple lightweight method which enhances MLLMs\u2019 understand-\ning of 3D and temporal concepts using only 2D images, without modifying the\narchitecture or task-specific fine-tuning. Our method uses a lightweight tracking\nmodel to identify primary object correspondences between frames in a video or\nacross different image viewpoints, and then conveys this information to MLLMs\nthrough visual prompting. We demonstrate that this simple training-free approach\nbrings substantial gains to GPT4-V/O consistently on four benchmarks that require\n3D and temporal understanding, including +20.5% improvement on ScanQA,\n+9.7% on OpenEQA\u2019s episodic memory subset, +6.0% on the long-form video\nbenchmark EgoSchema, and +11% on the R2R navigation benchmark. Addition-\nally, we show that COARSE CORRESPONDENCES can also enhance open-source\nMLLMs\u2019 understanding of 3D space (by +6.9% on ScanQA) when applied in both\ntraining and inference and that the improvement can generalize to unseen datasets\nsuch as SQA3D (+3.1%). Taken together, we show that COARSE CORRESPON-\nDENCES effectively and efficiently boosts models\u2019 performance on downstream\ntasks requiring 3D and/or temporal understanding.",
    "keywords": [
        "Multimodal Language Model; 3D Understanding; Temporal Understanding"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8ibaVk4mU8",
    "pdf_link": "https://openreview.net/pdf?id=8ibaVk4mU8",
    "comments": [
        {
            "withdrawal_confirmation": {
                "value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."
            }
        },
        {
            "summary": {
                "value": "This paper presents a training-free visual prompting method with coarse images correspondance to enhances the 3D and temporal understanding of multimodal large language models (MLLMs). The proposed method works by identifying object correspondences across video frames or image viewpoints using a lightweight tracking model, identifying the topK most salient correspondences, and then visualizing these correspondences as visual prompts input for the MLLM for better 3D reasoning. The approach demonstrated substantial performance improvements across various benchmarks, including ScanQA, OpenEQA, EgoSchema, and R2R, outperforming state-of-the-art models in zero-shot settings. The author also curate a diagnostic dataset called the Spatial Orientation Test (SOT) to assess the models' ability to perform spatial perspective-taking from viewpoints other than the camera's. The results demonstrate that Coarse Correspondences significantly improves MLLMs' performance on these tasks, establishing new state-of-the-art results."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **Simplicity and Effectiveness**:   \nThe method is training-free, leveraging existing tracking models to create visual prompts, making it straightforward and efficient. By overlaying unique markers on frequently occurring objects, the method supplies explicit spatial cues that aid the model's reasoning capabilities. It can also be applied broadly to various tasks without the need for task-specific finetuning or additional training of the MLLMs.\n2. **Significant Performance Gains**:   \nThe method boosts performance on multiple 3D and temporal understanding tasks across proprietary and open-source MLLMs.\n3. **Reduced Computational Load**:   \nCoarse visual correspondence requires fewer resources to process compared to dense correspondence, which can be computationally intensive and may overwhelm the model with excessive data. By focusing on key correspondences, the proposed method maintains a balance between performance improvement and computational efficiency.\n4. **Introduction of the SOT Dataset**:  \nThe curated Spatial Orientation Test (SOT) dataset provides a valuable resource for evaluating spatial perspective-taking, a challenging aspect of spatial reasoning for MLLMs. The dataset points out current limitations and areas for future improvement."
            },
            "weaknesses": {
                "value": "1. **Dependence on Tracking Model Quality:**  \nThe effectiveness of Coarse Correspondences highly depends on the accuracy of the lightweight tracking model used to establish object correspondences. Errors or biases in the tracking model could negatively impact the MLLM's understanding, leading to incorrect or misleading reasoning.\n\n2. **Scope Limitation**:   \nObject-level correspondence may limit the model's overall 3D understanding when tasks require a deeper subject-level understanding and reasoning, such as interactions involving complex subjects (e.g., opening a drawer or detailed human interactions). \nAdditionally, the SOT dataset comprises only 10 scenes with a total of 50 questions, which may not be sufficient to fully assess spatial perspective-taking abilities.\n\n3. **Visual Occlusion Issues**:   \nThe addition of coarse correspondences and visual markers may not fully resolve visual occlusion challenges, where key visual information is partially or entirely blocked. This could limit model comprehension, especially when dealing with dense or complex scenes. Empirically find an optimal mark size may not work in some cases and rely on manual effort. \n\n4. **Assumption of Prominent Object Importance:**  \nThe method focuses on the most frequent object instances, which may overlook less frequent but contextually significant objects.\nThis could result in a biased understanding of the scene, neglecting critical elements necessary for accurate reasoning.\n\n5. **Lack of In-depth Analysis on Failure Cases**:   \nThe paper does not extensively cover the limitations or scenarios where coarse correspondences technique may not work as expected."
            },
            "questions": {
                "value": "- **Suggestion:**  \nThe paper presents a simple yet effective approach to enhancing 3D spatial and temporal understanding in multimodal large language models through the use of Coarse Correspondences. However, notable weaknesses include the method's reliance on the quality of the tracking models, potential visual clutter introduced by overlaying markers, and the limited scope of the method and the dataset. These concerns prevent me from giving a positive evaluation at this moment. Resolving the aforementioned issues would strengthen the submission.\n\n- **Additional questions:**  \nHow scalable is the method when applied to very long video sequences or datasets with extensive temporal changes? Does the sampling approach used in coarse correspondences retain sufficient contextual information for models to perform well on such datasets, or are there limitations that need addressing when handling longer temporal contexts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work focuses on leveraging different forms of prompts to significantly enhance the understanding of 3D spatial location information by mature LLMs like GPT-4-O."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This work focuses on leveraging different forms of prompts to significantly enhance the understanding of 3D spatial location information by mature LLMs like GPT-4-O."
            },
            "weaknesses": {
                "value": "This work focuses on leveraging different forms of prompts to significantly enhance the understanding of 3D spatial location information by mature LLMs like GPT-4-O. However, I have the following questions:\n1.How much improvement does this method provide for other 2D MLLMs besides those listed in the paper, such as LLAVA?\n2.Besides the benchmarks mentioned in the paper, can this method be applied to more benchmarks?\n3.This method seems a bit overly simplistic. Please restate its innovativeness and necessity, as well as how it differs from similar methods in the same category."
            },
            "questions": {
                "value": "This work focuses on leveraging different forms of prompts to significantly enhance the understanding of 3D spatial location information by mature LLMs like GPT-4-O. However, I have the following questions:\n1.How much improvement does this method provide for other 2D MLLMs besides those listed in the paper, such as LLAVA?\n2.Besides the benchmarks mentioned in the paper, can this method be applied to more benchmarks?\n3.This method seems a bit overly simplistic. Please restate its innovativeness and necessity, as well as how it differs from similar methods in the same category."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a simple, training-free, and effective visual prompting method, COARSE CORRESPONDENCES, to improve the MLLMs' spatial and temporal understanding ability. CC uses a tracking model to find object correspondences between images, mark the same objects across the image sequence, and show huge improvements on ScanQA, OpenEQA, and EgoSchema benchmarks with state-of-the-art results. This method can also improve spatial understanding of MLLM on downstream tasks such as navigation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This work points out that existing MLLMs have potential 3D spatial understanding capabilities and can be excited using simple prompt methods.\n2. The author provides an effective method to enhance MLLMs' 3D and temporal understanding by simple visual prompting without complex architectural designs or downstream fine-tuning.\n3. The author demonstrates the effectiveness of the method on downstream tasks like ScanQA, OpenEQA, and R2R navigation.\n4. This paper presents a new benchmark, SOT, to evaluate spatial reasoning ability from alternative viewpoints."
            },
            "weaknesses": {
                "value": "1. The SOT benchmark, which evaluates spatial understanding from another viewpoint, is interesting but not very closely related to the method.\n2. This method relies on the results of existing tracking models, which may introduce limitations in accuracy and robustness, particularly for long-form videos. Although existing tracking models such as SAMv2 already perform well, errors can still occur when only a few frames are sampled. The impact of these errors, as well as the solution, should be studied more.\n3. Most of the author's experiments were done on closed-source models such as GPT-4V/GPT-4O. I believe that experiments on open-source models under the SAME setting would be more useful.\n4. There is no section for analysis of related works, and many missing ones need to be discussed. To only mention a few:\n- PointLLM: Empowering Large Language Models to Understand Point Clouds\n- Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following\n- An Embodied Generalist Agent in 3D World\n- LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness\n\nSome recent public ones are also encouraged to be supplemented to make a more thorough discussion."
            },
            "questions": {
                "value": "1. I'm a little confused about the experiments in the PROMPTING OPENMODELS section. Since CC is a TRAINING-FREE method, why didn't the llava experiment take the same setting as before?\n2. I'm very interested in the inference time for each step of the method on the navigation task."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}