{
    "id": "vhPE3PtTgC",
    "title": "SWEb: A Large Web Dataset for the Scandinavian Languages",
    "abstract": "This paper presents the hitherto largest pretraining dataset for the Scandinavian languages: the Scandinavian WEb (SWEb), comprising over one trillion tokens. The paper details the collection and processing pipeline, and introduces a novel model-based text extractor that significantly reduces complexity in comparison with rule-based approaches. We also introduce a new cloze-style benchmark for evaluating language models in Swedish, and use this test to compare models trained on the SWEb data to models trained on FineWeb, with competitive results. All data, models and code are shared openly.",
    "keywords": [
        "dataset",
        "pre-training",
        "swedish",
        "danish",
        "norwegian",
        "icelandic"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We present the hitherto largest pretraining dataset for the Scandinavian languages: the Scandinavian WEb (SWEb), comprising over one trillion tokens.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vhPE3PtTgC",
    "pdf_link": "https://openreview.net/pdf?id=vhPE3PtTgC",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces SWEb, the largest pretraining dataset for Scandinavian languages, containing over one trillion tokens across Swedish, Danish, Norwegian, and Icelandic. SWEb aims to address the scarcity of large-scale, high-quality datasets specifically tailored for these languages. To create this dataset, the authors develop a model-based text extraction pipeline that enhances efficiency and reduces complexity compared to rule-based methods. Key contributions include:\n\n- SWEb Dataset: An extensive web dataset of Scandinavian languages with over one trillion tokens, surpassing existing resources by an order of magnitude.\n- Model-Based Extraction Pipeline: A novel, data-driven text extraction model that effectively filters high-quality content, yielding about 60% more usable tokens than previous approaches like FineWeb.\n- Swedish Benchmark (HP-MEK): A new cloze-style benchmark for Swedish, derived from the Swedish Scholastic Aptitude Test, to evaluate language models trained on SWEb and demonstrate competitive performance against models trained on FineWeb.\n\nThe authors openly release the SWEb dataset, extraction pipeline, and the HP-MEK benchmark to support further research and development in Scandinavian language modeling\u200b"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality\n\nThe SWEb dataset is original in its approach to handling Scandinavian languages. The authors create a model-based extraction process that moves away from rule-heavy, manual extraction methods, simplifying the pipeline. They also introduce HP-MEK, a benchmark specific to Swedish, which adds value by providing a relevant evaluation tool for Scandinavian models.\n\nQuality\n\nThe quality of the work is evident in the detailed steps of the SWEb pipeline. The authors carefully build a process to select and clean high-quality data, resulting in 60% more usable tokens than previous approaches like FineWeb. They validate the dataset with clear metrics, comparing models trained on SWEb and FineWeb to show the effectiveness of their extraction model.\n\nClarity\n\nThe paper is organized well, making each pipeline stage easy to understand. Diagrams and examples help clarify complex steps like content extraction and filtering. The authors document their choices for filtering and quality control, making the approach easier to follow and replicate.\n\nSignificance\n\nSWEb is significant because it makes a high-quality, large-scale dataset available for Scandinavian languages, which traditionally have fewer resources. This dataset and the HP-MEK benchmark can help researchers build better models for these languages, making SWEb a useful resource for Scandinavian language research."
            },
            "weaknesses": {
                "value": "1. Limited Applicability Beyond Scandinavian Languages\nWeakness: The SWEb pipeline is tailored specifically for Scandinavian languages, potentially limiting its scalability or adaptability to non-Scandinavian or low-resource languages. This narrow focus may reduce the general utility of SWEb\u2019s approach in multilingual or global settings where language resources are scarcer.\n\nRecommendation: It would be beneficial to discuss adapting the pipeline to other language families or the performance challenges in non-Scandinavian languages. Providing generalization strategies, such as multilingual training or enhanced language detection techniques, could expand SWEb\u2019s relevance. Including preliminary results or small-scale tests on other language groups would further strengthen the paper\u2019s broader applicability.\n\n2. Reliance on Manual Annotation in Model Training\nWeakness: Although SWEb\u2019s model-based extractor is innovative, it relies on manually annotated data (1,380 samples) for training the extraction model, which may be resource-intensive and impractical for other languages or domains. Annotating thousands of samples for every new language could become a bottleneck, especially in low-resource contexts.\n\nRecommendation: Introducing semi-supervised or weakly supervised learning approaches to reduce the dependency on manually annotated data could improve scalability. Alternatively, SWEb could consider leveraging existing rule-based systems or transfer learning from similar languages to jumpstart the model training in new language settings, potentially improving data efficiency.\n\n3. Evaluation Restricted to Swedish Benchmark\nWeakness: The evaluation uses only a Swedish benchmark (HP-MEK) to compare SWEb against FineWeb. While effective for a Swedish-specific evaluation, this choice does not cover other Scandinavian languages in the dataset (e.g., Danish, Norwegian, Icelandic), making it difficult to generalize the effectiveness of SWEb\u2019s extraction pipeline across the entire language set.\n\nRecommendation: Expanding the evaluation to include benchmarks for other Scandinavian languages or adapting HP-MEK for Danish, Norwegian, and Icelandic could enhance the assessment\u2019s robustness. Providing a language-specific performance analysis would offer insights into whether the extraction model\u2019s quality varies across languages and help optimize future language-specific models.\n\n4. Lack of Qualitative Analysis of Extracted Content\nWeakness: The quantitative metrics (e.g., token count, perplexity, accuracy) demonstrate SWEb\u2019s improvements but lack a qualitative assessment of the extracted text's relevance or coherence. Without this, it\u2019s challenging to understand how well the extraction model preserves the content\u2019s intended meaning or cultural context, especially when removing ads and navigation elements.\n\nRecommendation: Including a qualitative evaluation of content extracted by SWEb compared to FineWeb, such as reader surveys or manual inspection of content fidelity, would offer a deeper understanding of its cultural and contextual accuracy. This analysis would help validate the extractor\u2019s ability to maintain high content relevance, especially for Scandinavian-specific terms, phrases, or topics that might be lost during processing.\n\n5. Limited Error Analysis in Content Extraction\nWeakness: The paper does not provide an in-depth error analysis for the types of errors encountered during extraction, such as failures to remove advertisements, incorrect content classification, or issues in handling specific webpage structures. This gap makes it difficult to assess the limitations of the extraction model and how it could be improved.\n\nRecommendation: An error analysis that categorizes extraction mistakes (e.g., missed ads, incorrectly retained menu items, or misclassified headers) would clarify the model\u2019s boundaries and suggest refinements. Detailing any challenges in handling regional dialects or slang in Scandinavian languages would identify areas for improvement in future iterations of SWEb.\n\n6. Computational Expense of the Extraction Process\nWeakness: The computational requirements for SWEb\u2019s extraction model, which consumed 20,000 GPU hours on AMD MI250X GPUs, may be prohibitive for many research labs or developers working with limited resources. This factor could limit the pipeline\u2019s accessibility and adoption.\n\nRecommendation: Considering optimizations in the extraction model, such as fine-tuning on smaller, more frequent batches or experimenting with lighter transformer architectures, could reduce computational demands. Additionally, presenting a cost-benefit analysis comparing SWEb\u2019s compute usage to the downstream performance gains would offer a more balanced view of the pipeline\u2019s scalability and efficiency."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper describes the creation of a currently largest Scandinavian data set for training language models.\nA new method for collecting texts from the web is proposed, based on an encoder model, and compared with a rule-based method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The data set will be a valuable tool for researching language models and understanding their properties for Scandinavian languages, which are all under-resourced and under-investigated."
            },
            "weaknesses": {
                "value": "Some important points about the evaluation are not fully clear.\n\n\"Benchmark HP-MEK\" is unclear: what was the motivation to created this test set and what was exactly evaluated exactly on it?  \nThe new text extraction method or language models trained on the extracted texts? \nFor language models (section 4.2) it is said that there is 90/10 training/test splitting.\nTherefore it is not clear what was involved in evaluations (text extractor or language models or both) and how (on which test set/s)."
            },
            "questions": {
                "value": "182: What were are the annotators exactly instructed to do? How did they mark the main content? \n\n195 isn't => is not\n\n200-202: what are the \"binary line annotations\" exactly? \nCan they be seen in the example in Listing 1?\n\n300-301: What is \"trafilatura\"? Explanation citation? \n\nWhy is markdown better than plain text? \n\n308: alternative benchmarks => alternative to what?\n\n309: to evaluate performance on => performance of what? Of the proposed text extraction method? \n\n\n315: didn't => did not\n\n351: which model? \n\n376: on the two test sets -- which ones? the one from 90/10 split and the other is HP-MEK?\n\n\nFigure 1: what is \"en\"? It is not discussed in the text.\n\n\n\n426: as the desired extraction output is demonstrated instead of encoded as heuristic => the meaning of the sentence is unclear; what does \"demonstrated\" means? What does \"encoded as heuristic\" means?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces a new pretraining dataset for Scandinavian languages, as well as a cloze-style evaluation dataset for Swedish language models."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Usefulness and relevance: Improving the quality and assessing that this quality has indeed improved for LLMs targeting languages other than English is a clearly relevant topic that will have uses even outside of academia.\n\n- Open-source: Authors promise the release of the dataset, benchmark, and utils used to generate/evaluate them.\n\n- Technical details and examples: The paper features many detials about the implementation. Even if it was not open-source, I feel fairly confident that this work could be majorly reproduced."
            },
            "weaknesses": {
                "value": "While it's a sound contribution, I'm not sure ICLR is the right venue for this work. It lacks algorithmic or theoretical novelty, and it's rather a (very good) application of well-known NLP principles to process a new dataset for specific languages.\n\nFor example, the heuristics mentioned in the paper are very similar to \"old\" related work, e.g. [1]\n\nhttps://arxiv.org/abs/1912.07076"
            },
            "questions": {
                "value": "The pretraining dataset supports Scandinavian languages, but the benchmark targets Swedish. With the Swedish benchmark as a starting point, how much work would it be to generate similar benchmarks for other Scandinavian languages?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a new pretraining corpus for Scandinavian languages:Swedish, Danish, Norwegian, and Icelandic. To create this data, they propose a new pipeline that uses a model-based text extractor trained with a small amount of human-labelled data on what is considered the main content in a markdown version of an HTML webpage. They contrast their proposed approach against the FineWeb pipeline, which extracts plain text directly from HTML pages. They further introduce a new cloze style test with a dataset, HP-MEK consisting of 460 examples based on the Swedish Scholastic Aptitude Test (H\u00f6gskoleprovet),  to benchmark pre-trained performance and show that on a small dataset+model setting, their extractor can match the performance of FineWeb filter, despite being simpler in complexity and using an extractor that is only trained with 1380 human-labeled examples.\n\nOnce validated, they use this pipeline to create SWEb, which includes 1.01 trillion tokens."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors present a novel model-based extractor, trained with 1380 human-labelled examples, to identify the main context from documents (HTML converted to markdown).\n2. They introduce a new task and benchmark to validate their pipeline against a baseline, FineWeb and show that their pipeline, while being simpler, results in close performance on this task.\n3. They present a new pretraining corpus of 1.01 trillion tokens for Scandinavian languages, which will be a valuable resource to the community."
            },
            "weaknesses": {
                "value": "1. It is unclear why the authors chose to retain the markdown tags for pretraining (section 4). They also do not explain much about why HP-MEK is a reasonable benchmark for the pretraining setting.\n2. No direct analysis is presented on the efficacy of the model-based extractor, given that it is one of the main component of the paper apart from reporting an F1 score. The downstream application in section 4 is useful but it doesn't say much about what this extractor does or is capable of filtering.\n3. The dataset pipeline only includes filters like content length, # of alphanumeric characters, and unigram entropy for quality filtering. However, it would have been useful if there was any direct consideration of what is considered high-quality data in the context of pretraining and if additional checks were made in place about safety and fairness of representation in the dataset."
            },
            "questions": {
                "value": "See weakenesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper presents a new pretraining dataset for SWEb which I believe is a very useful resource for the community but there is no discussion of ethical aspects of this dataset and if and whether measures where considered to prevent toxic and harmful content in the curation process."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}