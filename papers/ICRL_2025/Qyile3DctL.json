{
    "id": "Qyile3DctL",
    "title": "Improving LLM Reasoning through Scaling Inference Computation with Collaborative Verification",
    "abstract": "Despite significant advancements in the general capability of large language models (LLMs), they continue to struggle with consistent and accurate reasoning, especially in complex tasks such as mathematical and code reasoning. One key limitation is that LLMs are trained primarily on correct solutions, reducing their ability to detect and learn from errors, which hampers their ability to reliably verify and rank outputs. To address this, we scale up the inference-time computation by generating multiple reasoning paths and employing verifiers to assess and rank the generated outputs by correctness. To facilitate this, we introduce a comprehensive dataset consisting of correct and incorrect solutions for math and code tasks, generated by multiple LLMs. This diverse set of solutions enables verifiers to more effectively distinguish and rank correct answers from erroneous outputs. The training methods for building verifiers were selected based on an extensive comparison of existing approaches. Moreover, to leverage the unique strengths of different reasoning strategies, we propose a novel collaborative method integrating Chain-of-Thought (CoT) and Program-of-Thought (PoT) solutions for verification. CoT provides a clear, step-by-step reasoning process that enhances interpretability, while PoT, being executable, offers a precise and error-sensitive validation mechanism. By taking both of their strengths, our approach significantly improves the accuracy and reliability of reasoning verification. Our verifiers, Math-Rev and Code-Rev, demonstrate substantial performance gains to existing LLMs, achieving state-of-the-art results on benchmarks such as GSM8k and MATH and even outperforming GPT-4o with Qwen-72B-Instruct as the reasoner.",
    "keywords": [
        "Verifier",
        "Math Reasoning",
        "LLMs"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=Qyile3DctL",
    "pdf_link": "https://openreview.net/pdf?id=Qyile3DctL",
    "comments": [
        {
            "comment": {
                "value": "Dear Reviewer FHq1,\n\nThank you for your constructive feedback on our work. We appreciate the opportunity to clarify and enhance our paper based on your suggestions.\n\n---\n\n### Comment:\n> **The approach seems to be applicable generally for reasoning domains, but the authors only test on math reasoning. To what extent can the technique be applied to other domains?**\n\n**Response:**\nWe agree that testing the technique across multiple reasoning domains could strengthen the generalizability claims of our approach. In addition to math reasoning, we have also evaluated our model on code reasoning tasks, which is another critical area within reasoning domains. As shown in Table 2, our approach demonstrates robust performance across both math and code reasoning, suggesting its applicability to diverse reasoning tasks.\n\n---\n\n### Comment:\n> **While there are improvements above the baselines, they vary dramatically between models.**\n\n**Response:**\nOur method is designed to develop stronger verifiers on top of existing reasoners, so the performance can indeed vary depending on the specific reasoner employed. However, when we use a fixed backbone reasoner, our method consistently achieves significant improvements over various baselines. This consistency is demonstrated in Table 3, where using the same reasoner shows reliable performance enhancements, highlighting the robustness of our verifier design.\n\n---\n\n### Comment:\n> **To what extent can the technique be applied to other domains?**\n\n**Response:**\nThank you for this question. The verification approach outlined in our paper has significant potential for broader application in two key domains:\n\n1. Output Selection and Verification:\n\nLLMs face inherent limitations as auto-regressive generators, where they only have limited context during the early stages of generation. When an error or suboptimal strategy is introduced early in the output sequence, it becomes challenging for the model to produce a correct final response. Additionally, due to their stochastic nature, LLMs often generate multiple varied responses to the same input. Techniques like sample-then-rank or best-of-N strategies are therefore commonly used to improve the consistency and accuracy of outputs, as seen in models like GPT-o1. By using a verifier, which has access to the entire generated response upfront, it is possible to provide more precise feedback on sampled candidates, leading to more reliable output selection.\n\n2. Synthetic Data Generation:\n\nAs sourcing high-quality training data from real-world resources becomes increasingly challenging, state-of-the-art models (e.g., LLaMA 3.2) are increasingly relying on synthetic data generated by LLMs. A critical aspect of this approach is maintaining the quality of generated data. The verifier method serves as an effective filtering tool to eliminate low-quality data, thus facilitating a more robust self-learning cycle and enhancing knowledge distillation and transfer for LLMs. By incorporating a verification mechanism, synthetic data quality can be better controlled, making it a valuable component in training pipelines for future LLMs.\n\n---\n\n### Comment:\n> **The paper could benefit from more analysis on what is learned by the verifiers, for example if there are certain types of solutions where the verifiers perform better?**\n> \n> **Are there certain types of programs/solutions where the verifier is better at judging?**\n\n**Response:**\nThank you for this insightful suggestion. Figure 6 in our paper illustrates examples where our verifier effectively detects reasoning errors in solutions, showing that the verifier is sensitive to errors involving incorrect use of numbers, operators, or knowledge points.\n\nWe have also included additional descriptions to explain that while model-based verifiers can detect surface-level logical errors, they are less sensitive to subtle calculation mistakes or deeper logical inconsistencies. For example, the verifier may score incorrect answers highly if they involve only minor calculation errors. As shown in Figure 6, our verifier sometimes assigns a high score to equations like \\(3.5 + 2.5 + 4.5 + 1.5 = 13\\), even though the correct result should be 12. This limitation motivated the development of our CoTnPoT method, which aims to improve detection of these finer-grained mistakes.\n\n**Revision Details:** The above points have been clarified in the updated manuscript on Lines 221\u2013227.\n\n---\n\nWe hope that these revisions and clarifications address your concerns. Thank you once again for your valuable feedback.\n\n\nBest regards,\n\nAuthors of ICLR 5271"
            }
        },
        {
            "comment": {
                "value": "## Part 2 of Our Rebuttal\n\n**Comment:**  \nThere is a notable performance imbalance between CoT reasoning and PoT reasoning.\n\n**Response:**  \nThank you for pointing out this discrepancy. Our method actually benefits from cases where PoT outperforms CoT. In these instances, the PoT solution acts as a strong indicator of correctness, supporting the collaborative verification process.\n\nOur approach primarily focuses on verifying CoT solutions, regardless of whether CoT or PoT performs better on a given task. However, we recognize that PoT can be preferable in certain tasks, and for such cases, we suggest directly utilizing PoT and applying our verification method to code reasoning tasks, such as MBPP, as shown in Table 2.\n\n---\n\n**Comment:**  \nWhat is the performance improvement achieved by fine-tuning the base model on the generated dataset?\n\n**Response:**  \nThank you for this question. We are currently implementing this experiment and will present the results in the rebuttal as soon as they are available.\n\n---\n\n**Comment:**  \nHow does the performance curve change with varying sizes of sampled solutions?\n\n**Response:**  \nWe appreciate this suggestion. We are currently working on adding this performance curve and will include the results in the rebuttal as soon as possible.\n\n---\n\n**Comment:**  \nIt seems that CoTnPoT sacrifices some recall, which may degrade the upper bound of LLM reasoning performance. Are there any potential solutions to address this issue?\n\n**Response:**  \nWe acknowledge that CoTnPoT, by filtering solutions, can lead to a recall reduction. As a potential solution, we propose using CoTnPoT as a scoring mechanism rather than a strict filter. Instead of outright removal, each solution would be assigned a score based on CoTnPoT results (1 for passing, 0 for failing). By integrating these scores with SimPO verifier scores, we can retain more solutions and alleviate the recall degradation.\n\n---\n\nRespectfully,\n\nAuthors of ICLR 5271"
            }
        },
        {
            "comment": {
                "value": "## Part 1 of Our Rebuttal\n\nDear Reviewer AjnZ,\n\nThank you for your thoughtful and detailed feedback. We have addressed each point carefully and believe our responses clarify the novel contributions and effectiveness of our proposed methods. We hope these clarifications and planned updates meet your concerns. \n\n**Comment:**  \nThe technical contributions of this paper are somewhat limited. For example, the introduction of a verifier has already been highlighted; Comparing the CoT reasoning with its formal version have also been explored.\n\n**Response:**  \nThank you for your valuable feedback. We acknowledge the prior work in verifiers within math reasoning. We respectfully clarify that the paper offers two primary contributions with unique insights:\n\n1. **Systematic Comparison of Verifier Training Methods**: We agree that scaling inference-time compute and employing verifiers are existing approaches, yet the field lacks a comprehensive overview of verifier training techniques. Through our dataset and verifier comparison, we conclude that the SimPO method is the most effective approach among current methods, offering valuable insights and benchmarks for future verifier-building efforts.\n   - **Revision**: We have revised the abstract to accurately position this contribution.\n\n2. **Novel CoTnPoT Verification Approach**: We identify a key weakness of LLM-based verifiers: they often overlook subtle calculation errors and inconsistencies in math reasoning and struggle to verify highly abstract and structured code. To address these limitations, we propose a novel method called CoTnPoT, designed to make verification more comprehensive and robust. Our approach fundamentally differs from previous work [1] and [2], as we focus on translating between math and code rather than explicitly verifying language solutions in code format. Translation, in this context, is a more feasible task than direct verification, especially for complex math problems where verification can be challenging. For example, while the major experiments in [1] and [2] used simpler datasets like GSM8k, MultiArith, and MMLU, achieving a performance improvement of 63.69 -> 73.54 on MATH, our method achieves a notable improvement from 59.7 -> 76.9. Additionally, unlike prior studies focused mainly on math reasoning, our approach is effective in both math and code reasoning, demonstrating a broader application potential.\n   - **Revision**: Lines 99-100 and 509-515 clarify the motivation and distinction of our approach, with the contribution statement revised accordingly.\n\n[1] Don\u2019t trust: Verify\u2013grounding llm quantitative reasoning with autoformalization. ICLR 2024.\n\n[2] Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. ICLR 2024.\n\n---\n\n**Comment:**  \nThe improvement of collaborative verification is relatively marginal compared with SimPO.\n\n**Response:**  \nThank you for highlighting this point. We would like to emphasize that while SimPO is indeed effective, CoTnPoT contributes unique strengths that complement the overall verification process, especially in complex tasks.\n\nFor math reasoning, model-based verifiers are proficient at catching surface-level logical errors, such as operator misuse or numerical inaccuracies (see Figure 6). However, they often miss subtler mistakes, like calculation errors or deeper logical inconsistencies. For instance, the verifier frequently assigns a high score to expressions such as $3.5+2.5+4.5+1.5=13$, where the correct solution should yield 12.\n\nAlthough the improvement from SimPO verifier is significant, CoTnPoT addresses a different category of error, providing a more thorough and nuanced approach to detecting LLM errors. By focusing on distinct error types, CoTnPoT enhances the robustness of the verification process and bolsters overall accuracy.\n\n---\n\n**Comment:**  \nSimply merging these models' knowledge, even without verification, could also enhance the reasoning performance of the LLMs.\n\n**Response:**  \nWe appreciate this insight and agree that knowledge distillation from multiple strong models could improve performance. However, our study finds two limitations with this approach:\n\nScaling training resources may lead to performance saturation, which we reference in [3], making it less effective than scaling inference resources.\n\nVerifier training allows weak-to-strong generalization, enabling improved reasoning in weaker models. For instance, our experiments demonstrate that using data from models like Mistral, Phi3, and InternLM2-Math to train SimPO yields improvements in larger models, such as LLaMA-70B and Qwen2-72B, which traditional knowledge distillation cannot achieve.\n\nMost importantly, we see verifier training and knowledge merging as orthogonal approaches that, when combined, could further enhance reasoning performance.\n\n[3] Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters\n\n---\n\n\n(Connect to Part 2)"
            }
        },
        {
            "comment": {
                "value": "Dear Reviewer R5WH,\n\nThank you for your insightful feedback, which has been valuable in refining our work. We look forward to further clarifying and enhancing our study based on our discussion!\n\n---\n\n### Comment 1:\n**\"Have the authors tried to compare their approach with verification based on say 50% of the test cases?\"**\n\n**Our Response:**\n\nWe appreciate this thoughtful question and understand there may be a slight misunderstanding regarding our approach. Our verification process for coding tasks relies solely on the coding problem, the solution, and the CoTnPoT commentary, with no test cases utilized in the verification phase. Instead, test cases are employed only to compute the final performance, as reported in Table 3, in alignment with prior studies on MBPP and MBPP+.\n\nIn our study, we evaluate the pass@1 rate in Table 3 to measure our verifier's ability at Best-of-N, highlighting the verifier\u2019s effectiveness without incorporating any test cases during verification.\n\n---\n\n### Comment 2:\n**\"What is the point of doing CoTnPoT for code? How to do?\"**\n\n**Our Response:**\n\nWe apologize for any lack of clarity in describing CoTnPoT within the context of code reasoning. Here, we aim to clarify both the rationale and implementation for CoTnPoT in coding tasks:\n\n1. **Rationale for CoTnPoT in Code Reasoning**:  \n   As noted in lines 226-229 of our paper, the abstract and structured nature of code often renders it challenging for verifiers to fully understand, leading to similar scores for different solutions. This similarity indicates difficulty in accurately identifying nuanced errors within the code. CoTnPoT aids in mitigating this issue by providing descriptive commentary to enhance interpretability and reduce scoring ambiguities for the verifier.\n\n2. **Implementation of CoTnPoT in Code Reasoning**:  \n   We have revised Figure 2 to better illustrate how CoTnPoT functions across math and coding tasks. Additionally, Section 2.3 will be expanded to distinguish the application of CoTnPoT in these different domains.\n\n   - **CoTnPoT for Math**: We sample multiple CoT solutions ($S_{CoT}$), translate each into PoT format ($S_{PoT}$), filter out any $S_{CoT}$ solutions if their answers don\u2019t match $S_{PoT}$, and subsequently apply an LLM-based verifier on the remaining $S_{CoT}$.\n\n   - **CoTnPoT for Code**: For coding tasks, we first sample multiple PoT solutions ($S_{PoT}$), then write a descriptive commentary ($S_{Des}$) based on $S_{PoT}$. This commentary is concatenated with $S_{PoT}$, providing a richer input for the LLM-based verifier to analyze.\n\n---\n\n### Comment 3:\n**\"Even so, it seems that SimPO is already very strong and CoTnPoT may not be necessary.\"**\n\n**Our Response:**\n\nThank you for highlighting this point. We would like to emphasize that while SimPO is indeed effective, CoTnPoT contributes unique strengths that complement the overall verification process, especially in complex tasks.\n\nFor math reasoning, model-based verifiers are proficient at catching surface-level logical errors, such as operator misuse or numerical inaccuracies (see Figure 6). However, they often miss subtler mistakes, like calculation errors or deeper logical inconsistencies. For instance, the verifier frequently assigns a high score to expressions such as $3.5+2.5+4.5+1.5=13$, where the correct solution should yield 12.\n\nAlthough the improvement from SimPO verifier is significant, CoTnPoT addresses a different category of error, providing a more thorough and nuanced approach to detecting LLM errors. By focusing on distinct error types, CoTnPoT enhances the robustness of the verification process and bolsters overall accuracy.\n\n---\n\n### Comment 4:\n**\"Same as B3, CoTnPoT should be compared with Best-of-2N, from the perspective of computational resource.\"**\n\n**Our Response:**\n\nWe appreciate the reviewer\u2019s suggestion regarding computational resources, and we acknowledge the importance of fair comparison. In response, we have initiated additional experiments to compare CoTnPoT with Best-of-2N and will share these results in our final rebuttal submission.\n\nBest,\n\nAuthors of ICLR 5271"
            }
        },
        {
            "comment": {
                "value": "# Part 2 of our Rebuttal\n\n### Comment:\n> _\"Grammar issues on line 219 (preference-tuning) and line 128 ('surpassing the performance of existing verifiers').\"_\n\n**Response**: We have corrected these grammatical issues:\n- Line 219: Updated \u201creference-tuning\u201d to \u201cpreference-tuning.\u201d\n- Line 128: Revised wording to improve clarity.\n\n---\n\n### Comment:\n> _\"Inaccurate terminology: 'recall rate' in Figure 1 should be 'Pass@64'; 'sample-then-rank' should be 'best-of-N.'\"_\n\n**Response**: We appreciate this feedback and have revised terminology for clarity:\n   - **Revisions**: Lines 60-61 and 75-76 now use the terms \u201cPass@64\u201d and \u201cbest-of-N,\u201d respectively.\n\n---\n\n### Comment:\n> _\"Why was MAmmoTH-7B-plus used for math reasoning verification but not in previous sections?\"_\n\n**Response**: We chose MAmmoTH-7B-plus for two reasons:\n1. Enhanced performance with an upgraded model.\n2. Demonstrates the generalizability of the training method across different models. We acknowledge that this could be confusing but believe it does not affect the paper\u2019s conclusions.\n   - **Revision**: We expanded this explanation in Lines 370-374.\n\n---\n\n### Comment:\n> _\"Figure 2 is misleading in its comparison.\"_\n\n**Response**: We acknowledge that Figure 2 could cause misunderstanding. To improve clarity, we have removed Figure 2, and we emphasize the clearer comparisons in Table 3.\n   - **Revision**: Removed Figure 2, referencing clearer comparisons in Table 3.\n\n---\n\n### Comment:\n> _\"Contradiction in reducing reliance on external LLMs but using them for math reasoning.\"_\n\n**Response**: Apologies for the confusion. Our aim is to minimize reliance on external models in coding verification by using the same LLMs to write comments. However, many math models lack the capability to translate CoT to PoT, necessitating the use of external LLMs.\n   - **Revision**: Further clarified in Lines 246-247.\n\n---\n\n### Comment:\n> _\"Inconsistent number of candidate solutions in Figure 4.\"_\n\n**Response**: We accept this suggestion and have standardized the number of candidate solutions in Figure 4.\n   - **Revision**: Updated Figure 4 to show consistent testing with 100 candidate solutions.\n\n---\n\n### Comment:\n> _\"Generalizability of the verifier, line 82.\"_\n\n**Response**: Previous verifiers were often trained on outputs from a single model, making them specific to particular solution formats. In contrast, our verifiers are trained on diverse backbones, enhancing generalizability. We realize that our original statement may have caused confusion, so we have removed it for clarity.\n   - **Revision**: Adjustments made on Lines 81-83.\n\n---\n\nBest regards,\n\nAuthors of ICLR 5271"
            }
        },
        {
            "comment": {
                "value": "Dear Reviewers,\n\nWe sincerely thank each reviewer for the thoughtful and constructive feedback, as well as the time and effort invested in evaluating our paper. Your insights have been invaluable in helping us refine and improve our work. Through your questions and comments, we gained a deeper understanding of areas for enhancement, which has allowed us to make meaningful revisions that we believe have strengthened our paper considerably.\n\nWe respectfully invite you to review our itemized responses and would be most grateful if you could share any further questions or concerns in the comments. We are fully committed to addressing each point and provide additional clarifications if needed. If our responses have satisfactorily resolved your initial concerns, we would greatly appreciate it if you might consider an update to the evaluation score.\n\nThank you once again for your valuable feedback and guidance, which have been instrumental in enhancing the quality of our work.\n\nBest Regards,\nAuthors of ICLR 5271"
            }
        },
        {
            "comment": {
                "value": "# Part 1 of our Rebuttal\n\nDear Reviewer dNMD,\n\nThank you very much for the detailed and thoughtful review, which has been invaluable in improving our work. We are fully committed to addressing each of your concerns and clarifying any areas that may have lacked precision in our initial submission. Should you have any further questions or additional concerns, please feel free to reach out to us. We would be more than happy to engage in further discussion to ensure our contributions are presented as clearly and accurately as possible. Our responses:\n\n---\n\n### Comment: \n> _\"The paper lacks significant novelty; the primary differences lie in the volume of data, variety of sources, and minor variations in training.\"_\n\n**Response**: We acknowledge that certain statements may have overclaimed novelty in our draft. However, we respectfully clarify that the paper offers two primary contributions with unique insights:\n\n1. **Systematic Comparison of Verifier Training Methods**: We agree that scaling inference-time compute and employing verifiers are existing approaches, yet the field lacks a comprehensive overview of verifier training techniques. Through our dataset and verifier comparison, we conclude that the SimPO method is the most effective approach among current methods, offering valuable insights and benchmarks for future verifier-building efforts.\n   - **Revision**: We have revised the abstract to accurately position this contribution.\n\n2. **Novel CoTnPoT Verification Approach**: We identify a key weakness of LLM-based verifiers: they often overlook subtle calculation errors and inconsistencies in math reasoning and struggle to verify highly abstract and structured code. To address these limitations, we propose a novel method called CoTnPoT, designed to make verification more comprehensive and robust. Our approach fundamentally differs from previous work [1] and [2], as we focus on translating between math and code rather than explicitly verifying language solutions in code format. Translation, in this context, is a more feasible task than direct verification, especially for complex math problems where verification can be challenging. For example, while the major experiments in [1] and [2] used simpler datasets like GSM8k, MultiArith, and MMLU, achieving a performance improvement of 63.69 -> 73.54 on MATH, our method achieves a notable improvement from 59.7 -> 76.9. Additionally, unlike prior studies focused mainly on math reasoning, our approach is effective in both math and code reasoning, demonstrating a broader application potential.\n   - **Revision**: Lines 99-100 and 509-515 clarify the motivation and distinction of our approach, with the contribution statement revised accordingly.\n\n[1] Don\u2019t trust: Verify\u2013grounding llm quantitative reasoning with autoformalization. ICLR 2024.\n\n[2] Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. ICLR 2024.\n\n---\n\n### Comment:\n> _\"Overclaimed contributions, such as 'To address this, we scale up inference-time computation\u2026' and 'We develop inference-time verifiers to enhance the reasoning capabilities of LLMs.'\"_\n\n**Response**: We acknowledge these inaccuracies and have revised the statements to reflect our approach more accurately:\n1. Revised to: \u201cTo address this, we adopt a widely used method to scale up inference-time computation by generating multiple reasoning paths and employing verifiers to assess and rank the generated outputs by correctness.\u201d (Lines 17-18)\n2. This item has been removed from the contributions list.\n\n---\n\n### Comment:\n> _\"CoTnPoT should be compared with Best-of-2N to ensure fair use of computational resources.\"_\n\n**Response**: We appreciate this suggestion. We are currently conducting additional experiments to compare CoTnPoT with the Best-of-2N approach. Results will be included in the rebuttal once the experiments are complete.\n\n---\n\n### Comment:\n> _\"Clarify the distinction between CoTnPoT in math vs. coding tasks.\"_\n\n**Response**: We have clarified this distinction by updating Figure 2 and providing a detailed description in Section 2.3.\n\n- **CoTnPoT for Math**: Generate multiple CoT solutions \\( S_{CoT} \\), translate each to PoT solutions \\( S_{PoT} \\), and filter out \\( S_{CoT} \\) if its answer does not match \\( S_{PoT} \\), followed by LLM-based verification.\n- **CoTnPoT for Code**: Generate multiple PoT solutions \\( S_{PoT} \\), generate comments \\( S_{Des} \\) based on \\( S_{PoT} \\), concatenate \\( S_{PoT} \\) and \\( S_{Des} \\), and apply LLM-based verification.\n\n   - **Revision**: Figure 2 and Lines 256-274 have been updated to clarify this distinction.\n\n---\n\n\n(connect to Part 2)"
            }
        },
        {
            "summary": {
                "value": "The paper provides a dataset of correct and incorrect responses for mathematical and coding problems, which are generated by multiple large language models (LLMs). This dataset is used to train verifiers to judge the correctness of candidate solutions from models at test-time. The verification is done on both chain-of-thought (CoT) and program-of-though (PoT) candidate solutions for a problem. The trained verifiers for math and coding tasks show improvements on GSM8K, MATH500 and MBPP."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The compilation of a large dataset containing both correct and incorrect candidate solutions from multiple LLMs, covering math and coding tasks, offers a valuable resource for training solution verifiers.\n\nThe CoTnPoT verification strategy improves accuracy by leveraging the complementary strengths and synergies of both CoT and PoT formats, incorporating insights that each provides.\n\nThe paper effectively explores a range of verifier training methods, including outcome-reward models (ORM) classifiers and various preference optimization techniques. The experiments, conducted on several models, provide valuable insights into the effectiveness of these approaches and their impact on verification performance.\n\nThe authors perform rigorous verification experiments with several generators, including weighted majority voting and thorough hyper-parameter search."
            },
            "weaknesses": {
                "value": "The paper lacks significant novelty; the primary differences lie in the volume of data, the variety of sources, and minor variations in training (and verification with both CoT and PoT candidates). The tone in the abstract and introduction suggests that the approach is a bit more groundbreaking than it actually is, particularly regarding verification and inference-time compute scaling. For instance, the paper states \"To address this, we scale up the inference-time computation by generating multiple reasoning paths and employing verifiers to assess and rank the generated outputs by correctness.\" However, this approach is already an established direction in the literature. It would be more appropriate to position this as a current method for scaling inference-time compute, rather than framing it as a novel solution.\n\nAnother instance of this is \u201cWe develop inference-time verifiers to enhance the reasoning capabilities of LLMs\u201d. This item may not be suitable for inclusion in the contribution list, as similar work has already been completed in the past.\n\nThe CoTnPoT method relies on PoT generation from an external LLM, but such code-based solutions may not always be feasible or reliable. Additionally, if the proposed method generates K more (code) solutions per query, it would be appropriate/fair to compare other methods with a best-of-2K budget to ensure a fair comparison. Similarly, for code problems, there are extra description tokens generated by the language model. For a fair comparison, other methods should have access to a similar budget for generation of candidate solutions.\n\nThe distinction between the CoTnPoT approach for math tasks and coding tasks could be clarified further in the paper. For example, Figure 3 explains the method only in the context of math reasoning problems, but it is unclear how this approach adapts to coding tasks, especially given that there is no final answer in coding problems. \n\nThere are some grammatical or dictation errors in the paper. For instance, on line 219, I believe the authors meant to write preference-tuning instead of reference-tuning. \n\nThe phrase \"and surpassing the performance of existing verifiers\" in line 128 seems grammatically awkward and could be omitted without losing meaning."
            },
            "questions": {
                "value": "Here are some of my questions and comments other than the ones mentioned above: \n\nIf i understand correctly \"recall rate\" in Figure 1 is basically Pass@64. If that\u2019s the case, it would be clearer to simply use the more widely recognized Pass@64 term.\n\nSimilarly, the \"sample-then-rank\" approach could be replaced with the more widely used \"best-of-N\" sampling strategy.\n\nIn section 3.2 it is mentioned that \"We upgraded the backbone model of our verifier in math reasoning\nfrom Mistral-7B to MAmmoTH-7B-plus to enhance performance.\". Why not use MAmmoTH-7B-plus for the previous sections of the paper?\n\nFigure 2 is misleading, one should compare Maj@k of models (without verifiers) with verification-based approaches (models with verifiers). I don't believe this is mentioned in the paper.\n\nLine 256 states \"Because using the same LLMs for both code and description generation reduces over-reliance on external LLMs\". However, for math reasoning problems, the proposed method is in fact using and relying on external LLMs for generation of $S_{PoT}$. Doesn't this seem contradictory?\n\nIt would have been better if the number of candidate solutions $k$ in experiments for performance of different verifiers was consistent. Figure 4 shows some models have been tested with 100 and some with 64 candidate solutions.\n\nLine 82 states \"In other words, these verifiers are not generally applicable across different tasks and backbone reasoners\". However, the proposed method and trained verifiers are also based on a few backbones. How is the proposed verifier more general?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a number of contributions to improve LLM reasoning:\n- First, they create a dataset of math and code reasoning tasks used to train verifiers\n- Next, they train various verifiers and compare their performance\n- Finally, they use these verifiers to improve performance on math tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The method is simple and effective, while providing improvements above the baseline model.\n- There is good analysis on the success of the verifier (Table 4), which provides evidence for why the verifier \n- The paper provides relevant ablations (Section 3.4) that highlight the effectiveness of various components of their approach."
            },
            "weaknesses": {
                "value": "- The approach seems to be applicable generally for reasoning domains, but the authors only test on math reasoning domains. The paper would benefit from an assessment on another domain as well. \n- While there are improvements above the baselines, they vary dramatically between models\n- The paper could benefit from more analysis on what is learned by the verifiers, for example if there are certain types of solutions where the verifiers perform better?"
            },
            "questions": {
                "value": "- To what extent can the technique be applied to other domains? \n- Are there certain types of programs/solutions where the verifier is better at judging?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes two ways to improve LLM reasoning performance via verification. The first way is to train reward models / verifiers on correct and incorrect solutions. The reward models can either be trained with simple outcome-based or preference based. The second way is to filter solutions when the COT solution and PoT solution answer do not disagree with each other. The paper shows that by using these verification methods, the performance can be improved considerably on math and coding datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper tackles an important topic, improving LLM reasoning, through verification based approach. The motivation makes sense and the authors study different options for training the verifiers. The filtering of comparing if CoT and PoT agree is also reasonable to do."
            },
            "weaknesses": {
                "value": "There are a few concerns I have for the proposed approach and settings.\n\n1. For code reasoning, many existing work assumes we have access to some if not all test cases for the problem. Because of this, pass@k is typically used to evaluate model performance. I wonder if the verification setting for code reasoning makes sense. Have the authors tried to compare their approach with verification based on say 50% of the test cases?\n\n2. For code reasoning, what is the point of doing CoTnPoT if the problem we are solving is already a coding problem? Also I generally have a hard time understanding equation (2) and the text around it. The LHS is S_Des while the text mentions \"we concatenate the description and the code as an integrated input for the verifier...\"\n\n3. While CoTnPoT is nice to help and always seems to help, it essentially doubles the amount of inference time compute which makes verification much heavier now (see Equation 1). For a fair comparison, CoTnPoT should be compared against the baseline of sampling twice the number of solutions as before. Even so, it seems that SimPO is already very strong (see Table 1) and CoTnPoT may not be necessary."
            },
            "questions": {
                "value": "Please see Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a new method for verifying LLM reasoning by assessing the consistency between CoT and PoT solutions.The proposed approach filters each CoT solution, ensuring alignment with its corresponding PoT version. Additionally, a trained verifier is employed to evaluate and select from the filtered CoT solutions. Experimental results demonstrate that this method achieves state-of-the-art performance on both GSM8k and MATH benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed verification framework, which combines CoT and PoT, is very intuitive and promising. \n2. Some empirical results are resource-intensive in terms of time and GPU usage, but could be valuable to the research community. \n3. The performance of the proposed method appears very good and achieves state-of-the-art performance on GSM8K and MATH.\n4. The authors have committed to releasing both the code and data, which will be beneficial for reproducibility and further research."
            },
            "weaknesses": {
                "value": "1. The technical contributions of this paper are somewhat limited. For example,  the introduction of a verifier has already been highlighted in [1]; Comparing the CoT reasoning with its formal version have also been explored in [2, 3]. \n2. It is challenging to assess the actual effectiveness improvements resulting from the proposed method. The improvement of collaborative verfication is relatively marginal compared with the SimPO (see Table 1). However, as to the data generation for the verifier, six models (Mistral, Phi3, InternLM2-Math, MAmmoTH2-plus, LLaMA-3-8B, and GPT-4o) are involved. Simply merging these models' knowledge, even without verification, could also enhance the reasoning performance of the LLMs.\n3. There is a notable performance imbalance between CoT reasoning and PoT reasoning.This discrepancy may hinder the application of the proposed method in more difficult benchmarks, e.g., AI Mathematical Olympiad. Particularly, CoT exhibits significantly lower accuracy compared to PoT, which could undermine the effectiveness of collaborative verification.\n\n[1] Let\u2019s verify step by step. ICLR 2024.\n\n[2] Don\u2019t trust: Verify\u2013grounding llm quantitative reasoning with autoformalization. ICLR 2024.\n\n[3] Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. ICLR 2024."
            },
            "questions": {
                "value": "Please also refer to the weakness section.\n\n1. What is the performance improvement achieved by fine-tuning the base model on the generated dataset?\n1. How does the performance curve change with varying sizes of sampled solutions?\n1. It seems that CoTnPoT sacrifices some recall, which may degrade the upper bound of LLM reasoning performance. Are there any potential solutions to address this issue?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}