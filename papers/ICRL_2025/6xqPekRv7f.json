{
    "id": "6xqPekRv7f",
    "title": "Understanding and Mitigating Gender Bias in LLMs via Interpretable Model Editing",
    "abstract": "Large language models (LLMs) have achieved great success in various tasks. While LLMs can learn powerful capabilities from large datasets, they also inherit the gender bias present in that data. Existing studies usually propose methods to reduce bias by data cleaning and model retraining/fine-tuning. Although these methods have shown some success, the cost of designing data and retraining/fine-tuning an LLM increases significantly as the model size grows larger. Furthermore, a lack of understanding of the mechanisms behind gender bias prevents researchers from effectively tailoring solutions to address it. In this paper, we utilize mechanistic interpretability methods to construct the neuron circuits for gender bias cases and locate the important neurons storing gender bias. Then we propose the Interpretable Model Editing (Interpret-ME) method to reduce gender bias without designing huge datasets or fine-tuning. Compared to fine-tuning methods, our approach shows competitive results in reducing gender bias across experiments with 8 LLMs. At the same time, our method does not affect the performance in other tasks. Overall, our analysis is useful for understanding the mechanism of gender bias and our method paves a potential way for reducing bias.",
    "keywords": [
        "large language models",
        "gender bias",
        "mechanistic interpretability",
        "model editing"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We propose the neuron-level interpretable model editing method to reduce the gender bias in LLM, without hurting the LLM's existing abilities.",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6xqPekRv7f",
    "pdf_link": "https://openreview.net/pdf?id=6xqPekRv7f",
    "comments": [
        {
            "title": {
                "value": "response to reviewer wmNw"
            },
            "comment": {
                "value": "Thank you very much for your valuable comments. We thank you for understanding the strengths of our work regarding the analysis of neuron-level information flow and the neuron-level model editing.\n\nRegarding the weaknesses, here are our responses:\n\nQ: The located neurons may not be sufficiently representative. Since only five sentences per gender are used to locate neurons, these sentences may not adequately capture real-world gender stereotypes. Moreover, there is no experiment in the paper that demonstrates whether using more or fewer sentences would affect the performance of the Interpret-ME method.\n\nA: We agree that the located neurons may not be sufficiently representative. However, we think that this is one of the advantages of our work: we do not need to analyze all the probable sentences and identify all the important neurons. Only editing the neurons in 10 sentences can reduce the gender bias very much. All the gender bias datasets in Section 4 are general gender bias datasets, rather than designed datasets. Different gender bias cases are contained in these datasets. The experimental results in Ssection 4 can prove that the identified neurons are important for these datasets and these different sentences. When zero-editing the neurons, the gender bias is reduced much.\n\nWe will add the experiments about the number of the sentences. We conducted the experiments on Llama-7B with number 1,2,5,10,20,30,40 for each gender. The metrics increase sharply from 1 to 5, increase slowly from 5 to 20, and starts to decrease from 20. \n\nQ: Why is Table 5 not a comparison between the Interpret-ME method, fine-tuning methods, and the original model? Why does Table 5 not include a comparison between the Interpret-ME method, fine-tuning methods, and the original model? I would like to know whether Interpret-ME causes less performance drop compared to fine-tuning methods.\n\nA: We cannot get the model parameters of the fine-tuned models in [1]. Therefore, we cannot conduct the experiments to compare the results.  \n\n[1]  A trip towards fairness: Bias and de-biasing in large language models"
            }
        },
        {
            "title": {
                "value": "response to reviewer W11j"
            },
            "comment": {
                "value": "Thank you very much for your valuable comments. And thank you for understanding the strengths about our work, regarding the importance of the task, the correctness and effectiveness of our proposed method.\n\nQ: My major concern is that the models studied by the authors may be outdated. With the continuous development of LLMs, more and more drawbacks of them vanished. It\u2019s unclear whether recent LLMs suffer from such an issue and whether the proposed method could generalize to recent LLMs. I suggest more experiments to clarify this point. Otherwise, the contribution of this work will be vague or limited.\n\nA: We understand that your main concern is: is this work still useful? If the gender bias is reduced in better LLMs, it may be useless to study reducing the gender bias. Here are our responses: \n\nFirstly, gender bias is obtained by LLMs during pre-training. [1] find that the co-occurrence of word pairs is important for LLMs to predict the answers. [2] find that LLMs learn the associate certain professions with specific genders based on gender stereotypes, but these data\u2019s distributions are not \u201cwrong\u201d for the model. For example, if the co-occurrence of \u201cman\u201d and \u201cguard\u201d is larger than that of \u201cwoman\u201d and \u201cguard\u201d, the output of \u201cThe guard is a\u201d is more likely to be \u201cman\u201d than \u201cwoman\u201d. Under this mechanism, it is very hard to reduce the gender bias during pre-training. Experimentally, [3] find that the gender bias is the hardest to be reduced compared with other bias (e.g. racism).\n\nSecondly, most current methods try to reduce the gender bias during SFT or RLHF. However, [4] find that the capabilities learned from pre-training are not removed, but rather bypassed. In other words, the parameters storing toxicity/bias still exist in the fine-tuned model. When LLMs get unseen inputs or designed prompts, they will still generate toxicity/bias outputs.\n\nBased on the mechanism analysis, gender bias cannot be reduced during fine-tuning or SFT/RLHF. This is the reason why we use model editing method to solve this problem. The gender bias parameters are stored in neurons, and we can reduce the gender bias by zero-editing these neurons.\n\nBesides, we do the same experiments with Table 4 on Llama 3.1-8B. The lms/ss/nss/icat scores of Llama 3.1 are 94.54/68.81/62.37/58.97. The performance is even worse than Llama-7B. This means that Llama-3.1-8B still has gender bias, and it is not better than Llama-7B. This conclusion is similar to that of [3]: gender bias is not reduced in LLMs with better abilities.\n\nCan our method be utilized in Llama-3.1-8B? The answer is yes. We apply our method on Llama-3.1-8B and the lms/ss/nss/icat scores are 94.63/67.93/64.13/60.69, which is better than that of the original model.\n\nQ: It seems that the activated neurons vary across different prompts, and gender bias is often implicitly represented by models. Is the women-and-man (or specific-prompt) setting generalizable and convincing enough to arrive at the research conclusions? Are the adopted settings representative enough?\n\nA: Actually, the activated neurons are similar under different prompts. The results of Table 1, 2, and 3 can prove that the sentences with different genders can activate the same neurons. Only the coefficient scores of the neurons are different. When we zero-edit these neurons, the gender bias is reduced. Furthermore, all the datasets in Section 4 are general gender bias datasets contain different prompts and cases (e.g. personality traits). The experimental results can also prove that our method is not only useful for the designed prompts, but also useful for different gender bias sentences.\n\nQ: Do recent LLMs suffer from gender bias issues (e.g., o1, GPT-4o, GPT-4, GPT-3.5-turbo, LLaMa 3.1, LLaMa 3.2)?\n\nA: First, it is important to note that our work is a mechanistic interpretability work, which needs to locate and edit the important neurons. Compared with previous works which regard LLMs as blackboxes, it is much harder to analyze the inner mechanism in LLMs and leverage the inpretability findings to solve real tasks. In previous mechanistic interpretability works, the experiments are usually conducted on small models such as GPT2 [5,6]. It is a breakthrough to do the neuron-level model editing in LLMs. \n\nBecause our work needs to locate and edit the inner parameters, we cannot do experiments on the close-source LLMs (e.g. o1, GPT-4o, GPT-4, GPT-3.5-turbo). But our experiments on Llama-3.1-8B can prove that the gender bias is still not reduced.\n\n[1] Impact of Co-occurrence on Factual Knowledge of Large Language Models\n\n[2] Gender Bias and Stereotypes in Large Language Models\n\n[3] A trip towards fairness: Bias and de-biasing in large language models\n\n[4] A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity\n\n[5] Locating and editing factual associations in GPT\n\n[6] How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"
            }
        },
        {
            "title": {
                "value": "response to reviewer iAJp"
            },
            "comment": {
                "value": "Thank you very much for your valuable comments. We thank you for understanding our work\u2019s strengths regarding crucial issue, good experiments and good interpretability.\n\nHere are our responses regarding the weaknesses.\n\nQ: The writing is difficult to follow ... the authors omit essential background on the key concept of the unembedding space, which is central to understanding the proposed methodology.\n\nA: Our work is a mechanistic interperetability work exploring the mechanism of gender bias. In the field of mechanistic interpretability, projecting the internal hidden states and neurons in unembedding space is a commonly-used method [1,2,3,4,5] to analyze the interpretability of different parameters. In some recent mechanistic interpertability works [6], these backgrounds are considered as common sense and even not introduced any more. In comparison, we introduce it in lines 188-193, and Section 2.2. The audience can understand the background well from these lines. \n\nQ: Figures and tables ... (e.g., page 5).\n\nA: We will modify the format.\n\nQ: The method relies heavily on existing interpretability frameworks, leading to limited innovation and making the contributions feel incremental.\n\nA: The understanding about how LLMs work will not happen overnight. Therefore, using the existing interpretability frameworks is essential. However, we do not agree that our work has limited innovation and incremental contributions. First, regarding the mechanistic interpretability of gender bias, the neuron-level information flow about gender bias is not explored. Most works aim to evaluate the gender bias by reducing the metrics on gender bias datasets, but the mechanism is not explored much. In other words, the neuron-level information flow of gender bias is still not clear. In our work, we study the neuron-level information flow of gender bias, which provides a deep understanding. \n\nQ: The idea of addressing bias in LLMs by identifying and editing specific neurons is not new, and similar approaches have been explored before. The authors do not adequately acknowledge this and fail to distinguish their work from related studies like those by Chintam et al. (2023) and Lutz et al. (2024)...\n\nA: We will introduce the mentioned works in Section 2. Our work aims to understand and reduce the gender bias in large language models. But these works cannot be applied in LLMs, which do not fit our work's situations. The methods in the mentioned works can only be applied in small models like GPT-2 and BERT due to the computational costs. They apply the causal-based methods and circuit discovery methods to identify the important parameters. However, these methods cannot be applied at neuron-level in large language models, because there are too many neurons in LLMs. Take Llama-7B as an example, there are 483,328 neurons in the model. If using causal-based methods, the forward computation stage for identifying the important neurosn should be done 483,328 times.\n\nQ: The study's use of bias-evaluation datasets, such as CrowS-Pairs and StereoSet, which have been criticized for noise and reliability issues (Blodgett et al., 2021), raises concerns about the robustness of the evaluation.\n\nA: We choose the same datasets with the most popular LLMs. These datasets are commonly used to evaluate LLMs. For example, CrowS-Pairs is used in Llama and GLM. And StereoSet is used in GPT3 and GLM.\n\nQ: Could you provide a clearer explanation of how the hypotheses in Section 3.3 were derived from the previous analyses?\n\nA: In FFN layers and attention heads, the subvalues (fc2 in Eq.5) are the same in all the cases. The changing thing is the coefficient score of each neuron (m in Eq.5). In Table 1 and Table 2, if identified neurons\u2019 top tokens are related to \u201cman\u201d, their last tokens are related to \u201cwoman\u201d. If these neurons\u2019 top tokens are related to \u201cwoman\u201d, their last tokens are related to \u201cman\u201d. Therefore, the neurons store the gender bias. When the coefficient scores are larger than zero, the top tokens\u2019 probability increase and the last tokens\u2019 probability decrease. When the coefficient scores are smaller than zero, the top tokens\u2019 probability decrease and the last tokens\u2019 probability increase. Furthermore, in Table 3 we can see that the sign of the coefficient scores on the same neuron is different under different genders, which matches our analysis.\n\n[1] interpreting GPT: the logit lens, 2020\n\n[2] Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space,  2022\n\n[3] Analyzing Transformers in Embedding Space, 2022\n\n[4] Future Lens: Anticipating Subsequent Tokens from a Single Hidden State, 2023\n\n[5] Neuron-Level Knowledge Attribution in Large Language Models, 2024\n\n[6] Dissecting Recall of Factual Associations in Auto-Regressive Language Models, 2023"
            }
        },
        {
            "title": {
                "value": "response to reviewer rUmF"
            },
            "comment": {
                "value": "Thank you very much for your valuable feedbacks. We thank you for understanding our work\u2019s strengths about good representation, proper method, and thorough experiments.\n\nHere are our responses regarding the weeknesses.\n\nQ: My only concern is that from the experiments, it seems that in order for Interpret-ME to not hurt models\u2019 performance on other tasks, it requires very delicate hyper-parameter search. Therefore I\u2019m not convinced that compared to fine-tuning approaches, the proposed approach is more beneficial from the perspective of maintaining LLM\u2019s existing capability. More experiment designs and results along this line would be very helpful.\n\nA: Firstly, our method does not require the \u201cvery delicate hyper-parameter search\u201d. The interpretability analysis in Section 3.2 and the experiments in Section 4.3 is an ablation study to understand the roles about different neurons. In model editing stage, the neuron selection is done automatically by calculating the neurons\u2019 top tokens in unembedding space, which are introduced in Section 3.3, lines 278-279. \n\nSecondly, our method has the following advantages compared with fine-tuning. a) Our method achieves better performance in Table 4 and Table 5. b) Our method does not require much data. We only needs 10 cases to identify the neurons, and the results are good on all the 8 models. c) Our method is much faster. The neuron selection stage only takes 20-30 seconds for one case. \n\nThirdly, previous methods have proved that fine-tuning cannot solve the gender bias problem. As we introduced in lines 38-51 in Section 1, gender bias is not reduced much during fine-tuning [1]. [2] investigates that the fine-tuning data for reducing gender bias can bring factuality errors and potential risks. [3] point out that in-training methods risk corrupting the pre-trained language understanding due to catastrophic forgetting. Based on these problems, using other methods rather than fine-tuning is essential. In this work, we design the interpretable model editing method, and the experimental results are good. \n\nFourthly, previous interpretability research [4] explore the mechanism of toxicity and find that capabilities learned from pre-training are not removed, but rather bypassed. In other words, the parameters storing the toxicity still exist in the fine-tuned model. Undering prompts that are unseen in the fine-tuning data, these parameters can still be activated and causing the toxicity sentences. The situation in gender bias is similar. Our interpretable model editing method is a good way to solve this problem. We locate the important paramters and delete them. Since different gender bias cases activate similar gender bias neurons, the gender bias will not be activated by unseen sentences. And this is why our method can achieve good results when we only use 10 gender bias cases to locate the neurons.\n\nIn conclusion, according to previous studies, fine-tuning is not enough for reducing gender bias. Our interpretability analysis explores the mechanism of gender bias, and reduce the gender bias by editing the important neurons. Compared with fine-tuning methods, our method is faster because the neruon-selected stage of our method is automatic, and our method requires much less data than fine-tuning. \n\n[1] A trip towards fairness: Bias and de-biasing in large language models\n\n[2] Language generation models can cause harm: So what can we do about it? an actionable survey\n\n[3] Bias and fairness in large language models: A survey\n\n[4] A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity"
            }
        },
        {
            "title": {
                "value": "response to Reviewer jVvc"
            },
            "comment": {
                "value": "Thank you very much for your valuable feedback. First, we appreciate your recognition of the strengths of our work. Our method requires minimal data and operates at a fast speed. Moreover, it preserves the model's original capabilities while effectively reducing gender bias. Regarding the weaknesses, here are our responses:\n\nQ: Some notations can be more clear. For example, B and d in section 3.1. \n\nA: In Section 3.1, we introduce in lines 152-154 that B is the number of the tokens in unembedding space. D is the dimension of each word embedding. We follow the introduction of recent publications [1,2]. Does the introduction of B-dimension vector (line 153) make confusion? \n\nQ: The method does not compare changes in entropy difference on WinoG/CPairs with fine-tuning. Without this comparison, it is unclear if Interpret-ME is as effective as or better than fine-tuning in terms of reducing bias on these datasets.\n\nA: The results of the fine-tuning method comes from [3], the state-of-the-art fine-tuning method for reducing gender bias. They only do the experiments on StereoSet, and we cannot get the parameters of the fine-tuned models to compare the other experiments with them.\n\nQ: It remains unclear whether different gender-biased sentences activate the same neurons or if varying sentences affect the method's results. This uncertainty suggests that the method might not generalize well to a broad range of gender-biased language, potentially impacting its consistency and reliability across diverse examples.\n\nA: The analysis in Section 3.2 can prove that: sentences with different genders can activate the same neurons. Take Table 2 and 3 as an example. When projecting the neurons into unembedding space [4,5] to analyze the neurons\u2019 interpretability, the neurons\u2019 top tokens are related to \u201cman\u201d and the last tokens are related to \u201cwoman\u201d. This means that the neurons save the gender bias. If the neuron\u2019s coefficient score is larger than zero, it is useful for increasing the probability of \u201cman\u201d and decreasing the probability of \u201cwoman\u201d. If the coefficient score is smaller than zero, it is useful for decreasing the probability of \u201cman\u201d and increasing the probability of \u201cwoman\u201d.\n\nFurthermore, our experiments are done in the commonly used gender bias datasets. These datasets contain not only the profession cases, but also other gender-bias sentences. The results in Table 4 and Table 5 can prove that our method can achieve good results even if we do not analyze all the gender-bias sentences\u2019 neurons. And this result indicates that different gender-bias sentences can activate similar neurons. It is an advantage of our method that we do not need to locate the neurons of all the gender-bias sentences. As a result, our method can be done with very small number of sentences and very fast speed. \n\nQ: Will different types of gender-biased sentences activate distinct important neurons? The selected sentences focus on professions. If sentences featuring other gender-stereotyped topics, such as personality traits or colors, are used, would we observe similar results?\n\nA: The gender bias datasets contain other gender-stereotyped topics, such as personality traits or colors. The experimental results on these datasets can prove that the bias is reduced. When only use 10 sentences, the result is already good, indicating that this is a promising way to reduce the gender bias.\n\nBut we agree that doing analysis on other cases can help understand the mechanism better. We analyze the cases \"The strong person is a\" -> \"man\", \"The beautiful person is a\" -> \"woman\". We find that most of the activated neurons are the same with the important neurons identified by the professions. For instance, Table 1's attention neuron $A^{18,7}_{54}$'s coefficient score is 0.5281 in \"The beautiful person is a\" -> \"woman\". This is not surprising because the interpretability results can prove that this neuron contains the information about increasing \"woman\" probability and decreasing \"woman\" probability when the coefficient score is larger than zero.\n\nQ: What happens if the sentence is changed to \"This woman is a ==> nurse\"?\n\nA: In this case, the shallow FFN neurons and the attention neurons are similar. For example, the shallow FFN neuron $F_{2026}^4$ is activated (coefficient: 0.0108) by the word \"woman\", and added into this position's residual stream. Then the attention neuron $A^{18,7}_{83}$ (coefficient: 0.0361) is activated by the woman position's hidden states. The deep FFN neurons are different, because these identified neurons contain the information about the word \"nurse\", rather than \"woman\".\n\n[1] Locating and Editing Factual Associations in GPT\n\n[2] Dissecting Recall of Factual Associations in Auto-Regressive Language Models\n\n[3] A trip towards fairness: Bias and de-biasing in large language models\n\n[4] Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space\n\n[5] Analyzing Transformers in Embedding Space"
            }
        },
        {
            "summary": {
                "value": "This paper explores gender bias in large language models (LLMs) and highlights challenges with current methods, like data cleaning and fine-tuning, which become costly as models get larger. The authors use interpretability tools to identify specific neurons linked to gender bias and introduce a new method, Interpretable Model Editing (Interpret-ME), to reduce this bias without needing extensive retraining."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The method does not rely on a large amount of data, making it more practical and cost-effective compared to approaches that require extensive datasets for bias mitigation.\n\n2. Since the method does not require fine-tuning the entire model, it saves substantial computational resources and time, especially for large language models.\n\n3. The method has minimal impact on performance across common datasets, ensuring that the model\u2019s general abilities remain intact while reducing gender bias."
            },
            "weaknesses": {
                "value": "1. Some notations can be more clear. For example, B and d in section 3.1.\n\n2. The method does not compare changes in entropy difference on WinoG/CPairs with fine-tuning. Without this comparison, it is unclear if Interpret-ME is as effective as or better than fine-tuning in terms of reducing bias on these datasets.\n\n3. It remains unclear whether different gender-biased sentences activate the same neurons or if varying sentences affect the method's results. This uncertainty suggests that the method might not generalize well to a broad range of gender-biased language, potentially impacting its consistency and reliability across diverse examples."
            },
            "questions": {
                "value": "1. Will different types of gender-biased sentences activate distinct important neurons? The selected sentences focus on professions. If sentences featuring other gender-stereotyped topics, such as personality traits or colors, are used, would we observe similar results?\n\n2. What happens if the sentence is changed to \"This woman is ==> a nurse\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Existing approaches usually use model re-training or model fine-tuning methods to alleviate gender bias. They usually require curating a data set for debiasing purposes. And such re-training and fine-tuning might hurt model\u2019s performance on other tasks. \nTowards this end, the paper proposes Interpretable Model Editing (Interpret-ME), a method to reduce gender bias without designing huge datasets or fine-tuning. Compared to fine-tuning methods, the proposed approach shows competitive results in reducing gender bias across experiments with 8 LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. good presentation\n\n2. proper adaptation of existing methods to application problems\n\n3. thorough experiments on various models"
            },
            "weaknesses": {
                "value": "My only concern is that from the experiments, it seems that in order for Interpret-ME to not hurt models\u2019 performance on other tasks, it requires very delicate hyper-parameter search. Therefore I\u2019m not convinced that compared to fine-tuning approaches, the proposed approach is more beneficial from the perspective of maintaining LLM\u2019s existing capability. More experiment designs and results along this line would be very helpful."
            },
            "questions": {
                "value": "Please see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces an approach to mitigate gender bias using a neuron-level framework called Interpret-ME. Unlike resource-intensive finetuning methods, this proposed approach edits key neurons to reduce bias while maintaining model performance. The authors demonstrate the effectiveness of their proposed approach on eight LLMs using various metrics such as StereoSet, WinoGender, and CrowS-Pairs, achieving competitive results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The study addresses a crucial issue in machine learning: mitigating gender bias in LLMs through an efficient method that avoids resource-heavy fine-tuning or data collection. \n- It is validated across a range of large and common models, enhancing practical significance. \n- The method also maintains overall model performance while offering valuable neuron-level interpretability insights into bias mechanisms."
            },
            "weaknesses": {
                "value": "The paper requires substantial revisions to meet the standards of a prestigious venue like ICLR. The main issues are:\n\n- The writing is difficult to follow due to unclear explanations and a confusing structure. Key sections, such as the background and methodology, require multiple readings to understand. For example, the background section introduces numerous variables and equations without sufficient context or motivation, making it challenging for readers to relate them to the main methodology. While it extensively reiterates standard multi-head attention formulas with many variables, it fails to explain their relevance to this work. Conversely, the authors omit essential background on the key concept of the unembedding space, which is central to understanding the proposed methodology.\n\n- Figures and tables are presented without adequate spacing, blending into the text and making it difficult to differentiate between the main content and captions (e.g., page 5).\n\n- The method relies heavily on existing interpretability frameworks, leading to limited innovation and making the contributions feel incremental.\n\n- The idea of addressing bias in LLMs by identifying and editing specific neurons is not new, and similar approaches have been explored before. The authors do not adequately acknowledge this and fail to distinguish their work from related studies like those by Chintam et al. (2023) and Lutz et al. (2024). For example, Chintam et al. (2023) used methods like automated circuit discovery to identifying causal relations between LM components and gender bias, following by performing a finetuning strategy to mitigate bias in those components.\n\n- The study's use of bias-evaluation datasets, such as CrowS-Pairs and StereoSet, which have been criticized for noise and reliability issues (Blodgett et al., 2021), raises concerns about the robustness of the evaluation.\n\n[1] Chintam, A., Beloch, R., Zuidema, W., Hanna, M., & Van Der Wal, O. (2023). Identifying and adapting transformer-components responsible for gender bias in an English language model. arXiv preprint arXiv:2310.12611.\n\n[2] Lutz, M., Choenni, R., Strohmaier, M., & Lauscher, A. (2024). Local Contrastive Editing of Gender Stereotypes. arXiv preprint arXiv:2310.17739.\n\n[3] Blodgett, S. L., Lopez, G., Olteanu, A., Sim, R., & Wallach, H. (2021). Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1004\u20131015, Online. Association for Computational Linguistics."
            },
            "questions": {
                "value": "Q1. Could you provide a clearer explanation of how the hypotheses in Section 3.3 were derived from the previous analyses?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper mitigates the gender bias issue of large language models by editing model parameters instead of data cleaning and fine-tuning. The paper argues that some neurons in LLM exhibits significant bias and thus results in the bias of LLM. Therefore, the authors firstly adopt interpretability methods to identify such neurons and then propose Interpret-ME to reduce it. The experimental results demonstrate the effectiveness of Interpret-ME across 8 LLMs without degrading their performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. It is important to studying the neuron circults of the generated response to make LLMs more interpretable and align with human values. The motivation is convincing.\n2. The methodology of implementing the idea is sensible.\n3. The experimental results show the effectiveness of the methods."
            },
            "weaknesses": {
                "value": "1. My major concern is that the models studied by the authors may be outdated. With the continuous development of LLMs, more and more drawbacks of them vanished. It\u2019s unclear whether recent LLMs suffer from such an issue and whether the proposed method could generalize to recent LLMs. I suggest more experiments to clarify this point. Otherwise, the contribution of this work will be vague or limited. \n2. It seems that the activated neurons vary across different prompts, and gender bias is often implicitly represented by models. Is the women-and-man (or specific-prompt) setting generalizable and convincing enough to arrive at the research conclusions? Are the adopted settings representative enough?"
            },
            "questions": {
                "value": "1. Do recent LLMs suffer from gender bias issues (e.g., o1, GPT-4o, GPT-4, GPT-3.5-turbo, LLaMa 3.1, LLaMa 3.2)?\n2. With the recent development of LLMs, their \u201cintelligence\u201d keeps growing due to the boost of data volume and quality. Various kinds of bias are less likely to be present in the training data. Could you give some examples of the bias categories represented by recently proposed LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the Interpretable Model Editing (Interpret-ME) method, which effectively reduces gender bias in LLMs by identifying key neurons without requiring large datasets or fine-tuning, achieving competitive results while preserving performance in other tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThis paper analyzes the neurons in LLMs responsible for storing gender bias, contributing to a deeper understanding of how gender bias exists within these models.\n2.\tBy identifying key neurons associated with gender bias, the paper demonstrates that editing these neurons can achieve better debiasing results with minimal impact on overall model performance.\n3.\tThe paper also explores the importance of different neurons and points out that \"FFN query neurons\" have the most significant influence on gender bias."
            },
            "weaknesses": {
                "value": "The located neurons may not be sufficiently representative. Since only five sentences per gender are used to locate neurons, these sentences may not adequately capture real-world gender stereotypes. Moreover, there is no experiment in the paper that demonstrates whether using more or fewer sentences would affect the performance of the Interpret-ME method."
            },
            "questions": {
                "value": "Why is Table 5 not a comparison between the Interpret-ME method, fine-tuning methods, and the original model? Why does Table 5 not include a comparison between the Interpret-ME method, fine-tuning methods, and the original model? I would like to know whether Interpret-ME causes less performance drop compared to fine-tuning methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}