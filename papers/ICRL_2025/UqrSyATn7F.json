{
    "id": "UqrSyATn7F",
    "title": "Tokenizing 3D Molecule Structure with Quantized Spherical Coordinates",
    "abstract": "The application of language models (LMs) to molecular structure generation using line notations such as SMILES and SELFIES has been well-established in the field of cheminformatics. However, extending these models to generate 3D molecular structures presents significant challenges. Two primary obstacles emerge: (1) the difficulty in designing a 3D line notation that ensures SE(3)-invariant atomic coordinates, and (2) the non-trivial task of tokenizing continuous coordinates for use in LMs, which inherently require discrete inputs.\nTo address these challenges, we propose Mol-StrucTok, a novel method for tokenizing 3D molecular structures. Our approach comprises two key innovations: (1) We design a line notation for 3D molecules by extracting local atomic coordinates in a spherical coordinate system. This notation builds upon existing 2D line notations and remains agnostic to their specific forms, ensuring compatibility with various molecular representation schemes. (2) We employ a Vector Quantized Variational Autoencoder (VQ-VAE) to tokenize these coordinates, treating them as generation descriptors. To further enhance the representation, we incorporate neighborhood bond lengths and bond angles as understanding descriptors. Leveraging this tokenization framework, we train a GPT-2 style model for 3D molecular generation tasks. Results demonstrate strong performance with significantly faster generation speeds and competitive chemical stability compared to previous methods. \nFurther, by integrating our learned discrete representations into Graphormer model for property prediction on QM9 dataset, Mol-StrucTok reveals consistent improvements across various molecular properties, underscoring the versatility and robustness of our approach.",
    "keywords": [
        "molecule structure tokenization",
        "generation",
        "spherical coordinates",
        "VQ-VAE"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "We propose a method to linearize 3D molecular structures and quantize them using VQ-VAE, enabling their generation with language models.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=UqrSyATn7F",
    "pdf_link": "https://openreview.net/pdf?id=UqrSyATn7F",
    "comments": [
        {
            "summary": {
                "value": "This paper focuses on the task of 3D molecule generation with language models (LMs). The main step is to convert 3D molecules to sequences of discrete tokens. The authors first use SMILES (or SELFIES) to convert a molecule's atom and bond information to a sequence. They then use spherical coordinates and VQ-VAE to convert 3D information to discrete tokens. After obtaining these 3D molecule sequences, they train a GPT-style model for molecule generation by next-token-prediction. The model is evaluated on the random generation and conditional generation tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow, with clear and informative tables and figures. \n2. The proposed method performs well, especially on the conditional generation task.\n3. The ablation study is thorough and provides useful insights."
            },
            "weaknesses": {
                "value": "1. The proposed method is quite similar to existing methods, such as FoldSeek and FoldToken. Specifically, similar to the SE(3)-invariant spherical coordinates here, FoldSeek also uses distances and angles computed based on reference nodes as SE(3)-invariant representations. In addition, Furthermore, both methods employ VQ-VAE to learn discrete tokens. These overlapping components limit the novelty of this work.\n2. About the datasets: the proposed method is only evaluated on QM9 dataset, which includes only small molecules. It will be better to evaluate on larger molecules, such as geom-drug dataset.\n3. More LM-based baseline methods should be included and compared, such as Geo2Seq and BindGPT.\n4. Further analysis and potential visualizations of the learned structural alphabet, for example, which types of local structures can be mapped to the same code? Refer to Figure S2 and S3 of FoldSeek.\n\n\nOther questions:\n1. line 52: SE(3) invariance is invariance under rotation and translation, but not reflection\n2. What is the model size?"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce a novel method for tokenizing 3D molecular structures. The authors design a 2D line notation for 3D molecules by extracting the local atomic spherical system, then employ VQ-VAE to tokenize upon this 2D line notation to tokenize the spherical coordinates. Results show the competitive performance compared to several SOTA methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The combination of spherical line notation with vector quantization enables language models to process complex 3D data, which is challenging to discretize. This approach stands out from traditional graph-based or continuous-coordinate models by providing a discrete representation for language models without losing SE(3)-invariant information. Particularly, the augmented tokens incorporate both generation and understanding descriptors, including local spherical coordinates, bond lengths, and angles, allowing a better capture of molecular topology and spatial arrangements."
            },
            "weaknesses": {
                "value": "### Major\nThe authors should clarify the rationale behind selecting exactly four neighbors for the atomic descriptor and explicitly address how the descriptor $\\mathbf{z}_i$ is defined for atoms with fewer than four neighbors. This is essential, as molecules with varying coordination environments will likely have different numbers of neighbors, impacting the generality of the descriptor across datasets.\n\n### Minor\n1. The paper\u2019s notations are somewhat inconsistent and could benefit from simplification and unification. For instance, symbols like $\\mathcal M$ are used only within specific sections, such as Section 3.1, and do not appear elsewhere. Additionally, the notation in Appendix C is different from that in Section 3.1. A more streamlined notation would improve readability and coherence across sections.\n2. Line 177, the phrase should be rephrased as \"we tokenized the molecular graph into a sequence of atom tokens $\\mathbb A$ and non-atom tokens $\\mathbb B$\".\n3. In Line 248 and Line 268, references are made to \"Section X\" and \"Figure X,\" which seem placeholders. These should be updated with specific section and figure numbers\n4. Line 239, \"Inspired by previous works\" is ambiguous. The authors should specify which studies or methods they are referring to here."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors express 3D structure using spherical coordinates, then tokenize them with a VQ-VAE. The tokens are then used for training a GPT-2 model for 3D molecular generation tasks. They also use the discrete representations in a Graphormer model for property prediction on QM9, and observe consistent improvements across various molecular properties."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The authors conduct an extensive set of experiments. They measure validity+uniqueness of generated molecules with different bond assignment methods, perform PoseBusters tests, evaluate quantum mechanical properties, and measure MAE for QM9 property prediction. They achieve state-of-the-art results in most experiments.\n2. They also perform additional analysis regarding the inference speed of their method and the effect of the generation temperature on balancing quality and diversity."
            },
            "weaknesses": {
                "value": "1. This is a hand-crafted tokenization scheme and should be compared to other tokenizers (e.g. BPE-based tokenizers), not just diffusion models and MPNN-based methods.\n2. It may also be helpful to compare with structures expressed in other coordinate systems. I'd imagine that without SE(3) invariance there would be a wider range of possible tokenized sequences, making it harder for the GPT-2 model to learn."
            },
            "questions": {
                "value": "1. To the best of my knowledge, PoseBusters has an energy ratio test, which tends to be very difficult to satisfy. Have you tried on that test? I think that test would be more meaningful since most methods already perform very well on the tests you demonstrated in Table 2.\n\n2. What's the motivation for a purely MLP-based quantized auto-encoder architecture? Most other autoencoder-based tokenizers use some form of GNN or sequence model. What are the tradeoffs between element-wise discretization and discretization with message propagation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes a novel language model based 3D molecule generation framework. Novel spherical coordinate based 3D line notation method combined with VQ-VAE model is adopted to tokenize 3D molecules, and GPT-2 models are applied to this tokenization framework. Benchmark experiments are conducted to show the promising performance of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Originality:  \nThis work makes a good originality contribution of  proposing a novel 3D molecule tokenization method that is applicable for language models.\n\nQuality:  \nThis work presents sufficient theoretic analysis and proofs for the proposed 3D tokenization method, and benchmark experiments are conducted to demonstrate the promising performance.\n\nClarity:  \nThe writing of this paper is fairly good. The paper gives sufficient description to help readers understand the key pipeline and steps of 3D molecule generation.\n\nSignificance:  \nThe proposed method is useful in leveraging the power of large language models to the generation and modeling of complicated 3D molecule structures."
            },
            "weaknesses": {
                "value": "(1) Some important methods details are not clearly clarified or presented. Authors are encouraged to add explanations to the following questions: \n- How the order of atom tokens in equation (2) is decided? By BFS or DFS traverse algorithm on 2D topology graphs?\n- The $x_i$ in line 266 is actually $g_i$ in equation (7)?\n- Why the sign of torsion angle $sign(\\varphi_i)$ appears in the generation descriptor of equation (7) but not in the combined descriptor of equation (9)?\n- Why local atomic environment (line 266-267) is needed? Can authors discuss the negative impacts of not including it and conduct ablation studies if possible?\n\n(2) In benchmark experiments, an important baseline Geo2Seq [1] (it also proposes a spherical coordinate based 3D molecule tokenization framework) is not presented and compared. Particularly, Geo2Seq has higher molecule stability than authors' method, so more discussion about differences and advantages over Geo2Seq is needed.\n\n(3) VQ-VAE based vector quantization is applied to atom descriptor in equation (9) to form the final generation target. A natural question is what is the advantages of VQ-VAE over simple discretization (i.e., map floating numbers to pre-split bins, e.g., 0-30, 30-60, 60-90, ... for angles) ? Authors are encouraged to discuss different possible quantization methods and conduct ablation experiments to justify the advantages of VQ-VAE.\n\n[1] Li, Xiner, et al. \"Geometry Informed Tokenization of Molecules for Language Model Generation.\" arXiv preprint arXiv:2408.10120 (2024)."
            },
            "questions": {
                "value": "See Weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}