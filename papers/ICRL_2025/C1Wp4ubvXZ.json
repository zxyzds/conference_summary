{
    "id": "C1Wp4ubvXZ",
    "title": "FairlyUncertain: A Comprehensive Benchmark of Uncertainty in Algorithmic Fairness",
    "abstract": "Fair predictive algorithms hinge on both equality and trust, yet inherent uncertainty in real-world data challenges our ability to make consistent, fair, and calibrated decisions. While fairly managing predictive error has been extensively explored, some recent work has begun to address the challenge of fairly accounting for irreducible prediction uncertainty. However, a clear taxonomy and well-specified objectives for integrating uncertainty into fairness remains undefined. We address this gap by introducing FairlyUncertain, an axiomatic benchmark for evaluating uncertainty estimates in fairness. Our benchmark posits that fair predictive uncertainty estimates should be consistent across learning pipelines and calibrated to observed randomness. Through extensive experiments on 10 popular fairness datasets, our evaluation reveals: (1) A theoretically justified and simple method for estimating uncertainty in binary settings is more consistent and calibrated than prior work; (2) Abstaining from binary predictions, even with improved uncertainty estimates, reduces error but does not alleviate outcome imbalances between demographic groups; (3) Incorporating consistent and calibrated uncertainty estimates in regression tasks improves fairness without any explicit fairness interventions. Our benchmark package is designed to be extensible and open-source. By providing a standardized framework for assessing the interplay between uncertainty and fairness, FairlyUncertain paves the way for more equitable and trustworthy machine learning practices.",
    "keywords": [
        "Heteroscedastic",
        "Uncertainty",
        "Fairness",
        "Benchmark"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We introduce FairlyUncertain, an open-source benchmark that evaluates the consistency and calibration of uncertainty estimates in fairness, and discuss how uncertainty affects fairness across various predictive tasks.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=C1Wp4ubvXZ",
    "pdf_link": "https://openreview.net/pdf?id=C1Wp4ubvXZ",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces FairlyUncertain, a Python package designed to evaluate uncertainty in fairness for machine learning algorithms. The package proposes methods and standards to incorporate uncertainty estimation into fairness evaluations in predictive models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper identifies and addresses the underexplored intersection of fairness and uncertainty in predictive modeling.\n2. The study spans multiple datasets and predictive tasks, enhancing the benchmark's relevance and applicability.\n3. Designing the benchmark as an open-source tool encourages ongoing development and community contributions."
            },
            "weaknesses": {
                "value": "1. The paper targets uncertainty as a critical factor in predictive fairness, given the real-world impact of machine learning decisions. However, it does not adequately justify the importance of the consistency and calibration axioms themselves within this context. Specifically, while uncertainty is broadly recognized as significant for transparent AI, the paper could strengthen its justification by explaining why consistency and calibration specifically matter for fair uncertainty estimation. \n\n2. Although the paper targets \"fair uncertainty,\" the fairness component feels less integrated with uncertainty until Section 5, where it discusses the abstention framework and Uncertainty-Aware Statistical Parity. The paper could benefit from introducing the importance of fairness alongside uncertainty earlier, so readers understand the relationship between these two aims from the outset. \n\n3. This paper is largely an implementation paper, presenting a framework and accompanying package for uncertainty-fairness evaluation rather than introducing fundamentally new theoretical insights into either fairness or uncertainty estimation. Its main contribution lies in providing a package that can support standardized evaluations of uncertainty estimation methods within fairness contexts. Therefore, it may be better suited for a benchmark track or tools/package track than a general research track."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces FairlyUncertain, a benchmark designed to assess the integration of uncertainty into fairness contexts within machine learning. Addressing the inherent prediction uncertainty challenge for fairness, the paper emphasizes that uncertainty estimates should ideally be consistent across similar models and calibrated to observed randomness. Through experiments across 10 datasets, including binary classification and regression tasks, the authors evaluate various methods for uncertainty estimation and introduce a new fairness metric, Uncertainty-Aware Statistical Parity (UA-SP), tailored for regression tasks. FairlyUncertain provides a structured benchmark to explore the nuanced relationship between uncertainty and fairness, specifically examining the effects of abstention and confidence thresholds."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1- FairlyUncertain introduces a standardized benchmark that is both theoretically grounded and practical, which allows researchers to evaluate how uncertainty affects fairness and vice versa. This is timely, given the increasing need for fair AI applications under uncertainty.\n2- The paper defines fairness-focused axioms: Consistency (Axiom 2.3) and Calibration (Axiom 2.4), which set criteria for reliable uncertainty estimates. These axioms are clear and can be used for the practical goal of having fair predictive models that remain robust even under model variations.\n3- The authors provide extensive evaluations of consistency and calibration across datasets and methods. They cover both abstention and confidence thresholding. The benchmark\u2019s focus on these uncertainty handling methods, without endorsing one as definitive, offers a structured way to assess their impact on fairness. \n4- The benchmark is open-source which could be used in future research.\n5- The benchmark\u2019s use of consistency as a measure for similar pipelines indirectly addresses concerns about model multiplicity. FairlyUncertain's evaluation across slight hyperparameter changes serves as a practical check for this multiplicity effect."
            },
            "weaknesses": {
                "value": "1- Axiom 2.3 suggests that small changes in hyperparameters should not significantly impact uncertainty estimates. While this is a reasonable assumption, it would benefit from explicit clarification on the stability limits of uncertainty under various hyperparameter changes to avoid overgeneralization.\n2- Although the benchmark evaluates abstention as a fairness strategy, abstention does not reduce demographic disparities in binary classification tasks. Addressing why abstention fails to impact fairness in this context could inform potential adjustments to the benchmark. Also, given that abstention was shown not to reduce demographic disparities, the paper could address situations where abstention might inadvertently introduce biases. For example, explaining conditions under which abstention could be counterproductive would deepen the understanding of its impact on fairness.\n3- While tables like Table 2 and Table 4 capture calibration and consistency scores, the paper could strengthen its analysis by discussing why certain methods perform better/worse across different datasets. Additionally, the appendix includes valuable demographic-specific breakdowns that could be better integrated into the main analysis to improve understanding of method reliability and demographic-specific impacts.\n4- While XGBoost is effective for tabular data, expanding to deep learning models like MLPs, or transformers would align with recent trends in fairness research and test FairlyUncertain\u2019s utility on more complex architectures. Reproducibility might vary across model architectures, especially complex ones, due to the implicit assumption of consistency under slight hyperparameter variations. While the benchmark is robust as presented, extending it to a broader model set might require tuning and validation."
            },
            "questions": {
                "value": "1- How does this work compare with existing research on fairness evaluation (beyond Section 6.1)? i.e., to what extent does the authors' focus on uncertainty help the practical selection of fairness measures and mitigation strategies?\n\n2- Could this framework be adapted for more complex models or architectures and extended to additional fairness notions, such as intrinsic fairness? If so, how?\n\n3- What are the societal implications of incorporating uncertainty in the selection of fair models? Could the authors expand on the strengths of their approach relative to existing methods, providing examples to illustrate the impact beyond simple numerical metrics? Existing literature on model multiplicity highlights the uncertainty involved and explores its societal, legal, and philosophical implications (there are many papers out there on this topic). I am wondering, what new insights does this paper contribute to these ongoing discussions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces \"FairlyUncertain\", a benchmark to evaluate predictive uncertainty in fairness contexts. It emphasizes the need for consistency and calibration of uncertainty estimates across learning pipelines. Experiments on ten datasets reveal that a simple uncertainty estimation method outperforms prior work, that abstaining from uncertain binary predictions reduces errors but not demographic imbalances, and that incorporating calibrated uncertainty in regression improves fairness. The benchmark is extensible and aims to standardize fairness assessments involving uncertainty, promoting equitable and trustworthy machine learning practices."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Originality: Introduces FairlyUncertain, a novel benchmark for integrating uncertainty into fairness.\n\n- Quality: Validated through extensive experiments across ten datasets; open-source and extensible.\n\n- Clarity: Clear motivation, methodology, and presentation.\n\n- Significance: Provides a foundational, scalable framework for enhancing fairness in machine learning."
            },
            "weaknesses": {
                "value": "- Findings on reducing errors but not outcome imbalances could be expanded with deeper insights or additional benchmarks.\n\n- The paper does not fully explore variations in models, parameters, or datasets, which limits its generalizability and insights into broader use cases."
            },
            "questions": {
                "value": "- Are there plans to include additional fairness interventions?\n- Which new calibration metrics could enhance FairlyUncertain?\n- Why does regression improve fairness without explicit interventions?\n- How generalizable are the results beyond the ten datasets used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce FairlyUncertain, an axiomatic benchmark for evaluating uncertainty estimates in fairness. The benchmark posits that fair predictive uncertainty estimates should be consistent across learning pipelines and calibrated to observed randomness. FairlyUncertain suggests that: In the binary setting, natural uncertainty estimates beat complex ensemble-based approaches and abstaining improves error but not imbalance between demographic groups. In the regression setting, consistent and calibrated uncertainty methods can reduce distributional imbalance without any explicit fairness intervention."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The organization and writing are fairly clear, and the structure of the paper is sound.\n\n2. The motivation is clear, and the background as well as the related works are well explained.\n\n3. The paper shows enough originality and novelty, by providing a novel framework for fairness and uncertainty.\n \n4. The experiments are extensive, comparing the performances on various datasets and different prediction tasks."
            },
            "weaknesses": {
                "value": "1. The connection between \"Fairness\" and \"Consistency/Calibration\" is not so clear: in the description of consistency/calibration metrics, the sensitive attributes don't seem to be included and discussed. For example, for consistency, all the metrics (SD or p-values) are computed over different ensembles, but how is it related to fairness? For calibration, it says \"We identify groups of individuals with similar uncertainty estimates and empirically evaluate the standard deviation of the residual difference between the observed and predicted outcomes\", but in the previous definition of calibration (Axiom 2.4), the group should be defined by sensitive attributes. The relationships between fairness and consistency/calibration should be further explained.\n\n2. The methodology is limited to ensemble methods: the consistency/calibration metrics discussed in the paper all focus on ensemble methods (i.e., XGBoost), how can this be generalized to a wider class of ML methods (e.g., neural networks with stochastic optimizers)?\n\n3. The experiments on fairness interventions are not very convincing: in section 5, the authors show that \"abstaining from binary predictions, even with improved uncertainty estimates, reduces error but does not alleviate outcome imbalances between demographic groups\", can the authors provide more insights into this phenomenon? This point is linked to my previous request for more explanation on the connection between fairness and uncertainty estimates."
            },
            "questions": {
                "value": "1. Is there more discussion on the connection between fairness and consistency/calibration? For example, how does the standard deviation across different hyper-parameters (which don't take sensitive attributes into account) help mitigate bias towards certain groups (which need information on sensitive attributes)?\n\n2. Can this definition of consistency/calibration be generalized to other ML methods other than ensembles?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents the FairlyUncertain benchmark, which is aimed at measuring the fairness properties of model uncertainty estimates. They focus on two metrics which are called consistency and calibration, and explore the results on these metrics across several algorithms and several datasets, concluding that probabilistically-grounded methods tend to return the best results. They explore the ramifications of abstention on fairness metrics, and propose a new uncertainty-aware form of statistical parity."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- this is an important topic for which there are no currently existing benchmarks that I\u2019m aware of, I think this could be useful for people\n- good level of thoroughness here exploring the various different metrics across datasets\n- abstention experiments are interesting - surprising to me that abstention doesn\u2019t improve most fairness metrics\n- uncertainty-aware Statistical Parity is a good idea - incorporating the uncertainty estimate into a CDF-based metric makes sense"
            },
            "weaknesses": {
                "value": "- I\u2019m not sure I agree with the characterization (around L90) of A/B types as unmeasurable and C/D/E types as estimatable from a fixed sample. I think both of these statements are conditional on various distributional assumptions - for instance, you can measure individual variance if you assume you know the mean (or its parametric form and inputs). I think explicating the underlying data generating process would make this whole contribution more substantial\n- A couple issues with formalisms in Section 2: 1. I don\u2019t understand Def 2.2 - \u201cdiffer only in hyperparameter settings\u201d and \u201cthere is some j\u2026\u201d seem to say different things: the first implies P and P\u2019 are in some shared \u201cpipeline class\u201d parametrized by different hyperparameters, the second implies that just one hyperparameter differs. 2. On Line 127, f(P) is used - this seems wrong to me, as on L117, f is named as the *output* of P, not a function of it\n- I\u2019m not sure I agree with the given definition of consistency as a useful fairness axiom - it seems like more of a Lipschitz condition over what a well-parametrized pipeline might look like. My impression of a fairness axiom of this type might be of the flavor that uncertainty estimates should be as invariant as possible to hyperparameter choices (I\u2019m not sure this is a good axiom either but it seems more fairness-related).\n- there is a rich literature on calibration metrics which is not connected to at all in this paper - it would be good to get a better understanding of how those relate to the chosen metrics\n- L287 and elsewhere: for consistency, it seems like a non-robust choice to output a maximum std. Dev over individuals - I\u2019m wondering if like a 90th percentile might be a better choice\n- L391 - it seems odd to optimize an objective function that mixes statistical parity and equalized odds. Usually, one or the other is picked, since they are very different objectives\n- Def 6.2 - it seems overly specific to me to assume that the model outputs are parametrizing a normal distribution. In particular, I don\u2019t think this makes sense in the binary Y case - and the end of the definition (CDF comparison for all outcomes y \\in Y) I think doesn\u2019t make sense in the binary y case either (rather you want this to be true at all points in the range)\n\nSmall points:\n- L142: \u201ca model that always returns 0 is consistent\u201d - by the previous definition this is not true, as the definition of consistency applies to a pipeline class, rather than the model which is outputted. Such a pipeline may output a constant-0 model for some set of hyperparameters, but not others.\n- how is clustering done for calibration? I don\u2019t think this is discussed\n- Table 4: not sure how Disp. Impact is defined"
            },
            "questions": {
                "value": "- I\u2019m not sure the framework in Table 1 does a ton for me - doesn\u2019t really connect to the rest of the paper at the moment so tightly and I\u2019m not sure what the takeaways are. I would be interested to see a clearer explication of how these types of uncertainty connect to epistemic/aleatoric\n- would like to see a more fleshed out argument for the consistency axiom\n- would be interested to know more about connections to current calibration literature\n- how much do results change if each fairness metric is optimized for individually (in combination with error)\n- is there a more general version of the UA-SP definition?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}