{
    "id": "U42IGDU3q5",
    "title": "CIRQRS: Evaluating Query Relevance Score in Composed Image Retrieval",
    "abstract": "Composed Image Retrieval (CIR) retrieves relevant images using a reference image and accompanying text that describes how the desired images differ from the reference. However, the commonly used evaluation metric Recall@k only checks if the target image is retrieved, without considering the relevance of other images to the query, potentially leading to user dissatisfaction. We introduce Composed Image Retrieval Query Relevance Score (CIRQRS), an evaluation metric that scores each retrieved image based on its relevance to the query, offering a comprehensive evaluation. CIRQRS is trained using a reward model objective to prefer highly relevant, positive images over less relevant, negative ones. We propose a strategy motivated by self-paced learning to dynamically adjust the negative set based on the relevance of each image by using CIRQRS's current training status. To validate CIRQRS's ability to measure relevance, we created the human-scored FashionIQ (HS-FashionIQ) dataset and compared it with scores from human evaluators. CIRQRS correlates with human scores 2.625 times better than Recall@k, highlighting its superior ability to capture relevance. Additionally, by ranking images based on their CIRQRS, we check if the target image appears in the top k. The results show that CIRQRS achieves state-of-the-art performance on two representative CIR datasets, CIRR and FashionIQ.",
    "keywords": [
        "composed image retrieval",
        "evaluation metric",
        "self-paced learning"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=U42IGDU3q5",
    "pdf_link": "https://openreview.net/pdf?id=U42IGDU3q5",
    "comments": [
        {
            "summary": {
                "value": "This paper is about CIRQRS (Composed Image Retrieval Query Relevance Score), a novel evaluation metric designed for Composed Image Retrieval (CIR) tasks, where users search for images based on a reference image and text query. The proposal is to use CIRQRS instead of the traditional Recall@k metric, which only checks if a target image is retrieved in the set of retrieved images. Since typically only one image is the target one, Recall@k can't evaluate if the other retrieved images may be still relevant to the composed query. CIRQRS, instead, assesses the relevance of each retrieved image to the query. The metric is trained using a reward model and a self-paced learning approach, where the negative set is dynamically adjusted based on relevance, to ensure it can rank images by query relevance accurately. The paper also introduces human-annotated data defining the HS-FashionIQ dataset, where CIRQRS is also validated and showing state-of-the-art performance, i.e. better correlation with human preferences over Recall@k. The method is also evaluated on the standard FashionIQ and CIRR datasets, demonstrating state-of-the-art performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is timely for the CIR community, I found the paper well structured and well written. \n\n- CIRQRS addresses a core limitation in current CIR evaluation metrics by focusing on user-perceived relevance rather than simply on retrieving a target image. Traditional metrics like Recall@k fail to account for how well the retrieved set of images matches the user\u2019s desired attributes, often leading to retrievals that technically meet the query but don\u2019t align with user intent.\n\n- Experiments on the FashionIQ and CIRR datasets show clear state of the art performance.\n\n- The creation of the HS-FashionIQ dataset is a valuable contribution to the CIR field."
            },
            "weaknesses": {
                "value": "- The idea of proposing a metric based on a specific method to automatically evaluate images of the set is interesting, but it raises issues regarding alignment. Proposing CIRQRS as a primary CIR metric can be deceptive for future proposed methods, as is derived from a method that aims to align with user judgments but cannot perfectly capture user-defined relevance. This creates a risk that future CIR models, optimized based on CIRQRS, will only be partially aligned with actual user expectations. Specifically, this metric\u2019s reliance on training data and model biases (in this case, the HS-FashionIQ dataset) introduces a limited view of \u201crelevance\u201d that might not generalize well across diverse retrieval contexts. Consequently, using CIRQRS as the evaluation standard could narrow the objective of future CIR models to a version of relevance that aligns more closely with CIRQRS\u2019s training biases than with diverse user intent, specifically that of the ~2,7k valid queries. I understand the reason to evaluate the entire set instead of the single target image, but I do not see why this specific method should be better to be used as metric instead of any other existing state of the art CIR methods and check their correlation. Perhaps a better approach would be to treat CIRQRS as a methodological advancement\u2014an effective way to improve the relevance evaluation capability of retrieval models\u2014rather than promoting it as a general-purpose metric for CIR.\n\n- The novelty of the self-paced learning strategy is limited to the application of the general strategy (like for instance in Jiang, Lu, et al. \"Self-paced learning with diversity\" Neurips 2014) to the method.\n\nTypos\n- page 6, FasionIQ should be FashionIQ."
            },
            "questions": {
                "value": "- I understand the reason to evaluate the entire set instead of the single target image, but I do not see why this specific method should be better to be used as metric instead of any other existing CIR methods. Perhaps a better approach would be to treat CIRQRS as a methodological advancement\u2014an effective way to improve the relevance evaluation capability of retrieval models\u2014rather than promoting it as a general-purpose metric for CIR. Can you compare CIRQRS with existing CIR methods as evaluation metrics, discussing their respective advantages and limitations?\n\n- Are the HS-FashionIQ dataset and code of the method going to be released?\n\n- Do you see potential mitigation strategies for these alignment concerns, such as methods to reduce bias in the training data or plans to validate CIRQRS across a more diverse range of retrieval contexts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces CIRQRS into the CIR task, providing a user-centric evaluation metric. The HS-FashionIQ dataset with human scores was created to validate CIRQRS. The paper proposes a new strategy to refine negative images during training and improve query-based image ranking. Experiments on the CIRR and FashionIQ datasets validate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1 ) The paper is well-written and has a clear structure. It effectively presents a novel CIR evaluation metric. The appendix includes additional details on the new dataset. \\\n2 ) The proposed method achieves top-ranking results, placing first and second on the FashionIQ and CIRR datasets."
            },
            "weaknesses": {
                "value": "1 ) The paper does not sufficiently highlight the novelty and effectiveness of the proposed approach. The authors should explain more to clarify the method\u2019s advancements beyond merely introducing a new loss function. \\\n2 ) Since the goal is to introduce a new evaluation metric for CIR, a more detailed review of its effectiveness is needed. For example, testing CIRQRS on other models could help show its efficiency. \\\n3 ) In Tables 2 and 3, the paper wants to demonstrate that CIRQRS is more effective than R@5, but the process by which these data were calculated should be clarified in greater detail. Since R@5 is an important point here, it would also help to show a comparison of R@5 results on the FashionIQ dataset as in Table 4 with other methods."
            },
            "questions": {
                "value": "The paper contains minor grammatical errors. For example, on page 2, line 85, \"Additionally, when CIRQRS is higher on a set of retrieved images\" should use \"was\" instead of \"is,\" and \"limitation\" on line 95 should be \"limitations.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a new evaluation metric for Composed Image Retrieval (CIR) called CIRQRS (Composed Image Retrieval Query Relevance Score), aimed at overcoming limitations of the commonly used Recall@k metric. The authors offer a new dataset HS-FashionIQ, which scores different candidate groups of images of a few CIR models. The authors shows SoTA performance in most metrics of the CIRR and FashionIQ datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The idea of relevance score in image retrieval is good, especially human preference of retrieved results. \n- Authors show SoTA performance on the main CIR datasets\n- Training procedure involving hard-negatives is shown in the paper, suggest a better training paradigm to CIR."
            },
            "weaknesses": {
                "value": "General:\n\n- Line 037: The comment \u201cCIR provides a more intuitive way to express search intent\u201d - seems subjective. While CIR does enable a multimodal query format (image + text), which can simplify certain cases (e.g., describing a specific T-shirt design that may be hard to convey in words alone), using text as a query is often more intuitive for me (text-to-image retrieval task). I recommend rephrasing this sentence to clarify the intended advantage.\n- Line 042: The example provided is helpful, but it would be especially beneficial to include the ground truth target image for clarity. This is important for readers who may not be familiar with the dataset.\n- Related Work:  It would be more logical to introduce the \u201cVision-Language Foundation Model\u201d paragraph first, as it provides the foundation for most CIR-related studies.\n- Line 227: The claim that \u201cas images with higher CIRQRS than the target are likely to be highly relevant after sufficient training\u201d needs support. Consider using \u201cPrecision@K\u201d labeling, accounting for false negatives (images that match but are not labeled as such). Otherwise, higher scores than the GT image could just be noise or indicate an imperfect model.\n- Line 262 refers to Figure 5: This example is unclear. What distinguishes set A from set B? Why do the same images have different CIRQRS scores across sets? What is the GT image, and what do the User Preference symbols (v or x) mean? This example should clarify the annotation process (line 262), but it currently feels confusing.\n- The scores CIRQRS-model outputs, to my understanding, are just similarity scores as any other casual CIR model produce. The authors shows SoTA performance, in some metric, with their different training procedure that considers \u201chard-negative\u201d samples. The authors claim to introduce a new evaluation metric that \u201covercomes the limitation of Recall@k\u201d (line 479), but in practice, all different model in the paper was evaluated using the Recall@K metric and not the new one.\n\nAnnotations: \n- I have a major concern about the annotation process. Image Retrieval (IR) datasets often include many \u201cfalse-negative\u201d examples\u2014images labeled as \u201cnegative\u201d by default but which humans would consider \u201cpositive.\u201d Ideally, a human annotator would review the entire image corpus for each query to mark each image as positive or negative, but that\u2019s impractical. While CIRQRS score annotations are valuable for the community, I worry they may still include \u201cfalse-negatives,\u201d where more positives are marked per sample. I suggest an extra step to label images in the candidate set as positive or negative, alongside CIRQRS score, to better distinguish true positives from \u201crelevant but negative\u201d images.\n\nEvaluation:\n - Tables 4 and 5 compare different methods to the paper results (CIRQRS). As described in Sections 3 and 5, the authors leveraged the BLIP-2 foundation model for the the CIR task. Despite being trained differently, it is essential to present the BLIP-2 baseline performance on these datasets, compared to current CIRQRS. It will clarify from where the improvement in results comes from: theoretically, one may suspect that the BLIP-2 model (that shown to be a strong backbone) may outperform the CIRQRS baseline which maybe only decreases the result of its backbone. On the other hand, the CIRQRS baseline may improve BLIP-2 baseline in a large margin, due to a better training procedure. It is an essential comparison to make here, which is missing in the evaluation tables.\n\n- Just to make it clear: does the authors method rely solely on the FashionIQ training set, or it also trained on HQ-FashionIQ?\n* how beneficial would it be to train on both HQ-FashionIQ and FashionIQ? Since your method selects \u201chard negatives\u201d using the scoring model (Sec 3.2), could adding HQ-FashionIQ data improve the Recall@K metrics?\n\nEvaluation on HS-FASHIONIQ:\n\n- In the paper, \u201cCIRQRS\u201d is referred as a score, metric and as a model/method (based on BLIP-2)..  Please clarify this usage in context or consider renaming (e.g., \u201cCIRQRS-Model\u201d).\nSpecifically, in Sec 5.2, you compare Recall@K and CIRQRS metrics to human-preferences. What model was used for these results? What is the actual values of these numbers on this model, and how it changes across different models?\nIt is not fully clear to me.\n\nCorrelation with Human Score (Sec 5.2):\n\nI\u2019m not convinced this experiment is necessary. Different metrics assess different things: Recall@K measures how many GT targets appear in the top K, while human preference between two sets of results measures, indeed, human preference, and takes into account only top-k results. Imagine a bad CIR model that leaves all GT targets least in the results, but still top ranks \u201crelevant\u201d images based on shallow chartersitic such color - in this case, the Recall@K should be very low, while human preference will be high. It would be more \u201ccomparable\u201d to conduct this experiment with Precision@K metric (but, it is not widely used as Recall@K).\n\n\nI think the paper idea is good and important, but the flow is not entirely clear. The authors shall distinguish between method, metric and score, and provide more evidence for some claims. For example, since the CIRQRS metric presented as the main contribution in the paper, it is essential to include a substantial comparison of CIRQRS values across several CIR models. Without this, it is unlikely other CIR studies would adopt this metric, which would limit the paper\u2019s impact."
            },
            "questions": {
                "value": "1. Eq. 1: \u201cthe inner product of the query and image embeddings\u201d - I assume you mean \u201cthe inner product of the image embeddings of the query and the candidate image.\u201d Is that correct?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}