{
    "id": "mXh8LbXXpx",
    "title": "Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation",
    "abstract": "Large Vision-Language Models (VLMs) are increasingly being regarded as foundation models that can be instructed to solve diverse tasks by prompting, without task-specific training.\nWe examine the seemingly obvious question: \\emph{how to effectively prompt VLMs for semantic segmentation}.\nTo that end, we systematically evaluate the segmentation performance of several recent models guided by either text or visual prompts on the diverse MESS dataset collection.\nWe introduce a scalable prompting scheme, \\emph{few-shot prompted semantic segmentation}, inspired by open-vocabulary segmentation and few-shot learning.\nIt turns out that even the most advanced VLMs lag far behind specialist models trained for a specific segmentation task, by about 30\\% on average on the Intersection-over-Union metric.\nMoreover, we find that text prompts and visual prompts are complementary: each one of the two modes fails on many examples that the other one can solve.\nOur analysis suggests that being able to anticipate the most effective prompt modality can lead to a 11\\% improvement in performance.\nMotivated by our findings, we propose PromptMatcher, a remarkably simple baseline that combines both text and visual prompts, achieving state-of-the-art results for training-free semantic segmentation.",
    "keywords": [
        "Foundation Models",
        "Prompting",
        "Semantic Segmentation",
        "Vision-Language Models",
        "VLM",
        "Training-free"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mXh8LbXXpx",
    "pdf_link": "https://openreview.net/pdf?id=mXh8LbXXpx",
    "comments": [
        {
            "summary": {
                "value": "This paper explores how to effectively prompt large VLMs for semantic segmentation by systematically evaluating text and visual prompts on the MESS dataset. It benchmarks some visual prompt and text prompt models on the dataset. It proposes a simple boosting technique by combining two different methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is straightforward and easy to understand. The proposed method is reasonable and logically structured.\n- The paper includes a detailed analysis of model behavior in some sections."
            },
            "weaknesses": {
                "value": "High-level weaknesses:\n- The first claimed contribution states, \u201cWe design a novel benchmarking task to probe the performance of VLMs as semantic segmentation engines.\u201d However, this benchmark appears identical to the one introduced in the MESS paper, which also evaluates more models in both its paper and associated webpage. Therefore, it may be inaccurate to present this as a novel benchmarking effort. Additionally, it would strengthen the paper to include models proposed in the MESS paper for comparison.\n- The paper's main contributions seem to focus on (1) benchmarking models on the MESS dataset and (2) proposing a simple ensembling method. However, there is a lack of direct comparison to established benchmarks like MESS and an absence of ablation studies to validate the proposed method. The conclusions drawn from the results are shallow and straightforward, limiting the insights provided.\n- For example, for the proposed PromptMatcher, the readers are interested in the contribution of each components, like the point proposals, rejection models, or influence of different text prompts.These ablations are totally missed in the paper.\n- The third claimed contribution, stating \u201cWe show that text and visual prompting complement each other, and that being able to anticipate the most effective prompt modality can lead to an 11% improvement in performance,\u201d does not align with the results. As Table 4 shows, the PromptMatcher only improves by 2.5% over the previous SOTA, LISA. Citing oracle results as representative of the paper's impact is misleading.\n- The title of the paper suggests it explores prompting for VLMs; however, the proposed method primarily connects VLMs with other models, as evidenced by the PromptMatcher\u2019s design. The title may therefore misrepresent the actual scope of the work.\n\nDetailed weaknesses:\n- The caption for Table 1 is inconsistent with the table\u2019s structure. The first block should indicate text-prompt methods, while the second block should cover visual prompts. This order is reversed in the caption, leading to confusion.\n- There are discrepancies in results across tables: for example, SoftMatcher+ achieves a score of 41.6 in Table 1 but 41.8 in Table 2. This inconsistency could raise questions about the article\u2019s credibility. \n- Similarly, the results of CAT-Seg are different from the results in MESS, which needs more clarification."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the main prompt-based semantic segmentation methods, one is based on text prompts and another one is based on visual prompts, each of which exhibits distinct advantages in different scenarios. The authors observe that these two approaches have complementary effects in certain scenarios. Therefore the authors propose a combined prompt scheme that integrates both approaches, achieving good results on benchmarks of training-free semantic segmentation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The author's observation is of practical significance, as text prompt-based and visual prompt-based segmentation methods should indeed be combined in different scenarios to achieve better results.\n- This paper is well-written and easy to follow.\n- The author has validated the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- **Limited Novelty**: The author's observation has already been discussed in previous papers, like T-Rex2 [1]. Much of the article simply restates this conclusion for semantic segmentation without offering more insightful observations. Moreover, the method proposed by the author is a simple combination of the existing works SoftMatcher+ and LISA, which has limited innovation.\n- **Limited Scope and Generalizability of the Model**: The paper primarily conducts experiments within the scope of semantic segmentation. However, in real-world applications, tasks like panoptic segmentation and instance segmentation may be more relevant. The proposed approach has limited generalizability and may not extend effectively to other segmentation scenarios, such as panoptic and instance segmentation.\n- **Limited Performance Improvement**: The proposed method aims to reduce the performance gap between the Oracle Ensemble & Oracle Ensemble+ methods (combining the best results from text and visual prompts) and using only visual or text prompts individually. However, the PromptMatcher still shows a significant gap from Oracle Ensemble+ (45.3 vs. 53.8). Given that varying the text prompt can substantially impact model performance, it raises the question of whether more precise refinement of the text prompt could further enhance the performance of this pipeline.\n\nReferences:\n\n[1] [T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy (ECCV 2024)](https://arxiv.org/abs/2403.14610)"
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new task setting: few-shot prompted semantic segmentation, to explore the performance of VLMs guided by text and visual prompts. The authors found that text and visual prompts are complementary. Based on this finding, they combined text and visual prompts, proposed PromptMatcher, and achieved state-of-the-art performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "S1: This paper is well written and easy to follow.\n\nS2: This paper proposes a new task, few-shot prompted semantic segmentation, to explore VLMs and conducts a thorough experimental analysis.\n\nS3: The paper finds that text and visual prompts are complementary, and based on this, proposes a combined approach that achieves significant performance improvement."
            },
            "weaknesses": {
                "value": "W1: Limited originality. Although the complementary nature of text and visual prompts discussed in this paper is interesting, the proposed PromptMatcher appears to be merely a simple combination of existing methods (LISA and SoftMatcher+). This is an engineering combination with a lack of methodological contribution.\n\nW2: Insufficient experiments. The paper is only validated on the MESS dataset, while experiments on more general datasets (such as COCO, PASCAL, RefCOCO, etc.) are necessary. These experiments could provide stronger support for the paper's conclusions.\n\nOverall, this work appears to be in an early stage in terms of both methodology and experimentation. The authors are encouraged to explore commonalities across different models more thoroughly and to combine prompts from various modalities in a way that distills the essential structure rather than simply combining existing models. To guide this exploration, I suggest conducting an ablation study to determine which components of LISA and SoftMatcher+ contribute most to the observed performance gains. Additionally, the authors could explore techniques like knowledge distillation to create a more unified model, enhancing the theoretical contribution and overall robustness of their approach."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates how to effectively prompt Large Vision-Language Models (VLMs) for semantic segmentation using the MESS dataset.  The authors introduce few-shot prompted semantic segmentation and discover that text and visual prompts are complementary. The paper also proposes PromptMatcher, a simple baseline combining both prompt types, achieving considerable results in training-free semantic segmentation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces a novel benchmarking task to evaluate the performance of Large Vision-Language Models (VLMs) for semantic segmentation, providing a systematic assessment of their capabilities and limitations.\n\n2. The  paper shows that text and visual prompting complement each other, and propose a simple training-free framework to capitalize on the complementary strengths of text and visual prompts."
            },
            "weaknesses": {
                "value": "1. The proposed PromptMatcher appears to be a simple combination of SoftMatch [2] and LISA [1]. The complementary nature of visual prompts and text prompts has already been extensively described in these previous works, which means that the innovation in this paper is somewhat limited.\n\n2. Recent work on Large Language Models (LLMs) for segmentation [3] has already demonstrated superior performance in certain aspects compared to previous expert models. However, the paper does not provide sufficient discussion on these advancements, making the claim that \"the latest models remain far below custom models trained for a specific task and data domain\" somewhat debatable.\n\n3. There are issues with the experimental comparisons presented in the paper. For instance, in Table 4, LISA shows significantly better performance on the Earth task compared to PromptMatcher. Additionally, the overall improvement in average performance by PromptMatcher is relatively minor.\n\n[1] LISA: Reasoning Segmentation via Large Language Model\n\n[2] SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning\n\n[3] LLaFS: When Large Language Models Meet Few-Shot Segmentation"
            },
            "questions": {
                "value": "Please reder to the Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}