{
    "id": "zi8YBcmXqA",
    "title": "PokeChamp: an Expert-level Minimax Language Agent for Competitive Pokemon",
    "abstract": "We introduce \\texttt{Pok\\'eChamp}, a Large Language Model (LLM) powered game-theoretic aware agent for two-player competitive Pok\\'emon battles, that uses an LLM prior and collected high-Elo human data to model minimax search without any additional training. \\texttt{Pok\\'eChamp} uses a depth-limited minimax search online where the LLM replaces three key components: 1) action sampling from the LLM guided by prompts (including from a damage calculation tool), 2) opponent-modeling via the historical likelihood of actions from our dataset to model the effect of LLM-predicted opponent actions, and 3) state value calculation for the LLM to reflect on each intrinsic state. \\texttt{Pok\\'eChamp} outperforms all existing AIs (76\\%) and heuristic bots (84\\%) by an enormous margin, including winning consistently (>50\\%) against prior human-parity work run with a frontier model, GPT 4-o, while using an open-source 8 billion parameter Llama 3.1 model. \\texttt{Pok\\'eChamp} achieves expert performance in the top 10\\% of players on the online ladder against competitive human players at an Elo of 1500. Finally, we collect the largest Pok\\'emon battling dataset, including 1 million+ games with 150k+ high Elo games, prepare a series of battling benchmarks based on real player data and puzzles to analyze specific battling abilities, and provide crucial updates to the local game engine. Our code is available \\href{https://sites.google.com/view/pokechamp-llm}{online}.",
    "keywords": [
        "multiagent",
        "LLM agents",
        "competitive games",
        "game theory",
        "reinforcement learning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zi8YBcmXqA",
    "pdf_link": "https://openreview.net/pdf?id=zi8YBcmXqA",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces Pok\u00e9Champ, a large language model (LLM)-powered agent designed for competitive Pok\u00e9mon battles. The agent integrates three LLM-enabled components for action sampling, opponent modeling, and state evaluation, which enable it to make informed and strategic decisions during gameplay. It demonstrates superior performance over existing bots and heuristic-based models and achieves a top 10% ranking in online Pok\u00e9mon battles."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. **Novel Integration of LLM with Minimax**: The paper innovatively combines an LLM with a minimax search to simulate human-like  decision-making in Pok\u00e9mon battles. This approach enables competitive performance without additional training and is adaptive to partially observable information.\n  \n2. **Performance on Real-World Benchmarks**: Pok\u00e9Champ\u2019s efficacy is validated in real-world benchmarks and against heuristic bots, achieving a high Elo rating of 1500 and consistently outperforming other state-of-the-art agents. \n\n3. **Comprehensive Dataset and Benchmarks**: The paper provides a large dataset of over one million Pok\u00e9mon battles, including 150,000 high-Elo games. These benchmarks, based on real player data and tailored puzzles, significantly enhance the study\u2019s reliability and offer a valuable resource for further research in this domain."
            },
            "weaknesses": {
                "value": "1. **Limited Prediction Accuracy for Opponent Modeling**: The limited accuracy in human and opponent action prediction, with opponent prediction only reaching 13\u201316%, may constrain the overall performance of the method, which relies on accurate opponent modeling.\n\n2. **Limited Exploration of Depth-Limitation Trade-offs**: The choice of depth-limited minimax search is justified as a balance between computational feasibility and decision quality. However, the trade-offs between search depth, LLM accuracy, and action quality are not thoroughly analyzed. Further exploration, potentially with ablation studies, would clarify the impact of depth limitations on performance.\n\n3. What is the role of Nash equilibrium in this paper? The paper does not seem to analyze the Nash equilibrium outcomes, which makes the definition of Nash equilibrium in Section 2 appear somewhat disconnected. It would be beneficial to include Nash equilibrium results in addition to Elo.\n\n4. How accurate is the next-state prediction? Since the minimax search relies on simulated rollouts of actions, the accuracy of next-state predictions could significantly impact the agent's performance."
            },
            "questions": {
                "value": "1. How does Pok\u00e9Champ\u2019s computation time compare to that of Pok\u00e9LLMon, which also utilizes GPT-4o, considering the additional requirements for minimax tree search and LLM queries?\n   \n2. How many human players were involved in obtaining the online ladder results presented in Table 5?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an LLM agent, PokeChamp, for competitive Pokemon battles. The model leverages a depth-limited minimax search to play the game, where the LLM plays the role of action sampling, opponent modeling, and state value calculation. The agent is shown to outperform all existing AIs significantly and achieve expert performance on the online ladder."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The paper introduces a novel integration of LLMs with minimax search. The agent leverages LLMs for three key components of minimax search: action sampling, opponent modeling, and value calculation. This integration allows the agent to employ human-like strategic thinking, bringing an expert-level game-playing agent.\n\n* The authors present a comprehensive set of experiments that demonstrate the capabilities of the agent across different competitive settings. The online ladder performance against human players with a competitive Elo rating provides a real-world evaluation of the agent.\n\n* he paper is well-organized and presented in a logical structure, allowing readers to follow both the technical intricacies and high-level motivations of the research."
            },
            "weaknesses": {
                "value": "*  The agent's design heavily relies on an in-depth understanding of competitive Pokemon gameplay, and its success relies on domain-specific engineering in the action sampling, opponent modeling, and value calculation components. While these adaptations make the agent effective in this domain, they limit the model\u2019s generalizability to other game-playing tasks with different mechanics or structures.\n\n* The idea of integrating LLMs with the minimax search framework for game-playing agents is closely related to prior work by Guo et al. (2024), which explores a similar concept in two-player zero-sum games.\n\n* While the paper provides a mathematical formalization of POMGs and makes assumptions like perfect recall, the connection between this theoretical framework and the practical implementation of the agent is not entirely clear.\n\n* The paper lacks an ablation study that examines the impact of each LLM-based component within the minimax search framework on the agent's overall performance. Since the authors use the LLM to replace three primary components, an ablation study would be invaluable in demonstrating how each component contributes to the agent's success.\n\n\nGuo, Wei, et al. \"Minimax Tree of Thoughts: Playing Two-Player Zero-Sum Sequential Games with Large Language Models.\" ICML 2024 Workshop on LLMs and Cognition."
            },
            "questions": {
                "value": "* How does the mathematical formalization in Section 2 relate to the design of the agent?\n* Can additional fine-tuning with the collected data improve the performance of the agent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "PokeChamp aims to bridge the gap in game theory aware LLM agents. The work uses competitive Pokemon as their case study and propose a minimax search method. Specifically, LLM is used in three key components in constructing the minimax tree: action sampling, opponent modeling, and state value calculation. PokeChamp exhibits good performance against human players."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors propose a novel application setting: competitive pokemon, where the turn-based nature of the game leads to a nice formulation as POMG. They manage to construct a minimax tree with the help of a LLM prior. PokeChamp is able to achieve top human performance in real game settings."
            },
            "weaknesses": {
                "value": "1. The paper is not so well-written: Missing multiple figures, tables, and appendix that is referred to in the main body. Also, as someone not familiar with competitive Pokemon, I found some of the concepts like Damage Calculator hard to grasp. It would be very helpful if you could add explanations of how the game works.\n2. Overall purpose of the work: It's hard to understand the contribution of this work. While the application case is interesting, I don't see this general framework being applicable to other games. For most games, it is not realistic to use LLM to replace state value function unless the LLM itself has enormous knowledge on the game."
            },
            "questions": {
                "value": "1. Confusion of damage calculator: In line 92-94, the authors mention that this external tool \"calculates the core game engine capabilities in combination with loading historical data from real player games in order to load likely stats for the opponent\u2019s team\". I didn't quite get this expression. Also, I found this definition conflicting in Figure 3 where the calculator seems to just output the number of turns needed to KO opponent's current Pokemon for each possible moves of player's current Pokemon. \n2. Action Prediction: The goal of the work is making LLM agent game theory aware, yet the 1M dataset collected are of human plays. I wonder how game theory optimal are those data? If not, what is the point of accurately predicting opponent's action when those action can be bad moves?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents Pok\u00e9Champ, a large language model (LLM)-powered agent designed for competitive Pok\u00e9mon battles. Utilizing the minimax approach, Pok\u00e9Champ integrates LLM-driven action sampling, opponent modeling, and value estimation to achieve strong performance in two-player, turn-based Pok\u00e9mon games. The paper also introduces a dataset of Pok\u00e9mon battles and benchmarks the system's performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The idea of utilizing LLMs for competitive gameplay is interesting, and the results seem promising."
            },
            "weaknesses": {
                "value": "The biggest weakness of the paper is that the proposed method is specifically designed for Pok\u00e9mon, which severely limits its generalizability. This also narrows the paper's target audience, and it's unclear how the methodology could be transferred or applied to other problems. The paper lacks a discussion on whether the framework would work for more complex, real-world competitive tasks, or for different game types where randomness, complexity, and strategic depth may vary significantly.\n\nThe writing also has significant room for improvement. First, readers unfamiliar with Pok\u00e9mon may find it difficult to understand many details in the paper. Additionally, several concepts are introduced before being properly defined, such as the Abyssal bot on line 197 and EV/IV on line 240. Furthermore, in Section 2, \"MATHEMATICAL FORMALIZATION,\" numerous symbols and terms are defined, but these concepts are not used in the subsequent main text. It's unclear what purpose this section serves\u2014perhaps it was included simply to make the paper appear more mathematical?\n\nThe minimax-based approach combined with LLMs may not be as novel as it initially appears. Minimax tree search has been extensively explored in AI for games, and while integrating LLMs offers an interesting twist, the underlying framework is still fundamentally a minimax search, which limits the novelty. Additionally, there is no evidence that Pok\u00e9Champ advances the state of the art in game-theoretic modeling beyond prior work in other competitive games such as chess, Go, or poker.\n\nThe paper heavily relies on heuristic tools like damage calculators and historical data, raising concerns about the system\u2019s true adaptability. This reliance on pre-defined tools limits the agent's flexibility and its ability to dynamically adapt to new or unseen scenarios. This suggests that the system lacks generalizability beyond the specific setup of competitive Pok\u00e9mon, making the approach less scalable to other domains or even future game updates.\n\nThe accuracy of opponent modeling also remains a concern. The relatively low accuracy in predicting opponent actions suggests that more refined or adaptive modeling techniques may be needed to further enhance performance.\n\nLastly, while the paper acknowledges the limitations of LLMs in planning and strategy, it fails to convincingly address these issues. The reliance on LLMs for action sampling and opponent modeling could lead to brittle decision-making, especially in cases where long-term strategy and deep reasoning are required."
            },
            "questions": {
                "value": "Please refer to the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}