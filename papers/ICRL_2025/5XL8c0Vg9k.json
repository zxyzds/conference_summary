{
    "id": "5XL8c0Vg9k",
    "title": "Infinite-parameter Large Language Model",
    "abstract": "In the standard transformer architecture, increasing model parameters leads to linear growth in computational cost and activation memory. To address this issue, we propose a novel Infinite Parameter Large Language Model (IP-LLM) architecture that decouples model size from computational cost and device memory. Existing large language models are all fixed-parameter models, while human knowledge is infinite and expands daily. Finite parameters are inherently limited in their capacity to accommodate this boundless knowledge. Our IP-LLM architecture can potentially accommodate infinite knowledge, resolving this issue and laying the foundation for realizing a truly omniscient and omnipotent artificial general intelligence in the future.",
    "keywords": [
        "lifelong learning"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "Achieving a large model with a dynamic number of parameters through routing.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5XL8c0Vg9k",
    "pdf_link": "https://openreview.net/pdf?id=5XL8c0Vg9k",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes to divide the LLMs into two step paradigm: (1) a routing step to have the transformer output the category of tasks; (2) for each category of tasks, a delegated transformer (base transformer + category special layers)  is applied to generate the outputs. Experiments are described in the paper to demonstrate the performance is comparable to existing models but not better."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "\u2022 An interesting attempt to look for continuous scalable architecture for LLMs."
            },
            "weaknesses": {
                "value": "\u2022 It is not clearly to me how the proposal can achieve infinite-parameter models just by classifying the input to into different classes and training/using different networks for different classes."
            },
            "questions": {
                "value": "1. How the routing network (Equation 6) can be generalized to unseen categories so as to be generalized to infinite many categories? Is the routing token set fixed or open? \n2. Is section 5 not completed in this version? It seems that a significant amount of texts are truncated between line 270 and line 282."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a language model based on a router that selects domain-specific parameters. A base model parses an input, a router classifies the input into a category corresponding to a domain, then inference is done with parameters corresponding to the selected domain.\n\nEach domain-specific parameter set is implemented using the last transformer layer of the base model replicated four times, and then trained with a \"defined proportion\" of domain-specific data and general data. The router has a similar parameterization, and is trained using classes corresponding to the domain-specific data.\n\nThe paper reports metrics on MMLU, C-Eval, GSM8K, and MATH, and also reports metrics from Llama2, Mistral, and Qwen1.5."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea of routing to domain-specific parameters is interesting (though more discussion of related work is needed, e.g. [1][2]). \n\n\n[1] Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models, Li et al 2022\n\n[2] Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM, Sukhbaatar et al COLM 2024"
            },
            "weaknesses": {
                "value": "The paper appears to be in an early stage. For example, key details of the method and experiment are not described or justified, and only 1 experiment (not fully described) has been performed. There is also missing discussion of key related work (e.g. [1], [2]). Here are some specific examples:\n\n- The datasets have not been described. The data can substantially impact the downstream tasks that are evaluated in the experiment. Similarly, the number of tokens trained on is important and has not been reported.\n\n- The evaluation is done on a proprietary evaluation pipeline, making reproducibility difficult.\n\n- The experiments need a controlled comparison of the method against alternatives. Currently it is difficult to draw conclusions from the experiment provided. For example, IPLLM-24B and Qwen1.5-32B are not comparable since IPLLM has been trained on additional domain-specific data (which has not been specified, and may be relevant for the experimental comparison). One example comparison could be finetuning Qwen1.5-32B on the union of corpora that IPLLM finetunes on.\n\n- Several claims made in the introduction and conclusion have not been justified. For example:\n    - \"Significant advantages in terms of reduced device memory requirements for both training and inference\": this has not been justified. For example, the proposed method requires two forward passes at inference time, and an additional 4 x (number-of-domains + 1) layers to train.\n    - \"Enabling the model to learn new knowledge without catastrophic forgetting\". This has not been justified experimentally.\n\n- Ablations on key design decisions have not been done. For example, the routing strategy, number of layers, and the base model.\n\n- Regarding novelty, Branch-Train-Merge [1] proposed to train different parts of the model independently on different subsets of the data (each subset corresponding to a domain, such as scientific or legal text). They also have a domain posterior that models the probability of a sequence belong to each domain (akin to the functionality of the proposed router). BTM and related follow-up work such as Branch-Train-MiX [2] should be discussed and compared with.\n\nI would encourage the authors to continue improving the work since their idea has potential, but I believe the current manuscript is not yet ready for ICLR. \n\n[1] Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models, Li et al 2022\n[2] Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM, Sukhbaatar et al COLM 2024"
            },
            "questions": {
                "value": "Please address the points discussed in the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an \"Infinite-Parameter Large Language Model\" with the idea to accommodate the increasing amount of information generated in the world that it hypothesizes models with a fixed number of parameters will eventually not be able to contain. The implementation involves training the model as a Mixture of Experts."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper tackles an important question."
            },
            "weaknesses": {
                "value": "- The paper does not make a distinction between the proposed approach from MoE training. And if there is indeed a difference, please include the MoE baseline. \n- **Major issue**: The paper compares their model trained on downstream tasks with other pre-trained models, zero-shot on the downstream tasks. Therefore, it is not making an apples-to-apples comparison\n- It would be important to add in a couple of baselines to showcase the benefit of the proposed method over others\n  - A single model trained on all the data that the base, router, and individual experts are trained on\n  - MoE baseline trained on the data used to train the IP-LM. \n- Why are there no entries in the table for some models on C-Eval?\n- There are not many details provided on training, model architecture, and dataset. Unclear what data is additionally used to train the base model. What is the architecture of the model? \n- The writing in the paper is clear but not precise. For example, the abstract or intro does not tell you anything concrete about what the paper builds, it only goes so far as to specify the problem and motivate it."
            },
            "questions": {
                "value": "Please take a look at the section on weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents IP-LLM inspired from MoE which use routing mechanism to enable continual learning of multi-domain tasks and memory efficient inference. The authors use segmented pre-training strategy to train a base block to acquire general linguistic skills and then train router and domain-specific modules.\nThe authors evaluate IP-LLM's performance on 4 different tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors propose a novel pre-training strategy to parse the general linguistic comprehension skills in the base model and later train individual experts on top on different domains."
            },
            "weaknesses": {
                "value": "1. Evaluation\n\n- I think the biggest issue in current manuscript is evaluation of the model. While the current evaluation only includes monolithic architectures without MoE strategy, wouldn't it be fairer to include the models using MoE's in terms of both performance and memory/compute efficiency? \n\n- The authors claim in the list of contributions that the new approach allows higher routing accuracy but I cannot find the explicit result for this.\n\n- Also the authors claim the memory and training efficiency but having a explicit numerical comparison and what exact 'training cost' is meant here. \n\n2. Related works on lifelong learning / continual learning using MOE\n\n- I believe there are already few literatures on lifelong learning of LLM using MoE e.g Chen et al (2023) https://arxiv.org/pdf/2305.12281. I suggest authors to incorporate more relevant literatures and what is the novelty of their method. \n\n3. Paper presentation should be improved\n\n- There are many typos and inconsistent citing notations which makes readability very low. For example, I found Section 2 related work very hard to parse the included citations (spacing, parentheses etc). I highly recommend authors to do careful proofreading of the entire manuscript. \n\n- Section 4 training strategy can be improved with adding a schema for better delivery."
            },
            "questions": {
                "value": "I believe my comments about the possible improvements and weaknesses incorporate my questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}