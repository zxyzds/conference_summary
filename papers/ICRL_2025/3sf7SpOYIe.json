{
    "id": "3sf7SpOYIe",
    "title": "ACUS: Audio Captioning with Unbiased Sliced Wasserstein Kernel",
    "abstract": "Teacher-forcing training for audio captioning usually leads to exposure bias due to training and inference mismatch. Prior works propose the contrastive method to deal with caption degeneration. However, the contrastive method ignores the temporal information when measuring similarity across acoustic and linguistic modalities, leading to inferior performance. In this work, we develop the temporal-similarity score by introducing the unbiased sliced Wasserstein RBF (USW-RBF) kernel equipped with rotary positional embedding to account for temporal information across modalities. In contrast to the conventional sliced Wasserstein RBF kernel, we can form an unbiased estimation of USW-RBF kernel via Monte Carlo estimation. Therefore, it is well-suited to stochastic gradient optimization algorithms, and its approximation error decreases at a parametric rate of $\\mathcal{O}(L^{-1/2})$ with $L$ Monte Carlo samples. Additionally, we introduce an audio captioning framework based on the unbiased sliced Wasserstein kernel, incorporating stochastic decoding methods to mitigate caption degeneration during the generation process. We conduct extensive quantitative and qualitative experiments on two datasets, AudioCaps and Clotho, to illustrate the capability of generating high-quality audio captions. Experimental results show that our framework is able to increase caption length, lexical diversity, and text-to-audio self-retrieval accuracy. We also carry out an experiment on two popular encoder-decoder audio captioning backbones to illustrate that our framework can be compatible with a diversity of encoder-decoder architectures.",
    "keywords": [
        "audio captioning",
        "exposure bias",
        "multimodal learning"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We develop the audio captioning with an unbiased sliced Wasserstien kernel to alleviate caption degeneration",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=3sf7SpOYIe",
    "pdf_link": "https://openreview.net/pdf?id=3sf7SpOYIe",
    "comments": [
        {
            "title": {
                "value": "Response to reviewer 2JoP (2/2)"
            },
            "comment": {
                "value": "**Question 1:** There are some minor errors, like in line 187, it should be ( \\nu = \\frac{1}{N} \\sum_{j=0}^N \\delta_{z_y^j} ), not ( \\nu = \\frac{1}{M} \\sum_{j=0}^N \\delta_{z_y^j} ), to make two empirical distributions have the same number of supports.\n\nWe would like to thank the reviewer for pointing out these typos to improve our submission. We will update our manuscript accordingly based on your suggestion.\n\n**Question 2:** Could you provide more analysis on how the proposed method performs in more challenging scenarios (e.g., multi-speaker or noisy environments) to better highlight its strengths? If not, do you believe that temporal information is important in audio captioning task?\n\nThose mentioned benchmarks and settings are irrelevant to our work, therefore, we do not think our proposed method should be evaluated on the suggested benchmarks. The explanations are articulated in my aforementioned response.\nWe conducted experiments to compare our proposed method against the contrastive learning and baseline methods, which are incapable of considering temporal information. Our method remarkably outperforms baseline methods in terms of quantitative and qualitative metrics for the audio captioning task in the section 5.1 and 5.2, respectively. We also provide qualitative examples in Appendix. A4 to demonstrate that our method is capable of generating more high-quality captions against baseline methods. According to experimental results, the temporal information is crucial for the audio captioning task, therefore, it should be taken into account when measuring the similarity between acoustic and linguistic modalities for encoder-decoder audio captioning models.\n\n**Question 3:**  The application area of this work seems limited. \n\nI think that applying our method for multilingual audio captioning would be great. If there is a standard dataset for multilingual audio captioning, we would like to evaluate our method on that dataset to show its effectiveness. There is no benefit to utilizing our method for the automatic speech recognition (ASR) task. We developed the ACUS framework to deal with the local temporal distortion issue for text-audio alignment, and the ASR task does not suffer from this issue. The reason is that ASR task requires a monotonic alignment across acoustic features and ground-truth transcript, therefore there is no benefit of leveraging our method for ASR.\n\nOur proposed kernel method is a non-parametric method and works on the output space of the audio encoder and text decoder. Thus, our method can be seamlessly utilized with the multilingual audio captioning tasks for encoder-decoder models without any modification in model's architectures. \n\n**Question 4:**  Since you use stochastic decoding strategies in the inference stage, which may lead to high computational costs, and the reported score in your results is not very decent, we might not need such a high-cost method to get a minor improvement. Thus, could you provide more details on the differences in diversity and quality of captions generated by your approach?\n\nWe understand the reviewer's concern regarding the trade-off between efficiency and effectiveness. Thus, we would like to provide a comparison of inference time between our method and the traditional approaches. The inference time is measured by using real-time-factor (RTF) metric, which measures how many seconds a method needs to generate a caption.\n\n| Method     | Inference time on a A6000 GPU (RTF) |\n|------------|----------------------------|\n| MLE        | 0.33                                |\n| MLE + contrastive learning   | 0.65                            |\n| MLE + ACUS (ours) | 0.76                         |\n\nAs illustrated in the above table, our framework is able to generate audio captions real time, therefore, it can be utilized for real-world applications. The inference time of our method increases twice compared with the baseline method, however, our method is able to generate more diverse and high-quality captions. We conduct extensive qualitative experiments in the Section 5.2(both subjective and objective experiments)  to demonstrate the high-quality of generated captions by leveraging our proposed framework against traditional methods. Our method can improve caption length, lexical diversity, and semantic of generated captions, as shown in the Table. 3, and the ACUS framework substantially enhances the descriptiveness and correctness of generated caption, evaluated by human evaluation, in the Table. 4. Furthermore, we also provide qualitative examples in the Appendix. A4."
            }
        },
        {
            "title": {
                "value": "Response to reviewer 2JoP (1/2)"
            },
            "comment": {
                "value": "We appreciate the valuable time and feedback of reviewer to improve your work. We have revised our manuscript and would like to address your concerns outlined below\n\nBefore addressing the review's concerns, we would like to emphasize the main contribution of our work is that *we develop a framework method to address the exposure bias of current training frameworks, maximum likelihood and contrastive learning, for audio captioning models. Our contribution is a new framework rather than a new model for the audio captioning task*. We dedicated a section 3.2 and the Figure. 1 to detail the training and inference procedure of our proposed framework. The Figure.1 demonstrates how the proposed USW-RBF kernel is integrated into encoder-decoder architectures for training and inference stages. Furthermore, we would like to argue that audio and speech are similar to some extent, but they are different. Speech tasks focus on understanding the linguistic and speaker information of audio. On the other hand, audio tasks focus on understanding acoustic information and sound events of audio.\n\n**Weakness 1:** Evaluate our method on speech benchmarks: SUPERB, Dynamic-SUPERB, and SpeechCaps\n\nAll those mentioned benchmarks are designed to evaluate the transferable capability of pretrained speech models which is totally irrelevant to our work, our work mainly focuses on solving the exposure bias for audio captioning. Even though speech and audio are similar to some extent, they are two different. I would like to articulate the reasons why these benchmarks are not suitable for evaluating our proposed method as follows. Both SUPERB and Dynamic-SUPERB are used to evaluate the transferable capability of universal pretrained speech models, which is irrelevant to the main research question of our work, solving the exposure bias issue for audio captioning models. The second reason is that we propose a new framework to deal with the exposure bias of audio captioning tasks rather than a new model or architecture for transfer learning, therefore, there is no connection between our work and SUPERB and Dynamic-SUPERB benchmarks. The SpeechCaps benchmark is designed to evaluate the understanding capabilities of speech models in prosodic information. Thus, it is not appropriate to use for benchmarking our method.\n\n**weakness 2:** lack of sufficient details on how this kernel is integrated within the overall model architecture\n\nWe detailed the training and inference stages of our proposed framework in the section 3.2. We also demonstrated how the proposed kernel is integrated with the encoder-decoder architecture for training and inference with audio captioning models in the Figure. 1."
            }
        },
        {
            "title": {
                "value": "Response to reviewer bPjz (2/2)"
            },
            "comment": {
                "value": "**Question 1:** How does the performance of USW-RBF compare with other non-Wasserstein-based kernels for audio captioning tasks?\n\nWe compare with soft-DTW and Dynamic Time Wrapping (DTW) methods which are equivalent to non-Wasserstein kernels as mentioned in [1,2]. The experimental results are illustrated in the Table. 5. As discussed thoroughly in our work, DTW and soft-DTW are incapable of handling local temporal distortion for the audio captioning task, while our kernel is able to deal with the distortion. Thus, our proposed kernel substantially outperforms DTW and soft-DTW for measuring similarity across acoustic and linguistic modalities for the audio captioning task.\n\nReferences:\n\n[1] A kernel for time series based on global alignments\n\n[2] Fast Global Alignment Kernels\n\n**Question 2:** What are the potential trade-offs between the accuracy improvements and the computational costs introduced by Monte Carlo sampling and stochastic gradient optimization in ACUS?\n\nWe conducted an ablation study regarding trade-offs between efficiency and effectiveness of our method regarding Monte Carlo sampling in the Table. 8 in the Appendix.3.  According to proposition 3, the approximation rate of USW-RBF kernel is $\\mathcal{O}(L^{-1/2})$ which suggests that we might not need a very large number of Monte Carlo samples to obtain a good approximation. As shown in the Table. 8, our proposed kernel can improve the performance of the baseline method significantly with a decent number of Monte Carlo samples $L=50$. I would like to illustrate the experimental results below for clarity:\n\n| Dataset   | Method               | METEOR       | ROUGE_L      | CIDEr        | SPICE        | SPIDEr       |\n|-----------|----------------------|--------------|--------------|--------------|--------------|--------------|\n| AudioCaps | Enclap + MLE         | 0.254        | 0.5          | 0.77         | 0.186        | 0.48         |\n|           | Enclap + ACUS (L=50) | 0.262$\\pm$0.001 | 0.509$\\pm$0.001 | 0.807$\\pm$0.003 | 0.192$\\pm$0.001 | 0.5$\\pm$0.002  |\n| Clotho    | Enclap + MLE         | 0.182        | 0.38         | 0.417        | 0.13         | 0.273        |\n|           | Enclap + ACUS (L=50) | 0.186$\\pm$0.001 | 0.38$\\pm$0.001 | 0.419$\\pm$0.004 | 0.133$\\pm$0.001 | 0.275$\\pm$0.003 |\n\n**Question 3:**\n\nWe choose rotary positional embedding based on the ablation study in the table. 6. There are a lot of the same observations[1, 2] aligned with our ablation study, which is the rotary PE outperforming absolute PE in encoding positional information for sequential data. We believe that if we have a better positional encoding(PE) technique, the PE technique can enhance our proposed method.\n\nReferences:\n\n[1] The llama 3 herd of models\n\n[2] Palm: Scaling language modeling with pathways\n\n**Question 4:** Why audio captioning? Can the method be useful to other tasks? Audio understanding? Speech Recognition?\n\nWe chose the audio captioning task to show the effectiveness of our proposed framework for two reasons. The first reason is that the audio caption task requires cross-modal alignment, in which temporal information from each modality is crucial to consider when measuring cross-modal similarity between acoustic and linguistic features to generate high-quality captions. The second reason is that the cross-modal alignment for the audio captioning task is not a monotonic alignment. Thus, the traditional method, like Dynamic Time Wrapping (DTW), can not handle non-monotonic alignments since local temporal distortion exists.\n\nThe audio understanding and audio question-answering tasks are similar to the audio captioning task. All of them require measuring cross-modal alignment to better generate either a caption or answer for a given audio, therefore, our method is well-suited for these tasks. I do not see any benefits in applying our method to the speech recognition task. The speech recognition task requires measuring monotonic alignment between acoustic and linguistic features, and our method is not better than traditional methods, such as DTW, for handling monotonic alignments."
            }
        },
        {
            "title": {
                "value": "Response to reviewer bPjz (1/2)"
            },
            "comment": {
                "value": "We appreciate the valuable time and feedback of reviewer to improve your work. We have revised our manuscript and would like to address your concerns outlined below\n\n**Weakness 1:** which contrastive method and how does it ignore \"the temporal information when measuring similarity across acoustic and linguistic modalities\"?\n\nWe would like to thank the reviewer for helping us improve the quality of the submission. We will definitely rephrase these sentences to make our abstract more clear . We mention the classical contrastive learning (CL) method for multimodal learning in this work. The relevant CL works for audio captioning are cited in our introduction for clarity. The main drawback of CL method is performing an average-pooling operation before measuring the cosine similarity across acoustic and linguistic modalities. Thus, the temporal information in acoustic and linguistic features are discarded due to the average-pooling operation. Our method avoids this drawback by directly measuring the discrepancy between two sequences.\n\n**Weakness 2:** The Monte Carlo sampling and stochastic gradient optimization may increase computational costs.\n\nThank you for your insightful comments. We understand the reviewer's concern about the computational cost of the Monte Carlo sampling. However, sampling projecting directions for sliced Wasserstein is very computationally efficient i.e., it is equivalent to sampling from a standard multivariate Gaussian. Moreover, projecting directions do not need to be resampled multiple times i.e., we can reuse the projecting directions for some iterations before resampling. Finally, from Proposition 3, the approximation rate is $\\mathcal{O}(L^{-1/2})$ which suggests that we might not need a very large number of Monte Carlo samples to obtain a good approximation.\n\n**Weakness 3:** missing prefix-tuning baselines for the audio captioning task\n\nThank you for pointing out the relevant prefix-tuning baselines for audio captioning. We will update the experiment section to include all relevant baselines based on your suggestion. Also, we would like to emphasize that our proposed method still significantly outperforms all prefix-tuning baselines for audio captioning.\n\n| Dataset   | Method      | METEOR       | ROUGE_L      | CIDEr        | SPICE        | SPIDEr       |\n|-----------|-------------|--------------|--------------|--------------|--------------|--------------|\n| AudioCaps | [1]         | 0.256        | 0.525        | 0.751        | 0.186        | 0.471        |\n|           | [2]         | 0.240        | 0.503        | 0.733        | 0.177        | 0.455        |\n|           | ACUS (ours) | 0.262 $\\pm$ 0.001 | 0.509 $\\pm$0.001 | 0.807 $\\pm$0.003 | 0.192 $\\pm$0.001 | 0.5 $\\pm$0.002 |\n| Clotho    | [1]         | 0.177        | 0.395        | 0.411        | 0.125        | 0.224        |\n|           | [2]         | 0.170        | 0.378        | 0.392        | 0.118        | 0.255        |\n|           | ACUS (ours) | 0.186$\\pm$0.001 | 0.38$\\pm$0.001  | 0.419$\\pm$0.004 | 0.133$\\pm$0.001 | 0.275$\\pm$0.003 |\n\nCitations\n\n[1] https://ieeexplore.ieee.org/abstract/document/10448030.\n\n[2] https://ieeexplore.ieee.org/abstract/document/10096877."
            }
        },
        {
            "summary": {
                "value": "This paper introduces ACUS (Audio Captioning with Unbiased Sliced Wasserstein kernel), a novel framework that addresses exposure bias and temporal misalignment issues in audio captioning systems. The key technical contribution is the development of an unbiased sliced Wasserstein RBF (USW-RBF) kernel equipped with rotary positional embeddings, which effectively measures similarity between acoustic and linguistic features while preserving temporal information. Experimental results on AudioCaps and Clotho datasets demonstrate significant improvements over state-of-the-art methods across multiple metrics."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper introduces an unbiased sliced Wasserstein RBF (USW-RBF) kernel that effectively handles temporal information across modalities while avoiding dimensionality curse issues that affect traditional Wasserstein distances.\n\nStrong Theoretical Foundation: Provides formal proofs for the kernel's properties (positive definiteness, unbiasedness).\nDemonstrates convergence rate for Monte Carlo estimation.\n\nComprehensive Evaluation: Tests on multiple datasets (AudioCaps and Clotho). Uses both automatic metrics and human evaluation\nIncludes detailed ablation studies for various components.\n\nAchieves state-of-the-art performance on multiple metrics"
            },
            "weaknesses": {
                "value": "No analysis of computational overhead from the USW-RBF kernel\n\nUnclear how the method performs on longer audio sequences\n\nWhile ablation studies are included, there's limited discussion of how sensitive the method is to various hyperparameters\nCould benefit from more guidance on hyperparameter selection for new datasets.\n\nLacks detailed analysis of failure cases"
            },
            "questions": {
                "value": "How does the computational complexity scale with audio length and batch size compared to baseline methods?\nHow robust is the method to different audio qualities or noise levels? Was this tested?\nWhat is the impact of different positional embedding choices on the final performance? While rotary embeddings performed best, is there a theoretical justification for this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel approach for audio captioning to address issues related to exposure bias and temporal misalignment. Here\u2019s a summary of the key contributions:\n1. Unbiased Sliced Wasserstein RBF Kernel (USW-RBF): The authors propose a novel kernel method to accurately measure cross-modal similarity between audio and textual data. This kernel, equipped with rotary positional embedding, captures temporal information more effectively than traditional methods, addressing limitations like dimensionality and temporal distortion.\n2. Mitigating Exposure Bias: ACUS employs stochastic decoding techniques, such as nucleus and top-k sampling, to reduce exposure bias during the inference stage, enhancing caption diversity and quality. This is achieved by leveraging the USW-RBF kernel to improve alignment between generated captions and audio inputs.\n3. Extensive Evaluation and Compatibility: The framework\u2019s efficacy is validated through experiments on two datasets, AudioCaps and Clotho. Results demonstrate improved caption length, lexical diversity, and self-retrieval accuracy, and compatibility with diverse encoder-decoder architectures.\n\nIn essence, ACUS represents an advancement in audio captioning by integrating the unbiased USW-RBF kernel with stochastic decoding, leading to more descriptive and temporally coherent audio captions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The introduction of the unbiased sliced Wasserstein RBF (USW-RBF) kernel, which captures temporal information across audio and text modalities, is an advancement. By accounting for temporal alignment, it addresses limitations in prior contrastive methods that often ignore the temporal structure of audio data.\n\t\n2. ACUS effectively addresses exposure bias\u2014a common issue in captioning tasks\u2014by combining the USW-RBF kernel with stochastic decoding methods. This approach ensures that generated captions maintain diversity and relevance across varying contexts.\n\t\n3. The ACUS framework enhances not only the length and diversity of captions but also their semantic alignment with audio events. By capturing temporal details, it generates more descriptive and meaningful captions, which are validated by both quantitative metrics and qualitative assessments.\n\n4. The paper thoroughly derives and proves the properties of the USW-RBF kernel, reinforcing its validity for multimodal tasks. Additionally, by introducing a practical approach to reducing exposure bias, it offers a methodological contribution that may extend beyond audio captioning to other sequence generation tasks."
            },
            "weaknesses": {
                "value": "1. While the paper introduces a promising method, the improvements observed in the current experiments are relatively modest. To more rigorously validate the effectiveness of your approach, I recommend evaluating your model on additional, more challenging benchmarks, which may make this work more convincing if your method reach higher score in these benchmarks, such as:\n\na. SUPERB: This benchmark includes a broad range of speech processing tasks, covering content, speaker, semantics, and paralinguistics. It would provide a comprehensive baseline, helping clarify how well your model generalizes across core speech tasks.\n\nb. Dynamic-SUPERB: This benchmark extends SUPERB with instruction-tuning and zero-shot tasks, pushing models to handle more complex and varied speech processing scenarios. Testing on Dynamic-SUPERB could demonstrate your method\u2019s robustness and adaptability in handling multi-task and instruction-following requirements, offering deeper insights into its generalization capabilities.\n\nc. SpeechCaps: Given the emphasis in your work on speaker-specific and temporal information, SpeechCaps offers a relevant test for multi-talker and speaking style captioning. Its focus on speaker and prosodic information could highlight the strengths of your model in more intricate, real-world audio scenarios, such as multi-speaker dialogues and expressive speech.\n\n2. Authors provide a detailed explanation of the USW-RBF kernel. However, it lacks sufficient details on how this kernel is integrated within the overall model architecture. You can try these to make it better, such as:\n\na. Integration Details: Please provide a clearer, step-by-step description of how the USW-RBF kernel is incorporated into the model pipeline. \n\nb. Diagram or Flowchart: Consider adding a diagram or flowchart that visualizes the integration process, illustrating where and how the USW-RBF kernel interacts with audio and textual embeddings within the architecture."
            },
            "questions": {
                "value": "1. There are some minor errors, like in line 187, it should be \\( \\nu = \\frac{1}{N} \\sum_{j=0}^N \\delta_{z_y^j} \\), not \\( \\nu = \\frac{1}{M} \\sum_{j=0}^N \\delta_{z_y^j} \\), to make two empirical distributions have the same number of supports. I did not thoroughly inspect every math part in this paper, but I think authors could check the whole paper again thoroughly.  Also, in conclusion, it should be \"unbiased kernel\", not \"unbias kernel\". (No worries, they are just minor error, but it is better to correct for clarity.)\n\n2. As I mentioned in weakness part, the reported improvements over baselines appear modest. Could you provide more analysis on how the proposed method performs in more challenging scenarios (e.g., multi-speaker or noisy environments) to better highlight its strengths? If not, do you believe that temporal information is important in audio captioning task? \n\n3. The application area of this work seems limited. Do you have any plans to extend this work for multilingual audio captioning or automatic speech recognition? If so, how might the kernel method adapt to language diversity in audio processing? how you modify the text embedding under multilingual scenario? \n\n4. Since you use stochastic decoding strategies in inference stage, which may lead to high computational costs, and the reported score in your results is not very decent, we might not need such a high-cost method to get a minor improvement. Thus, could you provide more details on the differences in diversity and quality of captions generated by your approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new framework for audio captioning designed to mitigate exposure bias and improve temporal cross-modal similarity measurement. The authors claim that traditional audio captioning models trained via maximum likelihood estimation face exposure bias, leading to \"degeneration\" in generated captions. This paper introduces the unbiased sliced Wasserstein (USW-RBF) kernel equipped with rotary positional embedding to capture temporal information across acoustic and linguistic modalities, thereby reducing exposure bias. The authors show improvements on benchmark datasets (AudioCaps and Clotho)"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is decently written and easy to follow for an expert. However, I would like to mention it might be difficult for a traditional audio captioning community to read. A bit more background on the 1, the unbiased sliced Wasserstein RBF kernel would have been appreciated.  The equations could have been improved by defining the notations better. Fro examples, if I want to read Eqtn 9., I need to find what the notations mean somewhere else in the paper.\n- The problem handled in the paper is new. The approach is also novel. ACUS combines the USW-RBF kernel with stochastic decoding methods like nucleus and top-k sampling to alleviate exposure bias during inference. A lot of work in audio captioning ideally propose new architectures. This paper brings a fresh perspective in the problem space.\n- The evaluation is sound. The 2 usual benchmark datasets are used and it is also combined with human evaluation. The metrics of descriptiveness, correctness, and fluency are good metrics for comparison as ideal benchmark metrics seem to have saturated and require style memorization."
            },
            "weaknesses": {
                "value": "- The abstract says\" \"Prior works propose the contrastive method to deal with caption degeneration. However, the contrastive method ignores the temporal information when measuring similarity across acoustic and linguistic modalities, leading to inferior performance.\" -- which contrastive method and how does it ignore the \"the temporal information when measuring similarity across acoustic and linguistic modalities\"? This first line of the paper is very difficult to understand.\n- The Monte Carlo sampling and stochastic gradient optimization may increase computational costs, potentially impacting efficiency in real-world large-scale applications.\n- While I understand that the authors focus on Enc-Dec framework, a good number of baselines were missed for evaluation. ACUS can act as complimentary to most other methods proposed in literature as all methods require an audio encoder and a language decoder (including prefix based architectures). Thus, some baselines were missed. See [1,2] as examples and papers compared in [1,2].\n- The analysis section is just ablations. A deeper analysis section (see questions below) would have strengthened the paper.\n\n\n### Citations\n\n[1] https://ieeexplore.ieee.org/abstract/document/10448030.   \n[2] https://ieeexplore.ieee.org/abstract/document/10096877."
            },
            "questions": {
                "value": "- How does the performance of USW-RBF compare with other non-Wasserstein-based kernels for audio captioning tasks?\n- What are the potential trade-offs between the accuracy improvements and the computational costs introduced by Monte Carlo sampling and stochastic gradient optimization in ACUS?\n- Why was the rotary positional embedding favored over other encoding techniques, and could alternative embeddings further enhance the results?\n- Why audio captioning? Can the method be useful to other tasks? Audio understanding? Speech Recognition?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel framework tackling the training-inference mismatch in automated audio captioning (AAC) by introducing a temporal-similarity score based on the unbiased sliced Wasserstein RBF (USW-RBF) kernel with rotary positional embeddings. By integrating this score with a stochastic decoding strategy, the approach effectively addresses caption degeneration issues encountered during inference. Experimental results on established AAC benchmark datasets demonstrate notable improvements in model performance, validated through both quantitative and qualitative metrics."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper introduces a novel temporal-similarity score, utilizing the unbiased sliced Wasserstein RBF (USW-RBF) kernel with rotary positional embeddings, to mitigate exposure bias in audio captioning models. Unlike prior research, which has not employed the USW-RBF kernel for cross-modal similarity calculation, this study leverages it to capture temporal dynamics more effectively.\n2. The proposed framework is adaptable to a wide range of existing AAC models, with experimental results underscoring its effectiveness in improving model performance.\n3. Comprehensive qualitative and quantitative experiments support the method's efficacy. Ablation studies comparing various similarity metrics further highlight the advantages of the USW-RBF kernel over alternative approaches."
            },
            "weaknesses": {
                "value": "1.\tCombining the USW-RBF kernel with stochastic decoding strategies may lead to high computational costs. For example, at inference time, generating $\\mathcal{B}$ candidate captions through stochastic decoding results in increased computational time. While the paper demonstrates effectiveness, it lacks a detailed discussion of computational overhead in training and inference.\n2.\tThe performance increase with the proposed approach is relatively minor. According to the original paper, EnCLAP-large achieved SPIDEr scores of 49.5 and 27.8 on AudioCaps and Clotho, while the proposed method reached only 50.0 and 27.5, making the claimed improvement less convincing. According to this comparison, the exposure bias is not that important in AAC tasks.\n3.\tThe application scope of this work appears limited, focusing primarily on AAC tasks. The authors have not explored the framework\u2019s performance on other audio-text multimodal tasks, such as audio-text retrieval, and automatic speech recognition. For instance, can the proposed temporal-similarity score enhance the temporal reasoning capability of the CLAP model?\n4.\tThe study does not deeply explore the sensitivity of the framework to key hyper-parameters, such as the coefficient $\\alpha$ in the objective function or the number of Monte Carlo samples $L$ used for the USW-RBF kernel.\n5.\tThe paper dedicates considerable space to explaining the USW-RBF kernel but provides a limited description of how it integrates with the model itself. For example, it\u2019s unclear whether the text embedding is derived from the penultimate layer of the text decoder or from another layer.\n6.\tAlthough the paper separates AAC models into encoder-decoder and prefix-tuning architectures, with experiments performed only on the encoder-decoder type. The difference between these two types of architecture is not substantial. Both approaches essentially share the same structure of an audio encoder and a text decoder.\n(Minor problems - Line 44: the former architecture \u2192 the latter architecture)"
            },
            "questions": {
                "value": "1. Could the authors provide a more detailed explanation of the differences between the two types of AAC architectures? Additionally, could the proposed method be adapted for application within prefix-tuning structures?\n2. What is the increase in computational cost introduced by the framework? For example, how much additional inference time is required when using stochastic decoding to generate $\\mathcal{B}$ candidate captions?\n3. Considering the advanced reasoning and generative capabilities of large language models, frequently used in AAC tasks, could the proposed approach be adapted to work alongside LLMs to achieve higher-quality captions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper is written to solve the exposure bias problem in audio captioning.\nThey propose the unbiased sliced Wasserstein RBF kernel, which is a better cross-modality similarity measure.\nTogether with the contrastive learning method, gains are observed in the audio captioning tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper propose the unbiased sliced Wasserstein kernel framework to improve the audio captioning performance."
            },
            "weaknesses": {
                "value": "The motivation of the paper does not sound convincing.\nExposure bias is a general problem to auto-regressive networks. \nIt is not a critical problem for audio captioning.\nIn general, exposure bias can be mitigated by a better training of model. \nI believe using a larger decoder will ease the problem a lot.\nSpecifically, I do not see in the paper how much the exposure bias problem is harming the audio captioning performance.\nEven if we regard this as a serious problem, reinforcement learning (RL) should be a popular way to solve it as RL trains the model according to its inference output. The paper doesn't discuss about RL and address the audio captioning problem in a very narrow perspective."
            },
            "questions": {
                "value": "As proposing the unbiased kernel is the core technical contribution in the paper, how much gain is there from a biased kernel to a unbiased kernel ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}