{
    "id": "SI6zocV2SS",
    "title": "CAN - CONTINUOUSLY ADAPTING NETWORKS",
    "abstract": "Catastrophic forgetting is a fundamental challenge in neural networks that prevents continuous learning, which is one of the properties essential for achieving true general artificial intelligence. When trained sequentially on multiple tasks, conventional neural networks overwrite previously learned knowledge, hindering their ability to retain and apply past experiences. However, people and other animals can learn new things continuously without forgetting them. To overcome this problem, we devised an architecture that preserves significant task-specific connections by combining selective neuron freezing with Hebbian learning principles. Hebbian learning enables the network to adaptively strengthen synaptic connections depending on parameter activation. It is inspired by the synaptic plasticity seen in brains. By preserving the most important neurons using selective neuron freezing, new tasks can be trained without changing them. Experiments conducted on standard datasets show that our model significantly reduces the risk of catastrophic forgetting, allowing the network to learn continually.",
    "keywords": [
        "Continual Learning",
        "Catastrophic Forgetting",
        "Synaptic Plasticity",
        "Hebbian Learning",
        "Adaptive Neural Networks"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "A new architecture using Hebbian Learning and Selective Training that enables Continual Learning",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=SI6zocV2SS",
    "pdf_link": "https://openreview.net/pdf?id=SI6zocV2SS",
    "comments": [
        {
            "summary": {
                "value": "In this paper, the authors attempt to address the continual learning problem by selectively freezing important neurons when training on a new task. In contrast to previous continual learning methods, important neurons are identified with Hebbian learning using a network of the same architecture. The model is then evaluated on a simple MNIST task and is shown to significantly outperform a vanilla model."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The idea of utilizing Hebbian learning to help the model continually learn is interesting, as Hebbian learning is a well-known synaptic plasticity rule in animals. Experiments might provide insights into why these mechanisms can help animals continually learn."
            },
            "weaknesses": {
                "value": "1. Overall the motivation behind the method is not well explained. The motivation behind Hebbian learning is to take inspiration from animals, but it is not clear to me why we want to use a separate network evolved with Hebbian learning to compute the importance score. It's unclear how this dual-network design is linked to biological continual learning.\n2. More importantly, the experiments are way too lacking. The method is only tested on MNIST with 0-4/5-9 as two separate tasks and only compared to the vanilla network. The paper mentioned quite a lot of previous work but they are not compared as baselines. It's understandable that these bio-inspired methods might not surpass SOTA methods in the field but at least the method should be tested on a range of scenarios against some reasonable baseline."
            },
            "questions": {
                "value": "In addition to the weaknesses mentioned above:\n1. How is this linked to previous methods that also utilize important scores and what's the advantage of Hebbian learning here? I also expect more evidence that the computed importance score can actually identify important neurons, for example, how does the method compare to pre-assigning a set of neurons for each task?\n2. How is the network tested on different tasks? Is the whole network used in each task or some mask is used even in the forward pass?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper connects the Hebbian learning with neuron freezing, computing neuron importance through a Hebbian layer and selectively freezing neuron weights based on importance scores. Subsequently, it uses hooks to selectively train and infer, effectively assigning different neurons for each task. The paper combines biological neural mechanisms with continual learning, providing some insights. However, it does not sufficiently summarize previous work, and the novelty of this idea is limited. Additionally, the figures and tables in the paper are quite rudimentary, lacking details on model training and testing. The results are not comprehensively compared with existing methods, which makes them unconvincing."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper connects the Hebbian learning with neuron isolation, selectively freezing neuron weights based on importance calculation, which provides some valuable insights."
            },
            "weaknesses": {
                "value": "The writing of this paper is poor, lacking a summary of related work in the field. There are many existing methods that combine Hebbian learning with weight updates, but the paper fails to mention the limitations of current methods or the advantages of the proposed approach. Additionally, the figures and tables are quite rudimentary, making it difficult to understand the authors' intent. The experimental results lack detail, including mathematical descriptions and specific training and testing procedures. More importantly, there is no extensive and comprehensive comparison, and the limited experimental results are insufficient to demonstrate the effectiveness of the model."
            },
            "questions": {
                "value": "1. There have been many continual learning methods based on Hebb learnin before; what advantages does the proposed method have over these previous works? \n2. What are the specific implementations of the model training and testing process, and can they be described in detailed mathematical terms? \n3. Are there more comprehensive and detailed comparison results available in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors proposed a novel algorithm named \u201cCAN\u201d that can dynamically freeze weights to prevent catastrophic forgetting. CAN uses Hebbian rule to measure the importance of neurons with presented tasks. While the dynamic gating of gradient flow onto individual neurons may sound interesting, this study does not provide any convincing evidence that this algorithm can be used in modern deep learning models."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The use of Hebbian rule when evaluating the neurons' importance is interesting."
            },
            "weaknesses": {
                "value": "1. The authors did not provide specific details that are crucial for their study. Specifically, the second equation does not include any indices, which makes its interpretation difficult, and the architecture of a neural network used in this study is not specified. \n\n2. The authors split MNIST into two disjoint tasks. The experimental setup is too simple to evaluate the algorithm properly. \n\n3. Although multiple earlier studies used MNIST to evaluate continual learning algorithms, the authors did not provide any comparison to other studies."
            },
            "questions": {
                "value": "\u201cCAN\u201d evaluates the importance of individual neurons sequentially, but can the authors evaluate its complexity? Modern deep learning models have a massive number of neurons, and if CAN requires too much computations, it may not be practical."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides a method to prevent catastrophic forgetting, with externally-defined task boundaries, using Hebbian learning to enable/disable neurons that were relevant for a prior task."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "If there is a good conceptual reasoning behind the proposed use of Hebbian learning (see my comment on the matter under \"weaknesses\"), the direction explored in the paper may be promising."
            },
            "weaknesses": {
                "value": "1) While experiments demonstrate that Hebbian learning mechanism prevents catastrophic forgetting, it is not clear why (conceptually) this would be the case. In other words, why does association by activity select the nodes that are relevant for one task while excluding those that belonged to past ones? It may be my missing backgeound in the application of Hebbian learning, but I recommend authors to include a more detailed explanation of this part of the conceotual background for people who are lacking this background.\n\n2) All experiments are basically the same measured with different metrics. One of the metrics would suffice.\n\n3) Task identification and new task recognition is not a trivial task. Without this, it is questionable whether any method can be called to realize continual learning or not. (The authors mention that this is not a challenge they are tackling, but without this ffull continual learning setup the relevance of a destructive adaptation problem is questionable.)"
            },
            "questions": {
                "value": "See point (1) above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}