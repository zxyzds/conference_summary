{
    "id": "oSEsSDFxyw",
    "title": "DETER: Detecting Edited Regions for Deterring Generative Manipulations",
    "abstract": "Generative AI capabilities have grown substantially in recent years, raising renewed concerns about the potential malicious use of generated data, or \"deep fakes.\" Despite being a longstanding and important research topic, deep fake detection research on most existing datasets has not kept pace with generative AI advancements sufficiently to develop detection technology that can meaningfully alert human users in real-world settings. In this work, we introduce DETER, a large-scale dataset for DETEcting edited image Regions and deterring modern advanced generative manipulations. After a comprehensive study of prior literature, our proposed dataset makes contributions along three main axes: the upgrade on modern manipulations via the state-of-the-art generative models; the mitigation of biased spurious correlations in prior deep fake datasets; and a more unified formulation suitable for various detection models in different granularities. Equipped with DETER, we conduct extensive experiments and detailed analysis using our rich annotations and improved benchmark protocols, revealing future directions and the next set of challenges in developing reliable regional fake detection models.",
    "keywords": [
        "Deepfake detection",
        "regional manipulation",
        "dataset and benchmark"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We introduce a large-scale image dataset constructed with state-of-the-art generators for regional manipulations in different granularities.",
    "creation_date": "2024-09-21",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=oSEsSDFxyw",
    "pdf_link": "https://openreview.net/pdf?id=oSEsSDFxyw",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a new dataset, which includes different and new deepfake tasks like inpainting. It includes the images edited by two GANs and two Diffusion models. The size of the dataset is 30,000. Through human studies and GPT-4 evaluation, the paper shows the difficulty of prediction and detection over DETER by human beings and GPT-4. Through further benchmarking, the paper shows how incorporating unseen and unaltered images improves model performance. It also delivers insights like the regions edited by inpainting is counterintuitively hard to predict."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper presents a new dataset of 30,000 images, which is larger and incorporates different granularities for image editing. It also includes masks for more accurate evaluations.\n2. The paper includes many different models for predictions and detection."
            },
            "weaknesses": {
                "value": "1. In the user study, it is a bad idea to include \"I'm not sure\" as an option, which will greatly harm the data quality and amount of information. Instead, binary selections can still be enforced, and you can ask participants to indicate their confidence scores. The same is true for GPT-4.\n2. The dataset only includes edited images from four different models, which might be insufficient for a comprehensive dataset, especially considering the current flourishing development of generative AI technologies. The claims of difficulty of predicting regions edited by GANs over Diffusion Models are not well supported. By the way, how many models do other SOTA datasets include, as listed in Table 1?\n3. A cutoff of IoU of 0.5 is not very informative in showing the benchmarked performance. Why not show the averaged IoU directly?\n4. Many of the insights obtained from Section 4.3 are not very informative and are limited to observations. For example, why are images edited by GANs more difficult to predict? Why do eyes and eyebrows raise more errors? Why do models trained on DETER transfer better to OpenForensics (noted that both are face-swapping)?\n5. The paper does not include instruction-based image editing, especially those conducted by commercial platforms. This is, however, probably the most significant threat when considering edited images as deepfakes, which can be used and accessed by any ordinary user on the Internet.\n6. The dataset is not yet open-sourced. Nor does the paper mention any plans to open-source the dataset.\n7. The biggest novelty of the dataset is that it includes different granularities. However, there are no insights into granularities in human studies. For example, why is DETER harder for human beings to identify? Among FaceSwap, Attribute, and Inpaint, which one is the most difficult? It seems that the only purpose of the human study is to show that \"it is hard for human eyes\".\n8. Although images including \"multiple faces\" are considered novel, no insights are given about this, either in human studies or the benchmark."
            },
            "questions": {
                "value": "1. In your training setup, to address the issue where \u201cmodels tend to assume target regions exist in every image,\u201d why didn\u2019t you just use the original, unaltered fake images from the open-source dataset?\n2. Considering \"another 140K unseen unmanipulated images\" in the improved setting, will they be released to the research community?\n3. It would be interesting to benchmark AIGC detectors pre-trained on previous SOTA datasets against the DETER dataset.\n4. Model performance appears similar across various models, from the 2015 Faster R-CNN to DINO. Does model capacity influence performance in this case, and if not, why?\n5. What instructions were fed to GPT-4 for prompting in Section 3.3?\n6. Other questions are already mentioned in Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper proposes a new deepfake dataset that includes human faces."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes DETER, a large-scale dataset for detecting edited images and regions. The contributions of this work include: 1) in addition to face swapping and attribute editing, the authors also include image inpainting in the data editing operations; 2) to address the spurious correlation challenge in the current datasets for regional fake detection, the authors included additional 90K unedited images; and 3) the authors introduced region-based image-level classification accuracy as an additional assessment criterion. Interesting and important work. Enjoyed reading."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "see the above summary."
            },
            "weaknesses": {
                "value": "Two points to clarify, rather than weakness."
            },
            "questions": {
                "value": "Two minor comments/questions: first, in the first paragraph, the authors wrote \u201cthe upstream SOTA generative models and their applications, the midstream existing deep fake datasets, as well as the downstream fake detection formulation and models\u201d. I am just curious why the authors define \u201cupstream, midstream, downstream\u201d this way. Also, the authors consider datasets that include humans in this work, and target human face (either the whole face or part of it) manipulation. What about other datasets of natural scenes, animals, etc.? Can the proposed method be applied there as well?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a dataset for detecting AI-generated image manipulations. There are 3 main contributions:\n1. A new dataset that contains state-of-the-art generation methods, which are not incorporated in previous datasets\n1. An improved evaluation framework that considers both whole-image and region-specific manipulations\n1. Provides techniques to mitigate spurious correlations in fake detection\n\nThe dataset contains 300,000 edited images using three types of manipulations: face swapping, attribute editing, and inpainting, created using both GAN and DM-based generators."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The new dataset introduced by this paper can potentially make valuable contributions by addressing a critical challenge in the era of widespread AI image manipulation. It has the following merits: \n- Incorporates current state-of-the-art generative models\n- Comprehensive evaluation framework\n- Extensive experiments with multiple detection methods\n- Well-documented human evaluation studies\n- Thorough cross-domain analysis\n\nClarity. This paper has clear methodology presentation and well-structured experimental results"
            },
            "weaknesses": {
                "value": "The main concerns about this paper are ethical and legal concerns. \n- There is no clear discussion of whether they have rights to modify/redistribute CelebA and WiderFace images. For example, the CelebA dataset has the following agreement: \u201cYou agree not to further copy, publish or distribute any portion of the CelebA dataset. Except, for internal use at a single site within the same organization it is allowed to make copies of the dataset.\u201d\n- And there is no discussion of consent from individuals in the images. \n- And there is no discussion of rights clearance for redistributing modified faces."
            },
            "questions": {
                "value": "1. How do you ensure the dataset remains relevant as new generative models emerge?\n1. How does the post-processing pipeline affect the natural artifacts that might help in detection?\n1. Why not include more diverse manipulation techniques beyond facial modifications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "1. No clear discussion of whether they have rights to modify/redistribute CelebA and WiderFace images.\n1. No mention of original image licenses or usage restrictions.\n1. No discussion of consent from individuals in the images\n1. Unclear if celebrity images can be legally manipulated and redistributed."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents an image dataset for regional deepfake detection task. Recent images generated by current generative models are included."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The research problem is useful.\n2. Dataset for regional deepfake detection is indeed valuable for deep fake detection tasks.\n3. The organization of the paper is good."
            },
            "weaknesses": {
                "value": "1. The size of the dataset is not large, which I am afraid cannot be termed as 'large-scale'.\n2. In section 4.2, method DINO (Zhang et al., 2023) is published in 2023, in Table 4, it is referred to as DINO' 22.\n3. In Table 4, the methods are relatively not up-to-date.\n4. Grammar errors: \na. we use GANs-based E4S (Liu et al., 2023b) and MAT (Li et al., 2022),\nand DMs-based DiffSwap (Zhao et al., 2023) and DiffIR (Xia et al., 2023) as the deep generators.\nb. Too long sentence: We posit ourselves in the entire research pipeline of\ndeep fake detection, present an in-depth and comprehensive study, covering the upstream SOTA\ngenerative models and their applications, the midstream existing deep fake datasets, as well as\nthe downstream fake detection formulation and models, that motivates us to introduce this novel\nlarge-scale fine-grained deep fake detection dataset."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}