{
    "id": "aKJr5NnN8U",
    "title": "Toward Understanding In-context vs. In-weight Learning",
    "abstract": "It has recently been demonstrated empirically that in-context learning\nemerges in transformers when certain distributional properties are present in the training data, but this ability can also diminish upon further training.\nWe provide a new theoretical understanding of these phenomena by identifying simplified distributional properties that give rise to the emergence and eventual disappearance of in-context learning.\nWe do so by first analyzing a simplified model that uses a gating mechanism to choose between an in-weight and an in-context predictor.\nThrough a combination of a generalization error and regret analysis we identify conditions where in-context and in-weight learning emerge.\nThese theoretical findings are then corroborated experimentally by comparing the behaviour of a full transformer on the simplified distributions to that of the stylized model, demonstrating aligned results.\nWe then extend the study to a full large language model,\nshowing how fine-tuning on various collections of natural language prompts can elicit similar in-context and in-weight learning behaviour.",
    "keywords": [
        "In-context learning",
        "generalization error",
        "transformers"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose a theoretical explanation on when a model will choose in-context learning vs in-weight learning and support the explanation with experiments.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=aKJr5NnN8U",
    "pdf_link": "https://openreview.net/pdf?id=aKJr5NnN8U",
    "comments": [
        {
            "summary": {
                "value": "This paper provides a theotical framework which studies in-context learning's emgerence and transient nature through a simple model which can adapatively switching between an in-weight predictor and an in-context predictor. The key insights from this paper shows that when the training data has long-tail, containg rare classes that appear infrequently, ICL tends to emerge. And if there are enough training data, IWL tends to dominate. Their experiments on both synthetic data and Ominiglot daat shows that ICL exhibit for rare classes and IWL for common ones."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces a novel theoretical framework that investigates how a model selects between ICL and IWL. This framework offers insights into the conditions under which ICL emerges, especially highlighting the role of rare labels in promoting ICL.\n2. The writing and presentation is clear.\n3. this paper includes experiments in both synthetic and natural few-shot learning dataset and validated their findings."
            },
            "weaknesses": {
                "value": "1. This paper assumes a simple switch between the IWL and ICL based on error minimization but in real world LLM the transition may not be instantaneous and involves more complexities. \n2. The in-context predictor is modeled as a convex combination of the labels in the context, weighted by input similarity. This raises questions because more commonly the in-context predictor should learn the relationship between inputs and outputs (i.e., the mapping from x to y rather than combining labels. This simplification might limit the applicability of the theoretical findings to real-world scenarios where the input-output relations are more complex.\n3. The experiment with the real LLM, Gemini Nano 1 seems somewhat limited. It primarily tests the LLM's ability to follow in-context labels versus trained labels, which may be influenced by how different LLMs are instructed to trust new labels versus their trained knowledge. A more comprehensive and persuasive experiments could include various LLMs to assess the generality of the findings and provide deeper insights across different architectures and scales."
            },
            "questions": {
                "value": "A recent work[1] shows that ICL exhibits dual operating modes: task retrieval and task learning. As the number of correctly labeled examples grows, the model transitions from task retrieval to task learning, but initially, even with correct labels, it may retrieve the wrong task, leading to errors. Wondering how does your finding explain this dual-mode behavior?\n\n[1]Dual Operating Modes of In-Context Learning. https://arxiv.org/abs/2402.18819"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The study of in-context learning (ICL) has emerged in recent years as a tractable and yet important piece of the puzzle for understanding how transformers are able to achieve such good performance across such a wide range of data distributions. There is now a growing literature combining theoretical models with empirical study of ICL in synthetic settings, with the aim of understanding the mechanism by which ICL works, and why it emerges in the first place. \n\nThe present paper adds to this literature, by studying the tradeoff between memorisation (or in-weight learning) and ICL in a simple theoretical model and associated experiments, and how this tradeoff varies with the nature of the data distribution."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Reasonable theoretical foundation for their arguments\n- Well-designed experiments to test these theoretical predictions\n\nOverall I think this has the makings of a good paper that makes a significant and original contribution to an important thread of research in the science of transformer generalization."
            },
            "weaknesses": {
                "value": "- At the moment I am not convinced that the experiments reported in Fig 2 really do support the theoretical predictions. I hope this is just my misunderstanding and that the authors can explain what I missed, and perhaps provide a clearer explanation in the paper.\n- While I am pretty well-versed in this corner of the literature and so prepared to care about the contents of Fig 2, it still took me a while to process all the acronyms and setup."
            },
            "questions": {
                "value": "The main theoretical results seems to be that \u201cICL will first emerge on rare classes, and will eventually be replaced by the IW predictor on those classes\u201d (line 207,208). This prediction is studied experimentally in the setting described in Figure 2, where three models are trained for 100K gradient steps (Appendix B.1.1) across a number of sample sizes N. The three models are all trained with a single (x,y) pair in the context (L = 1) and for the IW predictor the context is always \u201crelevant\u201d (so if I understand correctly the correct output given (x,y), x\u2019 is to simply ignore x\u2019 and copy the input y to the output) and for the IC predictor the context is always \u201cirrelevant\u201d (so given (x,y), x\u2019 the correct output cannot be learned from the context, which should be ignored). The main transformer is trained to have 90% of its contexts relevant.\n\nMy reading of Fig 2 is that on the rare classes where IC is possible (second column, top row) the transformer behaves very similarly to the IC predictor from the beginning, and only converges to the IW predictor when the IC predictor does. The behavior on high-frequency classes seems qualitatively similar, and so I don\u2019t see on what basis this is evidence for the \u201cfirst emerge\u201d part of the claim. Nor do I see how these experiments really support the idea that for large N the transformer switches to the IW predictor. A simpler explanation for this experimental data seems to be that, by construction, the transformer should be somewhere between the IC and IW predictor since its training distribution is between theirs, and for large N the IW and IC predictor behave similarly on relevant contexts.\n\nIt seems therefore that the crux of the argument might lie in the OOBD plots (Fig 2 second row). My reading here is that on the low-frequency classes OOBD performance of the general transformer is initially bad, improves, and then degrades, whereas for high-frequency classes the OOBD performance is initially roughly as good as it ever gets on the low-frequency classes, and then degrades. I do not see here the evidence for \u201cICL emerges first on rare classes\u201d? The degradation of OOBD performance also seems to be much slower on the low-frequency classes than the high-frequency classes, which makes me unsure if the OOBD data shows any evidence for the \u201creplaced by the IW predictor on those classes\u201d claim either.\n\nIn short, I\u2019m not sure how to read Fig 2 as evidence for the theoretical claims. This leaves me unsure of the soundness of the main experimental contribution of the paper.\n\nMinor issues:\n\n- \u201cCdot\u201d line 149\n- \u201cTransient\u201d -> \u201ctransience\u201d line 303\n- It\u2019s a bit strange to see H defined twice in quick succession (differently) in 130 and 158\n\nFactors that would improve my score:\n\n- As already mentioned, clarifying the link between Fig 2 and the theoretical prediction.\n- Compared to the other literature in this area the models are rather small (two layers, one head per layer) and the contexts are very short (e.g. L = 1 in Fig 2). Overall I am left with the sense that maybe these models are just not capable of learning the ICL solution the theory is talking about, and this could be confounding some of the other aspects of the paper. This is actually how I read Fig 6. To what extent is the choice of L = 1 to illustrate something theoretically, and to what extent is it just \u201cthe only length that works\u201d as a result of the small model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Summary of the paper:\n\n* The paper defines a sequential, tabular supervised classification task as a\n  simplified setting in which to study the dynamics of in-context vs.\n  in-weight learning.\n* The paper considers three models in this setting:\n  1. An 'in-weight learner' model capable of predicting labels based only on\n     the most recent query.\n  2. An 'in-context learner' model that predicts labels based instead on a\n     sequence-dependent weighted combination of previous labels appearing in\n     the sequence (specifically, the weights are given by a softmax\n     distribution based on similarity to the previous queries).\n  3. A contextual mixture model that can (learn to) choose based on the input\n     sequence whether to predict according to the first model or the second\n     model, or interpolate between the two.\n* The paper describes a theoretical analysis of the performance of these\n  three models in the synthetic task:\n  * Deriving an upper bound on the expected test loss of the in-weight\n    learner.\n  * Deriving an upper and lower bound on the loss for a particular input of\n    the in-context learner.\n  * Giving a family of algorithms for learning all three models and bounding\n    the regret of the combination model in terms of the regret of the\n    learning algorithms used for learning some of the components.\n* Based on the nature of these bounds, the paper argues that this model\n  captures phenomena observed in prior empirical work on in-context learning\n  and in-weight learning, namely that rare classes drive the emergence of\n  in-context learning (Chan et al.) and that ICL disappears after enough\n  training (Singh et al.).\n* The paper also includes some experiments with small transformers on\n  synthetic data and natural language data and comparisons of the theoretical\n  model's qualitative predictions to the resulting empirical observations.\n* There is also a small qualitative study of a LLM's dependence on knowledge\n  from fine-tuning versus knowledge from its context.\n\nI wrote a somewhat long review. Therefore, I also include a summary of my\nreview as follows.\n\n* I think this paper presents some interesting ideas and results. For\n  example, I like the sequential supervised classification task and I like\n  the three-part model.\n* However, I find the results do not achieve the paper's stated aims of\n  capturing empirical phenomena to the degree claimed, due to what appear to\n  me to be several issues, namely:\n  * fundamental mismatches between the theoretical framework and results and\n    the phenomena studied in prior work,\n  * some limitations in the theoretical results that do not appear to have\n    been acknowledged, and\n  * a lack of connection between the theoretical results and the experiments.\n* Overall, I consider the framing of the paper to be inaccurate and therefore\n  in my judgement the paper should not be accepted in its current form,\n  though I am certainly open to discussion and revisions to the framing to\n  acknowledge and address these gaps as I think the work does have value.\n* I also noted several questions and minor corrections, mainly regarding the\n  theoretical sections."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper addresses an important topic in the science of deep learning,\nnamely the phenomena of the emergence and transience of in-context learning\n(versus in-weight learning).\n\nOn this topic the paper presents an interesting learning setting involving an\nin-context supervised classification task that captures several important\nelements of in-context learning. At the same time, this setting is simpler in\nsome ways than in-context linear regression or other comparable synthetic\nlearning settings.\n\nI think the overarching story developed in the paper (roughly, in my\nunderstanding, that in-context learning arises as a heuristic that can reduce\nloss in areas of the input distribution where in-weight learning has not\ngathered enough experience to provide a specialised and more accurate\nprediction) is an interesting and thought-provoking perspective.\n\nI think the three-part model is an interesting and thought-provoking toy\nmodel of in-context learning versus in-weight learning, and I think the\ntheoretical analysis is informative and supports the aforementioned\noverarching story."
            },
            "weaknesses": {
                "value": "**W1. Mismatch with Singh et al.'s setting.**\nIn the introduction you write that your analysis \"explains the findings of\nSingh et al. (2023),\" namely the transience of in-context learning, giving\nway to in-weight learning. It doesn't seem to me that your model captures the\nphenomenon observed by Singh et al. In my understanding, a fundamental\nfeature of the setting of Singh et al. is that there is no train-loss\nincentive for an ICL solution over an IWL solution, because both paradigms\nare equally capable of solving the task (yet, they observe, ICL still emerges\nand still gives way to IWL in the end). In contrast, in your model, as you\nshow, based on your formulation of ICL and IWL there is a performance lower\nbound to ICL which the upper bound on IWL performance is eventually able to\nsurpass.\n\n\n**W2. Incomplete characterisation of the emergence of ICL for rare classes.**\nOn lines 207--209, you write \"Under this [oracle] algorithm, ICL will first\nemerge on rare classes ... This phenomenon is consistent with the empirical\nobservation that in-context learning emerges with rare classes in the\ntraining data...\".\n\nIt appears that this characterisation of the oracle algorithm is based on the\nobservation that, for some specific values, the IW upper bound starts higher\nthan the IC upper bound (figure 1), suggesting that in these cases the simple\nmodel will use the IC sub-model rather than IW sub-model. However, this is\nnot implied by the theory as the IW bound is merely an upper bound on the IW\nrisk (I also note in a question below that the IC bounds are not of the IC\nrisk but of a particular sample of the risk).\n\nI believe a more accurate characterisation would require the stronger\nassumption that the IW risk happens to be higher than the IC risk for the\nrare classes in question. This arrangement is permitted but not implied by\nthe comparison between upper bounds. It would be implied if you had a lower\nbound on the IW risk and this was higher than an upper bound on the IC risk.\nI am unsure whether such a bound is plausible. In my opinion this gap between\nthe model and the phenomenon should be noted explicitly as a limitation of\nthe analysis.\n\n(Note that this issue is not localised to lines 207--209, but that appears to\nbe the most explicit discussion of the comparison between the phenomenon and\nthe theoretical framework. The favourable result of the comparison is a theme\nthroughout the rest of the paper.)\n\n\n**W3. Incompletely addressed limitation of tabular analysis.**\nThere are several details in the theoretical framework and analysis that\nassume a tabular setting. In footnote 1 you claim that the results can be\neasily be extended to a continuous input space. It is not clear to me that\nthis extension would be easy, since, beyond (yes) easily extending the\ndefinition of the task and the models, the bounds and the proofs seem to be\ntightly coupled to the tabular setting. It is unclear to me whether\nqualitatively similar bounds could be achieved in a continuous input setting.\nOn what basis have you made this assertion?\n\n\n**W4. Specific model of in-context learning appears quite restrictive.**\nThe paper talks generally about in-context learning in transformers, but for\nthe three-part model your formulation of in-context learning appears\nsurprisingly restrictive, namely at first limiting the prediction to a convex\ncombination of context labels and then by the relevant theorem further\nlimiting to the specific modelling choice of using a softmax distribution\nbased on query similarity for weighting the labels.\n\nIt seems to me that mechanisms for in-context learning in a transformer are\nunlikely to be limited in this way, and the ability to use alternative\nmechanisms would generally serve to broaden the model class under\nconsideration and therefore put your performance lower bounds into question.\nIn other words, these restrictions on the ICL sub-model seem to be the source\nof the conclusion that IWL is eventually preferable to ICL, which does not so\nmuch answer the question of why IWL is eventually preferred as raise the\nquestion of why we should believe that ICL is limited in this specific way.\n\n**W5. Gap between theory and experimental study.**\nThe experimental methodology involves comparing one transformer's performance\nwith that of two other 'baseline' transformers, one trained on data that is\nsupposed to incentivise ICL and another trained on data that is supposed to\nincentivise IWL. The comparisons between the 'middle' transformer and these\nbaselines are then used to corroborate the predictions of the theory.\nHowever, the use of pretrained transformers as baselines creates a confound\nthat the 'IWL transformer' may not resemble the IWL model in the theory and\n(even more plausibly) the 'ICL transformer' may not resemble the ICL model in\nthe theory. This would potentially undermine the connection between the\ntheory and the experimental results (and, in turn, the relevance of the\ntheory). This gap appears not to have been acknowledged in the paper.\n\nGiven that you have details models of these learners and have posited\nspecific training algorithms capable of achieving your regret bounds, and in\nthe synthetic data experiments your set up seems to match the theoretical\nassumptions (apart from the tabular input space, though presumably the\nsynthetic data setting could be modified to satisfy this assumption along\nwith any others I have overlooked), it seems like it would have been possible\nto implement your theoretical models directly and compare the performance of\nthese models to the baseline transformer (or the middle transformer).\n\n\n**W6. Unclear relevance and novelty of LLM experiments.**\nThe LLM experiments are interesting in their own right, however I came away\nfeeling unimpressed for several reasons:\n\n1. I felt you could have done more to connect these experiments and their\n   results to the theoretical framework developed in section 2. At the moment\n   I don't see what the LLM experiments say about the theory and I don't see\n   what the theory says about the LLM experiments.\n\n2. The total number of examples tested is small (if my understanding is\n   correct and all examples are listed in the appendices). No statistical\n   analysis has been performed on any specific questions about the results to\n   see if the trends reported are statistically significant.\n\n3. Though it is not my area, I have the feeling that there exists at least\n   a small number of papers having already studied the topic of the\n   competition between knowledge from fine-tuning and knowledge from contexts\n   in determining the predictions of LLMs. (I regret I have not been able to\n   locate any examples, but one that comes somewhat close is Wei et al.,\n   2023, arXiv:2303.03846, which is actually cited in passing in the\n   introduction but is not discussed in any detail). I would have liked to\n   see you contextualise their experiment and results within this literature."
            },
            "questions": {
                "value": "Theory:\n\n1. What parameterisation of $\\alpha$ do you have in mind? Simply to learn a\n   value for every $\\tilde x$ (i.e., again tabular)?\n   * If so, how can you expect it to know to route to the IC model when you\n     haven't seen enough of an example for $g$ to be useful? Is it supposed to\n     be initialised as biased towards routing to $h$?\n\n2. In Proposition 1 (for example), over which distribution are the\n   expectations of $y$ taken? Is $y \\sim y^\\ast(x)$?\n\n3. In Proposition 2 and Corollary 1, if I am not mistaken, your bounds apply\n   to a given sample of the expected risk, and they depend on the sample. For\n   example, in corollary 1, you bound $\\mathrm{CE}(h(\\tilde x), y)$ for a\n   particular $\\tilde x$ and $y$. Since you later compare these bounds to the\n   IW upper bound which is of the expected risk, could you comment on the\n   relationship between these bounds and the expected risk for the IC learner?\n\n4. In Corollary 1, I was confused by the line \"Assume the labels $y^\\ast$ are\n   one-hot (deterministic)\".\n   * If I understand correctly, in the data generating process, the concrete\n     *labels* $y$ in any given example are *already* one-hot, as stated on\n     lines 120--121 \"When we sample $x$ in the context of as the query, we\n     sample $y=e_i$, the $i$-th standard basis vector, ...\"\n   * I thought, possibly you meant that the ground truth labelling *function*\n     $y^\\ast$ is assumed to be such that the ground truth label $y^\\ast(x)$\n     is deterministic? But this seems like too strong of an assumption, since\n     you go on to talk about IWL being better with enough samples *if* the\n     variance is sufficiently low, and if you were assuming the variance was\n     zero for your ICL bound then it wouldn't make sense to say this.\n   * In fact, the assumption that $y$ is one-hot appears to be used in the\n     proof for proposition 2, whereas no additional assumption on $y$ or\n     $y^\\ast$ seems to be used in the proof of corollary 1.\n   * So, I think this assumption is redundant. Is that correct?\n\n5. In Corollary 1, you give both a lower bound in terms of $k$ and also an\n   exact value in the case where $k=L$. There is no restriction on $k$ in the\n   general case and so I assumed that if I put $k=L$ the lower bound\n   should lower bound the exact value. However, if I am not mistaken, if $k=L$\n   then the lower bound simplifies to $1-\\epsilon$, which is larger than the\n   exact value $-\\log(\\epsilon)$ for some values of $\\epsilon$. Is there a\n   mistake in the lower bound?\n\n6. In the figure 1 caption, what is the \"Big-O constant of the IW risk\"? I saw\n   no asymptotic analysis in the IW bound.\n\nNotation and terminology:\n\n1. Corollary 1: I invite you to consider using a roman font to denote cross\n   entropy loss (as in \"$\\mathrm{CE}$\") so as to prevent lexical ambiguity\n   with the variable $C$ and an undefined variable $E$ for readers parsing\n   the result statement.\n\nTypos:\n\n1. There are two separate reference list entries for Edelman et al. (2024).\n2. There are two separate reference list entries for Zhang et al. (2024).\n3. Line 90 \"attention attention\"\n4. Line 148 \"cdot\" presumably should be \"\\cdot\"\n5. Line 304 \"transient of ICL\"\n6. Line 345: It says \"A generic model trained on data with $p_{high}$ is\n   referred to as the *transformer.*\" Is that meant to be $p_{relevant}$?\n7. The proof of Proposition 1 uses notation $R_x$ for expected risk without\n   having introduced this notation anywhere (as far as I can see)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tries to understand how trained Transformers choose between in-weights learning (IWL) and in-context learning (ICL). The paper is well grounded in recent literature studying the contrast of phenomenology and inference time behavior of IWL and ICL, especially with Singh et al. 2024\u2019s finding that ICL can be transient. The paper first sets up a mathematical framework where vectors are input and noisy labels are used in a classification setup. The paper then discusses analytic predictors using ICL and IWL, and explains why IWL is asymptotically superior and why a neural network can find the optimal solution by gradient descent. The paper conducts experiments comparing a transformer\u2019s validation loss to the ICL and IWL predictors to identify that noise level and fraction of rare tasks are critical variables deciding ICL/IWL. Finally, the paper shows, by fine tuning Gemini Nano 1, that fine tuning can induce loss of ICL abilities in real LLMs"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall, the paper\u2019s motivation is clear and the flow is consistent. Here is a list of strengths:\n\nS1: The paper connects an observed phenomenon (ICL\u2019s transience into IWL) with a synthetic model and theory.\n\nS2: The intuitive interpretation that new training samples are effectively test samples under the assumptions made in Sec. 2 is useful.\n\nS3: This work clearly demonstrates how noise level and the fraction of rare tasks can affect the ICL versus IWL.\n\nS4: Unlike concurrent work exploring similar phenomena with similar methodology (https://openreview.net/forum?id=XgH1wfHSX8, https://openreview.net/forum?id=INyi7qUdjZ, https://openreview.net/forum?id=LbceJJc9h2), this work provides experiments with real language models. However see below for some criticism as well. Despite the criticism, the investigation of how well findings on synthetic data generalize to real models is valuable."
            },
            "weaknesses": {
                "value": "The paper has some weaknesses, listed below:\n\nW1: Theory seems to rely on assumptions not met in realistic applications of ICL. The theory assumes that the data are drawn i.i.d, while this is usually not met in many realistic applications of ICL. Thus it is quite unclear whether new training samples can safely assumed as test samples in real LLM training\n\nW2: The setup seems to perform a classification task where x,y are given in pairs. However, this *might* be not a good model of ICL in real LLMs as it does not involve a complex sequence-space computational structure. Recent works, e.g. Akurek et al. 2024 (https://arxiv.org/abs/2401.12973), seems to model the sequence space nature of ICL better. It seems like this seems important to understand ICL \\textit{in Transformers} as Tong et al. 2024 (https://arxiv.org/abs/2405.15618 ) points out that MLPs can learn in context.\n\nW3: I was not able to draw these conclusions in line 484 from the given Figure 7: \u201cin line with our theory, the IBD error of the \u201cselected\u201d IC learner is significantly lower for low-frequency classes. Furthermore, both the IC and IW predictors achieve similar IBD errors on CH, suggesting that IWL more easily emerges on common classes even with larger input noise (Figure 7, right).\u201d It would be great if this can be explained further\n\nW4: While I agree with most conclusions drawn from the plots, it would be better to compute explicitly a ICL_vs_IWL ratio as a metric since currently it seems like these conclusions are drawn by matching the experiment curves with predictors by eye.\n\nW5: It is intuitively hard to grasp the experiment. This is solely a presentation issue, it would be very helpful if there was a schematic diagram which describes all of $p_{relevant}$, $p_{high}$, OOBD, IBD, IC model, IW model, etc. The figures could also be improved, as mentioned below in questions.\n\nW6: It is unclear how well the real LLM experiments justify the claims in the main text and the claim in Singh et al. 2024 that ICL can be transient *during pretraining*. The current experiments seem to suggest that fine-tuning can override ICL abilities, but it is very probable that the ability itself is still there while it is simply suppressed superficially. (See Jain et al 2023 (https://arxiv.org/abs/2311.12786, https://arxiv.org/abs/2410.1653 ). This would not be corresponding to the claim in the main text and in Singh et al. 2024, which I believe discusses a pre-training stage transience requiring orders of magnitude more steps than fine tuning."
            },
            "questions": {
                "value": "Questions\n1. How could the real vs invented (name, city) pair result be explained? If the model is being fine-tuned, why can\u2019t it memorize invented (name, city) pairs? Is there an intuition or is this simply the case?\n2. How applicable is the theory to real LLMs?\n\nSuggestions:\n1. The current figures were quite confusing for me, the axis and plot labeling constantly changed through Fig 2,3,7,8. It would be great if the plot colors/style can be improved so that they can be better understood.\n2. I think Figure 16 is main-text material, perhaps more so than Figure 8."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}