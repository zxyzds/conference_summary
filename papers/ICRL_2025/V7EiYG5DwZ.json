{
    "id": "V7EiYG5DwZ",
    "title": "Mutual-Inform SMoE: Improving Routing Stability via Probabilistic Graphical Model",
    "abstract": "Sparse Mixture of Experts (SMoE) has emerged as a breakthrough approach for achieving unprecedented scalability in deep learning. By enabling models to expand their parameter count exponentially while selectively activating only a small subset of parameters per sample, SMoEs maintain high efficiency. However, SMoE models are susceptible to routing fluctuations, leading to instability and non-robustness. In this work, we unveils SMoE-based attention as a point estimate of a regression function of a 3-layer hierarchical mixture of experts regression. Through this probabilistic graphical model (PGM) framework, we highlight the conditional independence in expert-selection process of tokens, which exposes the model to routing fluctuation and non-robustness. Motivating by this PGM framework, we propose Mutual-Inform SMoEs, including Similarity and Attention-Inform SMoE, which eliminate the assumption of conditional independence by allowing tokens to directly influence each other on expert-decisions. We theoretically demonstrate that our methods lower the entropy in decision-making, enabling more confident and consistent expert assignments. Finally, we empirically validate our models on ImageNet classification and Wikitext-103 language modeling, showing significant improvements in reducing routing fluctuations, enhancing performance, and increasing model robustness compared to baseline Transformer-SMoE models.",
    "keywords": [
        "mixture of expert",
        "transformer",
        "probabilistic graphical model",
        "robustness"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=V7EiYG5DwZ",
    "pdf_link": "https://openreview.net/pdf?id=V7EiYG5DwZ",
    "comments": [
        {
            "summary": {
                "value": "This paper formalized the transformer attention head as a probabilistic graph model and suggested that independent expert selections for different tokens in Sparse MoE contributes to the routing fluctuations. Based on such analyses, the authors proposed two intuitive approaches of aggregating other tokens' expert selections into the decision of the current token with either similarity between output features or adapted attention scores as the weights. Experiments in the paper demonstrated that such techniques could bring the improvements in terms of model performance and routing stability."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The structure of the paper is well-organized and comprehensive. The paper clearly identified the potential issues behind routing fluctuations at start and formed two solutions step by step."
            },
            "weaknesses": {
                "value": "1. The paper's writing did not fully meet the standard of a professional research publication . \n- Lack of rigorous and concise formula definitions although the authors tried to analyze the problems in a formal way. \na. Diverse meanings of the same subscript in different formula. In Line 75-77, the subscript should be $k$ rather than $i$. It is not appropriate to use $i$ in line 87. Please do not confuse these two index symbols and try to keep one index have a unique meaning\nb. Some notations lack explanations. What does the $f$ in Eq. (3) mean? What does $\\hat{E}$ in Figure 1 mean? In Eq. (1), A and V should have subscripts. Also, the distribution whose mean is calculated should be listed under $E$ in Eq. (5) & (8).\nc. Some equations have errors. In line 87-93, It would be better if the renormalization operation is taken over M rather than N. Namely, the denominator in Line should be the sum of M items since the remaining (K-M) scores are $-\\infty$. In Eq. (4), \"| X=X\". \nd. For some conventional definitions like the meaning of P(A) and E(A) in line 152-153, it is not necessary to point out them in the main text.\ne. Putting lots of formulas/notations into the paper sometimes makes the content hard to follow if there is not enough illustrations and explanations. For example, Figure 1 is hard to parse without any captions about the definitions of symbols. It could be companied with graph illustrations of attention blocks or Sparse MoE. \n- Redundant text. The preliminary and approach sections are redundant.  In Sec 1.2., there is no need to put too many words to explain the commonly-known fact that selecting top M values to calculate softmax is equivalent to the normalization over top M values after softmax. and I am not convinced that this explanation is necessary for further analyzes.  The explanations of two context-dependent expert selection methods could be condensed. The paper should be delivered in a concise way with compact information. \n- Some typos. In line 20,  \"Motivated by\" not \"Motivating by\". In line 166, it should be \"Considering\". In line 255, 261, 267, there are some indexes (5, 6, 7) coming from nowhere. In line 290, it should be \"similar to\". In 128-129 \u201cis as\u201d and a misplaced comma seems confusing to me.\n- Others. The EMPIRICAL ANALYSIS and EXPERIMENTAL RESULTS could be merged rather than set apart.\n\n\n2. The novelty of the paper is questionable. There is lack of sufficient literature review. The papers listed in the related sections seem to be \"out-of-date\", i.e. published two years ago and the proposed methods were compared against a limited set of long time-established baselines. Although it is interesting to see that the authors fit the MoE into PGM framework, the proposed \"attention-like\" aggregation techniques are still quite simple and straightforward, making me question the necessity of using this complicated PGM framework.\n\n3. Experiments. In Figure 2, the entropy curve for the baseline is missing in the right plot, and the dynamics of fluctuation values across different layers should be analyzed."
            },
            "questions": {
                "value": "See the above weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes \"MUTUAL-INFORM SMOEs\", MoEs with a tweaked routing mechanism that allows for information sharing between routed tokens. The authors motivate their proposed method as a solution to reducing the routing fluctuation problem in MoEs. They provide theoretical justification for their method's reduction in routing fluctuation by showing that the entropy of routing decisions of \"MUTUAL-INFORM SMOEs\" is upper bounded by standard topK + softmax routing. The authors provide additional experiments on language modeling (standard + adversarially perturbed text) and image classification, showing that their method outperforms a standard topK + softmax baseline in routing fluctuation and validation perplexity."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The Attention-Inform (S)MoE and Similarity-Inform (S)MoE routing functions are novel and potentially interesting to the community. The introduction of routing mechanisms that can share information between tokens can lead to better-performing MoEs.\n- It is nice to see the theoretical justification for the proposed method.\n- I like the idea of including an adversarial test set. \n- Providing code for the method is appreciated and helps validate the results."
            },
            "weaknesses": {
                "value": "My **main concern** is the lack of comparison to previous work that also improves the *routing fluctuation* problem of MoEs. Authors cite [1,2] numerous times when referring to the routing fluctuation problem. However, they do not provide a comparison to Stablemoe or X-MoE. To the best of my knowledge, [1,2] are the only works that claim this *routing fluctuation* problem exists in MoEs and to provide a solution for it. As clear follow-up work to [1,2], both more than two years old, it is very important to directly compare to their method. Adding a comparison to these methods would greatly strengthen the contributions of the work. \n\n**Other concerns** \n- The method relies on the attention matrix for similarity scores between different tokens. How does this influence the computational and memory complexity of an MoE forward pass? Providing figures that show the timings with respect to a baseline would address this concern.\n- The introduction is structured unconventionally and I found myself taking time to find the motivation for the work.\n- It would be helpful to understand how the performance of M-I SMoE changes as the number of experts is increased, the granularity of the experts changes, and the number of active parameters changes. \n- The architecture of the MoEs used (e.g., number of experts per layer) should be reported in the main text, not only in the appendix.\n- I am unable to find the dimensions of the transformer used in the language modeling experiments.\n\n\nTypos:\n- \"Motivating by this PGM framework\" Motivating --> Motivated\n- Page 10: \"We leave it for future work. We leave it for future work\"\n\n\n[1] Stablemoe: Stable routing strategy for mixture of experts ( https://aclanthology.org/2022.acl-long.489/ )\n[2] On the Representation Collapse of Sparse Mixture of Experts ( https://arxiv.org/pdf/2204.09179 )"
            },
            "questions": {
                "value": "- Is routing fluctuation discussed as a problem for MoEs outside of papers [1,2]?\n\n\n[1] Stablemoe: Stable routing strategy for mixture of experts ( https://aclanthology.org/2022.acl-long.489/ )\n[2] On the Representation Collapse of Sparse Mixture of Experts ( https://arxiv.org/pdf/2204.09179 )"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel perspective on Sparse Mixture of Experts (SMoE) through a probabilistic graphical model (PGM) framework. The authors identify that the conditional independence in expert selection for tokens leads to routing fluctuations and model instability. To address this, they introduce Mutual-Inform SMoE with two variants: Similarity-Inform and Attention-Inform, which allow tokens to influence each other's expert assignments based on their similarities or attention patterns. They provide theoretical analysis showing their method reduces entropy in routing decisions and demonstrate empirical improvements on ImageNet classification and WikiText-103 language modeling tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper provides a novel theoretical foundation for understanding SMoE through PGM, offering valuable insights into the routing fluctuation problem and presenting a well-grounded solution. The mathematical derivations and proofs are rigorous and well-presented.\n\n2. The proposed solution is elegant and practical, requiring minimal additional computational overhead while showing significant improvements in both performance and stability. The two variants (Similarity-Inform and Attention-Inform) provide flexibility in implementation.\n\n3. The experimental evaluation is comprehensive, covering both vision and language tasks, with thorough analysis of routing stability, entropy reduction, and robustness against various types of perturbations."
            },
            "weaknesses": {
                "value": "1. While the paper demonstrates improvements in routing stability, it doesn't fully explore the trade-offs between stability and adaptability. A more detailed analysis of whether increased stability might sometimes come at the cost of reduced model flexibility would be valuable.\n\n2. The hyperparameter sensitivity analysis is limited, particularly regarding the temperature parameter $\\tau$ in Similarity-Inform SMoE and $\\sigma$ in Attention-Inform SMoE. Understanding how these parameters affect performance would be crucial for practical implementations.\n\n3. The experiments focus primarily on medium-sized models. Given that routing issues often become more pronounced in larger-scale settings, evaluation on larger models would strengthen the paper's claims.\n\n4. While the paper mentions load balancing benefits in the appendix, this aspect deserves more attention in the main text, as it could be a significant practical advantage of the proposed approach."
            },
            "questions": {
                "value": "1. How does the approach scale with very large numbers of experts (e.g., hundreds or thousands)?\n\n2. Have you explored the possibility of dynamically adjusting the influence of token similarities based on training progress?\n\n3. How does the method perform when applied to multi-modal tasks where routing patterns might be more complex?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work addresses a core limitation of SMoEs\u2014routing instability\u2014by introducing collaborative token routing mechanisms, enhancing both model robustness and efficiency. This is show both theoretically and in a few benchmarks.\n\t\n**Contributions:**\n\nProbabilistic Graphical Model (PGM) Framework: Introduces a PGM framework to model SMoE-based attention, showing that expert selection is conditionally independent for each token, leading to unstable routing.\n\nMutual-Inform SMoE Models: Proposes two new SMoE variants:\n- Similarity-Inform SMoE: Routes similar tokens to the same expert, using token similarities from the MoE layer.\n- Attention-Inform SMoE: Utilizes relationships from the attention layer to inform expert assignments, aligning expert choice with token interactions.\n\nThis is validated theoretically (with entropy reduction) and empirically (with task specific benchmarks)"
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**Originality:** Introduces Mutual-Inform SMoE, a novel approach for stabilizing SMoE routing using token similarity and attention dependencies. Innovatively reinterprets SMoE through a probabilistic graphical model (PGM), which is a fresh way of addressing routing challenges.\n\n**Quality:** Very rigorous theoretical framework (congrats!); sound mathematical proofs back claims about reduced entropy and improved stability. \n\n**Significance:** Addresses a critical issue in SMoEs (routing instability), with potential to impact scalable model design for large-scale LLM and vision applications. Improvements in both standard and adversarial settings underscore its value in robust model deployment."
            },
            "weaknesses": {
                "value": "### Presentation weaknesses:\n- Writing needs to be considerably improved (sentences unclear, leading to confusions)\n- Many typos need fixing (eg: Abstract: In this work, we unveil**s**; Motivat**ing** by this PGM framework\u2026)\n- Proofs could be explained a bit better than just a series of latex equations.\n\n### Empirical work weaknesses\n- The empirical validation is weak, more benchmarks would be appreciated. The experiments could be strengthened by comparing against more recent or diverse baselines\n- Mutual-Inform SMoE models show strong results, additional ablations would clarify the impact of individual components (e.g., temperature parameters, entropy reduction mechanisms). Including these would highlight the contributions of each module in performance gains.\n- There is a lack of discussion on the computational costs of the method, and the computation penalty for using these more performant models compared to the baseline."
            },
            "questions": {
                "value": "- How does this work compare to other, more modern methods and across more benchmarks?\n- What is the computational cost of these methods?\n\n(See weaknesses for details)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}