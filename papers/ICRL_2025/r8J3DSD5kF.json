{
    "id": "r8J3DSD5kF",
    "title": "Stick-breaking Attention",
    "abstract": "The Transformer architecture's self-attention mechanism traditionally relies on the softmax operator, necessitating positional embeddings like RoPE, or position biases to account for token order.\nBut current methods still face length generalisation challenges.\nWe propose an alternative attention mechanism based on the stick-breaking process:\nFor each token before the current, we determine a break point $\\beta_{i,j}$, which represents the proportion of the remaining stick to allocate to the current token.\nWe repeat the process until the stick is fully allocated, resulting in a sequence of attention weights.\nThis process naturally incorporates recency bias, which has linguistic motivations for grammar parsing (Shen et. al. 2017).\nWe study the implications of replacing the conventional softmax-based attention mechanism with stick-breaking attention.\nWe then discuss implementation of numerically stable stick-breaking attention and adapt Flash Attention to accommodate this mechanism.\nWhen used as a drop-in replacement for current softmax+RoPE attention systems, we find that stick-breaking attention performs competitively with current methods on length generalisation and downstream tasks.\nStick-breaking also performs well at length generalisation, allowing a model trained with $2^{11}$ context window to perform well at $2^{14}$ with perplexity improvements.",
    "keywords": [
        "transformer",
        "attention",
        "stick-breaking",
        "softmax",
        "length extrapolation"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "Using the stick-breaking process formulation as a replacement for softmax attention.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=r8J3DSD5kF",
    "pdf_link": "https://openreview.net/pdf?id=r8J3DSD5kF",
    "comments": [
        {
            "title": {
                "value": "Questions on Evaluation"
            },
            "comment": {
                "value": "> Table 2, Figure 6: Why do you suppose stick-breaking attention helps on these tasks? ..\n\n\nFor language modelling, our hypothesis is also our motivation for the stick-breaking process for attention. \nThe observation from Physics of language models (as mentioned in Section 1): In a synthetic context-free grammar setting (a synthetic surrogate for natural language): \"attention mechanisms attend to the \"most adjacent non-terminal.\u201d (line 47).\n\nRelatedly, language models without further fine-tuning tend to exhibit failure to extrapolate to longer sequences, suggesting that even though RoPE is a relative position embedding, there are still failures to extrapolate to different relative positions to perform the above task.\nBaking the \u2018most adjacent\u2019 behaviour directly into the attention seemed like a reasonable thing to try to overcome this issue.\nIn a causal left-to-right decoder setting, most adjacent would be the most recent match. \n\nIn the case of tasks like MMLU where few-shot examples are given, we hypothesise that in the standard softmax attention, the prior in-context learning examples could become distractors when answering the final question, resulting in lower scores. \n\nAt the larger scale, it is difficult to make sense of the attention maps produced by the attention layers, not to mention our model has 40 layers in total.\nHowever, we do agree it is an interesting phenomenon and will find ways to investigate this behaviour further.  \n\n> Why does stick-breaking attention do better at length generalization on RULER if the retrieved data is not necessarily recent? I don't see why stick-breaking would be advantageous here.\n\nRecency is a relative notion. Our results suggest that after the pre-training process, our model has learned to attend to the most recent sequence matching the needle lookup query.\nHowever, we hypothesise that the benefit of stick-breaking here may be the combination of the fact that it is attending on the most recent occurence of the 'needle' being searched for, _and_ there was no overfitting on position embeddings seen in training.\n\n> Why does stick-breaking attention eventually fail on RULER at longer lengths? What is the failure mode?\n\nThis is a great question and something we are still investigating. \nOur current hypothesis is that given a sufficiently long sequence,  keys that 'capture' the attention weight can occur randomly, despite not being a match for the needle lookup query. This may be a phenomenon that exists in softmax attention as well, and is what results in failure to extrapolate.\n\nWe hope we've sufficiently addressed your questions and concerns, and that you will reconsider your score. Thank you!"
            }
        },
        {
            "title": {
                "value": "Addressing Reviewer Questions"
            },
            "comment": {
                "value": "> The keys and queries do not have positional encodings, but the values still do, right?\n\nRoPE position embeddings are not applied on the values, not just in our setting and baselines, but generally. \nFor stick-breaking, we remove position embeddings from the query and keys, so stick-breaking does not use position embeddings _at all_.\n\nAs for expressivity: Empirically, we are not seeing degradations in the model performance across benchmarks.\nWe are less familiar with the theoretical expressivity, but \nReviewer Myhj may have some references (https://arxiv.org/abs/2310.13897) that could hint at the limitations of stick-breaking.\n\n> ... can you make a more rigorous case as to why stick-breaking has a recency bias?...\n> Related to the above, did you verify that the model actually attends to more recent tokens....\n> 096: I'm not sure what this discussion is trying to say. Can you explain this more?\n\nTo be clear, the claim we are making about recency is in a setting where, assuming all logits are equal, stick-breaking will give higher attention weight to the more recent logit, rather than assigning them the same weight (Figure 1.), and formally in Line 96, which is a formalised discussion of the recency bias of stick-breaking.\n\nTo verify 'attending to more recent tokens' in a realistic language modeling setting like wikitext is not straightforward.\nThe previous token would be the most recent, and this would definitely result in poor language modelling capability if only the previous token were attended on by all heads.\nHowever, we verified this via a controlled toy setting as in the MQRAR toy problem, where the attention maps were much easier to analyse.\nThere, we do find that in some cases, softmax has difficulty attending on the most recent occurrence. However, we believe with higher head dimensions, this capability will improve, with the caveat that longer sequence will require yet higher head dimensions.\n\n\n> 208: Doesn't this make your method much less parallelizable than standard attention? ...\n\nOptimisation and workload assignment techniques that were used in flash attention could be adapted for stick-breaking in similar ways. Prefix-sum could be used in a CUDA implementation setting, which we are currently looking at.\nSpecifically, you may be concerned that the form of the gradient may mean serialised computation of the cumulative sum.\nThere also could be a block-wise serial version of this that can be parallelised across threads, and then the resulting gradients accumulated at the end accordingly.\nWhile we've tried to optimise the kernel as much as possible in this initial version, there are many more avenues for improvement in future versions of the algorithm.\n\n> 304: How much of each sequence is devoted to the initial set of pairs, and repeated pairs?\n\nThe number of initial sets of pairs is exactly the number of kv pairs for that experiment. However, after that, these assignments can repeat and new assignments can be made. The number of repeated pairs after the initial set of kv pairs is random, and so can vary from instance to instance."
            }
        },
        {
            "title": {
                "value": "Response: Addressing weaknesses"
            },
            "comment": {
                "value": "Thank you for your interest and extensive list of questions!\n\n> Their implementation relies on a very rough approximation of the exponential function...\n\n> Instead of using Eq 5, which is an inexact approximation, why not use the identity $\\log\u2061(1+\\exp(\u2061x))=c+\\log\u2061(\\exp\u2061(\u2212c)+\\exp\u2061(x\u2212c))$, where $c=\\max(0,x)$? This would solve the overflow problem while being an exact solution.\n\nTo answer both this question and the question of the approximation: \nIn bfloat16, the result of using the approximation is actually equivalent to what the $\\log(1 + \\exp(x))$ implementation will result in softplus is simply the identity in the positive regime (> 15), and 0 in the negative, often viewed as a \u2018soft\u2019 or \u2018relaxed\u2019 version of $\\max(0, x)$.\nSo, despite being inexact analytically, the function is exactly the same in practice. \n\nSpecifically, consider when $x$ is large and positive, $c = x$, and $c+\\log\u2061(\\exp\u2061(\u2212c)+\\exp\u2061(x\u2212c)) = x + \\log(0 + 1) = x$\n\nWith respect to your proposal: The goal of the kernel is speed. The max(0, x) op followed by 2 exponents inside the log will slow things down significantly, as this is an operation that is performed repeatedly inside the kernel for every tile.\n\n> To improve the readability of the paper ... For starters, why is it called stick-breaking?\n\nWe\u2019ve referenced the papers in related work on the background of the stick-breaking process, and elected to not go too deeply into the reasons behind the name. We\u2019ll add the following short description into the camera ready for completeness: \n\"Consider breaking a stick of unit length at some point beta_1, keeping the segment of $\\beta_1$ length, and keeping the segment of $1 - \\beta_1$. We repeat this process with the segment of $1 - \\beta_1$ length, breaking it at the proportion $\\beta_2$, and so on.\nThe length of the segments produced by this process is given by the stick-breaking formula.\n\n> Although the results are encouraging in Section 5.3, it is not clear to me that there was much hyperparameter tuning on the baselines. I would like to see more discussion of this.\n\nThe hyperparameters for the baseline were tuned with the same process as in the Power Scheduler method (reference), on a 1b parameter model. The stick-breaking model did not go through the same extensive hyperparameter tuning, but was rather a drop-in replacement and trained with the same hyperparameters.\nWe will include a short write-up about the process with a reference to the power scheduler paper."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "> The paper's short title \"Stick-Breaking Attention\" may give the impression that this is the first paper about stick-breaking attention.\n> The abstract does not mention previous work; on the contrary, it says \"We propose an alternative attention mechanism.\"\n\nWe will amend the abstract and title appropriately for the camera-ready (See general response).\n\n> The introduction, probably inadvertently, could be mis-read as saying that geometric attention only has one parameter (\"Geometric attention, named after the Geometric distribution, which only has one parameter\").\n\nWe will modify this statement to the following: \n\"Consider a parameter $p \\in (0,1)$ for the probability for success for a trial.  The geometric distribution then gives the probability for which $k$ trials are needed for the first success: $(1 - p)^{k-1}p$. But in stick-breaking attention and Geometric attention, each $p$ is assigned ...\"\n\nThe original phrasing was not written with the intent to mislead, and we apologise for implying that Geometric attention only has one parameter. We take the flagging of concerns for ethics review seriously, and we hope this addresses all of your concerns on the ethics front.\n\n> Eq. (2): Putting the remainder of the attention onto position j itself does not seem like the right choice. Probability $(1\u2212\\beta_{i,j})$  is the probability of pushing the attention to the left of position i, so $\\prod_i (1\u2212\\beta_{i,j})$ is the probability of pushing the attention all the way to the left. So it's not surprising that this turned out not to work well. Letting the attention weights sum to less than one (or equivalently, putting the remainder of the attention onto the zero vector) seems like the most sensible thing to do.\n\nWe agree, and your hypothesis is a reasonable explanation for the poorer performance. We intend to remove the remainder mechanism for simplicity in the camera-ready version of the paper and train and re-evaluate the experiments without the remainder mechanism. We have already done so for the MQRAR experiment with identical results.\n\n> What is the exact relationship of stick-breaking attention to geometric attention (Csordas et al 2021) and the stick-breaking attention of Shen et al (2023)?\n\nThe main difference between both those proposals is the Shen et. al. 2023 only considers a unidirectional decoder-only attention, while Csordas et al. 2021 includes a mechanism for accounting for both \u201cfuture\u201d and \u201cpast\u201d attention, in an encoder setup.\n\n>Eq (4): I don't know if it matters, but you could use `log1p(exp(z))` instead of `log(1+exp(z))`.\n\nAn interesting detail about the kernel implementation is that we use exp2 and log2 in the attention weight calculations, which we believe has hardware-level optimisations. \nWe multiply and divide by $\\ln 2$ in order to convert between base 2 and base $e$ logarithms.\n\nTo your point, though: 1) log1p would be an $e$ based logarithm, and 2) Triton does not expose this API (at least in the version we\u2019re using).\nThis is why we have used this implementation of softplus.\nUltimately, one of the reasons we chose this particular implementation of softplus was because exp(z) will give nans at larger values of z. \n\n\n> It may be interesting to node that while softmax with a temperature factor...\n\nThank you for making us aware of this paper! We did wonder if there were any expressibility differences in the extremes with stick-breaking vs. softmax, and from reading the introduction, it does seem that this paper might give some insight to that.\n\n\n> Another suggestion would be to use logsigmoid...\n\nLogsigmoid is a variant of softplus: $\\mathrm{logsigmoid}(x) = -\\mathrm{softplus}(-x)$. So we are already manipulating certain properties of logsigmoid in order to minimise usage of `exp` and `log` for speedups.\n\nWe hope we have addressed all of your concerns and hope you can reconsider the ethics review flag."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "Thank you for your kind comments!\n\n> What is $\\sigma$ function? It is never defined. is it sigmoid ?\n\nYes! We've overlooked adding this detail, and we will add it to the final paper.\n\n>  Method can be explained more clearly with diagrams, formulation should be defined more thoroughly.\n\nWe will revise the implementation section, add more details to the formulation of stickbreaking attention, and improve on Figure 1 to better explain the method."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "We'd like to thank you for reviewing our work in detail.\n\n> The paper lacks originality in machine learning ideas. Stick-breaking attention has been previously explored by Yikang Shen (in multiple papers) and especially by Csordas et al. (2021), the latter under the name \"Geometric attention\". \"Stick-breaking attention\" is a better name for the model used, but the model is exactly the same as in these prior works, limiting the originality of this paper. The value is mainly in the more extensive experimentation, including showing performance on larger scale, standard natural language benchmarks.\n\n> This paper lacks somewhat in significance because of this. It does have some significance, since it is really good to see that these ideas really do give gains on standard NL tasks like ARC, Hellaswag or RACE, but the basic correctness of the idea had already been established.\n\nWe understand your concerns on the claims made in abstract and implied in the title. We will address this by modifying the abstract and the title (See General response).\n\n\n> The differences between many models in Table 2 are fairly small and nothing is said about the detailed validation of the results. Are these from single runs rather than averages from 3-5 runs with different random initialization? How much variance would there be here, how confident can we be that a result of 63.4 is better than 63.1 for Winogrande on Softmax vs. SB w/o remainder correction, for example?\n\nThese are legitimate concerns for benchmark results in many larger models, but having 3-5 runs for 1B and 3B models requires a lot of compute resources. For comparisons of variance across each benchmark, we could include further results from other open source models, to have a better sense of how much each benchmark fluctuates.\n\nNonetheless, we understand skepticism with regard to these benchmark results being significantly better, but the results could at minimum be considered equivalent to existing models / standard atttention.\n\nWe would also like to point that on top of the regular evaluation benchmarks, we also performed length extrapolation results on the RULER suite of needle-in-a-haystack style benchmarks. The results there are fairly significant across all the different variants of NIAH tasks. We will include the results of the full suite of RULER tasks in the supplementary materials.\n\nThis suggests stick-breaking affords length extrapolation out-of-the-box without degradation in performance compared to softmax.\n\n> How to produce an efficient, numerically stable Triton kernel basically follows the methods of FlashAttention and standard good practice (using $\\log(1 + \\exp(x))$, etc.)\n\nWhile the Triton kernel implementation takes inspiration from FlashAttention, the specific formulation of the log-space computation of stickbreaking (As stated in Eq. 3)  is not as straightforward as suggested. Main considerations were reducing log and exp computations for speed, and we believe further optimisations could still be made.\n\n> For the paper, you should explain lines 300-301. This isn't a question. I figured it out, but the text of the paper should explain MQRAR better for people who haven't seen it.\n\nThank you for your feedback on this. We will modify the description of this experiment with the following paragraph:\n\"Multi-query Associative Recall (MQAR) is a task that required a sequence model to ingest a sequence with key-value pairs: `A 1 B 2...` , where the alphabets are the keys, and numbers are the values. Later in the sequence, the model has to associate the token A and predict 1, for example. In our variant, Multi-query Repeated Associative Recall (MQRAR), while the key is retrieved, it will also be reassigned a value: `A 1 ... A 2 A 3`. In this case, on the second A, the model has to predict 1, the previous value assigned to A, while the third A will retrieve 2, and so on. We provide a full example below...\"\n\nWe hope our response addresses your concerns satisfactorily, and you might increase your score. Thank you!"
            }
        },
        {
            "title": {
                "value": "Modifications to Abstract and Title"
            },
            "comment": {
                "value": "Reviewers wgvD and MyhJ have cited issues of novelty in our work. While we referenced some of the prior work on stick-breaking as an attention mechanism, the reviewers are concerned that the title and the abstract did not appropriately reflect the position of our paper.\n\nPrior work introduced stick-breaking attention, but introduced several other concepts and mechanisms without focusing on the contributions of replacing stick-breaking as the attention mechanism, and the inductive biases it introduces. Here, we solely replace the attention layer with stick-breaking attention, train, and evaluate the model on downstream tasks, making it a more focused evaluation on just the stick-breaking mechanism. The results on the larger scale experiments with its improvements over standard attention + RoPE, specifically on length extrapolation is a contribution in itself.\nFurther, we\u2019d also like to emphasise that the approach to scaling the method up is non-trivial. The Triton kernel implementation is important as it makes it amenable to large-scale training, whereas stick-breaking in its naively implemented form     has not been adopted, in part due to its low throughput and high memory usage.\n\nIn sum, we think that stick-breaking attention has positive properties that make it better than softmax attention, and our paper contributes the evidence for that, along with an implementation for ease of use.\n\nWe agree with the reviewer's concerns, and propose the following changes to the title and abstract to address their issues:\n\nProposed title change:\n\n_Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study_\n\nProposed modification to abstract:\n> We investigate an alternative attention mechanism based on the stick-breaking process in larger scale settings. \n> The method works as follows: For each token before the current, we determine a break point $\\beta_{i,j}$, which represents the proportion of the remaining stick to allocate to the current token..."
            }
        },
        {
            "title": {
                "value": "Equation (4)"
            },
            "comment": {
                "value": "Another suggestion would be to use $\\text{logsigmoid}(z_{i,j}) + \\sum_{k=i+1}^{j-1} \\text{logsigmoid}(-z_{k,j})$, which should have a more numerically stable gradient assuming logsigmoid is implemented correctly."
            }
        },
        {
            "summary": {
                "value": "This paper does a thorough evaluation of stick-breaking attention (also known as geometric attention) for synthetic test tasks and natural language tasks, showing its effectiveness at capturing a preference for locality, as is natural and correct for natural language tasks. The paper examines a fast, numerically stable implementation of stick-breaking attention in Triton, which enables larger scale experiments."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper provides thorough and useful experimental results on the performance of stick-breaking attention, providing good exploration of its effectiveness for artificial and natural language tasks, for length generalization, etc. The experimentation seem well thought through and well done.\n- The paper is generally clear and easy to read.\n- The paper is very honest about what it contributes and what it uses from prior work.\n- The paper provides useful and new empirical results on different forms of attention on different tasks.\n- The paper examines building an efficient, numerically stable Triton implementation of stick-breaking attention.\n- It's good to include comparisons to Gemma2-2B and Qwen1.5-4B so that people can easily see that the results are decent, even though the papers own results are the apples-to-apples comparisons."
            },
            "weaknesses": {
                "value": "- The paper lacks originality in machine learning ideas. Stick-breaking attention has been previously explored by Yikang Shen (in multiple papers) and especially by Csordas et al. (2021), the latter under the name \"Geometric attention\". \"Stick-breaking attention\" is a better name for the model used, but the model is exactly the same as in these prior works, limiting the originality of this paper. The value is mainly in the more extensive experimentation, including showing performance on larger scale, standard natural language benchmarks.\n- This paper lacks somewhat in significance because of this. It does have some significance, since it is really good to see that these ideas really do give gains on standard NL tasks like ARC, Hellaswag or RACE, but the basic correctness of the idea had already been established.\n- The differences between many models in Table 2 are fairly small and nothing is said about the detailed validation of the results. Are these from single runs rather than averages from 3-5 runs with different random initialization? How much variance would there be here, how confident can we be that a result of 63.4 is better than 63.1 for Winogrande on Softmax vs. SB w/o remainder correction, for example?\n- How to produce an efficient, numerically stable Triton kernel basically follows the methods of FlashAttention and standard good practice (using log(1 + exp(x)), etc.)"
            },
            "questions": {
                "value": "- The differences between many models in Table 2 are fairly small and nothing is said about the detailed validation of the results. Are these from single runs rather than averages from 3-5 runs with different random initialization? How much variance would there be here, how confident can we be that a result of 63.4 is better than 63.1 for Winogrande on Softmax vs. SB w/o remainder correction, for example?\n- I took the text around lines 101-107 as suggesting that things should work better doing stick-breaking with remainder, but the actual results go the other way. It would be good to provide more understanding of why this harms rather than helping.\n- For the paper, you should explain lines 300-301. This isn't a question. I figured it out, but the text of the paper should explain MQRAR better for people who haven't seen it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduce an alternative attention that includes a form of positional embeddings, avoiding the need to add PE techniques such as RoPE. The method is based on the stick-breaking process, which means for each token in the context (keys), they assign a break point that represents the remaining stick to the current token. This means that if 2 tokens have equal logits, the one closer to the current token will receive more stick value, representing the positional order.\nConducting this process through out the entire context results in a sequence of attention weights in lieu of traditional attention weights.\nThe paper suggests this process incorporate recency bias in nature without PE.\nExperiments done show that the new method is better at length generalization in perplexity compared to attention+RoPE."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper presents a novel addition to the zoo of \"attention alternatives\" by leveraging the stick breaking process, which performs both \"attention\" and positional embeddings intrinsically. The mechanism has a recency bias, meaning a token can prefer to allocate all its \"energy\" to few recent tokens, but it can also skip over and only attend to far-away tokens.\n- The paper also include details implementation in Triton for flash-attention style efficiency and speed-up optimization, which is hugely appreciated, especially when nowadays efficiency and scalability is valued more for massive training of LLMs.\n- The paper conducted diverse range of experiments, from throughput (only 20% slower than flash attention), perplexity and language modeling tasks (MMLU, ARC-c, hellaswag...), which all shows promising results.\n- The main driver of this work is to solve the length generalization challenge of LLMs, which show good results."
            },
            "weaknesses": {
                "value": "- Method can be explained more clearly with diagrams, formulation should be defined more thoroughly."
            },
            "questions": {
                "value": "- What is $\\sigma$ function? It is never defined. is it sigmoid ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper is about stick-breaking or geometric attention, an alternative to softmax that has a built-in bias towards more recent tokens. Given a query position j, each key/value position i < j computes a probability of \"yes\" or \"no\", and the attention weight on i is the probability that position j is \"yes\" but all intervening positions are \"no\".\n\nStick-breaking/geometric attention was introduced in previous work. It makes position embeddings unnecessary and can be computed in O(log n) parallel time. This paper presents an improved implementation, and shows that stick-breaking/geometric attention improves performance on the multi-query associative recall task and various benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This softmax alternative is simple and induces a bias towards attending to recent positions in a very natural way.\n\nThe experimental results all look strong."
            },
            "weaknesses": {
                "value": "As far as I can tell, stick-breaking attention is exactly the same as geometric attention (Csordas et al 2021), and stick-breaking was previously introduced by Shen et al (2023). Both papers are cited in the introduction, and the introduction concludes with an accurate list of the novel contributions of the paper. However, \n- The paper's short title \"Stick-Breaking Attention\" may give the impression that this is the first paper about stick-breaking attention.\n- The abstract does not mention previous work; on the contrary, it says \"We propose an alternative attention mechanism.\"\n- The introduction, probably inadvertently, could be mis-read as saying that geometric attention only has one parameter (\"Geometric attention, named after the Geometric distribution, which only has one parameter\").\n\nEq. (2): Putting the remainder of the attention onto position j itself does not seem like the right choice. Probability $(1-\\beta_{i,j})$ is the probability of pushing the attention to the left of position i, so $\\prod_i (1-\\beta_{i,j})$ is the probability of pushing the attention all the way to the left. So it's not surprising that this turned out not to work well. Letting the attention weights sum to less than one (or equivalently, putting the remainder of the attention onto the zero vector) seems like the most sensible thing to do.\n\nThe Flash Attention-like implementation of stick-breaking attention is 20% slower than Flash Attention."
            },
            "questions": {
                "value": "What is the exact relationship of stick-breaking attention to geometric attention (Csordas et al 2021) and the stick-breaking attention of Shen et al (2023)?\n\nEq (1):\n- I'd suggest not using \\cdot, as it might be misinterpreted as an inner product in this context.\n- The first summation only goes up to i-1, so a position cannot attend to itself, which is different from usual future masking. I didn't see this decision discussed; what's the reason for it?\n\nLine 94: typo, should be $z_{i,j} = z_{i',j}$? \n\nEq (4): I don't know if it matters, but you could use log1p(exp(z)) instead of log(1+exp(z)).\n\nIt may be interesting to node that while softmax with a temperature factor (\u03b1 = softmax (z/T)) as T -> 0 approaches average hard attention, which is used in many theoretical studies of transformers (e.g., https://arxiv.org/abs/1901.03429, https://arxiv.org/abs/2106.16213), stick-breaking attention (\u03b2 = \u03c3(z/T)) as T -> 0 approaches strictly future-masked rightmost hard attention, which is the kind of attention studied by https://arxiv.org/abs/2310.13897."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "As far as I can tell, stick-breaking attention is exactly the same as geometric attention (Csordas et al 2021), and stick-breaking was previously introduced by Shen et al (2023). Both papers are cited in the introduction, and the introduction concludes with an accurate list of the novel contributions of the paper.\n\nHowever, \n- The paper's short title \"Stick-Breaking Attention\" may give the impression that this is the first paper about stick-breaking attention.\n- The abstract does not mention previous work; on the contrary, it says \"We propose an alternative attention mechanism.\"\n- The introduction, probably inadvertently, could be mis-read as saying that geometric attention only has one parameter (\"Geometric attention, named after the Geometric distribution, which only has one parameter\")."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a new type of attention mechanism called \"stick-breaking attention\". It is meant to have a bias for attending to recent positions. Instead of using dot-products between keys and queries as logits and renormalizing them using a softmax to form the attention weights, the dot-products are each passed through the logistic function and then used as the probabilities in a stick-breaking process. In other words, the model first decides how much to attend to the current token, *then* decides how much of the remainder to allocate to attending to the previous token, and so on. It is a generalization of geometric attention proposed in prior work, where the probabilities at each timestep can be different. The authors discuss details for implementing it efficiently. They test it against standard scaled dot-product attention on a simple synthetic task that advantages recency bias, and on a variety of natural language benchmarks. Stick-breaking attention gets better scores on most tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The experiments include a variety of natural language benchmarks.\n1. The authors include a thorough comparison to similar prior work.\n1. The results as presented show that stick-breaking attention outperforms standard attention on a variety of benchmarks.\n1. Their new attention mechanism is reasonably fast, and further optimizations are possible."
            },
            "weaknesses": {
                "value": "1. Their implementation relies on a very rough approximation of the exponential function to avoid overflow (Eq 5), so their method is implementing a function that is quite different from what is proposed. I think this can be avoided (see Questions), and I am curious to see if using an exact solution affects the results.\n1. To improve the readability of the paper, a more intuitive explanation of stick-breaking would be useful, for those who are not familiar with the term. For starters, why is it called stick-breaking?\n1. Although the results are encouraging in Section 5.3, it is not clear to me that there was much hyperparameter tuning on the baselines. I would like to see more discussion of this.\n1. I would like to see more discussion of *why* recency bias helps on the natural language benchmarks tested (see Questions)."
            },
            "questions": {
                "value": "1. The keys and queries do not have positional encodings, but the values still do, right? Does the lack of PEs on queries and keys reduce the model's expressivity (i.e., are there certain functions that it can no longer implement because of the loss of PEs)? 474: Stick breaking attention doesn't completely get rid of PEs, right?\n1. Although I intuitively understand the motivation for using stick-breaking, can you make a more rigorous case as to *why* stick-breaking has a recency bias? Is it not true that both softmax and stick-breaking attention can implement arbitrary attention patterns?\n1. Related to the above, did you verify that the model actually attends to more recent tokens? Does this happen in wikitext?\n1. 096: I'm not sure what this discussion is trying to say. Can you explain this more?\n1. 094: I think $z_{i,j} = z_{i,j}$ is a typo.\n1. Instead of using Eq 5, which is an inexact approximation, why not use the identity $\\log(1 + \\exp x) = c + \\log(\\exp(-c) + \\exp(x - c))$, where $c = \\max(0, x)$? This would solve the overflow problem while being an exact solution.\n1. 208: Doesn't this make your method much less parallelizable than standard attention? I think you can actually get the same parallel time complexity using a variant of parallel prefix sum -- have you considered using that?\n1. 304: How much of each sequence is devoted to the initial set of pairs, and repeated pairs?\n1. Table 2, Figure 6: Why do you suppose stick-breaking attention helps on these tasks? It even gets slightly lower perplexity on wikitext, which is not a task that is specifically advantageous for recency-biased models. Can you spend more time discussing what kind of capability each natural language benchmark represents? Why do we expect recency bias to help with them? Do the gains come from recency bias, or is it more about length generalization thanks to the use of relative PEs?\n1. Why does stick-breaking attention do better at length generalization on RULER if the retrieved data is not necessarily recent? I don't see why stick-breaking would be advantageous here.\n1. Why does stick-breaking attention eventually fail on RULER at longer lengths? What is the failure mode?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}