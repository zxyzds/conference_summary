{
    "id": "S6cBH99BhB",
    "title": "Enhancing Multilingual Reasoning in LLMs: Insights from Cross-Linguistic Correlations and Optimal Data Proportions",
    "abstract": "Large language models (LLMs) typically rely on fine-tuning to enhance their reasoning capabilities across various languages. However, limited research has been conducted on the optimal balance of language proportions within multilingual reasoning datasets. To fill this gap, we performed a systematic study to examine how different proportions of language data in multilingual reasoning datasets influence fine-tuning performance. Our study revealed a clear relationship between language proportions in datasets and the fine-tuning performance of LLMs. By fine-tuning multiple LLMs using the appropriate language distributions and data volumes identified in our study, we achieved state-of-the-art performance in both multilingual mathematical reasoning and solving mathematical problems using Python code. Furthermore, our approach significantly reduced data volume requirements and translation costs compared to existing methods, providing a valuable reference for future research.",
    "keywords": [
        "Large Language Models",
        "Multilingual Reasoning",
        "Fine-Tuning"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=S6cBH99BhB",
    "pdf_link": "https://openreview.net/pdf?id=S6cBH99BhB",
    "comments": [
        {
            "title": {
                "value": "Comments from a Researcher with Similar Experimental Experience"
            },
            "comment": {
                "value": "I have read over 10 papers from ICLR 2025 that are related to my research area, and I must say this is the best one in my view. I find the three-phase approach proposed in this paper highly innovative.\n\nIn fact, I previously had similar ideas about exploring the optimal proportions of different languages in reasoning datasets. However, since it required directly exploring the optimal ratios among more than 10 variables, the enormous search space made the experiments infeasible. Consequently, I only obtained experimental results for a few proportions using the ChatGLM model. Although this differs from the models used by the authors, my experimental results largely align with the patterns they identified. This suggests that the authors' experimental conclusions have strong generalization capability.\n\nMoreover, I think the most innovative aspect of the paper is first using translation tasks to explore the alignment relationship between different languages and English to divide the 10 languages into 3 groups, and then determining the optimal relationship between these 3 variables. I wish I had thought of this approach at the time!\n\nI also have a few suggestions: I would like to know whether this method is effective for tasks other than mathematical reasoning. If it is, this could be a very promising research direction. Additionally, I hope the authors could include more raw experimental data in the appendix."
            }
        },
        {
            "title": {
                "value": "Responses to Reviews and Manuscript Revisions"
            },
            "comment": {
                "value": "Thank you for your insightful comments. Your feedback is valuable for improving our paper quality. We have revised the paper according to your suggestions. Here are our responses:\n\n### Figure 3 shows three scores (COMET, BLEURT, BLEU) without detailed explanation in the text.\n\nThank you for your suggestion. We have added a caption to Figure 3 and included descriptions of these three metrics in the appendix.\n\n### Fine-tuning configuration is not properly documented.\n\nThank you for your suggestion. We have briefly added the fine-tuning configuration in the new version. Detailed configurations will be provided in our public code repository.\n\n### Improving validation of translated content\n\nWe agree with your point. While LLMs have made rapid progress in translation capabilities, having native speakers review a subset would indeed enhance translation quality and validation, making the experiments more rigorous and convincing. However, considering cost constraints and the current strong translation performance of LLMs, we ultimately did not adopt this approach. We appreciate this excellent suggestion and your professional insights.\n\n### Why wasn't English to Non-English translation explored?\n\nYes, we agree with your point. Exploring both English to non-English and non-English to English translation in Phase 1 would indeed strengthen the preliminary experiments. Due to computational resource constraints (as it requires fine-tuning multiple language pairs across four models with substantial data), we didn't pursue this approach and acknowledge this limitation in our methodology. We appreciate your attention to detail and for pointing out potential experimental setup improvements.\n\n### What are the two examples used in the prompt?\n\nThe two examples are carefully selected high-quality samples, which will be detailed in our public code repository.\n\n### What's the percentage of discarded data entries?\n\nBased on our reasoning parameters, the discarded data accounts for 4.03% of total data. This typically relates to parameter settings, which will be detailed in the code.\n\n### Could it generalize to other multilingual tasks?\n\nOur research scope is limited to mathematical reasoning. Generalization to other domains requires further investigation. However, we believe this would be a valuable research direction to demonstrate our method's generalizability. Thank you for the suggestion.\n\n### Why was only llama3 investigated in phase 3?\n\nPhase 3 required fine-tuning models with large amounts of data, consuming substantial computational resources, especially considering the 70B parameter model. Additionally, previous experiments showed consistency across various LLMs. For these reasons, we selected the representative llama3 series for experiments while acknowledging this limitation. Your suggestion would indeed make the paper more rigorous, and we appreciate your feedback.\n\n### How should we understand the dropped reasoning performance after fine-tuning (like Swahili)?\n\nThis is an important question. We believe different evaluation metrics focus on different aspects, so some fluctuation across metrics is normal. As you noted, Swahili showed significant decline in COMET and BLEURT metrics but remained relatively stable in BLEU, possibly suggesting that mathematical reasoning data might reduce LLM's performance on Swahili. However, we cannot draw definitive conclusions without further experiments, as this might also simply be a normal fluctuation.\n\n### Can you explain why llama2 wasn't shown in Table 1 but was used in phase 3?\n\nWe did not use llama2 in Phase 3 experiments (Figure 7). We introduced llama2 when comparing our trained models with existing baselines (Figure 8). Since existing baselines were fine-tuned results based on llama2, for fair comparison, we also fine-tuned llama2 and compared it with the baseline. We apologize for any confusion in the paper and will clarify this in the new version. \n\n## Conclusion\n\nWe greatly appreciate your insightful and professional comments, which have significantly helped improve our work. Your recognition greatly encourages us, and we hope our responses and revisions enhance your evaluation of the paper. We welcome further discussion. Thank you!"
            }
        },
        {
            "title": {
                "value": "Responses to Reviews and Manuscript Revisions"
            },
            "comment": {
                "value": "Thank you for your insightful comments, which have greatly inspired us and helped improve our paper's readability and quality. We appreciate your time and professional advice. We have revised the paper according to your suggestions. Here are our responses:\n\n### Would the method still work if an LLM is trained primarily on non-English datasets?\n\nCurrently, most LLMs are trained primarily on English datasets, so we believe our proposed method still maintains strong generalizability. Although we did not explore this question in the paper, we acknowledge it is a profound and valuable question that would pose higher requirements for the method's universality. We believe investigating this question could reveal more interesting findings and advance the field. Thank you for raising this point, which provides great inspiration for our future work.\n\n### Regarding Python code benchmarks\n\nFor the Python code reasoning experiments, we did compare with existing SOTA baselines (see Figure 10), but we must acknowledge that the baseline selection was not clearly stated in the figure caption. We have revised the unclear description that caused confusion. Thank you for pointing this out.\n\n### Regarding Figure 3\n\nWe apologize for the confusion. While we mentioned the LLMs used in Table 1, we failed to clearly annotate this in the subsequent text. We have made corresponding revisions. Additionally, we have added the raw data and correlation analysis to the appendix. We hope this addresses your concern.\n\n### Fine-tuning with WMT datasets might lead to decreased mathematical reasoning performance.\n\nThis is a valuable question. Since we only fine-tuned the LLM using translation datasets without other measures, the decline in mathematical abilities was expected and normal. As the purpose of this experimental phase was to compare the effects of different languages, the decrease in translation ability does not affect our analysis at this stage. We have added relevant explanations to the paper based on your comment. Thank you for this suggestion.\n\n### How was the ratio of en:de:ru:fr:others = 24:8:8:8:12 determined?\n\nThis is an insightful question. Based on our previous experiments, we found that the proportions of German, French, and Russian should be increased, so we simply increased the ratios of these three languages when expanding to 25 languages. This is a qualitative approach, and we acknowledge its suboptimality. However, overall, this choice still proved effective, demonstrating the validity of our method. We apologize for not explaining this clearly in the paper and have made corresponding revisions.\n\n## Conclusion\n\nIn conclusion, we greatly appreciate your specific corrections to our paper, and your suggestions provide valuable inspiration for our future work. We apologize for any confusion caused by our previous presentation and have revised both the content and layout according to your suggestions to improve the paper's quality and readability. Your evaluation is very important to us, and we sincerely hope our improvements meet your satisfaction. Your approval would be a great encouragement for our efforts. We welcome further discussion and are happy to address any remaining concerns. Thank you!"
            }
        },
        {
            "title": {
                "value": "Responses to Reviews and Manuscript Revisions"
            },
            "comment": {
                "value": "Thank you for your highly professional review. Your expertise in this field is evident, as you have precisely identified one of the key challenges we encountered during our experiments. We indeed observed that LLMs struggle to provide reasonable translations for some very rare languages (like Sami), likely due to insufficient training data in these languages. Here is our detailed response:\n\n### How do we control translation quality?\n\nFor practical considerations, the 25 languages selected in our study are not extremely low-resource languages, and LLMs can generally provide reasonable translations for them. Beyond the filtering methods mentioned in the paper, we implemented additional safeguards:\n\nTaking Swahili (sw) as an example, we:\n1. Selected 30 English sentences and had them translated to Swahili using our chosen DeepSeek model\n2. Used GPT-4 and Claude 3.5-Sonnet to back-translate the Swahili content to English \n3. Manually evaluated the accuracy of the final English translations to verify whether LLMs could reasonably handle translation for this lower-resource language\n\nWhile we acknowledge this may not be a perfect solution, we believe this approach provides meaningful quality control. Actually, We have identified the development of more robust methods for translation verification as an important direction for our future research.\n\nAlthough this method helps evaluate translation quality, improving LLM translation capabilities for very rare languages (like Sami) remains an important research challenge. This might be addressed through fine-tuning approaches, but falls outside the scope of the current study.\n\n### How does the variation in language selection across phases affect the generalizability of findings to low-resource languages?\n\nWe appreciate you raising this important point. We acknowledge that Figure 7 did not explicitly show the performance improvements for each low-resource language individually. We have addressed this by adding detailed performance improvements for all low-resource languages in the appendix of the revised paper, which we hope provides the clarity you sought.\n\n### Conclusion\n\nIn conclusion, we sincerely thank you for your insightful review. We have revised the paper following your suggestions and hope our responses address your concerns satisfactorily. We welcome further discussion. Thank you!"
            }
        },
        {
            "title": {
                "value": "Responses to Reviews and Manuscript Revisions"
            },
            "comment": {
                "value": "Thank you for your insightful comments, which have greatly helped us improve our work. We have polished our paper according to your suggestions. Here are our responses to your concerns:\n\n1. Regarding W1 - Why is language alignment only studied within mathematical reasoning?\nOur primary objective was to enhance LLMs' mathematical reasoning capabilities across multiple languages. We first investigated the relationship between multilingual alignment and mathematical reasoning. After achieving success in mathematical reasoning, we applied this approach (using specific multilingual training data proportions) to other reasoning scenarios and found similarly positive results. We acknowledge that our original explanation may have caused confusion and have revised it in the new version.\n\n2. Regarding W2 - Criteria for selecting 10 or 25 languages:\na) Selection of 10 languages: These languages are commonly used in related works ( [1][2][3] and so on. ) to evaluate LLMs' multilingual mathematical reasoning capabilities. This selection enables both representation and comparison with existing research.\n\nb) Extension to 25 languages: To further validate our method's effectiveness, we expanded beyond the conventional 10 languages to include 25 languages, covering both commonly used and low-resource languages. The selection referenced historical WMT datasets for better representation. We have added this explanation to the paper for clarity.\n\n3. Regarding W3 - Basis for language grouping:\nOur grouping followed these principles:\n- English forms one group due to its dominance in training data and strongest reasoning capabilities\n- German, French, and Russian form another group based on their demonstrated positive correlation with English alignment and mathematical reasoning abilities\n- The remaining languages form the third group\nWe have clarified this explanation in the revised paper.\n\n4. Regarding W4:\nOur approach enhances reasoning capabilities across all languages (including low-resource ones) by increasing training data for high-resource languages (de, fr, ru) in fine-tuning datasets. We chose high-resource languages for alignment study because of their abundant, readily available data, which helps investigate how to generalize English reasoning capabilities to other languages. We have revised our explanation to make this clearer.\n\n5. Regarding W5:\nGiven the scarcity and high translation costs of low-resource language data, coupled with LLMs' relatively poor reasoning performance in low-resource contexts, efficient utilization of existing low-resource data is crucial. We have added relevant citations and clarified this explanation in the paper.\n\n6. Regarding W6 - Generalization to other benchmarks:\nYour concern is valid. We addressed this by:\n- Using both MGSM and MSVAMP datasets for evaluation (Figure 9)\n- Extending our method to Python code reasoning\n- These are the primary benchmarks in the field, also used by related works ([1][2][3] and so on ).\n- We will open-source our code, datasets, and models for community feedback\n\nRegarding Q1 - Missing references:\nThank you for pointing this out.  We have added the relevant citations in Section 1 to make our arguments more rigorous and convincing.\n\nRegarding Q2 - Data format reference:\nThank you for pointing this out. The data format has been validated in previous research, and we have added the missing reference to Figure 2.\n\nRegarding Q3 - Language extension motivation:\nWe have clarified that the extension from 10 to 25 languages was aimed at further validating our method's effectiveness. While the initial 10 languages were representative, expanding to 25 languages allowed us to test our approach more comprehensively across both high-resource and low-resource languages.\n\nRegarding Q4 - Language names:\nWe appreciate this helpful suggestion and have added a comprehensive table in the appendix that maps language codes to their full names.\n\nRegarding Q5 - Figure captions:\nThank you for pointing this out. We have updated all figures with complete and descriptive captions as suggested.\n\n### Conclusion\nIn conclusion, we sincerely appreciate your valuable feedback, which has helped us significantly improve our paper. We have implemented your suggestions and hope our revisions and responses provide a clearer understanding of our work. Your recognition is important to us, and we earnestly hope to receive a more favorable evaluation based on these modifications. We welcome further discussion and are happy to address any remaining concerns. Thank you!  \n\n\n\n[1] The Power of Question Alignment in Multilingual Reasoning: Broadened Scope and Deepened Insights\n\n[2] Question Translation Training for Better Multilingual Reasoning\n\n[3] MindMerger: Efficiently Boosting LLM Reasoning in non-English Languages"
            }
        },
        {
            "summary": {
                "value": "This paper investigates how varying language proportions in multilingual reasoning datasets impact the fine-tuning performance of large language models (LLMs). The authors aim to demonstrate that carefully balancing language data can enhance LLM performance, achieving state-of-the-art results in multilingual mathematical reasoning and problem-solving while reducing data and translation costs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "There is significant interest in optimizing the pre-training or fine-tuning mixtures for multilingual LLMs, and this work aims to provide valuable insights into this area."
            },
            "weaknesses": {
                "value": "W0: The overall writing of the paper lacks coherence, making it difficult to follow the experimental phases. The fragmented presentation of the experiment phases obscures essential aspects of the study, such as training characteristics, the mixture used for fine-tuning, the evaluation tasks, metrics, and analysis. Additionally, several figures, such as Figure 3, lack complete captions, contributing to the confusion. There is also repetition in section headers (e.g., \"Phase 2\" in Lines 269 and 320), which affects readability and structure.\n\n\nThere are a lot of points and claims that lock in motivation and support:\n- W1: Why is language alignment only studied within the context of mathematical reasoning? The rationale for limiting the focus to this particular setting is unclear.\n- W2: What criteria were used to select the 10 or 25 languages? How are \"high resource\" languages defined in this context? Is the selection based on a specific pretraining dataset, and is there a reference for this?\n- W3: The grouping of languages appears arbitrary. What was the basis for forming these specific language groups?\n- W4: In Line 189, why did the authors choose only high-resource languages for language alignment when, as stated in Line 35, the problem primarily impacts low-resource languages? This choice seems to contradict the motivation expressed in the introduction.\n- W5: In Line 45, the paper claims, \"The key is to efficiently leverage a small amount of low-resource language data to broadly enhance the multilingual reasoning capabilities of LLMs.\" Is there any literature or existing research that supports this assertion?\n\nW6: It appears that the evaluation was conducted solely on the MGSM benchmark. This raises concerns about overfitting the optimal mixture to this specific benchmark. How can the proposed method for creating an \"optimal mixture\" be generalized to other benchmarks?"
            },
            "questions": {
                "value": "Q1: Many references are missing from the claims presented in section 1/ introduction\n\nQ2: Figure 2: Is this data format known for the literature about its performance? Where is the reference?\n\nQ3: Line 076: The motivation for the language extension from 10 to 25 is not clearly explained. What does it mean that the authors extended the language coverage to 25 languages?\n\nQ4: It might be better to provide the full language names in the tables instead of the language codes since, depending on the implementation and the taxonomy used, the codes might be different. \n\nQ5: Please provide captions on all the figures."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper shows a way to enhance multilingual reasoning in LLMs by optimizing the proportion of language data in multilingual reasoning datasets. It addresses challenges in fine-tuning LLMs for low-resource languages, which often lack sufficient training data, by investigating the impact of language proportions on model performance. The paper shows optimal language ratios and volume of data for fine-tuning. The paper also present 'HighMath-350k' and 'HighCode-350k' datasets for multilingual mathematical reasoning and Python-based problem-solving."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The paper shows a unique way to improve multilingual reasoning in LLMs by optimizing the dataset language proportions and that is highly relevant for AI applications. This work contributes a systematic approach to identifying optimal language data ratios. Also, the paper shows extensive methodology and use of Gaussian Process Regression for language group optimization. The outcome/resultant datasets from this work (HighMath-350k and HighCode-350k) are specifically for multilingual reasoning tasks and that will be helpful for future research in this field."
            },
            "weaknesses": {
                "value": "There are 25 languages in the phase3 of this study. But in phase 1 and 2, they used a smaller subset. How does this difference may affect the generalizability of findings to low-resource languages is not covered. Also, the datasets generated in this study are based on Non-English to English translation task. However, the study does not cover how to safeguard against translation inaccuracies in low-resource languages."
            },
            "questions": {
                "value": "1. How do you plan to do quality control for translations in languages where direct verification might be challenging?\nI am asking this question because some low-resource languages may not generate accurate translations in English or vice-versa. It would be interesting to know the author's view on this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a systematic study on the impact of language data proportions in multilingual reasoning datasets on the fine-tuning performance of LLMs. The authors claim to have identified optimal language distributions and data volumes for fine-tuning, leading to state-of-the-art performance in multilingual mathematical reasoning and solving mathematical problems using Python code. The study also aims to reduce data volume requirements and translation costs compared to existing methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper addresses a significant gap in the literature regarding the optimal balance of language proportions in multilingual reasoning datasets for LLMs.\n\nThe research is methodologically sound, with a clear three-phase approach and over 600 groups of experiments providing a robust basis for the findings."
            },
            "weaknesses": {
                "value": "The paper outlines a three-phase methodology to minimize the search space to a manageable size. A crucial step involves categorizing languages based on their alignment with English. It raises questions about the general applicability of the findings. If a LLM is trained primarily based on non-English datasets, does it still work? \n\nAs stated in the abstract, the paper asserts that it achieves state-of-the-art performance in both multilingual mathematical reasoning and in solving mathematical problems using Python code. However, the experimental section lacks empirical comparisons with existing benchmarks that utilize Python code.\n\nLastly, the presentation requires further enhancement. The font size of the figures is too small, and it would be beneficial to include descriptions for figures, such as Figure 3 and Figure 7."
            },
            "questions": {
                "value": "In line 252, Figure 3 depicts the variations in translation performance. Which LLM do the results in the figure pertain to? Is the correlation consistent across different LLMs?\n\nIn lines 256 to 259, Figure 4 assesses the model's performance on the MGSM dataset after fine-tuning with various non-English to English translation pairs. However, does the author take into account that fine-tuning with WMT datasets might lead to a decline in mathematical reasoning performance?\n\nIn lines 368 to 370, when extending 10 languages to 25 languages, how to get the ratio of en:de:ru:fr:others = 24:8:8:8:12?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "LLM gains reasoning ability across multiple languages after finetuning. The typical approach is to use the same amount of data entries in each language during finetuning. This paper points out that it is more efficient and effective when the proportion of each language is different. Thru a 3-phase systematic approach, authors found the optimal balance in terms of the proportions and achieved the SOTA performance in math reasoning and python code reasoning. It effectively reduces the data volume and translation cost needed during finetuning for multilingual reasoning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is well written and nicely presented with comprehensive tables and figures.\n\nThe topic discussed is original and significant, especially during the finetuning stage; that is equal amount of data entries in different languages may not result in the optimal performance. \n\nAuthors made it clear that via experiments; by strategically distributing the amount of data across multiple languages, the multilingual reasoning ability can be acquired at much cost.\n\nThe experiment covers 25 languages and most performant open source LLMs.\n\nAuthors created two multilingual datasets, namely HighMath-350k and HighCode-350k.\n\nThe idea, \"given a fixed amount of data volume, how to achieve the optimal performance by including the right amount of data for each aspect\", can be borrowed at different stages of the training process, which will lower the cost of training as well as the GPU hours needed."
            },
            "weaknesses": {
                "value": "There are some minor grammar issues, such as no space after a comma or a period. For example, line 221 \"selection,in ...\" and line 231 \"system.In ...\"\n\nAnother minor layout issue is that the last two figures split the conclusion onto two pages.\n\nFigure 3 and figure 7 might need some description briefly explaining the purpose for improved readability. \n\nFigure 3 depicts three scores (COMET, BLEURT, BLEU), but there is no elaboration in the context about them.\n\nIt is unclear why the authors only adopted finetuning from non-English to English translation pairs and omitted bi-directional pairs in phase 1.\n\nFinetuning configuration is not properly documented.\n\nThe translation of dataset is solely based on the model DeepSeek-Chat-v2-236B. Even though authors adopted some methodology to ensure the quality of translation, like preserving formulas and using Arabic numerals, the validation of the translated content can be improved. For example, a subset sampled from the translation can be further validated by native speakers for coherency and consistency."
            },
            "questions": {
                "value": "1. what was the reason that English to Non-English translation is not explored?\n\n2. (line 208) what are the two examples used in the prompt? \n\n3. (line 215) what's the percentage of discarded data entries? \n\n4. does the pattern / ratio found in this paper single-purposely apply to math reasoning dataset? could it generalize to other multilingual tasks? or should researchers and developers follow the 3 phases and look for a new ratio?\n\n5. (table 1) why is only llama3 investigated in phase 3?\n\n6. (figure 3) the reasoning performance dropped after finetuning (like Swahili), and the dropped amount is close to the improvements. how should we understand this phenomenon?   \n\n7. can you explain the reason why model llama2 was not shown in Table1 but used in phase 3 in the experiment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}