{
    "id": "zBrjRswpkg",
    "title": "Foundation of Scalable Constraint Learning from Human Feedback",
    "abstract": "Constraint learning from human feedback (CLHF) has garnered significant interest in the domain of safe reinforcement learning (RL) due to the challenges associated with designing constraints that elicit desired behaviors. However, a comprehensive theoretical analysis of CLHF is still missing. This paper addresses this gap by establishing a theoretical foundation. Concretely, trajectory-wise feedback, which is the most natural form of feedback, is shown to be helpful only for learning chance constraints. Building on this insight, we propose and theoretically analyze algorithms for CLHF and for solving chance constrained RL problems. Our algorithm is empirically shown to outperform an existing algorithm.",
    "keywords": [
        "RLHF",
        "RL",
        "Constraint Learning",
        "Theoretical Analysis"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We investigate constraint learning from theoretical perspective and provide a scalable algorithm",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zBrjRswpkg",
    "pdf_link": "https://openreview.net/pdf?id=zBrjRswpkg",
    "comments": [
        {
            "summary": {
                "value": "This paper aims to provide theoretical results in the context of Constraint Learning from Human Feedback. There are a few impossibility results that have been provided on which type of constraints (expected cost constraint, chance constraint) can be learnt in the context of which models (E-CMDP, C-CMDP)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well written and mostly easy to understand. \n2. The results about where constraint functions can be learnt or not is quite interesting. \n3. The experimental results are quite detailed."
            },
            "weaknesses": {
                "value": "1. The notion of impossibility in learning is not explained. Does it imply you can never find a cost function that is safe? \n2.  There are other types of CMDPs that have not been referenced. Only expected cost constraint and value at risk cost constraint have been provided. There is hard constraint and Conditional Value at Risk constraint that have previously been introduced by Hao et al. [Reward Penalties on Augmented States for Solving Richly Constrained RL Effectively]. \n3. Why is learning of cost function important to safety? Can't systems be made safer without explicitly learning a cost function\n4. How are the experimental domains decided? Only a few environments from Safety Gym are considered. Frozen lake is a simpler problem setting. Why were other environments not considered. \n5. In the second paragraph of introduction, it is mentioned that CLHF has significant works. However, in the experiments there are not too many baselines provided. Can I please check why that is the case?\n6. There needs to be more intuition provided before utilising formal description. Otherwise, it gets difficult to keep track of all the symbols."
            },
            "questions": {
                "value": "Please refer to the weaknesses part for the questions. Here are a few more questions:\n\n1. There are other methods in the literature that have been provided for solving C-CMDPs (e.g., Hao et al, Chow et al -- risk constrained RL with percentile risk criteria), so why have the authors introduced a new approach and \n\n2.In 4.1, why are the weights \"w\" the same for RHS and LHS."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper critically analyzes two forms of human feedback for\nconstraint inference; namely *trajectory feedback* wherein humans\nclassify whether given trajectories are safe, and *policy feedback*,\nwherein humans classify whether decision policies are safe. Notably, the\npaper also considers two forms of constrained MDPs (chance-constrained\nMDPs and expected constrained MDPs), and analyzes which forms of human\nfeedback can induce constraint inference enabling solutions to each type\nof constrained MDP. Finally, the paper presents a new policy gradient\nmethod for joint constraint inference and control, and demonstrates its\neffectivess at safe policy optimization in some common benchmark tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The problem setting studied in this paper is interesting and relevant.\nThe notion of characterizing which forms of human feedback enable which\ntypes of safe RL is very neat, and so are the theoretical results\ntowards this end."
            },
            "weaknesses": {
                "value": "One major weakness of the paper is that it is not well-written; examples\nare given below and in the Questions section. As a consequence, I found\nit fairly difficult to follow the paper\u2014I had to write my own document\nof notes and math to fill in several missing or confusing pieces. The\nclarify of the paper could have been improved tremendously, and I\nencourage the authors to do so.\n\nThe part of the proof of Proposition 4 that you omitted should not have\nbeen omitted, and this proof can be explained better. The claims on ine\n816-820 are confusing. Particularly, it says \"policy feedback does not\nprovide no information on $c_1$\". Firstly, it should say \"policy\nfeedback does not provide any information\u2026\". More importantly, I don't\nnecessarily find this accurate. What the equation on line 818 implies is\nthat for any policy feedback dataset will have the form\n$\\{(\\pi_k, 0)\\}$, so there is no signal for identifying a safe policy.\nThis property is directly a consequence of the stucture of $c_1$, though\n(i.e., there are certainly alternative cost functions that will not have\nthis property here). Specifically, it is saying that the costs are\nsufficiently close to $0$ (or \"symmetric\" enough) that no policy is\ndangerous. This does give information about $c_1$. However, it does not\nprevent us from inferring that $\\hat{c}_1\\equiv 0$, so that the\nconstraint function induced from policy feedback does not restrict the\nclass of safe policies, which can result in the deployment of a\nC-CMDP-unsafe policy (by being a strict superset of the set on line\n809). I think this example / narrative would be very helpful.\n\nAgain, Proposition 2 should have a more complete proof, and perhaps\nshould be motivated more. What is the oracle here, and why should we\nassume access to one? I'm assuming what is really meant here is that\nwith unbounded preferences, we can learn safe policies. I think there is\na substantial amount missing in the proof here:\n\n1.  The policy feedback case is not immediately obvious to me. Maybe\n    it's obvious to you as the authors that have been thinking a lot\n    about this setting, but as a reader, I would like verification that\n    my thought process is actually what you had in mind; at the very\n    least, this would help me understand the claim of the proposition.\n    Am I correct in my interpretation that, in the policy feedback case,\n    you can theoretically enumerate policies, query the oracle until you\n    get feedback $0$, and then this certifies a safe policy by\n    definition (so you can return it)?\n2.  For the trajectory feedback case, is my interpretation correct that\n    you again enumerate policies like in the policy feedback case,\n    enumerate trajectories under each policy (given knowledge of\n    $P_1, P$), and return a policy once you find one that the oracle\n    only returns $1$ on at most $p|\\mathcal{T}|$ trajectories? And why\n    do you need knowledge of $r$?\n\nAnother issue with the paper is what I believe to be a lack of baselines\nwith respect to the empirical analysis. Notably, the authors had no\nresults for any baselines except for in the tabular domain. As such,\ngiven the prevalence of the SafetyGym domain, I am skeptical that no\nother method is able to solve the tasks attempted in this paper.\n\n## Minor Issues\n\nOn line 56, \"We denote sets by curly alphabets\" \u2013 I don't think\n\"alphabets\" is the right word, maybe \"braces\"?\n\nIn Definition 1, \"finte\" should be \"finite\".\n\nThe notation for the return in Definition 2 is not quite right. In\nparticular, you're simultaneously defining the return as a function on\n$\\mathcal{S}$ and on $\\Delta(\\mathcal{S})$.\n\nThe notation $v^\\pi_1$ in Definition 3 is technically not defined. I'm\nassuming this implicitly represents $v^\\pi_{r, h}$ where $r$ is the\nreward function in the MDP.\n\nIn theorem 1, you have defined $K := |\\mathcal{D}|$, so you can (and\nshould) replace the $\\forall (d, K)\\in\\mathcal{H}\\times\\mathbb{N}$ with\n$\\forall d\\in\\mathcal{H}$."
            },
            "questions": {
                "value": "Is assumption 1 actually mild? My interpretation of $\\nu(\\tau) = 0$ is\nthat no constraints are violated. Assumption 1 says any such policy has\nthe same preference distribution, which seems bizarre. Consider for\nexample a CartPole swingup domain where the constraint function is\npositive when the velocity of the cart is above a threshold, and zero\notherwise. Then if the system is initially stationary (pole in the\ndownward position), there is no preference between an agent that does\nnothing and one that swings up the pole successfully.\n\nWhat is $\\xi$ supposed to represent on line 722? Is this the constraint\nviolation probability written as $\\delta$ in the main text?\n\nIn MDP in the proof of proposition 3, is $H=1$?\n\nIn propositions 3 and 4, I think you want probabilities instead of\nindicators.\n\nI'm having difficulty following the proof of Proposition 3. How did you\ncome up with that upper bound on $\\hat{c_1}(s_1, a_2, s_2)$? If you're\njust going for existence, can't you find much simpler settings of\n$\\hat{c}$?\n\nIn Proposition 4, what is the purpose of the indicator? The clause\ninside the indicator is deterministic.\n\nIf Lemma 1 is not identical to Russo and Van Roy's Lemma 4, it should\nhave a proof. What is actually the difference between these two? It\nlooks to me like you're using $\\psi_k$ to be the cumulant generating\nfunction of $U_k$ instead of $Z_k$, is that it? Anyway, I think there\nshould be a proof here.\n\nRegarding Figure 5, is it known that existing ICRL methods struggle in\nthis environment? Are there no baselines that can solve this\nenvironment? That sounds suspicious. It also very much weakens the claim\nthat your method achieves superior performance\u2014I suspect some strong\nbaselines are missing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides a theoretical analysis on the feasibility of constraint learning from two types of human feedback and provides results quantifying the learning complexity of multiple constraint learning from trajectory-based feedback. The authors furthermore introduce a policy-gradient formulation based on the lagrangian objective corresponding to the latter theoretical result and develop a practical algorithm, labelled CCPG, that jointly learns constraints and a policy from trajectory-labelled data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper addresses a highly relevant problem, namely the inference of constrained objectives from coarsely-labelled data and its theoretical foundations: \n- The authors provide a thorough theoretical analysis of this learning paradigm in the case of trajectory feedback and policy feedback and show (to my knowledge) novel insights regarding the feasibility of constraint learning depending on the type of feedback. \n- The main theoretical finding establishes a risk bound that quantifies the learning complexity of learning constraints in terms of (among others) trajectory count, model class and permitted constraint violation probability. \n\nThese results are relevant to the field, as they lay a sound motivation for practical algorithms. The empirical results are furthermore reasonably aligned with the theoretical analysis."
            },
            "weaknesses": {
                "value": "- The proof of proposition 1 leaves some questions open to me (see questions).\n- I believe, a natural addition that many readers would appreciate in this context is if the analysis extended to preference-based feedback.\n- In addition to the point above, it would be interesting to see an experimental confirmation of the contents of proposition 1 and 2 (i.e. the learnability of constraints from diffferen types of feedback).\n\n\nGenerally speaking, the paper is written in concise and clear language, however there are a number of typos and sentences/notation I would consider rephrasing:\n- line 39, \"comprehensive\" is too strong in my view\n- line 40, \"fills\"\n- line 51, \"a more general constraints\"\n- line 53, \"empirical validated\"\n- line 57, I don't know the term \"curly alphabets\"\n- line 65, \"finte\"\n- line 79, the shorthand for the value function taking a probability distribution as an input should be mentioned\n- line 110, \"a constrained RL\""
            },
            "questions": {
                "value": "Concerning Propositions 1 & 2: \n\nIn proposition 3 and the subsequent paragraph (l. 738) it is stated that an estimated cost function can perfectly reconstruct a trajectory feedback dataset in the sense of Eq. 4) but it is not clear to me why we evalute the estimated cost function $\\hat{c}$ through the additional indicator function. In this example MDP, from what I can see a reasonable estimator $\\hat{c}_1$ should be able to converge to $\\hat{c}_1(s_1,a_2,s_2)=1$ and $0$ for any other possible trajectory. Would this estimate not enable one to recover safe policies? \n\nConcering the impossibility results with policy feedback: \n\nI fail to see the difference between the trajectory feedback setting and a policy feedback setting where annotators observe one trajectory sampled from policy $\\pi_k$. And consequently how could one obtain an impossibility result for one, if both feedback forms can be equal?\n\nI believe the overall contribution to be valuable to the literature and am willing to raise my score if the above points are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper focuses on constraint learning from human feedback (CLHF) and proposes a new decision function based on trajectory feedback for CLHF. It demonstrates that cost functions learned from feedback may not accurately represent the true constraints, which can result in policies that appear safe but are not actually safe in the true environment. The authors also provide some theoretical analysis to support their claims."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) CLHF is a valuable research direction and it is important for both reinforcement learning applications and the security of large language models (LLMs).\n2) The authors propose a new decision function based on trajectory feedback and provide the theoretical analysis for the proposed decision function."
            },
            "weaknesses": {
                "value": "1) There is no clear representation of the motivation and contributions of the paper.\n2) The theoretical analysis is limited, aspects such as convergence analysis and constraint violations related to constraint learning are not addressed.\n3) The overall presentation of the paper might benefit from improvements, as it does not clearly convey its main claims and contains some expression errors.\n4) The experimental results do not seem to adequately support the theoretical analysis. For example, Figure 3 shows significant constraint violations in CCPG w/PC.\n5) The paper lacks additional necessary experiments, including comparison experiments, ablation studies, and hyperparameter analysis, etc.\n6) The baselines and experimental environments in the paper are too few to illustrate the validity of the method.\n7) There is no code or detailed implementation description provided to support the reproducibility of the results."
            },
            "questions": {
                "value": "1) What do $q$ and $P$ represent in Algorithm 1?\n2) Could the authors discuss the reasons behind the constraint violations observed in CCPG during the experiments, as well as the differences in performance between CCPG w/PC and CCPG w/TC across different environments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}