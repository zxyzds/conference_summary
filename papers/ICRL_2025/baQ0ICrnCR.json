{
    "id": "baQ0ICrnCR",
    "title": "Robotic Programmer: Video Instructed Policy Code Generation for Robotic Manipulation",
    "abstract": "Zero-shot generalization across various robots, tasks and environments remains a significant challenge in robotic manipulation. Policy code generation methods use executable code to connect high-level task descriptions and low-level action sequences, leveraging the generalization capabilities of large language models and atomic skill libraries. In this work, we propose Robotic Programmer (RoboPro), a robotic foundation model, enabling the capability of perceiving visual information and following free-form instructions to perform robotic manipulation with policy code in a zero-shot manner. To address low efficiency and high cost in collecting runtime code data for robotic tasks, we devise Video2Code to synthesize executable code from extensive videos in-the-wild with off-the-shelf vision-language model and code-domain large language model. Extensive experiments show that RoboPro achieves the state-of-the-art zero-shot performance on robotic manipulation in both simulators and real-world environments. Specifically, the zero-shot success rate of RoboPro on RLBench surpasses the state-of-the-art model GPT-4o by 11.6\\%, which is even comparable to a strong supervised training baseline. Furthermore, RoboPro is robust to different robotic configurations, and demonstrates broad visual understanding in general VQA tasks.",
    "keywords": [
        "robotic manipulation",
        "vision language models",
        "code generation"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "We propose RoboPro, a robotic foundation model that performs zero-shot robotic manipulation by following free-form instructions with policy code. To improve data efficiency, we propose Video2Code to synthesize executable code from videos.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=baQ0ICrnCR",
    "pdf_link": "https://openreview.net/pdf?id=baQ0ICrnCR",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a method, RoboPro, that utilizes action-free video for zero-shot policy code generation in robotic manipulation tasks. The method consists of two components: a video-to-code model that generates robotic runtime code, and a code-generation policy trained on the synthesized code. In experiments, RoboPro was shown to be effective in performing tasks in unseen environments in a zero-shot manner."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The idea and motivation of using action-free data videos to synthesize robotic runtime code, which was then used to train a code policy is novel and interesting.\n\n2. The experimental results demonstrate that RoboPro can perform the task in a zero-shot manner, which is compelling."
            },
            "weaknesses": {
                "value": "1. [Major] While the paper compares its approach to BC-based and code-generation methods, I\u2019m curious how it performs compared to recent methods that use VLM prompting without model training, like MOKA [1], which leverages VLM reasoning and motion primitives, and PIVOT [2], which uses iterative visual prompting. These methods generally make fewer computational assumptions than RoboPro since they don\u2019t require any module training.\n\n\n2. [Major] In the real-world experiments, there is no comparison to other baselines. Would it be possible to include a comparison against at least GPT-4o to see if RoboPro performs better?\n\n\n3. [Major] While the idea of using action-free video to synthesize runtime code data is interesting, the impact of the chosen video datasets remains unclear. In this paper, the authors use DROID as their dataset\u2014was there any specific reason for this choice? How do the type and size of the dataset affect the final performance? If I\u2019m understanding correctly, in principle, the video data doesn\u2019t even need to be robotic. How would the approach perform if human videos, such as Ego4D or Something-Something, were used instead? It would be interesting to include the ablation over these points.\n\n\n4. [Minor] The downstream performance appears to be sensitive to the choice of VLMs used for draft/code generation, as shown in Table 6. Specifically, there is a large difference in results between using Gemini and DeepSeek for code generation. This sensitivity to the VLM choice may be a weakness of this approach.\n\n---\nReference\n\n[1] Fang et al., MOKA: Open-World Robotic Manipulation through Mark-Based Visual Prompting, 2024.\n\n[2] Nasiriany et al., PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs, 2024."
            },
            "questions": {
                "value": "1. How accurate is the generated code from the data curation pipeline? Are there any errors or failure cases and how are they handled?\n\n\n2. Could the authors provide more details on the training procedure? \u2013 e.g. hyperparameters, training steps, and the computational resources and time required for training, etc\n\n\n3. During the deployment phase, is it correct that only the initial image of the environment is used to generate the entire execution code, without using the newest images at each time step?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents RoboPro (Robotic Programmer), a robotic foundation model that generates executable policy code from visual information and language instructions to perform robot manipulation tasks without additional fine-tuning. The authors propose Video2Code, an automatic data curation pipeline that synthesizes code execution data from large-scale instructional videos. Extensive experiments are conducted in both simulators and real-world environments to validate the model's effectiveness. In short, the reviewer thinks this paper has good presentation, thorough evaluations, but lacks novelty and insights."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The Video2Code pipeline bridges the gap between visual understanding and code generation by combining the visual reasoning ability of VLMs and the coding proficiency of code-domain LLMs. This low-cost and automatic method reduces reliance on manually constructed datasets and expensive simulation environments.\n\n2. RoboPro shows adaptability to different API configurations and compatibility across environments (simulators like RLBench and LIBERO, as well as real-world settings), underscoring its robustness and usability in diverse practical scenarios.\n\n3. Extensive experiments in simulations and real-world scenarios verify the model's code generation abilities, with analysis on how different code LLMs affect performance."
            },
            "weaknesses": {
                "value": "1.The auto code data collection is intuitive and simple, which does not count as \"novel\" for the VLM agent training. The auto code collection pipeline is natural by itself and has been adopted in many applications like multi-modal OS agents and game agents. However, since you have real-time feedback from the real worlds, if would be of more interesting how you could accelerate this data collection and enhance the code quality from the multi-modal reflection on the world feedback.  \n\n2.The use of a foundation model for code generation lacks methodological innovation, as has been cited by the authors, there are already published papers that can fall into this category. The authors should try to highlight the difference of this work among other reference methods.  \n\n3.The model's zero-shot capability is restricted by the predefined API library it can call."
            },
            "questions": {
                "value": "1. Since the authors have conducted comprehensive experiments, maybe they can share more insight about the limitations of code generation approaches compared to VLAs? By comparing the difference between these two approaches, it would be more interesting for the authors to share some insights about what can be done or what cannot be done by their approach and where direction we could go for the future work. \n\n2. Include a more detailed analysis of failure cases, distinguishing between issues related to LLM reasoning and API limitations. This could provide more insight for this paper. It's unclear whether failures are due to issues with LLM Chain-of-Thought reasoning or problems with the API itself, etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents RoboPro which enables zero-shot performance on robot manipulation tasks by converting observations and language instructions into executable policy code. RoboPro's code generation is enhanced by Video2Code, a data curation pipeline that outputs executable code from input videos."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.  Experiments are comprehensive, with both simulation and real-world tasks, as well as general VQA tasks."
            },
            "weaknesses": {
                "value": "1. Performance on more complex long-horizon tasks is not thoroughly explored.\n\n2. RoboPro depends on consistent API libraries. It's unclear how this method scales with open-ended real-world tasks of arbitrary complexity."
            },
            "questions": {
                "value": "1. Can the authors provide additional contextualization in lines 122-136 with prior works such as [1]?\n\n2. I'm also not sure how scalable this method actually is. Certainly the generalizability of LLMs is well-leveraged, but the interface between various components seems like a bottleneck. Can the authors explain how the code API returned by Video2Code can be adapted if new functions are required to describe tasks in new video demonstrations? It seems like changing the code API requires re-running the Video2Code pipeline on the entire video dataset?\n\n[1] [Programmatically Grounded, Compositionally Generalizable Robotic Manipulation](https://arxiv.org/abs/2304.13826)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes RoboPro, a high-level planner that generates sequences of low-level API calls based on language instructions and observations. It leverages a two-stage approach, using a Vision-Language Model (VLM) and a code Language Model (code LLM) to process videos and generate runtime code data, which is then used for fine-tuning RoboPro. Experiments cover nine tasks from RLBench, eight tasks from LIBERO, and several real robot tasks, demonstrating RoboPro's zero-shot planning generalization when low-level skills and APIs change."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This work processes videos to generate robot planning data using a two-stage approach.\nThe experiments demonstrate the model's planning generalization ability across two benchmarks."
            },
            "weaknesses": {
                "value": "1. The contributions of this work are not clearly summarized.\n2. The writing could be improved, as there are many redundant sentences that convey the same ideas. The structure and presentation are somewhat difficult to follow.\n3. The implementation of low-level skills is not well-explained for both simulation and real work experiments. The assumption of predefined API calls and low-level skills is too strong for developing effective manipulation policies.\n4. The experiments are primarily limited to low-precision, open-loop pick-and-place tasks."
            },
            "questions": {
                "value": "Since RoboPro is pre-trained on related robot runtime code data, were the baseline planners also fine-tuned on this data?\n\nPeract is a multi-task policy learned from demonstrations, directly outputting the keyframe action without a high-level planner. How is it an apples-to-apples comparison with RoboPro which uses a high-level planner equipped with low-level skills?\n\nRoboPro is referred to as a generalist. However, popular generalist baselines like RT-2[1] and Open-VLA[2] are missing from the comparisons.\n\nCan RoboPro complete tasks when a target is moving during planning? Is there some way to address it?\n\nCould you briefly explain how RoboPro might be extended to solve deformable object manipulation tasks, such as folding cloth?\n\nWhat is the main difference between RoboPro and RoboCodeX, aside from the pre-training on robot runtime data?\n\n[1] RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control\n[2] Open-VLA: An Open-Source Vision-Language-Action Model"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}