{
    "id": "z1ohBxWeL2",
    "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation",
    "abstract": "LLM inference for popular enterprise use cases, such as summarization, RAG, and code-generation, typically observes orders of magnitude longer prompt lengths than generation lengths. This characteristic leads to high cost of prefill and increased response latency. \nIn this paper, we present SwiftKV, a novel model transformation and distillation procedure specifically designed to reduce the time and cost of processing prompt tokens while preserving high quality of generated tokens. SwiftKV combines three key mechanisms: i) SingleInputKV, which prefills later layers' KV cache using a much earlier layer's output, allowing prompt tokens to skip much of the model computation, ii) AcrossKV, which merges the KV caches of neighboring layers to reduce the memory footprint and support larger batch size for higher throughput, and iii) a knowledge-preserving distillation procedure that can adapt existing LLMs for SwiftKV with minimal accuracy impact and low compute and data requirement. For Llama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50% and the memory requirement of the KV cache by 62.5% while incurring minimum quality degradation across a wide range of tasks. In the end-to-end inference serving using an optimized vLLM implementation, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100 GPUs. Our training, inference, and model implementations are open-sourced at https://anonymized.link.",
    "keywords": [
        "LLM",
        "Inference",
        "System",
        "Compression",
        "Distillation"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "This paper presents a model transformation and distillation approach that reduces prefill computation by 50%, memory usage by 62.5%, and delivers 2x higher inference throughput, all with minimal impact on model quality.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=z1ohBxWeL2",
    "pdf_link": "https://openreview.net/pdf?id=z1ohBxWeL2",
    "comments": [
        {
            "summary": {
                "value": "This paper studies the optimization of inference in Transformer-based LLMs. It presents SwiftKV, a solution to reducing the KV cache and inference time to long contexts up to 128K. The proposed method features three parts: SingleInputKV, AcrossKV, and knowledge recovery. Experiments show the effectiveness of the proposed method and the usefulness of its components, as well as how they work jointly with other optimization techniques."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1. Inference optimization for in Transformer-based LLMs is an important topic which has been extensively studied in recent years.\n\nS2. Several key components have been proposed in this paper, with their usefulness showcased in the evaluation. \n\nS3. The proposed method is orthogonal to many existing optimizations and they can be used jointly to further optimize the performance."
            },
            "weaknesses": {
                "value": "W1. This paper borrows borrowing observations and ideas from SingleInputKV, as stated in the submission (such observation has also been utilized in the InfiniGen paper published at OSDI 2024).\n\nW2. A core technique in the proposed method is cross-layer KV cache compression. The comparison/discussion with state-of-the-art KV cache compression/merging/cross-layer works is missing, e.g., PyramidKV and infini-attention. It is encouraged to discuss the difference and the novelty compared to existing KV cache compression techniques in the related work. Some surveys can be found here:\nhttps://github.com/October2001/Awesome-KV-Cache-Compression\n\nW3. Whereas the paper discusses long inputs, it lacks discussions with recent works on long contexts (see the above link), such as MInference, which optimize the prefilling of long contexts. Some of them exploit the sparsity to reduce KV cache and speed up inference, e.g., ALISA. \n\nW4. The proposed method is not training free, yet only Llama-3.1 models are evaluated. It is unclear if the performance (and its optimal parameter settings) also translates to models. Extension to other open models, such as Mistral, would be beneficial to understanding the contributions of this work."
            },
            "questions": {
                "value": "Q1. In Table 1, there is a significant drop in performance for 8B model on the math dataset GSK-8K. I suppose this is the harder case, meaning that the proposed method may not work well for the case of small models on tasks demanding more logic and reasoning. An analysis on why this performance drop occurs would be interesting. \n\nQ2. In Table 3, to show the impact of distillation, AcrossKV is disabled. However, to reduce KV cache, AcrossKV should be enabled for inference serving, right?\n\nQ3. In Table 4, the performance of \"our fine-tuned model\" is significantly inferior to the base model. The result seems to be negative to the usefulness of your techniques. I don't quite understand the logic here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper studies LLM core technology. I don't find anything that needs ethics review in this paper."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes two methods to reduce the cost of the prefill stage during LLM inference. \nThe first method, called SingleInputKV, reuses the output hidden state vector from the i-th layer in attention layers as the input vector to generate key and value (KV) vectors of the subsequent layers. In previous methods, the j-th layer used the output hidden state vector from the (j-1)-th layer as input.\nThe second method, called AcrossKV, enables the KV vectors generated by the i-th layer to be reused by the following layers. In previous methods, each layer generates its own KV vectors by multiplying the input vector with its weight.\nThese techniques reduce computational costs by reusing the input and KV vectors of earlier layers for later layers. They also decrease the number of KV vectors that need to be cached for the decode stage. The proposed methods build on prior work [1], which showed minimal differences in the values of input vectors across layers as the number of layers increases in transformers.\nThe authors implemented these techniques in Llama-3.1-8B and Llama-3.1-70B models,  showing that while the performance on the LLM benchmark remains largely unaffected, both time and memory usage in the prefill stage are reduced by almost two times.\n\n[1] Songwei Liu, Chao Zeng, Lianqiang Li, Chenqian Yan, Lean Fu, Xing Mei, and Fangmin Chen. Foldgpt: Simple and effective large language model compression scheme, 2024c. URL https://arxiv.org/abs/2407.00928."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1. The paper proposes two techniques derived from insights from prior research, demonstrating their efficacy in reducing computational and memory costs during LLM inference.\n\nS2. The authors show that fine-tuning can alleviate the decline in benchmark scores, emphasizing the practicality of the proposed methods without notably sacrificing model performance."
            },
            "weaknesses": {
                "value": "W1. The experiments in the paper are somewhat limited.\n- The authors evaluate the proposed techniques only on Llama-3.1 models. Testing a wider variety of models would strengthen the results. If the proposed methods could demonstrate their benefits across transformer models with different attention mechanisms (e.g., sparse attention, low-rank attention), scaling approaches (e.g., wide scaling, deep scaling, sparse scaling), and sizes (Llama-3.2-1B, Llama-3.2-3B, Llama-3.2-8B,  Llama-3.2.-11B, Llama-2-13b, and Llama-3.2-70B, Llama-3.2-90B, and Llama-3.1-405B), it would enhance the paper\u2019s contribution. \n- The authors need to demonstrate whether applying SwiftKV to larger models yields more significant results compared to small models. Incorporate models such as  Llama-2-13B and Llama-2-7B. If applying the proposed methods to Llama-2-13B yields better results than Llama-2-7B in terms of both cost and the benchmark scores, it would strengthen the contribution of the paper.\n- There is no experiment to show the independent effect of AcrossKV without the presence of SingleInputKV, leaving the isolated impact of AcrossKV unexplored. The authors need to compare a baseline model to one with only AcrossKV applied.\n\nW2. The justification for the claimed reduction in computational cost is insufficient. The paper needs to specify which operations are being skipped by SingleInputKV clearly by providing a detailed breakdown of the computational costs for each component of a Transformer model, comparing the baseline to SwiftKV. In Figure 1, SingleInputKV still appears to need to generate the output hidden state vector of every attention layer, which is a primary computational task in Transformer models. This is because the proposed method needs to generate the query vector for each attention layer in Figure 1, and the output hidden state vector from the (i-1)th layer is required to compute the query vector for the i-th layer."
            },
            "questions": {
                "value": "Please refer to W1 and W2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "SwiftKV proposes several techniques to improve to reduce computation and memory footprint for LLM inference while maintaining a similar level of accuracy. In particular, they propose to skip the pre-fill stage of later layers by rewiring the model and rely instead on intermediate computation results from an earlier layer, leading to a reduced amount of computation. The authors also reduce the memory footprint of later layers by sharing a KV cache across multiple subsequent layers. Additional they use a distillation/fine-tuning process of the affected model part to reduce the difference in accuracy compared to the original model. They show in their evaluation computation improvements for throughput as well as latency."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The ideas for the various optimizations are presented reasonable clearly and they seem novel as well, especially their combination. The evaluation on a number of models and datasets/benchmarks supports their performance claims and a reasonable ablation study is provided as well."
            },
            "weaknesses": {
                "value": "For me, the biggest issue is that end-to-end results are missing, which makes it hard for me to put the presented inference results (throughput, latency) into context, which also makes me question how useful the presented numbers are.\n\n* apart from SingleInputKV, all the other optimizations are not properly motivated regarding the reasoning why they should work (some form of microbenchmark)\n* end to end results are missing, especially since some of their writing, if I am not mistaken, suggests that they target a part of the pipeline that only compromises 5% of the \"runtime\"\n* line 314: \"The accuracy drops by another 0.32, going from 2-way to 8-way\" - it is actually 1.32 according to the table\n  * similarly the number for 16-way is wrong as well\n* The authors might consider removing 5.5 to have more space for presenting the other content/result in more detail.\n* not clear why the used benchmarks are representative for the use cases mentioned as motivation in the introduction\n\ndetailed copy editing comments:\n* related work:\n  * \"their optimized implementations in TensorRT (NVIDIA, 2019)\" - all previously mentioned techniques were published after 2019\n* Figure 2, right side:\n  * The parameters used in the legend are not explained at all in the caption. It is possible to understand after reading the subsequent text.\n  * The subfigure is never actually referenced in the text, except in the appendix, as \"proof\" for some statement later and in a footnote.\n* Figure 4: Artifacts in the layering of the curves, sometimes a dot is at the top for one datapoint and and then further down for other datapoints. But maybe that was intentional?\n* Table 3: There is a horizontal line missing after \"(a) The effect of distillation\".\n* line 447: \"which suggest that MLP layers player a more prominent role\" - missing verb, probably it should be \"play\" instead of \"player\"\n* Figure 5 is not readable\n* minor issues:\n  * typos:\n    * line 119: \"Tensor-Parallelism(Shoeybi et al., 2020)\" - missing space\n    * lines 130 to 138: additional brackets around the year for the citations\n    * line 157/158: \"(Holmes et al., 2024; Agrawal et al., 2024))\" - additional bracket at the end \n    * line 360: \"toks/sec\" - probably \"token/sec\"\n    * line 859: \"superme\" - perhaps \"supreme\"?\n    * line 923: \"hyper-paramter\" - missing e\n    * line 923: \"but did not invest deeper\" - probably \"investigated\"\n  * references:\n    * Clark et al. 2018: cited differently than the other arXiv papers\n    * Cobbe et al. 2021: misses place, where the paper was published\n    * Dao et al. 2024:\n      * year states 2024, but conference abbreviation suggests 2022\n      * conference abbreviation is nowadays NeurIPS\n    * Ding et al. 2021: misses place, where the paper was published\n    * Elhage et al. 2021: url not clickable\n    * GretelAI 2024: url not clickable\n    * Hendrycks et al. 2021: cited differently than the other ICLR papers\n    * Hinton et al. 2015: cited differently than the other arXiv papers\n    * Kuzim et al. 2024:\n      * year states 2024, but conference abbreviation suggests 2022\n      * conference abbreviation is nowadays NeurIPS\n    * Lewis et al. 2020: conference abbreviation is nowadays NeurIPS\n    * Liu et al. 2024a:\n      * cited differently than the other arXiv papers\n      * cited twice (2024b)\n    * Liu et al. 2024d:\n      * year states 2024, but conference abbreviation suggests 2023\n      * conference abbreviation is nowadays NeurIPS\n    * Meng et al. 2024:\n      * year states 2024, but conference abbreviation suggests 2022\n      * conference abbreviation is nowadays NeurIPS\n    * Pourreza and Rafiei 2024:\n      * year states 2024, but conference abbreviation suggests 2023\n      * conference abbreviation is nowadays NeurIPS\n    * Sakaguchi et al. 2019: cited differently than the other arXiv papers\n    * Wei et al. 2023: cited differently than the other arXiv papers"
            },
            "questions": {
                "value": "* Section 3.4, Knowledge Recovery: The description suggests that the distillation is done for every of the later layers, but Figure 1 suggests that at least W_K and W_V are only trained for the initial layer of each AcrossKV block.\n* Why are the results more or less consistently better for 4-way caching compared to 2-way caching for the 70B model? That seems kind of counterintuitive.\n* footnote 5, page 7: What are the end-to-end results?\n* Section 4.3: \"a combined throughput of over 16K toks/sec over 4xH100 GPUs which corresponds to 560 TFLOPS/GPU\"\n  * So that is around 4k token per second for each GPU compared to 30k tokens/sec/GPU for 8B Llama model. But because the 70B model is much more complex, there are more floating point operations necessary?\n  * Any notion why the pure compute performance increases despite a more \"distributed\" setting (multiple GPUs)?\n* Any notion why the full model fine-tuning performs so much worse than the partial model fine-tuning?\n* Section 5.3: \"This may be due to the lack of math and coding examples in the two datasets we picked to train the model.\"\n  * Why did you choose these datasets, if at least the coding use case is serving as a motivational example?\n* Doesn't the discussion in Appendix B the whole point of the paper, i.e. trying to optimize a part than accounts for less than 5% of the total compute time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes SwiftKV, a method that reduces LLM inference latency while preserving knowledge. SwiftKV combines Early Exit, KV cache compression, and Knowledge Distillation techniques, demonstrating latency improvements in performance evaluation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The writing is good.  \n2. Experimental results indicate that the proposed method effectively reduces latency while preserving knowledge."
            },
            "weaknesses": {
                "value": "1. Limited Efficient Experiments: VLLM serves as the only baseline in the performance results, which limits the demonstration of this work\u2019s necessity and effectiveness. The authors\u2019 method is a lossy optimization approach, and they should compare it with more serving systems to demonstrate respective performance improvements and knowledge retention. Although other methods may not conflict with the authors' approach, they may not be easily integrated (e.g. the strategy of Early Exit can hardly apply to Speculative Decoding[1], or combined with certain sparse attention methods like PowerInfer[2], quantization method like GPTQ[3] may result in significant performance degradation.). If the authors cannot demonstrate the effectiveness of their method compared to others, or show that it can integrate with other methods for added benefits, the significance of this work is greatly diminished.\n\n2. Lack of Key Assumptions: Some critical assumptions are missing, such as noting that latency-sensitive servers often adopt disaggregated systems to handle the prefill and decode stages separately. This omission could impact the reported TTFT and TPOT performance results, because in the disaggregated systems, TPOT will hardly be influenced due to improvements in the prefill stage.\n\n[1] Cai, Tianle, et al. \"Medusa: Simple llm inference acceleration framework with multiple decoding heads.\" arXiv preprint arXiv:2401.10774 (2024).\n\n[2] Song, Yixin, Zeyu Mi, Haotong Xie, and Haibo Chen. \"Powerinfer: Fast large language model serving with a consumer-grade gpu.\" arXiv preprint arXiv:2312.12456 (2023).\n\n[3] Frantar, Elias, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. \"Gptq: Accurate post-training quantization for generative pre-trained transformers.\" arXiv preprint arXiv:2210.17323 (2022)."
            },
            "questions": {
                "value": "1. The code link appears to be invalid. Could you make the code open-source to enhance reproducibility?\n2. SwiftKV focuses primarily on optimization during the prefill stage. How should we interpret the decrease in TPOT shown in the performance results?\n3. Could you provide results comparing the performance of SwiftKV with more competitive baselines, such as Minicache, as mentioned in your paper? Could you clarify the connections and differences between your method and existing work, including its strengths and weaknesses? Could you demonstrate whether your method can be integrated with other approaches? Additionally, could you outline the potential application scenarios for your method?\n4. As I understand, most datasets used in your paper consist of multiple-choice questions, leading to longer prefill times and shorter decoding times. I\u2019m interested in seeing SwiftKV's performance on more diverse datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}