{
    "id": "0OTVNEm9N4",
    "title": "Rethinking Copyright Infringements In the Era Of Text-to-Image Generative Models",
    "abstract": "The advent of text-to-image generative models has led artists to worry that their individual styles may be copied, creating a pressing need to reconsider the lack of protection for artistic styles under copyright law. This requires answering challenging questions, like what defines style and what constitutes style infringment. In this work, we build on prior legal scholarship to develop an automatic and interpretable framework to \\emph{quantitatively} assess style infringement. Our methods hinge on a simple logical argument: if an artist's works can consistently be recognized as their own, then they have a unique style. Based on this argument, we introduce ArtSavant, a practical (i.e., efficient and easy to understand) tool to (i) determine the unique style of an artist by comparing it to a reference corpus of works from hundreds of artists, and (ii) recognize if the identified style reappears in generated images. We then apply ArtSavant in an empirical study to quantify the prevalence of artistic style copying across 3 popular text-to-image generative models, finding that under simple prompting, $20\\%$ of $372$ prolific artists studied appear to have their styles be at risk of copying by today's generative models. Our findings show that prior legal arguments can be operationalized in quantitative ways, towards more nuanced examination of the issue of artistic style infringements.",
    "keywords": [
        "evaluating copying",
        "copyright",
        "generative ai",
        "text-to-image",
        "ai art",
        "law",
        "interpretability",
        "social impact"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "Rethinking how we define artistic style and characterize style infringement, with an efficient and interpretable quantitative framework based in legal scholarship",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=0OTVNEm9N4",
    "pdf_link": "https://openreview.net/pdf?id=0OTVNEm9N4",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces ArtSavant, a tool designed to assess artistic style copying in text-to-image generative models. Built on legal scholarship, ArtSavant combines two methods, DeepMatch (a neural classifier) and TagMatch (an interpretable tag-based approach), to detect unique artistic styles and assess whether they are replicated in generated images. An empirical study using ArtSavant indicates that around 20% of the artists in their dataset appear at risk of style copying by generative models, raising concerns for the need to protect artistic styles under copyright."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper addresses a timely issue -- potential copyright infringements in text-to-image generation -- that bridges technical, legal, and ethical domains. \n2. ArtSavant\u2019s combination of DeepMatch and TagMatch represents a thoughtful approach, with one method offering high accuracy and the other interpretability. This approach is likely beneficial for non-technical audiences, such as legal professionals and artists.\n3. The paper is well-grounded in legal discussions, positioning ArtSavant as a tool that can potentially support legal decision-making regarding style infringement."
            },
            "weaknesses": {
                "value": "1. The use of a limited reference dataset (372 artists) could affect the generalizability of ArtSavant\u2019s findings, especially for artists with less established styles. Expanding the dataset to include more diverse artistic styles could strengthen the conclusions.\n2. ArtSavant may struggle with assessing artists whose work doesn\u2019t conform to traditional or well-known styles, limiting its broader applicability. It may inadvertently favor more mainstream artistic elements, possibly overlooking style copying for non-Western, niche, or experimental art styles.\n3. Although TagMatch aims to make the tool interpretable, the subjectivity inherent in artistic tagging could affect its reliability, especially in legal contexts. This may be partially addressed by improving tagging accuracy, as noted by the authors."
            },
            "questions": {
                "value": "1. How does ArtSavant perform when applied to more obscure or emerging artists whose styles may be less distinctive or well-known?\n2. The TagMatch method relies on zero-shot tagging with CLIP, which may not capture subtleties in artistic style. Have the authors considered evaluating the reliability of TagMatch across different art genres or complex styles, and could a more refined tagging approach improve interpretability and consistency?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces ArtSavant, an explainable classifier for identifying artistic style infringements in generated art. The proposed framework consists of DeepMatch, a black-box neural classifier, and TagMatch, an interpretable tag-based method, to quantify the uniqueness of an artist\u2019s style and recognize if it appears in generated images. The central idea is that if an artist\u2019s works are consistently recognizable, they contain a unique style that can be classified. The approach uses both holistic and analytic style comparisons. It combines CLIP embeddings and tagged stylistic elements to support style infringement claims in a legally relevant, interpretable manner."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. TagMatch offers an interpretable method for identifying stylistic elements, making it particularly valuable in legal contexts where explainability is essential.\n\n2. The paper includes a comprehensive evaluation of the proposed methods, including both quantitative and human evaluation."
            },
            "weaknesses": {
                "value": "1. TagMatch relies on LLMs to generate concept vocabularies, which may limit its effectiveness for less-known artists whose stylistic elements may not be well-covered in pretraining data. Could you show how TagMatch performs on less-known artists? If there are some gaps between known and well-known artists, I am curious if there is way to enhance the vocabulary to better capture these unique styles?\n\n2. DeepMatch uses a back-box for detection. However, such black-box classifiers may  pick up on spurious details rather than genuine stylistic features. For example, if an artist always includes a certain animal in his art works, DeepMatch might use this feature to classify the style. Could you provide some evidence that DeepMatch\u2019s classification is based on broader stylistic elements instead of just this minor feature?\n\n3. The preliminary study uses DINO features, which might be limited in representing stylistic nuances. Could you explore using features that are specifically trained for style similarity [1] to compare with your method as a baseline? What is the pro and con for classifier based approach proposed in this paper and embedding based approach? \n\n\n4. The authors noted that a new artist could easily retrain the detector to include their works for the DeepMatch approach, as it\u2019s quite efficient. However, I\u2019m curious about the potential impact on performance. Does retraining lead to issues like catastrophic forgetting of previously learned styles? It would be interesting to see a case study where the existing classifier is expanded to include new artists, observing how this affects both new and original classifications.\n\n[1] Unsupervised Image Style Embeddings for Retrieval and Recognition Tasks"
            },
            "questions": {
                "value": "1. In line 21, there should be a space between the method name and the next word. \n\n2. How many training examples from one artist are required to reliably detect the style of that single artist in DeepMatch? \n\n3. Do DeepMatch and TagMatch provide different predictions for certain examples? If so, in what situations does this occur, and what are the characteristics of these artworks that lead to differing predictions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores a significant question of how GenAI might infringe upon the styles of individual artists and if legal frameworks could protect these styles. In particular, the author developed a tool, ArtSavant, to measure and quantify artistic style infringement. ArtSavant mainly utilizes two methods: \n* DeepMatch: aneural network classifier to establish a recognizable \"signature\" for an artist's style based on images. \n* TagMatch: An interpretable, tag-based method, which decomposes artworks into stylistic elements or \"tags\".\n\nTheir empirical results show that GenAI models have the potential to reproduce unique artistic styles, rasing copyright concerns."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-written and addresses a timely, important problem relevant to today\u2019s AI and creative industries. The authors provide a solid combination of qualitative and quantitative results that contribute valuable insights into the field.\n2.  Considering \u201cstyle\u201d as a central focus is an innovative approach. By shifting from image-wise similarity detection to a style-based classification specific to individual artists, the paper redefines the task in a way that offers a deeper understanding of style infringement.\n3. The paper also emphasizes interpretability through the TagMatch method, which is especially useful in legal contexts, where clarity on how stylistic similarities are identified can support arguments around style infringement."
            },
            "weaknesses": {
                "value": "1. Although I enjoyed reading this paper and find \u201cstyle\u201d to be an intriguing approach to this problem, I am concerned about the inherent ambiguity surrounding this concept. The paper assumes that \u201cstyle\u201d can be quantitatively defined and detected, yet style is fundamentally a qualitative and fluid concept, often shaped by subjective interpretation. Additionally, even in the real world, many artists have very similar \u201cstyles,\u201d which complicates the notion of unique stylistic signatures.\n\n2. I wonder how a similarity-based method would perform on this dataset (please correct me if I missed this comparison in the paper). Are there cases where the style-based method detects something that a similarity-based method does not, or vice versa? A direct comparison could provide clearer insights into the advantages and limitations of each approach.\n\n3. Regarding TagMatch, I understand its goal of enhancing interpretability; however, I find it somewhat limited in scope. First, it\u2019s a relatively naive approach in some respects, relying solely on zero-shot CLIP with predefined tags. Second, \u201cstyle\u201d implies something more subtle and nuanced than broad artistic categories. Even within the same category, there can be vast differences between artworks, so I\u2019m unsure of TagMatch\u2019s practical utility in capturing the deeper, unique aspects of an artist\u2019s style."
            },
            "questions": {
                "value": "1. Could you provide more quantitative and qualitative discussions for similarity-based vs. style based method\n2. would appreciate any further clarifications regarding my concerns about Weaknesses. And I am willing to raise my score if I find them convincing"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper aims to develop an \u201cintuitive, automatic, legally-grounded\u201d approach to determine style infringement. To do this, it trains two classifiers: one \u201cDeepMatch,\u201d a traditional image classifier trained to classify 372 artists based on a WikiArt training set, and a second, \u201cTagMatch,\u201d which classifies artist styles  using a human-interpretable tag-matching heuristic on top of more than 100 CLIP-derived tags across 10 aspects of artistic styles. Finally it conducts a measurement of images generated by popular diffusion models to quantify the number that resemble an artist\u2019s style according to DeepMatch, and generates some explanations using TagMatch."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The problem area is important, with image synthesis models creating potentially huge economic impacts for the artistic professions. There is a need for scientific analysis to help guide discussions about implications for copyright and copyright law. Quantifying the amount of style imitation in the large models is a worthy goal. And in developing its methods, the paper recognizes the importance of interpretable explanations when it comes to human arguments."
            },
            "weaknesses": {
                "value": "The paper is not ready for publication in ICLR.  There are several specific problems\n\n1. The legal discussion is not well-supported, and it is not sufficiently pedagogical for this audience.\n2. The suitability of the classifiers for the task is not sufficiently measured or justified or compared to previous technical work.\n3. The evaluation of style imitation in diffusion models does not support its conclusions.\n\nThe legal discussion is the most serious problem. For the technical audience at ICLR, a paper discussing legal issues must play a tutorial role. The audience at the conference is technical and not made of legal experts, so when making claims about legal issues, it is especially important for information about legal reasoning to be representative, explanatory, and correct. In contrast, this paper seems to be advancing an adventurous legal viewpoint in a venue with a legally unsophisticated audience.\n\nSpecifically, in the section on the legal framework, the paper puts in boldface: \u201c**the task of showing the existence and uniqueness of artistic styles can be reduced to classification** \u2013 something deep networks are particularly adept at doing.\u201d That claim appears to contradict the paper\u2019s own legal citations. For example, when contemplating legal tests for style infringement, the cited work by Sobel makes a distinction between \u201cextrinsic\u201d similarity that can be described in words as opposed to an \u201cintrinsic similarity\u201d which is perceived by humans but not easily described in a mechanical way. Sobel illustrates the distinction by surveying many subtle situations that have appeared in case law. In the perspective provided by Adobe\u2019s proposed style infringement legislation, the test is not pinned on the output, but rather, the intent of the AI user is placed at the center of style infringement. Both of these legal perspectives seem to be at odds with paper\u2019s proposed reduction of the style infringement test to an automated and mechanical artist-identification image classification problem. Neither of these central legal issues are surfaced to the ICLR reader: the paper omits any contemplation, measurement, or comparison to the intrinsic judgements that would need to be made by a jury, nor does it make any measurement, prediction, or discussion of intent by the user of the AI.\n\nThis reviewer strongly feels that ICLR should not be the place to advance a new legal theory. Plenty of scientific questions arise in the legal discussions, such as whether automated methods might be able to anticipate the judgement of a jury (and if not, why not), or whether the intent of the user can be correctly guessed by an automated method.  At ICLR it would be more appropriate for the paper to pose and investigate a scientific question, and should not lead with a novel legal theory in boldface.\n\n\n\nOn the suitability of the classifier. More careful comparisons to previous work are needed. Several previous works have focused on the style classification such as Karayev and van Noord cited in footnote 2. However, the current work does not attempt to compare its approaches to any previous approaches, and it does not build on top of any of the evaluation approaches. For example van Noord takes on artist identification using the \u201cRijksmuseum Challenge\u201d and analyzes and breaks down failed predictions. Do the proposed classifiers work better or worse than van Noord? Do they fail in similar ways? What is different in the technical approach that might lead us to expect the classifiers are more suitable?  Another insufficient comparison is between TagMatch and Concept Bottleneck Models. Table 1 in the appendix does a single pair of comparisons but does not quantify the sparsity advantage of TagMatch, or give any systematic comparison of meaningfulness to humans.  The heuristic in TagMatch seems ad-hoc: why would we expect its sparse set of labels to be more meaningful to a jury than the ones provided by CBM? No evaluation on that is done.\n\n\n\nOn the evaluation of existing style copying.  The paper\u2019s conclusions are not sufficiently supported. The paper\u2019s analysis of output from Stable Diffusion and OpenJourney concludes that most of the artist styles are not copied accurately, identifying just 16 of 372 artists whose styles are copied strongly.  However, no triangulation is done on this measurement, so it is unclear whether the low rate of identification is due to a weakness in the classifier, or whether it is due to a lack of style-imitation in the diffusion models.  A human evaluation on generated art style imitation could be done to check the estimate.  Or larger-scale data resources could be used, for example, the \u201cparrotzone.art\u201d project has identified several thousand styles that SD copies well, and these could potentially be used as an independent source of human assessment of style similarity."
            },
            "questions": {
                "value": "Is the goal of the paper to create an automated system to make quick judgements about style infringement without requiring humans to look?\n\nDoes your goal contradict Sobel's view that these judgments are not possible to articulate in clear categories, and that they are inherently the province of a human jury to make?\n\nTo what extent do you believe that your system matches the style-infringement judgements that a jury would make \"by hand\"?\n\nWhat kinds of failure cases does the system have? What patterns characterize these failures?\n\nIf another scientist wishes to improve upon your system, what measurement can they do, that would indicate that they have made an improved system?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}