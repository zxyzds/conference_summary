{
    "id": "o9ewXD1JuB",
    "title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering",
    "abstract": "In the medical domain, numerous scenarios necessitate the long-form generation ability of large language models (LLMs). Specifically, when addressing patients' questions, it is essential that the model's response conveys factual claims, highlighting the need for an automated method to evaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset reconstructed using long-form question-answering datasets related to the biomedical domain. We use MedLFQA to facilitate a cost-effective automatic evaluations of factuality. We also propose OLAPH, a simple and novel framework that utilizes cost-effective and multifaceted automatic evaluation to construct a synthetic preference set and answers questions in our preferred manner. Our framework leads us to train LLMs step-by-step to reduce hallucinations and include crucial medical claims. We highlight that, even on evaluation metrics not used during training, LLMs trained with our OLAPH framework demonstrate significant performance improvement in factuality. Our findings reveal that a 7B LLM trained with our OLAPH framework can provide long answers comparable to the medical experts' answers in terms of factuality. We believe that our work could shed light on gauging the long-text generation ability of LLMs in the medical domain. Our code and datasets are available.",
    "keywords": [
        "medical question answering",
        "automatic evaluation",
        "factuality",
        "hallucination"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=o9ewXD1JuB",
    "pdf_link": "https://openreview.net/pdf?id=o9ewXD1JuB",
    "comments": [
        {
            "summary": {
                "value": "This paper proposed a dataset MedLFQA for automatic evaluation of factuality long-form question-answering in the biomedical domain. The authors also proposed a training framework and claimed that this framework can optimize LLMs to reduce hallucination step-by-step. \nThe study topic in this paper is important as the hallucination problem is critical when applying LLMs in the health domain. The manual evaluation and the dataset would be beneficial to the community. \nMy main concerns are:\n1. The MedLFQA sets the answers in MUST HAVE and NICE TO HAVE and then calculates the hallucination and comprehensiveness metrics by comparing the generated text and the reference text. Although automatic hallucination detection and quantization are difficult, and it is worth exploring automatic evaluation methods, it is not persuasive that the current setting can effectively serve as the hallucination metric. The problems are: a) this setting only evaluates a subset of hallucination; the LLM generated incorrect facts that outside of the MH and NH will not be taken into account in the metrics; b) The calculation of contradicts and entails are based on a fine-tuned BioBERT model, which brings uncertainty of the evaluation, especially on the medical domain. The metrics are highly affected by the performance of the fine-tuned BioBERT model, so a sensitivity analysis needs to be tested. In total, it is not persuasive that the current hallucination and comprehensiveness metrics are sufficient. \n2. It is no surprise that the proposed OLAPH framework can improve the three metrics (Word composition, Semantic Similarity, and Factuality), as these three metrics are directly optimized during the training (similar to the comparison that fine-tuned model is better than non fine-tuned). \n3. Question: In Table 5, when setting the alpha_3 as 1.0, the performance seems better than other smaller alpha_3 scores. So if we enlarge the alpha_3 over 1 (2,3,5 or more), will the model's performance continue to improve? Furthermore, it would be interesting to see the performance of the single loss function (i.e., set two other alpha as zero) to confirm which loss function contributes the most."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The study topic in this paper is important as the hallucination problem is critical when applying LLMs in the health domain. The manual evaluation and the dataset would be beneficial to the community."
            },
            "weaknesses": {
                "value": "1. The MedLFQA sets the answers in MUST HAVE and NICE TO HAVE and then calculates the hallucination and comprehensiveness metrics by comparing the generated text and the reference text. Although automatic hallucination detection and quantization are difficult, and it is worth exploring automatic evaluation methods, it is not persuasive that the current setting can effectively serve as the hallucination metric. The problems are: a) this setting only evaluates a subset of hallucination; the LLM generated incorrect facts that outside of the MH and NH will not be taken into account in the metrics; b) The calculation of contradicts and entails are based on a fine-tuned BioBERT model, which brings uncertainty of the evaluation, especially on the medical domain. The metrics are highly affected by the performance of the fine-tuned BioBERT model, so a sensitivity analysis (with different models other than fine-tuned BioBERT) needs to be tested. In total, it is not persuasive that the current hallucination and comprehensiveness metrics are sufficient. These evaluation limitations need to be discussed. \n2. It is no surprise that the proposed OLAPH framework can improve the three metrics (Word composition, Semantic Similarity, and Factuality), as these three metrics are directly optimized during the training (similar to the comparison that fine-tuned model is better than non fine-tuned)."
            },
            "questions": {
                "value": "Question: In table 5, when set the alpha_3 as 1.0, the performance seems better that other smaller alpha_3 scores. Why not to enlarge the alpha_3 over 1 (2,3,5 or more) to test the model performance? Furthermore, it would be interesting to see the performance of the single loss function (i.e. set two other alpha as zero)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to improve factual accuracy of large language models (LLMs) in generating long-form answers within the biomedical domain through two main contributions.\nThe authors first introduce MedLFQA, a benchmark dataset designed to facilitate the automatic evaluation of factual claims in LLM responses.\nNext, the paper proposes OLAPH (Optimizing Large language models' Answers with Preferences of mitigating Hallucination) framework, which iteratively trains LLMs to reduce hallucinations and incorporate essential medical information by leveraging synthetic preference sets derived from cost-effective, multifaceted automatic evaluations.\nExperimental results demonstrate that a 7B parameter LLMs trained with OLAPH can generate responses with improved factuality comparable to those of medical experts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+  The paper addresses an important problem in the medical domain, where factuality is crucial for patient safety and trust in medical AI systems.\n+ The paper is clearly written with well-structured presentation, clear visualizations, and illustrative examples.\n+  The introduction of MedLFQA as a unified benchmark for evaluating factuality in biomedical LFQA is a valuable contribution to the field.\n+ The effectiveness of OLAPH is comprehensively validated with thorough analyses, comparisons with proprietary models, and evaluation using metrics independent of the training process,"
            },
            "weaknesses": {
                "value": "+ The novelty of the proposed OLAPH framework is limited, as it mostly follows the standard preference optimization process (SFT and DPO)"
            },
            "questions": {
                "value": "+ The paper should address the convergence properties of OLAPH: Is convergence guaranteed? How do the number and types of evaluation metrics influence convergence rates?\n+ Can the evaluation criteria for the qualification of generated data in section 3.2 be used as preference criteria for preference optimization process in section 4.2?\n+ Why does the paper follow cross-validation approach for experiments instead of the traditional train/test split approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "the paper introduces MedLFQA dataset to evaluate long-form answers in biomedical contexts. The authors generated this dataset by combining multiple existing datasets to enable comprehensive assessment. This fills a gap in the automated evaluation of LFQA in a domain where factual accuracy is critical. Also, the authors propose & evaluate OLAPH framework on word composition, semantic similarity, factuality- using benchmarks and metrics like FACTSCORE. This framework shows improvements (in factual accuracy) for 7B small parameter models. This study is a novel attempt to reduce hallucinations in medical responses."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper shows an innovative approach to enhancing the factual accuracy of long-form biomedical question answering. This work also creatively combines current techniques in preference-based learning and factual consistency checks to improve medical domain answers, addressing key limitations in factuality and response quality in prior research. This work is good in quality of methodological design. They have detailed each step in OLAPH\u2019s alignment process making it easy for others to build on top of it. The evaluation approach is also comprehensive as they used well known metrics. The paper is clearly written and answers major questions readers (at least I) may have. The significance of OLAPH lies in its potential to impact the application of LLM in healthcare. Medlfqa dataset can help other researchers in developing and evaluating factually accurate medical response systems."
            },
            "weaknesses": {
                "value": "The framework instroduced in this study is using GPT-4 to generate must-have and nice-to-have statements in medlfqa. My concern is that it may introduce biases or inaccuracies into the dataset. Although the researchers show that GPT-4-generated responses are close to human-curated answers, i did not find a critical analysis of where synthetic statements might diverge from medical experts.\n\nAlso, i think the study framework is a good step towards medical LLMs, but it does not cover the framework's behavior for a domain-specific case like disease diagnosis or recommendation of treatment."
            },
            "questions": {
                "value": "1. Were there specific medical domains or types of questions where synthetic responses diverged from expert standards?\nI think it would be helpful if you could discuss any plans to validate the quality of synthetic data using a diverse set of generative models to assess robustness.\n\n2. As we know, medical knowledge advances over time. So, I am curious to understand how you plan to deal with the challenges of maintaining factual relevance if your framework/dataset is deployed over a longer period?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This research paper introduces OLAPH, a framework designed to improve the factual accuracy of long-form question answering in the medical domain. The authors address the challenge of hallucinations in large language models (LLMs) by utilizing a cost-effective and multifaceted automatic evaluation system to generate synthetic preference sets. These sets guide the LLMs in prioritizing factuality, semantic similarity, and word composition. \n\nTo facilitate automatic evaluation, the authors introduce MedLFQA, a reconstructed benchmark dataset that includes long-form answers and crucial statements derived from existing biomedical question-answering datasets. Through experiments, the authors demonstrate that even 7B LLMs, trained with OLAPH, can generate answers comparable to GPT-4 and medical experts in terms of factuality."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. the data set collection follows good practice with human verification (high agreement).\n2. method is clearly explained.\n3. experiments are thorough (though I have questions and suggestions below).\n4. most part of the paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The method itself (OLAPH) is not really \"novel\" as the authors claim - the general frameworks has been used in many post-training of LLMs such as Llama and Claude models. I suggest softer contribution claims here.\n2. Some claims are not very rigorous - see my questions below.\n3. The final analysis can be made clearer.\n\nRecommended related work. \nRAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering."
            },
            "questions": {
                "value": "1. For the analysis of or RQ 3, what's exactly experimental setup? I am not getting why you will need to supply domain knowledge here, and since you train multiple models in RQ 1 and 2, which models did you end up using for this?\n2. In Figure 5, if using SFT leads to initial drop of performances, have you try to remove it? And what would the impact be?\n3. Line 235 - 236, this may simply suggest that answers in K-QA is terrible, not necessarily that GPT-4 answers are good. Do you have more analysis here?\n4. Line 248 - 252, I am not 100% convinced by this claim. Could it be that annotators are biased to believe whatever the model generates?\n5. Line 282 + 283, do you use statements as SFT targets too? If so, what's the data format?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}