{
    "id": "TMYe4rUuTc",
    "title": "Evolving Alignment via Asymmetric Self-Play",
    "abstract": "Current RLHF approaches for aligning large language models (LLMs) typically assume a fixed prompt distribution, which is sub-optimal and limits the generalization capabilities for language models. To address this issue, we introduce a general framework that casts alignment as an asymmetric game between two players:  (i) a creator, which strategically generates informative prompt distributions using reward signals, and (ii) a solver, which learns to produce preferred responses on prompts produced by the creator.\n\nThis framework of Evolving Alignment via Asymmetric Self-Play (`eva`), results in a simple and efficient approach that can utilize any existing RLHF algorithm. `eva` achieves a new state of the art in widely adopted alignment benchmarks, without the need of any additional human crafted prompts, e.g., it can improve the win rate of finetuned gemma-2-9b-it on Arena-Hard from 51.6% to 60.1% with DPO, from 55.7% to 58.9% with SPPO, from 52.3% to 60.7% with SimPO, and from 54.8% to 60.3% with ORPO, surpassing its 27B version and matching Claude-3-opus. Finally, we show `eva` is effective and robust under various ablation settings. \n\nWe hope `eva` can serve as a scalable methodology for the research community to build open-ended, robust, and self-improving language agents, that align with human values.",
    "keywords": [
        "large language model",
        "RLHF",
        "open-ended learning",
        "alignment"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We present a new framework for open-ended self-training large language models.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TMYe4rUuTc",
    "pdf_link": "https://openreview.net/pdf?id=TMYe4rUuTc",
    "comments": [
        {
            "summary": {
                "value": "This paper looks to bring RLHF training past using a fixed prompt distribution. The authors pose the learning problem as an asymmetric game between two players: a prompt creator and solver. The authors then introduce eva, which is a simple prompt evolution approach that can be used in conjunction with existing RLHF algorithms. eva improves the performance of DPO, SimPO, SPPO, and ORPO \non widely-used benchmarks within the community."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- It is very cool to see a curriculum learning approach leading to improvements in practical RLHF settings with LLMs. \n- I thought the writing quality and presentation of the paper was generally pretty strong throughout with the discourse feeling well structured.\n- I appreciated the way that the approach was motivated through discussion of minimax regret in section 3.2. \n- I also found the connection with learning progress highlighted in section 3.4 quite interesting and feel that my concerns about the heuristic nature of the approach are largely addressed through this motivation. \n- The results in Table 1 are very impressive, especially in comparison to human prompts. \n- It is nice to see that the success of eva is very robust to the particular RLHF optimizer used. \n- A nice set of benchmarks and ablations are considered for the experiments. \n- I also thought the robustness with respect to more training iterations was a nice benefit to highlight."
            },
            "weaknesses": {
                "value": "- The novelty of the approach is not huge based on the prior literature, although I do believe a very practical contribution is made. \n- The approach is motivated intuitively and includes heuristics i.e. the \"Informativeness Proxy\" rather than a mathematical derivation of the optimization rule from first principles. \n- Evolving seems important for achieving performance based off table 4, but the particular algorithm chosen is not really described or contrasted with other approaches. This is discussed in the \"Prompt synthesis for language models\" section of the related work as an orthogonal contribution because other related approaches could be plugged into evolve(\u00b7), but it would be very interesting to understand more about how critical this particular choice is for the success of eva. \n- There are some typos that the authors missed such as \"DIFFERENT INTUITIVE WYAS\" and \"Limitations and future directionss\"."
            },
            "questions": {
                "value": "Q1: Could you explain more about the particular choice of evolution algorithm used in your implementation of eva and different potential strengths and weaknesses related to this choice? \n\nQ2: Do you see empirical evidence of your intuition about learning progress discussed in section 3.4? It seems like some of these claims are directly testable. \n\nQ3: Could you visualize the curriculum learned in your experiments with eva? It would be very nice to get an intuition for why performance improves and what the heuristic prioritizes over time. \n\nMinor - Q4: When discussing future directions, the authors write  \"(vii) further scaling up w/ million-level data\". Can you clarify what this means? Seems like some important context is missing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Majority of the recent alignment/RLHF pipelines have been designed on static prompt distribution leading to several issues of distribution shift on different prompt distribution. Hence to mitigate this issue, the current work formulates the problem as an asymmetric game where the creator generates novel and informative prompts whereas the solver solves it. This approach improves the performance of existing baselines on SOTA benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The reliance of static prompt distribution has inherently limited the capabilities of various alignment designs eventually resulting in issues such as distribution shifts and even jailbreaks can be considered as a special case of the same. Hence, the problem statement is timely and crucial. The paper introduces a creator-solver framework evolving the prompt distribution which enhances the generalization ability of LLMs, allowing them to adapt to more diverse and challenging prompts, which better mirrors real-world applications. An additional advantage lies in the fact, the algorithm can be added on top of current baselines which is crucial rather than building new algorithm from scratch. The empirical results show improvements on existing benchmarks when applied on top of baseline algorithms."
            },
            "weaknesses": {
                "value": "1. How is Equation 10 tractable and how is it being solved? Any heuristic of sampling and approximating should result in sub-optimality which is not clear where its accounted.\n2. The optimization is over \\pi in equation 9 for solving the minimax regret. However, its not absolutely clear how the KL divergence plays a role here and how it is ensured that the response and prompt distributions are close to reference. Without that, the alignment problem is ill-defined. Please provide concrete justifications in theory and empirical results.\n3. As described in Algorithm 1, informativeness is evaluated and a prompt subset is created based on current policy estimate and then the policy is updated based on the prompt subset. However, this causes an inter-dependence between the two which leads to nested structure, which is not clearly explained. Specifically, while computing the informativeness score for the prompts, it depends on \\theta^*(x_t-1) i.e optimal parameter for the previous distribution. Provide clear explaination on the same. \n4. While iterating, every new prompt distribution will require generating new response, how is the evaluation coming from which reward model?  Is the ground reward available, if not please explain how the preference is obtained and how does it affect suboptimality?\n5. Overall, which expression/Theorem guides us in understanding the improvement of prior suboptimality is not clear? Can you please point out/highlight how the current method improves upon the prior suboptimality due to static prompt distribution?\n6. Its extremely crucial to show the prompt distribution and demonstrate its perplexity to ensure its not generating some meaningless or irrelevant prompts, since its not very evident on the KL divergence in the prompt space and its relation with the informative measure. Please provide detailed explaination to clarify that."
            },
            "questions": {
                "value": "1. Since equation 7, can't be directly solved, and is solved in an asymmetric fashion, then in the solver loop the KL should be over the response distribution and not joint right?\n2. How is the KL divergence w.r.t reference policy for the algorithm? Please provide detailed ablation.\n3. Whats the reward model availability? Is the true reward model available? \n4. There is a recent line of works on Stacklberg and Bilevel RLHF which deals with the entanglement in a leader-follower setting. Although not specific to updating prompt dist, but can be trivially applied. Provide a detailed comparison with the literature around that [1,2, 3].\n5. Can you provide intuitions behind equation 7, on the KL divergence between the joint policy for both prompt and response? Is it even tractable to estimate or approximate this KL \n\n\n[1]. Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF\n[2]. SAIL: Self-Improving Efficient Online Alignment of Large Language Models\n[3]. STA-RLHF: Stackelberg Aligned Reinforcement Learning with Human Feedback"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes EVA: Evolving Alignment via A symmetric Self-Play, a framework that that casts alignment as an asymmetric game between two players, a prompt creator and a policy learner. They selects prompts which induce responses with largest reward gaps, and generate similar prompts with those selected prompts. The proposed method improves the baseline by a margin."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper is well-written, easy-to-follow and informative; It contains thorough ablation study to show the effectiveness of the proposed method; The empirical evaluation shows much improvement."
            },
            "weaknesses": {
                "value": "**Main concerns**\n\n1. The number of iterations in the main results is 2, with only one EVA step in each experiment, which is a little different from what the demonstration in Figure 3 shows. If the EVA step is performed multiple times, would the results be better or worse? What is performance like when you access all data in ultrafeedback?\n\n2. The connection between the minimax regret objective and the algorithm is a somehow vague. The regret concerns the performance gap with the optimal policy. It's not reflected by the informativeness proxy.\n\n**Minor issues**\n\nLine 278: type wrost -> worst"
            },
            "questions": {
                "value": "The informativeness proxy seems to be similar to the variance of the rewards because they all concern the diversity of the generated responses. However, in lines 393-395, the results shows using variance leads to poor performance. How to interpret this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}