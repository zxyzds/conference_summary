{
    "id": "UT5B7fktaw",
    "title": "Variational Inference for Self-Supervised Speech Models Fine-tuning on Downstream Tasks",
    "abstract": "Despite the growing interest in self-supervised speech models, recent research has primarily focused on modifying upstream model architectures and pretraining techniques, with less attention given to how features from self-supervised models are used. In this paper, we explore the use of variational inference to enhance the performance of self-supervised audio models in downstream tasks. We hypothesize that adaptively reweighting the outputs of the model layers is crucial to improving performance on these tasks. We extensively evaluate our method alongside widely used baselines, demonstrating that understanding sample-specific information is essential for improved performance on several tasks. Our proposed method surpasses existing approaches and generalizes to various speech tasks, including automatic speech recognition, speaker verification, and emotion recognition. Finally, we analyze our method to provide deeper insight into the importance of our modifications.",
    "keywords": [
        "SSL models",
        "Fine-tuning",
        "Variational Inference",
        "SER",
        "ASR",
        "SV"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=UT5B7fktaw",
    "pdf_link": "https://openreview.net/pdf?id=UT5B7fktaw",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces VARAN, an innovative layer aggregation technique for finetuning self-supervised speech models motivated from variational inference field. The authors emphasize that adaptively adjusting the weights of outputs from various model layers based on individual samples is essential for boosting performance in downstream tasks. VARAN generates weights for each layer tailored to the specific sample, achieving superior results compared to existing layer-aggregation methods like weighted sum and layerwise attention pooling (e.g., SV and SER). Additionally, it demonstrates competitive performance on the ASR task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "VARAN adapts the weight of different layers used conditioned on each sample. It is shown to be very effective for the SER task."
            },
            "weaknesses": {
                "value": "The authors do not compare VARAN to adapters, which are (i) a more cost-effective finetuning method (involving fewer parameters) and (ii) better suited for adapting models to different downstream tasks, especially when there is limited data available for finetuning. This is demonstrated in the paper, \"CHAPTER: Exploiting Convolutional Neural Network Adapters for Self-Supervised Speech Models.\"\n\nFor tasks like ASR, SV, and SER, the performance gains with VARAN may not be as significant compared to adapters, as shown in the CHAPTER paper. Thus, a direct comparison becomes essential to substantiate general claims about VARAN.\n\nMoreover, the reliance on choosing an appropriate prior for each task underscores the importance of understanding what each layer encodes. Tables 4 and 5 demonstrate that selecting the right prior is crucial for achieving performance gains."
            },
            "questions": {
                "value": "In Figure 1, how multihead self attention is used is unclear to me. \n\nWhy using data2vec for SER task and not for all the tasks?\n\nTypo in line 34. with these classical >> with classical."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces VARAN, which demonstrates that different layers encode varying levels of information for different tasks. The authors argue that simple weighted summation is insufficient to capture sample variance. To address this challenge, VARAN automatically learns layer posterior distributions and performs tasks (ASR, SR, and ER) on each layer separately. The aggregated results show improved performance compared to using only the last layer."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "VARAN adopts variational inference to semi-automatically extract layer-wise information, providing novel insights.\n\n\nThe paper is written in a clear and concise manner, making it easy to follow both the ideas and experiments.\n\n\nThe experimental results support the authors' claims, demonstrating that VARAN outperforms last-layer counterparts in several tasks (ASR, PR, and ER)"
            },
            "weaknesses": {
                "value": "While the experimental results show interesting findings on common ASR, speaker recognition, and emotion benchmarks, these results are not entirely convincing as they rely on relatively simple benchmarks. For ASR evaluation, more challenging robust benchmarks (as used in Whisper or detailed in https://arxiv.org/abs/2401.10446) would be more appropriate. Overall, the experimental validation appears insufficient.\n\nThis limitation is reflected in Table 1, where VARAN fails to outperform weighted combination methods - a drawback explicitly mentioned in lines 53-54. This suggests that VARAN may still 'not account for sample variation and its possible dependence on information from SSL layers.'\n\nThe reliance on human knowledge for prior distribution is problematic. Specifically, finding appropriate prior distributions for each task is impractical in real-world applications. Even within a single task like ASR, it's unclear whether a single prior distribution can effectively handle variations across languages, dialects, and noise conditions. This raises questions about the method's practical utility, given that humans already inherently adjust such 'priors' for different tasks.\n\nSeveral technical details require clarification:\n\nHow is discrete variational inference (both prior and posterior) modeled? Is Gumbel-Softmax used? The paper mentions softmax application but lacks implementation details.\n\nThe architecture appears to differ between speaker-dependent and speaker-independent tasks - this needs elaboration.\nState-of-the-art ASR systems typically incorporate language model integration. Why was this omitted?\n\nIn conclusion, while the core idea is interesting, the experimental validation is insufficient to fully support the proposed approach."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This study proposes a novel layer aggregation approach using variational inference. It demonstrates that this method outperforms traditional aggregation strategies across various downstream tasks. By allowing the adjustment of layer importance through a prior distribution, this paper shows that models can be better adapted to task-specific situations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper introduces a new layer aggregation method that differs from Multi-head Factorized Attentive Pooling layer aggregation by adjusting the prior distribution to control layer output weight.\n2. The experiments and analyses demonstrate that the actual posterior distribution follows the prior distribution, and shows good performance on various downstream tasks."
            },
            "weaknesses": {
                "value": "1. The proposed method (VARAN) involves setting a prior distribution for knowledge distillation (KD) learning, which presents the limitation that one must already know the optimal layer weight distribution for each downstream task.\n2. In the ASR experiments, the evaluation was conducted using LibriSpeech data, the domain employed in the pre-training. This method does not align with the goal of assessing generalizability, suggesting a need to test with other speech datasets.\n3. This paper conducted experiments on automatic speech recognition, speaker verification, and emotion recognition. Additional datasets are necessary to demonstrate more generalized performance compared to existing methods."
            },
            "questions": {
                "value": "1. How was the prior distribution chosen, and what were the criteria for selecting the non-central Chi-squared distribution?\n2. Why were the WavLM and Data2vec chosen among various self-supervised models, and why did you not experiment with other models?\n3. In Figure 2, experiments were conducted to set various beta values for the non-central Chi-squared distribution. Is the optimal beta value found simply by grid search, or if another search method was used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an layer aggregation approach inspired by variational inference. It adds a classifier after each layer along with a final attention layer, and then minimize the variational lower-bound loss to find the best importance distribution for the outputs from each layer. To validate the approach, the authors conducted a series of experiments on SUPERB benchmark, including ASR, speaker verification, and emotion recognition."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The idea of using variational to generate the impact distribution from all the layer outputs seem to be new and interesting.\n* The authors conducted extensive experiments on 3 speech related tasks, along with a set of ablation studies with insightful analysis."
            },
            "weaknesses": {
                "value": "* For ASR evaluations, the WER improvement is too limited. Usually the 2nd digit after the decimal point differences are mostly noise. In this case, it would be helpful to run experiments on other testsets (e.g., common voice, TED-LIUM) to verify if we are actually seeing performance gains.  \n* The proposed approach requires running additional multi-headed attention during serving time. How many more parameters are in this layer? What\u2019s the latency impact?\n* The presentation needs improvement. Please see my questions below. In addition, the title and abstract sound a bit confusing. The proposed approach is on layer aggregation, but neither the title or the abstract mentioned this."
            },
            "questions": {
                "value": "General question: Is the proposed layer aggregation approach applicable to SSL model? Feels like they can work for models trained from scratch?\n\nSection 2.2: How does it enforce V to learn speaker-discriminative information only, while K has phonetic information?\n\nSection 2.2: The authors mentioned that MHFA requires additional requires searching additional hyperparameters. However, in section 5.2, the regularization weight also needs to be tuned. This doesn\u2019t seem to be a fair comparison to prior studies.\n\nSection 3.1: \u201cChoosing the right layer or combination of layers is challenging, as the training data does not have the optimal layer distribution.\u201d Please explain more on this. Does it mean the empirically learned weights from training data (is it from pretraining or finetuning?) are not exactly the same as their real distribution?\n\nSection 3: Please explain how exactly the proposed method works during inference time. Is it using a weighted average of pooled hidden as the input to the final softmax layer?\n\nSection 4:Please explain the baselines. How are the \u201cweighted\u201d exactly computed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}