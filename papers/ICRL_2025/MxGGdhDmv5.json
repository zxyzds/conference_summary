{
    "id": "MxGGdhDmv5",
    "title": "Higher Order Transformers: Efficient Attention Mechanism for Tensor Structured Data",
    "abstract": "Transformers are now ubiquitous for sequence modeling tasks, but their extension to multi-dimensional data remains a challenge due to the quadratic cost of the attention mechanism.  In this paper, we propose Higher-Order Transformers (HOT), a novel architecture designed to efficiently process data with more than two axes, i.e. higher-order tensors. \nTo address the computational challenges associated with high-order tensor attention, we introduce a novel Kronecker factorized attention mechanism that reduces the attention cost to quadratic in each axis' dimension, rather than quadratic in the total size of the input tensor. To further enhance efficiency, HOT leverages kernelized attention, reducing the complexity to linear. This strategy maintains the model's expressiveness while enabling scalable attention computation.\nWe validate the effectiveness of HOT on two high-dimensional tasks, including multivariate time series forecasting, and 3D medical image classification. Experimental results demonstrate that HOT achieves competitive performance while significantly improving computational efficiency, showcasing its potential for tackling a wide range of complex, multi-dimensional data.",
    "keywords": [
        "transformers",
        "multihead attention",
        "high order tensor",
        "kronecker decomposition",
        "multivariate timeseries forecasting",
        "3D medical image classification"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We introduce HOT, a transformer architecture that efficiently applies attention on multi-dimensional tensor data using tensor factorization.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=MxGGdhDmv5",
    "pdf_link": "https://openreview.net/pdf?id=MxGGdhDmv5",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a new transformer-based model, i.e., Higher-Order Transformers (HOT) which can efficiently process data with more than two axes (i.e., higher order tensors). In addition, the authors propose a novel Kronecker factorized attention mechanism that reduces the attention cost and provide theoretical proof of Kronecker decomposition. The experiments over multivariate time series data and 3D medical images are straightforward and clear."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The paper is clear and well written. It is easy to follow and presents a concise idea of Kronecker factorized attention mechanism based on Kronecker decomposition. \n+ The paper also do a good job in describing existing research/roadmap in transformer-based models.\n+ For multivariate forecasting tasks, the proposed HOT model always outperforms baselines."
            },
            "weaknesses": {
                "value": "- For multivariate forecasting tasks, the model's effectiveness is assessed only through MAE and MSE. Can the authors also report other metrics, e.g., MAPE? In addition, I am curious about the confidence intervals for prediction of the HOT model compared to other baselines.\n- Why Kronecker decomposition: there are also other higher-order structure decomposition approach, e.g., CP decomposition, Tucker decomposition, etc. I wonder if the authors have compared Kronecker decomposition with other tensor decomposition methods? \n- Improvements are minor: although the HOT model achieves the best prediction accuracy across all datasets for multivariate forecasting tasks, improvements are not significant. Also, for 3D medical images, the HOT model's performances are less competitive. \n- It would be helpful if the authors could conduct robust experimental comparisons, i.e., if the proposed Kronecker factorized attention mechanism can help improve model's robustness."
            },
            "questions": {
                "value": "See my questions from the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel Transformer architecture to extend traditional Transformers for processing higher-order data, while reducing the attention cost to quadratic in each axis' dimension and the complexity to linear."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Considering the importance of higher-order data, designing specialized Transformer architectures for higher-order data is meaningful.\n\n* The proposed Higher-Order Transformers (HOT) can efficiently capture essential cross-dimensional dependencies with lower complexity, thus overcoming the limitations of previous methods.\n\n* The experimental results also support the superiority of the proposed HOT, both in effectiveness and efficiency."
            },
            "weaknesses": {
                "value": "* Different form previous works which directly flatten the input tensor into a sequence, it's impossible for HOT to use the same architecture process data with different orders, thus limiting its scalability.\n\n* Consideing the high-order feature of HOT, it's necessary to compare the model parameters with its baselines.\n\n* Besides, it's also necessary to conduct more experiments to demonstrate the generality of the proposed method on various kinds of higher-order data, such as typical 2D image datasets and video dataset."
            },
            "questions": {
                "value": "Please see the **weaknesses**."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposed a transformer architecture to efficiently deal with higher-order structure data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The research question is important since the attention operator is very computationally expensive on higher-order data.\n2. The usage of Kronecker to factorize attention is sound.\n3. Empirical studies demonstrate the effectiveness of proposed work on higher-order data."
            },
            "weaknesses": {
                "value": "1. The major problem is the novelty. This work is essentially about attention instead of transformer. The Kronecker factorization is applied to attention, which is part of the transformer architecture. Actually, applying Kronecker factorization to attention on higher-order data has been explored in [1]. Currently, the novelty is very marginal from my current understanding. It is important for authors to demonstrate the distinct contributions from [1]. \n\n2. Since the proposed attention is not necessarily integrated with the transformer architecture, it would be better to show its effectiveness in other architectures, such as Attention U-Net [2].\n\n\n[1] Gao, Hongyang, Zhengyang Wang, and Shuiwang Ji. \"Kronecker attention networks.\" In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 229-237. 2020.\n[2] Oktay, Ozan, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori et al. \"Attention u-net: Learning where to look for the pancreas.\" arXiv preprint arXiv:1804.03999 (2018)."
            },
            "questions": {
                "value": "Please see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a high-order Transformer (HOT) for high-order tensor input. The authors apply Kronecker tensor decomposition and kernel attention in Performer to propose a novel Kronecker factorized attention. The experimental results in time-series forecasting and 3D medical image classification illustrate the efficient of HOT."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The authors skillfully perform tensor operations."
            },
            "weaknesses": {
                "value": "1. This paper demonstrates a limited degree of innovation. The proposed methodology in this paper is merely based on Kronecker tensor decomposition [1] and kernel attention [2].\n\n[1] Phan, K. (2012). On Revealing Replicating Structures in Multiway Data: A Novel Tensor Decomposition Approach. In Latent Variable Analysis and Signal Separation (pp. 297\u2013305). Springer Berlin Heidelberg.\n\n[2] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, \\& Adrian Weller (2021). Rethinking Attention with Performers. In International Conference on Learning Representations."
            },
            "questions": {
                "value": "1. How does the use of low-rank approximation via Kronecker decomposition in Section 4.2 impact the trade-off between computational complexity and precision? The authors need to provide more analysis on this and offer guidelines for choosing the appropriate rank R or kernels to help practitioners adapt the method to different datasets and needs.\n\n2. The authors need to provide more insights or proof on how HOT maintains low complexity when handling datasets with more than 3 dimensions. It would be helpful to understand how the model scales and whether the current complexity reduction techniques, like Kronecker decomposition remain effective as the dimensionality increases."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}