{
    "id": "WDxa9hnz4p",
    "title": "Auto-Demo Prompting: Leveraging Generated Outputs as Demonstrations for Enhanced Batch Prompting",
    "abstract": "Batch prompting is a common technique in large language models (LLMs) used to process multiple inputs simultaneously, aiming to improve computational efficiency. However, as batch sizes increase, performance degradation often occurs due to the model's difficulty in handling lengthy context inputs. Existing methods that attempt to mitigate these issues rely solely on batch data arrangement and majority voting rather than improving the design of the batch prompt itself. In this paper, we address these limitations by proposing \"Auto-Demo Prompting,\" a novel approach that leverages the question-output pairs from earlier questions within a batch as demonstrations for subsequent answer inference. We provide a formal theoretical analysis of how Auto-Demo Prompting functions within the autoregressive generation process of LLMs, illustrating how it utilizes prior outputs to optimize the model's internal representations. Our method effectively bridges the gap between batch prompting and few-shot prompting, enhancing performance with only a slight compromise in token usage. Experimental results across five NLP tasks demonstrate its effectiveness in mitigating performance degradation and occasionally outperforming single prompts. Furthermore, it opens new avenues for applying few-shot learning techniques, such as demonstration selection, within batch prompting, making it a robust solution for real-world applications.",
    "keywords": [
        "Batch Prompting",
        "In-Context Learning",
        "Large Language Models"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We present \"Auto-Demo Prompting\" which improves batch prompting performance by using previously generated \"question+answer\" pairs as demonstrations in the autoregressive generation process of LLMs.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-13",
    "forum_link": "https://openreview.net/forum?id=WDxa9hnz4p",
    "pdf_link": "https://openreview.net/pdf?id=WDxa9hnz4p",
    "comments": [
        {
            "revert_withdrawal_confirmation": {
                "value": "We approve the reversion of withdrawn submission."
            }
        },
        {
            "withdrawal_confirmation": {
                "value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."
            }
        },
        {
            "summary": {
                "value": "In batch prompting, many inputs are formatted consecutively in the prompt, and the model then produces responses for each input in single generation. This can improve computational efficiency by reducing the number of tokens processed (for instance, only needing one instruction for many samples). However, batch prompting tends to degrade prompting performance, while few-shot prompting---which uses the same in-context examples, but formatted differently---can improve it. Motivated by this observation, the paper proposes Auto-Demo prompting, which follows the format of batch prompting but instructs the model to output both the question and the answer, given the question. The intuition is that the model can then treat the consecutive (question, answer) pair as a traditional few-shot demonstration, while still attaining the parallelism of batch prompting. The paper also considers how to better select demonstrations, and finds that the method can outperform standard batch prompting techniques."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Quality: auto-demo prompt outperforms standard batch prompting on 7/10 tasks across batch sizes. \n\nSignificance: improving inference-time efficiency in terms of prompting techniques is an important aspect of using LLMs on large amounts of data."
            },
            "weaknesses": {
                "value": "Quality:\n- Limited evaluation: I could not find any baseline against few-shot prompting, which auto-demo prompting aims to \"combine\" with batch prompting. Moreover, there is no evaluation of efficiency/cost, even though I believe auto-demo prompting generates more tokens and thus is more expensive. Given this tradeoff and the lack of evaluation against few-shot prompting, it is unclear if we can conclude that auto-demo prompting provides a cost-performance tradeoff improvement over standard methods. Overall, the performance improvements appear to be minor, while incurring extra cost. \n- Missing ablations: what is the role of the embedding space, and do other metrics for data selection also work?\n\nOriginality: my understanding is that models can process in-context demonstrations better when input output pairs are consecutive. Is there a deeper understanding of why few-shot prompting does better than batch prompting, despite having the same content? Is it simply because training data tends to have sequential (q, a) pairs? If so, the intuition of this paper is to align the prompt more with the data the LLM is trained on. This approach has limited novelty given that it directly combines two existing ideas, as well as standard demonstration selection techniques.\n\nClarity: Method 1 doesn't seem to generate $a_i$ conditioned on $q_i$. Also, Method 2 appears to generate $a_i$ preceded by $q_1 q_2$, when my understanding is that batch prompting has any $a_i$ preceded by $q_1 q_2 \\dots q_n$."
            },
            "questions": {
                "value": "1. Can you discuss or show results on few-shot prompting \n2. Can you discuss or show results on efficiency of the method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method called auto-demo prompting, aiming to improve performance when batched processing is needed in in-context learning. The authors propose to ask the model to repeat each question in the batch following by answering each question, showing that this method can outperform previous batching methods that simply ask the model to provide the answers."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed method is interesting, showing that repeating the question in the batch can better help the model answer each question more accurately.\n\n- The experiments are relatively comprehensive over 5 commonly used NLP datasets."
            },
            "weaknesses": {
                "value": "- The main idea is rather straightforward, it can be regarded as a simple optimization over the initial prompt used for batching requests. There could be many other possible optimizations to the initial prompt for improved performance (e.g., insert label ids after each input and ask the model to predict the label ids), it would be more interesting if this paper can provide a more comprehensive study rather than proposing a single optimization technique.\n\n- The current analysis in the paper is not sufficient to support the claim that the performance improvement is due to \"earlier outputs act as demonstrations of later questions\". It could be simply that after repeating each question, the model pays better attention to the question. Can the authors provide more analysis on this to support their main claim?\n\n- In experiments, can the authors add standard deviation by running over multiple permutations of the batched examples? It's unclear if the current results are statistically significant since the numbers are very close.\n\n- For the main results in Figure 3, why the batch sizes (x-axis) are inconsistent? RTE has 1, 16, 32, 48, BoolQ has 1, 8, 16, while other tasks have 1, 16, 32. Can the authors run more comprehensive experiments and show results with batch size 1, 8, 16, 32, 48?"
            },
            "questions": {
                "value": "- Can the authors add standard deviations to Figure 3?\n\n- Can the authors make the x-axis / batch-size range consistent across experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores batch prompting, where the prompt groups similar questions to be able to optimize execution efficiency. The work examines how to shape the input context of the prompt to achieve improved quality in the batch prompting scenario."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper evaluates on multiple datasets\nThe idea that we can exploit the similarity across questions to reap efficiency improvements is interesting"
            },
            "weaknesses": {
                "value": "The writing is not very clear; there are several unnecessary details. For instance, in the intro:\n- The definition of batch prompting is not clear in the intro Lines 43-48; Why does it help efficiency? The writing assumes that the reader is familiar with this.\n- How does one decide which questions to batch? \n- It is not clear where the model-generated answers as few-shot prompts come into play in Lines 100-102. - - The concept is not introduced in the intro before this. \n- What does \u201cproper design\u201d mean on Line 105?\n- It is not clear why having 0 to N-1 additional demos would help with long context (Lines 114-115)\n- The contributions (Lines 123-141) say nothing about efficiency, despite this being an initial motivation for batch prompting\n- It is also not clear why the reader needs any of the details in lines 213-219\n\nHow does this compare to methods like Hydragen (https://arxiv.org/abs/2402.05099), which do not increase the context length, but rather share the KV-cache on the shared prefix? \n\nThe Batch Data Selection with Retrieval approach is not novel \u2013 many approaches select similar examples via these embedding similarity scores\n\nResults:\n- The improvements from this method are unclear (0.2%, Lines 370-371)\n- There is no comparison to few-shot learning with retrieval-based few-shot selection\n- A motivation of batch prompting was efficiency \u2013 there is no evaluation of the efficiency improvements"
            },
            "questions": {
                "value": "This work is not up to par for the conference and it is unlikely that author responses will change my opinion"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}