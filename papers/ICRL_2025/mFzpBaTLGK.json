{
    "id": "mFzpBaTLGK",
    "title": "Sounding the Alarm: Backdooring Acoustic Foundation Models for Physically Realizable Triggers",
    "abstract": "Although foundation models help increase performance on many downstream tasks while reducing the amount of labeled data needed, \ntheir proliferation has raised a natural question: To what extent can a model downloaded from the Internet be trusted?  We tackle this question for acoustic foundation models (AFMs) and propose the $\\textbf F$oundation $\\textbf A$coustic model $\\textbf B$ackdoor (FAB) attack against AFMs, showing that state-of-the-art models are susceptible to a new attack vector. Despite preserving model performance on benign data, AFM induces backdoors that survive fine-tuning, and, when activated, lead to a significant performance drop on various downstream tasks.  Notably, backdoors created by FAB can be activated in a ${physically\\ realizable}$ manner by ${inconspicuous}$, ${input}$-${agnostic}$ triggers that ${do\\ not\\ require\\ syncing}$ with the acoustic input (e.g., by playing a siren sound in the background). Crucially, FAB also assumes a weaker threat model than past work, where the adversary has no knowledge of the pre-training data and certain architectural details.  We tested FAB with two leading AFMs, on nine tasks, with four triggers, against two defenses, as well as in the digital and physical domains, and found the attack highly successful in all scenarios.  Overall, our work highlights the risks facing AFMs and calls for advanced defences to mitigate them.",
    "keywords": [
        "security",
        "speech model",
        "backdoor attack"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mFzpBaTLGK",
    "pdf_link": "https://openreview.net/pdf?id=mFzpBaTLGK",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces the Foundation Acoustic model Backdoor (FAB) attack, a method for inserting backdoors in acoustic foundation models (AFMs) using physically realizable, input-agnostic audio triggers, such as sirens or barks. FAB attacks maintain model performance on benign data but degrade it significantly when activated, impacting various downstream tasks. Unlike prior work, FAB assumes a weak threat model, with attackers having limited knowledge of training data and model parameters. Experiments across multiple AFMs, tasks, triggers, and defenses show FAB's effectiveness in both digital and physical settings, even against standard defenses like fine-pruning and input filtering. This study highlights security risks in AFMs and the need for stronger defenses against backdoor attacks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Task-Agnostic and Physically Realizable Backdoor Attack: The proposed FAB is task-agnostic and physically realizable. Unlike prior work, FAB uses simple, inconspicuous sounds like sirens or dog barks as triggers without needing synchronization, making it adaptable to real-world settings.\n\n- Comprehensive Evaluation: Extensive experiments demonstrate FAB's effectiveness across various tasks and AFMs. The paper also tests FAB against defenses like fine-pruning and input filtration, showing its resilience. This thorough evaluation highlights the attack\u2019s robustness and underscores the need for stronger defenses in model security."
            },
            "weaknesses": {
                "value": "- Limited Novelty: While the FAB attack presents an interesting application, fine-tuning-based backdoor injection is not new, and FAB still requires auxiliary data with a distribution similar to the target AFM. This reliance on auxiliary data limits its originality and does not fully address existing challenges in realistic backdoor attacks.\n\n- Unrealistic Threat Model: The threat model lacks practical relevance, as it assumes the model provider itself would inject backdoors that degrade performance significantly. Major AFM providers, like OpenAI, are unlikely to introduce such vulnerabilities into their own models, limiting the real-world applicability of this attack.\n\n- Limited Evaluation Scope: The study focuses primarily on AFMs like HuBERT and WavLM, without assessing generalizability to a broader range of models. Testing additional AFM architectures, such as wav2vec 2.0 from Google and Data2Vec from Meta.\n\n- Lack of Audio Quality Evaluation: The study does not assess the audio quality of samples embedded with backdoor triggers compared to the original, unmodified samples. Metrics like ViSQOL could provide insights into whether the trigger significantly affects perceived audio quality. This evaluation is essential to ensure that the trigger remains inconspicuous to users, as noticeable quality degradation could alert users to the presence of an anomaly, undermining the stealth of the attack."
            },
            "questions": {
                "value": "- Could you elaborate on the need for auxiliary data with a similar distribution to the target AFM? Are there any approaches under consideration to remove this requirement? Clarifying this could help address concerns regarding FAB's novelty and broader applicability.\n\n- Given that major AFM providers like OpenAI are unlikely to risk their model\u2019s reputation by embedding backdoors, who do you envision as the realistic adversary for FAB? Additional clarification on feasible attacker scenarios could strengthen the practical relevance of the proposed method.\n\n- The study\u2019s evaluation primarily focuses on HuBERT and WavLM. Have you considered expanding your experiments to include other prominent AFMs, such as wav2vec 2.0 or Data2Vec? \n\n- To better understand the stealth of the embedded trigger, could you provide an audio quality evaluation using metrics like ViSQOL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Foundation models enhance performance across various downstream tasks and minimize the need for labeled data. This paper's authors revealed that acoustic foundation models are vulnerable to backdoor attacks, and extensive experiments confirmed the effectiveness of these attacks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Exploring the security issues in acoustic foundation models is both an important and intriguing topic."
            },
            "weaknesses": {
                "value": "1) The paper is poorly written.\n2) The threat model lacks clarity.\n3) Baseline attacks are not included."
            },
            "questions": {
                "value": "Thank you to the authors for this interesting paper. I have a few comments regarding the current work:\n\n1) Overall, the paper is not well-written. In Section 4, instead of just stating that FAB minimizes a compound loss function, the authors should provide the mathematical details of  L_back and L_benign. Without this, it becomes difficult for readers to follow the explanation.\n2) Typically, foundation models and downstream tasks use different encoders. However, the threat model does not clearly specify whether the attacker is aware of the encoders used in the downstream tasks.\n3) In recent years, numerous papers have targeted foundation models to mislead downstream tasks, such as [A] and [B]. The main distinction in this paper seems to be its focus on the security of acoustic foundation models. Aside from this, the technical challenges appear similar. The authors should clarify in the related work section the key differences between this research and existing literature. Why not simply apply previous attack techniques to acoustic foundation models? What are the unique challenges in attacking acoustic foundation models?\n4) The current paper does not seem to compare the proposed attacks with existing ones. It should not be difficult to apply the attacks described in [A] and [B] to the scenario outlined in this paper.\n5) A potential defense against the proposed attack could be for the downstream task to use a conditional diffusion model to denoise the audio.\n\n\n[A] Badencoder: Backdoor attacks to pre-trained encoders in self-supervised learning.\n\n[B] Adversarial Illusions in Multi-Modal Embeddings."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a backdoor attack against the acoustic foundation model in self-supervised learning, which can degrade the performance of corresponding downstream tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Language is mostly accessible, though with some minor issues\n2. Experimental results verify the effectiveness of the proposed scheme."
            },
            "weaknesses": {
                "value": "The research motivation is not practical enough"
            },
            "questions": {
                "value": "1. key concern: The author chose prominent sounds such as sirens, oboes, flutes, and bark as backdoor triggers, aiming to degrade the performance of associated downstream tasks rather than inducing a specific target behaviour. Theoretically, readers might question why attackers do not directly utilize noise-like triggers to impair the model performance if the attacker was required to activate the trigger actively. Besides, considering stealthiness, selecting white noise or ultrasound could obviously be seen as more covert than distinctive sounds like sirens. Therefore, It seems that the article's motivation is not clearly explained.\n2. The methods section (Section 4) writing is overly redundant and tedious, making an otherwise straightforward approach appear complicated. It is recommended that the author revise the methods section to present the information more logically and streamlined."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors assume that the backdoor attacker performs a type of \u2018man-in-the-middle\u2019 attack between the (benign) model provider and the victim user. Specifically, the attacker injects a backdoor into a pretrained Acoustic Foundation Model (AFM) obtained from the provider, and then releases it to the victim, who further fine-tunes it for a downstream task. According to their claim, the proposed backdoor is physically realizable, inconspicuous, input-agnostic, and sync-free (not requiring synchronization between the trigger sound and the sonic input)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "# Advantage\nThe main contribution of this work is the method for injecting the backdoor in a realistic scenario where the attacker has limited capability: the attacker can only access the pretrained model, cannot access the provider's training dataset, and has no knowledge of the downstream task. To inject a backdoor, the attacker reduces the representation distance between poisoned samples (benign sample + trigger) and a \u2018non-useful\u2019 representation from intermediate layers, without affecting the model\u2019s performance on benign tasks. During the downstream task, once the trigger is present in the input, the input is mapped to the \u2018non-useful\u2019 representation, leading to misclassification.\n\nThe main challenge is maintaining the model\u2019s performance on benign tasks. According to the authors, the best approach is to retrain the pretrained model in the same way the provider did (but with an additional backdoor loss). However, since the attacker does not know the exact codewords and other parameters, the authors propose approximating the missing parameters and using pseudo-labeling."
            },
            "weaknesses": {
                "value": "# Weakness\n\nSeveral weaknesses can be identified:\n\n1.\tThe authors assume that the user will use two fine-tuning paradigms: fine-tuning only the last layer or applying a weighted sum of all layers\u2019 representations. However, to attach the trigger and the \u2018non-useful\u2019 representation, it seems necessary to freeze the feature extractor of the AFM. It is important to verify whether the backdoor remains robust after fine-tuning the entire AFM. Based on my experience, it is hard to ensure backdoor mapping without freezing.\n\n2.\tThe authors also assume that the attacker knows the same training procedure as the provider. How realistic is it for the model to publicly release the training algorithm?\n\n3.\tThe two defenses are outdated and specifically don\u2019t focus on backdoor detection in SSL. The authors should also consider more recent methods, such as:1.\tZheng, Mengxin, et al. \"SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning.\" ECCV 2024;2.\tFeng, Shiwei, et al. \"Detecting Backdoors in Pre-Trained Encoders.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}