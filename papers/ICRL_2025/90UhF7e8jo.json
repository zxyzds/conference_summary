{
    "id": "90UhF7e8jo",
    "title": "Goal Achievement Guided Exploration: Mitigating Premature Convergence in Reinforcement Learning",
    "abstract": "Premature convergence to suboptimal policies remains a significant challenge in reinforcement learning (RL), particularly in tasks with sparse rewards or non-convex reward landscapes. Existing work usually utilizes reward shaping, such as curiosity-based internal rewards, to encourage exploring promising spaces. However, this may inadvertently introduce new local optima and impair the optimization for the actual target reward. To address this issue, we propose Goal Achievement Guided Exploration (GAGE), a novel approach that incorporates an agent's goal achievement as a dynamic criterion for balancing exploration and exploitation. GAGE adaptively adjusts the exploitation level based on the agent's current performance relative to an estimated optimal performance, thereby mitigating premature convergence. Extensive evaluations demonstrate that GAGE substantially improves learning outcomes across various challenging tasks by adapting convergence based on task success. Applicable to both continuous and discrete tasks, GAGE seamlessly integrates into existing RL frameworks, highlighting its potential as a versatile tool for enhancing exploration strategies in RL.",
    "keywords": [
        "reinforcement learning",
        "exploration",
        "deep reinforcement learning"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We propose a novel approach to balance exploration and exploitation in reinforcement learning by incorporating an agent's goal achievement as a dynamic criterion.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=90UhF7e8jo",
    "pdf_link": "https://openreview.net/pdf?id=90UhF7e8jo",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes an approach called Goal Achievement Guided Exploration (GAGE) to address premature convergence in reinforcement learning algorithms. Instead of using intrinsic rewards for exploration, the proposed approach maintains an estimate for the optimal performance level, comparing this level to the current performance for controlling between exploration and exploitation.\n\nThe main claim of the paper is that the proposed approach enhances exploration in reinforcement learning."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed approach aims to balance between exploration and exploitation in reinforcement learning. The approach is interesting in that it assumes that each reward term is equally important and exploration magnitude is kept as high as how far each reward term is from an assumed optimal solution.\n\nIn more detail, the approach uses a goal achievement term that is the minimum of goal achievement terms of each reward part. Each of these individual goal achievement terms is computed using Monte Carlo estimates of recent samples divided by a heuristic estimate of the optimal value, or, total maximum reward. An implicit assumption is that an agent should be able to succeed in all parts of the reward function sum.\n\nDiscussion of the \"Game Console\" problem in exploration is valuable."
            },
            "weaknesses": {
                "value": "The approach is based on assuming explicit knowledge of the reward function and  the individual parts (terms) that as a sum define the reward function. This needs to be discussed and motivated in detail. Most of the exploration approaches in reinforcement learning do not need explicit knowledge of the reward function.\n\nThe approach makes strong assumptions about the task. I assume the approach only works if these assumptions are satisfied and can easily lead to slow convergence. The approach controls exploration according to the reward term that is furthest away from being satisfied. This means, for example, that if there is a single reward term that is very hard to get close to optimal, large amounts of exploration is used although the total reward would be already high. Moreover, the approach can lead to excessive exploration noise that may hinder improving reward terms which require small amount of noise.\n\nEvidence for the main claim of the paper that the proposed approach enhances exploration is needed. That the algorithmic design and computations used in the approach improve on state-of-the-art need significantly stronger theoretical or/and empirical evidence.\n\nFig. 2 and the main text aim to motivate the proposed approach by saying that exploration methods typically somehow change the order of probabilities. This is not true. For example, target entropy [Haarnoja et al., 2018] is commonly used and does not change the order of the action probabilities.\n\nThe action smoothing procedure in Section 3.2 for discrete actions includes several computations for which there is some discussion of the motivation but no theoretical or empirical evidence. There should be a much more convincing discussion on why each of the steps 1. to 4. in Section 3.2 is used to compute the adaptive temperature of the softmax distribution.\n\n\nExperiments:\n\nMethods:\nOne of the main motivations for the proposed approach in the paper is that intrinsic motivation based approaches may converge to local optima. However, there are methods designed specifically to address this problem. For example, [Chen et al., 2022], explicitly optimizes the original optimization objective while taking advantage of intrinsic motivation. These kind of methods need to be added as baselines.\n\nTypical exploration methods need to be added as baselines. This includes pre-defined entropy schedules: linearly descreasing entropy, constant entropy, constant + linearly decreasing etc.\n\nBenchmarks:\nIn the continuous action setting, the proposed new benchmarks are valuable. However, to provide readers sufficient information also well known benchmarks should be used where existing baseline results are available. Examples of continuous action benchmarks which require exploration such as AntMaze etc. can be found for example in the hierarchical reinforcement learning literature (see [Nachum et al., 2018] and follow the citations to the newest work with the largest environments).\n\nThe \"Game Console\" problem in exploration is valuable and interesting but what is the relationship of the proposed approach compared to other methods that do not use intrinsic rewards? In \"Game Console\" type of problems, mostly intrinsic rewards cause problems?\n\n\nDetails:\n\nPlease explain \"More severely, for discrete actions, the entropy loss can not maintain the distribution shape, i.e., the order of actions\u2019 probabilities of the learned policy.\" in more detail.\n\nRegarding control of policy variance in Equation 4, it seems that identical variances for all action dimensions is assumed?\n\nIn Fig. 2, please define what entropy maximization means. For a discrete distribution, maximum entropy results in a uniform distribution which differs from Fig. 2b.\n\nThe presentation is overall OK but there are typos such as  \"probablities\" that should be fixed.\n\n\nChen, E., Hong, Z. W., Pajarinen, J., & Agrawal, P. (2022). Redeeming intrinsic rewards via constrained optimization. Advances in Neural Information Processing Systems, 35, 4996-5008.\n\nNachum, O., Gu, S. S., Lee, H., & Levine, S. (2018). Data-efficient hierarchical reinforcement learning. Advances in neural information processing systems, 31.\n\nHaarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P. and Levine, S., 2018. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905."
            },
            "questions": {
                "value": "I recommend rejecting the paper. The authors can improve the paper by improving the motivation for the approach, discussing in more detail in which situations the approach works and does not work, providing proper experimental baselines and benchmarks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents Goal Achievement Guided Exploration (GAGE), an algorithm to prevent premature convergence and encourage exploration in deep RL. The paper describes the major causes of premature convergence in RL and describes an algorithm to address specific types of issues, which is then evaluated in several different task domains."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper is well written, and does an excellent job of contextualizing and motivating work on premature convergence, and intuitively develops and explains the GAGE algorithm, which is simple yet not trivial. This issue is an important one that is common in practice, but has received little attention from prior work, and thus a worthwhile topic of study. The GAGE algorithm requires several well-stated priors to work, most notably a human estimate of what the maximum achievable reward is for each reward term, but in my option this is a reasonable prior for many RL domains and not overly restrictive. The experimental validation of the algorithm is reasonably thorough."
            },
            "weaknesses": {
                "value": "I didn't have any major issues with this paper, though there's a few issues I've noted in the questions section which could be improved. \n\nThe biggest concern I have is that the benefits of doing any form of action smoothing to prevent premature convergence versus the specific algorithm of GAGE are not clear- it could be the case that a simpler baseline would be just as good (though I suspect this is not the case). However, this paper does not claim to be definitive regarding premature convergence prevention or action smoothing algorithms (and it doesn't need to be to provide a meaningful contribution), so I don't find this to be a critical flaw.\n\nWhile not the final word on the problem (if such a thing is even possible), this work seems like a worthwhile step forward, with real implications for deep RL in practical and scientific use. I could see GAGE or a similar algorithm plugging in nicely as a standard tool to improve performance and stability alongside other methods. As such, I am inclined to recommend acceptance- this is good work."
            },
            "questions": {
                "value": "Some minor issues and questions:\n\n-What do the upper and lower brackets in equation 7 denote? I don't see this explained in the text and it is unusual notation in my experience of the field.\n\n-The temperature computation for smoothing action probabilities is somewhat complex compared to simpler alternatives mentioned (e.g. mixing with uniform). I don't see any ablations testing whether this more sophisticated smoothing is better than the naive baseline, however, which be useful to see.\n\n-The lines in figure 3 are a bit too small to comfortably read, please make them bigger (plot size is fine, the lines are too narrow).\n\n-The captioning and plot spacing in figure 4 is a little confusing, the right two plots should be closer together to show that they are a pair, unlike the left plot.\n\n-Section 4.2's writing takes a sudden nosedive- there's a number of instances of odd phrasing and wrong grammar here in what is otherwise an excellently written paper. This could use a pass to revise.\n\n-For figure 5, what is the baseline performance for each method on the non-game-containing version of this task? I assume all algorithms can learn the task successfully? It would be good to make this clear if it is so as it strengthens the point being made.\n\n-I would have liked to see more aggressive stress tests on the reward upper bound estimate where performance is lost as a result of a bad estimate. What happens if V_star in figure 4a is set to 1? What if it is set to 99? I imagine these won't perform well, but it would be useful to know what happens when things break down since sometimes human estimates of the maximum possible reward will be quite wrong."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to use goal achievement as a learning progress measure to schedule the noise for exploration, where goal achievement is defined as the ratio of the current policy's expected return to the optimal policy's expected return. The results showed that goal achievement improves PPO's performance in robotic tasks with intensive reward shaping and hard-exploration tasks in MiniGrid."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This method is very easy to implement and seems to improve the performance greatly. The authors claim that current methods suffer from premature convergence. Thus, they propose to tune the noise of exploration adaptively using a goal achievement rate, with the assumption that the maximum reward is known."
            },
            "weaknesses": {
                "value": "- Lack of theoretical discussion. This is fine since I understand this paper's contribution is a practical algorithm. Still, it would be great to see why adjusting the noise level with goal achievement leads to improvement.\n- Writing is a bit verbose. Section 2 is mostly about previous works. The proposed method doesn't come until page 4, which is too long in my opinion."
            },
            "questions": {
                "value": "- Figure 3's legends are too small to read.\n- Where are the differences between GAGE-50 and GAGE-100?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an exploration approach in reinforcement learning aimed at mitigating the premature convergence issue. The proposed Goal Achievement Guided Exploration (GAGE) measures the ratio of currently achieved cumulative rewards over the expected maximum cumulative rewards as a criterion. If the agent has not reached an expected level of performance, it is encouraged to continue exploring."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The GAGE algorithm is straightforward and easy to understand. The core idea is to set an expected \"goal\" and have the agent keep exploring (rather than converging) until the set goal is reached. The presentation is smooth, and the paper provides a comprehensive review of related works. The targeted issue of premature convergence is clearly stated and effectively addressed. The paper discusses both continuous and discrete action spaces and proposes appropriate solutions for each."
            },
            "weaknesses": {
                "value": "1. The \"goal achievement\" is defined as the ratio of achieved cumulative rewards for the current policy over the maximum or optimal cumulative rewards. Since the optimal policy is unknown, the paper proposes setting a hyperparameter as a threshold. However, this introduces two limitations:\n\n(1) The goal-setting determines the upper bound of learning performance, or at least, heavily influence the learning process. If the goal is set too high, the algorithm may struggle to converge as the agent will always perceive its performance as insufficient. Conversely, if the goal is set too low, the agent will reach it too easily, which may still lead to premature convergence.\n\n(2) In this case, the expected \"goal\" is highly task-specific, requiring prior knowledge to define an appropriate threshold for different tasks.\n\n2. The paper identifies four main factors contributing to premature convergence (discussed in Section 2.1). However, the five continuous control tasks used in the experiments do not seem to reflect these factors well. The motivation for selecting these tasks, and how they are capable of demonstrating the effectiveness of the GAGE algorithm in addressing premature convergence, should be more clearly explained.\n\n3. In the experiments, the five continuous control tasks only compare GAGE with the backbone PPO algorithm. I believe comparisons with some benchmarks are necessary to fully demonstrate the advantages of GAGE."
            },
            "questions": {
                "value": "1. In Section 4.1 (around Line 400), the experiments show that \"When the target speed is set to 5m/s, which is below the learned optimal speed (~7m/s), the GAGE agent is still able to learn the optimal speed.\" Referring to Equations (2) and (4), if the learned policy achieves higher rewards than the expected target, the \"goal achievement\" $g(\\pi) >1$, which means the lower bound $\\sigma_L(\\pi) = -\\sigma_0 g(\\pi) + \\sigma_0 < 0$. Additionally, if the learned policy has already achieved the target goal of 5m/s, it would focus mainly on convergence and less on exploration, how is it able to continue optimizing to reach 7m/s?\n\n2. What happens if the target goal is set too high? Will this result in the agent lacking confidence and failing to converge?\n\n3. While GAGE is designed to avoid local optima, in Figure 3, we can observe that some GAGE variants still become trapped in local optima. For instance, in *Ant Acrobatics*, both GAGE-75 and GAGE-100; in *Humanoid Pole*, GAGE-100; and in *Humanoid Tightrope*, both GAGE-50 and GAGE-100 converge to relatively low episodic returns. Does this indicate that the local optima issue is not fully addressed?\n\nI would like to increase the score if these concerns are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}