{
    "id": "XWb6dPuhmC",
    "title": "DUAL-TASK VAE FOR NODE-LEVEL DATA AUGMENTATION",
    "abstract": "Graph Neural Networks (GNNs) are adept at handling graph-structured data but are often limited by the scarcity of high-quality labeled data. To address this limitation, our study presents a Variational Autoencoder (VAE)-based approach for node-level data augmentation, which enriches the original graph data by creating new representations in the latent space. We adopt a two-stage framework: a VAE learns latent representations in the first stage, while a Graph Attention Network (GAT) performs node classification in the second stage. Our experiments on the Cora dataset indicate that integrating raw data with latent representations (Raw+NR), coupled with dual-task training and loss weight adjustment, markedly improves model performance. The optimized model achieves a peak accuracy of 0.886 and an F1 score of 0.873, approaching state-of-the-art performance with a simpler model. This study offers valuable strategies for enhancing graph data augmentation and GNN training, which could contribute to advancing research in the field.",
    "keywords": [
        "VAE",
        "GNNs",
        "Graph Data Augmentation",
        "Node Classification",
        "Dual-task Training",
        "Loss Weight Adjustment"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "Introducing valuable strategies for enhancing graph data augmentation,  enabling simpler models to achieve better performance.",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=XWb6dPuhmC",
    "pdf_link": "https://openreview.net/pdf?id=XWb6dPuhmC",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a novel approach for node-level data augmentation in graph neural networks (GNNs) using a dual-task Variational Autoencoder (VAE). By encoding graph data into a latent space, it aims to improve node classification tasks through an augmentation strategy that combines raw features with latent representations. Experimental results on the Cora dataset suggest that this method could potentially enhance model performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1 - The paper proposes an original VAE-based framework for graph data augmentation, which could be valuable for improving data availability and robustness in GNNs.\n\n2 - The use of a multi-channel convolutional layer in the VAE\u2019s encoder, including various GNN models (e.g., GCN, GAT, SAGE, GIN), demonstrates a well-thought-out design to capture complex node representations.\n\n3 - The authors evaluate their model\u2019s effectiveness using multiple performance metrics, including accuracy, F1 score, and precision, providing a comprehensive set of evaluation perspectives."
            },
            "weaknesses": {
                "value": "1 - The paper lacks a comparison with more recent state-of-the-art GNNs, which limits the ability to contextualize the performance gains claimed. Evaluating against stronger baselines could more convincingly demonstrate the proposed method\u2019s advantages.\n\n2 - The quality of the figures and tables is inadequate. Important architectural details and quantitative comparisons are not well-visualized, making it challenging to interpret the findings effectively.\n\n3 - Although the approach was tested on the Cora dataset, it\u2019s unclear if the findings would hold across other datasets with different characteristics, limiting the method\u2019s applicability."
            },
            "questions": {
                "value": "Why did you not include recent state-of-the-art GNNs as baselines? How would your approach compare to these models in terms of performance and computational cost?\n\nCould you elaborate on the rationale behind the selection of the Cora dataset? Would you expect similar improvements on larger, more complex datasets?\n\nCould you improve the visual clarity of figures and tables? Specifically, the architecture diagram and performance comparison tables would benefit from higher resolution and better layout."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies the data augmentation strategy for node-level classification. The authors propose to integrate a Variational Autoencoder (VAE) to augment the node features. They find this augmentation strategy achieves impressive performance on Cora, when applied to a specially designed GNN architecture."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(+) This work presents an interesting trial that tries to enrich the node features to perform the data augmentation;\n\n(+) The authors integrate two strategies, including the feature fusion, and the VAE to augment the node features;\n\n(+) Some simple experiments demonstrate certain effectiveness of the enriched features;"
            },
            "weaknesses": {
                "value": "(-) The novelty is limited. For example, the fusion of multiple GNN convolutional features is one of the standard machine learning tricks in data science competitions like Kaggle;\n\n(-) The presentation is clear, yet most of the contents in this paper are already known to the community;\n\n(-) The results lack of convinceness, as it only covers simple datasets, single random seed and GNN backbones;"
            },
            "questions": {
                "value": "1. The novelty is limited:\n- For example, the fusion of multiple GNN convolutional features is one of the standard machine learning tricks in data science competitions like Kaggle;\n- Meanwhile, it is also unclear why VAE features could help with the task;\n\n2. The presentation is clear, yet most of the contents in this paper are already known to the community. \n- In addition, the current manuscript looks unready for publication due to the lack of formality in multiple sections, such as Table 1.\n\n3. The results lack of convincingness:\n- The experiments only cover simple datasets, single random seeds and GNN backbones;\n- The visualization of features offer limited insights;"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose a Variational Autoencoder (VAE)-based method for node-level data augmentation to improve Graph Neural Network (GNN) performance on node classification tasks. The approach combines raw graph data with latent representations generated through a VAE, using a dual-task framework involving node classification and data reconstruction."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The paper presents an attempt to use VAE-based data augmentation for GNNs, which could be beneficial in enhancing model robustness to noise and incompleteness."
            },
            "weaknesses": {
                "value": "- The paper\u2019s novelty is minimal.\n- The methodology is somewhat incremental and lacks clarity.\n- The experimental results are superficial.\n- The writing quality is poor."
            },
            "questions": {
                "value": "- Novelty and Motivation. While the paper claims to propose a novel VAE-based method for data augmentation in GNNs, it primarily combines standard techniques (VAE and GNNs) with minimal innovation. The authors should clarify how their approach distinguishes itself from existing data augmentation methods for graph neural networks or from previous work on dual-task models. \n\n- What is the rationale for combining multiple GNN architectures (GCN, GAT, SAGE, GIN) within the VAE model? In Section 3.2.1, the authors describe a multi-channel convolutional layer that integrates several GNN backbones but do not explain how these specific architectures complement each other or the benefits of combining them. For instance, why is it necessary to incorporate both GAT and SAGE, and how do they contribute to the model\u2019s performance?\n\n- No hyper-parameter analysis for\u00a0the weighting parameters $a$ and $b$ introduced in Eq.5\n\n- Section 4.7 discusses the impact of architectural complexity but does not compare the computational complexity and efficiency of the model's multi-GNN backbone architecture.\n\n- The experimental results lack comparisons with adequate baselines. Why did the authors not compare their approach with other data augmentation methods, such as edge dropping, node dropping, feature masking, or subgraph sampling, to validate their results? Could the authors consider including more competitive baselines, such as GraphMAE [1] and GraphCL [2]?\n\n- Could the authors explain why they used only the Cora dataset, a relatively small and well-known benchmark, instead of employing more challenging and widely adopted datasets, such as Citeseer, Pubmed, ogb-arxiv, or Wiki-CS?\n\n- The paper\u2019s figures are non-vector images, and its structure is disorganized with vague descriptions, impairing clarity. Specifically, figures such as the architecture diagrams in Sections 3 and 4 should be presented as vector graphics to improve visual quality, especially when zoomed in. These issues compromise the paper\u2019s professionalism and readability, particularly for a technical audience.\n\n[1] Hou, Zhenyu, et al. \"GraphMAE: Self-supervised masked graph autoencoders.\" *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, 2022.  \n[2] You, Yuning, et al. \"Graph contrastive learning with augmentations.\" *Advances in Neural Information Processing Systems* 33 (2020): 5812-5823."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a two-stage training framework to alleviate the supervision shortage issue. In the first stage, the VAE is trained with the reparameterization method to model the distribution of node representations. Then its output will be used to improve the performance of the GNN in the second stage."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper provides a very detailed introduction to the backbone architectures used in the work."
            },
            "weaknesses": {
                "value": "The motivation of the paper is unclear. Numerous strategies already address label scarcity in graph machine learning (e.g., Graph SSL). Why is this new framework necessary, and what specific advantages does it offer over existing approaches?\n\nThe experimental evaluation is limited to the Cora dataset, which lacks comprehensiveness. Even within the scope of smaller datasets, the authors have many options to broaden the evaluation. Expanding experiments to more diverse datasets is encouraged. Additionally, the authors should consider the significance if the proposed framework cannot effectively handle larger datasets.\n\nThe paper\u2019s presentation requires improvement. There is considerable redundant content, and the figures and tables lack clarity and effective demonstration."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}