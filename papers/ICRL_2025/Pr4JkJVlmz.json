{
    "id": "Pr4JkJVlmz",
    "title": "MAQL: Speeding up Q-learning with a model-assist",
    "abstract": "In reinforcement learning, model free methods such as Q-learning and policy gradient are extremely popular due to their simplicity but require a huge amount of data for training. Model based methods on the other hand, are proven to be sample efficient in various environments but are unfortunately computationally expensive. It is therefore only prudent to investigate and design algorithms that have best of features from both these classes of algorithms. In this work, we propose MAQL, a model-assisted Q-learning algorithm that is not only computationally inexpensive but also offers low sample complexity. We illustrate its superior performance to vanilla Q-learning in various RL environments and particularly demonstrate its utility in learning the Gittins/Whittles index in Rested/Restless Bandits respectively. We aim to spur discussion on how model-assists can help boost the performance of existing RL algorithms.",
    "keywords": [
        "bandits",
        "reinforcement learning"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "While learning the value functions through stochastic approximation based methods like Q-learning,  one can use the estimates learnt from the environment to drive value iteration style updates to improve Q-learning.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Pr4JkJVlmz",
    "pdf_link": "https://openreview.net/pdf?id=Pr4JkJVlmz",
    "comments": [
        {
            "withdrawal_confirmation": {
                "value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."
            },
            "comment": {
                "value": "We would like to express our sincere gratitude to the reviewers for their invaluable feedback and insights. After careful consideration, we have decided to withdraw our submission to further develop and refine our paper based on these constructive comments. Thank you once again for your time and thoughtful evaluation."
            }
        },
        {
            "summary": {
                "value": "This paper introduces MAQL, a novel algorithm that combines the strengths of model-based and model-free Reinforcement Learning (RL) to improve the sample efficiency of traditional Q-learning. MAQL alternates between traditional Q-learning updates and model-based value iteration-like updates. The authors also provide an extension of MAQL to DQN and show empirical results on some simple environments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper shows a way to combine model-free and model-based methods, which seems promising for RL."
            },
            "weaknesses": {
                "value": "- The empirical results are quite lacking. There are many model-based algorithms out there that should be compared to, as baselines, but the authors only compare to Q-learning. Not only that, there are also many variants of Q-learning that could be worthy baselines, but the authors also did not include those. The same is true for the set of DQN experiments. Because of this, I'm not adequately convinced in the usefulness of these methods. (For example, why not use some of the baselines mentioned in the intro?)\n\n- In MA-DQN, the tabular equation (2) is used for the model learning. But in DQN settings, we are in a setting with large state (or action) spaces, so this method of model learning is not going to scale well. I wonder how the authors would address this major issue?\n\n- Because the results are only shown for relatively simple environments, the above concern is not really tested. I'd recommend the authors test their method on real settings where DQN would be applied."
            },
            "questions": {
                "value": "- Can we say anything theoretically about this algorithm? It seems to me that under certain assumptions, one should be able to show convergence by leveraging standard arguments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces MAQL and Model-assisted DQN algorithms that learn by integrating a learned model into Q-learning and DQN, respectively. The authors experimentally demonstrate the performance improvements of MAQL over standard Q-learning in GridWorld and Gymnasium Taxi environments and of Model-assisted DQN over DQN in a Windy GridWorld. The authors also demonstrate an adaptation of MAQL to improve the learning of indices in rested and restless bandit settings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors propose a simple but effective algorithm that is applicable to various environments including both structured and unstructured problems."
            },
            "weaknesses": {
                "value": "1-\tLimited exploration of MAQL\u2019s performance across diverse, complex environments. While the empirical results demonstrate the algorithm's improvement over standard Q-learning and DQN in relatively simple (toy) environments (like grid-worlds, the Taxi domain), the paper does not investigate its scalability in larger, high-dimensional state spaces, where model-based assistance could potentially face the curse of dimensionality. For example, could you evaluate performance on Atari games or continuous control tasks from the MuJoCo suite?\n\n2-\tThe authors compared performance against simple baselines such as standard Q-learning and DQN and did not compare performance against highly relevant algorithms such as Dyna-Q. The authors could show the significance of their algorithm by comparing against other algorithms like Double-QL, Dyna-Q, Rainbow DQN, etc. This is especially important since their proposed algorithm is similar in spirit to Dyna-Q. \n- In particular, how does MAQL compare to Dyna-Q in terms of sample efficiency?\n- How does MAQL handle the exploration-exploitation trade-off compared to Double-QL? \n- Also, could you compare the computational efficiency of MAQL to these baselines, given that one of MAQL's claimed advantages is being computationally inexpensive?\n\n3- Lack of detailed ablation studies, e.g., investigating the sensitivity of MAQL to variations in model accuracy.\n- For example, how sensitive is MAQL to specific hyperparameters like the frequency of model-based updates (parameter 'c' in Algorithm 1) or the number of Q-iteration steps (parameter 'k')?"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies dqn with model-based approach. The transition probability is estimated by maximum likelihood method, i.e., how often it has occured."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper studies model-assisted DQN based on model-assisted Q-learning. The model-assisted Q-learning has been studied in quite detail in theoretical sense, which is not properly discussed in this paper. Please refer to weakness 1."
            },
            "weaknesses": {
                "value": "1. The model-assisted DQN approach requires $|\\mathcal{S}|^2|\\mathcal{A}|$ memory space to store the transition probability.  As the authors mentioned, it would be better to show that prioritized sweeping or Gaussian Process to show that this approach is still valid while not degrading the performance too much."
            },
            "questions": {
                "value": "1. I understand that this is an empirical paper but misses most of the literature on model-based Q-learning approach [1,2,3, 4] in the tabular setting. In particular, in [1], the update in equation (2) and (3) is exactly the one considered in [1]. Even though the details might slightly different in implementing the algorithm, the key update is the same.  Furthermore there are somewhat practical implementations based on such theory, e.g., [5], and can the authors provide comparison with it?\n\n\n**References**\n\n[1] Lim, Han-Dong, HyeAnn Lee, and Donghwan Lee. \"Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model.\" arXiv preprint arXiv:2402.11877 (2024).\n\n\n[2] Szita, Istv\u00e1n, and Csaba Szepesv\u00e1ri. \"Model-based reinforcement learning with nearly tight exploration complexity bounds.\" Proceedings of the 27th International Conference on Machine Learning (ICML-10). 2010.\n\n[3] Brafman, Ronen I., and Moshe Tennenholtz. \"R-max-a general polynomial time algorithm for near-optimal reinforcement learning.\" Journal of Machine Learning Research 3.Oct (2002): 213-231.\n\n[4] Tor Lattimore and Marcus Hutter. Near-optimal pac bounds for discounted mdps. Theoretical Computer Science, 558: 125\u2013143, 2014.\n\n[5] Henaff, Mikael. \"Explicit explore-exploit algorithms in continuous state spaces.\" Advances in Neural Information Processing Systems 32 (2019)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper seeks to enhance the performance of Q-learning by incorporating the state-counting defined in equation (2), and evaluates this approach through a series of simple experiments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper is self-contained and well-structured."
            },
            "weaknesses": {
                "value": "1. **Methodological Limitations**: The proposed approach involves counting states, actions, and next states from experiences to improve the prediction of future Q-values based on rewards. However, this method faces a fundamental limitation when applied to environments with continuous state or action spaces, where the number of possible (state, action, next state) pairs becomes infinite, making such counting impractical. As a result, the applicability of this method is extremely restricted.\n  \n2. **Insufficient Experimental Validation**: The experimental results are not sufficiently convincing. While the paper uses DQN, the experiments are conducted only in very simple toy environments. To demonstrate the robustness of the method, I believe experiments on more complex benchmarks, such as Atari games, are necessary.\n  \n3. **Poor Writing Quality**: The writing in this paper is not good, with numerous typographical, citation, and mathematical expression errors. For example, the term \"model-free\" is inconsistently used\u2014appearing as \"model free\" in the abstract, but \"model-free\" in lines 41-42. There is also an incorrect citation in lines 37-38, 46-47, 48-49, while an incorrect definition of the reward function exists in lines 96-97, and a typographical error with \"atleast\" in lines 101-102."
            },
            "questions": {
                "value": "How to adopt your algorithms into a environment with continuous action space and/or state space?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}