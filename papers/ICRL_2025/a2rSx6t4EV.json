{
    "id": "a2rSx6t4EV",
    "title": "EDU-RAG: A RAG Benchmark with Web-enhanced Content in Education Domain. Can RAG Help AI Tutor?",
    "abstract": "Hallucination has been a notable concern in leveraging Large Language Models (LLMs). Retrieval-Augmented Generation (RAG) is a popular method in addressing that and in maintaining context and coherence in generated outputs and including customized knowledge. In this paper, we aim to create an updated benchmark dataset using middle-school science-domain-ed textbook-question-answering, and web search results, and to evaluate the performances of different LLMs incl. GPT-4o, Llama2-7b, Llama3-8b, with and without RAG. Basically, we want to evaluate whether RAG would improve hallucination problem caused by pre-trained biased LLM model knowledge, or drawing from irrelevant retrieved knowledge despite of access to relevant information. The dataset and method can be used to evaluate different RAG methods.",
    "keywords": [
        "Large Language Models (LLM)",
        "Retrieval Augmented Generation (RAG)",
        "Search"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=a2rSx6t4EV",
    "pdf_link": "https://openreview.net/pdf?id=a2rSx6t4EV",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a benchmark dataset for evaluating LLMs in the education domain. Further authors experiment with RAG based approach to reduce hallucinations in LLMs in the context of education."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The motivation behind the research in the education domain is relevant and apt.\n2. Authors create a new dataset of MCQA for middle school level science. The dataset is augmented with information obtained via web-searches."
            },
            "weaknesses": {
                "value": "1. The paper lacks novelty as authors are implementing the standard RAG architecture for the QA task. \n2. The newly created dataset is mainly an extension of an existing TQA dataset augmented with content from the web, so there is very limited innovation.  \n3. The paper doesn't report any new findings; the authors show that RAG helps to mitigate hallucinations to some extent but this is already known and established by previous research. \n4. The paper is poorly written with grammar mistakes, typos, poor formatting. A figure (Figure 2) is also missing from the paper, in place of that a blank box appears."
            },
            "questions": {
                "value": "Suggestions:\n1. Authors should improve the formatting of the paper. For example, references are not in parenthesis (e.g., line 33, 35, etc.). Similarly, there are several grammatical mistakes in the paper that authors should fix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an RAG benchmark in the education domain. The benchmark builds on top of AI2's TQA dataset, augmented with web search results for each question. The paper evaluates a few LLMs on performance on this benchmark where the metric is answer correctness (note that the author defined \"hallucination\" in terms of answer correctness). The author finds that RAG improves performance of LLMs in answering these questions. The author claims this benchmark will be useful for \"advancing the evaluation and development of RAG techniques in mitigating hallucinations across diverse LLMs.\""
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "I like the direction that this paper attempts to go: evaluating and benchmarking RAG systems and LLMs and checking whether they hallucinate is very important. Doing all of these in the context of education is also important. I'm excited to see more work along this direction."
            },
            "weaknesses": {
                "value": "The paper seems to be poorly presented/written. Contributions seem insufficient and unclear.\n\nIn terms of presentation:\n- Missing figure (figure 2)\n- Table 1 and 2 are repeated \n- Not sure what the example question means in Fig 1 (the question is used multiple times in subsequent results and illustrations)\n\nIn terms of contribution:\n- It is unclear to me what the *fundamental* differences or advancements that the proposed benchmark has compared to prior work. For example, I'm not sure how this benchmark is different from Yang 2024's. Expanding their dataset to a new domain doesn't seem super novel to me unless there are some fundamental differences or challenges which requires innovations in either constructing the benchmark or evaluating on the data, neither of which I find novel in this paper. This is my main concern.\n- The research questions seem to be studying well-known conclusions. For example, RQ2 is, roughly speaking \"can RAG help improve LLM performance\"? I think the answer is YES by now and it is widely known. When reading the abstract, I thought the authors would evaluate how LLMs would perform in the presence of irrelevant information (line 22 ~ 23), which I think would be a deeper and more interesting analysis than RQ2, but unfortunately the authors did not present such analyses or findings. In general, I find the conclusions and analyses in this paper either already known or shallow.\n- Some of the suggested future work is already done a few years ago. For example, SFT on retrieved content is explored in this paper back in 2022: https://arxiv.org/pdf/2201.08239 \n\nGiven that the paper appears not ready for presentation and the contributions are unclear and not particularly novel (see above for details), I would not recommend acceptance of this paper."
            },
            "questions": {
                "value": "What are the fundamental differences between this benchmark and previous ones? Or more specifically, what are the fundamental differences in the education domain versus finance etc in Yang 2024?\n\nThe prompt says \"You must choose a letter even if you are unsure.\" How would one expect the LLM to output \"I don\u2019t know\u201d if they are not instructed to do so? \n\nWhat motivates the authors to define hallucination as answer correctness? The way the authors defined hallucination seems more like model's problem solving or reasoning capabilities rather than hallucination."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces EDU-RAG, a benchmark dataset designed to evaluate the performance of Retrieval-Augmented Generation (RAG) techniques in the context of middle-school science question answering. The dataset combines textbook questions with relevant web search results, addressing the challenge of hallucination in Large Language Models (LLMs) like GPT-4o and Llama2-7b."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Originality: The paper presents a new benchmark dataset, EDU-RAG, specifically tailored for evaluating RAG techniques in the domain of middle-school science question answering. \nQuality: The methodology for constructing the benchmark dataset is explained, including the selection of questions, retrieval of web content, and processing of the data.\nClarity: The authors also provide a comprehensive overview of related work, placing their contributions within the broader context of RAG research.\nSignificance: The EDU-RAG benchmark has the potential to significantly advance the evaluation and development of RAG techniques for educational applications."
            },
            "weaknesses": {
                "value": "- The paper only evaluates a basic RAG algorithm design, consisting of a retriever, reranker, and generator. While this serves as a useful baseline, it does not explore the effectiveness of more advanced RAG techniques, such as modular RAG or advanced reranking methods.\n- The paper acknowledges that RAG can reduce hallucination in some cases but also highlights instances where it may worsen the issue. However, the analysis of this phenomenon is limited, and the paper does not delve into the underlying reasons of it.\n- Typo: The title of section References appears twice.\n- Missing figure: The figure 2 is missing."
            },
            "questions": {
                "value": "- Have you considered expanding the scope of evaluation to include more complex question answering scenarios, such as open-ended questions or multi-hop reasoning tasks for educational applications?\n- How did you address the potential bias in the web content retrieved using Google Search?\n- How did you assess the quality and relevance of the web content retrieved for each question?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}