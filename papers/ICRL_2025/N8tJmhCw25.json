{
    "id": "N8tJmhCw25",
    "title": "On the Almost Sure Convergence of the Stochastic Three Points Algorithm",
    "abstract": "The stochastic three points (STP) algorithm is a derivative-free optimization technique designed for unconstrained optimization problems in $\\mathbb{R}^d$. In this paper, we analyze this algorithm for three classes of functions : smooth functions that may lack convexity, smooth convex functions, and smooth functions that are strongly convex. Our work provides the first almost sure convergence results of the STP algorithm, alongside some convergence results in expectation.\nFor the class of smooth functions, we establish that the best gradient iterate of the STP algorithm converges almost surely to zero at a rate arbitrarily close to $o(\\frac{1}{\\sqrt{T}})$, where $T$ is the number of iterations. Furthermore, within the same class of functions, we establish both almost sure convergence and convergence in expectation of the final gradient iterate towards zero.\nFor the class of smooth convex functions, we establish that $f(\\theta^T)$ converges to $\\inf_{\\theta \\in \\mathbb{R}^d} f(\\theta)$ almost surely at a rate arbitrarily close to $o(\\frac{1}{T})$, and in expectation at a rate of $O(\\frac{d}{T})$ where $d$ is the dimension of the space.\nFinally, for the class of smooth functions that are strongly convex, we establish that when step sizes are obtained by approximating the directional derivatives of the function, $f(\\theta^T)$ converges to $\\inf_{\\theta \\in \\mathbb{R}^d} f(\\theta)$ in expectation at a rate of $O((1-\\frac{\\mu}{dL})^T)$, and almost surely at a rate arbitrarily close to $o((1-\\frac{\\mu}{dL})^T)$,  where $\\mu$ and $L$\nare  the strong convexity and smoothness parameters of the function.",
    "keywords": [
        "Zeroth Order Optimization",
        "Almost sure convergence",
        "Stochastic Three Points Algorithm."
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=N8tJmhCw25",
    "pdf_link": "https://openreview.net/pdf?id=N8tJmhCw25",
    "comments": [
        {
            "summary": {
                "value": "This paper analyzes almost-sure convergence for the derivative-free stochastic three points (STP) algorithm in (Bergou et al., 2020), where only the convergence in expectation was studied. The convergence in expectation does not guarantee convergence of individual instance of trajectories, which is why, for example, extensive research on almost-sure convergence has been conducted for SGD. This paper's study of STP, therefore, aligns with recent theoretical advancements in SGD. In particular, this paper provides the almost-sure convergence results for three standard classes of functions, as summarized in Table 1."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper provides the first almost-sure convergence analysis of the STP method for three standard classes of functions, a non-trivial achievement. This analysis guarantees convergence of each trajectory instance, making it valuable both theoretically and practically."
            },
            "weaknesses": {
                "value": "- Motivation: The focus on STP in this paper is not well motivated in the abstract and introduction, aside from mentiong its low per-iteration complexity. Is the STP widely used in practice? Given its simplicity and strong theoretical guarantees, I cannot see why this would not be considered by practitioners. I suggest that the authors further elaborate on the theoretical and practical importance of studying the STP.\n\n- Comparison to other existing derivative-free methods: Although this paper focuses on the STP, it would have been helpful to include comparisons or discussions of the almost-sure convergence (rate) analyses for other existing derivative-free methods, if available. Without these, it is difficult to locate this work within the broader literature.\n\n- Experiment: What is the purpose of this experiment? It seems quite orthogonal to the paper's theoretical contributions. It looks like it was added to appeal to practitioner reviewers, but I believe it may ultimately satisfy neither theory nor practitioner reviewers. I suggest revising the experiment section to better align with the paper's main contributions."
            },
            "questions": {
                "value": "- Lines 111-112: Although it is straightforward, please explicitly state the consequence of the result.\n- Line 124: $\\theta$ denotes both the iterates and the step size parameter, and I suggest using other letter for the step size parameter.\n- Remark 2: This seems redundant and I suggest removing it.\n- Lemma 4: This does not seem necessary here, which is only used in the proof in the Appendix. I suggest moving it to Appendix.\n- Remarks 7,8 and 10: They discuss the condition that Assumption 3 can be satisfied. I believe that they can be simply discussed right after Assumption 3, or can be more clearly and briefly stated in the remarks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper establishes the almost sure (and last iterate) convergence of the stochastic three-point algorithm for zeroth-order optimization. The paper considers smooth nonconvex, convex, and strongly convex settings and gets the results under three different settings, respectively. Numerical experiments demonstrate the performance of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-written, with technical results clearly presented and explained. Also, different settings are considered in the paper."
            },
            "weaknesses": {
                "value": "My major concern is the technical novelty of the paper.\n\n1. Insufficient technical contribution.\n\n   Although the paper considers multiple settings and gets almost-sure convergence results, the techniques seem to be a combination of [1] and other papers that establish almost-sure convergence of SGD in different settings. Especially the results in section 3 and 4 look less surprising, since most of them are obtained by verifying some conditions and invoking an existing almost-sure convergence result from the SGD literature. \n\n2. Presentation issues.\n\n   The paper contains many technical results, either proven in this paper or cited from other papers. However, there is not sufficient explanation after each main result. This may make the results less accessible to the readers.\n\nOverall I think this is a solid work, and I would like the authors to further clarify the technical contributions in this paper."
            },
            "questions": {
                "value": "1. In the nonconvex case (Theorem 1), lower-boundedness (A2) is not assumed. But on line 624 in the appendix, the authors use the assumption that $f$ is bounded from below. \n2. A7 seems to be an artifact of analysis. Could you provide more intuitions behind it?\n3. Results in section 4 require the initial point to lie in the sublevel set. Here the cost of finding such an initial point is not discussed. Could you elaborate more on this?\n\n**Minor issues**\n\n1. Line 139\n\n   There seems to be an additional 2 in the complexity.\n\n2. Line 146\n\n   The Assump column does not match all the results in the paper (e.g., Theorem 7 in section 5 mentions \"Assumption 1 and 4 to 7\"). Also, assumption 7 does not appear in the table.\n\n3. Line 146, 533\n\n   Please put the caption of the table at the top.\n\n4. Line 204\n\n   The statement of the examples seems incorrect and inconsistent with [1]. The covariance matrix in [1] is $I_d / d$  instead of $I_d$.\n\n   For any arbitrary => For any.\n\n5. Line 318, 372\n\n   $\\ast$ and $^\\star$ are inconsistent.\n\n6. Line 890\n\n   $m$ is undefined.\n\n**References**\n\n[1] Bergou, E. H., Gorbunov, E., & Richtarik, P. (2020). Stochastic three points method for unconstrained smooth minimization. *SIAM Journal on Optimization*, *30*(4), 2726-2749."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Convergence analysis of the stochastic three points (STP) algorithm is studied. The rates for different classes of functions are shown. A almost sure convergence rate arbitrarily close to $O(1/\\sqrt{T})$ is shown for smooth functions.\nAnd almost sure convergence rate arbitrarily close to $o(1/T)$ and in expectation a rate of $O(d/T)$ is shown for smooth convex functions. For the class of strong convex functions, a rate of $O((1-\\mu/dL)^T)$ in expectation and a rate arbitrarily close to $o((1-\\mu/dL)^T)$ for almost sure convergence is shown."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Paper is well-written. Convergence rates of the STP algorithm for smooth, smooth convex and smooth strongly convex functions are shown."
            },
            "weaknesses": {
                "value": "The results are incremental. How does the convergence rate of STP compare with other methods both in expectation and almost sure. For example with RGF or GLD which is compared in the experiments ? This could be discussed or given as a table. What about the optimal convergence rate in each case ?  A brief discussion about the advantages of STP algorithm in the introduction would be helpful."
            },
            "questions": {
                "value": "1. Please add a table comparing the convergence rates (both in expectation and almost sure) of STP, RGF, and GLD for each function class studied. Also please discuss how STP's rates compare to known optimal rates for each case. This is important as it would provide valuable context for the significance of the STP results.\n\n2. Please add a paragraph or two in the introduction highlighting the key advantages of STP over other zeroth-order methods, such as its linear dependence on dimension compared to quadratic dependence for deterministic direct search methods. This would help readers better understand the importance of studying STP's convergence properties."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}