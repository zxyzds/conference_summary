{
    "id": "5o0phqAhsP",
    "title": "Learning under Temporal Label Noise",
    "abstract": "Many time series classification tasks, where labels vary over time, are affected by label noise that also varies over time. Such noise can cause label quality to improve, worsen, or periodically change over time. We first propose and formalize temporal label noise, an unstudied problem for sequential classification of time series. In this setting, multiple labels are recorded over time while being corrupted by a time-dependent noise function. We first demonstrate the importance of modelling the temporal nature of the label noise function and how existing methods will consistently underperform. We then propose methods that can train noise-tolerant classifiers by estimating the temporal label noise function directly from data. We show that our methods lead to state-of-the-art performance under diverse types of temporal label noise on real-world datasets.",
    "keywords": [
        "label noise; time series; healthcare; classification"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "",
    "creation_date": "2024-09-14",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5o0phqAhsP",
    "pdf_link": "https://openreview.net/pdf?id=5o0phqAhsP",
    "comments": [
        {
            "summary": {
                "value": "The manuscript introduced temporal label noise in time series classification tasks and proposed a novel framework that are robust to it. Experiments were conducted on 4 datasets with synthetic noise injection and a real-world noisy labelled dataset, in which the method proposed in the manuscript is superior to other methods. The authors also visualized the dynamic evolution of label noise over time."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe problem proposed by the authors is novel and interesting, and they evaluate it on real-world datasets.\n2.\tThe method introduced in the manuscript is cleverly crafted and has a theoretical foundation."
            },
            "weaknesses": {
                "value": "1.\tIn many settings, the performance of Plug-In method is inferior to that of static methods.\n2.\tOnly one of the five datasets in the manuscript contains both clean and noisy labels. In the remaining real-world scenarios, there is no experimental evidence to confirm that \u201cthe label noise evolves over time\u201d actually exists.\n3.\tThe experimental scenarios are focused on healthcare; validating the work in more diverse scenarios would broaden its applicability."
            },
            "questions": {
                "value": "1.\tWhat is the meaning of variable d in line 126? Is that mean a multivariate time series with d variables?\n2.\tWhen performing a train-test split on the datasets, were individuals also split?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper discusses the problem of learning from time series classification task, where the label noise can vary over time.\u2028Each instance is a sequence over T time steps, where at iteration $t$ we have access to the features at time $t$, and a noisy label $\\tilde y_t$ obtained from the true label $y_t$. Each of this instance is i.i.d., and the assumption is that the true label $y_t$ at time $t$ only depends on the past features $x_{1:t}$, and the noisy labels are conditionally independent to the features given by the true labels. In particular, the authors assume there exists a noise mapping $Q_t$ that describes the noise process at time $t$. They propose a novel method to learn a classifier that maps sequences $x_{1:t}$ to a label $y$. This model simultaneously approximates the noisy mapping $Q_t$ (Eq 3), that is used to approximate the true labels, which are used during training"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Learning from time series data is an interesting problem. The paper addresses a challenging problem, where the label noise can change over time, and it is well-motivated. Overall, the introduction of the paper and the experimental sections are well-written and easy to follow. I found the use of the minimum-volume simplex assumption as an objective to solve the problem to be intriguing."
            },
            "weaknesses": {
                "value": "(**important**). The appendix of the paper is only a draft and it looks like it was not finished to be written. It also seems to be dissociated to the main paper.\n- Appendix A is self contained and not referenced in the main paper. It is not discussed what the content of this section adds as a contribution.\n- Appendix B looks dissociated to the main paper. First of all, three assumptions are introduced (they seems related to the 2 assumptions used in the main paper. In this case, why re-introduce them, and why do we use 3 assumptions rather than 2?). Most of the proofs are just a sequence of mathematical equations with text. Lines 810 to 840 are just a sequence of equations without text (maybe do a table?). The proof of Proposition 1 does not appear, which is only the theoretical result in the main paper (is Theorem 3 the proof of Proposition 1? Why does it have  different notation?).\n- Section G.2 is empty. Page 25 to 47 include a sequence of a lot of figure without almost no text. I recommend the authors to only include the figures that are used to \u201csay\u201d something, together with a text explaination.\n\n\u2028The technical section is also sometimes unclear.\u2028\u2028\n\nNotations is sometimes unclear. What is q_t in line 140? According to the notation defined in the paper $\\mathcal{X} \\subseteq \\mathbb{R}^{d \\times T}$, but the function $h_{\\theta}$ that has domain $\\mathcal{X}$ can also have an input matrix $d \\times t$. \n\u2028\nThere is no comment on the assumptions 1 and 2 (except that they are two standard assumptions). I believe a few lines commenting on those assumptions would be helpful.\n\nIn lines 259-261, why is the Frobenius norm a convex surrogate for the volume? (A citation is also probably needed).\u2028\u2028\n\nIn Line 260, should $R_t$ be defined over $\\tilde{y}$ rather than $y$?\n\n In lines 262-264, my understanding is that $\\lambda$ is the Lagrange multiplier of the constraint of Equation~2. I would be clearer on this, since $\\lambda$ does not appear in Eq 2."
            },
            "questions": {
                "value": "See also weakness.\n\nThe model does not use the noisy label in the prediction (the predictor h_{\\theta} only depends on x_{1:t}).  I believe it would be interesting to include the noisy label in the prediction of the true label (i.e., if the noisy label is always accurate, can we use it for prediction? I understand this is a slightly different setting, as the goal of the paper is to learn a predictor).\u2028\u2028\u2028\n\nThe fact that we learn a noisy label structure Q_w(t) that is \u201ccontinual\u201d over time seems implicit in Eq. (3). However, it is not clear what the difference is between Eq(3) and the model discussed in \u201cdiscontinuous estimation\u201d. It seems to me that $t$ is an integer, and $Q_w(t)$ could be completely different than $Q_w(t+1)$ in principle (what is the temporal relationship discussed in line 274)\n\nIt is a bit unclear how Section 2, in particular section 2.2, is related to the proposed method in Section 3. It seems to me, that the \u201closs of the classifier\u201d is embedded in the constraint of Equation 2. Lines 233-234 says that Eq(2) minimizes the forward temporal loss as in Def 2, but this loss does not actually appear on Equation 2.\u2028\u2028\u2028\n\nTypos: 277 continuuity"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a model to incorporate varying noise rates for time-series classification. The method specifies a matrix-valued function indicating label-noise distribution. This noise function is incorporated into a forward temporal loss over noisy labels that is used to learn a classifier (using a neural net) minimizing the loss. The authors describe three approaches: continuous, discontinuous, and plug-in. They empirically evaluate these approaches in various benchmark datasets. They also demonstrate the approach in a real-world dataset within the healthcare domain in which the authors claim that the problem of temporal noisy labels is prevalent in time-series classification. The empirical results clearly indicate that the proposed approach works well compared to SOTA static approaches that treat noisy labels as static rather than temporal."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1) The formulation of the proposed methodology is clear and concise.\n2) The motivation and subsequent real-world example is well demonstrated."
            },
            "weaknesses": {
                "value": "(1) It is not clear how is learning achieved when weights are initialized for each time step in discontinuous estimation. It looks like the dataset is small for the learning models to converge as all the datasets largely seem to have less than 1000 samples.\n(2) Can the estimator in the discontinuous case be replaced by a simpler model (with lesser number of parameters)? If so, can such an alternative be used as baseline mechanisms to compare for temporal noise (as all alternative mechanisms shown in the paper are for static cases)? \n(3) It is interesting that both continuous and discontinuous estimations perform quite similarly on the real-world stress detection example. Does this mean that the proposed approach works best when there is large variance in noise rates?"
            },
            "questions": {
                "value": "(1) Is there an assumption that all data instances have equal number of time steps?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces the problem of temporal label noise, where label quality fluctuates over time due to time-dependent noise. This type of noise may improve, deteriorate, or change periodically, impacting the accuracy of predictions. The authors define this problem and highlight that existing methods fail to handle the temporal aspect of label noise effectively. They propose methods to estimate the time-varying noise function directly from data, allowing the development of noise-tolerant classifiers. Experiments on real-world datasets validate their proposal, demonstrating its effectiveness under various types of temporal label noise."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The problem of Temporal Label Noise is both interesting and important, and this paper is the first to formally define it.\n\n2. The paper is well-organized, clearly written, and easy to follow.\n\n3. The mathematical symbols and equations are well-defined and easy to understand."
            },
            "weaknesses": {
                "value": "1. My primary concern is with the problem formulation, or more specifically, the goal of the problem. In Section 2.1, the paper aims to \"estimate parameters $\\hat{\\theta}$ for a model robust to noise,\" which implicitly assumes the existence of a **fixed and static optimal model** $\\theta^\\star$\u2014i.e., that p(y_t|x_t) remains consistent across all timestamps. However, in time-series problems, the environment is constantly changing, and the optimal model for different timestamps should also be changing.\n\n2. The other major concern relates to the methodology. The paper appears to assume that the function Q is known to the learner, and focuses only on optimizing the parameter w. However, I think that **obtaining an appropriate function Q is the most challenging aspect of this problem**. Although in Section 3.2, the authors select Q as a fully connected neural network as a general form for the practical scenarios, it seems to be inconsistent with the earlier discussions in Table 1.\n\n3. Another concern is about the experiments. The authors seem to only conduct experiments on low-dimensional datasets. I think that including experiments on larger-sized datasets would strengthen the results and make the findings more convincing.\n\nPlease let me know if I have misunderstood any part of the paper."
            },
            "questions": {
                "value": "See Weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}