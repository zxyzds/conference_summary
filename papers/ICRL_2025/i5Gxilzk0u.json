{
    "id": "i5Gxilzk0u",
    "title": "Multi-modal Controlled Coherent Motion Synthesis",
    "abstract": "We walk and talk at the same time all the time. It is just natural for us. This paper tackles the challenge of replicating such natural behaviors in 3D avatar motion generation driven by concurrent multi-modal inputs, e.g., a text description ``a man is walking\" alongside a speech audio. Existing methods, constrained by the scarcity of aligned multi-modal data, typically combine motions from individual modalities sequentially or through weighted averaging. These strategies often result in mismatched or unrealistic movements. To overcome these limitations, we propose MOCO, a novel diffusion-based framework capable of processing multiple simultaneous inputs\u2014including speech audio, text descriptions, and trajectory data\u2014to generate coherent and lifelike motions without requiring additional datasets. Our key innovation lies in decoupling the motion generation process. During each denoising step, the diffusion model independently generates motions for each modality from the input noise and assembles the body parts according to predefined spatial rules. The resulting combined motion is then diffused and serves as the input noise for the subsequent denoising step. This iterative approach enables each modality to refine its contribution within the context of the overall motion, progressively harmonizing movements across modalities. Consequently, the generated motions become increasingly natural and fluid with each iteration, achieving coherent and synchronized behaviors. We evaluate our approach using a purpose-built multi-modal benchmark. Experimental results demonstrate that MOCO significantly outperforms existing baselines, advancing the field of multi-modal motion generation for 3D avatars. The code will be released.",
    "keywords": [
        "Human Motion Generation; Multi-Modal; Generative Models"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=i5Gxilzk0u",
    "pdf_link": "https://openreview.net/pdf?id=i5Gxilzk0u",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes MOCO, a diffusion-based framework for generating coherent motions from multi-modal inputs like text, speech, and trajectory data. It addresses the challenge of simultaneous multi-modal control in 3D avatar motion generation. The key innovation is a decoupled denoising process that generates and combines motions for each modality. Experiments on a custom benchmark show it outperforms baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(1) MOCO is quite straightforward and achieves better realism and coherence compared to existing methods that process modalities through weighted averaging.\n(2) Comprehensive evaluation: The use of a purpose-built multi-modal benchmark and a wide range of metrics for evaluation provides a thorough assessment of the method's performance.\n(3) The writing of paper is clear without ambiguities."
            },
            "weaknesses": {
                "value": "(1) While the paper mentions trajectory control, the evaluation focuses primarily on text-to-motion and speech-to-gesture tasks. A more comprehensive evaluation of trajectory control, including metrics specific to this modality, would be beneficial.\n\n(2) The statistics of Multi-Modal Benchmark are missing.\n\n(3) A Qualitative comparion among different methods (like User Study) are missing.\n\n(4) The results of many evaluation metrics  are not promising at all, also authors only provided a cherry-picked video as demo, the effectiveness of this method is not so convincing.\n\n(5)  Lacking theorical analysis of how decoupled-then-combined denoising works. Also, I am curious whethe is a general paradigm for multi-modal condition generation or only for this setting."
            },
            "questions": {
                "value": "(1) How about the performance comparison between \"Synchronous\" and  \"Asynchronous\" conditions?\n\n(2) How many clips does your methods support to generate?(each one might be conditioned on several condition), and how the performance changes with the number of clips. Because your Figure 1. and demo video show you support this feature."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces MOCO, a diffusion-based framework for generating 3D avatar motion driven by simultaneous multi-modal inputs, such as speech audio, text descriptions, and trajectory data. A novel decoupling mechanism that allows upper-body and lower-body motions to be generated separately and integrated cohesively is proposed, ensuring realistic behavior across modalities. Experiments on a purpose-built benchmark show that MOCO outperforms existing methods on generating synchronized and lifelike avatar motions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. MOCO introduces the first approach to handle simultaneous multi-modal inputs (speech, text, and trajectory) in motion synthesis, overcoming the limitations of sequential or averaged methods used in previous work.\n\n2. The framework\u2019s decoupling strategy, which independently processes each modality and then integrates them according to spatial rules, ensures high alignment and coherence across modalities, leading to realistic and synchronized avatar behavior."
            },
            "weaknesses": {
                "value": "1. Limited Real-World Testing: The experiments primarily focus on controlled benchmarks and do not demonstrate how MOCO performs in real-world or diverse environments, which limits understanding of its practical application.\n\n2. Analysis of Failure Cases: The paper lacks an in-depth analysis of failure modes or specific situations where MOCO might struggle, such as handling overlapping or conflicting input conditions from different modalities.\n\n3. The paper does not include user studies or subjective assessments to gauge the perceived naturalness and quality of the generated motions, which would add practical validation to the quantitative results.\n\n4. Although MOCO excels at combining multiple modalities for coherent motion generation, its joint performance comes at the expense of not achieving the highest scores in any single domain.\n\n5. The paper provides qualitative results but lacks detailed video examples or demonstrations of the generated outputs."
            },
            "questions": {
                "value": "1. While MOCO performs well across joint multi-modal metrics, it does not surpass single-modality baselines in their respective domains. Have you considered methods to optimize modality-specific contributions without compromising joint performance?\n\n2. Have you considered conducting user studies or collecting subjective feedback to assess the perceived naturalness and coherence of MOCO\u2019s generated motions? This would provide additional validation for the results.\n\n3. Could you provide more examples or analysis of specific scenarios where MOCO struggles, such as when input modalities provide contradictory cues or when generating highly dynamic or complex movements?\n\n4. Could you provide video samples to showcase MOCO's performance visually? This would be highly beneficial for evaluating the naturalness and coherence of the synthesized motions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces MOCO, a novel diffusion-based framework for 3D avatar motion synthesis that effectively integrates multi-modal inputs such as text descriptions, speech audio, and trajectory data. By independently modeling each modality using transformer-based denoisers and employing a decoupled denoising process, MOCO generates fluid and coherent motion sequences. The framework splits motion generation into upper-body movements driven by speech and lower-body movements influenced by text, refining each component iteratively. It also incorporates trajectory data to guide global body transitions and manages asynchronous conditions with a timeline-based strategy. Experiments on datasets like HumanML3D, BEATX, and a custom multi-modal benchmark demonstrate that MOCO outperforms baseline models, achieving superior synchronization and coherent motion. The paper suggests potential improvements to address minor limitations such as foot sliding during transitions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality: The paper presents a novel framework that effectively combines multiple modalities for motion synthesis, addressing a significant gap in existing research.\nQuality: The decoupled denoising approach is well-designed, allowing for independent modeling of upper-body and lower-body movements, which enhances the coherence and synchronization of generated motions.\nClarity: The methodology is clearly explained, with detailed descriptions of the transformer-based denoisers and how they interact within the framework.\nSignificance: The ability to synthesize lifelike motion sequences from multi-modal inputs has important implications for virtual avatars, gaming, and animation industries."
            },
            "weaknesses": {
                "value": "Computational Complexity: The paper lacks a discussion on the computational efficiency and scalability of the proposed framework, which is crucial for real-time applications.\nLimited Ablation Studies: While an ablation study is mentioned, more extensive experiments isolating the contributions of each component could strengthen the evaluation.\nDataset Diversity: The datasets used may not cover all possible real-world scenarios, potentially limiting the generalizability of the results.\nMinor Artifacts: The issue of foot sliding during transitions is acknowledged but not thoroughly addressed, leaving room for improvement in motion realism."
            },
            "questions": {
                "value": "How does MOCO perform in terms of computational efficiency, especially in real-time applications?\nCan the authors provide more details on how the decoupled denoising approach affects the overall computational complexity?\nHave the authors considered the integration of additional modalities, such as facial expressions or environmental context?\nWhat strategies could be employed to mitigate the foot sliding issue during motion transitions?\nHow generalizable is MOCO to scenarios beyond the datasets used, particularly in more complex or varied environments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review is needed."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work addresses a very specific problem in 3D human motion\u2014 speech generation, which involves generating both body movement and detailed facial and hand gestures conditioned on text , speech audio and trajectory. Specifically, the proposed method uses four models to achieve (1) text to lower body motion, (2) speech to upper body motion, (3) speech audio to facial and hand details, and (4) trajectory to global velocity, to synthesize actions for walking and talking simultaneously. Overall, the method is clear, and achieves state-of-the-art results in the benchmark made by mixed HumanML3D and BEATX.\n\nHowever, I feel that the work\u2019s novelty might be somewhat limited. This work focuses on a specific application. Although it could be useful for speech generation, it doesn\u2019t seem to demonstrate other areas of extensibility. Methodologically, it seems more like a combination and application of existing models, without considering deeper connection between speech content and body movement."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The problem of speech motion generation is interesting and has application value (although it may seem somewhat overly specific for a machine learning conference).\n\n+ The writing is clear. Each part of the proposed method is reasonable.\n\n+ The experiments are thorough. The benchmark is well-considered, addressing both movement (HumanML3D) and detailed gestures (BEATX). The ablation study is also comprehensive."
            },
            "weaknesses": {
                "value": "- Although the method is reasonable, it still appears to be a combination of different body parts. These combinations sometimes don\u2019t look entirely natural. For instance, around the 23 second in the demo, when the human starts moving, the upper and lower body seem somewhat unnaturally separate. Normally, human upper and lower movements have some interrelated dynamics (based on physical balance or habit), which I didn't see the authors address in the proposed solution.\n\n- The proposed method seems to rely on precise, pre-arranged text and speech audio clips rather than handling this autonomously. Specifically, given a speech content, the method does not help users plan when to say what or when to perform certain actions, which could limit its application scenarios. Perhaps the authors could consider using LLMs to assist with some high-level planning or text-conditioned generation? I believe doing so would not add too much workload but could make the method much fancier.\n\n- There appears to be a lack of semantic-level coherence between the speech content and the motion. In the demo, gestures seem to change with the rhythm of the speech, but the motion itself seems to lack meaning or understanding. Again, have the authors considered using language models to generate movement details that are more closely aligned with the content of the speech?\n\n- This work involves different modality conditions, one of which is audio to motion. However, some important references on \"audio to motion,\" such as dance generation (e.g., Bailando, CVPR 2022, and EDGE, CVPR 2023), seem to be missing."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a diffusion-based framework aimed at generating realistic and coherent 3D avatar motions driven by concurrent multimodal inputs, such as text descriptions and speech audio. The key innovation of MOCO lies in its decoupled motion generation process, which independently produces motions for each modality from input noise and assembles body parts according to predefined spatial rules. This iterative approach enables each modality to refine its contribution within the context of the overall motion, resulting in increasingly natural and fluid movements that achieve coherence and synchronization across modalities.\n\nThe authors tackle the challenge of aligning motions with multiple control signals by proposing a method that simultaneously processes speech audio, text descriptions, and trajectory data, eliminating the need for additional datasets. The MOCO framework is trained on various datasets to ensure it can independently generate motions conditioned on either text or speech inputs. At each denoising step, the model generates distinct motions for the upper and lower body, conditioned on speech and text, respectively, and combines them to produce a cohesive motion that is then diffused for the next iteration. Additionally, the paper introduces a multi-modal benchmark for evaluating the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper introduces a diffusion-based framework for multi-modal controlled motion synthesis, characterized by its innovative approach to decoupling motion generation. \n- This decoupling facilitates the independent processing of speech audio, text descriptions, and trajectory data, effectively addressing a significant gap in the field where aligned multi-modal data is limited.\n- The authors develop a purpose-built multimodal benchmark that enhances the validity of their claims.\n- The paper is well-structured and clearly articulated. The introduction presents a compelling motivation."
            },
            "weaknesses": {
                "value": "**Performance in Single-Modality Scenarios:** The paper does not provide a direct comparison of MOCO's performance when operating in single-modality scenarios (text-only or audio-only) against existing text-to-motion (T2M) and audio-to-gesture (A2G) methods. While MOCO is designed for multi-modal input, discussing its performance in these more constrained scenarios is crucial. \n\n**Conflict Resolution in Multi-Modal Control:** The paper describes a decoupled approach where audio primarily influences upper-body motion and text influences lower-body motion. However, it does not explicitly address how conflicts between modalities are resolved, particularly for the upper body. For instance, if text suggests lifting the left hand while audio cues indicate hand gestures, it is unclear how MOCO would reconcile these instructions. The paper would benefit from a more detailed discussion on conflict resolution strategies and potential limitations this might introduce. Experimental analysis demonstrating how such conflicts are handled and the impact on motion coherence would strengthen the paper's contributions.\n\n**Specificity in Upper Body Motion Control:** While the decoupled approach is innovative, the paper lacks specificity on how upper body motion is controlled when both audio and text are provided. It is not detailed whether the model prioritizes one modality over the other in case of conflict, or if there is a blending strategy that integrates both inputs effectively. Clarifying the control mechanism and its implications on motion realism and coherence is essential for a comprehensive understanding of MOCO's capabilities and limitations.\n\n**Limited visualization results:** Only one demonstration case is provided in the supplementary material. It is recommended to present more generated results as animations to visually assess the generalization ability of the proposed method."
            },
            "questions": {
                "value": "- Could the authors provide a comparison of MOCO's performance in text-only and audio-only scenarios with existing T2M and A2G methods?\n- How does MOCO address situations in which text and audio provide contradictory motion cues for the same body part? Could the authors provide some examples to illustrate this?\n- If users wish to control upper body motions using text, how flexible is MOCO's framework in accommodating such requirements? Can the authors elaborate on the potential modifications needed and any trade-offs involved? How does the framework integrate or prioritize different inputs when there is a mismatch?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}