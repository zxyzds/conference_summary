{
    "id": "SI2hI0frk6",
    "title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model",
    "abstract": "We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data.\nTransfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences.\nWe pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks.\nOur experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens.\nBy introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches.\nWe further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.",
    "keywords": [
        "multimodal foundation model",
        "multimodal generation and understanding",
        "diffusion",
        "next token prediction"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "Transfusion is a recipe for training a multi-modal model over discrete and continuous data.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=SI2hI0frk6",
    "pdf_link": "https://openreview.net/pdf?id=SI2hI0frk6",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a multi-modal transformer model where text tokens are sampled auto regressively and image \"tokens\" are diffused in a VAE latent space."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- problem is well motivated\n- method is simple and clear\n- results are good\n- abalations cover most of my questions"
            },
            "weaknesses": {
                "value": "the method relies on a pre-trained VAE. It would have been nice to see an ablation done in pixel space or with some type of co-training scheme to avoid a 2-stage training process."
            },
            "questions": {
                "value": "The structure of the UNet is unclear to me. the paper says \"since U-Net blocks contain bidirectional attention within, independent of the transformer\u2019s attention mask, this gap is less pronounced when they are applied.\" What U-Net architecture uses bidirectional attention? Is the UNet applied to each patch independently, or is patching done after the U-Net?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Transfusion, a unified model for joint understanding and generation. During training, it incorporates CE loss for text learning and diffusion loss for image learning. In inference, it uses casual attention for autoregressive text generation and bidirectional attention for parallel image generation. Compared to Chameleon, Transfusion attains good performance for visual understanding and generation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper verifies the feasibility combining different learning targets with one framework for respective understanding and generation.\n\n2. It reveals the scaling performance comparing Transfusion and Chameleon, providing good insights for future works.\n\n3. It provides detailed ablation study and analysis to demonstrate the effectiveness of each component."
            },
            "weaknesses": {
                "value": "1. The visual understanding performance of Transfusion is not convincing enough, as the paper doesn't provide any results on related benchmarks, such as MME, MMBench, etc.\n\n2. The image editing performance also needs quantitative results to show its efficacy.\n\n3. Does this framework has the potential to conduct image-text interleaved generation?"
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Transfusion, a novel approach for training a single unified model capable of understanding and generating both discrete and continuous modalities. The authors demonstrate that by combining the language modeling loss function (next token prediction) with diffusion, they can train a transformer model over mixed-modality sequences. This method allows for seamless integration of discrete text tokens and continuous image data, without the need for information loss through quantization."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Originality: The paper introduces a unique approach by combining language modeling and diffusion objectives over shared data and parameters. This method allows for the seamless generation of discrete and continuous modalities.\n\n2. Quality: The paper demonstrates the effectiveness of the Transfusion model through extensive experiments and comparisons with existing methods. \n\n3. Clarity: The paper is well-structured, with clear explanations of the Transfusion method, its architecture, and the training objectives. \n\n4. Significance: The paper addresses a critical challenge in handling both discrete and continuous data modalities."
            },
            "weaknesses": {
                "value": "#### More experiments are encouraged.\n1. Results on multimodal Understanding tasks are limited. For example, VQA, Summarization, VideoQA and etc.\n2. I encourage the authors to scale up the model parameters or training data size (Scaling Law on Transfusion is very important), but I understand the computation resources on scaling law are huge. Thus, this is a minor issue.\n\n#### Compare the technical differences of several recent works (e.g., Show-o, MIO, Emu3)."
            },
            "questions": {
                "value": "As shown in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Transfusion, a novel approach for training multi-modal models that effectively handle both discrete (text) and continuous (image) data. By combining next token prediction for text and diffusion for images within a single transformer architecture, Transfusion eliminates information loss associated with traditional quantization methods. The model was pretrained on a balanced dataset of text and images, utilizing modality-specific encoding and decoding layers to optimize performance. Experimental results demonstrate that Transfusion outperforms existing methods like Chameleon in various benchmarks, achieving significant efficiencies in both text-to-image and image-to-text generation tasks. The study concludes that Transfusion represents a significant advancement in creating integrated multi-modal generative models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Innovative Framework: Transfusion introduces a novel approach that seamlessly combines text and image modalities, achieving impressive results in image generation and editing.\n2. Robust Comparative Experiments: The paper includes comprehensive comparative analyses and detailed ablation studies that examine various dimensions, such as model architecture and parameters, providing strong evidence for the framework's effectiveness.\n3. Clear and Fluid Writing: The writing is clear and the structure of the paper is well-organized, making it easy to follow the authors' arguments and findings."
            },
            "weaknesses": {
                "value": "Lack of demonstration of multi-modal understanding capabilities: While the paper focuses on image generation and editing capabilities, it does not present extensive benchmarks for the model\u2019s general visual question answering (VQA) abilities. It would be beneficial to include benchmarks related to image understanding, such as VQAL, SEED Bench and MMMU."
            },
            "questions": {
                "value": "same as weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores how to bridge the modality gap to inject continuous image generation ability into discrete text-based LLMs by proposing a single transformer model trained with autoregressive modeling and diffusion loss simultaneously. This method of Transfusion avoids image discreteness and still enables input and output-level image tasks. By carefully designing attention masks and input formation, the transformer models text data by AR modeling with causal masking and generates images by denoising with bidirectional attention.\n\nCompared to injecting image comprehension/generation ability by introducing discrete image tokens(Chameleon), the authors demonstrated their approach performs better in various scales in text-to-image generation, image captioning, and even text-only tasks. The large-scale model of Transfusion showed comparable performance to previously published large image generative models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "-They suggested a novel training and inference pipeline for a transformer that allows performing both AR and diffusion modeling with shared parameters.\n\n-The manuscript contains extensive controlled experiments and ablation studies."
            },
            "weaknesses": {
                "value": "-The comparison with previously published works only covered text-only & text-to-image tasks and didn't include their performances on image comprehension tasks(e.g. image captioning). \nAn additional comparison for these tasks with previous multi-modal LLMs that process images differently(e.g. input-only with an external encoder like LLaVA, or image quantization like Chameleon) would support the effectiveness of Transfusion's approach."
            },
            "questions": {
                "value": "-How does Transfusion take the embedding of noise timestep in the training/inference of image latent denoising process?\n\n-In the training phase, the model calculates the AR loss and diffusion loss in one forward pass using text tokens and 'noised' image latent, which introduces unavoidable discrepancy when the model inference with clear latent. Wouldn't it be more accurate to calculate AR and diffusion loss separately in two model passes?\n\n\n-Is there a reason that the COCO FID scores of StableDiffusion models are not in Table 6?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}