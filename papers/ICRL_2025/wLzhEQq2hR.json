{
    "id": "wLzhEQq2hR",
    "title": "Do Vision-Language Models Really Understand Visual Language?",
    "abstract": "Visual language is a system of communication that conveys information through symbols, shapes, and spatial arrangements. Diagrams are a typical example of a visual language depicting complex concepts and their relationships in the form of an image. The symbolic nature of diagrams presents significant challenges for building models capable of understanding them. Yet, recent studies seem to suggest that Large Vision-Language Models (LVLMs) can even tackle complex reasoning tasks involving diagrams. In this paper, we investigate this phenomenon by developing a comprehensive test suite to evaluate the diagram comprehension capability of LVLMs. Our test suite uses a variety of questions focused on concept entities and their relationships over a set of synthetic as well as real diagrams across several domains to evaluate the recognition and reasoning abilities of models. Our evaluation of three LVLMs (GPT-4V, GPT-4o, and Gemini) shows that while these models can accurately identify and reason about entities, their ability to understand relationships is notably limited. Further testing reveals that the decent performance on diagram understanding largely stems from leveraging their background knowledge as shortcuts to identify and reason about the relational information. Thus, we conclude that LVLMs have a limited capability for genuine diagram understanding, and their impressive performance in diagram reasoning is an illusion emanating from other confounding factors, such as the background knowledge in the models.",
    "keywords": [
        "vision-language model",
        "visual language",
        "diagram reasoning",
        "evaluation"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wLzhEQq2hR",
    "pdf_link": "https://openreview.net/pdf?id=wLzhEQq2hR",
    "comments": [
        {
            "title": {
                "value": "Response to Review TikE"
            },
            "comment": {
                "value": "We thank the reviewer for the time! We would kindly point out that there are some misunderstandings about our work, and we would like to respond point by point for elaboration.\n\n>***Point 1**: However, contributions of this paper are not particularly prominent. Is the core contribution the proposed test suite, the evaluation dataset, or the insights gained?*\n\n>**Response**: Our core contribution is not about the test suite or datasets. We mentioned our contribution in the abstract, introduction, as well as contribution sections. For example in the conclusion section, we mention: \n>\n>*We evaluate three LVLMs on diagram understanding, and find that while these models can perfectly recognize and reason about entities depicted in the diagrams, they struggle with recognizing the depicted relations. Furthermore, we demonstrate that the models primarily rely on knowledge shortcuts when answering complex diagram reasoning questions. These results suggest that the apparent successes of LVLMs in diagram reasoning tasks create a misleading impression of their true diagram understanding capabilities.*\n>\n>To do that, we propose a test suite, generating and annotating datasets. These contributions are also helpful for the community and future explorations.\n \n---\n\n>***Point 2**: In fact, some of these insights have already been revealed, such as \u201cmodels consistently perform well on entity-related questions\u201d, \u201cmodels exhibit significant difficulty in identifying relationships\u201d. I suggest that authors review the existing work and clearly highlight the differences between this paper and previous work, including the evaluation methods and insights gained.*\n\n>**Response**: According to our knowledge, our work first obtains these insights for large models on knowledge-rich diagrams. As discussed with Reviewer 6WSg, we would like to include more discussions about other diagrams without rich background knowledge such as charts and graphs in the related work (Appendix C). We would appreciate it if the reviewer could provide more specific references to help us further improve the related work section.  \n\n---\n\n>***Point 3**: Additionally, beyond the overall evaluation results, I would like to see specific results across different domain and topics (as shown in Table 2). This could lead to new discoveries and spark more valuable discussions (involving different knowledge systems).*\n\n>**Response**: Thanks for the suggestion. Fortunately, we have all the performances. The model response as well as test accuracies are in the supplementary file. The reason why we do not include them in the paper is that diagrams in different domains vary quite differently, and the number of diagrams in each domain is a bit small. Thus, the test accuracy in each domain could be biased, and presenting them might need to be more careful. Thus, we only report the overall accuracies to make sure the results are reliable. We will publish all the data and encourage future work for further exploration.\n\n---\n\n>***Point 4**: The Chain of Thought approach mentioned by the authors is relatively simple, as it only involves adding prompts like \u201cthink Step-by-Step.\u201d Are there any more effective ways to use Step-by-Step prompts for this kind of chart? I suggest that the authors explore this in greater depth.*\n\n>**Response**: Regarding the CoT setting, we follow the popular setting proposed by Wei et al., 2022 and Kojima et al., 2023. We agree that proposing a better CoT prompt is interesting but this is not our focus. We would like to encourage future work to do that. \n\n---\n\n>***Point 5**: However, for some simple relational charts, how can we be certain that these large models have not encountered similar charts during training? This point remains uncertain.*\n\n>**Response**: We generate synthetic data on our own. Consider that the generation is purely random. We can reasonably assume that these diagrams are not seen by large models. \n\n---\n\n>***Point 6**: By the way, in this work, beyond the evaluations conducted by the authors, I would like to see a model proposed by the authors specifically for understanding basic relational charts.*\n\n>**Response**: Our work mainly focuses on the evaluation and interpretation side. Besides, we focus on the general diagram instead of specific charts (see section 2.1 for the diagram definition). We agree that exploring the model improvement is interesting but this is not our focus. We would like to encourage future work to do that.\n\n**In summary, we appreciate the reviewer's effort in reviewing our work. But we would like to kindly ask the reviewer to re-evaluate our work after clearing up these misunderstandings.**"
            }
        },
        {
            "title": {
                "value": "Response to Review Y6Hj"
            },
            "comment": {
                "value": "We highly appreciate the reviewer for recognizing our test suite and admiring our findings for the community. Let us try to answer the proposed questions point by point below.\n\n>***Point 1**: The evaluation of the middle figure needs to be revised, as the correct answers are not included in the answer options\u2026*\n\n>**Response**: We select the real example from the dataset as the case study. In fact, we also tried the case with the correct option. The answer is still the same. We may include these results in the main paper or Appendix.\n\n---\n\n>***Point 2**: \u2026To exclude the influence of spatial positionings on the observed hallucinations, I propose to rerun the case study with swapped spatial positions. Is the \"fish\" consistently predicted if swapped with the other entities?*\n\n>**Response**: Indeed, in Appendix E.2.1 (Figure 5), we explore if the position would affect the model\u2019s performance and the answer is yes. We suppose that there is position bias in real diagrams as well. But for your information, we tried several times manually with entities shuffled and the answer is still ``fish\u2019\u2019.\n\n---\n\n>***Question 1**: The main text does not state how the real-world diagram set was curated. Specifically, it would be interesting which filtering criteria were applied. Did you filter diagrams only by their domain? Did you apply a limit for the maximum number of nodes/relations/edge crossing?*\n\n>**Response**: We determine the domains first and manually select all diagrams in that domain for annotation. We filtered out some low-quality (e.g., blurred or over-simple) diagrams. During the question annotation, we also removed several diagrams that are extremely hard to annotate. We will briefly mention this in the paper."
            }
        },
        {
            "title": {
                "value": "Response to Review o1o6"
            },
            "comment": {
                "value": "Thanks for the positive comments! We are grateful that our experiment design is admired by the reviewer. It\u2019s our great pleasure to provide useful and reliable insights for the community. Let\u2019s try to address the proposed concerns point by point below.\n\n>***Point 1**: \u2026I think it is important to test if explicitly including instructions to ignore prior knowledge and solely answer using the information in the diagram improves the performance\u2026*\n\n>**Response**: We appreciate this constructive and delicate suggestion! Indeed, it is intuitive that the model performance drops when there is a contradiction between input content and prior knowledge. But in all our experiments, we ask the model to focus on the provided diagrams (see the system message in Appendix E.1) to answer questions, which can alleviate the contradiction. We will improve our presentation to make this clear in the main paper. \n>\n> Furthermore, we would like to kindly argue that in our scenario,  it is not reasonable to look at the diagram for the entities and give the answer based on prior knowledge (hypothesized model\u2019s manner). Specifically, it\u2019s worth noting that we design questions that can only be answered by relying on diagram images, where the only text input is the question (with answer options). Regarding the example in section 5.2, we ask \"How many food chains are there?\". If the model ignores the diagram, the answer can\u2019t be given even with prior knowledge. If the model gives the answers based on the diagram, the answer is naturally from the diagram instead of the prior knowledge. The question is not about picking from two contradictory options. It is about answering questions by analyzing the input, i.e., the diagram. Thus, we regard the hypothesized model\u2019s manner as unsuitable.\n\n---\n\n>***Point 2**: This paper tested 3 LVLMs. Perhaps testing a few more would be helpful, e.g., Claude 3.5 Sonnet.*\n\n>**Response**: In our experiments, we find that all three models have similar performance manners. Thus, exploring additional models would probably provide similar results. Besides, when we started our experiments, Claude, as well as other LVLMs (e.g., Molmo, LLaMA3.2-Vision), were not available (in many countries). Therefore, we encourage future work on broader explorations of models. \n\n---\n\n>***Point 3**: Even though I appreciate the inclusion of a real-world dataset, it is exclusively focused on science. A broader scope would be better.*\n\n>**Response**: Thanks for the suggestion. As mentioned by Reviewer 6WSg, we will discuss more types of diagrams e.g., charts in related work. Since we focus on diagrams that contain rich background knowledge, we mainly focus on the scope of science. We would like to leave more comprehensive explorations on the diagram scope for future work. \n\n---\n\n>***Question 1**: \u2026I am uneasy with the use of \"primarily\". The difference between KR and KF is at most ~15%. Saying \"primarily\" seems wrong to me\u2026*\n\n>**Response**: Thanks for the suggestion! We will improve the word usage and polish the paper further."
            }
        },
        {
            "title": {
                "value": "Response to Review 6WSg"
            },
            "comment": {
                "value": "We thank the reviewer for recognizing our extensive and holistic experiment design. We explore whether LVLMs really understand visual language by answering two questions with a series of experiments. To provide reliable insights, we include both carefully designed synthetic diagrams and comprehensive real diagrams from multiple domains. We try to address the proposed concerns point by point below.\n\n>***Point 1**: The paper write-up could perhaps be revisited. It is difficult to read Table 6 the first time \u2014 the relative improvement could perhaps be presented more intuitively.*\n\n>**Response**: We will change Tables 6, 7, and 8 to bar charts to better present and emphasize the performance gap. \n\n---\n\n>***Point 2**: The motivation behind using \u2018knowledge as a shortcut' in sections 4 and 5 was not clearly stated\u2026*\n\n>**Response**: Our paper is organized by answering the two questions in the Introduction. Specifically, we use one intuition, five observations, and one finding to connect all experiments. The motivation behind knowledge shortcuts is stated after observation 3 (lines 345-349) as well as at the beginning of section 4 (lines 353-356). We will emphasize these transition contents to make it easier to follow. \n\n---\n\n>***Point 3**: Since \u2018knowledge\u2019 is a quite generic word, it might be useful to define more precisely what kind of knowledge they are referring to early in the paper.*\n\n>**Response**: We will polish our word usage. The \u201cknowledge\u2019\u2019 here refers to the background knowledge (e.g., commonsense knowledge) to understand the diagram.\n\n---\n\n>***Point 4**: There are some relevant works from Chart, Graph, and 3D scene understanding that will be worth mentioning in the related works section...*\n\n>**Response**: Thanks for the references. We will discuss these works and include them in our related work.\n\n---\n\n>***Question 1**: Could it be possible to make the synthetic dataset public so that the reviewers have a better sense of its content?*\n\n>**Response**: We attached all materials in the supplementary, including the code, data, and model responses. The synthetic dataset used in our experiment is also contained. Besides, we provide examples (including diagrams and responses) in the Appendix (Figure 10-33) for each experiment. \n\n---\n\n>***Question 2**: The authors classified the diagram complexity based on the number of entities. Was there a reason they did not consider the number of relationship to measure complexity?*\n\n>**Response**: Thanks for this good question! For real diagrams, the number of entities is often positively proportional to the number of relations. We have the number of entities for all diagrams in our question annotations. Thus, for simplicity, we use the entity number to roughly measure the diagram complexity."
            }
        },
        {
            "summary": {
                "value": "The paper demonstrates an in-depth evaluation if Vision Language models (VLM) can understand visual digram. The authors show the results both on their synthetically generated dataset as well as real visual diagrams curated from real datasets. They curate an extensive list of possible questions to evaluate VLM\u2019s separately on each question category."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The template of questions used in the paper is quite extensive. The authors also carefully evaluated each template separately showing a holistic evaluation framework. Specifically, the key observation noticed under Q1 seems interesting to me. The ability of VLM to understand and reason well about entities while struggling with relationships shows that relationships are hard to decode in general. The performance gap between real and synthetic datasets is also interesting."
            },
            "weaknesses": {
                "value": "1. The paper write-up could perhaps be revisited. It is difficult to read Table 6 the first time \u2014 the relative improvement could perhaps be presented more intuitively.\n2. The motivation behind using \u2018knowledge as a shortcut' in sections 4 and 5 was not clearly stated. Was there a chance for choosing to construct a separate knowledge graph of textual content rather than labeling the visual entities in the original diagram? Providing the rationale behind this construction would be interesting. Since \u2018knowledge\u2019 is a quite generic word, it might be useful to define more precisely what kind of knowledge they are referring to early in the paper. \n\n3. There are some relevant works from Chart, Graph, and 3D scene understanding that will be worth mentioning in the related works section:\n\n        - ChartQA: https://arxiv.org/abs/2203.10244\n\n        - Talk like a graph: Encoding graphs for large language models: https://research.google/blog/talk-like-a-graph-encoding-graphs-for-large-language-models/\n\n        - CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning: https://openaccess.thecvf.com/content_cvpr_2017/html/Johnson_CLEVR_A_Diagnostic_CVPR_2017_paper.html"
            },
            "questions": {
                "value": "1. Could it be possible to make the synthetic dataset public so that the reviewers have a better sense of its content?\n2. The authors classified the diagram complexity based on the number of entities. Was there a reason they did not consider the number of relationship to measure complexity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper evaluates three LVLMs on diagram understanding. The authors curated a synthetic + real diagram dataset and investigated LLM performance on entity and relation recognition. Results suggest that LVLMs may not be truly understanding diagrams."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The writing is very clear. As a reader, I feel that the hypotheses, experiment designs, and results are all clearly conveyed. I especially like how the authors layer experiments based on previous results to provide deeper and deeper insights.\n- Experiments are overall well-designed. The tasks seem reasonable. The combination of synthetic and real diagrams is a strength that allows controllability and real-world validity. I like how the authors pay attention to details like varying the sizes and colors of arrows in the diagrams."
            },
            "weaknesses": {
                "value": "1. Making counterfactual variations of diagrams and asking LVLMs about them is certainly interesting, but I think it is not surprising that this should degrade LVLM performance. When strong prior is present and the diagram contradicts that, the LVLM could simply get confused. In such cases I think it is important to test if explicitly including instructions to ignore prior knowledge and solely answer using the information in the diagram improves the performance. Take Section 5.2 as an example. When no link is present (middle panel), asking \"How many food chains are there?\" sounds more like the goal is to test relevant ecology knowledge instead of diagram reading, and I think the LVLM's decision to hallucinate connections is actually warranted. In other words, this case in particular feels like an unfair trick question to me.\n2. This paper tested 3 LVLMs. Perhaps testing a few more would be helpful, e.g., Claude 3.5 Sonnet.\n3. Even though I appreciate the inclusion of a real-world dataset, it is exclusively focused on science. A broader scope would be better."
            },
            "questions": {
                "value": "- \"Furthermore, we demonstrate that the models primarily rely on knowledge shortcuts when answering complex diagram reasoning questions.\" I am uneasy with the use of \"primarily\". The difference between KR and KF is at most ~15%. Saying \"primarily\" seems wrong to me. Saying knowledge is a shortcut LVLMs exploit seems reasonable, but could the authors justify why they said \"primarily\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The presented work examines to what degree large vision-language models (LVLM) understand diagrams. A test suite is developed that includes diagram-understanding questions for a synthetic and real-world dataset. The questions are distributed into four categories: Every question is either a recognition or reasoning question, and every question is either knowledge-required or knowledge-free. The questions are either applied to entities in the diagram or to relations between them. While LVLMs can recognize and reason about entities in synthetic diagrams, they show poor performance for entities on real-world data. Surprisingly, their performance improves on more complex, real-world diagrams. The authors provide evidence that this performance leap originates from semantic background knowledge that the LVLM bears. In a case study, the authors show that the LVLMs hallucinate some answers due to their semantic background knowledge."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is written well, and the figures help the reader understand the main ideas. \n- The categorization of questions is intuitive and exemplified, and the test suite allows for further research in the direction of diagram understanding. An extensive appendix supports the main text by providing additional information about the curated test suite and the research methodology.\n- The findings are novel and counter-intuitive. The semantic background leakage is well-induced and confirmed by extensive experiments. It highlights a surprising weakness of state-of-the-art LVLMs that should be considered when using them.\n- The findings bear opportunities for future research. Semantic background leakage may also appear in other areas besides diagram understanding. Furthermore, it should be researched how LVLMs can be trained to be more robust and effective in diagram understanding."
            },
            "weaknesses": {
                "value": "The case study (Section 5.2) could be more extensive, and its design includes some shortcomings: \n- The evaluation of the middle figure needs to be revised, as the correct answers are not included in the answer options. I propose to include an additional experiment, possibly in the appendix, with correct answer options. It would be highly interesting to observe if LVLMs even deviate from correct reasoning due to their inherent semantic background knowledge.\n- Another important aspect that needs to be considered in the case study is the spatial positioning of the entities. To exclude the influence of spatial positionings on the observed hallucinations, I propose to rerun the case study with swapped spatial positions. Is the \"fish\" consistently predicted if swapped with the other entities?"
            },
            "questions": {
                "value": "The main text does not state how the real-world diagram set was curated. Specifically, it would be interesting which filtering criteria were applied. Did you filter diagrams only by their domain? Did you apply a limit for the maximum number of nodes/relations/edge crossing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an evaluation framework to assess LVLMs' capability in understanding diagrams. The results show that models like GPT-4V and Gemini rely primarily on their knowledge base rather than reasoning abilities when understanding relationships.\nHowever, contributions of this paper are not particularly prominent. Is the core contribution the proposed test suite, the evaluation dataset, or the insights gained? In fact, some of these insights have already been revealed, such as \u201cmodels consistently perform well on entity-related questions\u201d, \u201cmodels exhibit significant difficulty in identifying relationships\u201d. I suggest that authors review the existing work and clearly highlight the differences between this paper and previous work, including the evaluation methods and insights gained.\nAdditionally, beyond the overall evaluation results, I would like to see specific results across different domain and topics (as shown in Table 2). This could lead to new discoveries and spark more valuable discussions (involving different knowledge systems). \n\nThe Chain of Thought approach mentioned by the authors is relatively simple, as it only involves adding prompts like \u201cthink Step-by-Step.\u201d Are there any more effective ways to use Step-by-Step prompts for this kind of chart? I suggest that the authors explore this in greater depth. In the authors\u2019 experiments, it is mentioned that under certain circumstance, the test of LVLMs do not use any knowledge shortcuts. However, for some simple relational charts, how can we be certain that these large models have not encountered similar charts during training? This point remains uncertain.\n\nBy the way, in this work, beyond the evaluations conducted by the authors, I would like to see a model proposed by the authors specifically for understanding basic relational charts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The overall contribution of the paper lies in the comprehensive nature of the experiments, including a relatively thorough consideration of chart understanding (both explicit and implicit) and other relevant aspects."
            },
            "weaknesses": {
                "value": "However, contributions of this paper are not particularly prominent. Is the core contribution the proposed test suite, the evaluation dataset, or the insights gained? In fact, some of these insights have already been revealed, such as \u201cmodels consistently perform well on entity-related questions\u201d, \u201cmodels exhibit significant difficulty in identifying relationships\u201d. I suggest that authors review the existing work and clearly highlight the differences between this paper and previous work, including the evaluation methods and insights gained."
            },
            "questions": {
                "value": "Additionally, beyond the overall evaluation results, I would like to see specific results across different domain and topics (as shown in Table 2). This could lead to new discoveries and spark more valuable discussions (involving different knowledge systems). \n\nThe Chain of Thought approach mentioned by the authors is relatively simple, as it only involves adding prompts like \u201cthink Step-by-Step.\u201d Are there any more effective ways to use Step-by-Step prompts for this kind of chart? I suggest that the authors explore this in greater depth. In the authors\u2019 experiments, it is mentioned that under certain circumstance, the test of LVLMs do not use any knowledge shortcuts. However, for some simple relational charts, how can we be certain that these large models have not encountered similar charts during training? This point remains uncertain.\n\nBy the way, in this work, beyond the evaluations conducted by the authors, I would like to see a model proposed by the authors specifically for understanding basic relational charts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}