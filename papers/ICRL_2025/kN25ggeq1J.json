{
    "id": "kN25ggeq1J",
    "title": "Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment",
    "abstract": "The reasoning abilities are one of the most enigmatic and captivating aspects of large language models (LLMs). Numerous studies are dedicated to exploring and expanding the boundaries of this reasoning capability. However, tasks that embody both reasoning and recall characteristics are often overlooked. In this paper, we introduce such a novel task, code reasoning, to provide a new perspective for the reasoning abilities of LLMs.\nWe summarize three meta-benchmarks based on established forms of logical reasoning, and instantiate these into eight specific benchmark tasks. Our testing on these benchmarks reveals that LLMs continue to struggle with identifying satisfactory reasoning pathways.\nAdditionally, we present a new pathway exploration pipeline inspired by human intricate problem-solving methods. This \\textbf{R}eflective Hypothesis Decomposition and Amendment (RHDA) pipeline consists of the following iterative steps: (1) Proposing potential hypotheses based on observations and decomposing them; (2) Utilizing tools to validate hypotheses and reflection outcomes; (3) Revising hypothesis in light of observations. Our approach effectively mitigates logical chain collapses arising from forgetting or hallucination issues in multi-step reasoning, resulting in performance gains of up to $3\\times$. Finally, we expanded this pipeline by applying it to simulate complex household tasks in real-world scenarios, specifically in VirtualHome, enhancing the handling of failure cases. We release our code and all of results at \\url{https://anonymous.4open.science/r/code_reasoning}.",
    "keywords": [
        "Code Reasoning",
        "Hypothesis Decomposition",
        "Reflection"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kN25ggeq1J",
    "pdf_link": "https://openreview.net/pdf?id=kN25ggeq1J",
    "comments": [
        {
            "title": {
                "value": "Official Response to Reviewer 773h Part (2/2)"
            },
            "comment": {
                "value": "> what is \"the execution space\"?\n\nThe execution space refers to the set of all possible operations that a program can perform. In this space, each point represents a possible program that implements a particular function.\n\nSpecifically, a neural program synthesis model searches through this function execution space in an attempt to find a program that can satisfy the input-output example requirements.\n\n> what is \"abstract level understanding of function\u2019s behavior\" or \"perform abductive inference\" ?\n\nThe \"abstract level understanding of function's behavior\" refers to the ability of an LLM to comprehend the function's functionality given its output and program, even in situations where execution is not possible. This is assessing whether the LLM is capable of reasonably inferring the cause (input) after observing the result (output).\n\n> Can you provide any empirical evidence to back up the many claims made in the article?\n\nThe core contribution of our paper is the definition of the code reasoning task to explore the boundaries of LLM capabilities. We define three meta-benchmarks and concretize them into eight specific benchmarks. Additionally, we propose a novel method, the Reflective Hypothesis Decomposition and Amendment (RHDA) pipeline. We have conducted over 100 experiments on the state-of-the-art models GPT-4o and Claude3.5 to validate the effectiveness of our method. The concerns raised by Reviewer 773h do not pertain to our core contribution and do not require empirical evidence.\n\n[1] Aryabumi, Viraat, et al. \"To Code, or Not To Code? Exploring Impact of Code in Pre-training.\" *arXiv preprint arXiv:2408.10914* (2024).\n\n[2] Yang, Ke, et al. \"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents.\" *ICLR 2024 Workshop on Large Language Model (LLM) Agents*.\n\n[3] MA, YINGWEI, et al. \"At Which Training Stage Does Code Data Help LLMs Reasoning?.\" *ICLR 2024*\n\n[4] Zhang, Xinlu, et al. \"Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning.\" *arXiv preprint arXiv:2405.20535* (2024).\n\n[5] Yoshua Bengio, \u201cFrom System 1 Deep Learning to System 2 Deep Learning.\u201d https://nips.cc/Conferences/2019/Schedule?showEvent=15488\n\n[6] Yao, Shunyu, et al. \"Tree of thoughts: Deliberate problem solving with large language models.\" *Advances in Neural Information Processing Systems* 36 (2024).\n\n[7] Weston, Jason, and Sainbayar Sukhbaatar. \"Syste m 2 Attention (is something you might need too).\" *arXiv preprint arXiv:2311.11829* (2023).\n\n[8] Strachan, James WA, et al. \"Testing theory of mind in large language models and humans.\" *Nature Human Behaviour* (2024): 1-11.\n\n[9] Shanahan, Murray, Kyle McDonell, and Laria Reynolds. \"Role play with large language models.\" *Nature* 623.7987 (2023): 493-498.\n\n[10] Hutson, Matthew. \u201cHow does ChatGPT 'think'? Psychology and neuroscience crack open AI large language models.\u201d *Nature* vol. 629,8014 (2024): 986-988. doi:10.1038/d41586-024-01314-y\n\n[11] Schaeffer, Rylan, Brando Miranda, and Sanmi Koyejo. \"Are emergent abilities of large language models a mirage?.\" Advances in Neural Information Processing Systems 36 (2024).\n\n[12] Razeghi, Yasaman, et al. \"Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning.\" Findings of the Association for Computational Linguistics: EMNLP 2022. 2022."
            }
        },
        {
            "title": {
                "value": "Official Response to Reviewer 773h Part (1/2)"
            },
            "comment": {
                "value": "We would like to thank Reviewer 773h for the review.\n\nIt seems that the reviewer has misunderstood our work. We understand this and sincerely appreciate the time the Reviewer 773h has invested. The summary and strengths highlighted by the reviewer do not align with the core contributions of our work, and we respectfully disagree with some of the weakness pointed out. We will address each point of misunderstanding from Reviewer 773h and hope that the reviewer will reassess the quality of our paper after we have made our best efforts to clarify our work.\n\nWe would like to clarify that one of the expected contributions of our work is to draw the LLM community\u2019s attention to the task of \u201ccode reasoning.\u201d A substantial amount of research has demonstrated [1, 2, 3, 4] that incorporating code data during the pre-training and fine-tuning stages can improve performance on reasoning tasks beyond just coding tasks. This counterintuitive phenomenon has prompted our thinking, and we believe that utilizing code to enhance LLMs\u2019 reasoning capabilities is a promising approach. LLMs have a strong memorization ability but are not particularly adept at reasoning, so leveraging their ability to \u201crecall\u201d code and \u201creason\u201d with it could be beneficial.\n\nGiven that ICLR is an open, inclusive, and responsible top machine learning conference, we have attempted to offer a unified explanation of LLMs\u2019 \u201crecall\u201d and \u201creasoning\u201d from the perspective of cognitive psychology. This does not imply that our research is unrelated to ICLR. We have proposed an intriguing code reasoning benchmark and a novel method, RHDA, which has achieved high performance and garnered support from Reviewer FKt4 and Reviewer JURK. This is highly aligned with the ICLR theme of \"foundation and frontier models, including LLMs\".\n\n> LLMs perform better on System 1 tasks than on System 2 tasks. ???? Must define System 1 tasks and System 2 tasks for this to make any sense.\n\nSystem 1 and System 2 are concepts derived from cognitive theory and have been widely introduced in the field of deep learning [5, 6, 7]. As depicted in Figure 1 of our original paper, System 1 is a fast, automatic, and intuitive mode used for solving simple tasks, while System 2 is a slow, logical mode used for addressing complex reasoning problems.\n\n>We believe that, similar to humans, there is no significant distinction between memorizing and reasoning tasks for LLM\n>\n>----this is a scientific document NOT a statement of unprovable beliefs.\n\nIn the original text, the phrase \u201cWe believe\u201d is intended to articulate our observations and hypotheses regarding the current capabilities of LLM in handling memory and reasoning tasks, rather than an unprovable personal belief. For example, [8, 9, 10] explore the similarities between humans and LLMs, with [8] specifically emphasizing that \u201cLLMs exhibit behaviour that is consistent with the outputs of mentalistic inference in humans.\u201d These studies provide support for our perspective. Similarly, [11, 12] provide empirical evidence showing that LLMs require the collaboration of reasoning and memorization to perform mathematical reasoning. At last, we would like to clarify that we have already provided substantial empirical evidence for our core contributions, while the assertions cited by Reviewer 773h are not part of our core contribution and therefore do not require empirical evidence in original paper."
            }
        },
        {},
        {
            "summary": {
                "value": "This article presents a dynamic prompting method that automatically adjusts the number of prompting steps based on task complexity and model performance in real time."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The approach taken has significant potential."
            },
            "weaknesses": {
                "value": "This paper is not suitable for a computer-science/ML conference like ICLR. It seems best suited to a cognitive psychology or philosophy conference.\nThe paper starts off in line 42 with \"From the perspective of human cognitive psychology, reasoning can be viewed as a process of memory retrieval,\" and this is the perspective taken. There is insufficient mathematical theory or implementation to show results relevant to ICLR.\n\nThere are several claims made without defintion and/or validation. Examples include:\n\n - LLMs perform better on System 1 tasks than on System 2 tasks.    ???? Must define System 1 tasks and System 2 tasks for this to make any sense.\n\n - l. 52: \"We believe that, similar to humans, there is no significant distinction between memorizing and reasoning tasks for LLM\n\n----this is a scientific document NOT a statement of unprovable beliefs.\n\n - l. 112: what is \"the execution space\"?\n\n - l. 115: what is \"abstract level understanding of function\u2019s behavior\" or \"perform abductive inference\"  ?"
            },
            "questions": {
                "value": "Can you provide any empirical evidence to back up the many claims made in the article?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents \"code reasoning\" as a novel task requiring both reasoning and memory to explore the capabilities and boundaries of LLMs. The authors propose three new meta-benchmarks based on different forms of logical reasoning, namely: inductive code reasoning (predicting program from input-output pairs), deductive code reasoning (predicting output from input-program), and abductive code reasoning (predicting input from output-program). They instantiate these meta-benchmarks into eight concrete benchmarks: List Function, MiniARC, RobustFill, and DeepCoder for inductive reasoning, and CRUXEval and LiveCodeBench for both deductive and abductive reasoning. They propose a novel pipeline, \"RHDA\" (Reflective Hypothesis Decomposition and Amendment), that iteratively decomposes complex problems into simpler steps, verifies the results, and based on this feedback proposes amendments. This pipeline leads to as much as 3\u00d7 performance gains over baseline models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Novel formulation and pipeline:\n    - The paper introduces an intermediate task between reasoning and recall, giving us the ability to understand LLMs more deeply\n    - RHDA pipeline demonstrates robustness in handling complex reasoning tasks\n    \n2. Comprehensive Evaluation: \n    - Three types of meta-framework with 8 specific benchmarks\n    - Evaluation across different model types (GPT-4o and Claude 3.5)\n    - Thorough ablation studies show the importance of each component.\n3. Strong Results:\n    - On inductive reasoning: RHDA outperforms baselines by 18.45% (List Function), 5.89% (MiniARC), 33.31% (RobustFill), and 12.02% (DeepCoder)\n    - On deductive reasoning: Achieves 90.62% on CRUXEval and 84.16% on LiveCodeBench\n    - On abductive reasoning: Achieves 83.75% on CRUXEval and 71.57% on LiveCodeBench\n4. Extensibility: Successfully demonstrated application to VirtualHome environment, showing the framework can handle real-world complex scenarios like household tasks."
            },
            "weaknesses": {
                "value": "1. Limited Theoretical Analysis: \n  - While the paper positions code reasoning between memory and reasoning tasks, it lacks a deeper theoretical analysis of why this intermediate position is beneficial or how it relates to existing theories of reasoning.\n2. Limited Real-World Validation: \n - While they show the program is capable of performing in VirtualHome, having quantitative benchmarks and more complex scenarios would be helpful to understand real-world applicability.\n3. Incomplete Error Analysis: \n- Additional discussion on the failure modes of the RHDA pipeline would be helpful. Analysis of whether failures occur due to incorrect hypothesis formation, ineffective amendment etc is missing.\n4. Complexity Overhead: \n- The paper doesn't analyze the computational cost of running multiple iterations of RHDA compared to single-pass methods like CoT or PoT. This is particularly important given the iterative nature of the approach."
            },
            "questions": {
                "value": "1. Example analysis is said to be shown in Appendix E, but Appendix E is empty. \n2. How sensitive is the method to the quality of the initial decomposition? Are there cases where poor initial decomposition cannot be recovered through amendments?\n3. When and why does the RHDA pipeline fail - are the failures primarily due to poor initial decompositions, ineffective amendments, or do different tasks exhibit different failure patterns?\n4. The computational overhead analysis is missing:\n -  What is the computational cost compared to baseline methods? \n -  What is the trade-off between the number of iterations and performance gains?\n5. While VirtualHome examples demonstrate RHDA's potential for real-world tasks, can you provide quantitative metrics and more complex scenarios (e.g., handling multiple interdependent tasks or recovering from failures) to better evaluate its practical capabilities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper evaluates LLMs on code reasoning tasks, which require both recall and reasoning capabilities. The authors also propose a method RDHA which generally improves their performance in these tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "originality: RDHA is a novel method. It is interesting to see such a method to be applied to code reasoning and even planning tasks (e.g., Virtual Home).\n\nquality: the paper conducts a systematic investigation of code reasoning tasks and an additional planning task, which is a very comprehensive task setting.\n\nclarity: the paper is generally easy to follow.\n\nsignificance: it is important to understand LLMs' capabilities and limitations."
            },
            "weaknesses": {
                "value": "Generally, the contributions of this paper are not strong enough.\n\n1. If the main contribution is the evaluation of LLMs on code reasoning, it is not enough to test only two LLMs. Plus, it is not surprising that LLMs fail in complex reasoning tasks as widely testified in many existing works ([1,2], inter alia). \n\n2. If the strongest contribution is RHDA, I wonder how it compares to other prompting methods such as self-refine [3].\n\n2. Some arguments are a bit loose or controversial. For instance, the authors \"...position the code reasoning task between memory and reasoning\" because it requires memory of syntax from pre-training and reasoning about current question and context. In this sense, most, if not all of the reasoning tasks can be put in the middle of the Figure 1 spectrum, which has one extreme of reasoning and another extreme of recall. For instance, math, which the authors put on the reasoning extreme of the spectrum, requires the LLM to understand what operators mean as well (i.e., syntax, which also needs to come from training memory). The authors are encouraged to be more cautious in statements.\n\n[1] Zhuo, T. Y., Vu, M. C., Chim, J., Hu, H., Yu, W., Widyasari, R., ... & Von Werra, L. (2024). Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877.\n\n[2] Dziri, N., Lu, X., Sclar, M., Li, X. L., Jiang, L., Lin, B. Y., ... & Choi, Y. (2024). Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36.\n\n[3] Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., ... & Clark, P. (2024). Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36."
            },
            "questions": {
                "value": "1. What is the difference between Sub-Hyp and Chain of Thought?\n\n2. I didn't get the rationale of why the IO prompt is the best in terms of accuracy. More specifically, what does it mean when you say this? On page 6, the end of the paragraph preceding the ablation study says \"This may be a misunderstanding... than RHDA.\"?\n\n3. At the bottom of page 7, \"...we present a quantitative...\", did you mean qualitative?\n\n4. Did you conduct a full-scale experiment on Virtual Home? It seems that you only present two examples in the paper, which I'm not sure can generalize to the conclusion that RHDA is scalable and flexible.\n\n5. What is the cost of RHDA? Does it generalize to other non-symbolic domains?\n\n6. How does your method compare to self-refine?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}