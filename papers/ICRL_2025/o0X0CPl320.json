{
    "id": "o0X0CPl320",
    "title": "Jointly Training Task-Specific Encoders and Downstream Models on Heterogeneous Multiplex Graphs",
    "abstract": "Learning representations on Heterogeneous Multiplex Graphs (HMGs) is an active field of study, driven by the need for generating expressive, low-dimensional embeddings to support downstream machine learning tasks. A key component of this process is the design of the graph processing pipeline, which directly impacts the quality of learned representations. Information fusion techniques, which aggregate information across layers of a multiplex graph, have been shown to improve the performance of Graph Neural Network (GNN)-based architectures on various tasks including node classification, link prediction, and graph-level classification. Recent research has explored fusion strategies at different stages of the processing pipeline, leading to graph-, GNN-, embedding-, and prediction-level approaches. In this work, we propose a model extending the GraphSAGE architecture, which simultaneously refines layer-wise embeddings produced by the encoder while training downstream models. We evaluate the model's effectiveness on an HMG derived from a real-world travel dataset, comparing it to models utilizing either graph-level or prediction-level fusion without jointly optimizing their vector embeddings. We demonstrate that our approach enhances the model's performance on downstream tasks, particularly node classification.",
    "keywords": [
        "Multiplex",
        "Heterogeneous",
        "GNN",
        "GraphSAGE"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "Jointly optimizing layer-wise embeddings and downstream machine learning models on heterogeneous multiplex graphs increases predictive performance.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=o0X0CPl320",
    "pdf_link": "https://openreview.net/pdf?id=o0X0CPl320",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a novel method for jointly training task-specific encoders and downstream models on heterogeneous multiple graphs (HMG). The author first defines key concepts such as heterogeneous multiple graphs, inductive learning, and simultaneous encoder-classifier optimization SECO. Subsequently, they designed an encoder called HMGSAGE, which can handle graph data with multiple node and edge types. Building on this foundation, the author further expanded HMGSAGE and constructed a graph autoencoder GAE capable of inductive link prediction and graph structure data node classification. The core contribution of this paper is the introduction of the SECO paradigm, which learns the optimal node embeddings for the downstream task through supervised learning by iteratively and simultaneously updating the encoder and downstream classifier, thereby improving the performance of the downstream task.\nadvantage:"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.This paper proposes a new joint training framework that closely integrates the training process of the encoder and downstream models, providing an effective solution for tasks on heterogeneous multiple graphs.\n\n2.The author clearly defines and explains concepts such as heterogeneous multiple graphs, inductive learning, and SECO, providing a solid theoretical foundation for readers."
            },
            "weaknesses": {
                "value": "1.The writing quality of the entire paper is relatively lacking, with problems such as unclear expression and confusing logic, especially in the introduction section, where there are significant writing issues. The discussion of the recommendation system in the introduction is too long and not effectively connected to the core research content and motivation of the paper, making it difficult for readers to understand the actual significance and motivation of the research.\n\n2. The overall organizational structure of the paper is rather confusing, leading to confusion for readers in understanding. There is a lack of clear logical connection between the various parts, making it difficult to trace.\n\n3. The method proposed in this article is essentially a stack of existing technologies. Specifically, the author uses GraphSAGE as an encoder, stacks it with a joint training method based on SECO, and introduces the GAE model structure for learning. Although these technologies are effective in themselves, they do not demonstrate sufficient innovation and lack new theoretical or practical value.\n\n4. Only one dataset was used for verification in the experimental part, and there was no sufficient comparative experiment. In particular, other methods listed in Table 1 mentioned in the paper were not effectively compared as a baseline for comparison."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a Topology-Driven Residual-Enhanced Heterogeneous Graph Network (TDR-HGN) consisting of node type and one-hot encodings to generate initial features, residual networks to mitigate over-fitting and semantic confusion, and high-order neighbor aggregation through meta-paths to achieve feature enhancement. The main aim is to improve the feature completion in heterogeneous graphs and to prevent embeddings from becoming overly uniform, which leads to loss of semantic information."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The topic is interesting and relevant.\n2. propose a different architecture for heterogeneous graphs along with feature completion\n3. The proposed model demonstrates improvement over selected baselines.\n4. authors provided comprehensive results across multiple tasks, including node classification and clustering, with ablation studies and hyperparameter analysis."
            },
            "weaknesses": {
                "value": "1. While the model outperforms the selected baselines,  the authors should include a wider range of recent and diverse approaches.\n2. Small datasets have been chosen. authors should conduct experiments on larger datasets and provide a discussion on the scalability of the model for very large graphs.\n3. No insights have been provided into how the proposed model performs on computational efficiency/training time (compared to baselines).\n4. One-hot encoding is infeasible for large-scale graphs. How do you plan to handle large graphs?\n5. Sections of the methodology (e.g. the notation and details in Sections 4.1 and 4.2) could be improved for better understanding.\n6. the authors should share the code with pseudocode.\n7. There are numerous typos, and the paper requires proper refinement. For example, in eq. 4, alpha should be written alpha^l.\n8. the proposed architecture is not very well motivated and the novelty of its contributions is somewhat unclear."
            },
            "questions": {
                "value": "See the \"weaknesses\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an** improved** GraphSAGE model that optimizes layer-wise embeddings during encoding while training downstream models. Experiments on a heterogeneous multiplex graph from a travel dataset show that this method outperforms models using only graph-level or prediction-level fusion, particularly for node classification tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper introduces a new method with three graph learning models and a dataset. The models improve performance by maintaining the graph structure early and targeting specific tasks. This leads to better results in link prediction and node classification on complex graphs.\n\n2. The paper has a clear structure. Each section is well-organized, and the logic flows well. Readers can easily follow the research process."
            },
            "weaknesses": {
                "value": "1. The proposed method lacks innovation as it combines existing models without introducing significant new ideas.\n\n2. The encoder employs parallel GraphSAGE layers to sample and aggregate neighbors of nodes, but the paper does not assess the efficiency of this approach, necessitating further experiments.\n\n3. The experiments only compare variations of the proposed model, lacking convincing evidence from comparisons with a broader range of baseline models."
            },
            "questions": {
                "value": "Can the authors provide more detailed information about the computational efficiency of the HMGSAGE encoder? Specifically, how does the use of parallel GraphSAGE layers impact the model\u2019s overall runtime and memory usage?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}