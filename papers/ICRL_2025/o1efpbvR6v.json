{
    "id": "o1efpbvR6v",
    "title": "Application of Metric Transformation in One-Step Retrosynthesis",
    "abstract": "In this article, we investigate the impact of Deep Metric Learning and Transformer architecture on predicting the retrosynthesis of Simplified Molecular Input Line Entry System (SMILES) chemical compounds.\n\nWe demonstrate that combining the Attention mechanism with Proxy Anchor Loss is effective for classification tasks due to its strengths in capturing both local and global contexts and differentiating between various classes. \n\nOur approach, which requires no prior chemical knowledge, achieves promising results on the USPTO-FULL dataset, with accuracies of 53.4\\%, 83.8\\%, 90.6\\%, and 97.5\\% for top-1, top-5, top-10, and top-50 predictions, respectively.\n\nWe further validate the practical application of our approach by correctly predicting the retrosynthesis pathways for 63 out of 100 randomly selected compounds from the ChEMBL database and for 39 out of 60 compounds selected by Bayer's chemists and from PubChem.",
    "keywords": [
        "Retrosynthesis",
        "Chemistry",
        "Deep Metric Learning",
        "Transformer"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "Deep Metric Learning and Transformers for retrosynthesis of chemical compounds.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=o1efpbvR6v",
    "pdf_link": "https://openreview.net/pdf?id=o1efpbvR6v",
    "comments": [
        {
            "title": {
                "value": "Additional response"
            },
            "comment": {
                "value": "Hello reviewer, \n\nYou can check Table 5 in the paper, where I compare the models on the USPTO-50k dataset. However, as stated in the paper, we focus more on the USPTO-Full dataset because our primary goal is multistep retrosynthesis."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for their feedback.\n\n**Regarding the question:**\n\n- Our results fall behind state-of-the-art models such as LocalRetro, OGNN, and RetroExplainer on USPTO-50k. However, we show significant improvement compared to the baseline and the same architecture model, trained without the imbalance-handling methods (proxy anchor loss and subclass mapping) that we proposed.\n\n**Further discussion:**\n\n- We propose a solution to address the dataset's imbalance, aiming to improve classification accuracy while maintaining inference speed\u2014this being our main contribution.\n- The imbalance-handling methods we used are independent of the specific architecture, meaning they can be applied to other state-of-the-art template-based models to enhance their performance on imbalanced datasets, which we plan to explore in future work.\n- We believe the results of the multi-step retrosynthesis tasks are promising, both in terms of speed and the number of solved molecules, even with a significantly smaller dataset. This aligns with our main goal."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for their feedback.\n\n**Regarding the weakness:**\n\n- Our primary goal is to achieve fast inference with strong predictive capacity to support multi-step retrosynthesis. To achieve this, we address the dataset's inherent imbalance to improve classification accuracy while maintaining inference speed, which is our main contribution. Additionally, we reconfirm the effectiveness of Nested Monte Carlo search for multi-step retrosynthesis.\n- In the multi-step retrosynthesis section, we use AiZynthFinder, a template-based synthesis tool. This limited our work to this scenario. Moreover, we did not propose a novel model but rather a solution to tackle the class imbalance, which is a challenge present in many domains.\n\n\n**Regarding the question:**\n\n1. We tackle the imbalance in the dataset to achieve better performance using the same model architecture (in our paper, this is a vanilla attention layers). Due to the independence of the imbalance-handling methods (proxy anchor loss and subclass mapping) from the model architecture, we can apply these methods to other state-of-the-art architectures, which we plan to explore in future work.\n2. The ring-breaking model of AiZynthTrain is inspired by the work of Thakkar et al. (2020), where the model breaks the ring system to simplify the given molecule.\n3. Proxy Anchor loss is used to train the backbone model to distinguish the similarities and differences between samples from the same class or different classes before fine-tuning the model to predict the correct class (template). On the other hand, subclass mapping (a clustering approach) is used to reduce the imbalance between classes, improving performance on small and medium-sized classes. Together, these methods effectively learn the representation of the given molecule and reduce imbalance. As a result, this combination works better on larger datasets compared to smaller ones, as shown in Table 1 (USPTO-full) and Table 5 (USPTO-50k).\n4. We maintain the same ratio as the default to ensure a fair comparison on the multi-step retrosynthesis task. We thus, consider the worst-case scenario, where all these data points are considered failures to predict.\n5. Some models reported in Seidl et al. (2022) use significantly stronger GPUs than ours. Moreover, we wanted to test whether the proposed imbalance-handling methods would slow down inference speed. As shown in the paper, this effect is not significant.\n6. The coverage result is based on the fact that some reaction rules are absent from the training set. In other papers, such as LocalRetro and GLN, coverage (excluding reaction rules in the testing set but not in the training set) is often used as a theoretical upper bound. We propose this measure to show that, even though our model does not surpass state-of-the-art models, it still achieves good performance, even as the number of reaction rules grows.\n7. We also conducted ablation studies:\n   - If we remove the imbalance-handling methods and train our model as usual, the result shows only a slight improvement (about 0.1-0.5%) compared to the baseline, even when stacking the attention layers.\n   - The subclass mapping (clustering approach) works well on large datasets, where there is significant class imbalance.\n   - In Table 5, we show the improvement of our approaches even on small datasets, compare to baseline. However, it falls behind state-of-the-art models, which we find reasonable, as our proposal is purely focused on tackling the imbalance among classes.\n\n**Further discussion:**\n\n- We believe that the results of the multi-step retrosynthesis tasks are good, both in terms of speed and the number of solved molecules, even with a significantly smaller dataset, which aligns with our main goal."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for their constructive feedback.\n\n**Regarding the weakness:**\n- Our primary goal is to achieve fast inference with strong predictive capacity to support multi-step retrosynthesis. To support this, we address the dataset's inherent imbalance to improve classification accuracy while maintaining the inference speed. Additionally, we observed that using the same architecture of our model, but without the imbalance-handling methods (proxy anchor loss and subclass mapping) yielded results nearly identical to those of the less performant AZT model, even when attention layers were stacked.\n\n**Regarding the question:**\n\n- For the k-means clustering step, it took us less than an hour for the entire USPTO-full dataset, which is quite fast, even though we also applied it to the small/medium classes, which were not required for our proposed solution.\n- The optimal number of subclasses depends on the logic of the remap layer that we applied. However, we found that setting the number of subclasses to 6 is a good choice because it reduces the imbalance between classes while still maintaining good performance when remapped back to the original class using the max between its subclasses.\n- In Appendix A.1, we describe some enhancements that improve performance, but we have not yet experimented with other clustering methods.\n- We have only applied our method to the USPTO-full dataset and its subsets. We observed that the more severe the imbalance, the better the performance of our proposed method. However, other datasets are typically private, making it difficult to gain further insight.\n\n**Further discussion:**\n- We believe that the results of the multi-step retrosynthesis tasks are good, both in terms of speed and the number of solved molecules, even with a significantly smaller dataset, which aligns with our main goal.\n- The imbalance-handling methods we used (proxy anchor loss and subclass mapping) are independent of the specific architecture, meaning they can be applied to other state-of-the-art template-based models to enhance their performance on imbalanced datasets, which is something we plan to explore in future work."
            }
        },
        {
            "summary": {
                "value": "The article explores using Deep Metric Learning and Transformer architecture for predicting chemical compound retrosynthesis in SMILES format. It shows that combining Attention mechanisms with Proxy Anchor Loss improves classification by capturing local and global contexts, achieving promising accuracy on the USPTO-FULL dataset without requiring prior chemical knowledge."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The use of Attention mechanisms and Proxy Anchor Loss enhances the performance of classification tasks by capturing both local and global contexts."
            },
            "weaknesses": {
                "value": "The retrosynthesis accuracy is relatively low compared to state-of-the-art models.\nThe model is restricted to template-based approaches, which hinders its applicability and scalability across a broader spectrum of chemical environments."
            },
            "questions": {
                "value": "Why are key methods like RetroExplainer excluded from the comparison\uff1f\nHow do your results compare to the state-of-the-art on the USPTO-50k dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Retrosynthesis is a central challenge in chemistry, crucial for effectively leveraging newly discovered and complex molecules. In this article, the authors present a framework that combines proxy anchor loss with a depth metric Transformer to improve retrosynthesis accuracy in template-based methods, framing the inverse synthesis problem as a classification task. Although the results demonstrate some improvement, the method lacks novelty and does not achieve optimal accuracy."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "None"
            },
            "weaknesses": {
                "value": "1. Retrosynthesis accuracy is low: Compared with state-of-the-art models such as LocalRetro, the retrosynthesis accuracy is low, performing poorly across multiple datasets.\n2. Lacks comparative experiments: There is a lack of comparative experiments to demonstrate the improvement achieved by the applied method, making it difficult to see the advantages of this approach.\n3. Limited diversity and practicality: The model is restricted to template-based methods and cannot be extended to a broader chemical space.\n4. Inconsistent writing format: The manuscript's format presents issues,  such as the abstract, which lacks clear segmentation into distinct sections. This impacts the clarity and structured presentation of key information, making it challenging for readers to discern the main contributions and findings."
            },
            "questions": {
                "value": "1. The comparison lacks certain key methods, such as RetroExplainer and RetroKNN, and the reported results do not achieve state-of-the-art performance on the USPTO-50k dataset.\n\n2. In Appendix A.2, only the limitations of the ring break model are discussed; however, its functional role in the model is not clearly explained.\n\n3. Please clarify the distinction between the proxy anchor method employed and a clustering approach. Additionally, why is the same approach not applicable for segmenting larger subsets?\n\n4. Section 3.1 assumes that the additional 11,567 data points are all failures for both our model and the baseline. Why are some of these not classified as correct predictions, at least in proportion?\n\n5. Why was the inference speed of different models not benchmarked on the same device to ensure fair comparison?\n\n6. While template coverage has improved, the results do not surpass those in previous studies. Could you provide an explanation for this outcome?\n\n7. Are there corresponding ablation studies to validate the contribution of each component in the proposed framework? How do you explain the significant discrepancies between the results in Table 5 and those of the baseline model?\n\n8. Minor formatting issues include a missing period at line 182, an extraneous period at line 278, and an extra space before the period on line 433."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this work, the author addresses template imbalance in inverse synthesis by introducing a hierarchical classification approach. Besides, they use k-means clustering to split large template classes into subclasses and Implement proxy anchor loss for classification. Moreover, subclasses were mapped back to the original template categories."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The research effectively addresses a practical challenge in the field - the imbalance of template distributions in datasets. The approach demonstrates measurable improvements on the USPTO full dataset, offering a practical solution to reduce intra-class variance. The methodology shows progression in handling imbalanced data, with a clear focus on improving classification accuracy through hierarchical structuring."
            },
            "weaknesses": {
                "value": "Despite its contributions, the paper suffers from several limitations. The methodology presentation lacks clarity and precision, requiring interpretation to understand the core concepts. While improvements are shown, the achieved 50% accuracy falls behind the current state-of-the-art performance. The paper also omits comparisons with recent superior methods, raising questions about its relative contribution to the field. Additionally, the overall writing style lacks precision and clarity, making it challenging to fully grasp the technical details without considerable effort."
            },
            "questions": {
                "value": "What is the computational overhead introduced by the k-means clustering step, and how was the optimal number of subclasses determined? \nCould the approach be enhanced by implementing more sophisticated clustering methods beyond k-means? \nHow does this method perform when applied to datasets other than USPTO full?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}