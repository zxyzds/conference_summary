{
    "id": "VnNSkUXejc",
    "title": "Beyond Simple Sum of Delayed Rewards: Non-Markovian Reward Modeling for Reinforcement Learning",
    "abstract": "Reinforcement Learning (RL) empowers agents to acquire various skills by learning from reward signals. Unfortunately, designing high-quality instance-level rewards often demands significant effort. An emerging alternative, RL with delayed reward, focuses on learning from rewards presented periodically, which can be obtained from human evaluators assessing the agent's performance over sequences of behaviors. However, traditional methods in this domain assume the existence of underlying Markovian rewards and that the observed delayed reward is simply the sum of instance-level rewards, both of which often do not align well with real-world scenarios. In this paper, we introduce the problem of RL from Composite Delayed Reward (RLCoDe), which generalizes traditional RL from delayed rewards by eliminating the strong assumption. We suggest that the delayed reward may arise from a more complex structure reflecting the overall contribution of the sequence. To address this problem, we present a framework for modeling composite delayed rewards, using a weighted sum of non-Markovian components to capture the different contributions of individual steps. Building on this framework, we propose Composite Delayed Reward Transformer (CoDeTr), which incorporates a specialized in-sequence attention mechanism to effectively model these contributions. We conduct experiments on challenging locomotion tasks where the agent receives delayed rewards computed from composite functions of observable step rewards. The experimental results indicate that CoDeTr consistently outperforms baseline methods across evaluated metrics. Additionally, we demonstrate that it effectively identifies the most significant time steps within the sequence and accurately predicts rewards that closely reflect the environment feedback.",
    "keywords": [
        "Reinforcement Learning",
        "Delayed Reward"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=VnNSkUXejc",
    "pdf_link": "https://openreview.net/pdf?id=VnNSkUXejc",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes the CoDeTr architecture to address the credit assignment problem in a delayed reward environment. CoDeTr is a Transformer that learns a scalar compound reward and extracts the reward at each step through a in-sequence attention mechanism."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "[S1] The model focuses on scalar reward prediction, simplifying the loss function compared to RBT, which uses additional reward bags and state prediction errors, yet demonstrates superior performance.\n\n[S2] CoDeTr\u2019s design, with whole-sequence Transformer-based reward summation, accommodates non-Markovian rewards and variable step importance, making it potentially more effective in such environments."
            },
            "weaknesses": {
                "value": "[W1 & Q1] Reward and weight validity\n\nWithout additional constraints, it is unclear if $ r $ and $ w $ outputs genuinely reflect rewards and weights. \nSince $ R_{co} $ is a weighted sum of $ r $ and $ w $ outputs, additional experiments could help clarify their interpretive accuracy.\n\n[W2 & Q2] Performance in environments without actual per-step reward\n\nThe experiments are conducted on environments with known per-step rewards, artificially accumulated as composite rewards.\nTesting on complex environments with no per-step reward signals, such as robotics tasks with task-completion as the final reward or RLHF for language models with only a final reward, would enhance the assessment of CoDeTr\u2019s performance and applicability in broader contexts.\n\n[W3 & Q3] Impacts of model expressiveness\n\nIn Figure 4, the best two methods appear to be CoDeTr (proposed) and RBT. They both use a full-sequence Transformer model for credit assignment, while the other methods use a two-layer MLP. The performance of CoDeTr and RBT is also very close.\nThe full-sequence Transformer model takes in the entire sequence as input and is more powerful. However, the MLP only takes in the current Markovian state/action as input and is less expressive."
            },
            "questions": {
                "value": "[Q1-Q3] See Weaknesses\n\n[Q4] Open question: If r and w outputs are not actually predicting rewards and weights, what's the effect on downstream RL learning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies a delayed reward setup, where an RL agent observes only delayed rewards summarizing transition sequences rather than the true Markovian rewards. The main challenge addressed is when these delayed rewards are defined as non-linear transformations of the sequence of true rewards. The authors introduce the CoDeMDP framework to formalize the problem and address it by using a Transformer model, CoDeTr, which decomposes delayed rewards into state-action rewards that can then be used to train any standard RL algorithm. The experiments demonstrate that the proposed solution improves upon existing baselines when the delayed reward is defined as a non-linear function of the true rewards and performs comparably to the baselines when the delayed reward is simply a sum."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-structured, presenting the challenges it addresses and the proposed solution clearly. The experiments section is well-organized and Apenndices are comprehensive."
            },
            "weaknesses": {
                "value": "While the paper is well-structured and tackles an interesting challenge, I have some reservations about the importance of its contributions, mainly due to these two points:\n\n1. The problem of non-Markovian or weighted delayed rewards feels somewhat artificial. Beyond the rather vague examples in the introduction, it\u2019s unclear in what specific tasks or RL environments this problem would naturally occur. Without clearer context, the paper seems to be addressing a problem it has introduced.\n\n2. CoDeTr decomposes delayed rewards into per-transition rewards, but these per-transition rewards are still non-Markovian, and this limitation seems unaddressed.\n\nDue to the first point, the practical relevance of the proposed solutions is unclear, and the second point raises doubts about its theoretical value.\n\nThat said, I would consider recommending acceptance if these concerns are addressed in the authors' response."
            },
            "questions": {
                "value": "1. Why do you sum over all subsequences $\\tau\\subset \\mathcal{T}$ in Eq. (3) ?\n\n2. As I understand, the RL agent is trained in an otherwise standard MDP, with rewards defined as $r_t=w_t(\\psi)\\hat{r}_t(\\psi).$ Is that correct? If so, I believe this should be stated explicitly. Besides, this reward is nan-Markovian; please, comment on that.\n\n3. What would be the effect of using a non-fixed number of delayed steps? This seems more realistic for any potential applications that come to mind.\n\n4. When and how could the proposed method/ideas benefit the researchers and/or practitioners?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on learning reward functions in settings where rewards are delayed.  \nThe authors note that standard methods in this area assume (1) the existence of underlying Makov rewards and (2) delayed rewards are a sum of individual time step rewards. To that end, this paper proposes the Composite Delayed Reward Transformer, which allows for reward models that are non-Markovian and are the weighted sum of individual-level rewards."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well written and easy to follow.\n\n2. Finding which states/actions are significant and should be assigned higher weight is an important problem not only in the delayed reward setting but also in the preference-based RL setting.\n\n3. The addition of the Case Study was interesting and provided further insight into the quality of the proposed algorithm.\n\n4. The authors compared their algorithm against an extensive set of related baselines."
            },
            "weaknesses": {
                "value": "Similarity with previous work:\n\nThis paper is extremely similar to Reinforcement Learning from Bagged Reward [1]. Both algorithms use transformer networks for the instance-level (per time step) reward prediction, where the attention mechanism operates on the sequences corresponding to the rewards. The algorithm's (and the pseudocode's) training process is also very similar.  \nIn addition, both works assume non-Markovian rewards, even though the authors explicitly mention in the text that methods in this domain assume Markovian rewards.\n\nThe authors should clarify how their work differs from [1].\n[1] https://arxiv.org/pdf/2402.03771\n\n\nMissing related work:\n\nThe authors should mention the sub-field of preference-based RL, which can learn non-markovian reward models from sequences of (state, action) pairs, especially because the authors mention human feedback and evaluation in the introduction. \n\n\nUnclear on Experimental Design Choices:\n\n1. The authors decided to use 6 different reward delay lengths but elected not to run experiments in the truly sparse setting, where the agent receives a reward at the end of the episode. This seems surprising because the sparse reward setting is a commonly studied problem. \n\n2. Why did you include these specific composite delayed reward structures, SumSquare, SquareSum, Max?\n\n3. SumSquare is supposed to be the sum of the squared step rewards, but why is the formula the sum (abs(r)*r)? Shouldn\u2019t it be (r**2)? If the reward is negative, then abs(r)*r is not the same as (r**2)? The same issue occurs in the SquareSum. Can the authors explain this?\n\nOverclaiming on experimental results:\n\nThe authors claim their algorithm consistently outperforms baseline methods across different environments (L427). However, reviewing Figure 3, CoDeTr seems comparable to the other baselines. It would be helpful if the authors clearly specified in how many experiments their algorithm outperformed all baselines. In addition, running a statistical analysis (i.e., t-test) on these results would provide further support for any claims."
            },
            "questions": {
                "value": "1. It is unclear how contributions 1 and 2 are different.\n\n2. Definition 1 is the definition of a finite MDP. This should be included in the description.\n\n3. In Definition 1, why was no discount factor included?\n\n4. In Figure 5, are the observed and predicted rewards supposed to be similar?\n\n5. In Figure 5, why doesn\u2019t the observed delayed reward drop when the agent's performance fails?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper is really similar to [1], not only in the algorithm but also the writing itself. \n\nIn particular, the problem formulation in [1], Section 3.2 is very similar to the problem formulation presented in this work, Section 2.3 (Reinforcement Learning from Composite Delayed Reward).\n\nIn addition, Section 4.2, paragraph Causal Transformer for Reward Representation in [1] reads almost identical to Section 3.2, paragraph Instance-Level Reward Prediction from this submitted paper. \n\n[1] https://arxiv.org/pdf/2402.03771"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper generalizes approaches for RL with delayed rewards by relaxing two key constraints: Markovian reward and Simple additivity of stepwise rewards. The proposed architecture, Composite Delayed Reward Transformer (CoDeTr), provides a sequential reward model learning without imposing specific reward structures. Numerical results show that CoDeTr performs promisingly in various delayed reward environments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper is easy to read and follow.\n- Relaxing Markovian reward and simple additivity assumptions gives flexibility to a reward model learning,  possibly widening applicability in multiple cases of RL with delayed rewards. \n- I appreciate the authors writing the reproducibility statement to guarantee their work's reproducibility. Not all papers submitted in this venue explicitly describe their reproducibility."
            },
            "weaknesses": {
                "value": "1. **Discussion on the non-causality of weight calculation in Equation (7) is required.** \n- 1-1. The proposed architecture in Figure 2 uses a causal transformer for query, key, and $\\hat{r}$. However, in equation (4), I do not understand why $w$ incorporates state-action pairs up to $i+n_i-1$, not $t$ as in the case of the query, key, and $\\hat{r}$, which means that the learned instance-level rewards include future information. However, is it valid to assume that the current reward includes future information? \n- 1-2. I recommend using causal mixing in equations (6) and (7) (i.e., use $k_{t'}$ for $t' \\leq t$) and report the result as a part of the experiment section. (In this case, the weight calculation in (7) can adopt averaging over $i \\geq t$ for each $t$, not summing over all $i$).\n- 1-3. Plus, there is no $t$ in equation (7). I guess the authors unintentionally drop a subscript of $t$ by mistake.\n\n2. **Ablation study is required on core components: non-Markovian reward modeling and weighted sum.**\n\n- 2-1. The authors should report results by ablating the weighted sum part (e.g., use uniform weight instead) for both the sum-form and general composite form.\n- 2-2. The authors should report results by ablating non-Markovian reward modeling for both sum-form and general composite form that can be done by (i) ablate causal modeling of query and key, (ii) ablate causal modeling of $\\hat{r}$, and (iii) ablate both. Here \"ablate\" means not using the causal transformer and using MLP stepwise.\n- 2-3. Is there any specific reason why equation (5) uses linear transformation, not a non-linear one? Can non-linear compression improve performance?  \n\n3. **The novelty is not significant compared with a baseline (namely RBT)**\n- One of the proposed algorithm's two key components, modeling non-Markovian reward, appears to mirror the RBT algorithm closely. A main difference is the adaptive weight learning using softmax operation, but RBT also adopts softmax operation for the instance-level reward calculation. The authors should clarify the specific innovations over RBT more explicitly to guarantee their novelty.\n\n\n4. **Readability should be improved.**\n\n- 4-1. The authors should improve the motivation for the delayed reward setting. Is there any concrete example other than the human annotation perspective? \n- 4-2. Section 2.3 should be included in Section 3 since CoDeMDP is the authors' proposed concept.\n- 4-3. Placing the pseudocode in the main body can help readers' understanding for the algorithm.\n\n5. I recommend increasing the number of seeds to more than 3 (at least 5)."
            },
            "questions": {
                "value": "Please see the weaknesses part above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}