{
    "id": "7XNgVPxCiA",
    "title": "Forte : Finding Outliers with Representation Typicality Estimation",
    "abstract": "Generative models can now produce photorealistic synthetic data which is virtually indistinguishable from the real data used to train it. This is a significant evolution over previous models which could produce reasonable facsimiles of the training data, but ones which could be visually distinguished from the training data by human evaluation. Recent work on OOD detection has raised doubts that generative model likelihoods are optimal OOD detectors due to issues involving likelihood misestimation, entropy in the generative process, and typicality. We speculate that generative OOD detectors also failed because their models focused on the pixels rather than the semantic content of the data, leading to failures in near-OOD cases where the pixels may be similar but the information content is significantly different. We hypothesize that estimating typical sets using self-supervised learners leads to better OOD detectors. We introduce a novel approach that leverages representation learning, and informative summary statistics based on manifold estimation, to address all of the aforementioned issues. Our method outperforms other unsupervised approaches and achieves state-of-the art performance on well-established challenging benchmarks, and new synthetic data detection tasks.",
    "keywords": [
        "Generative Models",
        "Out-of-Distribution Detection (OOD)"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=7XNgVPxCiA",
    "pdf_link": "https://openreview.net/pdf?id=7XNgVPxCiA",
    "comments": [
        {
            "comment": {
                "value": "**Dear Reviewer,**\n\nThank you for your thoughtful review and recognition of Forte's strengths, including its performance, flexibility, evaluation, and innovative metrics. We value your feedback and appreciate the opportunity to address your concerns.\n\n### **Addressing Weaknesses:**\n\n1. **Paper Structure:**\n   - **Focus on Novel Contributions:**  \n     We acknowledge that the Introduction focuses heavily on existing literature, which may detract from our novel contributions. In the revised version, we will streamline the background and emphasize Forte's unique aspects early, clearly distinguishing it from prior methods like DoSE.  \n     - **Key Differences from Other Methods:**  \n       - Unlike DoSE, which relies on training complex generative models (e.g., Glow, VAEs), Forte leverages pre-trained self-supervised models, reducing computational overhead and eliminating the need for training.  \n       - Forte introduces per-point summary statistics in feature space, enhancing OOD detection performance with fine-grained data assessments.  \n       - By avoiding reliance on likelihood estimations prone to failure (e.g., DoSE), Forte offers a more robust and scalable solution.  \n\n2. **Complexity in Practical Implementation:**  \n   - **Computational Efficiency:**  \n     - Forte's reliance on pre-trained models ensures efficient feature extraction via parallelizable forward passes. Non-parametric density estimators (e.g., OCSVM, KDE, GMM) are lightweight, with low training and inference times.  \n   - **Simplification:**  \n     - Forte does not require training deep neural networks, unlike methods requiring complex generative models or supervised classifiers. It adapts to data drift without retraining, operating in a zero-shot setting.  \n   - **Flexibility:**  \n     - Our ablation study (Table 3) shows strong performance using a single model like CLIP (99.13% AUROC). Resource-constrained settings can achieve competitive results without needing all three models, while the ensemble provides additional performance benefits.\n\n3. **Insight into Self-Supervised Model Choices:**  \n   - **Rationale for Selection:**  \n     - CLIP, ViT-MSN, and DINOv2 were chosen for their complementary strengths:  \n       - **CLIP** captures semantic relationships through image-text alignment.  \n       - **ViT-MSN** emphasizes local structures via masked self-supervision.  \n       - **DINOv2** learns hierarchical representations through knowledge distillation.  \n   - **Enhancing Robustness:**  \n     - Integrating these models allows Forte to capture diverse data features, improving robustness against challenging OOD cases, including synthetic samples from models like Stable Diffusion.  \n   - **Empirical Support:**  \n     - Ablation studies confirm that combining representations from diverse models outperforms any single model, highlighting their complementary contributions.\n\n---\n\n### **Addressing Questions:**\n\n1. **Computational Demands and Real-Time Applicability:**  \n   - **Efficiency:**  \n     - Feature extraction is efficient, with CLIP processing images at ~30 ms per image. Anomaly detection involves simple computations on low-dimensional metrics, enabling real-time application.  \n   - **Adaptability:**  \n     - For resource-constrained or real-time use cases, a subset of models or optimization can be employed without significant performance loss. Forte's lack of training requirements further enhances its practicality.  \n\n2. **Effectiveness of Multiple Models from a Single Self-Supervised Approach:**  \n   - **Exploring Variations:**  \n     - While multiple models from one self-supervised approach may offer some diversity, combining models with distinct training objectives (e.g., CLIP, ViT-MSN, DINOv2) provides broader feature coverage and enhances robustness.  \n   - **Framework Flexibility:**  \n     - Forte is adaptable, allowing users to select models based on specific requirements and constraints.\n\n---\n\n### **Conclusion and Request for Reconsideration:**\n\nWe believe our planned revisions, emphasizing Forte's core differences from other methods\u2014particularly those relying on generative models like DoSE\u2014will enhance clarity and highlight its practicality and robustness. Forte avoids the computational challenges and limitations of such methods, offering an efficient, effective OOD detection solution.\n\nWe kindly request that you reconsider our paper, and give us a higher score in light of these clarifications and revisions. Your constructive feedback has been valuable in strengthening our work, and we are happy to address any additional questions.\n\nThank you again for your time and thoughtful review.\n\n**Sincerely,**  \nThe Authors"
            }
        },
        {
            "comment": {
                "value": "**8. Additional Clarifications**\n\n*Regarding Figure 1:* The figure is generated using the actual functions and code employed in our experiments, applied to simplified data points for visualization done via matplotlib. It accurately represents the definitions provided for the per-point metrics.\n\n*Regarding Minor Typos and Formatting:* We will carefully proofread the manuscript to correct any minor typos or formatting inconsistencies, such as the commas missing \"in Figure 2 Figure 3 Figure 4,\" \"near-ood\" instead of \"near-OOD,\" and \"Table 1 & 2\" instead of \"Tables 1 & 2.\"\n\n\n**9. Pseudocode for OOD Detection Using Per-Point PRDC Metrics**\n\nHere is an informal pseudocode for your understanding, and can add a more formal version to the appendix. We are happy to make a partial anonymized release of the codebase for Forte, if required for additional clarity during the review process. Post-acceptance, the code will be made public and open source.\n\n**Inputs**:\n\n- **Reference data features**: $\\{ x_j^r \\}_{j=1}^m$\n- **Test data features**: $\\{ x_i^g \\}_{i=1}^n$\n- **Number of nearest neighbors**: $k$\n\n**Outputs**:\n\n- **OOD detection performance metrics**: AUROC, FPR@95\n\n---\n\n**Algorithm Steps**\n\n1. **Feature Extraction**:\n\n   - Use pre-trained models (e.g., CLIP, ViT-MSN, DINOv2) to extract features for both reference and test data.\n     - Reference features: $\\{ x_j^r \\}$\n     - Test features: $\\{ x_i^g \\}$\n\n2. **Compute Nearest Neighbor Distances**:\n   - For each reference feature $x_j^r$, compute $\\mathrm{NND}_k(x_j^r)$: distance to its $k$-th nearest neighbor in $\\{ x_j^r \\}$.\n   - For each test feature $x_i^g$, compute $\\mathrm{NND}_k(x_i^g)$: distance to its $k$-th nearest neighbor in $\\{ x_i^g \\}$.\n3. **Compute Per-Point PRDC Metrics**:\n   - For each test feature $x_i^g$, compute per-point Precision, Recall, Density, and Coverage metrics relative to the reference data.\n     - **Note**: Detailed computations are omitted for brevity. Please check section 3 for exact details.\n\n4. **Assemble Feature Vectors**:\n   - For each test feature $x_i^g$, create a feature vector $\\phi^{(i)}$ consisting of its per-point PRDC metrics.\n\n5. **Prepare Training Data**:\n\n   - Split reference data features $\\{ x_j^r \\}$ into:\n     - **Training set**: for model training.\n     - **Validation set**: for model evaluation.\n\n6. **Compute Per-Point Metrics for Reference Data**:\n\n   - Repeat steps 2 and 3 for the reference training set to obtain per-point metrics $\\{ \\phi_{\\text{ref}}^{(j)} \\}$.\n\n7. **Train Anomaly Detection Models**:\n   - Use the reference per-point metrics $\\{ \\phi_{\\text{ref}}^{(j)} \\}$ to train unsupervised anomaly detection models:\n     - **One-Class SVM (OCSVM)**\n     - **Kernel Density Estimation (KDE)**\n     - **Gaussian Mixture Model (GMM)**\n8. **Evaluate Models on Test Data**:\n   - For each test feature vector $\\phi^{(i)}$:\n     - Compute anomaly scores using the trained models.\n9. **Assign Ground Truth Labels**:\n   - **In-distribution (ID)** samples: label $y^{(i)} = 0$\n   - **Out-of-distribution (OOD)** samples: label $y^{(i)} = 1$\n10. **Compute Evaluation Metrics**:\n    - Calculate performance metrics using the anomaly scores and ground truth labels:\n      - **AUROC**: Area Under the Receiver Operating Characteristic Curve\n      - **FPR@95**: False Positive Rate at 95% True Positive Rate\n---\n**Notes**:\n- The per-point PRDC metrics capture local relationships between test samples and the reference data manifold.\n- Anomaly detection models are trained solely on reference (in-distribution) data metrics to learn the typical data distribution.\n- Evaluation metrics assess the models' ability to distinguish OOD samples based on the per-point metrics.\n---\n**End of Pseudocode**\n\n---\n**Conclusion**\n\nWe are committed to improving the clarity and quality of our paper. We believe that Forte offers significant advancements in OOD detection by introducing novel per-point metrics and eliminating the need for training generative models. Our method provides a scalable and effective solution applicable to various challenging scenarios, including synthetic data detection and medical imaging.\n\nWe hope that our extremely detailed responses address your concerns and clarify the contributions and novelty of our work. We kindly request that you consider our explanations in your evaluation to increase our ratings and are open to any further questions or suggestions you may have.\n\nThank you again for your valuable feedback.\n\nSincerely,\n\nThe Authors"
            }
        },
        {
            "comment": {
                "value": "**4. Explicit Comparison with DoSE:**\n\nConcern: Is the method simply taking DoSE and running it on a new set of statistics? How does it differ from DoSE?\n\nResponse: While our method is inspired by the concept of typicality used in DoSE, Forte introduces significant novel contributions that differentiate it from DoSE. The differences are as follows:\n\n**Core Differences from DoSE:**\n- **Elimination of Generative Model Training:**\n    - DoSE requires training generative models (e.g., Glow, VAEs) on in-distribution data to estimate likelihoods, which is computationally intensive and impractical for large datasets, and access to a small sample of total in-distribution samples due to:\n           1. **Glow models** rely on invertible architectures and exact log-likelihood evaluations, resulting in inefficient computation and high memory requirements.\n           2. **VAEs** suffer from sample inefficiency on complex datasets, leading to poorly structured latent spaces and degraded performance.\n         - **Forte** eliminates generative models, using pre-trained self-supervised models (e.g., CLIP, ViT-MSN, DINOv2) for feature extraction. This reduces computational overhead and simplifies implementation. A forward pass suffices, with no retraining or fine-tuning needed.\n       - **Addressing Likelihood Estimation Challenges:**\n         - Likelihood-based methods can be unreliable in high-dimensional spaces, where OOD samples may have higher likelihoods than in-distribution data (e.g., CIFAR-10 vs. SVHN). DoSE partially addresses this but has limitations.\n         - Forte avoids likelihoods by operating in feature space and using per-point summary statistics to capture local data structures.\n       - **Introduction of Per-Point Metrics:**\n         - DoSE relies on global statistics, which may miss local nuances.\n         - Forte uses per-point statistics\u2014precision, recall, density, coverage\u2014computed in feature space, enabling fine-grained OOD detection by accurately estimating the manifold.\n\n   - **Performance Improvements:**\n     - **Empirical Results:**\n       - On CIFAR-10 (in-distribution) vs. CIFAR-100 (OOD), DoSE achieves an AUROC of **56.90%**, while Forte achieves **97.63% \u00b1 0.15%** (Table 2).\n       - Forte outperforms DoSE and all techniques benchmarked in the DoSE paper across tasks, including challenging scenarios with synthetic data and medical images.\n\n\n**5. Consistency of Density Definition with Figure 1**\n\n*Concern:* Density definition is inconsistent with Figure 1. As defined, it is just a scaled \"recall,\" which would make it useless to model.\n\n*Response:* The density definition is consistent with Figure 1. Figure 1 was generated using the actual functions and code we use in our experiments, applied to simplified 2D data points for illustrative purposes using matplotlib. The density metric measures the average number of reference points within the neighborhood of each test point, normalized by the product of $ k $ (the number of nearest neighbors) and the total number of reference points $ m $. Mathematically, it is defined as:\n\n$\n\\mathrm{density_{pp}^{(i)}} = \\frac{1}{k m} \\sum_{j=1}^m \\textbf{1}\\left( x_j^r \\in B\\left( x_i^g, \\mathrm{NND}_k(x_i^g) \\right) \\right).\n$\n\nThis metric provides an estimate of the local density around each test point, which is crucial for distinguishing between ID and OOD samples. It is not simply a scaled recall but captures different information.\n\n---\n\n**6. Novelty of the Summary Statistics**\n\n*Concern:* The four metrics are not newly proposed in this paper.\n\n*Response:* While the metrics of precision, recall, density, and coverage have been previously used in the context of evaluating generative models (e.g., in \"Reliable Fidelity and Diversity Metrics for Generative Models\" by Naeem et al., 2020), our contribution lies in adapting these metrics as per-point summary statistics for OOD detection.\n\nIn prior work, these metrics are computed as aggregate statistics over entire datasets, primarily to evaluate the performance of generative models in terms of fidelity and diversity. Our novel adaptation involves computing these metrics for individual data points in the feature space, which enables us to capture local anomalies and perform fine-grained OOD detection.\n\n---\n\n**7. Use of Gaussian Mixture Models (GMMs)**\n\n*Concern:* Incorrect claims are made, e.g., GMM is not non-parametric.\n\n*Response:* You are correct; Gaussian Mixture Models (GMMs) are parametric models. In our paper, we did not intend to misclassify GMMs as non-parametric. Our method employs GMMs without making strong assumptions about the underlying distribution because we perform hyperparameter tuning (e.g., varying the number of components) to best fit the data. While GMMs are parametric, our approach is flexible and does not assume a specific distribution a priori.\n\nWe will correct this in the manuscript to accurately describe GMMs as parametric models.\n\n(continued further in next comments)"
            }
        },
        {
            "comment": {
                "value": "Dear Reviewer GdRX,\n\nWe appreciate your thorough review of our paper and the valuable insights you've provided. We are glad that you found our proposed approach effective and that our experiments cover a wide range of scenarios. We would like to address your concerns and clarify some misunderstandings to improve the clarity and impact of our work.\n\n---\n\n**1. LaTeX Formatting and Notation**\n\n*Concern:* The paper is extremely poorly written. None of the math LaTeX in Section 3.2 is well formatted. Subscripts and superscripts are wrong. Variables are used without definition, making it difficult to follow the mathematical descriptions.\n\n*Response:* We apologize if the notation in Section 3.2 was unclear. We want to assure you that the LaTeX formatting and notation are correct. All variables are properly defined according to standard mathematical conventions followed in the literature, such as Naeem et al, 2020.\n\nIn Section 3.2, we introduce per-point metrics using the following notation:\n\n- $\\textbf{1}(\\cdot)$: Indicator function.\n-  $S({x_j^r}{j=1}^m) = \\bigcup{j=1}^m B(x_j^r, \\mathrm{NND}_k(x_j^r))$ : The union of Euclidean balls centered at reference points $x_j^r$ with radius equal to their $ k $-th nearest neighbor distance.\n- $ B(x, r) $: Euclidean ball centered at point $x $ with radius $ r $.\n- $ \\mathrm{NND}_k(x) $: Distance between point $ x $ and its $ k $-th nearest neighbor in the dataset.\n\nWe recognize that explicitly defining $ \\mathrm{NND}_k(x) $ and other variables could enhance clarity. We will add these definitions to ensure that all readers can follow the mathematical derivations seamlessly. For example, we will include a sentence like:\n\n\"Here, $ \\mathrm{NND}_k(x) $ denotes the Euclidean distance from point $ x $ to its $ k $-th nearest neighbor in the dataset.\"\n\n---\n\n**2. Use of Summary Statistics**\n\n*Concern:* No description is given on how the four metrics (precision, recall, density, and coverage) are used. Are they used as the \"summary statistics\" that the proposed method models?\n\n*Response:* Yes, the four per-point metrics are indeed used as the summary statistics in our method. We mention this in Section 3.2:\n\n\"We propose the following per-point summary statistics (precision, recall, density, and coverage) that effectively capture the 'probability distribution of the representations' using reference and unseen test samples in the feature space, enabling more nuanced anomaly detection.\"\n\nThese per-point metrics serve as summary statistics that capture local geometric properties of the data manifold in the feature space. They enable us to model the distribution of in-distribution (ID) data and identify out-of-distribution (OOD) samples effectively. We will emphasize this connection more clearly in the revised manuscript.\n\n---\n\n**3. Definition and Clarity of \"Forte\" Method**\n\n*Concern:* The method, referred to as \"Forte,\" is never truly defined or mentioned in the method section.\n\n*Response:* We apologize for any confusion. The entire Section 3 is dedicated to detailing our proposed method, which we refer to as \"Forte.\" We will make this explicit at the beginning of Section 3 by revising the section title and introduction as follows:\n\n\"**3. Forte: A Framework for OOD Detection Using Per-Point Metrics**\n\nIn this section, we introduce **Forte**, a novel framework that combines diverse representation learning techniques with per-point summary statistics and non-parametric density estimation models to detect out-of-distribution (OOD) and synthetic data.\"\n\nThis will ensure that readers understand that the subsequent subsections describe the components and methodology of Forte.\n\n(Continued further in next comments)"
            }
        },
        {
            "comment": {
                "value": "Dear Reviewer,\n\nThank you for your positive evaluation of our paper. We are pleased you found the problem relevant, the method sound, and the experimental framework thorough. Your feedback is valuable, and we are eager to address your concerns.\n\n**Addressing Weaknesses:**\n\n1. **Comparison with Current SOTA and DoSE:**\n\n   - **Explicit Comparison with DoSE:**\n     - We acknowledge our comparison with DoSE could be more explicit. In the revised manuscript, we will enhance the Introduction and related work sections to clearly state how Forte builds upon and differs from DoSE.\n     - **Core Differences from DoSE:**\n       - **Elimination of Generative Model Training:**\n         - DoSE requires training generative models (e.g., Glow, VAEs) on in-distribution data to estimate likelihoods, which is computationally intensive and impractical for large datasets, and access to a small sample of total in-distribution samples due to:\n           1. **Glow models** rely on invertible architectures and exact log-likelihood evaluations, resulting in inefficient computation and high memory requirements.\n           2. **VAEs** suffer from sample inefficiency on complex datasets, leading to poorly structured latent spaces and degraded performance.\n         - **Forte** eliminates generative models, using pre-trained self-supervised models (e.g., CLIP, ViT-MSN, DINOv2) for feature extraction. This reduces computational overhead and simplifies implementation. A forward pass suffices, with no retraining or fine-tuning needed.\n       - **Addressing Likelihood Estimation Challenges:**\n         - Likelihood-based methods can be unreliable in high-dimensional spaces, where OOD samples may have higher likelihoods than in-distribution data (e.g., CIFAR-10 vs. SVHN). DoSE partially addresses this but has limitations.\n         - Forte avoids likelihoods by operating in feature space and using per-point summary statistics to capture local data structures.\n       - **Introduction of Per-Point Metrics:**\n         - DoSE relies on global statistics, which may miss local nuances.\n         - Forte uses per-point statistics\u2014precision, recall, density, coverage\u2014computed in feature space, enabling fine-grained OOD detection by accurately estimating the manifold.\n\n   - **Performance Improvements:**\n     - **Empirical Results:**\n       - On CIFAR-10 (in-distribution) vs. CIFAR-100 (OOD), DoSE achieves an AUROC of **56.90%**, while Forte achieves **97.63% \u00b1 0.15%** (Table 2).\n       - Forte outperforms DoSE and all techniques benchmarked in the DoSE paper across tasks, including challenging scenarios with synthetic data and medical images.\n\n2. **Clear Statement about the Method and its Relation to DoSE:**\n\n   - **Method Description in the Introduction:**\n     - We will, as mentioned above, revise the Introduction to clarify how Forte builds on DoSE's typicality concept while introducing significant improvements, such as eliminating generative models and using per-point metrics.\n   - **Guiding the Reader:**\n     - Early in the paper, we will provide an overview of Forte, highlighting its core components and differences from DoSE and other methods.\n\n**Additional Enhancements:**\n\n- **Comparison with Other SOTA Methods:**\n  - We will expand the related work section to compare Forte with other SOTA OOD detection methods, highlighting how it addresses their limitations:\n    - **Versus ODIN (Liang et al., 2018):** Requires temperature scaling, input perturbation, and extensive tuning, dependent on NN architecture. Forte surpasses ODIN without these dependencies.\n    - **Versus VIM (Wang et al., 2022):** Relies on class labels and logit matching, limiting its unsupervised applicability. Forte excels without requiring labels or OOD exposure during training.\n    - **Versus NNGuide (Park et al., 2023):** Depends on labeled data and complex training. Forte matches or exceeds performance without these complexities.\n  - Our benchmarking considers these methods and others. Table 1 reports results against the best-performing methods for each task in the OpenOOD v1.5 leaderboard.\n\n- **Clarifications:**\n  - We will ensure the methodology section title explicitly states it describes Forte, guiding readers effectively.\n\n**Conclusion and Request for Consideration:**\n\nWe appreciate your constructive feedback, which will improve the paper's clarity and impact. By addressing your concerns, emphasizing Forte's core differences from DoSE, and comparing it with other SOTA methods, we aim to provide a comprehensive presentation.\n\nGiven Forte\u2019s significant advancements in performance, scalability, and practicality, we kindly ask you to consider our clarifications and enhancements in your evaluation and increase our score. Please let us know if further clarifications are needed.\n\nThank you again for your positive review and valuable suggestions.\n\nSincerely,  \nThe Authors"
            }
        },
        {
            "title": {
                "value": "Note of thanks"
            },
            "comment": {
                "value": "We are truly grateful for your detailed evaluation and positive feedback. We appreciate your recognition of our paper's clarity, implementability, and the comprehensive supporting materials we provided in the appendix. Your suggestion for conference highlighting is very much appreciated."
            }
        },
        {
            "summary": {
                "value": "The authors present a methodology, Forte, which enables the identification of outliers using a rigorous unsupervised algorithm.  The algorithm relies on establishing metrics to identify which samples are in-distribution vs out-of-distribution.  The method is very general, and applicable to many models.  The authors use SVM, KDE, and GMM models with their method, and show that it is capable of distinguishing between real and synthetic data."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The paper is very well written, easy to follow, and highly implementable.  An extensive appendix provides supporting information and data."
            },
            "weaknesses": {
                "value": "None"
            },
            "questions": {
                "value": "None.  The appendix cleared them up"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 10
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates OOD by modeling distributions in feature space. The paper is poorly written, but my guess is that the proposed method is a variant of DoSE, using models _per-sample metrics_ of the feature space, rather than the feature vectors themselves. Four metrics, including precision and recall, are used. Experiments are performed on real images from distinct classes, synthetic vs. real images, and medical images. Results indicate that the proposed method is effective."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "[The following is based on my guess of the proposed method, which is not well described in paper.]\n\n+ Proposed approach is a simple change over feature-space OOD methods, and appears effective.\n+ Experiments seems cover a wide range of scenarios"
            },
            "weaknesses": {
                "value": "+ The paper is extremely poorly written. I list some major issues here.\n    1. None of the math latex in section 3.2 is well formatted. Subscripts and superscripts are wrong.\n    2. Variables are used without definition, e.g., $\\text{nearest}_k$ in section 3.2. Is it different from the $k$ below?\n    3. No description is given on how the four metrics are used. Are they used as the \"summary statistics\" that the proposed method models?\n    4. The method, referred to as \"Forte\", is never truly defined or mentioned in the method section 3.\n    \n+ If my understanding of the method is correct, is it simple just taking DoSE and run it on the new set of statistics?\n+ Incorrect claims. E.g., GMM is not non-parametric, and I don't think that the four metrics are newly proposed in this paper.\n+ Density definition is inconsistent with Fig. 1. As defined, it is just a scaled \"recall\", which would make it useless to model."
            },
            "questions": {
                "value": "1. My main question is how the proposed method really works. If the authors could provide a pseudocode of the algorithm, and how it differs with DoSE, it would be great.\n2. See above weaknesses.\n3. Overall poor writing and latex formatting, in addition to the issues listed above. E.g., \"in Figure 2 FIgure 3 Figure 4\", \n3. Minor typos: \"near-ood\" -> \"near-OOD\", \"Table 1 & 2\" -> \"Tables 1 & 2\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Forte, a novel out-of-distribution (OOD) detection framework that leverages self-supervised learners. Forte enhances detection by combining representation learning methods (e.g., CLIP, ViT-MSN, and DINOv2) with non-parametric density estimators (OCSVM, KDE, GMM) to model the typicality of input samples. The proposed framework emphasizes detecting atypical samples through summary statistics (precision, recall, density, and coverage) to analyze representation distributions. Forte\u2019s performance was evaluated on synthetic datasets generated by Stable Diffusion and various medical image datasets, showcasing its advantages over existing supervised and unsupervised methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- **Performance**: Forte demonstrates superior OOD detection performance compared to state-of-the-art methods across multiple benchmarks, including synthetic data and medical image datasets, which often present significant OOD detection challenges.\n- **Flexibility**: Forte\u2019s unsupervised nature eliminates the need for labeled data or pre-exposure to OOD samples, making it adaptable to various tasks and practical for real-world applications where OOD examples may not be available during training.\n- **Comprehensive Evaluation**: Forte is rigorously tested on both synthetic and medical datasets, demonstrating the framework\u2019s versatility and robustness across vastly different domains.\n- **Insightful Metrics**: The use of novel per-point summary statistics (e.g., precision, recall, density, and coverage) contributes valuable insight into data distribution, enhancing OOD detection beyond standard density-based methods."
            },
            "weaknesses": {
                "value": "1. **Paper Structure**: The paper allocates a substantial portion of its Introduction to reviewing existing OOD detection literature and explaining the typicality concept. This approach detracts from an immediate focus on the novel contributions and design of Forte, which may hinder reader engagement and understanding of the primary contributions.\n2. **Complexity in Practical Implementation**: The integration of multiple representation learning techniques, combined with non-parametric density estimators, may lead to a higher computational overhead and increased complexity in practical deployment. The paper lacks an illustration figure to clearly explain the proposed framework how to integrate the representations from diverse models.\n3. **Insight.**: This work integrates representations from diverse models empirically. It lacks insight into the choice of self-supervised models, such as whether any specific attributes of CLIP, ViT-MSN, or DINOv2 contribute uniquely to Forte\u2019s robustness."
            },
            "questions": {
                "value": "Could the computational demands of Forte\u2019s ensemble approach limit its applicability in real-time OOD detection scenarios?\n\nWould Forte\u2019s effectiveness in OOD detection promote if one apply multiple models by one self-supervised approach in this framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method for identifying OOD and synthetic data created using generative models. The definition of OOD changes with the advent of foundation models that can generate very plausible data. However, even when this data seems real, there can still be distribution shifts that are difficult to detect. In this paper, the concept of typicality is used to assess OOD. However, computing typicality is challenging. This paper proposes Forte, a method for computing typicality, and they evaluate it and compare it with other methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is clear to understand. The problem is relevant. The method is sound. The experimental framework is correct and complete."
            },
            "weaknesses": {
                "value": "In the related work section I miss some comparison of the current SOTA with Forte.\n\nThis method is based on DoSE. I miss a clear statement in the intro about that and the changes over DoSE with an intuition of why. Also, it would be beneficial to clearly state the method to guide the reader on what will come."
            },
            "questions": {
                "value": "I miss some comparison with DoSE in some table for completeness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}