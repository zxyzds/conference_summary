{
    "id": "AsAy7CROLs",
    "title": "Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors",
    "abstract": "In recent years, there has been a significant growth in research focusing on minimum $\\ell_2$ norm (ridgeless) interpolation least squares estimators. However, the majority of these analyses have been limited to an unrealistic regression error structure, assuming independent and identically distributed errors with zero mean and common variance. In this paper, we explore prediction risk as well as estimation risk under more general regression error assumptions, highlighting the benefits of overparameterization in a more realistic setting that allows for clustered or serial dependence. Notably, we establish that the estimation difficulties associated with the variance components of both risks can be summarized through the trace of the variance-covariance matrix of the regression errors. Our findings suggest that the benefits of overparameterization can extend to time series, panel and grouped data.",
    "keywords": [
        "minimum norm solution",
        "ridgeless estimator",
        "benign overfitting",
        "double descent",
        "overparameterization"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=AsAy7CROLs",
    "pdf_link": "https://openreview.net/pdf?id=AsAy7CROLs",
    "comments": [
        {
            "summary": {
                "value": "In this paper, the authors investigate prediction risk and  estimation risk under more general regression error assumptions beyond i.i.d. errors. In particlar, they explore the benefits of overparameterization in a more realistic setting, which allows for clustered or serial dependence. This paper  demonstrated   that the estimation difficulties associated with the variance components  can be summarized through the trace of the variance-covariance matrix of the regression errors."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is written clearly.  I appreciate the significant contribution to high-dimensial data analysis.  The new approach is  promising for \n more  broad framework. This paper attacked the very challenging problem involved in least-squares estimators beyond the  assumiption with  i.i.d.  errors. The new idea about  benefits of  over-parameterization could be extended  to time series, panel and grouped data, etc.  with the broad impact."
            },
            "weaknesses": {
                "value": "The paper proved several  good theoretical results and properties. The experiemts of data set may not be comprehensive, Since there is no comparison of  the  proposed method with existing methods.   In addition,  the proposed approach only works for  the case  p>n. In practice,    the ultra-high dimensional case with p=exp(n) is very  common.  This is a more realistic setting in the big data era."
            },
            "questions": {
                "value": "I have several comments and suggestions for the authors to address.\n\n\n1. The paper is not complete. Please add the conclusion section in the revision. \n\n2. It is worthwhile to extend the proposed approach from the case  p>n   to the ultra-high dimensional case with p=exp(n). \n\n3. It is of interest for the authors to  compare   proposed method with existing ones  (including  Chinot et al. (2022) and Chinot & Lerasle (2023), etc.)  in the experiments.\n\n4. The new idea about benefits of over-parameterization could be extended to time series, panel and grouped data.  It is helpful  for the authors to  elaborate it by proving more details.\n\n5. There are some  typos, grammatical errors,  etc. in the paper. Please check it in the revision carefully. \n\nin page 1, line 022, \"can extend\" -> \"can be extended\". \n\nin page 5, line 312, \"appendix\" -> \"Appendix\".\n\nin page 9, line 432, \"appendix\" -> \"Appendix\".\n\nin page 10, line 500, \"appendix\" -> \"Appendix\".\n\nin page 14, line 704, \"Figure\" -> \"Figures\".\n\nin page 17, line 905, \"6\" -> \"(6)\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper extends the analysis of ridgeless least squares estimators to more\nrealistic error structures beyond the traditional i.i.d. assumptions. It\naddresses both prediction and estimation risks in settings where regression\nerrors may be correlated or exhibit heteroscedasticity.\n\nThe authors provide exact finite-sample expressions for both types of risk,\nwhich are decomposed into bias and variance components. The variance in\nprediction risk is shown to be the product of two distinct terms: one related to\nthe error covariance matrix and the other dependent on the feature distribution.\n\nThe paper also conducts a systematic asymptotic analysis of prediction and\nestimation risks. Numerical experiments with autoregressive and clustered\nregression errors illustrate the theoretical findings."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality:\n\nThis paper extends the analysis of ridgeless least squares estimators by\nrelaxing the i.i.d. assumption on the regression errors. Correlated and\nheteroscedastic errors are frequently encountered in practice but are not\ninvestigated in prior works in the high-dimensional setting. This work fills a\ngap in understanding the performance of least squares estimators, extending the\nobservation of double descent to more general settings.\n\nQuality:\n\nThe paper is technically sound and comprehensive in its treatment of both\nprediction and estimation risks. The finite-sample and asymptotic behaviors\nfinite-sample results are rigorously presented. The numerical result in Section\n1 demonstrates the motivation of the generalized assumptions and the results in\nSection 3.3 help validate the theoretical findings.\n\nClarity:\n\nThe paper is overall well written with clear presentation.\n\nSignificance\n\nThe paper extends the theory of least squares estimators to include the\nsituation with non i.i.d. errors in the context of overparameterization. It fill\na gap in the literature, so the results deserve to be documented."
            },
            "weaknesses": {
                "value": "For the high-dimensional setting considered in the paper, ridgeless least\nsquares estimators are seldom applied in practice. Regularization is almost\nalways helpful, and there is a large body of literature demonstrating the\nadvantages of regularized least squares. The paper should at least discuss this\nline of research. In particular, it has been shown that optimized ridge\nregression avoids bias inflation. Why, then, would practitioners care about\nridgeless least squares estimators in this scenario?\n\nWhile the assumptions are more general than the i.i.d. case, the scenario\nconsidered is still much simpler than what is encountered with real, practical\ndata. For example, the assumption of left-spherical symmetry for the design\nmatrix is limiting in practical scenarios. Most real-world datasets feature\nasymmetrical or skewed distributions, and many variables in real data are\nactually categorical, rather than numerical. It would help to add discussions on\nthe limitations of the current investigation in these contexts.\n\nMost of the theoretical results, especially the finite-sample results, appear\nsimilar to the low-dimensional or fixed-$p$ case. It would be helpful to include\ndiscussions on this point. If the results are not the same, do they reduce to\nthe low-dimensional results? Even if they have the same expression, it would be\ninsightful to explain why it is nontrivial to derive these results in the\nhigh-dimensional setting."
            },
            "questions": {
                "value": "1. Include discussions on relevant regularized least squares estimators, and\n   provide scenarios where the ridgeless least squares estimator excels or\n   underperforms.\n2. Provide comparative experiments with other regularization techniques to\n   demonstrate where ridgeless least squares excels or underperforms.\n3. Add discussions on the limitations of the current investigation, particularly\n   in terms of the assumptions.\n4. Provide discussions connecting the results in the high-dimensional setting to\n   those in the low-dimensional setting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the prediction and estimation risks of the ridgeless least square regression under a more general assumption on the noise. Specifically, consider the data\n$$\ny_i = x_i^\\top\\beta + \\epsilon_i,\\ i=1,...,n\n$$\nwith target $\\beta\\in\\mathbb{R}^p$ and $ \\epsilon_i$ is the noise independent of $x$ with $\\mathbb{E}[\\epsilon]=0$ and $\\mathbb{E}[\\epsilon\\epsilon^\\top]=\\Omega$ finite and positive definite. This includes the case of i.i.d. Gaussian noise, autoregressive noise and cluster noise.\n\nThis paper applies the classical bias-variance decomposition onto the risks and finds a closed form expression for the variance term for both risks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This paper provides rigorous proof and experiments to validate their claim. The assumption on the noise and the input is more general than previous works."
            },
            "weaknesses": {
                "value": "However, my biggest concern is the significance of the contribution provided by this paper. \n\nAs mentioned in line 132-133, this paper is not the first to consider non-i.i.d. Gaussian noise. Although this paper requires less assumptions on the noise than [1], it does not contain enough illustrations or interpretations on their main results (Theorem 3.4, 3.5) for \"allowing potentially adversarial errors\" as promised in line 142-143. Indeed, this paper does not explain how their main results could recover previous result with i.i.d. Gaussian noise or gain new insights with more general noise.\n\nAlso, it seems that the techniques used in the main results are rather standard and can be extended easily to more general settings like kernel ridge regression or kernel gradient flow, which could potentially increase the significance of this work.\n\n\nReference:\n[1] Geoffrey Chinot and Matthieu Lerasle. On the robustness of the minimum \u21132 interpolator.\nBernoulli, 2023."
            },
            "questions": {
                "value": "I believe this paper could improve its significance if it can answer the following questions:\n\n1. What new insights could one gain from Theorem 3.4, 3.5?\n\n2. Related to Q1, the true noise covariance $\\Omega$ is not observable in reality. Could the authors provide any examples or algorithms to approximate such noise covariance in real-world datasets? If it is impractical to approximate it, how could we still analyse the risk with your risk expression in $\\Omega$? \n\n3. Could the authors extend their results to more general settings like kernel ridge regression or kernel gradient flow? Or at least explain what are the technical difficulties that might hinder the extension?\n\n4. One Central idea of regularization is to balance out the effect of the noise in the labels. With the ridgeless linear regression setting, this paper unfortunately misses the opportunity to discuss the important interplay between the non-i.i.d. noise and regularization, which I find quite disappointing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}