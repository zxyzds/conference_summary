{
    "id": "M5LGyR71yS",
    "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources",
    "abstract": "Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage.  In this paper, we propose Source2Synth, a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources.\nSource2Synth improves the dataset quality by discarding low-quality generations based on their answerability.\nWe demonstrate the generality of this approach by applying it to two challenging domains:  we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA).\nOur method improves  performance by 25.51\\% for TQA on WikiSQL and 22.57\\% for MHQA on HotPotQA compared to the fine-tuned baselines.",
    "keywords": [
        "large language models",
        "llms",
        "synthetic-generation",
        "dataset-generation",
        "real-world data"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We proposed a new method for generating and curating high-quality synthetic data grounded in real data sources that can be used to produce high-quality samples for finetuning LLMs on challenging tasks like reasoning and tool-use.",
    "creation_date": "2024-09-17",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=M5LGyR71yS",
    "pdf_link": "https://openreview.net/pdf?id=M5LGyR71yS",
    "comments": [
        {
            "summary": {
                "value": "This works presents an end-to-end pipeline to generate synthetic training data based on some data source. The pipeline consists of three steps, namely data generation, curation, and finetuning. The data generation step selects the topic of the example based on a random crop of the data source, and then goes on to generate an example with instruction, query, reasoning chain, and the final answer. The curation step filters and modifies the generate examples by dividing the synthetic data into two portions, one used to train an intermediate model to judge and filter data from the other half. The model is then finetuned with the curated data.\n\nEvaluation on HotpotQA, WikiSQL, and Tabular QA show that the pipeline can significantly improve model performance in the target tasks, even competitive with finetuned baselines with supervised data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "\u2022\u00a0It is novel and interesting to use model train on part of the data to filter generated data in the other half\n\n\u2022 The proposed pipeline brings significant improvement in performance, even competitive with finetuned baselines."
            },
            "weaknesses": {
                "value": "\u2022 The paper could use much clearer presentation (grammar, wording, formatting, etc...)\n\n\u2022 Consider discussing how the proposed pipeline avoids data leak (does it?) during the data generation step, esp. given that HotpotQA is also constructed from Wikipedia articles.\n\n\u2022 For Tables 1 and 2, there is no comparison between LLMSynth (Synthetic dataset only) and LLMCurated (Synthetic dataset only). Adding such comparisons can further consolidate the importance of the curation step.\n\n\u2022 The scope of this method is limited because it requires knowing the target task itself before data generation. I.e., at its current form it cannot be generalized to produce, say, instruction-tuning data to train LLMs."
            },
            "questions": {
                "value": "Suggestions:\n\n\u2022 249-250: \"... (see C for more details).\" Consider writing \"Appendix C\".\n\n\u2022 If I understand correctly, during data curation one slice is used to train an intermediate model, and this model is used to filter examples from the other slice. What I don't understand is: why not do the reverse after that? Training on slice 2, filter examples from slice 1. As a result more generated examples will be kept -- analogous to cross-validation.\n\n\u2022 HotPotQA --> HotpotQA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an idea for generating synthetic datasets similar to the target task given a raw unstructured source as context. The generation consists of using a seed and data source to generate questions in multiple stages and combine the initial and final questions, to generate a more complex compositional query e.g. a multi-hop question or SQL query. They also propose a method for curating the synthesized data by filtering and imputation to identify higher quality examples. Prior to filtering they tune a model on the uncurated dataset (LLMSynth) to help with filtering. Their filtering involves taking k samples from the model, and if the model is unable to predict the answer in the k-samples then it\u2019s discarded. With imputation they blank out parts of the intermediate questions and regenerate them with the other queries (this is only applied in the context of multi-hop questions, and not SQL).\n\nThey apply their approach on HotPotQA and Tabular question answering via SQL, and evaluate settings that tune a base LLM on target data specific to the dataset as well as their curated dataset and compare improvements in the performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The paper proposes a nice idea to automatically generate compositional queries. They also propose techniques to automatically filter them using an LLM tuned on the synthesized data.\n\n* The idea is described neatly and baseline performances are evaluated."
            },
            "weaknesses": {
                "value": "1. The experimental evaluation steps are lacking clarity. A lot of important details are missing which significantly affects the quality of the work. Please see questions Q1-Q6.\n2. The method is applied somewhat in a limited context on two problems, but it\u2019s not clear how many source examples they generated the synthetic examples on. It sort of seems much more limited than the size of the original dataset e.g. HotPotQA already has 113k questions, but does not seem like the synthesized dataset matches a similar number of wikipedia articles, similarly with the tabular QA dataset,\n3. The experiments and the datasets do not quite necessitate the methods at least in terms of the size of the existing training set. And the experiment setup doesn\u2019t seem to include a comparison to how the models perform with the full original training set available with the datasets, especially since they do fine tuning.\n4. It is also unclear if the\n\nI really do like the ideas but there\u2019s some considerable mismatch in the application for these datasets. Maybe it is worth going after more challenging datasets which are smaller in size in terms of available data for training."
            },
            "questions": {
                "value": "Q1. How do you measure the quality of the LLMSynth model that does the filtering? Does the filtering step that uses k samples from the model output make the final dataset too easy e.g. by dropping valid but hard questions?\n\nQ2. How many questions did you construct using the Source2Synth pipeline for MHQA? How many wikipedia articles were used in the input? How many questions were generated in each stage? Articles to initial set then portion that went to tuning LLMSynth for filtering, and then how many questions remained after filtering? How much is this relative to the HotPotQA dataset? (which already has 113k questions).\n\nQ3. Similarly for TQA, did you use only the tables in the WikiSQL dataset? What is the number of tables? How many examples were generated at each stage?\nIn lines 385-386 what do you mean by slice? Why generate more questions on 2 slices (16k questions) but use only (8k) questions generated on 1 slice? Also if only 27% questions are retained after curation, it goes back to question-1, is it possible that your filtering is dropping some good harder questions that would have been able to improve the model further?\n\nQ4. Do you ensure that the synthesized questions do not overlap with existing validation and test questions in MHQA and TQA? If so, how do you do it? If not, is there a possibility of leakage?\n\nQ5. In Table 1.Evaluation of Source2Synth on MHQA, is there a comparison with a LLaMA-70B model finetuned on the entire train split of HotPotQA as opposed to just the 500 questions? Do I understand your setup correctly that all rows in the table (except for base LLaMA) are tuned with just 500 questions from HotPotQA and/or 1250 Questions generated from the Source2Synth pipeline?\n\nQ6. LLMCurated is not described in Sec 5.1 and Sec 6.1 clearly. Is LLMCurated tuned on uncurated synthetic examples + curated synthetic examples + 500 HotPotQA examples?\nFrom subsequent sections it appears like the 1250 is the number of uncurated synthetic examples, what is the number of curated synthetic examples then? What if you tuned only the curated examples and not the uncurated set.\n\nQ7. Similar questions with TQA, Is LLMSynth trained on 8k (or 16k) uncurated examples for TQA? Is LLMCurated trained only on the 2160 examples after curation?\n\nQ8. Clarification: I assume the imputation step is only applied in the case of HotPotQA and TQA."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a way to generate synthetic data on custom data sources with large language models. First the synthetic data is generated through LLM. Then a portion of the generated data is used to finetune an LLM, which in turn will be used to curate the other portion of the generated dataset. The curated portion would be the final curated dataset that will be used for training the final LLM for the targeted domain/skill."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea of using part of synthesized dataset to finetune LLM for filtering is interesting"
            },
            "weaknesses": {
                "value": "The paper is not well written, hard to follow, and some important details are missing. For example, it is not clear how the two slices of data is selected, where one is used to train an LLM to do curation for the other slice.\n\nThe generation relies on existence of data source and ability to get seed data, which limits its applicability, in particular in situation where the domain and tasks are new.\n\nThe method uses LLM to do data curation, which implicitly assumes the LLM already have reasonable capability at the target task. For example for the text to SQL task, the LLM will be used to generate SQL statement, which would not be possible is the LLM does not already have that capability. Therefore, I am not sure if this method can actually be used to enable new skills for LLMs."
            },
            "questions": {
                "value": "How is the slice 0 and 1 determined? What are their respective sizes?\nWhich setting is used for HotpotQA, distractor or full wiki?\nIs finetuning full finetuning or parameter efficient tuning like LORA?\nWhat k is used in the experiments for data filtering?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents source2synth, a novel approach for generating synthetic data that incorporates intermediate reasoning steps grounded in real-world sources. The method follows a three-stage pipeline: (1) dataset generation utilizing realistic external sources to ensure relevance and factuality, (2) data curation to eliminate noise and maintain data quality, and (3) model fine-tuning for better alignment with the target task. The experimental results demonstrate significant improvements in Multi-hop QA and Tabular QA tasks, highlighting the effectiveness of the approach in enhancing model performance on complex reasoning tasks"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper addresses two critical challenges in the community\u2014data synthesis and LLM reasoning. It presents an innovative approach that leverages the model\u2019s generation capabilities to create synthetic data designed to enhance reasoning abilities. By grounding intermediate reasoning steps in real-world sources, this method reduces hallucination, which is a frequent issue in LLMs, making it a valuable contribution toward more reliable model behavior.\n2. The paper is well-written, with a clear structure that facilitates understanding. The methodology is presented in a step-by-step manner, making the approach accessible. Additionally, the results are organized effectively, and the figures are both informative and visually accessible, enhancing the reader\u2019s comprehension of the findings.\n3. The demonstrated improvement is substantial, showing that this method offers a promising direction for bolstering model reasoning abilities in specific tasks without extensive data annotation or heavy computational requirements."
            },
            "weaknesses": {
                "value": "1. **Limited Generalizability:** Although the method is positioned as general, its evaluation is restricted to two tasks within a single domain and model, raising questions about broader applicability:\n- Across Task Types: Both tasks involve custom adaptations. It\u2019s unclear if the method could extend to other reasoning tasks like code generation. If it\u2019s adaptable, specifying which tasks and what adaptations are needed would be useful; otherwise, the paper may need to scale back general claims.\n- Across Domains: The method\u2019s transferability across domains (e.g., general QA to medical QA) is not tested. Exploring cross-domain flexibility would strengthen the paper.\n- Across Models: It\u2019s unclear if the method generalizes to models with different reasoning abilities, or how model capabilities affect gains. Addressing this would enhance understanding of the method\u2019s adaptability.\n\n2. **Lack of Deep Analysis on Method Mechanics:** The paper lacks in-depth analysis regarding how and why the method functions effectively.\n- Data Imputation Motivation: The motivation behind the selective data imputation approach is unclear. For example, it\u2019s not explained why some tasks are chosen for imputation over others, what specific elements within a task should be imputed, or how these choices are determined.\n- Primary Factors Influencing Effectiveness: It\u2019s uncertain if gains stem from source data quality, the LLM\u2019s generation ability, or both. For instance, does the synthesis from a weaker model impact performance differently than synthesis from a stronger model? Additionally, since the current synthesized data is based on the model\u2019s existing knowledge (the instances model can't do correctly are filtered), understanding how data that falls outside the model\u2019s known domain affects performance would provide valuable insights into the approach\u2019s mechanism.\n\n3. **Missing Baselines:** The paper claims that grounding synthetic data in external sources produces more factually accurate examples; however, the experiments do not fully substantiate this. Including a baseline where real sources are replaced with the model\u2019s synthetic sources would help validate this claim. Furthermore, the paper does not justify the exclusion of previous data synthesis methods. Adding these baselines or justifying their absence would bolster the paper\u2019s argument for its approach\u2019s advantages over existing methods."
            },
            "questions": {
                "value": "1. Why is there no baseline for TQA using a fine-tuned LLM on real data? Including this would offer a more direct comparison of the synthetic data\u2019s impact against real data performance.\n2. why does the 3-shot bring more significant improvements to LLMSynth than LLMCurated in Figure-5? Are these two methods with the same training parameters and Is there a possible explanation for that?\n3. Why was the base model for TQA switched from LLaMA 2-70B to Starchat-beta LM? Clarifying this would help us understand the model choice for each task.\n\n\nSuggestion\n1. It seems like a newline is missing for \"data filtering\" in Line 326 \n2. Adding a pointer in the main paper to Figure 14, which displays the COT prompt used for MHQA, would improve accessibility for readers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}