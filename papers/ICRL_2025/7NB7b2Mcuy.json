{
    "id": "7NB7b2Mcuy",
    "title": "Grond: A Stealthy Backdoor Attack in Model Parameter Space",
    "abstract": "Recent research on backdoor attacks mainly focuses on invisible triggers in input space and inseparable backdoor representations in feature space\nto increase the backdoor stealthiness against defenses.\nWe examine common backdoor attack practices that look at input-space or feature-space stealthiness and show that state-of-the-art stealthy input-space and feature-space backdoor attacks can be easily spotted by examining the parameter space of the backdoored model. \nLeveraging our observations on the behavior of the defenses in the parameter space, we propose a novel clean-label backdoor attack called Grond. \nWe present extensive experiments showing that Grond outperforms state-of-the-art backdoor attacks on CIFAR-10, GTSRB, and a subset of ImageNet. \nOur attack limits the parameter changes through Adversarial Backdoor Injection, adaptively increasing the parameter-space stealthiness.\nFinally, we show how combining Grond's Adversarial Backdoor Injection with commonly used attacks can consistently improve their effectiveness.\nOur code is available at \\url{https://anonymous.4open.science/r/grond-557F}.",
    "keywords": [
        "backdoor attack",
        "backdoor defense"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=7NB7b2Mcuy",
    "pdf_link": "https://openreview.net/pdf?id=7NB7b2Mcuy",
    "comments": [
        {
            "summary": {
                "value": "The authors propose a backdoor attack against image classification models that improves robustness and stealthiness. Current backdoor attacks are \"easily spotted by examining the parameter space\". The authors propose an Adversarial Backdoor Injection method that prunes weights of the backdoored network after each training epoch whenever they deviate too strongly from the mean weight within each layer. The authors evaluate their attack on relatively small-scale image datasets, such as CIFAR-10 and a 200-class subset of ImageNet, which includes nine backdoor removal and seven backdoor detection methods. The results show their attack is robust and undetectable against all surveyed defences."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Effectiveness: The paper's results are promising and show improvement over other attacks in all dimensions. \n- Ablation studies: The authors conducted extensive experiments across multiple datasets (CIFAR-10, GTSRB, ImageNet200) and architectures to analyse their attack\u2019s effectiveness. \n- Presentation: The methodology and results are presented clearly, making the paper easy to follow."
            },
            "weaknesses": {
                "value": "- Lack of Novelty: The approach of pruning weights to enhance stealth is not particularly original and provides only limited new insights into defending against these types of attacks. This limits the novelty of the proposed method.\n\n- Assumption of a Strong Attacker: The paper assumes a white-box threat model with complete control over the training process. This setting, also known as a \u2018supply chain attack\u2019 [A], is extremely challenging (hopeless?) to defend against. It may not represent more realistic attacks or limited-access scenarios. This was stated in previous works [A] and others even show that provably undetectable backdoors can be implanted into models in this setting [B]. \n\n- Lack of theoretical insights: There are no clear reasons why Grond should perform better than existing attacks, and the authors do not provide insights on the 'why' question. \n\n- Lack of adaptive defenders: From a security perspective, it appears that Grond does not evaluate an adaptive defender who knows the attack strategy used by Grond. For instance, pruning weights in the way the authors proposed could make the attack detectable. \n\n-------\n[A] Hong, Sanghyun, Nicholas Carlini, and Alexey Kurakin. \"Handcrafted backdoors in deep neural networks.\" Advances in Neural Information Processing Systems 35 (2022): 8068-8080.\n\n[B] Goldwasser, Shafi, et al. \"Planting undetectable backdoors in machine learning models.\" 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS). IEEE, 2022."
            },
            "questions": {
                "value": "- Why does Grond work better than any other method?\n\n- Why is the white-box setting significant?\n\n- Do adaptive defenders exist that can detect or remove Grond?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new clean-label backdoor attack that achieves stealthiness in both the input space and the parameter space. Specifically, to achieve the stealthiness in the input space, the paper utilizes the targeted universal adversarial perturbation as the backdoor trigger. For the parameter-space stealthiness, the paper restricts the magnitude of model weight parameters by setting particularly large weights to the mean value of the corresponding layer. The evaluation is conducted on three standard benchmarks. Comparing to existing backdoor attacks, the proposed attack is more resilient to existing defense and detection methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The studied topic is important as backdoor attacks can exploit the integrity of deployed deep learning models and cause unexpected consequences.\n2. The paper is overall easy to understand."
            },
            "weaknesses": {
                "value": "1. The proposed attack utilizes the targeted universal adversarial perturbation as the backdoor trigger, which is the same as an existing work [1]. To increase the stealthiness of the attack in the parameter space, the paper uses backdoor defenses to help reduce backdoor-related neurons. This technique has already been proposed in the literature [2]. The proposed attack is just a collection of existing techniques. The novelty is very limited.\n2. The paper assumes that the attacker has white-box access to the training processes, meaning that the attacker has whole control over the training. And yet, the paper chooses a clean-label attack, which is quite strange. The introduction of clean-label attacks is to simulate the scenario where adversaries have no control over the labeling and training procedures. The attacker can only modify a subset of the training images, making it a realistic threat model. Since this paper assumes white-box access to the training processes, there is no need to use the clean-label setting. Can the authors explain why such a setting is needed for a successful attack?\n3. According to Figure 2, the mask loss for the proposed backdoor attack is much lower than benign cases. Cannot one design a defense method by measuring the outliers of the mask loss? Clearly the mask loss for the proposed attack is much smaller, which can be easily detected.\n4. Following the above point, there is no evaluation on adaptive defenses, where the defender has the knowledge of the proposed attack. Since the proposed attack uses a small-size trigger with epsilon equal to 8, simple defenses can be existing adversarial detection methods and/or (universal) adversarial training. Another defense approach could be randomly perturbing the weight parameters. As the proposed attack reduces backdoor weights, the backdoor effect may be quite brittle when weights are perturbed.\n\n\n\n[1] Zeng, Yi, et al. \"Narcissus: A practical clean-label backdoor attack with limited information.\"\u00a0Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security. 2023.\n\n[2] Cheng, Siyuan, et al. \"Deep feature space trojan attack of neural networks by controlled detoxification.\"\u00a0Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 2. 2021."
            },
            "questions": {
                "value": "Please see above comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents Grond, a backdoor attack that achieves enhanced stealth across input, feature, and parameter spaces to avoid detection. Through Adversarial Backdoor Injection, Grond disperses backdoor effects across multiple neurons, making it harder to identify using parameter-space defenses. Extensive experiments on datasets like CIFAR-10, GTSRB, and ImageNet200 show that Grond outperforms other attacks in evading both pruning and fine-tuning-based defenses, highlighting its robustness and adaptability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper introduces Grond, a novel backdoor attack with comprehensive stealth across input, feature, and parameter spaces. Using Adversarial Backdoor Injection, Grond disperses backdoor effects across neurons, enhancing its stealth and evading parameter-space defenses. Extensive testing on multiple datasets (CIFAR-10, GTSRB, ImageNet200) and defenses demonstrates Grond's effectiveness and adaptability across diverse scenarios and model architectures."
            },
            "weaknesses": {
                "value": "1. Limited Threat Model in Terms of Defender Capabilities: The paper's threat model lacks a thorough consideration of the defender\u2019s capabilities, particularly regarding proactive measures they could take to identify and mitigate backdoors prior to deployment. This omission may limit the applicability of the model to real-world scenarios where defenders could leverage more advanced tools and strategies.\n2. Lack of Comparison with Stealthy Clean-Label Backdoor Attacks: The paper does not include a comparison with other existing stealthy clean-label backdoor attacks, such as Hidden Trigger Backdoor Attacks (HTBA).\n3. Limited Range of Defense Methods Evaluated: The paper tests Grond against a small selection of defense methods, primarily focusing on pruning and fine-tuning-based defenses."
            },
            "questions": {
                "value": "Can this type of backdoor attack be detected by sample-based detection defenses?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the researchers propose a new backdoor attack scheme to combat existing defense strategies based on model repairing. The core idea of this scheme is very simple and easy to understand. Specifically, this scheme first generates a trigger using TUAP, and then uses this trigger to poison the model. During the process of implanting the backdoor, it modifies parameters with higher activation values, thereby enhancing the stealth of the backdoor attack in the parameter space. Additionally, experimental results demonstrate the effectiveness of this scheme. However, both trigger generation and adversarial backdoor injection are based on existing works, so the innovation of this research is limited."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The experimental results of this approach are convincing. The results indicate that the approach can bypass existing defense mechanisms while maintaining a high attack success rate."
            },
            "weaknesses": {
                "value": "This work lacks novelty because two key steps\u2014trigger generation and adversarial backdoor injection\u2014are based on existing studies [1], [2]."
            },
            "questions": {
                "value": "1. The experimental section should clearly specify the number of clean samples used in the pruning-based and fine-tuning-based approaches, which is not clearly stated in the paper. \n2. The authors list some ImageNet200 backdoor samples; however, despite the authors claiming that this backdoor attack is stealthy in input space, feature space, and parameter space, a close examination of these backdoor samples reveals obvious perturbations. The authors should evaluate the quality of the backdoor images, especially in comparison with some imperceptible backdoor attacks, such as [3].\n3. It would be more convincing if the authors could provide Grad-CAM images of the backdoor samples and visualise their distribution in feature space.\n\n\n[1] Moosavi-Dezfooli, Seyed-Mohsen, et al. \"Universal adversarial perturbations.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n[2] Zheng, Runkai, et al. \"Data-free backdoor removal based on channel lipschitzness.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n[3] Doan, Khoa, et al. \"Lira: Learnable, imperceptible and robust backdoor attacks.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}