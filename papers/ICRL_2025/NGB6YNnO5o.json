{
    "id": "NGB6YNnO5o",
    "title": "Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis",
    "abstract": "Despite the empirical success of Diffusion Models (DMs) and Variational Autoencoders (VAEs), their generalization performance remains theoretically underexplored, particularly lacking a full consideration of the shared encoder-generator structure. Leveraging recent information-theoretic tools, we propose a unified theoretical framework that guarantees the generalization of both the encoder and generator by treating them as randomized mappings. This framework further enables (1) a refined analysis for VAEs, accounting for the generator's generalization, which was previously overlooked; (2) illustrating an explicit trade-off in generalization terms for DMs that depends on the diffusion time $T$; and (3) providing estimable bounds for DMs based solely on the training data, allowing the selection of the optimal $T$ and the integration of such bounds into the optimization process to improve model performance. Empirical results on both synthetic and real datasets illustrate the validity of the proposed theory.",
    "keywords": [
        "Generalization bounds; information theory; generative models; VAE; diffusion models;"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=NGB6YNnO5o",
    "pdf_link": "https://openreview.net/pdf?id=NGB6YNnO5o",
    "comments": [
        {
            "summary": {
                "value": "This paper provides a information-theoretic framework of generalization theory for variational auto-encoders and diffusion models. Authors consider generalization properities for both the encoder and the generator in VAEs. Their generalization bounds can be estimated using only training data, providing a practical guidance for hyperparameter selection."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors address the critical topic of generalization in generative models, and provide estimable bounds for both the encoder and the generator in VAEs. \n- Bounds for VAEs avoid Wasserstein distance and impose milder assumptions (bounded to sub-Gaussian). \n- Bounds for DMs overcome the challenges associated with KL-divergence's non-satisfaction of the triangle inequality, and contribute to a clearer understanding of diffusion time\u2019s role in generalization and model performance."
            },
            "weaknesses": {
                "value": "- In line 98, the paper asserts that the bounds for the encoder are tighter, yet this claim lacks sufficient detail. Although some comparisons to previous bounds are made in line 324, there remains a need for a more explicit, quantitative analysis to illustrate the improvements over existing bounds. Adding a direct comparison or detailed quantitative analysis would make the claim more substantiated and provide clearer evidence of the improvement.\n- The proposed generalization bounds do not clearly indicate a dependency on the number of samples, $m$, limiting their practical applicability. To make the bounds more actionable, it would be helpful if the authors explicitly stated the sample complexity (e.g., $O(1/ m)$) within the main theorems or discussed the bounds\u2019 scaling with respect to $m$. This addition could guide readers in understanding the bounds\u2019 robustness and sample efficiency.\n- The paper suggests a theoretical trade-off for diffusion models based on diffusion time, but this trade-off is not consistently reflected in the experimental results, as KL and BPD metrics do not show this effect. The authors could address this discrepancy by either providing a potential explanation for the divergence between the theoretical and experimental findings or suggesting additional experiments that might better capture the trade-off. Revisiting this section would enhance clarity and ensure its alignment with the paper\u2019s core contributions."
            },
            "questions": {
                "value": "- The generalization bounds resemble Theorem 4.1, which primarily links encoder mappings' complexity to the gap between empirical generalization and expected generalization. But this should also affects empirical generalization. Can you provide a theorem telling how this two terms together affects expected generalization? \n- As far as I know, in practical works of generative models, the target distribution is always unknown, which means the empirical generalization error is always unknown. Could the authors elaborate on the practical benefits of bounding the gap between expected and empirical generalization error, particularly when the empirical error itself may not be measurable? \n- As mentioned in line 350, generalization bounds are computable, can you explain how to compute term $\\hat{L}_{ESM}$ among upper bounds in Theorem 6.2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper derives generalization bounds for encoder-generator-based generative models, considering both the encoder and generator components. The analysis is specifically applied to Variational Autoencoders (VAEs) and Diffusion Models (DMs), with tailored generalization bounds presented for each. Notably, a novel trade-off relationship is introduced for DMs. The theoretical results are supported by numerical experiments, demonstrating that the derived bounds offer practical utility, particularly in capturing the trade-offs in generalization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper derives a generalization bound for encoder-generator architectures under the relatively mild assumption of sub-Gaussian loss functions. As noted in lines 286-291, the paper provides an intuitive explanation of these bounds and a convincing discussion of the trade-offs involved.\n- Corollaries 4.2 and 4.3 extend the analysis to evaluate the Wasserstein distance and KL divergence between the generative model's distribution and the data distribution, offering valuable tools for the theoretical analysis of a wide range of generative models.\n- For VAEs, the paper presents a bound that applies to arbitrary generators, distinguishing it from existing research.\n- In the case of DMs, a new trade-off relationship is derived. Under certain reasonable assumptions, the generalization bound involving mutual information of $G^{\u03b8}$ is established concerning sample size, score function bounds, and time $T$. This leads to a tighter $ m^{-1/2}$ bound.\n- The results are further substantiated by numerical experiments that validate the theoretical trade-offs observed in the upper bound."
            },
            "weaknesses": {
                "value": "While the results are significant in terms of learning theory by considering the effects of both the encoder and generator, some areas could be further improved:\n- Although challenging, the analysis does not incorporate the complexity of the learning models. Including bounds related to the complexity of simple neural networks or linear models could strengthen the work.\n- Aside from the theoretical analysis provided by the generalization bound, it would be beneficial to relate these results to those from sharp theoretical analyses in proportional limit settings. For example, in denoising encoders [1], autoencoders [2], or variational autoencoders [3, 4], the generalization error has been sharply evaluated by fixing the ratio $\\alpha = n/d = \\Theta(1)$, where $n \\to \\infty$ is the number of data points and $d \\to \\infty$ is the data dimension. Including such discussions in the related work section would provide a more comprehensive view of the literature.\n- It would be interesting to see the results of training on the complete datasets of MNIST and CIFAR-10 rather than in a few-shot setting.\n- The paper refers to the claime by Thesis et al. (2015) that \"accurately estimating the KL divergence and Bits-Per-Dimension (BPD) for high-dimensional data distributions with limited data is challenging\". However, this could be addressed using population MCMC or other Markov chain Monte Carlo methods. Investigating whether the trade-offs as in the upper bound remain evident in such scenarios would be intriguing.\n\n- [1]: Cui, Hugo, and Lenka Zdeborov\u00e1. \"High-dimensional asymptotics of denoising autoencoders.\" Advances in Neural Information Processing Systems 36 (2023): 11850-11890.\n- [2]: Refinetti, Maria, and Sebastian Goldt. \"The dynamics of representation learning in shallow, non-linear autoencoders.\" International Conference on Machine Learning. PMLR, 2022.\n- [3]: Ichikawa, Yuma, and Koji Hukushima. \"Dataset size dependence of rate-distortion curve and threshold of posterior collapse in linear vae.\" arXiv preprint arXiv:2309.07663 (2023).\n- [4]: Ichikawa, Yuma, and Koji Hukushima. \"Learning Dynamics in Linear VAE: Posterior Collapse Threshold, Superfluous Latent Space Pitfalls, and Speedup with KL Annealing.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2024."
            },
            "questions": {
                "value": "- The paper recommends adding terms for the generalization of $G$ to provide insights and practical guidance for VAEs. However, estimating these terms is generally considered difficult. Are there any methodologies that could facilitate this estimation?\n- The few-shot setting with $m=16$ is mentioned in the experiments on real data. Could you provide more details on this experimental setup? Does it involve generating new data with a pre-trained model in a few-shot learning manner, or is it simply trained with limited data?\n- What is the definition of $g$ in line 209?\n- How would the generalization bounds change if the data were assumed to lie on a low dimensional manifold? What predictions can be made regarding such cases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors explore the theoretical generalization behaviour for both variational autoencoders (VAEs) and diffusion models (DMs), noting that a DM can be seen as an infinite-layered hierarchical VAE. For both models, the behaviour of both the encoder and decoder in the VAE, and forward and reverse diffusion steps in the DM are analyzed. The authors then provide a theoretical upper bound for DMs with respect to the number of diffusion steps and then empirically verify it on several datasets, both synthetic and real (MNIST and CIFAR10), demonstrating its validity and usefulness in training optimal DMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper provides a very detailed and comprehensive information-theoretic analysis of generalization in VAEs and DMs along with experiments that empirically validate it. In particular, the incorporation of encoder-decoder / forward-reverse process into the analysis provides a novel view into their impact on the generative models' generalization behaviour, such as the finding that longer diffusion steps do not necessarily result in better estimates in DMs."
            },
            "weaknesses": {
                "value": "The paper's writing made it difficult to process the main contributions to the paper for two main reasons:\n\n(1) Despite the abstract suggesting that the VAE's generalization behaviour is studied, much of the paper's focus is on analyzing DM behaviour.\n\n(2) There is notably no experiments that validate VAE behaviour, which suggests that the VAE is studied here as a precursor to understanding the generalization behaviour of DMs."
            },
            "questions": {
                "value": "I suggest the authors make the writing more clear by including a separate background section on the relationship between VAEs and diffusion models; this will make it clear that the main contribution is providing a theoretical upper-bound for diffusion models as aided by the analysis of VAE generalization. A good place to start is the Variational Diffusion Models paper [1] that explicitly makes this connection by formulating the diffusion learning algorithm in terms of a variational lower bound.\n\nReferences\n[1] Diederik P. Kingma, Tim Salimans, Ben Poole, & Jonathan Ho. (2023). Variational Diffusion Models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work puts a bound on the generalization gap for generative models relying on an encoder-decoder architecture. \nUsing this, they introduce bounds on \n - the wasserstein distance between the data distribution and generated distribution for variational autoencoders (VAEs),\n - the KL-divergence between the data distribution and generated distribution for diffusion models. \n\nThey run experiments on a score based diffusion model using data from \"Swiss Roll\", MNIST and CIFAR-10."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1: VAEs and diffusion models are widely used, so theoretical work on them is very relevant. \n\nS2: The bounds introduced in the work are new. \n\nS3: The story of the article is easy to follow."
            },
            "weaknesses": {
                "value": "The weaknesses relate to the experimental part of the paper.\n\nW1: For both experiments you are using a fixed number of steps for generation while changing T. \n\tThis means that for varying T, you are also varying the step-size, which can have a large effect \n\ton generation (See e.g. Zhang and Chen 2023 \"Fast Sampling of Diffusion Models with Exponential Integrator\").\n\tIt is not clear to me whether this also affects your experiment in other ways (you should check this),\n\tbut if you want to say something about sample quality, my suggested solution is to choose a step size, e.g. 1/1000,\n\tand also generate images with varying T, but fixed step-size. \n\nW2: Figure 3 b) and c) do not support your claim. You say that the discrepancy comes from using too few samples \n\tto get a reliable estimation of KL divergence. My suggested solution is to do the experiment with enough samples \n\tto get a reliable estimation. Since your result is about KL-Divergence and not sample quality, it is not enough to \n\tlook at sample quality for verification. Especially since since performance in one is not directly linked to \n\tperformance in the other. This observation is also made in Theis et al. 2015 \"A note on the evaluation of generative models\"\n\twhich you also cite yourselves."
            },
            "questions": {
                "value": "Q1: Do you have a theoretical reason (or at least an intuition) for why the mutual information should follow \nthe upper bound in theorem 6.3?\n\nQ2: At the end of section 6.1, you write \"$T_3$, which characterizes the generalization of generator $G_T$ , will\nremain non-zero for a small sample size\". How small does the sample size need to be? And if you use enough samples in your \nMNIST and CIFAR-10 experiments to get a good estimation of KL-Divergence, will that make the contribution of $T_3$\nvery small? \n\nQ3: Could you add the next steps in your proof in appendix 1.C? As it is now, I don't see the final claim of the theorem \nin the last lines of the proof."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}