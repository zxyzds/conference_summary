{
    "id": "vgvnfUho7X",
    "title": "Beyond accuracy: understanding the performance of LLMs on exams designed for humans",
    "abstract": "Many recent studies of LLM performance have focused on the ability of LLMs to achieve outcomes comparable to humans on academic and professional exams. However, it is not clear whether such studies shed light on the extent to which models show reasoning ability, and there is controversy about the significance and implications of such results. We seek to look more deeply into the question of how and whether the performance of LLMs on exams designed for humans reflects true aptitude inherent in LLMs. We do so by making use of the tools of psychometrics which are designed to perform meaningful measurement in test taking. We leverage a unique dataset that captures the detailed performance of over 5M students across 8 college-entrance exams given over a span of two years in Brazil. With respect to the evaluation of LLM abilities, we show that the tools of Item Response Theory (IRT) provide a more informative evaluation of model performance than the usual accuracy metrics employed in previous studies. Digging deeper, we show that the modeling framework of IRT, by explicitly modeling the difficulty levels of questions, allows us to quantitatively distinguish between LLMs that answer questions in \u201chuman-like\u201d patterns versus LLMs that do not. We also show how to quantitatively identify cases in which exam results are not reliable measurements of an LLM's ability. Using the tools of IRT we can also identify specific questions that appear to be either much easier, or much harder, for machines than for humans, and we give some reasons for those differences. Overall, our study shows that the conventional focus on accuracy as the primary performance metric for LLM studies does not allow us to deeply understand the true capabilities of LLMs and compare them to that of humans. Thus, we claim that psychometric modeling should play a larger role in the evaluation of LLM capabilities on exams designed for humans.",
    "keywords": [
        "large language models",
        "model evaluation",
        "psychometrics"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We apply traditional psychometrics tools to evaluate the performance of large language models and compare their patterns of correct and incorrect answers against a large dataset of human students doing college-entrance level exams.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vgvnfUho7X",
    "pdf_link": "https://openreview.net/pdf?id=vgvnfUho7X",
    "comments": [
        {
            "summary": {
                "value": "The paper examines whether LLMs demonstrate human-like reasoning on exams designed for humans by using Item Response Theory (IRT). Analyzing a dataset of over 5 million Brazilian students' responses to college entrance exams, the study finds that traditional accuracy metrics inadequately assess LLM capabilities. IRT, by accounting for question difficulty, offers a more nuanced evaluation, distinguishing between human-like and non-human-like response patterns. The results show that while LLMs sometimes mimic human behavior, they often deviate significantly."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper is well-organized and clearly articulates both the limitations of using accuracy as the sole metric and the benefits of IRT for evaluation. Diagrams and data tables effectively support the paper\u2019s arguments, making complex psychometric methods accessible to a broader audience. \n2. The paper employs rigorous experimental design and utilizes a comprehensive dataset, enhancing the robustness of its findings. By analyzing various models, including GPT-3.5 and LLaMA variants, the authors demonstrate the generalizability of IRT\u2019s applicability. The study further uses well-defined psychometric to validate its claims, supporting the soundness of the technical approach.\n3. This work holds significance as it points out weaknesses in LLM evaluations. By moving beyond accuracy, the paper demonstrates that psychometric techniques can better represent model abilities quantitatively."
            },
            "weaknesses": {
                "value": "The paper has several notable shortcomings. \n1. Firstly, the idea of using psychometrics and IRT to replace traditional metrics like accuracy in AI benchmarking was proposed well before 2021, diminishing the novelty of the approach. \n2. The use of IRT to compare the response patterns of LLMs with those of humans has already been widely explored in existing research.\n3. The technical methods employed in the paper, such as IRT and Fisher information maximization, are already extensively applied in AI evaluation, further reducing the originality of the study's methodology.\n\nPresentation needs to be polished , and it remains some typos. \nResults section, line 271 , \u201cLMM\u201d may be a typo\nMethods section, line 237, \u201crun\u201d -> \u201dran\u201d"
            },
            "questions": {
                "value": "Please emphasize the innovative aspects and contributions of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors suggest comparing IRT parameters of LLMs vs. humans instead of just accuracy. They present results on a large dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is well written."
            },
            "weaknesses": {
                "value": "To me, Figure 1 suggests that accuracy and ability estimates and highly correlated.\n\nThis approach seems to have been already studied by: Liu, Yunting, Shreya Bhandari, and Zachary A. Pardos. \"Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis.\" arXiv preprint arXiv:2407.10899 (2024). I agree that this paper is recent, though.\n\nThe approach of using IRT for making more efficient benchmarks seems to be taken by Polo et al. (2024) and Zhuang et al. (2023), papers cited by the authors. However I do not feel that considering a IRT model trained on human responses, as stated by the authors, can be considered enough novel. Plus, the way the estimation of IRT parameters (LLM ability estimates) is done (if it depends on a prior, then it is biased) can hinder the reproducibility of results and the validity of findings."
            },
            "questions": {
                "value": "Shouldn't we estimate multidimensional IRT parameters of models vs. humans instead of just 2PL-IRT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates the performance of large language models (LLMs) on human-designed exams, emphasizing the need for deeper analysis beyond standard accuracy metrics. Utilizing a dataset of over 5 million Brazilian students across eight college entrance exams, the authors employ Item Response Theory (IRT) to assess LLM abilities more comprehensively. The study demonstrates that IRT can differentiate between human-like and non-human-like answering patterns and identify specific questions that vary in difficulty for LLMs compared to humans. Ultimately, it argues for the integration of psychometric modeling to better understand LLM capabilities and improve evaluations in academic contexts."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The study emphasizes the importance of construct validity, highlighting potential limitations of existing exams in measuring LLM abilities, thereby promoting a deeper understanding of LLM performance.\n2. By employing IRT, the research provides a more nuanced analysis of LLM performance, distinguishing between human-like and non-human-like response patterns, which leads to more reliable ability assessments.\n3. The study leverages a dataset of over 5 million student performances, providing a strong empirical foundation for analyzing LLM behavior, which enhances the credibility of the findings."
            },
            "weaknesses": {
                "value": "1. The paper relies on a single dataset for evaluating LLMs, which introduces bias. Given the complexity of large models, a more comprehensive evaluation is necessary, making it essential to use a variety of datasets for assessment.\n2. The paper exclusively employs the IRT model for assessment. However, there are many other cognitive diagnostic models available that can evaluate learners' abilities, such as MF[1], MIRT[2], and NCD[3]. The authors should explore these alternative models in greater depth to provide a more robust evaluation framework.\n3. The paper's technical innovation appears to be limited, primarily focusing on using IRT to evaluate LLMs. The methods employed mainly rely on prompting techniques, which do not demonstrate significant advancements in the evaluation approach.\n\n[1] Andreas Toscher and Michael Jahrer. Collaborative fltering applied to educational data mining. KDD cup, 2010.\n\n[2] Mark D Reckase. Multidimensional item response theory models. In Multidimensional item response theory, pages 79\u2013112. Springer, 2009.\n\n[3] Fei Wang, Qi Liu, Enhong Chen, Zhenya Huang, Yuying Chen, Yu Yin, Zai Huang, and Shijin Wang. Neural cognitive diagnosis for intelligent education systems. In Proceedings of the AAAI Conference on Artifcial Intelligence, pages 6153\u20136161, 2020."
            },
            "questions": {
                "value": "1. The study relies solely on Brazil's university entrance exams. Is there a risk of cultural or educational system biases? Can the findings be generalized to LLM evaluations in other countries or educational contexts?\n2. There are several models, such as MF and NCD, that can assess students' abilities more effectively than the IRT model. Why did the authors choose to use IRT to evaluate LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}