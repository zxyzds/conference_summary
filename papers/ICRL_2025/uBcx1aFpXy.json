{
    "id": "uBcx1aFpXy",
    "title": "OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction",
    "abstract": "We present OmniBooth, an image generation framework that enables spatial control with instance-level multi-modal customization. For all instances, the multi-modal instruction can be described through text prompts or image references. Given a set of user-defined masks and associated text or image guidance, our objective is to generate an image, where multiple objects are positioned at specified coordinates and their attributes are precisely aligned with the corresponding guidance. This approach significantly expands the scope of text-to-image generation, and elevates it to a more versatile and practical dimension in controllability. In this paper, our core contribution lies in the proposed latent control signals, a high-dimensional spatial feature that provides a unified representation to integrate the spatial, textual, and image conditions seamlessly. The text condition extends ControlNet to provide instance-level open-vocabulary generation. The image condition further enables fine-grained control with personalized identity. In practice, our method empowers users with more flexibility in controllable generation, as users can choose multi-modal conditions from text or images as needed. Furthermore, thorough experiments demonstrate our enhanced performance in image synthesis fidelity and alignment across different tasks and datasets.",
    "keywords": [
        "Image Synthesis",
        "Diffusion Model",
        "Controllable Generation",
        "Spatial Control",
        "Multi-modal Instruction"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We propose an image generation framework that enables spatial control with instance-level multi-model customization.",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=uBcx1aFpXy",
    "pdf_link": "https://openreview.net/pdf?id=uBcx1aFpXy",
    "comments": [
        {
            "comment": {
                "value": "We wish to convey our sincere appreciation to the reviewer for the thorough and insightful feedback provided. Below, we provide responses to the individual comments and questions raised by the reviewer:\n\n**(1) Comparison with MIGC++**\n\nWe express our gratitude to the reviewer for their insightful references. Briefly, MIGC++ enhances the original MIGC by integrating image-level conditions. The MIGC framework utilizes a cross-attention network to facilitate controllable image generation. Based on our understanding, a characteristic of these methods is that, the cross attention is conducted in the global marching between all the image tokens and condition tokens, followed by using the layout mask to refine the attention score. This process has unnecessary computation, and needs to be computed instance-by-instance. In contrast, our method achieves unified multi-instance customization by employing a single Latent Control Signal for all instances, ensuring both computational efficiency and simplicity. We have added the reference of MIGC++ in our revised version.\n\n**(2) Counterfactual scenarios**\n\nGood suggestion! During rebuttal, we provide counterfactual generation results in Section B and Figure 9,10 of our paper. Please check it out. Our method faithfully generates a skateboard field engulfed in flames which rarely happens in the real world. In summary, these results not only validate the effectiveness of our approach but also underscore its ability to extend beyond the confines of the training data, thereby demonstrating its robust generalizability.\n\n**(3) Benefit of edge loss**\n\nAs we have deleted the checkpoint trained without edge loss, it is difficult for us to provide a visual comparison. Here, we provide a theoretical analysis. In our framework, edge loss will enhance supervision over the edge area of each instance. This enhancement is expected to yield clearer object boundaries compared to models trained without such loss, and to ensure that instance contours are more accurately aligned with the provided masks.\n\n**(4) Bounding boxes condition**\n\nOur framework can be seamlessly adapted to box conditions by simply converting mask to box. Regarding overlapping instances, it is widely recognized that incorporating additional depth or 3D information can be beneficial. For instance, 3DIS [a] introduces a Depth-Driven Instance Synthesis framework that alleviates the issue of occlusion.\n\n[a] 3DIS: Depth-Driven Decoupled Instance Synthesis for Text-to-Image Generation"
            }
        },
        {
            "comment": {
                "value": "We would like to express our gratitude to the reviewer for their comprehensive feedback. Below, we provide responses to the individual comments and questions raised by the reviewer:\n\n**(1) Comparison with InstanceDiff** \n\nWe first note that InstanceDiff employs a restricted benchmark, it resizing all images to 512x512 before evaluating perceptual task accuracy, which differs from the standard COCO benchmark. To address this issue, we re-ran the benchmark, aligning it with the original COCO metrics without resizing, and evaluated at the original resolution of COCO images, thereby ensuring a more equitable comparison.\n\nFurthermore, we note that perception accuracy does not reflect generation quality. As shown in Figure 3 of our paper, the images generated by InstanceDiff contain human instances with unsatisfactory topology, potentially due to overfitting to the input layout guidance.\n\n**(2) Details**\n\n1. We utilize the COCO dataset, which comprises 220,000 annotated images.\n2. As stated in Line 356 of our paper, for each image reference, we perform random horizontal flips and random brightness adjustments as data augmentation to simulate multi-view conditions. Consequently, the instance-image inputs do not perfectly align with the masks during training. This is intentional, as we aim to learn transferability, ensuring that the input image reference and mask do not need to align, thereby avoiding overfitting.\n3. Yes, users are required to provide certain masks for controllable generation. The pipeline can be found in Figure 1 of our paper, where we leverage a white woman and a red bag for personalization.\n\n**(3) IP-Adapter results**\n\nThank you for your suggestion. IP-Adapter has achieved a DINO score of 0.667, a CLIP-I score of 0.813, and a CLIP-T score of 0.289. In comparison with IP-Adapter, our method has demonstrated a better improvement in both DINO and CLIP-T scores, although the CLIP-I score is slightly lower. Dreambooth-LoRA is great, but we do not find quantitative results of it.\n\n**(4) Reference**\n\nThe paper \"Be Yourself\" offers a versatile framework for multi-subject generation. We will add it to our revised version.\n\n**(5) OOD scenarios**\n\nAs suggested by the reviewer, we provide the following results during rebuttal:\n\n1. **Non-realistic image styles.** We provide counterfactual and OOD scenarios generation results in Section B and Figure 9 of our paper. In Figure 9 top row, we exhibit a non-realistic generation case, emulating the aesthetic of Hogwarts, which never happened in the real world.\n2. **Dog flying in the sky.** We display a surprising result in Figure 10 that a dog is flying high in the sky. We achieve this by altering the background instance (all instances except the dog) to the sky. Our method accurately generates these OOD scenarios of a dog flying in the sky.\n3. **Rectangular mask for a person.** Please check the results we provided in Figure 10 that we use a mask of cat to represent a human. Our method successfully warps the human body to conform to the contour of the mask.\n\nIn summary, these results not only validate the effectiveness of our approach but also underscore its ability to extend beyond the confines of the training data, thereby demonstrating its robust generalizability.\n\n**(6) Mask distributions**\n\nIn our supplementary video, we provide results for generation using user-drawn masks. Notably, the distinct disparities in mask distribution do not impede the quality of the generated images. For missing or noisy masks, they have widely existed in the COCO dataset. For instance, all the black regions in Figure 3 of our paper denote areas that are missing. Despite these challenges, our approach effectively completes the missing regions by leveraging global prompts or scene context. Consequently, the distribution issues of masks become less significant in the context of our method's performance."
            }
        },
        {
            "comment": {
                "value": "We would like to thank the reviewer for their positive and detailed feedback. Below, we provide responses to the individual comments and questions raised by the reviewer:\n\n**(1) Box condition or mask condition** \n\nFrom our technical university, our framework can be seamlessly applied to box control. We opt for mask conditioning as it allows users to freely manipulate the object's contour, thereby providing finer controllability. In contrast, box conditioning is more straightforward but offers only coarse geometry guidance, resulting in the random generation of object shapes. Both box conditioning and mask conditioning are applicable within our framework, which can be freely adapted based on downstream requirements.\n\n**(2) Regarding reference net in Animate anyone**\n\nWe thank the reviewer for the valuable reference. In \"Animate Anyone,\" the reference net utilizes a concatenation approach to fuse the condition and noisy latent. Essentially, our framework can be seamlessly adapted to the reference net in \"Animate Anyone.\" We can concatenate our Latent Control Signal with the noisy latent along the width dimension, then facilitate interaction between the condition and noisy latent through self-attention. One limitation of this approach is the quadratic increase in computational complexity, which is equivalent to doubling the resolution of the generated image, potentially posing challenges for high-resolution image generation."
            }
        },
        {
            "comment": {
                "value": "We would like to thank the reviewer for the insightful and detailed feedback. Below, we reply to individual comments and questions raised by the reviewer:\n\n**(1) The reliance on foundation model**\n\n\nAt first, it is believed that latent diffusion, ControlNet, and DINO are widely utilized foundational models or frameworks in the community. Numerous research works are based on them. As they contain powerful pretrained prior in computer vision, it is beneficial and essential to leverage these foundational models to attain enhanced performance and controllability. These would hardly be achieved if we were training from scratch.\n\n \n\nFurthermore, it is non-trivial to achieve a unified image generation with multi-modal instruction. Our proposed Latent Control Signal represents a novel generalization of spatial control from the RGB-image domain to the latent domain. It effectively and comprehensively extracts features from both DINO image prior and CLIP text prior, a capability that was previously unattainable. Finally, our method enables highly expressive instance-level customization with intriguing results that have achieved consensus among the reviewers.\n\n\n**(2) Ablate ratio of global embedding**\n\nWe appreciate the author's suggestion. In our paper, we have already conducted the experiment and provided the results in Appendix Section E. Overall, we found that there is an optimal mixing ratio between spatial embedding and global embedding.\n\n**(3) Complexity**\n\nAs a unified framework that enables controllable image synthesis with multi-modal instruction, our method eliminates the need for a separate training stage in a multi-task manner. Moreover, our training strategy is universal and can be applied across a variety of tasks and models.\n\n**(4) Relevant work of UniControl and Uni-ControlNet**\n\nWe concur with the reviewer's observation that there exists a body of pertinent literature in the realm of unified and multi-condition controllable image generation. However, in the context of UniControl and Uni-ControlNet, these frameworks are constrained as they come to leveraging multiple spatial-layout images as conditioning inputs. They can be difficult to achieve instance-level customization through the use of instance prompts or image references. From this perspective, our OmniBooth model attains a higher degree of controllability. Nonetheless, we will add these references in our revised version.\n\n**(5) Up and down arrows**\n\nThank you for your valuable advice! All metrics in Table 2 are such that higher values are indicative of better performance.\n\n**(6) Regarding Resolution**\n\nAs a characteristic of controllable generation with layout control, the smaller the object, the greater the difficulty in generation. In the context of our framework, we exclude instances with fewer than 50 pixels during training, as these types of objects are exceedingly challenging to learn and can impede the model's performance. Notably, despite this exclusion, our framework demonstrates the ability to generate high-quality images with precise control of small objects, as exemplified by the depiction of a white woman and a red bag in Figure 1."
            }
        },
        {
            "summary": {
                "value": "The paper introduces OmniBooth, a novel framework for image generation that leverages multimodal instructions to enable spatial and instance-level control. The method integrates 1) instance prompt, 2) reference image, 3) mask, and 4) global prompt to manipulate specific image attributes, allowing for precise and detailed control over the generated images. The core innovation is the use of a latent control signal that acts as a unified representation for various modalities, enhancing both flexibility and precision in image synthesis. It successfully integrates multimodal control signals into the latent to finetune the Diffusion Unet, similar to ControlNet, to enable more fine-grained controllability over the generated content."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The paper is well-written, exhibiting a clear logical flow and effective illustrations, particularly in Figures 1 and 2, which enhance understanding.\n\n+ The framework demonstrates competitive generative performance compared to established baselines such as InstanceDiffusion and ControlNet.\n\n+ The model offers precise and fine-grained control over generated content, which is highly valuable for practical applications.\n\n+ The model shows strong capability in inverting and fusing the reference image into the mask within the generated image, as evidenced by the attractive results in Figure 4."
            },
            "weaknesses": {
                "value": "- The paper's novelty and technical contribution appear limited. It primarily presents an application-driven approach that combines popular techniques such as latent diffusion, ControlNet, and DINO into a cohesive framework. While the application task is intriguing, the overall novelty is not significant.\n\n- Some explanations lack clarity. For instance, in lines 178-179, the authors state, \"we randomly drop 10% of the spatial embedding\u200b and replace it with the DINO global embedding\u200b to encode global identity,\" but do not clarify the rationale for injecting the global embedding. Additionally, there is no ablation study on the ratios of global embedding used.\n\n- The complexity of the framework suggests that it requires numerous manual adjustments and engineering tricks to function effectively.\n\n- The paper fails to reference some highly relevant works in the area of unified and multi-condition controllable image generation, such as:\n\n[1] UniControl: A Unified Diffusion Model for Controllable Visual Generation in the Wild (NeurIPS 23).\n\n[2] Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models (NeurIPS 23).\n\n- The metrics presented in Table 2 would benefit from the inclusion of up and down arrows to indicate performance trends."
            },
            "questions": {
                "value": "What is the impact of resolution for generated content? The condition latent should be aligned with the mask spatially but it is down-sized. In this way, how to enable the precise content control around the edge of each region within the mask? Are there any requirements of the the size (ie, >=10 px) of the object mask?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces OmniBooth, an image generation framework that supports spatial control and instance-level customization through multi-modal instructions. The goal of the framework is to generate images based on user-defined masks and associated text or image guidance, where multiple objects are positioned at specified coordinates, and their attributes are precisely aligned with the corresponding guidance. Through experiments, the authors demonstrate the method's performance enhancement in image synthesis fidelity and alignment across different tasks and datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Propose a comprehensive image generation framework achieving multi-modal control, including textual descriptions and image references.\n2. Introduced latent control signals, enabling highly expressive instance-level customization within the latent space.\n3. Demonstrate the method's ability to achieve high-quality image generation and precise alignment across various settings and tasks through extensive experimental results."
            },
            "weaknesses": {
                "value": "1. The image condition might be too strong and it is not usually utilized in real world. Maybe layout bounding boxes are more straightforward and enable more flexible generation. For example, if training with bounding boxes as conditions, the content is only required to be generated within the bounding boxes it would be possible to realize generating diverse images. However, training with masks as conditions not only requires providing precise binary masks but also limits the diversity of generated results because the positions and the edges are all fixed by the conditions.\n2. Has the author considered reference net [1]? This model architecture might be more suitable for your target because it provides spatial information and is widely used in situations where low-level and precise information in the given pictures is required to preserve. I think this architecture is based on attention, which should be easier to train. I think directly injecting features as described in your paper might destroy the original information contained in the image and I want to discuss this possible problem with the authors.\n\n[1] Hu, Li. \"Animate anyone: Consistent and controllable image-to-video synthesis for character animation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents OmniBooth, a method providing instance-level spatial control for text-to-image models. In addition to a global textual prompt, users can provide instance-level masks paired with a text prompt or image reference for each instance, guiding the generation to follow the mask. The authors introduce a Latent Control Signal, a feature map that spatially integrates textual and visual conditions. OmniBooth achieves fine-grained control over the generation, aligning with the user-defined mask and attributes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper addresses the critical task of fine-grained instance control for image generation and proposes support for instance-level conditioning on both text and image inputs.\n\n- The authors introduce an interesting methodology for combining multi-modal inputs into a single Latent Control Signal. The use of ROI Alignment for spatial warping of the image condition is particularly elegant.\n\n- The qualitative results are strong, demonstrating the ability to generate objects in precise locations.\n\n- The writing is clear, and the paper includes numerous qualitative examples."
            },
            "weaknesses": {
                "value": "- The qualitative results in Table 1 are not very convincing, showing comparable results to existing work (InstanceDiff). Additionally, there are discrepancies between the InstanceDiff performance in your Table 1 and its reported performance in Table 1 of their paper. For instance, they report an AP50_mask score of 50.0 on the COCO validation set, while you report a score of 47.0. What is the reason for this difference?\n\n- Missing details:\n    * How many images were used for training?\n\n    * Do the instance-image inputs always perfectly align with the mask during training? If so, why would the model handle instance-level images that don\u2019t correspond exactly to the mask?\n\n    * How does subject-driven image generation work (i.e., personalization)? Does the user need to provide a mask?\n\n- Subject-driven image generation: comparisons to IP-Adapter in Table 2 are missing. Additionally, Dreambooth-LoRA is a more widely used approach than plain Dreambooth, so it would be better to compare with it.\n\n- Missing citation in the related work:\nBe Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation, Dahary et al.\n\n- The analysis is limited. How does the model handle OOD input masks, non-realistic image styles, etc.?\n\ntypos:\n- L49: omnipotent of controllability -> omnipotent controllability"
            },
            "questions": {
                "value": "- How does your method handle unlikely positions for instances? For instace, a dog flying in the sky?\n- What happens when the mask does not match the instance description? For example, a rectangular mask for a person.\n- Your method was trained and evaluated on COCO panoptic segmentation masks. How does it handle segmentation masks from other distributions? How well can it manage missing or noisy masks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the OMNIBOOTH method, which allows users to perform spatial control with instance-level multi-modal (i.e., text and image) customization. Specifically, OMNIBOOTH integrates multi-modal instructions into a unified control latent space, which is then injected into the Stable Diffusion model via ControlNet. The proposed unified control latent represents a novel innovation, and experimental results indicate that OMNIBOOTH outperforms previous methods in terms of performance. The structure of the paper is well-organized, and the writing is clear and easy to understand."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed unified control latent is novel. Experimental results demonstrate that the method described surpasses previous state-of-the-art approaches. The structure of the entire paper is clear, and the content is easily comprehensible."
            },
            "weaknesses": {
                "value": "I think that this article currently has no significant issues; however, I still have some reservations. Please see the Question Section for further details."
            },
            "questions": {
                "value": "Q1\u3001I would like to note that instance-level multi-modal customization was previously implemented in MIGC++[1]. Could you clarify how OMNIBOOTH differs from or improves upon MIGC++?\n\nQ2\u3001How effective is OMNIBOOTH in generating counterfactual scenarios? For example, what would be the outcome if the effect in Fig.4(a) were replaced with a 'flame'? Could you provide additional examples of generated counterfactual scenarios?\n\nQ3\u3001Which specific examples benefit most from the improvement in edge loss? Could you provide some visual comparison charts?\n\nQ4\u3001In your opinion, if fine-tuning or retraining is permitted, what would be the effectiveness of using unified control latent when specifying instance positions with bounding boxes, especially in scenarios with overlapping instances?\n\n[1] MIGC++: Advanced Multi-Instance Generation Controller for Image Synthesis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}