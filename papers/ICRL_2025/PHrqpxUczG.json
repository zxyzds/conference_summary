{
    "id": "PHrqpxUczG",
    "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation",
    "abstract": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important tokens based on earlier attention patterns. Both approaches, however, can result in performance bottlenecks or frequent mispredictions.\n\nLogQuant takes a different approach. By applying a log-based filtering mechanism, it selectively compresses the KV Cache across the entire context, achieving better performance with the same or even reduced memory footprint compared to existing methods. In benchmark tests, it enhances throughput by 25\\% and boosts batch size by 60\\% without increasing memory consumption. For challenging tasks such as Math and Code Completion, LogQuant improves accuracy by 40\\% to 200\\% at the same compression ratio, outperforming comparable techniques. LogQuant integrates effortlessly with popular inference frameworks like Python\u2019s \\texttt{transformers} library and will be made open-source upon publication.",
    "keywords": [
        "Large Language Models",
        "KV Cache",
        "Memory Efficiency",
        "Quantization"
    ],
    "primary_area": "infrastructure, software libraries, hardware, systems, etc.",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PHrqpxUczG",
    "pdf_link": "https://openreview.net/pdf?id=PHrqpxUczG",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes LogQuant, a KV cache quantization method for improving the memory efficiency and throughput of LLM inference.  Previous methods, such as KIVI and StreamingLLM, assume the most recent tokens and the first few sink tokens are more important for model performance. This work observes using log-distributed recent tokens are better for preserving model accuracy. Hence, the authors propose LogQuant, which stores a set of tokens at log-distributed positions in full precision, while keeping all other tokens quantized to achieve KV cache compression. Empirical evaluations show that LogQuant outperforms KIVI at 2-bit quantization."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. This paper studies an important problem.\n2. The presentation, including the figures and tables, are overall good."
            },
            "weaknesses": {
                "value": "1. The proposed method lacks novelty. The proposed LogQuant is highly similar to KIVI: they both use integer quantization with a fixed-sized full-precision cache, and the only difference is the selection method for the full-precision tokens. This work is also similar to mixed-precision approach for KV cache quantization such as [1,2], which identify outlier tokens in the KV cache and preserve in higher precision or full precision.\n2. The token selection process of the full-precision cache in LogQuant is fixed and non-adaptive. The token selection is determined only by token position, and not dependent on attention score or token importance. As the authors illustrate in Figure 2, the outliers in attention score do not follow a fixed pattern. Hence, it is questionable whether using a fixed pattern of full-precision cache improves the accuracy of  KV cache quantization universally for all downstream tasks.\n3. The experiments are not comprehensive. 4-bit quantization offers better quality than 2-bit quantization, and it is missing from the experiments. The baseline KIVI is also missing in the memory usage and throughput comparison in Figure 7.\n\nReferences\n\n[1] He, Yefei, et al. \"ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification.\" NeurIPS 2024.\n\n[2] Dong, Shichen, et al. \"QAQ: Quality Adaptive Quantization for LLM KV Cache.\" arXiv preprint arXiv:2403.04643 (2024)."
            },
            "questions": {
                "value": "1. How does LogQuant compare with KIVI for 4-bit quantization? And how do they compare in terms of inference latency and memory usage?\n2. In Table 2, is LogQuant achieving better accuracy than KIVI using equal or less memory budget?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents LogQuant, a new 2-bit quantization approach for compressing the KV cache in LLM inference. This approach applies a log-based filtering mechanism that enables significant memory savings while preserving performance. Unlike traditional methods that prioritize recent tokens or rely on predicted attention patterns, LogQuant uses a log-distributed approach to selectively compress tokens. LogQuant shows improvements in throughput (by 25%) and batch size (by 60%), and reportedly improves task accuracy for complex tasks like math and code completion by 40-200% at similar compression ratios."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This work is based on interesting findings of attention score distribution among token positions.\n- Leveraging a log distribution for token selection is innovative and addresses a core limitation in existing KV cache compression methods, improving the balance between memory use and performance.\n- The method\u2019s compatibility with popular inference frameworks makes it easily adaptable."
            },
            "weaknesses": {
                "value": "- The evaluation only compares with KIVI [1] on task performance, which lacks a broad comparison with other compression methods. A more comprehensive range of baseline methods\u2014like KVQuant [2], SKVQ [3], etc. Other types of compression methods [4, 5] under similar compression rates can be included. Also compression settings like 4-bit quantization\u2014would provide a fuller view of its strengths and trade-offs.\n- For many tasks, LogQuant results in a substantial accuracy drop (10+ points in some cases), which raises concerns about its reliability in sensitive tasks.\n- This work does not sufficiently discuss the overhead from operations like slicing and concatenating.\n- In many cases, the model experiences unexpectedly large accuracy drops, compared to numbers in other KV cache compression methods [1,2,3].\n\n[1] Kivi: A tuning-free asymmetric 2bit quantization for kv cache\n\n[2] Kvquant: Towards 10 million context length llm inference with kv cache quantization\n\n[3] SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models\n\n[4] Flexgen: High-throughput generative inference of large language models with a single gpu\n\n[5] Atom: Low-bit quantization for efficient and accurate llm serving"
            },
            "questions": {
                "value": "- Could you expand the comparisons to include other compression strategies, especially those operating at similar compression rates? And what are the performance results for LogQuant under 4-bit quantization?\n- Could you provide discussion and profiling of the overhead from additional operations, such as slicing and concatenating, to quantify their impact on throughput?\n- What are the results on models like LLaMA3-8B (3.0), Mistral, and Lonchat? Refer to settings of https://github.com/henryzhongsc/longctx_bench [1].\n\n[1] Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable approaches"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new KV-cache quantization algorithm called LogQuant.\n\nThe design of LogQuant relies on the empirical observation that the position of high-attention spikes follows a log distribution\u2014namely, it becomes sparser as tokens move further from the current position. Accordingly, LogQuant keeps non-quantized cache entries with respect to the observed distribution and quantizes the rest to INT2.\n\nThe paper also proposes an interesting throughput optimization by observing that $Softmax(Q \u00b7 K^T)$ and $V$ positions can be reordered without changing the computation's outcome.\n\nEvaluation results over several LLMs and benchmarks show that LogQuant is more accurate than KiVi."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper makes the interesting observation that high-attention positions follow a log-like distribution. This observation can help guide the design of future approaches.\n-\tThe reordering observation is interesting and can help improve future approaches' inference speed.\n-\tThe empirical results seem to set the new SOTA for 2-bit KV-cache quantization"
            },
            "weaknesses": {
                "value": "-\tSome design choices are not compelling:\n\n    o\tThe quantization scheme of using INT2 is arbitrary and should be elaborated on.  \n\n    o\tThe design choice of which token to keep accurate is somewhat arbitrary. One can achieve a sparse pattern according to the desired distribution in different ways. \n\n-\tThe evaluation leaves more to be desired:\n\n    o\tThe prompt and generation lengths appear to be very small. Testing with larger context windows and generated sequences (e.g., 128K) would improve the paper.\n\n    o\tThere is no evaluation of the quantization error of the tokens that are not quantized. \n\n-\tThe degradation compared to the baseline is significant, and it is unclear where such a compromise can be acceptable."
            },
            "questions": {
                "value": "- What would the performance of LogQuant be if the quantized tokens were discarded completely? \n- What group size is used in INT2? Is this overhead taken into account when comparing to other schemes?\n\n- What about picking the retained accurate tokens in a way that preserves the distribution but non-deterministically? \n\n- Will the accuracy of a model with LogQuant be better than that of a smaller baseline model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents LogQuant, a KV-cache quantization technique based on the observation that the positions of high-attention spikes follow a log distribution."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper points out an interesting observation regarding the distribution of the positions of high-attention spikes. \n- The paper targets the KV-cache quantization of LLM, which has a high importance in deploying LLM."
            },
            "weaknesses": {
                "value": "- The paper is mostly based on observation and proposes a heuristical solution. \n- The presentation of LogQuant (mostly section 3) requires revision to clarify the solution better."
            },
            "questions": {
                "value": "- The authors use the terms \u201cquantization\u201d and \u201ccompression\u201d alternatively, however, these terms have different meanings.\n- I couldn\u2019t find which quantization/compression technique is used in LogQuant. Furthermore, I found Figure 5 (and its caption) unclear."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper 1) observes the log-distribution patterns of attention score magnitudes and attention spike locations, 2) introduces LogQuant to exploit these properties, and 3) shows that their approach can achieve higher accuracy and competitive throughput at the same compression ratio. LogQuant achieves this by applying a log-based filtering mechanism in the 2-bit quantization of the KV cache."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper observes an interesting phenomenon in the attention score pattern and designs a simple but effective KV cache quantization framework around it.\n2. The paper is well-written."
            },
            "weaknesses": {
                "value": "1. Latency is the most important metric of efficiency, but it was not compared in the experiments.\n2. The efficiency comparison does not use real-world inference traces and only includes a naive BF16 baseline.\n3. The accuracy comparison only includes quantization baseline KiVi but no eviction-based baseline."
            },
            "questions": {
                "value": "Given that LogQuant and PartialLogQuant excel at different tasks, how do you decide when to use LogQuant or PartialLogQuant?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}