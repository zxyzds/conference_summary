{
    "id": "PxBzxO02Ef",
    "title": "LVP: Language-guide Visual Projector for Efficient Multimodal LLM",
    "abstract": "Visual projector plays a crucial role in bridging the visual model and the large language model (LLM) in modern multimodal LLM. \nTypically, MLLMs utilize a simple MLP to preserve all visual tokens, causing a heavy computational burden and redundant visual tokens.\nSome recent works adopt either a resampler or an adaptive pooling to reduce the visual tokens. However, they only reduce the visual tokens based on the image feature,\nleading to the feature misalignment between visual tokens and text tokens. In this paper, we present a novel Language-guidance Visual Projector (LVP), where the text \nfeature serves as a guide to selecting the important visual tokens. Specially, we first adopt a lightweight text encoder to extract the text feature. Then, a lightweight\ncross-modal feature enhancement module is proposed to enhance the cross-modal feature alignment. Finally, we select the important visual tokens according to the feature similarity between visual tokens and text tokens and apply\na deformable attention module to integrate the visual features from the visual encoder into the selected visual tokens. We further propose a multi-level language-guidance visual projector, which selects the visual tokens from different stages of the visual encoder.\nExtensive experiments demonstrate that our LVP compresses the visual tokens by 75\\%~95\\% while achieving competitive even better performance across diverse benchmarks with a significant efficiency advantage. For instance, LLaVA1.5-LVP with Vicuna-13B\nobtains 71.8\\% accuracy on VQA$^\\text{T}$, realizing the state-of-the-art result.",
    "keywords": [
        "Multimodal large language model",
        "Visual projector",
        "Language-guide visual token selection."
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PxBzxO02Ef",
    "pdf_link": "https://openreview.net/pdf?id=PxBzxO02Ef",
    "comments": [
        {
            "summary": {
                "value": "This paper discusses advances in multimodal large language modeling by presenting a novel language-guided visual projector (LVP).The LVP utilizes textual features to guide the selection of visual tokens, which enhances the consistency between visual and textual data and improves computational efficiency. It employs a cross-modal feature enhancement module and a multi-level approach to capture fine-grained features, reducing the number of visual markers while maintaining performance. Experiments show that LVP achieves state-of-the-art results using fewer visual markers, demonstrating its effectiveness in multimodal tasks. Its main contributions include the innovative use of linguistic knowledge to optimize visual marker selection and improve multimodal learning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The paper introduces a new approach to align visual and text tokens by using language features to guide the selection of essential visual tokens, which is novel in reducing token input for LLMs.\n2) LVP improves the alignment between text and visual features with a cross-modal enhancement module, facilitating better integration of multimodal data.\n3) By reducing visual tokens to 25% of the original input, LVP maintains high performance while enhancing computational efficiency in text generation tasks.\n4) The multi-level projector design captures both detailed and global visual features, enhancing the system\u2019s versatility and adaptability across different tasks."
            },
            "weaknesses": {
                "value": "1) The generalization of the method mentioned in the paper in the scenario of multi-round conversations with multi-modal large models is yet to be proved.\n2) The structure proposed in the paper is similar to the one mentioned in paper LXMERT: Learning Cross-Modality Encoder Representations from Transformers, with an additional step of visual token selection."
            },
            "questions": {
                "value": "1) It is suggested to add relevant experiments to show that LVP still has excellent performance in the scenario of multi-round conversations with large multimodal models;\n2) Specify the innovativeness of the structure, and make a distinction and appropriate citation from the more popular structures previously developed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces a language-guided visual projector for multimodal large language models (MLLMs).  LVP first proposes a lightweight text encoder to extract text features, followed by the introduction of a cross-modal feature enhancement module and a multi-level language-guidance feature selection module to generate compressed tokens. Experimental results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The performance is impressive compared to existing methods.\n2. The research focus is both interesting and effective."
            },
            "weaknesses": {
                "value": "1. The presentation of this paper is similar to that of TokenPacker.\n2. In lines 245-251 on page 5, the feature injection using enriched features through cross-attention shares a similar idea with Mini-Genimi and TokenPacker, although you employ deformable attention in this work.\n3. Minor issue: In Table 1, line 344, the value 53.9 should be in bold with the best performance, instead of 53.5."
            },
            "questions": {
                "value": "Considering the impressive performance, will the code be released publicly to facilitate reproduction of the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Emergency review for Submission #6390\n\nThis paper presents the Language-guide Visual Projector (LVP) for multimodal Large Language Models (MLLMs). Existing visual projectors in MLLMs have issues like heavy computational burden or feature misalignment. LVP uses text features to guide the selection of important visual tokens, with a lightweight text encoder, a cross-modal feature enhancement module, and a deformable attention module. In addition, the authors propose a multi-level language-guide visual projector to generate the visual tokens from different stages of the encoder. Experiments show that LVP compresses visual tokens by 75% - 95% and achieves good performance across various MLLM benchmarks."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The advantages of the submission are obvious:\n\nS1. The research on reducing tokens is quite useful. The methodology of the submission is also technically sound.\n\nS2. The performance of the proposed is very good. It beats a lot of baselines. The MME score is also very high.\n\nS3. This paper is also well-written and is easy-to-follow.\n\nS4. According to the visualization, the performance of author's method is also sound."
            },
            "weaknesses": {
                "value": "There are three weaknesses from my point of view.\n\nW1. Compared with image tasks, the research on video tasks can better highlight the authors' motivation. We know, there is a lot of redundant information in video. Therefore, I suggest the authors extend their paper to video tasks as in VideoLLaMA and LLaVA-OV.\n\nW2. I suggest the authors present TPS in Tab.1 and Tab.2. \n\nW3. As a submission to ICLR, the paper should have some discussions on the representations of the visual token. Some related experiments need to be conducted."
            },
            "questions": {
                "value": "I have some questions.\n\nQ1. Why the datasets evaluated in Tab.1 are not in line with those of Tab.2? Can you report them in total?\n\nQ2. What is the influence of your method applied to video tasks?\n\nQ3. The idea of using text features has already been discussed in InstructBLIP. What are the advantages of your technique compared with InstructBLIP?\n\nQ4. How $N_q$ influence the last model performance?\n\nQ5. Why deformable attention is utilized. How about a regular self-attention?\n\nQ6. Is it possible to quantitatively or qualitatively analyze the representation of tokens input to LLM?\n\nQ7. The visual feature encoder and LLM used by the author are relatively outdated. I know this is for a fair comparison. I want to know how good the performance would be if you consider SigLIP and QWEN2.5 and develop a version under the high-resolution setting.\n\nJustification on rating:\n\nI think the proposed method is useful. I have no firm reasons to reject this paper. I will stick to a positive one if all my concerns are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a visual projector called LVP, designed to compress visual tokens for efficient multimodal\nlarge language models (LLMs).  LVP is a language-guided visual projector in which text features serve as a guide for \nselecting important visual tokens based on feature similarity. Extensive experiments are conducted to validate the performance of LVP."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The text-guided selection for visual token compression presents a novel approach to the task.\n2. Compared to previous token compression methods, LVP achieves state-of-the-art performance \nunder identical experimental conditions."
            },
            "weaknesses": {
                "value": "1. The overall discourse of this paper closely resembles the previous research on TokenPacker, \nparticularly in the second and third paragraphs of the Introduction, the first and second paragraphs \nof the Related Work section, and the descriptions of the datasets and main comparison results in the Experiments section. \nThis version of the paper requires substantial revisions to eliminate textual repetition.\n\n2. In Figure 2, the visual attention map should indicate which layer in the LLM it corresponds to.\n\n3. In the high-resolution experiments, this paper adopts the same dynamic image slicing method proposed in TokenPacker-HD with resolutions of 1088x1088 and 1344x1344. If this is the case, it should be clarified."
            },
            "questions": {
                "value": "I noticed that in Table 3, the performance of 64 tokens (9x compression ratio) utilizing multi-level features achieves comparable results to that of the MLP with 576 tokens, which is impressive. \n\nI am concerned whether the code will be made publicly available to facilitate better reproduction of results. \nI encourage the authors to release the code to benefit the field.\n\nI lean to the positive if the authors can resolve my concerns during the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}