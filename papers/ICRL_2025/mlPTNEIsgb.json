{
    "id": "mlPTNEIsgb",
    "title": "Solving Blind Non-linear Forward and Inverse Problem for Audio Applications",
    "abstract": "We propose a novel framework to address the blind forward and inverse problems, where the goal is to infer either the function or the input signal solely from the output signal without access to the other. These problems are challenging due to the highly nonlinear and complex nature of the forward operator\u2014the function mapping between the input and output signals. To tackle the blind forward problem, we introduce a reference encoder that analyzes the reference output (wet signal) to approximate an arbitrary forward operator. For the blind inverse problem, the approximated forward operator is leveraged to guide both the training and inference stages of a diffusion model, enabling a more accurate reconstruction of the input signal. Furthermore, we propose a twisted particle filtering method to enhance the model's performance in solving the inverse problem. Our framework significantly improves the handling of these complex signal-processing tasks, offering new insights into forward and inverse problem-solving strategies in nonlinear systems. Codes are available at \\url{https://github.com/BlindForwardInverse/BlindForwardInverseAnonymous}",
    "keywords": [
        "audio effect",
        "zero-shot system identification",
        "inverse problem",
        "diffusion model",
        "sequential monte carlo"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mlPTNEIsgb",
    "pdf_link": "https://openreview.net/pdf?id=mlPTNEIsgb",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a framework for blind estimation of a forward operator and solving the corresponding inverse problem with application in audio signal transformation. The forward operator is estimated using a neural model consisting of a reference encoder and a \u201cmain\u201d neural operator. The inverse problem is solved using a score-matching diffusion model. The authors show results on audio effect learning and speech enhancement."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Training pipeline including creating a DAG for the forward operator is interesting, and appears to model more complex forward operators than used in the previous work, e.g., (Rice, 2023)."
            },
            "weaknesses": {
                "value": "The paper proposes a model for estimating the forward operator, and a score-based model for solving the corresponding inverse problem.\nThe most relevant existing work is (Rice, 2023). However, that is mentioned only in passing, and the difference or the advantages of the proposed model are not clearly presented. Furthermore, there's not a single baseline system used in the experimental section.\nThis reviewer would suggest, at minimum, to include (Rice, 2023) as a baseline. Furthermore, it would be helpful to include some of the recent diffusion-based speech enhancement models as baselines in the speech enhancement experiment (for example, score-matching based SGMSE).\n\nSurprisingly, for a paper dealing with estimating audio, the authors found not provide a single audio example.\nAt minimum, the authors should provide randomly-sampled test example pairs of input and output audio (for different processing setups).\nA link to the examples should be provided in the paper.\n\nThe paper is unfinished and clearly not well prepared for peer review.\nFor examples, Tables {2,3,4} are not referred to in the paper.\nFootnote 2 on page 10 is particularly interesting, where the authors claim that:\n\"The numbers of the table are relying on the model with insufficient training iteration, thus will be updated during reviewing process.\"\nIn this reviewer's opinion, submitting incomplete results is unacceptable.\nThe authors should have completed the experiments before the initial submission, and all tables should have been properly referenced."
            },
            "questions": {
                "value": "\"Dry and wet signals\" naming is used on page 1, but never formally introduced. While that's a common naming used in audio effects, it's not the ideal choice when handling, for example, speech enhancement. The authors could change that to make it more clear. Using general terminology, such as model input and model output or estimated signal, would be more appropriate."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a framework for solving blind nonlinear forward and inverse problems with applications to audio. Specifically, the authors propose using a reference encoder module to approximate an arbitrary forward function for the forward nonlinear problem. Then, based on the estimated forward operator, they propose solving the inverse problem using guidance from the estimated forward function to train the diffusion model for recovering the input signal. Improvements to the main framework are also discussed by using a particle filter-based approach for the inverse problem. Experimental results on audio effects modeling for the forward problem and speech enhancement for the inverse problem are presented to demonstrate the effectiveness of the proposed method, with both objective and subjective evaluations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed framework is theoretically grounded and can be applied to solving both nonlinear forward and inverse problems.\n\n- Robust performance to distribution shift and recording environment mismatch of the training and test audio data; enabling zero-shot audio effect learning.\n\n- Effectiveness of the proposed method across a wide range of audio effects and manipulations, supported by both objective metrics and subjective listening testing scores."
            },
            "weaknesses": {
                "value": "- The main weakness of the work is the lack of comparison with existing approaches for better understanding of where the proposed method stands. For example, for the inverse problem of speech enhancement, the authors could have compared their approach to other diffusion based speech enhancement methods (e.g., *Lu et al., \"Conditional Diffusion Probabilistic Model for Speech Enhancement,\" ICASSP, 2022*; *Tai et al., \"DOSE: Diffusion Dropout with Adaptive Prior for Speech Enhancement,\" NeurIPS, 2023*) on more standard benchmark datasets such as VoiceBand+DEMAND (*Cassia et al., \"Noisy speech database for training speech enhancement algorithms and TTS models,\" 2016*) and CHiME-3 (*Vincent et al., \"An analysis of environment, microphone and data simulation mismatches in robust speech recognition,\" Computer Speech Language, 2017*). Having such comparison to prior works will help strengthen the contribution of this paper.\n\n- Another weakness is the lack of information on the model size, computational complexity, or real-time processing performance of the proposed model. From a practical perspective, many speech and audio processing applications have certain constraints on the latency and memory requirements for the models to be deployed on edge devices (e.g., smart speakers, intelligent home appliances, hearing aids, etc). Therefore, providing relevant information would be helpful for researchers and developers to consider adopting the proposed model."
            },
            "questions": {
                "value": "- In Definition 1 (Forward Operator), it is assumed that both $x\\in K$ and $y\\in K$, where $K\\subseteq X=\\mathbb{R}^T$. Does it mean that the proposed framework is only applicable to the case where the input $x$ and output $y$ have the same signal length $T$? Could you provide further details on how the proposed framework can be potentially extended to other inverse problems where the input and output signals may have different dimensionality?\n\n- In eq. (1), what does $\\mathcal{A}\\in\\mathcal{C}(K)$ mean?\n\n- At line 159, maybe directly using $c_g$ for $t$ and $c_l$ for $c$ makes it more clear as they refer to the same thing. This could also help the reader understand what $c_g$ and $c_l$ are in the subsequent sentence at line 161.\n\n- In Figure 2, what are the upper-case $X$ and $Y$? It looks like they are the spectrogram representations of $x$ and $y$, respectively, but such information is missing. In addition, do you use the complex-valued spectrograms or just their magnitude components for training the model?\n\n- In Table 3, what do \"Mix\", \"Blind\" and \"Known\" stand for respectively?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a framework for solving blind forward and inverse problems in audio effect modeling and speech enhancement, where the authors aim to recover the applied audio effect or remove degradations without prior knowledge of the effect chain. The approach includes a dynamic pipeline that generates paired dry and wet signals dynamically and a reference encoder that conditions the network to apply or reverse audio effects. The method is evaluated through objective metrics, with code being released to support reproducibility."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper introduces a dynamic signal pairing pipeline that enhances the adaptability of the model across diverse audio effects and environments, a practical contribution to real-time audio processing.\n- Using a reference encoder for conditioning allows the model to handle multiple unknown degradations in a versatile manner, making it potentially useful in scenarios requiring \"real-time\"/zero-shot  adaptability.\n- Objective metrics provide an initial evaluation of performance across varied conditions."
            },
            "weaknesses": {
                "value": "- Theoretical and Practical Limitations: The model employs a Directed Acyclic Graph (DAG) with a semiring-based approach to formalize audio effect chains, which could theoretically add rigor if the properties (like associativity and distributivity) were applied explicitly to audio effects. However, without clear practical benefits, the connection risks remaining overly theoretical. The paper could benefit from further exploration of how this semiring-DAG structure enhances computation or generalization in practice.\n- Irreversible Transformations and Restoration Quality: Certain audio effects, such as clipping and band limiting, are irreversible and introduce permanent information loss. While restoration is theoretically possible if the model learns the underlying signal distribution, the quality of restoration is critical and must be rigorously evaluated. In particular, reconstructing fine details - like distinguishing between high-frequency fricative sounds (e.g., \"s\" and \"f\") - may be inherently ambiguous after irreversible effects. This limitation should be acknowledged in the framework to set realistic boundaries on reconstruction accuracy.\n- Importance of Perceptual Validation: While the objective metrics provide quantitative insight, they are insufficient to fully assess restoration quality, especially in cases involving irreversible transformations. An actual subjective evaluation through listening tests is essential to validate whether the restored audio meets perceptual quality standards, as small deviations from the original can significantly affect listener experience. The lack of subjective tests here limits the confidence in the model\u2019s real-world applicability.\n- Limited Comparative Evaluation and Reproducibility: The absence of task-specific baseline comparisons detracts from a clear evaluation of the method\u2019s effectiveness. Additionally, the lack of accessible qualitative examples (e.g., audio samples) limits reproducibility and a deeper evaluation of perceptual quality.\n- Result Interpretation: Although the results section provides metrics, a subjective evaluation is missing."
            },
            "questions": {
                "value": "- How does the proposed method compare to task-specific baselines in terms of objective and perceptual quality?\n\n- Could the model incorporate assumptions about the original signal distribution to enhance the robustness of the restoration, and would this improve restoration quality?\n\n- In the context of semiring theory, could the properties (like associativity and distributivity) of operations in the DAG be explicitly defined to enhance computation or generalization?\n\n- To improve validation, could a listening test be added to evaluate the perceptual acceptability of restored audio, especially in cases where irreversible transformations have introduced ambiguity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel framework for solving blind forward and inverse problems using diffusion models. The approach consists of two main steps:\n\n1. Training an encoder network that takes a transformed signal (wet signal) as input and outputs the parameters of the transformation process (forward operator).\n2. During inference, using the parameters estimated by this encoder network to solve the inverse problem, with the diffusion model serving as a prior distribution.\n\nThe framework is applied to address the following problems:\n\n- Estimating the original signal (dry signal) from a wet signal created by audio effects with unknown parameters.\n- Speech enhancement, which involves restoring the original speech signal from degraded speech that has undergone various processing.\n\nThis approach enables the solution of inverse problems where the transformation process is unknown, leveraging the power of diffusion models as priors."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This research topic is important both theoretically and practically as it explores new applications using diffusion models as a foundation. A particularly noteworthy aspect is the potential to solve blind inverse problems with minimal additional training."
            },
            "weaknesses": {
                "value": "I find it difficult to recommend this paper for acceptance due to the following reasons. Detailed comments and questions are provided in the Questions part.\n\n1. There are several parts of insufficient explanations. For example, the meaning of 'hallucination effect' on line 57 is unclear, and the caption for Figure 1 is incomplete. Addressing these and other similar issues would improve the paper.\n2. While the paper contains a lot of mathematical descriptions, many terms are undefined or seem unnecessary for the discussion. Although some meanings can be inferred, the lack of proper citations makes it challenging for readers to follow the discussion accurately. For instance, A_{\\theta}^{\\dagger} on line 145 and c_g, c_l on line 161 are not properly defined. Additionally, Proposition 1 and Theorem 1 are stated without proofs.\n3. The problem addressed in this paper appears to be inadequately defined, and it's unclear from the paper whether the method described in section 6.2.3 actually solves this problem.\n4. A major issue is that the experimental results in Section 7 do not include any comparative methods. This makes it impossible to discuss the performance advantages of the proposed method."
            },
            "questions": {
                "value": "I would like to request clarification from the authors on the following points:\n\n1. Please provide evidence for the claim near line 49 \u201cthat generative models are inferior to discriminative models in terms of quality\u201d.\n2. Explain the meaning and relevance of the statement on line 57 that particle filtering techniques reduce the 'hallucination effect'.\n3. In Definition 1, if a pure noise-adding operator (as might be expected in speech enhancement tasks) is unbounded, is it excluded from the forward operators considered in this paper?\n4. Complete the explanation following '(Right)' in Figure 1.\n5. Clarify the definition of 'forward problem' mentioned on line 129.\n6. Provide the definition of A_{\\theta}^{\\dagger} on line 145.\n7. Define c_g and c_l mentioned on line 161.\n8. Explain what 'sequence length' refers to on line 188.\n9. Demonstrate how the training objective in section 4.3 is derived from the objective function defined in section 3.\n10. Provide the proofs for Proposition 1 and Theorem 1.\n11. Explain how the results of Theorem 1 contribute to the paper's main arguments.\n12. Describe the derivation method for m(x_t) and C(x_t) in equation (8) of section 6.4.\n13. Can you add comparative methods to section 7?\n14. Is it possible to provide audio samples in the experimental section, e.g., audio samples corresponding to the spectrograms shown in Figures 6-9?\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}