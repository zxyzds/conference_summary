{
    "id": "XLt0eudh8t",
    "title": "Efficient Neural Common Neighbor for Temporal Graph Link Prediction",
    "abstract": "Temporal graphs are ubiquitous in real-world scenarios, such as social network, trade and transportation. Predicting dynamic links between nodes in a temporal graph is of vital importance. Traditional methods usually leverage the temporal neighborhood of interaction history to generate node embeddings first and then aggregate the source and target node embeddings to predict the link. However, such methods focus on learning individual node representations, but overlook the pairwise representation learning nature of link prediction and fail to capture the important pairwise features of links such as common neighbors (CN). Motivated by the success of Neural Common Neighbor (NCN) for static graph link prediction, we propose \\textbf{TNCN}, a temporal version of NCN for link prediction in temporal graphs. Based on a memory-based backbone instead of traditional static graph neural network, TNCN dynamically updates a temporal neighbor dictionary for each node, and utilizes multi-hop common neighbors between the source and target node to learn a more effective pairwise representation. We validate our model on five large-scale real-world datasets from the Temporal Graph Benchmark (TGB), and find that it achieves new state-of-the-art performance on three of them. Additionally, TNCN demonstrates excellent scalability on large datasets, outperforming popular GNN baselines by up to 6.4 times in speed.",
    "keywords": [
        "Temporal graph",
        "Neural common neighbor",
        "Efficient"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=XLt0eudh8t",
    "pdf_link": "https://openreview.net/pdf?id=XLt0eudh8t",
    "comments": [
        {
            "summary": {
                "value": "This work proposes TNCN, which is  a temporal version of NCN based on a memory-based backbone. Comprosing with three key parts: the memory module,the temporal CN extractor,,and the NCN-based prediction head, TNCN improves the performance in terms of efficiency and effectiveness. Comparing with a diverse set of baseline models, the experiments on five datasets demonstrate its outstanding performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The experiments is substantial and the result is good.  Comparing with 9 baseline models, TNCN performs best on three of the five selected datasets, which emphasizes its effectiveness.\n2. The method section is clearly described using formulas. With clear definition and detailed formulas, the method is well-presented.\n3. There are proofs on the theorems in appendix, which improves the professionalism of the paper."
            },
            "weaknesses": {
                "value": "1. Since the process is relatively complicated, it is recommended to provide a pseudo code to make it easier for readers to understand.\n2. I suggest that the experimental part be supplemented with an analysis of the hyperparameters, which can make the values of the hyperparameters more reasonable."
            },
            "questions": {
                "value": "1. High surprings values will degrade TNCN's performance. So will the model performance decrease monotonically as the surprise value increases? In other words, will TNCN have the best performance when the surprise value is the lowest?\n2. Why is the result of TNCN-0\u223c2-hop-CN lower than TNCN-0\u223c1-hop-CN on the commen dataset in Table 2, which is different from the other three datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method for link prediction on continuous-time dynamic graphs (CTDGs), aiming to unify two prominent model families: memory-based models and neighbor-based models. The authors introduce TNCN, demonstrating experimentally that it performs better in certain configurations and is more efficient than existing models in the literature."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- **Evaluation**: The proposed model is evaluated on established benchmarks, enhancing the reliability of the results.\n\n- **Engineering**: The paper introduces an engineering approach to combine common neighbor (CN) techniques with memory-based methods, integrating these two modeling approaches."
            },
            "weaknesses": {
                "value": "**Presentation**: The abstract implies that common neighbor methods are primarily used in static graphs, overlooking their established role in dynamic graph modeling. Additionally, the motivation for combining memory-based and neighbor-based techniques is presented only briefly at the end of the introduction.\n\n**Limited Novelty**: The proposed model's core components primarily consist of established techniques. For instance, the memory-based module closely resembles TGN, lacking additional innovations or a clear positioning relative to other memory-based approaches. Similarly, while TNCN incorporates common neighbor (CN) techniques with optimized computation, it employs multi-hop methods similar to those in models like CAWN, further limiting its novelty. \n\n**Theoretical Claims**: The paper presents familiar results as novel contributions, such as the $O(\u2223E\u2223)$ memory complexity for event aggregation, along with upper and lower bounds that are established in prior work (e.g., Caen, 1998). Presenting these as new theoretical results may be misleading.\n\nThe paper exhibits strong engineering but lacks clear new contributions. Theoretical results on memory complexity and specific experimental results are weak, with minimal performance gains in dynamic link prediction tasks. Given the limitations in presentation, contribution, and experimental validation, I recommend rejection.\n\n**Experimental results**: While combining memory and neighbor-based methods is interesting, the observed performance gains are minimal, with certain results (e.g., in Table 5) showing limited improvement using unclear metrics (possibly AUC or AP)."
            },
            "questions": {
                "value": "1. Could you clarify the main novelty of your memory-based module and its differentiation from TGN?\n2. What is the specific metric used in Table 5? Understanding whether it's AUC or AP is essential for interpreting the results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The author focuses on link prediction tasks in temporal graphs, which have ubiquitous applications in real-world systems. Instead of implicitly encoding common neighbor features from the historical neighborhood of the target node, the author proposes the Temporal Common Neighbor Extractor to explicitly integrate these features into the process of temporal graph representation learning, achieving both effectiveness and efficiency compared to existing methods on the TGB benchmark. Additionally, the author provides theoretical analysis based on their proposed method, offering a more thorough illustration of the model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper has several strengths worth noting:\n\n* **Interesting Motivation.** The motivation for extracting common neighbor features is compelling.\n* **Extensive Experiments.** The author has designed a variety of experiments to demonstrate the effectiveness and efficiency of their methods.\n* **Well-Organized Representation.** The paper is well-structured, and the theoretical analysis provides strong support for the proposed methods."
            },
            "weaknesses": {
                "value": "However, the paper also has some weaknesses, outlined as follows:\n\n* **Lack of Novelty.** Firstly, the idea of extracting common neighbors in temporal graphs has certainly been explored before. It seems that the design of your key component, the \"CN Extractor,\" closely resembles existing work in KDD2024 [1]. Moreover, your CN extracting component does not appear to include any specific improvements for temporal graphs. Simply extracting \"monotone k-hop events\" does not substantiate this claim.\n* **Lack of Important Baselines.** Since you \"extend Neural Common Neighbor for static prediction methods,\" these static methods should also be included in your comparisons.\n* **Lack of Case Study.** Providing specific case studies could enhance the understanding of your method.\n* **Ambiguous Expression.** What does $emb$ mean in Equation 2? It seems you haven't explained it anywhere\u2014does it refer to memory? Your method encodes the common neighbor neighborhood composed of source-destination node pairs; might this lead to a loss of other information (e.g., nodes that are not common)?\n* **Parameter Analysis.** Given that your method is based on multi-hop common neighbors, analyzing parameters across different multi-hop settings could better validate the robustness of your model.\n\n[1] Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction, KDD 2024."
            },
            "questions": {
                "value": "Q1: In what ways does your approach build upon or differ from existing methods that also extract common neighbors in temporal graphs?\n\nQ2: Can you clarify the importance of including static baselines in your experiments, and how their absence affects the interpretation of your results?\n\nQ3: What specific case studies can you provide to illustrate the practical application and effectiveness of your proposed method?\n\nQ4: Can you provide a detailed explanation of the notation used in your equations, particularly regarding $emb$ in Equation 2, and how this impacts the overall methodology?\n\nQ5: How do you plan to conduct a thorough parameter analysis for the multi-hop common neighbors, and what insights do you anticipate this will provide regarding your model's robustness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper is about temporal link prediction, which is an interesting topic. The authors propose a temporal version of NCN for link prediction in temporal graphs, which dynamically updates a temporal neighbor dictionary for each node and utilizes multi-hop common neighbors between the source and target node to learn a more effective pairwise representation. The paper is well written and well organized. However, there are several concerns in the current version of the paper that addressing them will increase the quality of this paper."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1 Cutting-edge research directions.\n\n2 Clear writing logic.\n\n3 Sufficient experimental results."
            },
            "weaknesses": {
                "value": "1 The authors should have a special discussion on whether the biggest difference between TNCN and NCN is the core contribution of this paper. If so, this should be highlighted. If not, more introduction is needed on the importance of the new scenario.\n\n2 Since the strategy proposed in the paper is built around the batch processing mode of sequential graph learning, whether the batch size will have a different impact on the strategy is something that needs to be considered and discussed.\n\n3 The motivation and contribution of the paper are worthy of recognition, but in the main text, the authors can consider putting more emphasis on the contribution description and logical arrangement. At present, it seems that the proof takes up a certain amount of space, making the method and experiment part seem less substantial, and the information that can be expressed is not clear and comprehensive enough.\n\n4 The authors could consider discussing the computational complexity, especially comparing it to similar methods (including static graphs and temporal graphs)."
            },
            "questions": {
                "value": "As above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}