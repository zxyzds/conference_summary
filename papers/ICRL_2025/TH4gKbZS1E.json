{
    "id": "TH4gKbZS1E",
    "title": "KAN versus MLP on Irregular or Noisy Functions",
    "abstract": "In this paper, we compare the performance of Kolmogorov-Arnold Networks (KAN) and Multi-Layer Perceptron (MLP) networks on irregular or noisy functions. We control the number of parameters and the size of the training samples to ensure a fair comparison. For clarity, we categorize the functions into six types: regular functions, continuous functions with local non-differentiable points, functions with jump discontinuities, functions with singularities, functions with coherent oscillations, and noisy functions. Our experimental results indicate that KAN does not always perform best. For some types of functions, MLP outperforms or performs comparably to KAN. Furthermore, increasing the size of training samples can improve performance to some extent. When noise is added to functions, the irregular features are often obscured by the noise, making it challenging for both MLP and KAN to extract these features effectively. We hope these experiments provide valuable insights for future neural network research and encourage further investigations to overcome these challenges.",
    "keywords": [
        "Kolmogorov-Arnold networks",
        "Multi-layer Perceptrons",
        "KAN",
        "MLP",
        "Irregularization",
        "Noise"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TH4gKbZS1E",
    "pdf_link": "https://openreview.net/pdf?id=TH4gKbZS1E",
    "comments": [
        {
            "summary": {
                "value": "This work empirically compares Kolmogorov-Arnold Networks with Multi-Layer Perceptron on\nlearning irregular or noisy functions. The experiment results show that KAN do not always perform\nthe best."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Experiment codes are provided for reproducibility.\n\nDo provide some insight on what KAN may be good at modeling."
            },
            "weaknesses": {
                "value": "The finding is purely empirical.\n\nThe paper does not clearly state the experiment setting in the main text.\n\nThe experiment does not provide conclusive results.\n\nThe experiment only tries to fit relatively simple functions. The result may not be relevant to real-world problems."
            },
            "questions": {
                "value": "It is possible to include more challenging problems for comparison? It is well established that MLP can model fairly complicated functions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Authors compare the performance of Kolmogorov-Arnold Networks (KAN) and Multi-Layer Perceptron (MLP) networks on irregular or noisy functions. The author experimentally demonstrated that KAN does not always outperform MLP."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The author compared KAN and MLP on various irregular and noisy functions and experimentally demonstrated in which cases KAN is worse than MLP."
            },
            "weaknesses": {
                "value": "- The author merely compared KAN and MLP experimentally but did not analyze why KAN or MLP performs poorly in certain situations.\n\n- The author experimentally demonstrated that KAN is sometimes inferior to MLP. It would be better to propose a new, improved KAN model to address this.\n\n- There are no experiments on high-dimensional functions. In one dimension, both KAN and MLP are likely to approximate well to some extent, but more experiments are needed to explore how they perform in high-dimensional spaces with irregular points.\n\n- If the experiments are conducted only on univariate functions, many models besides MLP can be compared with KAN. It would be beneficial to include other models commonly used in machine learning in the experiments."
            },
            "questions": {
                "value": "I do not have a complete understanding of KAN, but I think KAN appears to be a generalization of projection pursuit regression. Is this correct?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper conducts a comparative analysis of experiments between MLP and KANs, discussing the outcomes. It challenges the assumption that KANs consistently outperform MLP in modeling mathematical equations, particularly with irregular functions. The experiments involve applying MLP and KAN to various functions\u2014regular, non-differentiable, discontinuous, singular, and coherent oscillation, with and without noise. These functions are single input and single output. Variations include different training sample sizes, iteration counts, and optimizers. The findings demonstrate that KANs do not always surpass MLP.\n\nWhile this paper serves as a great exploration to KANs and does establish that KANs are not invariably superior to MLP, it falls short by only providing experimental evidence without introducing new theoretical insights or network structures, thus lacking substantial academic contribution."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The structure of the paper is clear and well-organized.\nThe experimental results are clearly presented.\nThe experiments validate that KANs are not consistently superior to MLP."
            },
            "weaknesses": {
                "value": "The experiments could be designed more targeted. For instance, in the experiments for non-differentiability, both functions feature only a single non-differentiable point. A comparison between functions with single versus multiple non-differentiable points would be more insightful, given the focus on the impact of these points.\n\nThe discussion lacks depth. Given the simplicity of both the functions and network structures used, there is potential for a more detailed examination of how parameters are trained and the reasons behind specific outcomes.\n\nThe discussion section does not yield any intriguing or unexpected conclusions, nor does it propose any novel theories or structures."
            },
            "questions": {
                "value": "Given that the structures of both MLP and KANs are well-known, a deeper analysis of their capabilities and limitations in the related work section would be beneficial. More thorough research could uncover more significant findings. For instance, some limitations of KANs identified in the paper are not due to the Kolmogorov-Arnold Theorem but rather due to B-spline, a critical component in KANs that is not discussed in the paper at all."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this empirical study, the authors compare the performance of two ad-hoc versions of KAN and MLPs, both with identical parameter counts, in learning ten single-dimensional real functions, with and without added noise."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The work has considered different classes of functions with some common irregularities. The empirical comparison is sound, and extensive plots provided let's the reader to compare the performance of tested KAN and MLPs in each case."
            },
            "weaknesses": {
                "value": "The comparison primarily centers on the resulting test accuracy curves; however, it lacks the necessary theoretical justification and fundamental analysis to substantiate the findings.\n\nWhile the authors have structured the text well, the plots are somewhat cluttered and could be presented more effectively for better clarity. \n\nOverall, the work appears basic and does not demonstrate the level of novelty typically expected from submissions to ICLR."
            },
            "questions": {
                "value": "If authors can provide some theoretical insights backing the observed empirical results in all or some of the function classes tested, it would make the work more promising and considerable for this conference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The authors cite a paper with \"our work\" in the first page, fourth paragraph of Introduction. This seems to be an instance of violation of anonymity of the authors in the double-blind review process."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}