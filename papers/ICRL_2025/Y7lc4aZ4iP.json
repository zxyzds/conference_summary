{
    "id": "Y7lc4aZ4iP",
    "title": "On Rademacher Complexity-based Generalization Bounds for Deep Learning",
    "abstract": "We show that the Rademacher complexity-based approach can generate non-vacuous generalisation bounds on Convolutional Neural Networks (CNNs) for classifying a small number of classes of images. The development of new Talagrand's contraction lemmas for high-dimensional mappings between vector spaces and CNNs for general Lipschitz activation functions is a key technical contribution. Our results show that the Rademacher complexity does not explicitly depend on the network length for CNNs with some common types of activation functions such as ReLU, Leaky ReLU, Parametric Rectifier Linear Unit, Sigmoid, and Tanh.",
    "keywords": [
        "Deep Learning",
        "CNN",
        "Generalisation Errors"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Y7lc4aZ4iP",
    "pdf_link": "https://openreview.net/pdf?id=Y7lc4aZ4iP",
    "comments": [
        {
            "title": {
                "value": "Reply to Reviewer's comments"
            },
            "comment": {
                "value": "Thank you much for your comments on our paper. The following are our answers to your questions:\n\n1. This paper only considers some specific types of activation functions\n\nAnswer:  Although our developed bounds are for any activation functions, but we think that they are only non-vacuous for CNNS with some special classes of activation functions. We believe that achieving non-vacuous bounds is a challenging task in deep neural networks in general. \n\n2.  The conclusion that the obtained Rademacher complexity does not explicitly depend on the depth of CNNs is not very convincing, since by Equations (30), (31), (34), the Rademacher complexity may exponentially depend on the depth of CNNs if the right-hand-side of (30), and (31) are large than 1. More discussions on this claim should be added.\n\nAnswer: In our work, by using Theorem 8 (or Theorem 2), we show that for CNNs or DNNs with some special classes of activation functions (ReLU, Leaky ReLU, PRLI, Sigmoid, and Tanh or odd functions in general), the Rademacher complexity can be upper bounded by\n$$\nO\\bigg(\\sqrt{\\frac{1}{n}}\\prod_{j=1}^L \\mu_j M_{\\infty} (j) \\bigg),\n$$ where $M_{\\infty} (j)$ is the infinity-norm of the weight matrix of the $j$-the layer, and $\\mu_j$ is the Lipschitz constant of the activation in the $j$-the layer. Note that $|\\sigma_j(0)|/\\sqrt{n}$ is usually very small compared to $\\mu_j M_{\\infty} (j)$. \n\nHence, under the assumption of a bounded product of the norms of weight matrices, our derived bound is independent of the network length. This represents a significant improvement over the bounds established by Golowich et al., particularly for deep neural networks (DNNs) with extensive network lengths. We have adopted the term \"size-independent\" from Golowich et al.'s research to convey this concept. Additionally, we highlight that our results apply to a broader range of activation functions, extending beyond the ReLU activations that were primarily considered in their work. \n\nWe will ensure to add \"under the assumption of a bounded product of the norms of weight matrices\" before stating that \"the bound is network-depth independent\" in our revised version.\n\nBased on our contributions, we look forward to receiving your better score."
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer's comments (next)."
            },
            "comment": {
                "value": "Next, we provide answers to your questions:\n\n1. About the remark 3: \nThe second bullet in remark 3 states that theorem 2 improves the bound for the case $m=1$. However, in Lemma 1 (Talagrand's lemma for $m=1$), the contraction rate is $\\mu$, (not $2\\mu$).\n\nAnswer: The Talagrand's contraction lemma in Lemma 1 is for \n$$\n\\mathbb{E}_{\\epsilon}\\bigg[\\sup\\_{h \\in \\mathcal{H}} \\sum\\_{i=1}^n \\epsilon\\_i (\\psi \\circ h)(\\mathbf{x}_i) \\bigg]. \n$$\n\nIf the expression inside the bracket is absolute value, i.e., \n$$\n\\mathbb{E}_{\\epsilon}\\bigg[\\sup\\_{h \\in \\mathcal{H}} \\bigg|\\sum\\_{i=1}^n \\epsilon\\_i (\\psi \\circ h)(\\mathbf{x}_i)\\bigg| \\bigg],\n$$ \n\nthen it is well-known that (Ledoux & Talagrand, 1991, Theorem 4.12)\n\n$$\n\\mathbb{E}_{\\epsilon}\\bigg[\\sup\\_{h \\in \\mathcal{H}} \\bigg|\\sum\\_{i=1}^n \\epsilon\\_i (\\psi \\circ h)(\\mathbf{x}_i)\\bigg| \\bigg]\n$$\n\n$$\n\\leq 2 \\mu \\mathbb{E}_{\\epsilon}\\bigg[\\sup\\_{h \\in \\mathcal{H}} \\bigg|\\sum\\_{i=1}^n \\epsilon\\_i h(\\mathbf{x}_i)\\bigg| \\bigg] \\qquad (3).\n$$\nHence, our contraction lemma in Theorem 2 improves (3) for many special classes of activation functions (See our Theorem 2) since the contraction rate is only $\\mu$. \n\n2. In Theorem 8, the upper bound $F_L$ of $\\mathcal{R}_n(\\mathcal{F})$ is defined using the recursive relation (34). Because of $\\gamma_i$, $F_L$ seems depending on the depth, unlike the explanation in the remark 9. Could you provide a bit more details?\n\nAnswer: In our work, by using Theorem 8 (or Theorem 2), we show that for CNNs or DNNs with some special classes of activation functions (ReLU, Leaky ReLU, PRLI, Sigmoid, and Tanh or odd functions in general), the Rademacher complexity can be upper bounded by\n$$\nO\\bigg(\\sqrt{\\frac{1}{n}}\\prod_{j=1}^L \\mu_j M_{\\infty} (j) \\bigg),\n$$ where $M_{\\infty} (j)$ is the infinity-norm of the weight matrix of the $j$-the layer, and $\\mu_j$ is the Lipschitz constant of the activation in the $j$-the layer. Note that $|\\sigma_j(0)|/\\sqrt{n}$ is usually very small compared to $\\mu_j M_{\\infty} (j)$. \n\nHence, under the assumption of a bounded product of the norms of weight matrices, our derived bound is independent of the network length. This represents a significant improvement over the bounds established by Golowich et al., particularly for deep neural networks (DNNs) with extensive network lengths. We have adopted the term \"size-independent\" from Golowich et al.'s research to convey this concept. Additionally, we highlight that our results apply to a broader range of activation functions, extending beyond the ReLU activations that were primarily considered in their work. \n\nWe will ensure to add \"under the assumption of a bounded product of the norms of weight matrices\" before stating that \"the bound is network-depth independent\" in our revised version.\n\nBased on our responses to your questions, we believe that your evaluation of our work is not correct. Our results are significant from both theoretical and practical perspectives. There are two main contributions in our work: (1) deriving a novel contraction lemma between vector spaces which improves Talagrand's contraction lemma for many cases; (2) deriving an upper bound on Rademacher complexity which improves Golowich et al.'s bounds and leads to non-vacuous generalisation error bound (at least) for CNNs with binary classification. To the best of our knowledge, this is the first result to show that the Rademacher complexity approach can yield meaningful (non-vacuous) generalization error bounds. This is particularly significant because our goal in developing generalization error bounds is to ensure they are applicable in practical settings. Therefore, we kindly ask the reviewer to be fair in evaluating our contribution."
            }
        },
        {
            "title": {
                "value": "Reply to reviewer's comments"
            },
            "comment": {
                "value": "Thank you very much for your comments on our paper. \n\nFirst, we provide rebuttals to your comments on the weaknesses of our paper.\n\n1. Questionable significance:\n\nAnswer: We believe that your comments on the significance of our paper is not correct. There are three reasons:\n+ The contraction lemmas in Maurer (2016) and Foster and Rakhlin (2019) are different from our contraction lemma in Theorem 2. It does not have the same form as ours. We note that the contraction lemmas in Maurer (2016) and Foster and Rakhlin (2019) are used for mapping between $\\mathbb{R}^m \\to \\mathbb{R}$, but our contraction lemmas are for the mapping $\\mathbb{R}^m \\to \\mathbb{R}^m$.  In addition, as (cf. (1) in Maurer 2016):\n$$\n\\mathbb{E}\\_{\\epsilon}\\bigg[\\sup_{f \\in \\mathcal{F}}\\sum_{t=1}^n \\epsilon_t \\phi_t(f(x_t))\\bigg]\\leq \\sqrt{2}\\mu \\mathbb{E}\\_{\\epsilon} \\bigg[\\sup_{f \\in \\mathcal{F}}\\sum_{t=1}^n \\sum_{i=1}^K \\epsilon_{t,i} f_i(x_t)\\bigg]\\qquad (1),\n$$\nso the form of Rademacher complexity is changed between the RHS and LHS. Hence, this contraction lemma looks hard to apply for DNNs where we need to peel layer by layer. \n+ At least for the ReLU family (ReLU,  Leaky ReLU, Parametric Rectifier Linear Unit) and tanh or any other odd activation function, we have $\\psi(0)=0$. Hence, by our Theorem 2 it holds that\n$$\n\\mathbb{E}\\_{\\epsilon}\\bigg[\\sup_{h \\in \\mathcal{H}}\\bigg\\\\|\\frac{1}{n}\\sum_{i=1}^n \\epsilon_i \\psi(h(\\mathbf{x}\\_i))\\bigg\\\\|_{\\infty}\\bigg] \\leq \\mu \\mathbb{E}\\_{\\epsilon}\\bigg[\\sup\\_{h \\in \\mathcal{H}}\\bigg\\\\| \\sum\\_{i=1}^n \\epsilon_i h(\\mathbf{x}\\_i) \\bigg\\\\|\\_{\\infty} \\bigg],\n$$ which improves (1).  Note that this contraction lemma also improves Talagrand's results (see our comments in the following box). \n\n+ As the peeling process, by using our Theorem 2 or Lemma 7, our Rademacher upper bound has the following form:\n$$\nO\\bigg(\\sqrt{\\frac{1}{n}}\\prod_{j=1}^L \\mu_j M_{\\infty} (j) \\bigg)  \\qquad (2),\n$$ \nwhere $M_{\\infty} (j)$ is the infinity-norm of the weight matrix of the $j$-the layer, and $\\mu_j$ is the Lipschitz constant of the activation in the $j$-the layer since $|\\sigma_j(0)|/\\sqrt{n}$ is usually very small compared to $\\mu_j M_{\\infty} (j)$ for a large number of training examples. However, if we apply contraction lemmas in Maurer (2016) and Foster and Rakhlin (2019) for DNNs with $L$ layers, we may achieve the following upper bound (although we believe that it is hard to apply these lemmas for deep learning as mentioned above)\n$$\nO\\bigg((\\sqrt{2})^L \\sqrt{\\frac{1}{n}}\\prod_{j=1}^L \\mu_j M_{\\infty} (j) \\bigg).\n$$ This bound is inferior to Golowich et al. and our bounds since the constant term is exponential in $L$ (See a detailed discussion in  Golowich et al. (Size-independent sample complexity of neural networks)). \n\n2. Non-vacuous bound:\n\nAnswer: In this work, we choose the infinity-norm for developing our bound since we believe that this norm can lead to tight bounds (see our experiments). Choosing the right norm is a contribution of our paper. We note that Nagarajan and Kolter (2019) experimented on the bounds provided by Neyshabur et al. (2018) and Bartlett et al. (2017) for ReLU DNNs. However, these bounds are even inferior to Golowich et al.  (2018) (see a detailed discussions in Golowich et. al (Size-independent sample complexity of neural networks)). They also use different norms from ours. We believe that the main reason why these bounds are vacuous since they are not tight enough. Tightening Rademacher complexity bounds is our target in this work. We achieve this by developing a novel contraction lemma for mappings between vector spaces in Theorem 2.\n\n3. Comparison with other CNN bounds:\n\nAnswer: As discussed in Golowich et al. (Size-independent sample complexity of neural networks), their bound offers the best performance in terms of dependence on network depth. Therefore, in our paper, we primarily compare our results with those of Golowich et al. (see Section 6). For a more detailed comparison of Golowich et al.'s work with other existing bounds, readers are encouraged to refer directly to their paper. Additionally, our results are built upon a new contraction lemma for vector spaces (Theorem 2), which distinguishes our approach from previous methods. In response to your suggestions, we will incorporate these clarifications into the revised version of our paper to enhance its readability.\n\n4. Clarity Concerns:\n\nAnswer: In the main text, we already provided a proof for Theorem 8. The proofs of other lemmas are given in Appendix since they are quite long. As you can see in Appendix, the proof of Theorem 2 are based on many lemmas, hence it requires a lot of spaces even for only stating these lemmas."
            }
        },
        {
            "title": {
                "value": "Reply to reviewer's comments"
            },
            "comment": {
                "value": "Thank you very much for your comments on our paper. \n\nFirst, we answer your comments question by question:\n1. Lemma 1: $\\boldsymbol{\\epsilon}$ is not defined. \n\nAnswer: We think that the readers can understand what is $\\boldsymbol{\\epsilon}$ from the context. However, based on your concern, we will define $\\boldsymbol{\\epsilon}=(\\epsilon_1,\\epsilon_2,\\cdots,\\epsilon_n)$ in our revised version. \n\n2. The definition of network length should be provided at the beginning of the paper. It actually refers to the number of layers in Section 6. It would be better to adopt the conventional name \"network depth\".\n\nAnswer: We mentioned about $L$ in Section 3.1 when we first introduced CNN. We will mention that $L$ is called \"network-length\" in Section 3.1 in the revised version as your suggestion.\n\n3. What is the benefit of the bound that does not depends on the network length? It could not reflect the effects of depth in deep learning.\n\nAnswer: To answer this question, we should look at the best known bound by Theorem 1.2. in Golowich et al. (Size-independent sample complexity of neural networks, 2018) where the authors showed that the Rademacher complexity of DNNs with ReLU activation functions is upper bounded by\n$$\n\\tilde{O}\\bigg(\\sqrt{\\frac{L}{n}}\\prod_{j=1}^L  M_F(j) \\bigg).\n$$\nHere, $M_F(j)$ is the Frobenius norm of the weight matrix of the $j$-the layer, and $n$ is the number of training examples (Please also see Section 6 in our paper). \n\nIn our work, by using Theorem 8 (or Theorem 2), we show that for CNNs or DNNs with some special classes of activation functions (ReLU, Leaky ReLU, PRLI, Sigmoid, and Tanh or odd functions in general), the Rademacher complexity can be upper bounded by\n$$\nO\\bigg(\\sqrt{\\frac{1}{n}}\\prod_{j=1}^L \\mu_j M_{\\infty} (j) \\bigg),\n$$ where $M_{\\infty} (j)$ is the infinity-norm of the weight matrix of the $j$-the layer, and $\\mu_j$ is the Lipschitz constant of the activation in the $j$-the layer. Note that $|\\sigma_j(0)|/\\sqrt{n}$ is usually very small compared to $\\mu_j M_{\\infty} (j)$. \n\nHence, under the assumption of a bounded product of the norms of weight matrices, our derived bound is independent of the network length. This represents a significant improvement over the bounds established by Golowich et al., particularly for deep neural networks (DNNs) with extensive network lengths. We have adopted the term \"size-independent\" from Golowich et al.'s research to convey this concept. Additionally, we highlight that our results apply to a broader range of activation functions, extending beyond the ReLU activations that were primarily considered in their work. \n\nWe will ensure to add \"under the assumption of a bounded product of the norms of weight matrices\" before stating that \"the bound is network-depth independent\" in our revised version.\n\n4. In Theorem 8, $L$ is not defined. Furthermore, $F_L$ depends on $L$. Does it contradict with the author's claim?\n\nAnswer: In Theorem 8, we mention that \"we consider the CNN defined in Section 3.1\" where we define $L$. \nFor the second question, please refer to our answer for Question 3.\n\n5. What do the numerical results in Section 5 imply? There is no elucidation. Furthermore, the authors only implement one experiment which is far from enough for verification.\n\nAnswer:  Our simulation results demonstrate that by refining the Rademacher complexity bound, we can derive non-vacuous generalization error estimates for certain CNN networks. To the best of our knowledge, this is the first result to show that the Rademacher complexity approach can yield meaningful (non-vacuous) generalization error bounds. This is particularly significant because our goal in developing generalization error bounds is to ensure they are applicable in practical settings.\n\nWe also provided two extra experiments in Appendix A.12 and mentioned this in Section 5. \n\n6. The title is not accurate. This paper only studies CNNs.\n\nAnswer: Although we only limit our experiments on CNNs, however, our development results such as Theorem 2 and Lemma 7 can be applied for general DNNs. We can change the title to \"On Rademacher Complexity-based Generalisation Bounds for CNNs\" if necessary.   \n\nBased on our responses to your questions, we believe that your evaluation of our work is not correct. Our results are significant from both theoretical and practical perspectives. There are two main contributions in our work: (1) deriving a novel contraction lemma between vector spaces which improves Talagrand's contraction lemma for many cases; (2) deriving an upper bound on Rademacher complexity which improves Golowich et al.'s bounds and leads to non-vacuous generalisation error bound (at least) for CNNs with binary classification. To the best of our knowledge, this is the first result to show that the Rademacher complexity approach can yield meaningful (non-vacuous) generalization error bounds. Therefore, we kindly ask the reviewer to be fair in evaluating our contribution."
            }
        },
        {
            "summary": {
                "value": "This paper explores the theoretical underpinnings of generalization in deep learning, specifically focusing on Convolutional Neural Networks (CNNs). The authors propose a new approach using Rademacher complexity to derive non-vacuous generalisation bounds for CNNs with certain activation functions like ReLU, Leaky ReLU, and Sigmoid. These bounds, unlike prior results, do not explicitly depend on the network depth. The authors validate their findings through experiments on MNIST image classification."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors employ the Talagrand's contraction lemma to develop novel   Rademacher-complexity-based generalization bound for CNNs and specialze the bounds to some common types of activation functions."
            },
            "weaknesses": {
                "value": "1. The paper's maths and presentation need to be significantly improved.\n2. The theoretical contributions seem not sound enough."
            },
            "questions": {
                "value": "1. Lemma 1: $\\epsilon$ is not defined.\n2. The definition of network length should be provided at the beginning of the paper. It actually refers to the number of layers in Section 6. It would be better to adopt the conventional name \"network depth\".\n3. What is the benefit of the bound that does not depends on the network length? It could not reflect the effects of depth in deep learning.\n4. In Theorem 8, $L$ is not defined. Futhremore, $F_L$ depends on $L$. Does it contradict with the authors' claim?\n5. What do the numerical results in Section 5 imply? There is no elucidation. Futhremore, the authors only implement one experiment which is far from enough for verification.\n6. The title is not accurate. This paper only studies CNNs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies the Rademacher complexity for the class of CNNs using any of ReLU family activation functions. Based on the authors' new version of Talagrand's contraction lemma, they provide the upper bound on the Rademacher complexity. Also, they conducted numerical experiments verifying the gap between the upper bound and error."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "They try to identify activation functions and consider wider hypothesis space (e.g. $\\mathcal{H}_+$) to provide new contraction coefficients."
            },
            "weaknesses": {
                "value": "**Questionable significance**\n\nThough this work provides the new contraction lemma, in my opinion, the result is not so significant.\nCompared to the known vector-valued Talagrands' contraction lemma ([1, 2]), theorem 2 seems to be able to have poor upper bounds since there is an additional term $\\frac{1}{\\sqrt{n}}| \\psi (0) |$, which can even worsen the upper bound since the denominator is sublinear.\nTo clarify, please add more details and rigorously compare the lemmas in [1] and [2].\n(e.g. why considering $\\mathcal{H}_+$ lead better upper bound, ...)\n\n\n**Non-vacuous bound**\n\nThe authors claim that the result provides a non-vacuous bound for CNNs with a small number of classes.\nTo verify this, they conducted several experiments under various setups.\nHowever, as illustrated in Nagarajan & Kolter (2019), the bound may be vacuous depending on the norms of matrices (e.g. kernel) even if we carefully choose a set of parameters.\nThus, to prove the tightness, please provide a theoretical analysis of how their bounds behave under different matrix norm conditions in light of the results from Nagarajan & Kolter (2019).\n\n**Comparison with other CNN bound**\n\nThough there are lines of research studying generalization bound for CNNs, the authors do not provide any comparison with other known bounds. To highlight the novelty of the work, kindly suggest including a specific comparison section in their paper, highlighting key differences between their approach and existing CNN generalization bounds.\n\n**Clarity Concerns**\n\nThere is no proof sketch or idea provided within the main text for any of the theorems; full proof in the main text or deferred to the appendix. For better readability, kindly suggest that the authors include brief proof sketches or key insights for the main theorems in the main text while keeping full proofs in the appendix.\n\n\n\n**Reference**\n\n[1] Maurer (2016), A vector-contraction inequality for Rademacher complexities\n\n[2] Foster and Rakhlin (2019), $\\ell_\\infty$ Vector Contraction for Rademacher Complexity"
            },
            "questions": {
                "value": "**1.About the remark 3**\n\nThe second bullet in remark 3 states that theorem 2 improves the bound for the case $m=1$. However, in Lemma 1 (Talagrand's lemma for $m=1$), the contraction rate is $\\mu$, not $2\\mu$.\n\n\n**2. depth-dependent**\n\nIn theorem 8, the upper bound $F_L$ of $\\mathcal{R}_{n}( \\mathcal{F} )$ is defined using the recursive relation (34). \nBecause of $\\gamma_i$, $F_L$ seems depending on the depth, unlike the explanation in the remark 9. Could you provide a bit more details?\n\n---\n\nI am hoping that the authors will provide the clarifications stated therein in the rebuttal phase"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper first generalizes the Talagrand contraction lemma to high-dimensional mapping case, then uses these new results to derive non-vacuous generalization bounds for CNNs. Their results show that the Rademacher complexity does not explicitly depend on the depth of the network for CNNs with some specific types of activation functions. some empirical results on the MNIST image classifications are also given."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper provides a novel and useful contraction lemma, which extends the previous Talagrand contraction lemma to high-dimensional mapping case.\n2. Based on the new contraction lemma, the authors derive the bounds of the Rademacher complexity for CNNs, which does not explicitly depend on the depth of CNNs. This improves the results in previous papers.\n3. The authors derive some non-vacuous generalization bounds for CNNs."
            },
            "weaknesses": {
                "value": "1. This paper only considers some specific types of activation functions.\n2. The conclusion that the obtained Rademacher complexity does not explicitly depend on the depth of CNNs is not very convincing, since by Equations (30), (31), (34), the Rademacher complexity may exponentially depend on the depth of CNNs if the right-hand-side of (30), and (31) are large than 1. More discussions on this claim should be added."
            },
            "questions": {
                "value": "See the Weaknesses part. In line 251, \"Let $\\psi(x)$ is\" should be \"Let $\\psi(x)$ be\" ."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}