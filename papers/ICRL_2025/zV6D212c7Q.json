{
    "id": "zV6D212c7Q",
    "title": "Masked Cross-attention Adapters Enable the Characterization of Dense Features",
    "abstract": "Learning meaningful representations is a core topic of deep learning. Throughout the last decade, many strategies for learning image representations have been proposed involving supervision and self-supervision and various data sources. \nIn most current work, evaluation is focused on classification tasks while neglecting dense prediction tasks, possibly because linear probing is more challenging in the latter case.\nFurthermore, dense prediction heads are often large and come with specific inductive biases that distort performance measurement further.\nIn this work we propose masked cross-attention adapters (MAXA), an adapter method that is capable of dense predictions independent of the size and resolution of the encoder output. This allows us to make dense predictions using a small number of additional parameters ($<0.3 $%) while allowing for fast training using frozen backbones.\nUsing this adapter, we run a comprehensive evaluation assessing instance awareness, local semantics and spatial representation of a diverse set of backbones. \nWe find that DINOv2 outperforms all other backbones tested - including those supervised with masks and language - across all three task categories.  \nCode is available at https://to.be.released.",
    "keywords": [
        "image features",
        "image backbones",
        "ViT",
        "instance segmentation"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zV6D212c7Q",
    "pdf_link": "https://openreview.net/pdf?id=zV6D212c7Q",
    "comments": [
        {
            "comment": {
                "value": "Thanks for your feedback. To clarify: We compare against fine-tuning methods [1] (and object-centric methods) in Fig. 5. Would comparisons with additional fine-tuning methods improve your judgment of our work?\nRegarding the set of visual tasks: Which additional tasks specifically would strengthen the paper?\n\n[1] Goldblum et al.: Battle of the backbones: A large-scale comparison of pretrained models across computer vision tasks, NeurIPS 2024."
            }
        },
        {
            "comment": {
                "value": "Thank you for your comments. To avoid confusion: We agree that architectural biases should be controlled for. Therefore, we used the ViT-B/16 architecture (when possible) in our experiments. Does your suggestion on different encoders refer to using CNN backbones?\nIn terms of potential biases from MAXA: In Fig. 5 we assess consistency with fine-tuned non-dense adapters [1] and object-centric [2,3] methods. Do you miss a comparison with a specific method?\n\n[1] Goldblum et al.: Battle of the backbones: A large-scale comparison of pretrained models across computer vision tasks, NeurIPS 2024.  \n[2] Aydemir et al.: Self-supervised object-centric learning for videos, NeurIPS 2023.  \n[3] Seitzer et al.: Bridging the gap to real-world object-centric learning, ICRL 2023."
            }
        },
        {
            "comment": {
                "value": "We appreciate your feedback and will implement the ablations you suggested. To clarify: Regarding correlation with fine-tuned methods we relate to object-centric [1,2] and fine-tuned methods [3] in Fig. 5. If you were aware of this, which additional direction or experiment would convince you specifically?\nIn terms of the FPN, findings from the ViTDet paper [4] suggest that relying on the last ViT layer is favourable. Furthermore, the goal of our work is primarily to evaluate backbone representations (this should be stated more clearly!), thus a simple method is preferable. Does this additional context change your assessment?\n\n[1] Aydemir et al.:  Self-supervised object-centric learning for videos, NeurIPS 2023.  \n[2] Seitzer et al.: Bridging the gap to real-world object-centric learning, ICRL 2023.  \n[3] Goldblum et al.:  Battle of the backbones: A large-scale comparison of pretrained models across computer vision tasks, NeurIPS 2024.  \n[4] Li et al.: Exploring plain vision transformer backbones for object detection, ECCV 2022."
            }
        },
        {
            "summary": {
                "value": "This work introduces the Masked Cross-Attention Adapter (MAXA), a method designed to provide a cost-effective approach for probing transformer backbones for their dense prediction capabilities. The method employs a masked cross-attention readout layer that uses positional encodings as fixed query vectors. This is followed by a second unmasked cross-attention layer and a deconvolution network. Additionally, each attention head incorporates a learnable locality bias term during cross-attention. The authors evaluate MAXA's performance using multiple pretrained backbones (such as MAE, DINO, and DINOv2) across tasks including instance awareness, semantic segmentation, and monocular depth estimation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The ability to perform a meaningful cost-effective evaluation of backbones for their dense prediction capabilities is currently lacking, and as a result, only a limited number of studies offer meaningful evaluations. The proposed method is well-motivated and clearly described, making it easy to follow. The authors provide a solid introduction to the problem and include an informative related work section. Additionally, the ablation studies demonstrate that the advantages of certain design choices are consistent across multiple tasks and different pretrained backbones."
            },
            "weaknesses": {
                "value": "The relevance of a cost-effective dense prediction evaluation largely depends on its high correlation with currently optimal but more resource-intensive evaluation techniques. However, the experiments provided are insufficient to establish confidence in this correlation. While the authors evaluate MAXA across multiple tasks and backbones, they do not present a statistical analysis of its correlation with fine-tuned results, nor do they quantify the trade-off between training cost and performance compared to state-of-the-art techniques. Such context is necessary to make the presented evaluations meaningful.\n\nThe choice to follow simple FPNs and rely solely on information from the last layer appears questionable. Although Vision Transformers (ViTs) maintain the same resolution across all layers, it is doubtful that the final layer alone contains all the necessary information without fine-tuning the backbone. For instance, MAE demonstrated that linear probing performance of ViTs is not always a reliable indicator of fine-tuning performance. Furthermore, [1] showed that employing cross-attention readouts from every layer leads to significant performance improvements compared to using simple FPNs.\n\nminor: Lines 260-270 contain some incomplete sentences, such as \"SAM2 also good.\".\n\n[1] Chen, Zhe, et al. \"Vision Transformer Adapter for Dense Predictions.\" The Eleventh International Conference on Learning Representations."
            },
            "questions": {
                "value": "The ablations show that a feature dimension of 8 performs better than a dimension of 16. Why only report a change in one direction? How did a feature dimension of 4 perform? Similarly, without the second cross-attention layer MAXA performs worse. Does a third cross-attention layer improve performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores vision adapter for pretrained ViTs on dense visual tasks like Segmentation and Depth Estimation.  The paper detailedly benchmark the performance with the tasks and also ablates some adapter designs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Comprehensive Benchmark for Dense Vision Tasks: The paper makes a valuable contribution by introducing a comprehensive benchmark specifically designed for evaluating the dense prediction capabilities of pre-trained vision encoders, addressing a notable gap in the current landscape of benchmarks that primarily focus on classification tasks. \u00a0 \n- Methodological Rigor: The authors employ a rigorous methodology, including the use of masked cross-attention adapters (MAXA) to enable fair comparisons across different encoders. The choice of MAXA is well-justified, as it allows for fast training and evaluation. \u00a0 \n- Clarity and Insightful Presentation: The paper presents a clear and well-organized set of experiments, covering a diverse range of pre-trained models and dense tasks. The results are presented in a readily understandable and comparable manner, providing valuable insights into the relative strengths and weaknesses of different encoders for dense prediction tasks."
            },
            "weaknesses": {
                "value": "- Limited Insight into Learned Representations: While the benchmark effectively compares the performance of different encoders, it lacks deep analysis regarding the specific representations learned by each encoder. Simply stating that \"DINOv2\" achieves the highest numbers isn't sufficient; the paper would benefit from a more in-depth investigation into the characteristics of the learned representations that contribute to performance differences. \u00a0 \n- Overlooking Architectural Biases: The paper does not explicitly address how architectural biases in different encoders might contribute to their performance on dense tasks. A discussion on this aspect would be valuable, as it could help disentangle the effects of pre-training from those inherent to the encoder architectures. \u00a0 \n- Potential Bias from MAXA: Although the authors justify the use of MAXA, the paper could be strengthened by exploring whether the findings hold consistent with other adapter methods or lightweight dense heads. This would provide further validation of the results and address potential concerns about biases introduced by the specific choice of MAXA. \u00a0 \n- Missing Comparisons with Key Adapter Methods: The paper lacks a direct comparison with other relevant adapter methods, such as ViT-Adapter and FeatUp. Including these methods in the evaluation would offer a more complete picture of the adapter landscape for dense prediction tasks."
            },
            "questions": {
                "value": "Please see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The article introduces a new method called Masked Cross-Attention Adapters (MAXA) for evaluating and characterizing the performance of different visual feature extractors (backbone networks) in dense prediction tasks. MAXA employs a cross-attention mechanism that enables effective feature extraction and dense prediction without relying on the size and resolution of the backbone network's output. This method introduces a learnable masking radius, allowing the model to adapt to the locality of various features, thereby achieving fast training while maintaining a low number of parameters."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- MAXA can adapt to downstream tasks with only a small number of additional parameters (less than 0.3%).\n- Since the backbone network is frozen during training, MAXA achieves faster training speeds."
            },
            "weaknesses": {
                "value": "- Although MAXA has been evaluated across three main task categories, these categories may not fully cover all possible visual tasks.\n- There is a lack of comparisons with other fine-tuning methods, such as Adapters Strike Back.\n- The novelty is relatively weak."
            },
            "questions": {
                "value": "Is there more discussion on the reasons behind DINOv2's superior performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents the Masked Cross-Attention Adapter (MAXA), a lightweight solution for making dense predictions on frozen vision backbones. By utilizing cross-attention, MAXA decouples the encoder output's resolution from the final prediction, filling a gap in the evaluation of feature extractors for dense tasks such as segmentation and depth estimation. The authors assess multiple vision backbones along three dimensions: instance awareness, local semantics, and spatial understanding, with DINOv2 emerging as the top performer across all tasks. The study emphasizes MAXA\u2019s effectiveness in characterizing dense features while requiring minimal parameters and training effort.\n\nOverall, the concept is straightforward and well-presented. The authors explore various aspects of the capabilities of frozen features, providing a solid basis for evaluating the adapter's effectiveness. However, the paper falls short due to a limited literature review and insufficient experimental depth, which may impact its chances of acceptance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- (1) The idea is simple and easy to read.\n    \n- (2) This research has potential to become a standard for evaluating the dense awareness of frozen features.\n    \n- (3) The authors explore various aspects of the capabilities of frozen features, providing a solid basis for evaluating the adapter's effectiveness."
            },
            "weaknesses": {
                "value": "Major Weaknesses\n\n- (1) The literature cited is outdated. For instance, the authors state, \u201cAt the other end of the spectrum, using complex dense task heads, for example, Faster R-CNN (Ren et al., 2015) for object detection, adds a large number of parameters and introduces its own inductive biases.\u201d However, Faster R-CNN is nearly a decade old. The authors should clearly differentiate their approach from more recent works like ViTDet [1], ViT-Adapter [2], and Segmenter [3] in both the introduction and related work sections, as these studies also focus on developing lightweight dense task heads. Although ViTDet is briefly mentioned in the \u201cExperiment Design\u201d section, this reference is insufficient for establishing the distinction.\n    \n- (2) Lack comparison with zero-shot dense prediction using frozen features. Zero-shot segmentation using frozen features [4, 5] \u00a0from foundation models has been extensively studied. These models [4, 5] demonstrate strong segmentation performance with training-free dense heads.\n    \n- (3) The experimental comparison is insufficient. In the CLIP setting, the authors focus solely on ViT-based architectures (SigLIP for example), whereas ConvNet-based or hybrid architectures might be more appropriate for dense tasks. It is highly recommended that the authors include experiments with Hybrid CNN-Transformer architecture like ViTamin [6] and ConvNet architecture like CLIP-ConvNeXt [7]. These additional experiments are crucial, and I would consider raising the score if they are incorporated during the rebuttal phase.  \n    \n\n\nMinor Weaknesses\n\n- (1) In Figure 1, it is unclear how the M(q, \\sigma) is generated.\n    \n- (2) Section 3 mentions that \u201cspatial queries Q of size (HQWQ, 16)\u201d. It is unclear why the query dimension is chosen to be 16.\n    \n- (3) Concerns regarding reproducibility arise due to the unclear descriptions. Section 3 mentions that \u201cThis is realized through a small CNN operating on the output of all queries using transposed convolutions to increase spatial size.\u201d. Please specify the details of \"small CNN\". CNN usually refer to Convolutional Neural Network consisting with convolutional layer and non-linear activation layer. Please specify the type and number of layers, and the kernal size of each convolution operator.\n\n\n[1] Li Y, Mao H, Girshick R, et al. Exploring plain vision transformer backbones for object detection[C]//European conference on computer vision. Cham: Springer Nature Switzerland, 2022: 280-296.\n    \n[2] Strudel R, Garcia R, Laptev I, et al. Segmenter: Transformer for semantic segmentation[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 7262-7272.\n    \n[3] Chen Z, Duan Y, Wang W, et al. Vision transformer adapter for dense predictions[J]. ICLR, 2023.\n    \n[4] Sun S, Li R, Torr P, et al. Clip as rnn: Segment countless visual concepts without training endeavor[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 13171-13182.\n    \n[5] Wang F, Mei J, Yuille A. Sclip: Rethinking self-attention for dense vision-language inference[C]//European Conference on Computer Vision. Springer, Cham, 2025: 315-332.\n    \n[6] Chen J, Yu Q, Shen X, et al. ViTamin: Designing Scalable Vision Models in the Vision-Language Era. CVPR. 2024. https://huggingface.co/jienengchen/ViTamin-XL-384px\n    \n[7] https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup"
            },
            "questions": {
                "value": "As raised the weakness section, it is highly recommended to include experiments with Hybrid CNN-Transformer architecture like ViTamin [6] and ConvNet architecture like CLIP-ConvNeXt [7]. These experiments are essential, and I would consider increasing the score if they are added during the rebuttal phase.\n\nThe authors should also address the major concerns and minor concerns detailed in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}