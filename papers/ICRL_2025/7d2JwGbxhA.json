{
    "id": "7d2JwGbxhA",
    "title": "OCEBO: Object-Centric Pretraining by Target Encoder Bootstrapping",
    "abstract": "Object-centric representation learning has recently been successfully applied to real-world datasets. This success can be attributed to pretrained non-object-centric foundation models, whose features serve as reconstruction targets for slot attention. However, targets must remain frozen throughout the training, which sets an upper bound on the performance object-centric models can attain. Attempts to update the target encoder by bootstrapping result in large performance drops, which can be attributed to its lack of object-centric inductive biases, causing the object-centric model's encoder to drift away from representations useful as reconstruction targets.\nTo address these limitations, we propose \\textbf{O}bject-\\textbf{Ce}ntric Pretraining by Target Encoder \\textbf{Bo}otstrapping, a self-distillation setup for training object-centric models from scratch, on real-world data, for the first time ever. In OCEBO, the target encoder is updated as an exponential moving average of the object-centric model, thus explicitly being enriched with object-centric inductive biases introduced by slot attention while removing the upper bound on performance present in other models. We mitigate the slot collapse caused by random initialization of the target encoder by introducing a novel cross-view patch filtering approach that limits the supervision to sufficiently informative patches. When pretrained on 241k images from COCO, OCEBO achieves unsupervised object discovery performance comparable to that of object-centric models with frozen non-object-centric target encoders pretrained on hundreds of millions of images.",
    "keywords": [
        "Object-centric learning",
        "bootstrapping",
        "self-supervised pretraining"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=7d2JwGbxhA",
    "pdf_link": "https://openreview.net/pdf?id=7d2JwGbxhA",
    "comments": [
        {
            "summary": {
                "value": "In the research background, large-scale foundation models are common due to self-supervised learning techniques in deep learning, especially in computer vision. Cognitive psychology research indicates human visual perception is object-centric, leading to object-centric representation learning, though such models lack successful pre-training on large-scale real-world datasets. The research purpose is to propose the OCEBO method for pre-training object-centric models from scratch on real data to overcome limitations and unleash potential. The research methods involve a model architecture with an image encoder, slot attention encoder, slot decoder, and a target encoder of the same architecture, and a training objective formulated as a self-distillation bootstrapping problem with defined object-centric self-distillation loss including cross-view patch filtering and an optional mask sharpening stage. The experimental results on the MS COCO dataset and evaluation on multiple datasets with different metrics show that OCEBO can avoid slot collapse and achieve comparable performance to existing models with pre-trained target encoders while demonstrating good data scalability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. A new object-centric pre-training method, OCEBO, is proposed. It is the first self-distillation setup for training object-centric models from scratch on real-world data.\n\n2. Experiments prove that OCEBO can avoid slot collapse and achieve performance comparable to existing methods using a large number of pre-trained images on multiple datasets while demonstrating good data scalability.\n\n3. The importance of object-centric inductive biases is emphasized, and its positive impact on the target encoder is verified through experiments, providing new insights into the theory of object-centric learning."
            },
            "weaknesses": {
                "value": "1. Although good results have been achieved on the MS COCO dataset, the requirements for pre-training datasets are relatively high. Datasets containing simple scenes like ImageNet are not suitable for pre-training object-centric models, and a large-scale dataset suitable for pre-training object-centric models has not yet been found.\n\n2. When comparing with existing state-of-the-art object-centric models, due to different pre-training methods and datasets used, the models are not directly comparable, which, to some extent, affects the accurate evaluation of model performance.\n\n3. The experimental setup and evaluation system are still somewhat rudimentary and cannot fully demonstrate the scheme's advantages."
            },
            "questions": {
                "value": "1. When updating the target encoder as an exponential moving average (EMA) of the object-centric model encoder, how can we ensure that the introduced object-centric inductive biases do not overly affect the model's learning of other features, thus maintaining good generalization ability in different downstream tasks?  \n2. When the cross-view patch filtering method determines which patches to use for training, although it considers the feature quality of the target encoder, is it possible that this method may overlook some patch information that is potentially helpful for the model's learning? How can the accuracy and comprehensiveness of patch selection be better balanced? \n3. The paper mentions that constructing a large-scale dataset suitable for pre-training object-centric models remains an open question. Do the authors have any preliminary ideas or directions on how to construct such a dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose an approach to train object-centric models from scratch using real-world data, rather than relying on pre-trained non-object-centric foundation models.\nThe method is based on cross-view teacher-student self-distillation, in a similar fashion to DINO, IBOT and DINOv2.\nThe model architecture incorporates a slot-attention bottleneck and the patch-level loss uses a filtering strategy to stabilize training.\nThe method is trained on COCO and evaluated on different datasets on the task of unsupervised object discovery, where it attains performance comparable to (but lower than) previous methods that leverage large-scale pre-trained models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The main strength of the paper is succeeding in training an object-centric model from scratch on COCO, which is known from previous works to be challenging.\nThe architecture or training procedure per-se are not particularly novel, mostly resembling the global and patch losses of DINO, IBOT and DINOv2, with the addition of a slot-attention bottleneck in the architecture.\n\nWhat is novel is the idea of filtering noisy patches that could be detrimental to the object-centric objective, especially during the first stages of training.\nThis idea, albeit not well ablated, seems to be a strong contribution of the paper.\n\nThe paper is easy to follow, with a good balance between technical details, analogies, and high-level explanations.\nQuantitative results are presented clearly and accompanied by qualitative examples."
            },
            "weaknesses": {
                "value": "**Ablation on patch filtering:**\nFrom section 4.2, it appears that patch filtering is crucial to stabilize training.\nThe chosen strategy uses an heuristic to filter out patches, especially during the first stages of training, as show in figure 2.\nThe first question that comes to mind is: how sensitive is the method to the choice of the heuristic?\nIt could be that the chosen heuristic has no importance and what really matters is that initially the global loss drives the training and the object loss is introduced gradually later.\nIn my opinion, this is an important ablation study to perform in the paper.\nTwo alternatives that I would like to see tested are:\n- Keeping all the patches but gradually increasing $ \\lambda_{oc} $ from 0 to 1 during training.\n- Randomly dropping patches in $ \\mathcal{L}_{oc} $ as opposed to selecting them via nearest neighbors. The drop ratio could be gradually increased from 0 to 1 during training to mimic the proposed heuristic.\n\n**Measuring slot collapse:**\nAn important point of discussion is \"slot collapse\", defined in the footnote at L107.\nSince the authors claim that the proposed patch filtering strategy is crucial to avoid slot collapse, it would be helpful to have a quantitative and objective metric to measure slot collapse.\nThis could be, for example, the correlation between slots and spatial positions across images, to measure whether a slot encodes the \"bottom right corner\" or a category of objects.\nThe green/red results in table 1 would be more informative and convincing if accompanied by such a metric.\n\n**One model or several ones?**\nThe whole model is trained from scratch on COCO and evaluated on different datasets, each with a specific number of slots (L319).\nDoes it mean that a new model needs to be trained from scratch for each number of slots?\nIf so, this is highly impractical for real-world applications where a practitioner would like to sweep over the number of slots to find the best one.\nIn such a case, frameworks like DINOSAUR or SPOT are much less expensive to use.\nIf not, how is the number of slots changed in the model? Is it fixed before training or can it be changed at inference?\n\n**No evaluation of the learned representation:**\nAll evaluations focus on segmentation-based metrics (FG-ARI and mBO) on several datasets.\nThe task of \"object-centric learning\", however, implies that the model should learn a representation of objects, not just segment them.\nIt would be useful to include a section that evaluates the slot representation on downstream tasks in a quantitative manner. \n\n**Projection head design:**\nOn L328-331 it says \"The projection heads are identical to those of DINO (Caron et al., 2021), with the exception of setting L = 8192 instead of the original 65536. Compared to the DINO head, ours projects every patch rather than just the global representations and we find that the gain in performance does not justify the computational cost.\"\nHowever, both IBOT and DINOv2 use per-patch heads and find that a large number of heads, even up to 131072, is beneficial.\nIf time allows, I recommend running an ablation study on the design of the projection heads, possibly splitting the object and global heads.\n\n**Performance and usefulness:**\nWeaker performance when compared to other methods that leverage large-scale pre-trained models (table 2).\nThis is somewhat expected, since the model is trained from scratch on a smaller dataset.\nAt a high level, this paper demonstrates that training from scratch is possible, but fails to prove that is actually beneficial.\nIf a pre-trained model achieves better performance, why should one train from scratch?"
            },
            "questions": {
                "value": "**Equation 1:**\nI suggest renaming $\\mathcal{L}_{oc}$ to something else to avoid confusion with the actual loss used during training which is defined in equation 3.\n\n**Ablation of head design:**\nEquations 3 and 4, as well as the filtered version in 8, describe a cross-view teacher-student distillation loss.\nThis setup requires quite a few moving parts, especially the cropping strategy with overlapping parts and the inverse augmentation.\nWould it be possible to train the model without cross-view distillation, but only using the teacher's output on the same crop as the target?\n\n**Comparison with the DINO objective?**\nThe global loss in equations 5 and 6 is formulated exactly as in DINO, why does the paragraph above it cite other papers and not DINO?\n\n**Suggestion about notation:**\nParagraph 3.3 and line 269 \"where $nns_{n_n}(z_{t,1}, z_{t,2})_i$ denotes indices of nn nearest neighbors\".\n\nThere are too many \"n\" characters in the chosen notation and it's hard to read.\nI suggest trying to replace $n_n$ with $k$ if possible.\n\n**Where is SPOT in the introduction?**\nTo the best of my knowledge, SPOT is the first work that unfreezes the encoder during training, and it was published months before FT-DINOSAUR.\nHowever, in the introduction, FT-DINOSAUR is presented as the first and is discusses in depth, while SPOT is not mentioned. This is misleading and should be corrected.\n\n**Missing results:**\nL435 \"In fact, an attempt to train OCEBO on ImageNet results in a drastically lower performance.\" where are these results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposed an object-centric pretraining method that updates the target encoder by EMA. The experiment results show that the proposed method can successfully learn object-centric representation. When pretrained on 241k images from COCO, the proposed achieves unsupervised object discovery performance comparable to other models with frozen non-object-centric target encoders pretrained on hundreds of millions of images."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is well written and easy to follow.\n2. The proposed method can achieve unsupervised object discovery performance comparable to other models with frozen non-object-centric target encoders pretrained on hundreds of millions of images.\n3. The proposed method demonstrates scalability well beyond a few thousand training images."
            },
            "weaknesses": {
                "value": "1. How exactly object-centric inductive biases are captured by the target encoder, it may be better to explain the mechanism more intuitively or theoretically.\n2. As the author mentioned, although the proposed method has achieved comparable results in COCO pre-training, its advantage still needs to be verified on a larger scale of pre-training data."
            },
            "questions": {
                "value": "1. What distance is used when calculating nearest neighbors?\n2. I don't understand what is the meaning of ``invaug(q1)''."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the problem of effectively updating the target encoder in object-centric pre-training. Previous works use frozen pre-trained encoders as the target encoder, resulting in a performance upper limit. While updating the pre-trained encoders causes a significant performance drop, the paper proposes to bootstrap the target encoder from scratch. To prevent slot collapse, a cross-view patch filtering technique is proposed. Experiments show that OCEBO can be trained from scratch and learn from more data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is easy to follow and understand. \n2. The motivations for cross-view patch filtering and mask sharpening stage are straight-forward and these two techniques are proven to be effective."
            },
            "weaknesses": {
                "value": "1. The experimental evidence for scalability is too weak. A scaling plot, which shows how the model performs as training data increases, is more supportive. From only two data points, it's hard to tell the scaling trend. For example, what if the model is just in rapid growth on 100k images and has already plateaued on 200k images? The authors are suggested to provide a scaling plot instead of two data points.\n2. There still seem to be large gaps between the final results and previous methods, which can not support the claim that OCEBO is comparable to those with pre-trained encoders.\n3. Discussion on object-centric data and non-object-centric data should be added. While a frozen target encoder can be an upper limit, a feasible way is to use stronger target encoders, as shown in the comparison between DINOv2 and DINO. Stronger DINO can be trained using more data, where object-centric data and non-object-centric data can both be used. So what's the benefit of scaling object-centric data over scaling pre-trained data for target encoders?"
            },
            "questions": {
                "value": "1. The authors are suggested to provide more evidence on scalability. Moreover, it would be better to provide an estimation of data amount required to achieve comparable performance with SOTA models.\n2. More benchmarks should be compared. This paper only reported MOVi-C, MOVi-E, Pascal VOC, EntitySeg results, while only two of them are real-world datasets. The authors are suggested to add more real-world datasets, especially the COCO dataset."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}