{
    "id": "ZSbsX1sFo3",
    "title": "UNA: Unifying Alignments of RLHF/PPO, DPO and KTO by a Generalized Implicit Reward Function",
    "abstract": "An LLM is pretrained on trillions of tokens, but the pretrained LLM may still generate undesired responses. To solve this problem, alignment techniques such as RLHF, DPO and KTO are proposed. However, these alignment techniques have limitations. For example, RLHF requires training the reward model and policy separately, which is complex, time-consuming, memory intensive and unstable during training processes. DPO proposes a mapping between an optimal policy and a reward, greatly simplifying the training process of RLHF. However, it can not take full advantages of a reward model and it is limited to pairwise preference data.\n\nIn this paper, we propose \\textbf{UN}ified \\textbf{A}lignment (UNA) which unifies RLHF/PPO, DPO and KTO. Firstly, we mathematically prove that given the classical RLHF objective, the optimal policy is induced by a generalize implicit reward function. With this novel mapping between a reward model and an optimal policy, UNA can 1. unify RLHF/PPO, DPO and KTO into a supervised learning of minimizing the difference between an implicit reward and an explicit reward; 2. outperform RLHF/PPO while simplify, stabilize, speed up and reduce memory burden of RL fine-tuning process; 3. accommodate different feedback types including pairwise, binary and scalar feedback. Downstream experiments show UNA outperforms DPO, KTO and RLHF.",
    "keywords": [
        "LLM Alignment",
        "Unified Alignment",
        "RLHF",
        "PPO",
        "DPO",
        "KTO"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ZSbsX1sFo3",
    "pdf_link": "https://openreview.net/pdf?id=ZSbsX1sFo3",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces the UNA framework, which integrates three prominent alignment techniques for LLMs: RLHF/PPO, DPO, and KTO. The UNA framework seeks to unify these methods by using a generalized implicit reward function to align language model policies through supervised learning. UNA also accommodates diverse feedback types (pairwise, binary, and score-based) and aims to outperform each technique on downstream tasks, simplifying the RLHF fine-tuning process."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. By combining RLHF, DPO, and KTO, UNA supports a range of feedback types (pairwise, binary, and score-based), which enhances its versatility.\n2. UNA replaces RLHF\u2019s unstable, memory-heavy RL process with supervised learning, making the alignment process more straightforward and efficient. \n3. The paper provides a mathematical proof linking the RLHF objective to a generalized implicit reward function."
            },
            "weaknesses": {
                "value": "1. Evaluation of the experiments is relatively small-scale. The authors only evaluate UNA over DPO & KTO when fine-tuning with LoRA and evaluate online RLHF using a 1.5B model. It may be difficult to prove whether UNA can take advantage when using large-scale models. I suggest the authors to try larger models like ~8B models since they claim that they use 8 x 80G A100 GPUs. \n2. The improvement of UNA over three baselines are not significant, which is considerate since they are somewhat equivalent. However, I think the authors should show the advantages of UNA, e.g., the time and computational cost, compared to baselines. \n3. The paper seems written in a rush with several clarity issues. For example, some references (Line 215 and 352) are missing."
            },
            "questions": {
                "value": "1. The paper states that $f(x)=0$ when implicit and explicit reward models are exactly the same. However, when we optimize a model using UNA with the simplified implicit reward $f(x)=c=0$, will it guarantee that it will lead to the convergence?\n2. DPO also use log pi / pi_ref as an indicator of the reward. However, this implicit reward cannot well reflect the training status. Can the UNA framework better stabilize the training process in the DPO manner?\n3. In Table 3, maybe you can try other reference models rather than GPT in Alpaca-eval? The win rates against GPT-4 is to low and difficult to compare among different models.\n4. Can you include the results of the original DPO?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides a unifying framework for value alignment that has:\n(i) explicit and implicit reward functions\n(ii) can handle different types of preferences (pairwise, binary, score etc.)\nand seems to provide better performance than state of the art approaches, RLHF, DPO, KTO."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The unified framework does seem to have significant advantages.\n2. The paper's ideas are strong.\n3. Experimental results are detailed and strong."
            },
            "weaknesses": {
                "value": "1. The biggest concern I have is that presentation is extremely poor, with many typos in notation, proofs, text and references. \n2. There are notational inconsistencies in proofs as well.\n3. There is no intuitive explanation for why such a unified approach would provide better performance than any of the individual approaches. \n\nI find that the paper is written in a rushed manner and even though the ideas are interesting, it does not seem ready yet with typos all over the paper. Happy to revise my scores based on the responses of the authors."
            },
            "questions": {
                "value": "1. What is the intuitive explanation for why a unified approach provides better results? DPO provides better results than RLHF as it does direct optimization of preferences, rather than first creating a reward function and then optimizing. In a similar vein, is there an intuitive explanation. \n2. There are errors in the main proof in Appendix B. While I seem to follow the broad outline, I am not entirely sure and would request the authors to provide more explanation and a corrected version. \"y\" is present on the left hand side, there is a \"y\" used in expectation. I am not sure how the expectation over \"y\" makes sense?\n3. Are the experimental improvements significant? The improvements are there, but are they significant? How can it be quantified?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an algorithm which they call UNified Alignment (UNA), by utilizing a simplified DPO implicit reward. UNA's loss minimizes the difference between the implicit reward and the explicit reward (provided by an existing reward model), hence can be generalized to different modes of feedback: pairwise, binary or scalar. Empirical results are presented to compare UNA with several baselines."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. There are math derivations for the proposed method.\n2. The reward distillation loss is  general for different modes of feedback."
            },
            "weaknesses": {
                "value": "W1. The paper's presentation does not meet the standard of ICLR. \n* Some references are missing (215,353); there are grammar and spelling errors (026,106,120,141). \n* Figures 1 and 2 are almost identical except for the different parts highlighted, which makes me feel they are redundant. You could refine the two figures and make them respectively focused on the topic you want to highlight: algorithm comparison & applications to different modes of feedback.\n* The overall tune is not formal, and the writing is not easy to follow (too many pieces). If your goal is to develop a unified framework for LLM alignment, in your methodology you would start from defining the overall process of alignment, then the objectives of it, followed by your approach. Maybe at the end you can make connections with existing methods like RLHF with PPO, DPO, KTO and say they are some special cases. This suggestion is only one possible way for improvement. There are certainly more that you could consider to improve the current version.\n\nW2. The technical novelty is limited. The implicit reward (eq6) of DPO is standard and well-known. One of the contributions is \"simplifying\" the implicit reward by setting f(x)=c=0. Could you provide insights behind it? Besides, the other contribution is a supervised loss of minimizing the difference between implicit and explicit rewards (eq7) is also not novel. For example, [1] proposed such a distillation-style loss for robust preference optimization.\n\n\n[1] Fisch A, Eisenstein J, Zayats V, Agarwal A, Beirami A, Nagpal C, Shaw P, Berant J. Robust preference optimization through reward model distillation. arXiv preprint arXiv:2405.19316. 2024 May 29."
            },
            "questions": {
                "value": "It seems to me UNA also needs a reward model to learn the policy, which is not too different from RLHF. But in your abstract, \"RLHF requires training the reward model and policy separately, which is complex, time-consuming, memory intensive and unstable during training processes.\" Can you clarify the advantage of UNA compared to standard RLHF? For example, it could be that UNA saves some computation, or converges faster, etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a method called Unified Alignment (UNA), which aims to unify existing alignment techniques like RLHF, DPO, and KTO. The authors argue that RLHF, though effective, suffers from high complexity and instability, while DPO simplifies the process but is limited in certain aspects, such as not fully leveraging the reward model and being constrained to pairwise preference data. The proposed UNA framework claims to generalize and improve upon these methods by creating a novel mapping between reward models and optimal policies, allowing it to handle different feedback types and improve performance across downstream tasks. The paper suggests that UNA can simplify, stabilize, and speed up fine-tuning processes while reducing memory requirements, and it reports outperforming RLHF, DPO, and KTO in experiments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "It is of interest that the authors try to narrow the gap between the RM-based and RM-free alignment methods."
            },
            "weaknesses": {
                "value": "The paper is not solid enough for me and the contribution is limited. The reasons are concluded as follow:\n\nFirst, It is not clear to me why the proposed method differs from DPO, thus the first contribution claimed is not novel at all. In my understanding, the DPO has already proved the equivalent between the optimal policy and the reward model's objective (under the BT model setting). Besides, it is also very confusing why the authors introduce f(x) and c.\n\nSecond, for the claimed contribution 2, the author used a objective introduced by DPO model to conduct a supervised learning, which is also confusing. What is the difference between using the proposed method to train an additional model and directly using a well-trained DPO model? And what if the DPO model is not well-trained, then the label would be very noisy? It is a very confusing step and I am open to more discussion.\n\nThird, the proposed method is compared to some baselines and is claimed to be better, however I believe the comparisons are not fair and the experimental results are suspicious.  In Table 1, the authors use 4 UNA variants and only use KTO and Mistral as the baselines, and the best scores in each benchmark are distributed to all 4 variants, so which one is the best? The authors also claim UNA perform better than RLHF while they didn't provide the experimental detail about RLHF. The proposed method is not even a RL algorithm since it doesn't shown any potential in exploration but more like a distillation process, it is meaningless to announce that it perform better than PPO regarding speeds or hardware consuming.\n\nThe presentation of the paper needs to be largely improved. There are many compile Errors such as line 215 in the 4th page. Given all these reasons, I will not recommend the paper to be accepted."
            },
            "questions": {
                "value": "Please refer to weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}