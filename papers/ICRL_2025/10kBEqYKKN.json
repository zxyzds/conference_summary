{
    "id": "10kBEqYKKN",
    "title": "Impact of Prompt on Latent Representations in LLMs",
    "abstract": "The effectiveness of zero-shot learning frameworks, particularly in Large Language Models (LLMs), has lately shown tremendous improvement. Nonetheless, zero-shot performance critically depends on the prompt quality. Scientific literature has been prolific in proposing methods to select, create, and evaluate prompts from a language or performance perspective, changing their phrasing or creating them following heuristics rules. While these approaches are intuitive, they are insufficient in unveiling the internal mechanisms of Large Language Models. In this work,  we propose exploring the impact of prompts on the latent representations of auto-regressive transformer models considering a zero-shot setting. We focus on the geometrical properties of prompts' inner representation at different stages of the model. Experiments conducted give insights into how prompt characteristics influence the structure and distribution of vector representations in generative models. We focus on binary classification tasks on which prompting methods have shown robust performance and show that prompt formulation has indeed an influence on latent representation. However, their impact is dependent on the model family. Using clustering methods, we show that even though prompts are similar in natural language, surprisingly, their representations can differ. This is highly model-dependent, demonstrating the need for more precise analysis.",
    "keywords": [
        "Explainability",
        "Representation analysis",
        "LLM",
        "prompting",
        "zero-shot"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "Geometrical analysis of prompt  impact on latent representation in LLMs",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=10kBEqYKKN",
    "pdf_link": "https://openreview.net/pdf?id=10kBEqYKKN",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates the effect of prompts on the representation of the EOS token across prompts and LLMs."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The idea of studying the effects of prompts through the geometry of latent representations is interesting."
            },
            "weaknesses": {
                "value": "- Studying only the representation of the EOS token seems to be an oversimplification. While this representation technically can depend on all of the context/generation, it might not attend to its meaningful parts, thus failing to capture interesting patterns.\n- The paper lacks clear/practical insights besides observing that the EOS token representation does depend on the prompt in some way.\n- For an empirically oriented work, studying only binary sentiment classification datasets is insufficient. Hypotheses should be verified across a broader range of tasks, including open-ended generation.\n\nOther:\n- Many typos throughout the paper (missing spaces and periods, inconsistent usage of citet and citep, etc.)\n- IsoScore is not defined in the paper. Only a high-level explanation is provided.\n- RIS is not carefully defined - is k' the same as c? \"value of k is equal to itself\" is not a clear statement."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates how prompts affect the intrinsic geometry of LLMs. The authors explore two research questions: (1) whether prompts alter the intrinsic dimensionality of representations, and (2) whether prompts can be grouped by their impact on model performance and vector representations using clustering methods."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper tries to address the intriguing question of how prompt representations relate to model performance, with a specific focus on how prompt formulation impacts performance robustness."
            },
            "weaknesses": {
                "value": "The paper's format is somewhat disorganized, with unexpected line breaks and misplaced punctuation. Also, some figures like figure 3 do not have illustrations, making it hard to understand."
            },
            "questions": {
                "value": "(1) What are the motivations of grouping the prompts and how the groups of prompts correlate to the focus of the model generation? Why do the authors use the last layer rather than the embedding layer for clustering the prompts? \n\n(2) What is the mathematical formulation of the Isoscore? What does it used for in the experiments?\n\n(3) Several studies, such as PromptRobust [1], focus on evaluating prompt robustness. How does this approach offer advantages over existing methods?\n\n(4) The paper offers limited technical contributions and focuses narrowly on the classification task. Could the work be applied to generation task? \n\n[1] Zhu, K., Wang, J., Zhou, J., Wang, Z., Chen, H., Wang, Y., ... & Xie, X. (2023). Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies how variations in the prompt change the distribution of hidden states in LLMs for zero-shot binary classification. The last hidden state representation is extracted at each layer for a variety of different prompts. The authors plot how the IsoScore of the hidden states evolves across layers. These hidden states are then clustered, based on which the authors conclude that \"[the] clustering tends to focus on other characteristics than the prompt itself.\""
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper studies a general and interesting question of how LLM internals (such as hidden states) evolve/change when certain parts of the input (such as the prompt) change. Further understanding of this process would be useful for several downstream applications, such as (as shown in prior work) detecting adversarial prompt injection attacks."
            },
            "weaknesses": {
                "value": "The biggest issue with the draft is the presentation of the results. There are numerous spelling, writing, and formatting errors, a subset of which I listed below. The first two paragraphs of the introduction are unnecessary background for the ICLR audience. The related work section on LLMs is not particularly related to the focus of this paper (understanding how prompts change internal model representations). That section is missing citations to LogitLens and its derivatives (e.g. https://arxiv.org/pdf/2303.08112), which also study how changes in the prompt change model internals. For example, that paper develops classifiers (based on features extracted from model internals) for detecting prompt injection. I.e., they already studied this paper's \"hypothesis 2\" that \"The geometrical characteristics are sufficiently discriminating to facilitate the grouping or separation of prompts and comprehend how the model processes them (HP2).\" The details of very relevant parts of the paper, such as a brief description of the IsoScore algorithm, are missing. Isotropy is never formally defined and the draft never explains why it is a desirable property for decoder-only models (beyond references to other work studying isotropy in the context of embedding models). The details of precisely how the IsoScore is computed are missing to the point where I'm not sure the results of the paper are reproducible.\n\nI think minor writing and presentation issues are not disqualifying, but in this case they are pervasive and make it difficult to judge the technical aspects of the paper on merit.\n\nSubset of writing issues:\n- L16 \"heuristics rules\" --> \"heuristic rules\"\n- L24 \"their impact\" --> \"its impact\"\n\n- first two paragraph of the intro are unnecessary bg for the ICLR audience.\n\n- L53 starts \"However\" but is not contradicting a previous point\n\n- L67 what does \"intrinsic\" mean here?\n- L67 double period\n- L69 missing period.\n\n- L74 \"LLMs representations leveraging prompting\" --> \"LLM representations that leverage prompting\"\n\n- L101 \"section 4\" --> \"Section 4\".\n\n- L123, 137, L286 have \\citet that should be \\citep\n\n- L174 \"information, Therefore\" --> \"information. Therefore,\"\n\n- L178 \"Dimensionnality\"\n\n- L184 \"PCA get some limitations\"\n\n- L214-L215 $k$ not in latex formatting\n\n- L284 \"We describe, as follow,\"\n\n- L290, L306 \"for instance :\", \"be written :\"\n\n- L303 \"we constraint the model\"\n\n- L317 \"First analyse results of isoscore are discussed\"\n\n- L322 \"Since IsoScore is a recent algorithms\"\n\n- L323 \"accross\"\n\n- L483 \"This allows us give answers\""
            },
            "questions": {
                "value": "- Why are we interested in isotropy of the EOS token hidden states?\n\n- Can we use the isoscore as a selection criterion for picking a good prompt? This possibility is hinted at in the introduction \"Our objective is to establish a direct correlation between the prompts, latent representations, and the model performance.\" but not deeply explored. The results sound negative on this front---\"we cannot link the IsoScore to the performance of the model and prompt\". How does this finding relate to the cited works that show isotropy is a good property for embedding models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}