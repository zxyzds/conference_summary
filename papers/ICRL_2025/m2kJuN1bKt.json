{
    "id": "m2kJuN1bKt",
    "title": "Reformer: A Deep Learning Model for Runtime Selection of Convolution Kernels",
    "abstract": "As neural networks grow larger, optimizing GPU kernel selection becomes increasingly essential to minimizing the time, cost, and energy demands of model training and inference. Current methods rely on hand-written rules-based heuristics, which often yield suboptimal performance, are labor-intensive to develop, and are difficult to adapt across hardware architectures and firmware releases. In this paper, we frame kernel selection as a sequence classification problem solved on the CPU, thereby leaving GPU resources free for user training and inference tasks. Traditional transformers are less effective in this context because CPU deployment limits the advantages of parallelism in attention mechanisms. In this regard, we propose the $\\Gamma$-block, which performs only three matmul operations compared to the six required by a transformer block, while maintaining the same depth in terms of learnable layers. Our experiments on the IMDB and Reuters datasets demonstrate that a small model based on the $\\Gamma$-block delivers comparable sequence classification accuracy to a similar model based on transformer blocks, while also providing faster inference times on the CPU. By stacking multiple $\\Gamma$-blocks, we develop a lightweight model for kernel selection, named Reformer. To train the model, we propose a novel approach that assigns optimality probabilities to kernels based on their runtimes, offering a more robust alternative to one-hot probabilities. We demonstrate the effectiveness of Reformer by integrating it into MIOpen for convolution kernel selection, achieving an average speed-up of approximately 3x in convolution operations on the AMD Instinct$\\texttrademark$ MI100 GPU.",
    "keywords": [
        "Reformer",
        "Kernel Selection",
        "Kernel Optimization"
    ],
    "primary_area": "infrastructure, software libraries, hardware, systems, etc.",
    "TLDR": "This paper proposes a novel transformer-inspired AI model to optimize GPU kernel selection and lower the training and inference time of deep learning models",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=m2kJuN1bKt",
    "pdf_link": "https://openreview.net/pdf?id=m2kJuN1bKt",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses the problem of optimizing GPU kernel selection for neural network operations, with a focus on convolution kernels in AMD's MIOpen library. The authors propose framing kernel selection as a sequence classification problem and introduce an efficient alternative to transformer block for CPU deployment. The proposed alternative, \u0393-block, requires 3 matmuls instead of transformer's 6, making it more suitable for CPU inference where parallelism is limited. They build a lightweight model called Reformer by stacking multiple \u0393-blocks and train it to predict the optimal kernels based on convolution problem features. To handle the challenge of noisy runtime measurements and the sensitivity of one-hot encoding, they introduce a novel probability assignment method that reflects the relative performance of kernels. Compared to existing hand-tuned heuristics, the Reformer model gives an average 3x speedup in conv ops on the AMD MI100 GPU."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Optimizing GPU kernel selection is an important problem with practical implications for reducing compute and energy consumption of deep learning workloads.\n* The paper presents a novel approach to kernel selection by recasting it as a sequence classification problem and introducing the \u0393-block as a more CPU-efficient alternative to transformer blocks.\n* Thorough experimental evaluation of \u0393-block. The authors demonstrate comparable accuracy with reduced inference and training times on standard text classification datasets. Integrating Reformer to real-world convolutional kernel selection in MIOpen also shows significant performance improvements over existing heuristics.\n* The paper is well-written and organized. The architectures of the \u0393-block and Reformer model are clearly described with helpful figures."
            },
            "weaknesses": {
                "value": "* While the authors show significant improvements in convolutional kernel selection, the evaluation is limited to the AMD MI100 GPU and the MIOpen library. It is unclear how well the Reformer model generalizes to other GPU architectures or kernel libraries (e.g. NVIDIA). Broader evaluation across different hardware and software environments would strengthen the generalizability claim.\n* The paper compares the Reformer model with hand-tuned heuristics and an off-the-shelf ResNet18 model but doesn't compare against other approaches like gradient boosting commonly used for kernel selection. Including such comparisons would provide a comprehensive assessment of Reformer's effectiveness."
            },
            "questions": {
                "value": "1. Have you tested the Reformer model and the \u0393-block on other GPU architectures or with other kernel libraries? How does the model performance vary across different hardware platforms?\n2. You mention that noise due to distributed benchmarking and environmental factors introduced inconsistencies in the dataset. How does this noise affect the model's predictions in practice, and what strategies could mitigate its impact?\n3. Have you considered comparing the Reformer model with other models (e.g., XGBoost) for kernel selection? If so, how do they perform in terms of accuracy and inference time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, this paper propose a new deep learning model, \"Reformer,\" for GPU kernel selection optimization. \nBased on \u0393-block, transformer reconstructs the kernel selection problem as a sequence classification problem, and works efficiently in CPU to provide faster inference time.\nExperiments show that transformer performs on average three times faster than MIOPen's manual heuristic, and automates the kernel selection process to simplify the deployment of AI hardware and software."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The research is CPU-friendly design. Therefore, the research may efficiently use CPU resources.\n2. The Reformer model reconstructs the kernel selection problem specifically for convolution operations as a sequence classification problem, and predicts the most suitable convolution kernel based on the features of the operation. This automates the selection process and quickly finds the optimal convolution kernel, improving overall model training and inference speed."
            },
            "weaknesses": {
                "value": "1. This study conducted experiments using a limited dataset and a restricted set of model comparisons. Are there no experiments conducted on other datasets?\n2. Although a comparison with MIOpen's manual heuristics was performed, there is a lack of comparison with other state-of-the-art kernel optimization techniques, making it necessary to further substantiate Reformer\u2019s performance improvements."
            },
            "questions": {
                "value": "1. It was previously mentioned that the structure of Reformer could be applied to other types of optimization problems. However, is there more concrete evidence that it guarantees performance improvements in kernel optimization for other operations as well?\n2. Could you provide experimental results on other recent datasets or state-of-the-art models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Deep learning models rely heavily on efficient convolution operations, and choosing the right GPU kernel for these operations is crucial for performance.  Traditionally, developers have relied on hand-crafted rules to select kernels, a process that's time-consuming, prone to errors, and often tied to specific hardware.  This paper introduces \"Reformer,\" a model that intelligently automates this kernel selection process.  Instead of relying on fixed rules, Reformer learns to predict the optimal kernel by treating the problem as a sequence classification task.  Cleverly, it performs this analysis on the CPU, freeing up valuable GPU resources for the actual deep learning computations.\n\nReformer presents a few key innovations: a streamlined building block called the \"\u0393-block\" that's efficient enough for CPU inference; a unique training method called \"ratio-preserving probability training\" which produces a more robust kernel selection model; and seamless integration with AMD's MIOpen library.  In tests on an AMD MI100 GPU, Reformer demonstrated a significant speed boost\u2014around 3x faster\u2014compared to existing hand-tuned methods.  This suggests that Reformer offers a promising path towards more adaptable and performant deep learning systems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**Originality:** The core idea of using a deep learning model for kernel selection is novel. While previous work has explored machine learning for kernel scheduling or parameter tuning, this paper tackles the more challenging problem of dynamically selecting the best kernel from a pool of options based on the specific operation and its operands.  The introduction of the \u0393-block, specifically designed for CPU inference in this context, also contributes to the originality. \n\n**Quality:** The paper demonstrates a thorough understanding of the kernel selection problem and the limitations of existing methods. The proposed Reformer model is well-motivated and carefully designed. The experimental results, including the integration with MIOpen and the 3x speedup achieved, provide strong evidence of the effectiveness of the approach.  The comparison with a ResNet18 baseline further strengthens the argument for Reformer's efficiency.\n\n**Clarity:** The paper is generally well-written and easy to follow. The figures effectively illustrate the architecture of the Reformer model and the \u0393-block, and the tables clearly present the experimental results. The paper provides sufficient background information on kernel selection and related work to contextualize the contributions.\n\n**Significance:** The work addresses a critical bottleneck in GPU-accelerated deep learning: efficient kernel selection. The proposed method has the potential to significantly improve the performance and reduce the cost of deep learning training and inference. The automation offered by Reformer also simplifies the development and deployment process for GPU software and hardware.  Furthermore, the paper's approach of using deep learning to optimize deep learning hardware opens up exciting new possibilities for future research in this area."
            },
            "weaknesses": {
                "value": "**Hardware Specificity:**  The experimental evaluation primarily focuses on convolution kernels within MIOpen and AMD GPU. While this is a significant proof, it would be beneficial to assess Reformer's generalizability by applying it potentially to other kernel libraries like cuDNN.  \nOr please add discussion about how the proposed approach can be applied to other libraries if there is any difficulty in testing on Nvidia GPU. It's important to investigate how Reformer performs on different GPU architectures (both AMD and Nvidia) and across varying hardware generations. This would demonstrate the broader applicability of the proposed method.\n\n**Ablation Study:**  A more detailed ablation study would be helpful in understanding the contribution of each component of the Reformer model.  For example, how does the performance change with varying numbers of \u0393-blocks?  Is the ratio-preserving probability training truly superior to softmax in all cases, or are there scenarios where softmax performs comparably?  From table 2, the one-hot, softmax and ratio-preserving probability actually concur with each other? (ratio-preserving probability indeed has more salient probability value) \n\n**Scalability:** The paper focuses on a small model size, justifiable for kernel selection. However, it's crucial to discuss the scalability of the approach. How does the training time and inference latency of Reformer scale with increasing model complexity (i.e., for larger numbers of kernels and input features)? This information would be relevant for considering the potential application of Reformer to other domains beyond kernel selection.\n\n**Noise Robustness:**  While the ratio-preserving training aims to address noise in runtime measurements, a more direct analysis of Reformer's robustness to noise would be valuable.  For instance, artificially introducing varying levels of noise into the training data and measuring the impact on accuracy would provide a clearer picture of the method's resilience to real-world variations in runtime measurements."
            },
            "questions": {
                "value": "- Questions listed in weakness section\n- You are talking about convolutional kernel selection, I feel it a bit strange to use text classification as case study, wouldn't the image classification problem be more natural? This could also expand the generalizability of the approach. a CIFAR classification would be sufficient"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The Reformer paper introduces a deep learning model designed to optimize convolution kernel selection for GPUs, significantly improving runtime efficiency. By utilizing the novel \u0393-block, which reduces the number of matrix operations compared to traditional transformer blocks, Reformer accelerates kernel selection while maintaining comparable accuracy. Integrated into AMD's MIOpen, the model achieves a 3x speed-up in convolution operations. It replaces labor-intensive manual heuristics with a machine learning approach, treating kernel selection as a classification task using runtime data to predict optimal kernels. The paper also highlights how Reformer selects convolution kernels to enhance runtime performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper presents the following strengths. Its introduction of the \u0393-block reduces the number of matrix operations compared to traditional transformer blocks, leading to significant improvements in runtime efficiency, including a 3x speed-up in convolution kernel selection. The novel approach frames kernel selection as a machine learning classification task, replacing labor-intensive hand-tuned heuristics with a more automated and accurate method. The model is also lightweight and optimized for CPUs, which frees up GPU resources for training and inference, making it ideal for practical deployment. Its adaptability to various hardware architectures, with the ability to automatically retrain when firmware or hardware changes, adds to its scalability across different systems."
            },
            "weaknesses": {
                "value": "The paper has weaknesses that limit its broader impact. Its scope is primarily focused on optimizing convolution kernel selection, and it does not explore the generalizability of the approach to other deep learning operations or tasks. Additionally, while the \u0393-block demonstrates efficiency gains, the paper lacks a detailed comparison of computational complexity with other cutting-edge methods, like Transformers, making it harder to evaluate its scalability on larger models and datasets. Testing is also primarily limited to GPUs, with little discussion of how the model performs on other hardware, such as GPUs or TPUs. Furthermore, while the model streamlines kernel selection, the initial benchmarking and training process needed to build the dataset may introduce overhead when adapting to new environments. The approach depends on accurate runtime data, which can be noisy or inconsistent, potentially affecting the robustness of the model in real-world scenarios. The approach also doesnt compare with other similar work such as AutoTVM."
            },
            "questions": {
                "value": "A few comments and questions to improve the paper:\nIn Figure 3, the way of depicting a linear layer or matrix multiplication could be simplified for clarity, reduce amount of text in the diagram. \nAdditionally, it's worth noting that there is already a transformer variant named \"Reformer,\" so the author maight want to rename it. \nHow does the Reformer model compare with other compilation techniques, such as Auto-TVM or the PyTorch compiler? \nWhy does the paper focus solely on optimizing CNN kernels, rather than exploring optimizations for other types of deep learning operations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "A sequence of kernel programs are executed to run a model on processors. To maximize the model execution efficiency, usually there will be many candidates for each kernel and the best one will be picked up during runtime, and this process is called kernel selection. This paper proposes a new transformer-variant called Reformer to model the kernel selection as a sequence classification problem and run the proposed model on CPU to save the GPU resources. Instead of using one-hot label or softmax sore as the optimization target, authors proposed to use optimality probabilities. Experiments show that the proposed kernel selection model selects better kernels than the MIOpen hand-written heristics for the convolution kernels on AMD MI100 GPU."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper is easy to understand.\n2. The proposed method can be executed only on CPU, which saves the GPU resources during kernel selection."
            },
            "weaknesses": {
                "value": "1. The motivation to use a new transformer-variant instead of original transformer architecture is weak.\n2. The evaluation is weak as well.\n\nSee comments below for details."
            },
            "questions": {
                "value": "1. The motivation to use a new transformer-variant instead of original transformer architecture is weak.\nFrom Table 1, we can see that the proposed block is about 25% faster than transformer block. In Section 4, the authors report that the proposed kernel selection model using the new block design only takes 49 macro-seconds. If we directly use transformer model with similar number of parameters as the proposed model, can we achieve similar selecting performance (like <0.1 ms)? If so, why do we need to use a new architecture design when the model execution efficiency is not a bottleneck?\n\n2. The evaluation is weak as well.\nThe paper only evaluates the hand-tuned heristics baselines. But the kernel selection (or operator/kernel tuning) problem has been studid widely in deep learning compiler domain. Many other baselines are used like using simple linear layers to predict latency, use xgboost to predict or select kernels, or use evolutionary algorithms to select best kernels (see AutoTVM [1]). The paper does not compare with these baselines. \nIf the main novelty of the paper is to propose a new transformer-variant, then a bunch of other transformer-variants should be compared as well.\n\n[1] TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}