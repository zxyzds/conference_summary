{
    "id": "x8mr9zGkpr",
    "title": "Attributing Model Behavior: The Predominant Influence of Dataset Complexity Over Hyperparameters in Classification",
    "abstract": "Understanding how different factors contribute to model performance is critical for advancing the attribution of model behavior. Previous research has independently explored the effects of hyperparameters and dataset complexity meta-features on classification performance. However, a key knowledge gap remains in understanding how these factors comparatively influence model behavior, particularly bias and variance. This study bridges this gap by investigating which factors, hyperparameters or complexity meta-features, exert a greater influence on the bias-variance in classification algorithms, specifically Random Forests (RF) and Support Vector Machines (SVM). Using 290 diverse OpenML tabular datasets and 304 hyperparameter configurations, we employ functional analysis of variance (fANOVA) to quantify the impact of these factors. Our findings reveal that dataset complexity meta-features exert a more significant influence on both bias and variance than hyperparameters for both RF and SVM models. To further substantiate our findings, we conducted an analysis based on the Manipulation Theory of Causation. This analysis demonstrates that optimizing dataset complexity can simultaneously reduce bias and variance, while hyperparameter tuning often leads to a bias-variance trade-off with limited impact on overall performance. To the best of our knowledge, this research is the first to directly compare the effects of hyperparameters and complexity meta-features on bias and variance in classification, contributing new insights into model behavior attribution.",
    "keywords": [
        "Model Behavior Attribution",
        "Complexity Meta-features",
        "Hyperparameters",
        "Bias-Variance Decomposition"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=x8mr9zGkpr",
    "pdf_link": "https://openreview.net/pdf?id=x8mr9zGkpr",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes an analysis of the influence of hyperparameter tuning and training \"data complexity\" (the author call it \"complexity meta-features\") on the performance of two classic classification algorithms:SVMs and Random forests. the paper includes  run extensive experiments on 290 OpenML tabular datasets. The author's end with a summary of their findings: dataset complexity matters the most."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1) the paper is easy to read\n2) The paper confirms a fact that is well know by most data science / ML prectioners, the complexity of the data set matters for classification performance."
            },
            "weaknesses": {
                "value": "1) The content, experiments and conclusions of the papers are very outdated. It reads like a paper that was written 15-20 years ago. Most citations are from many years ago. Hence the contribution are practically irrelevant to the current state of ML / data science in 2024. Hence there is no significant contribution or relevance to the ICLR community.\n\n2) Furthermore, the main conclusion of the paper is that dataset complexity (class overlap, dimensionality, etc) matters when training a classifier (RF or SVM). These are well known fact that are thought in introductory ML class and hence there is no new information provided here."
            },
            "questions": {
                "value": "I dont have any follow-up questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the relative influence of dataset complexity and hyperparameters on classification model behavior, specifically for RF and Kernel SVM. \\\nAuthors utilize the fANOVA framework and OLS to quantify the influence on bias and variance brought by dataset complexity and hyperparameters. \\\nBased on the analysis across 290 datasets and 304 hyperparameter configurations, the study finds that dataset complexity meta-features\u2014such as class overlap, data sparsity, and class imbalance\u2014have a more substantial impact on bias and variance than hyperparameters."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper addresses a gap by directly comparing the impacts of dataset complexity and hyperparameters on model performance. Previous studies have often examined these factors separately, but this research provides a unified framework to assess their relative influence on bias and variance. \n\nThe paper comprehensively considered nearly 300 datasets and over 300 configurations, enabling a more convincing conclusion.\n\nApart from the numerical results, the paper also includes very detailed arguments of why this happens and what this indicates.\n\nSome of the estimated coefficients shown in the OLS summary table do align with our common understanding of how random forests deal with bias-variance tradeoffs, e.g. coefficients associated with min samples leaf, bootstrap, and max features."
            },
            "weaknesses": {
                "value": "1. lack of details on the experiment design. \n\n (1) as the paper claims, nearly 300 datasets of varying sample sizes, response categories, and feature dimensions are used. Why are they comparable? I believe 0-1 loss is not a typical loss function people use for multiclassification problems.  And high-dimensional datasets wouldn't react necessarily the same as n>p datasets in terms of hyperparameters. \n \n(2) hyperparameters like C and gamma have huge variations in scale. How is it being included in OLS? Is it logarithmized?\n\n (3) For readers not familiar with meta-features of datasets, it would be very helpful to at least sketch some general ideas of how these meta-features are defined. Are those features immune to data transformation?  The same for how fANOVA works. \n\n\n2. lack of details on why the experiments are conducted in such a way. \n\n (1) In my perspective, the number of trees is RF's one of the most important hyperparameters. Why is this not considered?\n \n(2) I believe the neural net is the framework that people are most curious about. The authors also mention it in the introduction. Why is that not considered? \n \n(3) Based on the pymfe package, there are plenty of meta-features that characterize data complexity from different perspectives. Why specifically these 3, N1, T2, C1, are chosen?\n\n3. Some of the results that are confusing to me. \n \n(1) if those meta-features are immune to data transformation, how can we benefit from your research even though we know that data complexity itself is much more important than tuning hyperparameters? if not, shouldn't you include some examples of how bias and variance are reduced after some preprocessing of the data that reduces data complexity? For example, class imbalance issues can be alleviated by reweighing samples or bootstrapping.\n\nIn general, I do agree that the data's quality is much more important than tuning parameters. If the data is always linearly separable, I believe logistic regression would suffice. It's just the data quality is not something we can work on but the model and the model's parameter choice. Please correct me if I am wrong. \n\n (2) Based on your OLS example, the features included are all significant neq to 0. If the trend is determined, does it mean that choosing a smaller c or some certain kernel can always help with the prediction?"
            },
            "questions": {
                "value": "please check the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Authors compare impact of dataset complexity and hyperparameter tuning on the performance of binary Random Forest and SVM classifiers. They perform extensive experiments which support their finding that the dataset complexity has dominant impact on the performance."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- extensive experiments\n\n- clearly written, easy to read\n\n- the topic of bias/variance tradeoff is very important"
            },
            "weaknesses": {
                "value": "I think the paper sends, in essence, a wrong message to readers.\nAuthors are basically suggesting that hyperparameter optimization\nisn't useful, which I disagree with.  As I write this, I am tuning\nhyperparameters of a neural network classifier, and the AUROC has gone\nfrom 0.55 to 0.75, exclusively due to the (gradually improving) choice\nof hyperparameters. The conclusion is at best narrowly limited to SVM\nand RF, but that makes the manuscript 1) not that useful, given\nlimited scope 2) misleading, since many readers may walk away with a\nwrong impression that the conclusions apply generally \n\nTo be clear, not disagreeing with the idea that hyperparameter tuning\nhas a natural limit, and going beyond that may require additional or\ndifferent data. But the paper leaves an impression to the reader that\nhyperparameter tuning doesn't help in general, which I disagree with. At a minimum, the title should say that the results are limited to SVM and RF binary classifiers. \n\nAlso keep in mind that \"optimizing dataset complexity\" is a vague and\nhardly actionable advice. I personally don't quite know how to\noptimize dataset complexity, whereas hyperparameter tuning is well\nunderstood. This should be clearly stated/discussed."
            },
            "questions": {
                "value": "No questions, but recommendations for future submission:\n\n- drop the analysis of dataset complexity vs. hyperparameters; just\n  focus on the impact of hyperparameters on the bias/variance\n  trade-off. It is sufficiently important topic on its' own. Impact of dataset complexity is not useful because 1) it is well understood that dataset complexity has major impact on classifier performance 2) it is not clear what to do about it.\n\n- include multi-class problems\n\n- include XGBoost and neural networks in the analyses. I don't think\n  one can make practically useful conclusions without including those\n  state-of-the-art classifiers. I would personally also add logistic\n  regression\n\n- consider adding image classification datasets in your analyses. If\n  the behavior - in terms of bias/variance, and hyperparameter tuning\n  contribution - is quite different from tabular data, that would be a\n  valuable result\n\nI think this *could* become a good paper, but not without extensive\nrevision, which is not feasible in the ICLR timeframe.\n\n\nMinor points\n\n3.3\n---\n\n- 2 out of 3 is not really \"generally\". Please just be specific: for\n  N1 and T1, higher values indicate greater classification\n  difficulty. For C1, lower values indicate greater classification\n  difficulty. That would be simpler and easier to read. \n\n  \"Higher values of these meta-features generally indicate greater\n  classification difficulty (except for C1).\"\n\n4.2.2\n-----\n\n- \"When considering variance, a similar trend emerges. According to\n  the fANOVA results (Figure 3b), C1 continues to dominate, accounting\n  for 37.78% of the variability in variance.\"\n\n  I wouldn't call this a similar trend. For bias, C1 accounts for 71%,\n  for variance, 38%. Please point out that the C1 impact on variance is significantly lower than on bias. This suggests other factors play greater role. Discuss what those factors might be."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper focuses on the attribution of predictive model behavior. In particular, it describes a comprehensive empirical comparison of the influence of hyperparameters and dataset meta-features on the bias and variance of classifiers. The analysis utilizes functional ANOVA. The main result is that most of bias and variance can be attributed to dataset characteristics, as opposed to hyperparameters."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Well structured and clearly written paper.\n- A many ways a very comprehensive experiment.\n- Tackles an important issue for ML practitioners."
            },
            "weaknesses": {
                "value": "I have two main concerns. The first is that the experiments, while in some ways very comprehensive, are in other ways very limited:\n\n- Only two classification algorithms.\n- No missing values in the dataset is a very strong criterion.\n- A limited number of complexity measures.\n\nThe second is that when carefully interpreted, the results are not that general or actionable:\n- N1 is in essence a classifier (probably along the lines of LDA). So the results can basically be summarized that most of the bias and variability of RF and SVM can be explained by running another reasonable classifier and seeing how it performs. That of course makes perfect sense, but it can also be derived from what we already know, that classifiers tend to perform similarly (the differences between classifiers are less than the differences between datasets).\n- We should be more careful when interpreting the result that model performance can be attributed more to dataset characteristics than to hyperparameters. First, it is the nature of commonly used classifiers that they are relatively robust in terms of hyperparameter selection - being easy to tune is what makes them popular. Second, . And third, the range of several parameters is limited. For example, would results change if max_features was allowed to go below 0.1 or above 0.9? Or if 20 different kernels were considered? Similarly, the experiments are limited to 1500 features, which diminishes the importance of regularization.\n- In practice, I can in most cases freely tune the parameters and select models. I can't really change my problem (or dataset) though.\n- The paper does not consider model selection, which I would in this context consider as part of hyperparameter tuning. I would not be surprised that a lot more can be attributed to model selection than to tuning the parameters in this paper. Choosing a different model is also actionable.\n\nThere are also other methodological concerns (see Questions)."
            },
            "questions": {
                "value": "1. fANOVA is one of many ways of decomposing model predictions. Why this approach? And are there any potential issues due to taking into account pairwise interactions only?\n2. l. 60: How can normalization or scaling affect the intrinsic complexity of the datasets? Intuitively, wouldn't a reasonable measure of complexity be invariant to these? Of the three in the paper, C1 is invariant. T2 is not, because it depends on PCA, but that just makes me wonder if T2 even makes sense. I wouldn't want my dataset to become more complex, just because I convert a feature from meters to centimeters. N1 is based on a two-sample test, so, while I didn't go into the details of the test, I'd assume that we'd like all of our two sample tests also to be invariant to scaling. ... To clarify, I'm not criticizing the choice of not scaling, we can argue either way. But I am concerned about the use of complexity metrics that are not invariant.\n4.But not normalizing does have an effect on SVM and regularization? This is not how we would apply SVM in practice.\n5. N1 is a bit outdated. Two sample tests have progressed a lot in recent years. In particular, tests based on machine learning models directly or using classification performance as a proxy.\n6. It seems that N1 would fail to be attributed when the dataset is so complicated that RF and SVM can perform well, but the test in N1 doesn't?\n7. The attribution to C1 for SVM is to me the most surprising result in the paper. Any explanations of this difference between SVM and RF? How correlated are N1 and C1?\n8. l. 658: How reasonable is this assumption that there are no hidden confounding factors?\n9. Parameter configurations were generated by sampling uniformly and independently from each hyperparameter range? I'm asking because of the following scenario. Let's say that the optimal range for a parameter is relatively narrow (0 - 0.05), while after that (0.05 - 1.0) the model performs majority class baseline poorly. Because the range of good values is small, this, as a variable in a regression, would not be that important. So, it would not get much of an attribution in the experiment, but it is definitely important in practice. In other words, isn't the importance of a hyperparameter determined by the difference it can make, not by the variability of the performance over some arbitrary set of its values?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}