{
    "id": "bRa4JLPzii",
    "title": "CoMRes: Semi-Supervised Time Series Forecasting Utilizing Consensus Promotion of Multi-Resolution",
    "abstract": "Long-term time series forecasting poses significant challenges due to the complex dynamics and temporal variations, particularly when dealing with unseen patterns and data scarcity. Traditional supervised learning approaches, which rely on cleaned, labeled data, struggle to capture these unseen characteristics, limiting their effectiveness in real-world applications. In this study, we propose a semi-supervised approach that leverages multi-view setting to address these limitations. By introducing a consensus promotion framework, we enhance agreement among multiple single-view models using unseen augmented data. This approach not only improves forecasting accuracy but also reduces error accumulation in long-horizon predictions. Furthermore, we explore the impact of autoregressive and non-autoregressive decoding schemes on error propagation, demonstrating the robustness of our model in extending prediction horizons. Experimental results demonstrate that our proposed method not only surpasses traditional supervised models in accuracy but also exhibits greater robustness when extending the prediction horizon.",
    "keywords": [
        "Time series forecasting",
        "Multi-scale",
        "Semi-supervised learning"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "we propose a novel semi-supervised time series forecasting utilzing con",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=bRa4JLPzii",
    "pdf_link": "https://openreview.net/pdf?id=bRa4JLPzii",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses challenges in long-term time series forecasting, particularly in managing unseen patterns and data scarcity. The authors propose CoMRes, a semi-supervised, multi-view approach to enhance forecasting by promoting consensus among multiple models on augmented, unseen data. This method aims to improve prediction accuracy and reduce error accumulation over extended horizons. Additionally, the study examines both autoregressive and non-autoregressive decoding schemes, emphasizing the robustness of non-autoregressive decoding in minimizing long-term error propagation. Experimental results highlight CoMRes's superior performance and stability compared to traditional supervised models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The strengths are as follows:\n\n- The paper is clear, well-written and presented.\n- The authors cover most of the commonly used datasets in the field, and some relevant baselines: the empirical constributions appear sound as a result.\n- Results indicate the method does indeed provide some life over the baselines."
            },
            "weaknesses": {
                "value": "The weaknesses are as follows:\n\n- Absence of submitted code making reproducibility more difficult.\n- The paper could benefit from more detailed comparisons with other time-series forecarsting methods.\n- It would be interesting to include error bars on the results table, especially given the proximity of different methods."
            },
            "questions": {
                "value": "- Could the authors add error bars?\n- What are the author's plans wrt. the release of code?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose CoMRes, a long-term forecasting architecture based on Pathformer, which leverages a consensus promotion mechanism for multi-resolution views. The core idea is to align the supervised predictions of multiple resolutions with the aggregated prediction. Furthermore, they use augmentation strategies (Time Warping, Interpolation and Noise Injection) to construct the different views.  The work also presents an evaluation using autoregressive and non-autoregressive decoding schemes, aiming to show the robustness of the forecasts in longer prediction horizons. The authors claim that CoMRes outperforms traditional supervised models, exhibiting greater robustness and accuracy on multiple popular datasets."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper introduces an interesting concept of promoting consistency among multi-resolution views, which has potential in capturing complex temporal relationships.\n\n- The amount of datasets used in the experimental section is exhaustive, which adds impact to the of the experimental results.\n\n- The addition of limited-resource scenarios is valuable, as this is often the case in time series applications.\n\n- In general i think the writing of the paper is good and understandable.\n\n- The discussed related work seems to be exhaustive."
            },
            "weaknesses": {
                "value": "- The title and abstract do not clearly explain why the proposed approach is semi-supervised. It is challenging to understand how forecasting can be performed in a \"semi-supervised\" manner, given that \"labels\" can be constructed by shifting the time window. It would be helpful to provide more concrete explanations on this aspect, particularly regarding the proposed use of augmented data and the challenges with temporal dynamics.\n\n- In lines 031-033, the statement about the superiority of transformers in forecasting is misleading. Multiple recent works (Das et al., 2023; Wang et al., 2024; Zeng et al., 2023) have shown that transformers are not always state-of-the-art. This statement should be reconsidered or at least clarified with respect to specific contexts or benchmarks.\n\n- The comparison against relevant baselines is lacking. The paper does not include benchmarks like TimeMixer, TiDE, PatchTST, D/NLinear, and other advanced transformer-based models (e.g., iTransformer). Including these baselines would significantly strengthen the evaluation and provide a better context for the proposed model.\n\n- The tables, particularly Table 1, are difficult to read. I'm not sure what red or blue means at least its not stated in the caption.\n\n- The baseline method achieves the best result multiple times without the proposed augmentations, which weakens the argument for using the augmentations. In general I'm pretty confused about what the idea of multiple resolutions vs augmentations is in this work.\n\n- While the limited-resource scenario is a useful addition, it does not effectively demonstrate how the method performs on datasets with many variables (e.g., Traffic or Electricity). This should at least be stated in the limitations/conclusion section.\n\n## Refs\nDas et al., Long-term forecasting with TiDE: Time-series dense encoder. (TMLR 2023)\n\nWang et al., TimeMixer: Decomposable multiscale mixing for time series forecasting. (ICLR 2024)\n\nZeng et al., Are transformers effective for time series forecasting? (AAAI 2023)"
            },
            "questions": {
                "value": "1. In line 032, the paper cites Wang et al., suggesting multi-resolution methods \"overlook consistency.\" Could you provide specific details about what was missed in the multi-resolution reconciliation methods used by Wang et al.?\n\n2. In Section 3, you mention that augmentations are less effective in forecasting compared to classification and anomaly detection. Could you elaborate on why this is the case? Specifically, how does your chosen augmentations differ?\n\n3. Why do you assume a Euclidean distance to measure consensus between the aggregated prediction and each view? Would other measures, such as soft-DTW, work better?\n\n4. Does your framework also work with more \"complex\" augmentation strategies beyond the three mentioned (time warping, noise injection, interpolation)? Maybe discussing the limitations is a valuable contribution.\n\n5. Could you clarify whether MRes is your method (with ablated components)? I would not count that as a baseline.\n\n6. I'm a bit confused, at what points do you use augmentations, and at what different resolutions (scales)? Could you describe this process in more detail?\n\n7. The paper claims superior robustness compared to traditional supervised learning approaches. Could you clarify how exactly robustness was demonstrated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a \"consensus promotion\" learning objective to enhance the consistency of multi-scale time series data predictions. Additionally, given this learning objective is well-suited for semi-supervised learning, the authors introduce data augmentation strategies aligned with the proposed framework to further improve model generalizability. Extensive experiments are conducted on various datasets and ablation settings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(1) This paper proposes and investigates a novel perspective on improving time-series prediction tasks: the consistency of predictions across different patch sizes of time-series data, which represents an interesting and promising research direction."
            },
            "weaknesses": {
                "value": "The main weakness of this paper lies in its evaluation. \n\n(1) The variance of model performance and metrics is not reported for any experimental results, making it difficult for readers to assess whether the minor improvements are consistent and significant or simply due to stochastic gradient descent. The authors are encouraged to report variances for all results and conduct comprehensive statistical tests to demonstrate whether the improvements are statistically significant.\n\n(2) There is no experimental evidence to demonstrate whether the proposed consensus promotion is actually beneficial. For example, what is the exact prediction MSE for each \"individual-view model\"? Do they actually become more consistent after applying the unsupervised consistency loss? Additionally, in Table 1, if I understand correctly, the only difference between \"MRes (SL)\" and \"MRes w. consensus\" is that \"MRes w. consensus\" includes additional consensus promotion learning objectives. However, there are no consistent patterns indicating which model's MSE is generally smaller. The authors are encouraged to design clear experiments that demonstrate whether the proposed consensus promotion works as expected and whether it is beneficial for the prediction task."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper discusses multi-resolution long-term time-series forecasting (LTSF). This work extends the Pathformer architecture with two components: data augmentation, and consensus-based training loss for multiple MLP outputs. Comparison between the ablated components and Pathformer is held on widely used time-series prediction benchmarks. Proposed CoMRes improves over the baselines.  Further analysis is done on resource-efficient case."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1) The multi-resolution time-series forecasting task is an important direction.\n\n2) Authors have done an in-depth study on the ablation study."
            },
            "weaknesses": {
                "value": "1) I am uncertain about the novelty of the proposed paper. This paper borrows most of the architecture from the Pathformer, except the MLP for the individual component output and the aggregation layer. Furthermore, the consensus-based loss that minimizes the relative difference can be naturally formulated while utilizing the ensemble. I feel like this paper is a sheer extension of the Pathformer and the previous methods that utilize data augmentation. \n\n2) The experiment results do not convince the efficacy of the method. MRes with consensus training often underperform over the MRes baseline. Furthermore, performance differences between the data augmentations are small. This questions the efficacy of each component as a general method. Finally, ComRes drastically underperforms Pathformer in one dataset, which questions the efficacy of the whole algorithm in general.\n\n3) The extra MLP layer may induce an extra computation burden on the training and inference stage, especially since (M+1) calculations are all required for the final computation. Maybe further analysis of the computation efficiency will further help to understand the limitations."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}