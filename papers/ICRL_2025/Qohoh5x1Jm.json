{
    "id": "Qohoh5x1Jm",
    "title": "Reformulating Strict Monotonic Probabilities with a Generative Cost Model",
    "abstract": "In numerous machine learning contexts, the relationship between input variables and predicted outputs is not only statistically significant but also strictly monotonic. Conventional approaches to ensuring monotonicity primarily focus on construction or regularization methods. This paper establishes that the problem of strict monotonic probability can be interpreted as a comparison between an observable revenue variable and a latent cost variable. This insight allows us to reformulate the original monotonicity challenge into modeling the latent cost variable and estimating its distribution. To address this issue, we introduce a generative model for the latent cost variable, referred to as the Generative Cost Model (\\textbf{GCM}), and derive a corresponding loss function. We further enhance the estimation of latent variables using variational inference and importance sampling, which reformulate our loss function accordingly. Lastly, we validate our approach through experiments on an artificial gamble simulation and two public datasets, demonstrating that our method significantly outperforms traditional techniques.",
    "keywords": [
        "monotonic model",
        "variational inference",
        "generative model"
    ],
    "primary_area": "generative models",
    "TLDR": "We propose a generative method that solves the monotonic modeling task.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Qohoh5x1Jm",
    "pdf_link": "https://openreview.net/pdf?id=Qohoh5x1Jm",
    "comments": [
        {
            "summary": {
                "value": "The paper models a conditional monotonic distribution as a marginalization over latent variables. The key idea is to modify the monotonic modeling problem into modeling an element-wise cumulative distribution function (over a cost variable C). To actually model the latter, the authors introduce a latent variable modeling problem and solve it via a standard importance-weighted likelihood estimate with or without an additional ELBO term. The paper presents improved results in two experiments over a variety of baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Clean reformulation of the monotonic problem into a classification over a latent variable.\n- Well-motivated techniques to solve the problem.\n- Comparison against a variety of baselines."
            },
            "weaknesses": {
                "value": "No confidence intervals in the results tables. It's not clear that the improvement achieved by GCM is large enough (even over the simple MLP) to justify the much more complicated modeling procedure."
            },
            "questions": {
                "value": "- How wide are the confidence intervals for each of the metrics? Just do standard bootstrap if possible and report please.\n- Why introduce the extra prior $p_\\theta(Z)?$ Also, why is the prior different from $\\pi(Z)$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "-"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel approach to model strict monotonic probabilities. Without loss of generality, it considers the binary classification formulation $y|x, r\\sim Bernoulli(y;G(x,r))$, where $G(x,r)$ is a function that is monotonic in $r$. The target is to learn the function $G$. Instead of directly learning $G$, the paper introduces a cost variable $c$ and reformulates $G(x,r)$ as an integration $\\int_{c<r} p(c|x)d c$. The paper then introduces a generative cost model to approximate the conditional distribution $p(c|x)$."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper tackles an interesting problem of modeling monotonic probabilities. The problem itself is important, and the reformulation proposed by the paper is unique. Extensive experiments are conducted to support this new method."
            },
            "weaknesses": {
                "value": "The paper lacks theoretical results on the finite sample efficiency of the proposed algorithm. For the experiment design, an important setup that requires strict monotonicity is quantile regression, where the conditional quantile should be monotonic in the quantile argument. It would be interesting to see experiments designed for it and a comparison with the existing benchmarks."
            },
            "questions": {
                "value": "No additional questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes the Generative Cost Model (GCM) to enforce strict monotonicity by modeling a latent cost variable, using variational inference and importance sampling. This approach avoids architectural constraints and outperforms traditional methods on synthetic and real datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper seems to present a novel approach to formulate the problem of strict monotonic probability, it provides convincing theoretical analysis and emprical results."
            },
            "weaknesses": {
                "value": "The paper lacks empirical validation across diverse, high-dimensional real-world datasets, limiting the demonstrated generalizability of the Generative Cost Model (GCM). Moreover, the code was not provided in the supplementary materials to assess the reproducibility of the results."
            },
            "questions": {
                "value": "1. Can the authors provide additional justification or empirical validation for the assumption of conditional independence between the latent variable z and revenue r given x, as this assumption may affect model flexibility in real-world applications?\n\n2. Could the authors elaborate on the computational efficiency of the Generative Cost Model (GCM) compared to traditional monotonic models, especially when applied to high-dimensional datasets? How does the computational time compare to the benchmarks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a latent variable model capturing both a monotonic part and a part that need not be monotonic. They give some background, and then propose a loss function for a variational training with neural nets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "It is an interesting problem and their construction is quite novel and creative."
            },
            "weaknesses": {
                "value": "The objective function and implementation leaves a lot of details missing and unexplained. The paper does not inspire confidence in the method."
            },
            "questions": {
                "value": "Overview with some questions embedded:\n\nLines 054-059 could be clearer with a graphical model diagram. Edit: there is one later, nice.\n\nLine 063 may not be quite right; p_theta(x,r) cannot be ignored in the evidence unless you drop the dependence on theta. Maybe rework the setup / explanation a bit, this shouldn\u2019t be a big issue.\n\nLine 101 equivalent to what?\n\nLine 150 consider the machinery within https://arxiv.org/abs/2301.11695 and also the separate line of work on normalizing flows that all involve e.g. invertibility, monotone transformations, etc.\n\nLine 210 (0,1) should be {0,1}\n\nLine 189 should **y** and r have the same dimension?\n\nLine 289 elementwise\n\nLine 304 looks like it would suffer from high variance since pi(z) is fixed but p(z|x) depends on x.\n\nLine 323 the combination of losses looks a little suspicious.\n\nLine 323 Note that IWAE (Burda) is equal to ELBO (Jordan et al) for IWAE number of samples set to one; why is adding ELBO to IWAE sensible? Shouldn\u2019t pi be something else and then no ELBO? Please expand, this is unconvincing.\n\nLine 324 it seems like this doesn\u2019t really match the beta vae setup; it would if you put a beta in front of the kl in line 318. What is this doing?\n\nLine 378 how about some ablations of alpha and beta terms? Edit: there are some in appendix C.\n\nTable 1 does not seem to match appendix C for any value of the parameters, what is missing? Please add details.\n\nLine >= 378 how did you set all of the parameters?\n\nLine 698 affect"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}