{
    "id": "PUXy7vQ5M3",
    "title": "Benchmarking the Fidelity and Utility of Synthetic Relational Data",
    "abstract": "Synthesizing relational data has started to receive more attention from researchers, practitioners, and industry. The task is more difficult than synthesizing a single table due to the added complexity of relationships between tables. For the same reason, benchmarking methods for synthesizing relational data introduces new challenges. Our work is motivated by a lack of an empirical evaluation of state-of-the-art methods and by gaps in the understanding of how such an evaluation should be done. We review related work on relational data synthesis, common benchmarking datasets, and approaches to measuring the fidelity and utility of synthetic data. We combine the best practices and a novel robust detection approach into a benchmarking tool and use it to compare six methods, including two commercial tools. While some methods are better than others, no method is able to synthesize a dataset that is indistinguishable from original data. For utility, we typically observe moderate correlation between real and synthetic data for both model predictive performance and feature importance.",
    "keywords": [
        "data generation",
        "literature review",
        "empirical comparison",
        "discriminative detection",
        "data quality"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "Our review and benchmark of synthetic relational data generation methods demonstrate that current methods cannot produce data that is indistinguishable from the original. We introduce a detection-based evaluation approach for relational fidelity.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PUXy7vQ5M3",
    "pdf_link": "https://openreview.net/pdf?id=PUXy7vQ5M3",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a benchmark for evaluating synthetic data generation techniques specifically designed for relational data. Notably, it introduces a method to assess synthetic data generation based on fidelity metrics and reports benchmark evaluation results on existing relational data generation techniques."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Establishing benchmarks for synthetic relational data generation is an interesting contribution, particularly with a focus on fidelity and utility as critical evaluation metrics."
            },
            "weaknesses": {
                "value": "- W1: While the focus on benchmarking is indeed valuable, the evaluation experiments on the proposed benchmark and existing techniques appear insufficient. Specifically, in Table 1, the comparison between statistical cardinality and the proposed approach is discussed only briefly in Section 4.3, with only around four lines of explanation, leaving the analysis of the proposed method\u2019s advantages underdeveloped. I suggest the authors redesign the whole experiment section to have a new question, such as \"Q1: Is the proposed benchmark more effective than SOTAs?\". \n\n- W2: Although the primary purpose of the paper, as stated in Chapter 1, is **privacy protection**, it does not assess some major approaches in this field, such as relational data generation using differential privacy. I would suggest the authors either 1) to add several existing data generation techniques based on differential privacy, or 2) to explain why you exclude these techniques in spite of claiming privacy protection is the benefit of synthetic data generation. \nSome SOTAs based on differential privacy are as follows:\n  - J. Yang, P. Wu, G. Cong, T. Zhang, and X. He. \u201cSAM: Database generation from query workloads with supervised autoregressive models.\u201d In SIGMOD, 2022.\n  - K. Cai, X. Xiao, and G. Cormode. \u201cPrivlava: Synthesizing relational data with foreign keys under differential privacy.\u201d SIGMOD, 1(2), 2023.\n\n\n\n- W3: The paper places a heavy emphasis on fidelity, with only limited evaluation of utility, creating an imbalance. Since the tile and abstract claim that fidelity and utility have the same importance in benchmarking synthetic data generation, the experiment of benchmarking should also have similar importance over fidelity and utility. For instance, granularity (single-column, single-table, multi-table) is extensively examined with regard to fidelity, taking up considerable space in the experiments, while utility is assessed only for single-table input, with a relatively small space devoted to these experiments. I would suggest adding more experiments for utility evaluation: 1) using regression/classification for estimating different attribute values, and/or 2) joining different numbers of tables and using them as input to train the models. \n\n- W4: The proposed method in Section 3 appears relatively simple, so its novelty is not clear. In detail, Algorithm 1 simply trains the model to discriminate the original data and (derived) synthetic data using the Binominal test, which is just a standard technique. Also, the extension to multi-table in Section 3.2 is quite simple, in that it utilizes simple aggregation from child tables. \n- W5: Issues with readability and consistency:\n  - Several graphs in the experimental section lack descriptions on the X and Y axes.\n  - In Figure 2, the description \u201cMost methods model the parent table (store) better since the tests find more differences for the child table (historical)\u201d applies to (b) but not to (a).\n  - Inconsistencies exist between Section 4.5 (first and third most important features) and Figure 5 (1st and 4th most important features)."
            },
            "questions": {
                "value": "- Q-error is a standard measure for evaluating record count accuracy in query results, but this paper does not adopt Q-error. What is the reason for this?\n- Although the paper asserts the importance of machine learning utility, machine learning models typically only handle single-table structured data as input. Conversely, the paper emphasizes the importance of multi-table data generation, creating a potential inconsistency in its argument. How does the paper propose to input multi-table data into machine learning models? Additionally, how is the input data prepared from multiple tables for Section 4.6?\n- The phrase \u201cgenerating child rows conditionally on parent rows\u201d implies a propagation of information to the parent side. However, the paper associates this with \u201cpropagating errors down the hierarchy,\u201d referring to propagation to the child side, which seems inconsistent."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "As pointed out by the authors, the use of synthetic relational data is\nattractive as it can help to preserve the privacy of the original\ndata, and it can help to alleviate the scarcity of data. Of course, to\nhelp with these problems and be useful, the synthetic data has to\npreserve some properties of the original data. This is an old problem\nthat has been studied in the database area; in particular, in this\ncontext, the type of properties one would like to preserve in the\nsynthetic data are related to the complexity of evaluating certain\nclasses of queries in the real and synthetic scenarios. However, what\nis new in the scenario presented in this paper is that an ML model is\ngoing to be trained on the synthetic data to later make predictions on\nthe real data. Hence, the synthetic data has to preserve the structure\nof the real data that allows the ML model to make accurate\npredictions on the real data.\n\nThe main aspects used to measure the quality of synthetic relational\ndata are fidelity and utility. The former refers to how close the\nsynthetic data is to the real data, while the latter refers to how\nwell an ML model would perform on a prediction task over the real data\nif the real data is replaced by the synthetic data. In this paper, the\nauthors propose a synthetic relational data benchmark, which combines\nknown techniques to measure fidelity and utility with a new general\napproach to measure fidelity for relational data. Moreover, the\nauthors use this approach to compare known methods for synthesizing\nrelational data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1) The authors present a fairly general framework to compare methods\nfor synthesizing relational data.\n\nS2) The paper presents an experimental evaluation where the proposed\nframework is used to compare known methods for synthesizing relational\ndata. This evaluation provides useful information about these methods."
            },
            "weaknesses": {
                "value": "W1) The general approach to measure fidelity with discriminative\ndetection can be understood. However, the notion of relational data\nused in the paper is not properly formalized."
            },
            "questions": {
                "value": "The notion of relational data used in the paper is not properly formalized.\n\n- The authors mention that the datasets used in the paper are\n  organized based on the structure of their relational schema. For\n  example, they mention that AirBnB uses only linear relationships,\n  which means one parent and one child table. But they do not indicate\n  how the parent-child relationship is defined between tables. Is this\n  relationship defined considering the foreign keys in the tables?\n\n- A foreign key for the table T_i is defined p_{T_j}, T_i ~ T_j. I\n  assumed this foreign key is defined with respect to the primary key\n  of the table T_j, since this foreign key is defined for T_i. But\n  which attributes of T_i participate in this dependency? The notation\n  has to include this information.\n\n- A row of a table is defined as a set of values. Since a set is not\n  ordered, how do you know what is the value of an attribute of the\n  table in this row?\n\n- The authors indicate that if {a_1^{T_i}, ..., a_l^{T_i}} are the\n  attributes of a table T_i, then a row of this table is a set of\n  values {v_{p_{T_i}}, v_{k_1}, ..., v_{k_o}, v_{a_1^{T_i}}, ...,\n  v_{a_l^{T_i}}}. I assume that v_{a_j^{T_i}} is the value of\n  attribute a_j^{T_i}. But then I do not understand what the other\n  values are, as they do not correspond with the attributes of the\n  table."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Experimental study that benchmarks various tools for generating synthetic relational data for a given source database. Uses standard approaches for single-table quality assessment (with straightforward but important modifications) and proposes a new approach for multi-table quality assessment. The main takeaway for me is that even single-table assessment fails with current data generation methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1. Independent assessment and comparison of various synthetic data generation tools is a valuable contribution.\n\nS2. The proposed \"discriminative detection\" for single-table assessment is a common, useful baseline approach."
            },
            "weaknesses": {
                "value": "W1. Technical novelty low. For single-table assessment, the proposed \"discriminative detection\" method is standard practice, as it amounts to training and evaluating a discriminator for real vs. synthetic examples. According to the authors, in the field of assessing synthetic relational data, prior work only used logistic regression as a discriminator. Although I find this hard to believe (e.g., some synthesis methods are GAN-based), I agree that that's took weak. For multi-table assessment, the proposed method is neither well-presented nor well-argued for. It computes/compares count and value statistics over joins, but it's not clear (i) why this is a good idea in the first place and (ii) which joins and which statistics should be used. For hierarchical data, it seems to modify the denormalization approach mainly by adding aggregation, but why/how does this actually lead to an improvement? Moreover, the approach seems to be basic and ignores the complex schemata of real relation data. As this is a benchmark paper, technical novelty is not a critical metric for assessment, but I include it here because the paper presents them as contributions.\n\nW2. Insight low. I am not sure what I learned from this paper other than that even single-table assessment methods fail for the tested synthesizers. First, assuming that this wasn't known before already, the main reason must have been that a too weak model (logistic regression) was used as discriminator. That's a valuable insight, but it relevant to members of the subfield mainly, not the machine learning community as a whole (we know this). Second, given that single-table assessment already fails, doing multi-table assessment is not really helpful; it will also fail. In order to gauge the usefulness of the proposed aggregation method empirically, we want it to produce insights in cases where single-table assessment doesn't fail already. Third, the paper reports a large number of empirical results in tables and figures, but no further analysis, deeper discussion, or suggested next steps."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of relational data synthesis by introducing a general approach termed 'discriminative detection' to evaluate the fidelity of various relational data synthesis algorithms. The authors compare several recent solutions, including those utilizing Generative Adversarial Networks (GANs), and conduct an experimental study to assess the performance of these methods across different datasets."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper presents a good vision for establishing a benchmark that effectively evaluates both the fidelity and utility of synthesized relational data. Additionally, the initiative to define a generic metric is commendable, as it enhances the framework for assessing various synthesis methods."
            },
            "weaknesses": {
                "value": "The paper has several key weaknesses:\n\nW1. Limited Identification of State-of-the-Art Methods: The authors only reference a few papers and overlook recent works exploring GANs and diffusion models for tabular data synthesis. For example, the VLDB Journal 2024 article below systematically analyzes the design space of tabular data synthesis using GANs, while studies like 'TableDiffusion' also utilize diffusion models. A comprehensive benchmark requires an up-to-date summary of SOTA methods.\n\n- Tabular data synthesis with generative adversarial networks: design space and optimizations, VLDBJ 2024\n- Diffusion Models for Tabular Data Imputation and Synthetic Data Generation (https://arxiv.org/pdf/2407.02549)\n\nW2. Unclear Claims about Related Work: The statement regarding the lack of accessible APIs or source code for related studies is ambiguous. Notably, source code for significant works, such as the mentioned GAN paper and others that combine diffusion models, is publicly available. This oversight suggests that the authors may not have fully explored existing resources.\n\n- https://github.com/ruc-datalab/Daisy for the above VLDBJ paper\n- Tabsyn (https://github.com/amazon-science/tabsyn) that combines a diffusion model and a VAE, \n- CoDi (https://github.com/ChaejeongLee/CoDi) also uses diffusion models\n- and there are many more papers with code using GANs and diffusion models\n\nW3. Insufficient Dataset Variety: A robust benchmark necessitates a wide array of datasets, yet this paper includes only five. In contrast, other studies, like the one on diffusion models, utilize ten datasets for evaluation, indicating that the current work lacks adequate data diversity.\n\n- Diffusion Models for Tabular Data Imputation and Synthetic Data Generation (https://arxiv.org/pdf/2407.02549)\n\nW4. Neglect of Data Type Considerations: The paper does not address the critical distinction between numerical (or continuous) and categorical (or discrete) data in tabular synthesis. Different data types require tailored generation techniques, which is a significant oversight in the discussion.\n\nW5. Generalization of Fidelity Metrics: The fidelity evaluation should be context-dependent; thus, the authors must justify how a single metric can effectively encompass diverse applications before proposing a general metric.\n\nW6. Lack of a Consistent Conclusion: The paper fails to provide a clear and consistent conclusion that can effectively guide researchers and practitioners in applying the findings."
            },
            "questions": {
                "value": "Q1: How many relational data synthesis methods in each category should be considered sufficient to establish a robust benchmark? (Refer to Weaknesses W1 and W2)\n\nQ2: What is the minimum number of datasets required to create an effective benchmark? (Refer to Weaknesses W3 and W4)\n\nQ3: How can real-world applications be used to motivate or justify the choice of a generic metric, and what makes this metric suitable for various real-world applications? (Refer to Weakness W5)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}