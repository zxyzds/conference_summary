{
    "id": "0SpkBUPjL3",
    "title": "Unremovable Watermarks for Open-Source Language Models",
    "abstract": "The recent explosion of high-quality language models has necessitated new methods for identifying AI-generated text. Watermarking is a leading solution and could prove to be an essential tool in the age of generative AI. Existing approaches embed watermarks at inference and crucially rely on the large language model (LLM) specification and parameters being secret, which makes them inapplicable to the open-source setting. In this work, we introduce the first watermarking scheme for open-source LLMs. Our scheme works by modifying the parameters of the model, but the watermark can be detected from just the outputs of the model. Perhaps surprisingly, we prove that our watermarks are $\\textit{unremovable}$ under certain assumptions about the adversary's knowledge. To demonstrate the behavior of our construction under concrete parameter instantiations, we present experimental results with OPT-6.7B and OPT-1.3B. We demonstrate robustness to both token substitution and perturbation of the model parameters. We find that the stronger of these attacks, the model-perturbation attack, requires deteriorating the quality score to 0 out of 100 in order to bring the detection rate down to 50%.",
    "keywords": [
        "watermark",
        "large language model"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We construct a watermark for open-source language models that is provably unremovable, even by an adversary with access to the model's weights.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=0SpkBUPjL3",
    "pdf_link": "https://openreview.net/pdf?id=0SpkBUPjL3",
    "comments": [
        {
            "summary": {
                "value": "The paper focuses on the problem of LLM watermarking and proposes a scheme applicable to open-source models. The key claimed advantage of the scheme is its provable unremovability which the authors rigorously derive. Experimental results on two OPT models are shown, including the evaluation of robustness to token substitution and Gaussian perturbations to model weights."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- As the authors recognize, watermarking of open-source LLMs is one of the most important open problems in current generative model watermarking research, so studying it has the potential for high impact."
            },
            "weaknesses": {
                "value": "Unfortunately I believe the paper in its current state is far from being able to deliver that impact. Namely: \n- While I agree that some definition must exist, formalizing \"LLM that produces high-quality text\" as \"closeness to the original LLM in weights of the last bias layer\" seems arbitrary and far from realistic notions of quality. This greatly simplifies the problem, unfortunately making the theoretical results (claimed key contribution) largely not relevant. While I appreciate the rigor and work authors have put in proving the results, formalizing the intuition that a random vector in N-dimensional space is unlikely to match a particular unknown direction, I unfortunately do not think this provides any valuable insight in terms of the robustness of an OSS watermark to realistic transformations.\n- Given this, the blanket claims that the watermark is \"unremovable\" (title, abstract, introduction) render as dangerous overclaims that may cause confusion in the field if the paper is accepted. These should be greatly adjusted and qualified to explain the peculiar definition of quality. To actually get a meaningful notion of unremovability, the authors could consider realistic transformations commonly applied to OSS models such as finetuning, PEFT, quantization, pruning, or at least random modification of all weights (the argument on L129/130 is unclear). These are currently neither discussed in theory nor included in the evaluation. Interestingly, the authors recognize that prior work Gu et al. (2024) considers fine-tuning yet do not consider this themselves. \n- As the authors also recognize, the proposed scheme is a variant of UnigramWatermark. While scheme simplicity is not a weakness per se, interesting/novel technical aspects of the proposed scheme are also not a strength of this paper. This is further harmed by the fact that popular LLMs often do not use the final layer bias, making the proposed scheme inapplicable. In fact, this is true for OPT models used in this work (https://github.com/huggingface/transformers/blob/v4.46.0/src/transformers/models/opt/modeling_opt.py#L1052), bringing into question the current evaluation. \n- LLM watermarking, which this paper positions itself as part of, generally focuses on detecting LLM-generated outputs. Yet, this paper starts from the related but different notion of detecting that a model was based on a watermarked model from its weights, and prove key results in this case. This is a new scenario which is unexplained and unmotivated, should be explicitly separated from the common understanding of LLM watermarking promised in early parts of the paper, and raises many questions. For example, if we assume transformations of our OSS model change nothing but the final bias layer, can't we use the other (unchanged) weights to demonstrate that the resulting model was made from our model?\n- Evaluation has many drawbacks, among else it does not include any baseline (such as Gu et al. (2024)), uses high FPRs, and uses no realistic attacks on text such as paraphrasing, generally used in prior work. As the authors note, the performance of the watermark is below non-OSS baselines, which is to be expected, but does not present a case for this method as useful beyond the OSS case.\n- The paper is written and presented in a confusing and convoluted way, seems to be written in a rush, and it is often very hard to understand the key parts. I include some examples/suggestions below, in hopes that this helps the authors get insight into the issues and improve their writing in the future to the level expected at ICLR. I am happy to further assist the authors here if they have questions. \n- (Minor) While this does not affect my assessment, L173 contains another dangerous claim, citing Zhang et al. (2024) to say that any LLM watermark is removable. This is a misunderstanding of the original paper which studies an idealized case where a random walk on the space of \"equivalent\" documents is possible while preserving quality, and the random walk is rapidly mixing. To avoid misinforming readers, this citation should be appropriately qualified.\n\nOverall, while I do appreciate the authors tackling such a hard and important problem, I do not see the contribution of the paper at this point, and believe it thus to be clearly below the bar for acceptance.\n\n---\n\nThe list of writing/formatting/presentation comments for the authors follows, which I hope they find helpful. I do not expect authors to reply to each point, although I should be corrected if I misinterpreted some points.\n- L53: the phrase \"unremovability from a modest amount of text\" is confusing and should be made more precise\n- L54-60 seems to repeat the same point about the adversary twice, requiring several readings \n- I appreciate the inclusion of the Overview section; however, instead of previewing and summarizing the technical details, this section is often the only place where concepts are explained in terms of the concrete instantiation of interest (LLMs). E.g., 4.1. does not reflect on what unremovability means in the setting we consider, but only provides abstract definitions. This makes the paper hard to read and understand the actual instantiation. \n- Another choice that contributes to this is the use of \"content\" to counterinuitively often mean \"last layer bias vector\" instead of \"text\". Similarly in Alg. 3 it is not made clear if \"original content\" refers to the watermarked or pre-watermarked model weights; effort by the reader is needed to understand this. \n- Sec. 2 uses (M*, M') for (original, watermarked) model, inconsistent with ($w^\\star, w_{wat}$) below, causing some confusion. \n- L87: \"checking the correlation\" is quite imprecise\n- L104: why the region is a halfspace is not explained; while it is a simple property of dot product, this should be made explicit to help readers grep this part \n- L107: \"add to $w^*$\" is unclear. I suspect this should say \"the adversary can add a vector to $w_{wat}$\" instead; this should be made precise.\n- L230: Q should probably be L? Such mistakes should be especially avoided in key definitions.\n- L231: \"p.p.t.\" should be defined, I believe it is not a standard term in this community \n- L318: logits $l_i$ seem to refer to values before adding any bias? This is very ambiguous and should be made clear in the writing.\n- \"Quality score\" shows up first in Fig. 3 but is not previously introduced which is quite confusing.\n- The paper has no figures before the evaluation which is highly unusual, especially as there ary many instances where a visualization would greatly aid understanding (e.g., halfspaces / gaussians in the model parameter space). I suggest the authors generally consider this when writing.\n- The margins on top of each page are very small which suggests the style file was tweaked. Note that the ICLR formatting instructions state \"Tweaking the style files may be grounds for rejection.\". While the paper doesn't seem to have been desk rejected in this instance, I strongly suggest the authors remedy this and follow the instructions."
            },
            "questions": {
                "value": "- In Theorem 1 shouldn't the distribution over $w^\\star$ be centered at $w_{wat}$ and not 0? This is also present in the proof and Theorem 3. Is this a mistake or my misunderstanding of the statements?\n- L145 states that Aaronson (2022), Christ (2024) and Fairoze (2023) are based on partitioning the tokens into red and green lists. Can you elaborate on this view of these methods, as my understanding was that they are quite different and do not use the red/green concept? \n- Were OPT models modified to use the final layer bias to enable experimental evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method for embedding watermarks in large language models (LLMs). This method incorporates watermark information by adding noise that follows a normal distribution to the model's output, with the noise serving as the watermark's key. The authors also demonstrate that, under certain assumptions, the embedded watermark is unremovable. The feasibility of the proposed scheme is validated using the OPT-6.7B and OPT-1.3B models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors attempt to embed noise information following a normal distribution as a watermark into the text output of the model. This is an interesting endeavor that could potentially aid future watermark embedding algorithms.\n\n2.The paper attempts to theoretically discuss the unremovability of watermarks, which is also an interesting analysis."
            },
            "weaknesses": {
                "value": "1. The paper's description of the algorithm is not clear enough and does not reflect the specific implementation of the watermark embedding algorithm. There is watermark detection for text in Algorithm 4, but there is no embedding algorithm for text.\n\n2. The paper discusses the unremovability of watermarks, which is generally referred to as robustness in other papers. The paper does not compare the robustness of its approach with those of other papers. It should also discuss the soundness property of the watermark, which typically contradicts robustness.\n\n3. The writing in the paper is not clear enough, which makes it difficult to understand the algorithms it contains. Specific issues will be provided below."
            },
            "questions": {
                "value": "1. What is the relationship between Algorithm 4 and Algorithm 3? If Algorithm 4 is the primary method for text watermark detection, then when is Algorithm 3 invoked?\n\n2. How is the symbol \\Delta (x_i) defined in Algorithm 4? How is it calculated? \n\n3. In watermark detection, each token should be evaluated. Why is it necessary to check x_i \\in S in line 4 of Algorithm 4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the first watermarking scheme for open-source LLMs. The idea is to embed a watermark directly in the model's parameters rather than through the sampling algorithm, making it resilient to tampering in open-source environments. The authors define \"unremovable watermarks\" for neural networks and implement a scheme that perturbs neuron biases in the model's final layer with Gaussian noise. Detection of the watermark is achieved either by examining the weights for specific bias patterns or by analyzing output text for token frequency markers. The watermark is shown to be unremovable, as attempts to erase it degrade model quality, with experimental results on OPT-6.7B and OPT-1.3B supporting this claim."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The paper introduces a new watermarking scheme to embed unremovable watermarks directly in model weights and resist tampering in open-source environments.\n- The paper defines \"unremovable watermarks,\" providing proofs and an analysis of the watermark\u2019s robustness against attacks and conducting experiments with OPT-6.7B and OPT-1.3B models to demonstrate the approach's effectiveness.\n- The paper is well-structured, logically presenting its motivation, methodology, and findings, with clear definitions and algorithms. I highly commend the authors for the nice presentation."
            },
            "weaknesses": {
                "value": "- Now, the authors claim to have introduced the first watermarking scheme for open-source LLMs. What do they mean by this? There are many watermarking schemes which could be deployed in open source LLMs, so this claim might not be right, as the proposed scheme can also be deployed in closed source LLMs by the model owners. Which leads to the next question. If the LLM is open source, what exactly is the benefit of watermarking when the attacker has direct access to model's weights. Can the authors expand more on their motivation?\n- The proposed approach embeds watermark signals to the bias of the last layer's neurons. There is another approach by ByteDance that injects watermark into the LLM weights by finetuning (https://arxiv.org/pdf/2403.10553). Why is there no comparison with this approach? Infact, why is there no comparison with other watermarking schemes at all?\n- There are adaptive ways to bypass watermarks. One is by using adaptive paraphrasers. If the proposed watermark scheme is unremovable, yet detectable, why are there no empirical results proving the 'unremovability' claim using adaptive paraphrasers, or even normal paraphrasers like Dipper, or even using open source LLMs for paraphrasing.\n- How efficient is the detection process? How many tokens does it require to detect the proposed scheme, especially using its optimal hyperparameters? I feel the experiments the authors provided to prove the efficiency and strength of this approach are not enough."
            },
            "questions": {
                "value": "Please answer those in weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The problem of watermarking the output of LLMs has been around for some time. Previous work has focused on changing the output distribution of the LLMs, sometimes in an \u201cundetectable/distortion-free\u201d way. But this work starts by making the following point:\nIf one has access to the parameters of an LLM, they can run it to generate output that is not watermarked. \n\nThe main problem of this paper is to watermark the parameters of the model itself, in a way that it both gets reflected in the output + even if one gets their hand on the model parameters, they cannot modify it in a way that the watermark is removed from subsequent outputs.\n\nOf course one shall consider attackers who can train the model from scratch. So the paper assumes that doing so is hard (e.g., by making the access to the data generation process costly).\n\nThe contribution of the paper is the following. They propose a scheme that works by adding Gaussian noise to the last layer of the model before publishing it. Also knowing the original model and the noise, they show how to verify whether the generated output comes from their model or not.\n\nThe paper then makes multiple assumptions to prove that their scheme is secure. The final watermark suffers from not having robustness or undetectability. It is not clear if such weaknesses are inherent or not."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "As far as I know, this is the first work that aims to formally define and address \u201cunremovable watermarks\u201d that are planted in open source models."
            },
            "weaknesses": {
                "value": "The paper does not fully address several, by now well-recognized aspects, of the watermark:\n1. Robustness of the watermarks. E.g., what if one changes even two characters of the produced output? Or that it deletes parts of the output. Here the paper claims it has done experiments but i could not figure out what exact perturbation channels are studied.\n2. It seems that the output of the watermarked model here is *not* indistinguishable -- sometimes called undetectable or distortion free -- (in comparison with non-watermarked model's output). This is the ultimate way of arguing that the model\u2019s utility does not degrade after adding the watermark and the paper does not discuss it clearly. Note that here, I am not talking about \"removability\". This is about the item above (robustness) but rather if the output of the watermarked model differs (in a computationally noticeable way) from the output of non-watermarked model.\n\nTo partially address the above issues, the paper should first define clearly what class of perturbation channels they study (and why they are interesting) for the robustness property evolutions (which are seemingly done under the name of Detectability) and for the item 2 above (undetectability of the output -- which is different from the Detectability study) they should design experiments specifically for this goal or make a theoretical assertion.\n\nAlso, the proofs are based on multiple assumptions, which make the final conclusion far from ideal. (See my question below)\n\nAlso, what happens to the watermarks if the model is fine tuned? (note that black-box methods still work, if the model is fine tuned). This issue should be addressed using experiments. they could be simple experiments that simply test the detectability and utility of the outputs after a fine tuning for specific goals (also see my question below).\n\nThe writing also is not great and lacks discussions and justifications with regard to the issue mentioned above (e.g., of the assumptions). \nOther than the issues above, the intuition behind why this non-black-box approach is working could be much better.\n\nOther minor comments on writing:\n\nDef 2 seems to be more like a \u201csimilarity\u201d measure, because the loss in quality seems to be different. For example, two models could look very different but have the same quality of responses.\n\nDef 4: seems to mix the input space of Q and \\ell, right?"
            },
            "questions": {
                "value": "Can you address the two issues of robustness (to small change in the output) and the undetectability of the output (compared to the non-watermarked model) ?\n\n In your experiments, how do you measure that the utility of the model has not degraded after adding the watermark. I know that you have an oracle that measures the degrading, but then you instantiate the oracle using mathematical formulas regarding the model. But how do you make sure that this reflects the actual quality of the model\u2019s output? For example, you could use specific metrics or human evaluation methods to assess output quality more directly.\n\nCan you discuss why the assumptions are fine? There are 3 explicit assumptions and (multiple) implicit assumptions in the statement of Theorem 1 (eg., \u201clet C be such that\u2026\u201d or \u201cc_2-high quality\u2026\u201d) I think that discussion is needed before calling assumptions reasonable (instead of putting the word reasonable in the theorem statement).\n\nCan you argue either way about the effect of fine tuning in your watermarked model?\n\nIn your experiments: can you be more explicit about what your attacker is? e.g., using a pseudocode."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}