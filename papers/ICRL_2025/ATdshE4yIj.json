{
    "id": "ATdshE4yIj",
    "title": "TransAdapter: Vision Transformer for Feature-Centric Unsupervised Domain Adaptation",
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to leverage labeled data from a source domain to address tasks in a related but unlabeled target domain. This problem is particularly challenging when there is a significant gap between the source and target domains. Traditional methods have largely focused on minimizing this domain gap by learning domain-invariant feature representations using convolutional neural networks (CNNs). However, recent advances in vision transformers, such as the Swin Transformer, have demonstrated superior performance in various vision tasks. In this work, we propose a novel UDA approach based on the Swin Transformer, introducing three key modules to improve domain adaptation. First, we develop a Graph Domain Discriminator that plays a crucial role in domain alignment by capturing pixel-wise correlations through a graph convolutional layer, operating on both shallow and deep features in the transformer. This module also calculates the entropy for the key attention features of the attention block to better distinguish between the source and target domains. Second, we present an Adaptive Double Attention module that simultaneously processes Windows and Shifted Windows attention to increase long-range dependency features. An attention reweighting mechanism is employed to dynamically adjust the contributions of the attention values, thereby improving feature alignment between domains. Finally, we introduce Cross-Feature Transform, where random Swin Transformer blocks are selectively transformed using our proposed transform module, enhancing the model\u2019s ability to generalize across domains by transferring the source to the target features. Extensive experiments demonstrate that our method improves the state-of-the-art on several challenging UDA benchmarks, confirming the effectiveness of our approach. In particular, our model does not include a task-specific domain alignment module, making it more versatile for various applications.",
    "keywords": [
        "Unsupervised Domain Adaptation",
        "Transformer",
        "Domain-Invariant Feature Representations"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ATdshE4yIj",
    "pdf_link": "https://openreview.net/pdf?id=ATdshE4yIj",
    "comments": [
        {
            "summary": {
                "value": "The paper addresses Unsupervised Domain Adaptation (UDA) in the context of Vision Transformers. Specifically, while several prior works on UDA have leveraged CNNs for adaptation, this work attempts to utilize Vision Transformers (ViTs), especially the Swin transformer architecture for UDA. The authors propose TransAdapter - a transformer-based approach to UDA that draws heavily from the Swin transformer. TransAdapter comprises three components - (i) a domain adversarial Graph Domain Discriminator (GDD) (ii) Adaptive Double Attention that simultaneously processes window and shifted window attentions (iii) Cross-Feature Transform (CFT) to transfer the source to target features. Experiments on the standard UDA benchmarks demonstrate the improvements that TransAdapter achieves over prior works."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper proposes a novel improvement to the Swin transformer specifically for the UDA task. Experiments on the standard Domain Adaptation benchmarks demonstrates the effectiveness of the proposed method compared to prior works."
            },
            "weaknesses": {
                "value": "### (a) Presentation and discussions\n- **Unclear motivation -** The fundamental motivation behind proposing improvements to the Swin transformer architecture is not concrete. The authors present the argument of using transformers rather than CNNs for UDA based on the improvements that transformers achieve on several vision tasks. However, they do not discuss the limitations of the Swin transformer and how their method improves on the same.\n\n- **Justification of design choices -** While the authors have chosen the Swin transformer architecture as the base and provided an intuitive justification of the same, there is no discussion on the design choices behind various aspects of the method. For example, why does the Graph Domain Discriminator (GDD) use graph convolutions rather than regular convolutions? Why use a domain discriminator to align the source and target representations? Why specifically use the third transformer block\u2019s attention output for the GDD? The authors should discuss all these aspects in detail for a good understanding of TransAdapter.\n\n- **Discussions on each component of TransAdapter -** Even though the authors have provided high-level intuitions behind each component of TransAdapter, this does not convey a clear understanding of the same. \n    - For example, in L254-256, the authors mention that integrating the window and shifted window attention mechanism along with reweighting enables the model to capture \u201cmultiple aspects of the feature representations\u201d. What aspects does the model capture? How do they help with the UDA task? How are they an improvement over the representations captured by the original Swin transformer architecture?\n    - In L223-225, the authors claim that processing the window and shifted window attentions in parallel enables the model to capture long-range dependencies and enhances spatial relationship understanding. There is, however, no discussion on how the Adaptive Double Attention Module achieves any of these aspects. How exactly does the modified attention module capture global dependencies better than a vanilla ViT and a Swin transformer? Moreover, what is long-range dependence in the context of a vision transformer?\n    - Overall, the authors should provide detailed discussions on each aspect of the proposed method, and how they improve over the corresponding components in the Swin transformer. The lack of these discussions makes it quite challenging to understand the motivation behind the work.\n\n- **Use of terminologies -** The authors make use of several terminologies without providing a concrete definition or intuition behind the same. For example, the authors specify \u201cglobal and local adaptation\u201d without explaining these terms. Do they refer to the alignment of global and local features across the source and target domains? What do global and local features refer to in the context of UDA and the Swin transformer? The authors should clearly specify what these terms mean and how they relate to the context of UDA.\n\n### (b) Experiments and results\n- **Missing comparisons-** While the authors have compared with several prior works that utilize the ViT backbone, the comparisons are not extensive and complete. Ideally, the authors should extend the ResNet-based methods to Swin or ViT for a fair comparison. The goal is to ensure a fair comparison by comparing the proposed method with prior works using the same / similar architecture. The list of comparisons missing is given below:\n    - SHOT and [R2, R3, R5] with ViT\n    - SHOT and [R2, R3, R5] with Swin transformer\n    - [R1] with DeiT\n    - SSRT [R4] with ViT, DeiT and Swin transformer\n- **Missing results-** The authors have not presented results on DomainNet, which is the most important and challenging UDA benchmark. They should present results on DomainNet, either in the single source or multi-source / multi-target setting (in case of time and compute constraints) to demonstrate the effectiveness of the approach. Additionally, how does the proposed method perform in a multi-source or multi-target setting?\n- **t-SNE visualidations-** There is no discernible difference among the four plots in Fig. 5. Moreover, there are no details provided on the setup for the visualisation experiment. Which layers\u2019 features were considered for visualisation? What is the expected pattern in the plots? What is observed in the plots and what does that indicate about the proposed method? The authors should answer all these questions for a clear analysis of the proposed method with respect to the visualisation.\n\n### (c) Missing references\n- R1 - Sanyal, Sunandini, et al. \"Domain-specificity inducing transformers for source-free domain adaptation.\"ICCV 2023.\n- R2 - Hoyer, Lukas, et al. \"MIC: Masked image consistency for context-enhanced domain adaptation.\" CVPR 2023\n- R3 - Yang, Shiqi, Shangling Jui, and Joost van de Weijer. \"Attracting and dispersing: A simple approach for source-free domain adaptation.\" NeurIPS 2022.\n- R4 - Sun, Tao, et al. \"Safe self-refinement for transformer-based domain adaptation.\" CVPR 2022.\n- R5 - Zhang, Yixin, Zilei Wang, and Weinan He. \"Class relationship embedded learning for source-free unsupervised domain adaptation.\" CVPR 2023."
            },
            "questions": {
                "value": "In addition to all the questions mentioned in the weaknesses section, the reviewer has a few follow-up questions.\n- What is the motivation behind TransAdapter? Is it intended to be an improvement over the Swin transformer specifically tailored to UDA? If so, what limitations of Swin does TransAdapter overcome?\n- What is the purpose of using graph convolutions instead of regular convolutions in the domain discriminator? What is the motivation behind the same?\n- What is the exact benefit of simultaneously processing the window and shifted window attentions rather than sequentially in the original Swin transformer? Can the authors demonstrate this through a simple motivating experiment or visualisation?\n- How does TransAdapter compare to the original Swin transformer in terms of parameter count, training and inference time?\n- Can the proposed method be extended to the multi-source and multi-target settings? How would the proposed method perform in these settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to solve the task of unsupervised domain adaptation (UDA) and proposes a method called TransAdapter. This framework consists of three main modules. Graph Domain Discriminator captures pixel-wise correlations through a graph convolutional layer. Adaptive Double Attention processes windows and shifted windows attention to increase long-range dependency features. Cross-Feature Transform transfers source features to target features. The authors conduct experiments on three datasets commonly used for UDA tasks: Office-31, Office-Home, and VISDA-2017. The ablation study in Table 4 provides preliminary verification of the effectiveness of three modules."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper focuses on UDA and proposes a novel framework, TransAdapter, applied to the Swin Transformer. The paper dedicates a significant portion to detailing the specific structure of the three modules. Experimental results demonstrate that TransAdapter can improve the performance of pre-trained Swin Transformer (Source Only) on UDA tasks."
            },
            "weaknesses": {
                "value": "1. Overall, the three modules in TransAdapter are specifically designed for Swin Transformer, and they appear difficult to generalize to other models, even those based on Transformer like ViT. Additionally, the paper does not provide any experiments or discussions that can be referenced.\n\n2. The motivation for using Swin Transformer is stated in lines 58-62 of the main text. The relevance of long-range dependencies and non-overlapping patches processing to demonstrate that Swin Transformer is suitable for UDA tasks does not seem strong. The authors need to provide more insights into this motivation.\n\n3. The paper states that the \"Graph Domain Discriminator (GDD) leverages key features from the third transformer block\u2019s attention mechanisms to capture fine-grained details,\" as mentioned in line 161. However, there is no discussion in the paper on why the third block is the best. The authors need to provide additional ablation studies to explain this choice.\n\n4. The authors mention the use of Gradient Reversal Layer (GRL) in lines 194-199. However, they do not explain how GRL is applied in GDD, and there is no explanation elsewhere in the paper, not even in the figures.\n\n5. The references to $f$ and $g$ in Equation 6 are unclear. Section 4.4 (Objective function) is inappropriately placed in the experiment section.\n\n6. In Section 4.2, the authors state that CutMix and MixUp are used as data augmentation strategies. However, there is no ablation study on their effects, making it difficult to discern whether the performance improvement of TransAdapter comes from data augmentation.\n\n7. The paper only uses Swin-B for experiments. At least Swin-T, Swin-S, and Swin-L should be used to demonstrate the reusability of TransAdapter across different Swin Transformer models.\n\n8. The paper lacks ablation studies on hyperparameters, such as $\\lambda_{local}$ and $\\lambda_{global}$."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents a new approach to UDA by using the Swin Transformer. It introduces three key modules: a Graph Domain Discriminator for pixel-wise correlation and domain alignment; an Adaptive Double Attention module to enhance long-range dependencies; and a Cross-Feature Transform for selective feature transformation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed method leverages the Swin Transformer architecture and introduces some modules that enhance feature alignment and domain adaptation, leading to performance enhancement on UDA tasks."
            },
            "weaknesses": {
                "value": "I have some questions about this paper that need further discussion. Please see them below.\n\nIf the authors can address my concerns, I am willing to raise my score."
            },
            "questions": {
                "value": "1. The related work section is outdated, lacking any references from 2022 onward. I believe the authors need to adequately address and enhance this section.\n\n2. Why using the attention from the third transformer block? (L161)\n\n3. The proposed GDD seems to capture relationships between samples from the source and target domains. Why is GCN employed for this purpose? I would appreciate a detailed explanation and a comparison with other common similarity computation methods.\n\n4. In the experiments, the authors introduced data augmentation. Was a baseline method used? If not, this comparison may not be fair.\n\n5. The baseline methods are outdated, such as CDTrans[1], SDAT[2], and MIC[3].  \n\n[1] CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation\n\n[2] A Closer Look at Smoothness in Domain Adversarial Training\n\n[3] MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces TransAdapter, a Unsupervised Domain Adaptation (UDA) framework based on the Swin Transformer. It includes three key modules: a Graph Domain Discriminator for enhanced domain alignment via pixel-wise correlations, an Adaptive Double Attention Module for capturing long-range dependencies, and a Cross-Feature Transform Module for improving generalization by dynamically transforming transformer blocks. These components collectively enhance feature consistency across domains, achieving state-of-the-art performance on challenging UDA benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe authors proposed to utilize Swin Transformer for UDA tasks.\n2.\tThe proposed TransAdapter is effective on three UDA benchmarks."
            },
            "weaknesses": {
                "value": "1.\tThe paper does not provide sufficient motivation for the proposed components:\n*\tWhat is the motivation behind introducing the GDD? How does it compare to the conventional domain discriminator, such as the one used in DANN [a]? Please elaborate on the advantages and insights of using GDD in this context. \n*\tWhy using double attention module? What\u2019s the motivation and insight? Can the authors explain how this module addresses specific challenges in domain adaptation? Additionally, providing further empirical or theoretical evidence to demonstrate the advantages of double attention over single attention in domain adaptation tasks would enhance the quality of the paper.\n2.\tMost of the comparison methods in Tables 1, 2, and 3 are outdated. It is recommended that the authors include more recent approaches proposed in 2023 and 2024.\n3.\tThe experiments on large-scale datasets, such as DomainNet, are missing.\n4.\tThe paper's structure can be improved; the content corresponding to Table 4 should be in Section 4.6 instead of Section 5.\n5.\tThe formatting of Figures 1, 2, 3, and 4 can be improved. The font sizes in these figures are inconsistent, with some being too large and others too small. Additionally, some elements in these figures require further clarification; for instance, in Figure 3, the origins of \u2018V\u2019 and \u2018V_shift\u2019 need to be explained.\n\n[a] Ganin et al. Unsupervised domain adaptation by backpropagation. In ICML, 2015."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}