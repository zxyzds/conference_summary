{
    "id": "QzR7Jfe8Tz",
    "title": "ALPBench: A Benchmark for Active Learning Pipelines on Tabular Data",
    "abstract": "In settings where only a budgeted amount of labeled data can be afforded, active learning seeks to devise query strategies for selecting the most informative data points to be labeled, aiming to enhance learning algorithms' efficiency and performance. Numerous such query strategies have been proposed and compared in the active learning literature. However, the community still lacks standardized benchmarks for comparing the performance of different query strategies. This particularly holds for the combination of query strategies with different learning algorithms into active learning pipelines and examining the impact of the learning algorithm choice. To close this gap, we propose ALPBench, which facilitates the specification, execution, and performance monitoring of active learning pipelines. It has built-in measures to ensure evaluations are done reproducibly, saving exact dataset splits and hyperparameter settings of used algorithms. In total, ALPBench consists of 86 real-world tabular classification datasets and 5 active learning settings, yielding 430 active learning problems. To demonstrate its usefulness and broad compatibility with various learning algorithms and query strategies, we conduct an exemplary study evaluating 9 query strategies paired with 8 learning algorithms in 2 different settings.",
    "keywords": [
        "Active Learning",
        "Benchmark",
        "Tabular Data",
        "Software Library"
    ],
    "primary_area": "infrastructure, software libraries, hardware, systems, etc.",
    "TLDR": "We investigate the performance of various Active Learning Pipelines on different datasets and Active Learning settings.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=QzR7Jfe8Tz",
    "pdf_link": "https://openreview.net/pdf?id=QzR7Jfe8Tz",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces and illustrates ALPBench, which is a benchmark for reproducible active learning pipelines that covers 86 modern, tabular classification datasets, together with a wide variety of base leaners, query strategies, and setups."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper fills in a clear gap in today's landscape of active learning for tabular data. The work is reasonably original (for an evaluation framework), clearly presented, and with the potential of having a high-impact in standardizing empirical validations (while also making them apples-2-apples). The wide availability of ALPBench would greatly impact future AL evaluations: far too many of the newly submitted AL papers stop after an arbitrary nmb of queries, w/o any indication on whether or not the achieved performance is in any way meaningful."
            },
            "weaknesses": {
                "value": "While the paper goes a long way towards standardizing the evaluation of active learners, it can be improved along tow main directions:\n1. First of all, instead than the \"[somewhat] dry analysis\" of the aggregated results in Figures 2 & 3 (which are excellent, but could go into an APPENDIX as supporting evidence), the paper would greatly benefit from a illustrative, step-by-step example of how to use ALPBench in a real-world scenario. Assume that you have a novel tabular dataset NTD for which active learning is essential. Ideally, you would like to follow  a procedure such as (i) identify which base learner BL performs best on NTD - after all, you what to reach SOTA performance with minimum data-annotation cost, (ii) identify which of the existing querying-strategies Q works best with BL for NTD, (iii) identify other datasets have properties similar to NTD, such as having Q+BL as a winner, and (iv) if the results are not satisfactory, invent a novel QS that will deliver better performance that Q+BL, add it to ALPFBench, and re-evaluate. In this scenario, the paper should provide guidance on how to add a new dataset or QS to ALPBench (assuming that it is possible), how to choose between ACC vs AUC metrics, how to choose the best base learner, etc. Similarly, the current insights on what works best for binary vs multi-class, small vs large setup, and the various base learners should be consolidated in a Lessons Leaned section (rather than spread throughout the paper)  \n2. along the same lines: (i) the paper should provide a table with the SOTA performance on each dataset; that is, which base learner is the best when trained/tuned on all available training/dev data, and (ii) the paper should provide an additional set of metrics that measure how many queries does the best AL approach need to reach 50%, 75%, 90%, 95%, and 99% of the SOTA performance in \"(i)\". This guarantees that we are doing AL for the right reasons (ie, reach SOTA-adjacent performance with as few labeled examples as possible), rather than measuring \"wins\" after an arbitrary number of queries"
            },
            "questions": {
                "value": "1. In each row from Table 2, please clarify \n    (i) how many of the \"capabilities\" in the previous four approaches are covered by ALPBench  \n    (ii) which are missing and why (eg, for QS-Hybr, two of the previous approaches cover 4, while you only cover 3; what is the overlap among them?)\n\n2. line 371: how big are the datasets that you have excluded? given that you have included them in ALPBench, you should -at the very least- give the reader a sense of the challenges/costs/time-constraints/ideal-setup of running experiments at that scale.\n\n3. In how many of the experiments is the performance of the base learner impacted by the 180 secs limit? If this is a real issue, you should have mentioned it at the very beginning of the Experimental Results. \n\n4. is it possible to add to the current QSs two classic strategies such as Query-by-Bagging and Query-by-Boosting? See Abe & Mamitsuka, ICML-1998, \"Query Learning Strategies Using Boosting and Bagging.\"\n\n5. In Fig 4, is KNN the best base learner on those 3 datasets? if not, should we care about the erratic perfornace?\n\n6. Is ALPBench \"automatically detecting & reporting\" situations in which, after a number of queries, AL hurts rather than helps (as in \"5.\" above\") \n\n7. how difficult would it be to extend ALPBench to cover additional scenarios, such as (i) simulating the stream-based scenario, (ii) interleaving AL and semi-supervised learning, and (iii) multiple-view learners?\n\nOTHER:\nline 206: please explain intuitively what is epistemic uncertainty; in its current form, it is a bit of a \"circular definition:\" \n              \"epistemic uncertainty sampling (EU) (Nguyen et al., 2019) samples instances that have the highest epistemic uncertainty.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a benchmark for active learning methods\nfor tabular data. Esp. propose the authors to study active\nlearning not just for a single down-stream model, but\nfor a wider selection such as xgboost, catboost, SVMs,\nMLPs etc. They suggest to evaluate three different\nactive learning regimes described by the size of the initial\nlabeled set, the batch size and the overall budget.\nIn experiments they compare different active learning\nmethods with different down-stream models on different\nevaluation metrics in different such regimes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- s1. interesting aspect: looking systematically at different down-stream\n  models.\n- s2. promised a pip installable python module that should be easy to use.\n- s3. well written."
            },
            "weaknesses": {
                "value": "- w1. there is not much innovation in this benchmark besides\n  just scaling to more downstream models and more datasets.  \n- w2. how the three active learning regimes (tab. 1) have been\n  chosen is not discussed.\n- w3. it is not clearly demonstrated how this new benchmark\n  now makes it easier to answer the three research questions\n  asked.\n- w4. the maybe main question one would want to answer\n  by looking at different down-stream models, namely\n  should we use different active learning methods for different\n  down-stream models, is not addressed.\n\nreview.\n\nLooking at different down-stream models that should\nbe actively learned is clearly useful and maybe often\noverlooked in the literature. \n\nHowever, I see several major issues with the paper\ncurrently:\nw1. there is not much innovation in this benchmark besides\n  just scaling to more downstream models and more datasets.  \n- it is not clear which problems the authors had to solve\n  to arrive at the current benchmark.\n- what are the limitations of current benchmarks, besides\n  looking at fewer down-stream models and datasets?\n\nw2. how the three active learning regimes (tab. 1) have been\n  chosen is not discussed.\n- Esp. it is not clear how these different regimes manage to\n  capture similar situations in different datasets. Are not\n  30 initial samples for a very simple dataset much, but\n  for a more difficult dataset very little?\n  \nw3. it is not clearly demonstrated how this new benchmark\n  now makes it easier to answer the three research questions\n  asked.\n- could we not just have used any of the existing benchmarks\n  and run it with your 8 downstream models? if not, can you\n  describe why not, and how this is now different in your\n  benchmark?\n\nw4. the maybe main question one would want to answer\n  by looking at different down-stream models, namely\n  should we use different active learning methods for different\n  down-stream models, is not addressed.\n- fig. 2 provides the best active learning pipeline. Not very surprising,\n  gradient boosted decision tree models as down-stream models\n  are found to perform best on average.\n- from an active learning perspective, would it not be more interesting\n  to ask the question for the conditional performance of the different\n  active learning methods: given a method, say catboost, what are\n  the best active learning methods? and esp. are they different\n  if I choose different down-stream models, say an SVM instead?\n\nFurter remarks:\n- i1. usually the value of a benchmark is seen in identifying an interesting\n  set of experiments that allow to ask and answer some research\n  questions. You argue your benchmark is large, e.g., but then you\n  subsample your own benchmark to provide results. How does this\n  provide a clear benchmark for further research in active learning?\n- i2. personally, I do not find the heavily colored tables providing\n  a very clear overview. A clear decision criterion for what would make\n  a good active learning method would be easier to use."
            },
            "questions": {
                "value": "- q1. What is the main problem you had to solve to scale existing\n  active learning benchmarks to more downstream models and\n  more datasets?\n- q2. How did you choose the three active learning regimes in tab. 1?\n  How does this capture similar situations in different datasets?\n- q3. How did the new benchmark make it easier to answer your three\n  research questions? Did you arrive at different conclusion than\n  those been drawn in earlier benchmarks?\n- q4. What recommendations does your benchmark provide for\n  chosing the best active learning method for a down-stream\n  learner and based on what evidence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a benchmark for active learning on tabular data, in particular, for tabular classification tasks. This aims to fix the issue of the lack of consistent benchmarks for evaluating various active learning methods in various settings with different combinations of learning algorithms and query strategies. The authors also attempt to perform extensive experiments to evaluate the effectiveness of the proposed benchmark."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1: The authors proposed a benchmark to compare different active learning strategies for tabular classification tasks under various settings but in a consistent environment, which is important for performing fair comparisons across the research community.\nS2: The benchmark integrates various datasets to compose a comprehensive benchmark, which covers a variety of settings for evaluating active learning strategies"
            },
            "weaknesses": {
                "value": "W1: The scope of this paper is quite narrow, only focusing on tabular classification and only a subset of active learning strategies. However, ICML usually focuses on more general areas including computer vision and NLP. Considering that active learning strategies have been broadly used in those areas, it would be better to take those settings and solutions from those areas into account\nW2: I think more learning algorithms should be included, such as the recently emerging transformer model for tabular data, such as the model proposed by \"TabTransformer: Tabular Data Modeling Using Contextual Embeddings\".\nW3: I guess it would be better to discuss some surprising findings by using the proposed benchmark. Although the authors mentioned the decrease in performance in some settings in the experiment section, it would be better to discuss more on that aspect, in particular those ignored by existing studies, thus highlighting the necessity of the proposed benchmark."
            },
            "questions": {
                "value": "Q1: Can the proposed benchmark be generalized to other modalities and other emerging machine learning models?\nQ2: Can the authors discuss more interesting findings that are ignored by existing studies?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}