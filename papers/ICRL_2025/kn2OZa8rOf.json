{
    "id": "kn2OZa8rOf",
    "title": "Rethinking Attentions in Zero-Shot Real Image Editing",
    "abstract": "Editing natural images using textual descriptions in text-to-image diffusion models remains a significant challenge, particularly in achieving consistent generation and handling complex, non-rigid objects. Existing methods often struggle to preserve textures and identity, require extensive fine-tuning, and exhibit limitations in editing specific spatial regions or objects while retaining background details. This paper proposes Context-Preserving Adaptive Manipulation (CPAM) -- a novel zero-shot method for complicated, non-rigid real image editing. Specifically, we propose a preservation adaptation module that adjusts self-attention mechanisms to effectively preserve and independently control the object and background. This ensures that the objects' shapes, textures, and identities are maintained while keeping the background undistorted during the editing process using the mask guidance technique. Additionally, we develop a localized extraction module to mitigate the interference with the non-desired modified regions during conditioning in cross-attention mechanisms. We also introduce various mask-guidance strategies to facilitate diverse image manipulation tasks in a simple manner. Extensive experiments on our newly constructed Image Manipulation BenchmArk (IMBA), a robust benchmark dataset specifically designed for real image editing, demonstrate that our proposed method is the preferred choice among human raters, outperforming existing state-of-the-art editing techniques.",
    "keywords": [
        "Stable Diffusion",
        "Image Editing",
        "Zero-Shot Algorithm",
        "Attention"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kn2OZa8rOf",
    "pdf_link": "https://openreview.net/pdf?id=kn2OZa8rOf",
    "comments": [
        {
            "summary": {
                "value": "- This paper presents Context-Preserving Adaptive Manipulation (CPAM) to address complex, non-rigid image editing using a tuning-free, zero-shot approach. \n- It utilizes both self-attention and cross-attention mechanisms in Stable Diffusion to enable intricate and text-guided image edits.\n- It proposes the Preservation Adaptation Process to adjust self-attention layers to retain the identity, texture, and shape of the object being edited while the background remains unchanged.\n- It also includes a Localized Extraction Module to avoid undesired changes to other image areas, and it enables the selective application of cross-attention.\n- It proposes various strategies to control the editing scope based on the task, e.g. object removal, replacement, or background alteration, and it allows users to define which parts of the image are editable.\n- The paper also introduces a new benchmark to evaluate real-image editing models and provides extensive qualitative and quantitative comparisons to show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper offers useful advances for real-image editing by removing the need for fine-tuning and allowing flexible, zero-shot editing.\n- This paper is organized well and gives clear explanations of each new component and how they work together to improve image editing.\n- The author provides extensive experiments to show that the method works better than existing methods.\n- This paper also introduces a new benchmark dataset for more comprehensive evaluation."
            },
            "weaknesses": {
                "value": "- The paper proposes various new modules, but does not delve into ablation studies that isolate and analyze the impact of each component/module within the framework.\n- The teaser figure shows unintended changes in the horse's view and texture, despite no prompt specifying these edits. This suggests some loss of subject-specific details. This also contradicts the authors' claim in Figure 14, where they state that without fine-tuning, the model cannot generate novel poses or views. It raises questions about the proposed method's control over subject attributes.\n- Table 1 shows only marginal improvements in metrics that measure overall image quality and background. This makes the advantage unclear. Also, the paper should include some notes on each metric. For example, CLIP score could mean either prompt alignment (image-text) score or the subject-level image similarity (image-image) score.\n- The dependency on high-quality masks can limit practicality in fully automated editing and in situations where generating precise masks is difficult.\n- Some recent related works in the same domain:\n\tProxEdit: Improving Tuning-Free Real Image Editing with Proximal Guidance\n\tEmuEdit: Precise Image Editing via Recognition and Generation Tasks"
            },
            "questions": {
                "value": "Could the authors provide an ablation study to isolate and analyze the impact of each component? Also, could the authors clarify the limitations of pose/view manipulation in the current framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The method proposes mask-guided image editing."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Editing performance is improved from baselines."
            },
            "weaknesses": {
                "value": "The method lacks novelty. The method of self-attention injection is already well-known method, and usage of mask-guidance is not a novel method. It seems the proposed method is combination of framework of Diffedit (automatic mask generation) and self-attention injection similar to Plug-and-play diffusion features. I think there is no new or technical idea which can further contribute to the generative AI field. \n\nAlso, the quality of output edited images are not satisfactory. The paper proposes that the editing method can be applied to all kinds of editing, but it seems that the outputs can be obtained with using baseline methods with proper parameter control."
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Context-Preserving Adaptive Manipulation (CPAM) for zero-shot real image editing. It includes three main components: 1) a preservation adaptation module that adjusts self-attention mechanisms to effectively preserve and independently control the object and background; 2) a localized extraction module that applies attention to the spatial pixels of the extracted object from the feature query to the target prompt, while the remaining pixels attend to a null text prompt; and 3) various masking strategies tailored to different editing needs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. This paper is well-written and esay to read. \n2. The performance of this paper is promising, achieving state-of-the-art results across various metrics.\n3. The insights on multi-text guided synthesis (Fig. 2 and Sec. 3.4) are interesting."
            },
            "weaknesses": {
                "value": "1. Lacking of experimental evidence supporting the claim that \"null text does not affect the output,\" as stated in Lines 206-211. \n2. The notation \"t > T, l > L\" in Fig. 3(b) is inconsistent with \"s > S, l > L\" in Eq. 2, and Fig. 3 requires further refinement.\n3. The proposed method, CPAM, seems incremental because: 1) the main design, \"Preservation Adaptation,\" is highly similar to \"Mask-Guided Mutual Self-Attention\" from the MasaCtrl paper; and 2) the masking strategy, which aggregates cross-attention maps across all steps and layers, also closely resembles that of MasaCtrl in Sec. 4.2. \n4. Lacking of ablation studies to validate the effectiveness of the proposed modules; only results under different classifier-free guidance scales are presented in Sec. A.1."
            },
            "questions": {
                "value": "I look forward to your response to address my concerns outlined in the 'Weaknesses' section. I will adjust my score based on your reply and the ratings from the other reviewers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a Context-Preserving Adaptive Manipulation method for real image editing. Preservation adaptation module and localized extraction module are introduced for keeping objects maintained and mitigating interferences. Additionally, some mask-guidance strategies and a new benchmark are mentioned. The proposed method demonstrates better results in terms of context maintenance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The insight of the paper is good, and the implementation of image editing with background preservation is close to the actual application scenarios.\n2. The visualized results show that the proposed method achieves the desired effect.\n3. The structure of the paper is well organized."
            },
            "weaknesses": {
                "value": "1. Although the problems attempted to be solved are interesting, the solutions lack innovation. \n2. The additional introduction of mask inputs leads to unfair comparisons with existing methods.\n3. The proposed mask-guidance strategies are rule-based and not flexible enough to cope with diverse editing needs.\n4. Inadequate visual comparison of ablation experiments.\n5. The structure and presentation of figure 2 is not sufficiently clear.\n6. English expression needs improvement: \u201cretaining object is true\u201d, \u201cwhere s, l, S = 3, L = 8 denotes\u201d and \u201celement-wise dot product\u201d."
            },
            "questions": {
                "value": "1. How much does the accuracy of the input mask affect CPAM?\n2. What will be the result of acquiring masks for CPAM inputs by means of adaptive perception or reference segmentation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}