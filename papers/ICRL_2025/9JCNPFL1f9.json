{
    "id": "9JCNPFL1f9",
    "title": "Visual Haystacks: A Vision-Centric Needle-In-A-Haystack Benchmark",
    "abstract": "Large Multimodal Models (LMMs) have made significant strides in visual question-answering for single images. Recent advancements like long-context LMMs have allowed them to ingest larger, or even multiple, images. However, the ability to process a large number of visual tokens does not guarantee effective retrieval and reasoning for multi-image question answering (MIQA), especially in real-world applications like photo album searches or satellite imagery analysis. In this work, we first assess the limitations of current benchmarks for long-context LMMs. We address these limitations by introducing a new vision-centric, long-context benchmark, \"Visual Haystacks (VHs)\". We comprehensively evaluate both open-source and proprietary models on VHs, and demonstrate that these models struggle when reasoning across potentially unrelated images, perform poorly on cross-image reasoning, as well as exhibit biases based on the placement of key information within the context window. Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented Generation), an open-source, lightweight visual-RAG framework that processes up to 10k images on a single 40G A100 GPU\u2014far surpassing the 1k-image limit of contemporary models. MIRAGE demonstrates up to 13% performance improvement over existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA multi-image QA benchmark, and achieves competitive performance on single-image QA with state-of-the-art LMMs.",
    "keywords": [
        "Large Multimodal Models",
        "Visual Question Answering"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We introduce \"Visual Haystacks,\" a vision-centric NIAH benchmark to evaluate LMMs, show that current models struggle with this task, and propose MIRAGE, a framework that improves performance by integrating retrieval with the VQA process.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=9JCNPFL1f9",
    "pdf_link": "https://openreview.net/pdf?id=9JCNPFL1f9",
    "comments": [
        {
            "summary": {
                "value": "The authors presents Visual Haystacks (VHs), a new vision centric benchmark designed to assess the performance of Large Multimodal Models (LMMs) in the multi-image question answering (QA) task. In addition, the author proposes a new visaul-RAG framework, MIRAGE, to enhance the task performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Novel Multi-Image QA Benchmark: The authors introduce an interesting multi-image QA benchmark, Visual Haystacks, designed around a vision-centric \"needle-in-a-haystack\" scenario, providing a fresh and challenging setting for the LMM evaluation.\n\n* Comprehensive Model Evaluation:  The paper conducts a thorough evaluation of LMMs on the VHs benchmark, uncovering important insights into current models, such as vulnerability to visual distractors, challenges with multi-image understanding, and tendencies toward positional visual bias.\n\n* Novel Visual RAG Framework: The authors introduce a novel visual RAG framework that combines a compressor and a retriever. The compressor efficiently processes up to 10,000 images on a single 40GB A100 GPU, while the retriever identifies the top-k most relevant images for a given question, enhancing the framework\u2019s scalability and efficiency."
            },
            "weaknesses": {
                "value": "* Limited Object Diversity: The authors constructed the VHs benchmark using objects from the COCO dataset, which contains only 80 object categories. This limited selection may restrict the diversity and comprehensiveness of the benchmark, potentially affecting its ability to evaluate models across a broader range of visual scenarios.\n\n* Restricted Question Diversity: The authors appear to rely on a few simple templates to generate questions, which may restrict the variety of question types in the benchmark.\n\n* More like Object Detection than QA Reasoning: Many questions in the benchmark (e.g., \"For the image with a truck, is there a dog?\") seem to primarily assess the model\u2019s object detection abilities rather than its visual QA reasoning skills. It is questionable if the benchmark requires the advanced visual QA reasoning skills from the models.\n\n* Missing Related Work: The paper does not reference several recent multi-image QA benchmarks, for example: \n 1. CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs\n 2. MANTIS: Interleaved Multi-Image Instruction Tuning\n 3. MUIRBENCH: A Comprehensive Benchmark for Robust Multi-Image Understanding. \n\nAdditionally, a similar multi-image retrieval approach was introduced in \"ColPali: Efficient Document Retrieval with Vision Language Models\", but this work was also not cited."
            },
            "questions": {
                "value": "Please see the weakeness. In addition,\n* How many templates were used to generate questions?\n* What advantages does the VHs benchmark offer compared to recent multi-image QA benchmarks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the limitations of Large Multimodal Models (LMMs) in multi-image question answering, where handling large visual contexts does not ensure effective retrieval and reasoning across images. Current benchmarks reveal biases and challenges in MIQA, such as poor cross-image reasoning and sensitivity to information placement. To overcome these, the authors propose \"Visual Haystacks (VHs),\" a vision-centric benchmark that tests retrieval and reasoning over multiple images, highlighting models' struggles with visual distractors and multi-image reasoning. They also introduce MIRAGE, an open-source Multi-Image Retrieval Augmented Generation framework capable of handling up to 10,000 images on a single GPU, achieving significant improvements over existing models and setting new standards in MIQA benchmarks like RetVQA. Key contributions include VHs, systematic LMM evaluation, and MIRAGE's scalable MIQA capabilities."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- I generally feel the direction is important to our community where design meaningful Visual Haystack benchmark for evaluating VLM. \n- Some interesting points are discovered when evaluating models on the proposed benchmark. Since random guess could achieve 50% accuracy in the proposed benchmark, some open-sourced VLMs performance significantly drop even the Haystack size is very small. However, those models maintain high scores in some public evaluation-datasets. \n- Some detailed experiments are conducted such as needle position and running time. \n- The proposed benchmark are made publicly available under MIT license, which is good for community."
            },
            "weaknesses": {
                "value": "- Benchmark construction is still mainly centered around recognition tasks, based on benchmark design principles listed in Line129~138. Basically, it requires a strong recognition among all the input images, rather than true visual reasoning. \n- Based on the Figure 2 and 3, certain models, such as Gemini, GPT and the proposed MIRAGE, consistently perform better on the proposed multi-needle challenges compared to single-needle tasks. However, the multi-needle challenges are intentionally designed to be more difficult, as they demand additional reasoning across multiple images. Does this mean failure in designing the benchmark?\n- Since the benchmark is constructed in way of examining recognition, therefore the proposed method contain ad-hoc modules, such as \"a retriever module then calculates relevance scores, ensuring that only the most relevant images are passed to the LLM for final reasoning.\" Does this design hold for general visual reasoning tasks? For example, many of the tested single image dataset used in this paper, do not need this retriever module at all. \n- The proposed framework achieved not-very-good performance on some of the tested datasets. Also, there are many datasets that not being tested such as SEED, MME, and CHAIR."
            },
            "questions": {
                "value": "- Could you please address the points raised in the above weakness?\n- Could you please add some randomly sampled failure cases made by GPT or Gemini? Sometimes failure cases can tell more than good cases. \n- Could you please address the ethics concerns around the code license?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Is it possible for the author to release their code under the MIT license, considering it is derived from the Apache 2.0-licensed LLaVA codebase? Could the author elaborate this point?"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a long context, visual needle in a haystack benchmark which composed of 1k yes/no questions changeling the model to reasoning and find the target object in the images. It evaluated on both open-source and closed-source LMMs and reveal several critical findings such as susceptibility to visual distractors, difficulty in multi-image reasoning, and a bias in image positioning. It introduces a new baseline called MIRAGE (Multi-Image Retrieval Augmented Generation) for better handling of VH tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper introduced a new visual needle in a haystack benchmark which composed of 1k yes/no questions. \n2. Evaluated on both open-source and close-source models and gained three insightful findings. \n3. Introduced a new baseline called MIRAGE for better handling of visual haystack tasks."
            },
            "weaknesses": {
                "value": "1. The questions are only limited to yes/no questions. \n2. The question template are very limited, seems only three. \n3. MIRAGE has a significant performance drop in 4 out of 7 general VQA tasks. \n4. The approach of MIRAGE, deselecting unrelated (distracting) images somehow circumvents the VH challenge, as the this challenge lies in how model can reasoning in long context.  \n5. The task of finding a target object seems still not simulating a real world scenario of long context visual reasoning task."
            },
            "questions": {
                "value": "1. I'm confused about the difference between the MIRAGE model in Table 1 and the Q-former Model in Table. Doesn't MIRAGE utilize Q-former. \n2. See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}