{
    "id": "Q7uE3M5aMD",
    "title": "Discrimination-free Insurance Pricing with Privatized Sensitive Attributes",
    "abstract": "Fairness has emerged as a critical consideration in the landscape of machine learning algorithms, particularly as AI continues to transform decision-making across societal domains. To ensure that these algorithms are free from bias and do not discriminate against individuals based on sensitive attributes such as gender and race, the field of algorithmic bias has introduced various fairness concepts, including demographic parity and equalized odds, along with methodologies to achieve these notions in different contexts. Despite the rapid advancement in this field, not all sectors have embraced these fairness principles to the same extent. One specific sector that merits attention in this regard is insurance. Within the realm of insurance pricing, fairness is defined through a distinct and specialized framework. Consequently, achieving fairness according to established notions does not automatically ensure fair pricing in insurance. In particular, regulators are increasingly emphasizing transparency in pricing algorithms and imposing constraints\non insurance companies on the collection and utilization of sensitive consumer attributes. These factors present additional challenges in the implementation of fairness in pricing algorithms. To address these complexities and comply with regulatory demands, we propose an efficient method for constructing fair models that align with the specific fairness criteria unique to the insurance pricing domain. Notably, our approach only relies on privatized sensitive attributes and offers statistical guarantees. Further, it does not require insurers to have direct access to sensitive attributes, and it can be tailored to accommodate varying levels of transparency as required. This methodology seeks to meet the growing demands for privacy and transparency from regulators while ensuring fairness in insurance pricing practices.",
    "keywords": [
        "Fairness",
        "Discrimination-free Insurance Pricing",
        "Insurance",
        "Privatized Attributes",
        "Privacy"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Q7uE3M5aMD",
    "pdf_link": "https://openreview.net/pdf?id=Q7uE3M5aMD",
    "comments": [
        {
            "summary": {
                "value": "This paper considers fairness insurance pricing problems. The method proposes to achieve actuarial fairness in insurance pricing. Actuarial fairness is different from conventional machine learning fairness concepts, like demographic parity and equalized odds. In insurance pricing, existing works mainly consider three methodologies in solving fairness: counterfactual approach, group fairness approach, and probabilistic approach. However, all aforementioned methods requiring direct access of sensitive information might not be available due to regulation.  This paper proposes a method that introduces the trusted third party (TTP) that deals with noiseless or noisy sensitive information, allowing discrimination-free premium without direct access to sensitive information."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper addresses a significant real-world challenge: achieving discrimination-free insurance pricing. The proposed method is not only grounded in solid mathematical foundations but also applicable to practical applications. It has been thoroughly evaluated through comprehensive experiments on scenarios with both known and unknown noise rates."
            },
            "weaknesses": {
                "value": "Please see the questions."
            },
            "questions": {
                "value": "1. question regarding the problem setting: Does the insurer have access to sensitive attributes, but only in an indirect manner, such that they cannot directly access these sensitive attributes? If so, the TTP may need to generate a fair premium without directly knowing the sensitive attributes. In that case, why do we still need the TTP? Could the insurer implement the method directly to generate the premium on their side instead?\n\n2.  Why does integral over $d$ with measure $\\mathbb{P}^*(d)$ bring discrimination-free price if $X$ contains information of $d$. \n\n3. Could the author elaborate on Theorem 4.5? What does $\\tilde{\\epsilon}$ represent here? When compareTheorem 4.5 to Theorem 4.3, the only difference is this term. Does it represent the error from estimating the error rate $\\pi$ and $\\bar{pi}$?\n\n4. Does this condition $M_g+\\frac{C_1+\\theta}{ln2}>\\tilde{\\epsilon}>\\theta$ have some meaning or is it simply a technical assumption?\n."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies the problem of providing discrimination-free insurance prices when information about true sensitive attributes is hidden from the insurer---in particular, it is held by a trusted third-party which, for example, may add noise to ensure that sensitive attributes are differentially-private. In addition to showing risk bounds for both known and unknown noise rates, the authors conduct thorough experimental evaluations for how properties of the problem instance and hyperparameters used in the algorithm affect overall performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Exposition in section 4 is straightforward (especially leading up to Theorem 4.3). \n* Experimental evaluation is thorough and well motivated."
            },
            "weaknesses": {
                "value": "* I'm not sure how realistic/practical this problem setup is at a high level; do such protocols currently exist? \n* Risk bounds in section 4 seem to follow pretty 'classic' concentration-type arguments, esp. in the dependence on VC(F). This is fine, though it does mean the interpretation should be more qualitative (scaling wrt $n$ and noise level) than quantitative\n\nPresentation comments:\n* it could be helpful to have a diagram illustrating the protocol for interaction between insurer and TTP.  \n* Maybe worth noting that because risk is defined wrt an arbitrary (possible per-group, if I understand correctly?) $L$, minimizing risk of $f$ is sufficient to optimize for (e.g.) the ideal price $h^*$ or whatever downstream utility the insurer gets."
            },
            "questions": {
                "value": "* Why is the transformation $T(X)$ necessary?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses fairness in insurance pricing where the use of sensitive attributes like race and gender is restricted. Recent regulatory demands requires approaches that prevent discrimination without directly using sensitive data. The authors propose a multi-party framework where insurers collaborate with a trusted third party (TTP) and the sensitive attributes influence pricing only indirectly.\n\nThe main contributions of the paper are: (i) The authors introduce a framework that enables insurers to calculate fair premiums without direct access to sensitive attributes, while the TTP uses a noised version of sensitive features to generate \"discrimination-free\" insurance prices. (ii) The authors provide theoretical guarantees for two settings -- when the noise of the sensitive features is known and unknown to the TTP. (iii) The paper demonstrates the method\u2019s effectiveness in two insurance datasets, showing its robustness to noise and the impact of noise estimation errors on performance."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "To start, I think this paper is a very strong paper. It studies an important research question -- fairness in insurance pricing, which is a domain that typically emphasizes actuarial fairness over algorithmic fairness. The authors introduce an innovative framework that adapts to regulatory requirements by using privatized, noised sensitive attributes for TTP to calculate discrimination-free premiums. This application of differential privacy within a multi-party setup in the insurance sector is novel and distinguishes this work from other fairness research. \n\nWhat I like about the paper the most is its high quality in its technical rigor and thoroughness. The authors offer theoretical guarantees for scenarios with known and unknown noise rates, with clear delineation of assumptions and the rigorous derivation of theoretical results. \n\nThe paper is also well-organized and easy to follow. Key concepts, such as \"discrimination-free pricing,\" are defined early on, and the technical sections are structured nicely, which allows readers to follow the framework\u2019s development from problem formulation to implementation. The explanations of complex privacy mechanisms, including local differential privacy and its implications for sensitive data handling, are accessible even for readers less familiar with privacy-preserving machine learning.\n\nThe paper also provides an in-depth empirical analysis section that effectively demonstrates the method\u2019s applicability and limitations, contributing to the clarity of the results."
            },
            "weaknesses": {
                "value": "1) The paper's setting relies on the feasibility of the framework via data sharing with TTP. What if this framework is not feasible, e.g., using a TTP is not feasible? Would the authors be able to comment on something along this line and also comment on the flexibility of their proposed framework under different regulatory requirements?\n\n2) The tightness of the upper bound provided in the two theorems. \n\n3) It would be nice to add comparisons to existing fairness methods or baseline models for insurance pricing. \n\n4) The study focuses on the US Health Insurance and Auto Insurance datasets, but additional datasets -- particularly those representing other insurance types or more complex demographic distributions -- could add value.  \n\n5) The paper could benefit from a more explicit discussion of its limitations and future directions."
            },
            "questions": {
                "value": "1) Could the authors provide more insights into the tightness of the upper bound presented in Theorems 4.3 and 4.5? For example, how does the gap between the empirical risk R^LDP (f)  and  R(f^*)  behave as noise decreases?\n\n2) Do the authors see potential for this framework to be applied in domains beyond insurance? For instance, could it be adapted for fields with similar regulatory constraints on fairness and sensitive data, such as finance or healthcare?\n\n3) The framework currently focuses on discrete sensitive attributes. Could the authors elaborate on how it might extend to continuous sensitive attributes, or discuss challenges that may arise? \n\n4) Could the authors explicitly discuss known limitations of the proposed approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper develops a method for learning a \"discrimination-free\" insurance pricing policy when sensitive attributes are private, and only noisy forms of these attributes are available to a trusted third party. Theoretical results are presented for the case where the privatization approach is a) fully known, b) has unknown noise rates. The method is applied to health and auto insurance datasets."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Though I'm not an expert in privacy-preserving ML, this paper's theoretical results are probably of interest to people in that field."
            },
            "weaknesses": {
                "value": "The experiments and evaluation section in this paper claims to show that the method \"achieves fair pricing effectively\", but it answers none of the questions that would allow us to determine if such pricing is fair, effective, or desirable. \n\n* How much do men and women pay for insurance after this method is applied?\n* How does this compare to the benefit they receive from insurance payouts?\n* Which other subgroups benefit or are made worse off by this method?\n* If insurance is under/overpriced it could lead to adverse selection, where, for example, high-risk male drivers buy more insurance because it\u2019s cheap, increasing premiums for everyone else. It could even lead to people driving more dangerously at the margin because they know that an incident won\u2019t increase their premiums much. Is there risk of adverse selection or other negative equilibrium effects from using this pricing?\n\nThis paper paper is clearly trying to develop a method for pricing that can be used by real insurers. Unfortunately, though, it treats insurance pricing as an exercise in privacy math, and not as an input to a crucially important product for people's physical and financial health."
            },
            "questions": {
                "value": "Please see my questions on the important practical implications of this method above. In addition:\n\nConvergence in a linear model with 6 features requiring thousands of epochs seems very slow to me - what's causing this?\n\nWhat's the reason for prioritizing convergence rate in the experiments? This doesn't seem like an important property of insurance pricing algorithms (unless, of course, the model diverges), since the time and expense of training the model will likely be very small compared to the revenue and expenses of actually delivering insurance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}