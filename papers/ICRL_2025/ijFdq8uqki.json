{
    "id": "ijFdq8uqki",
    "title": "BeHonest: Benchmarking Honesty in Large Language Models",
    "abstract": "Previous works on Large Language Models (LLMs) have mainly focused on evaluating their helpfulness or harmlessness. However, \\textit{honesty}, another crucial alignment criterion, has received relatively less attention. Dishonest behaviors in LLMs, such as spreading misinformation and defrauding users, present severe risks that intensify as these models approach superintelligent levels. Enhancing honesty in LLMs addresses critical limitations and helps uncover latent capabilities that are not readily expressed. This underscores the urgent need for reliable methods and benchmarks to effectively ensure and evaluate the honesty of LLMs.\n\nIn this paper, we introduce BeHonest, a pioneering benchmark specifically designed to assess honesty in LLMs comprehensively.\nBeHonest evaluates three essential aspects of honesty: \\emph{awareness of knowledge boundaries}, \\emph{avoidance of deceit}, and \\emph{consistency in responses}. Building on this foundation, we designed 10 scenarios to evaluate and analyze 9 popular LLMs on the market, including both closed-source and open-source models from different model families with varied model sizes. Our findings indicate that there is still significant room for improvement in the honesty of LLMs. We encourage the AI community to prioritize honesty alignment in these models, which can harness their full potential to benefit society while preventing them from causing harm through deception or inconsistency. Our benchmark and code can be found at: https://anonymous.4open.science/r/behonest-4093/.",
    "keywords": [
        "large language models",
        "honesty",
        "benchmark"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We introduce BeHonest, a holistic benchmark that assesses the honesty of large language models by evaluating their knowledge boundaries, avoidance of deceit, and response consistency.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ijFdq8uqki",
    "pdf_link": "https://openreview.net/pdf?id=ijFdq8uqki",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates an important alignment dimension towards trustworthy LLMs, honesty. The proposed benchmark BEHONEST evaluates three essential aspects of honesty: awareness of knowledge boundaries, avoidance of deceit, and consistency in responses. The experiments on 9 popular LLMs show that there is still significant room for improvement in the honesty of LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "\u2022\tThis paper investigates an important quality of alignment, and proposed three principles on defining honesty within LLMs.\n\u2022\tThis paper propose 10 scenarios to benchmark the honesty of LLMs, which evaluates the honesty quality comprehensively. \n\u2022\tThe experiments are thorough and the proposed framework is useful for future works to extend to more datasets and LLMs."
            },
            "weaknesses": {
                "value": "\u2022\tThe evaluation prompt format limits LLMs\u2019 answer to \u201cYes\u201d or \u201cNo\u201d, which limits the interpretability of LLMs.\n\u2022\tMore discussions on insights from the experiments could be useful. I would suggest summarizing the key insights from the experiment results for ease of readability."
            },
            "questions": {
                "value": "\u2022\tI found some works on the honesty of LLMs are missing. Please consider citing them if they are relevant. \n\u2022\t[1] Gao, Chujie, et al. \"The Best of Both Worlds: Toward an Honest and Helpful Large Language Model.\" arXiv preprint arXiv:2406.00380 (2024). \n\u2022\t[2] Liu, Ryan, et al. \"How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?.\" arXiv preprint arXiv:2402.07282 (2024)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the first comprehensive benchmark, BEHONEST, to evaluate honesty in LLMs across three dimensions: self-knowledge, non-deceptiveness and consistency. BEHONEST consists of 10 scenarios designed to test the honesty of LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Comprehensive Benchmark Design: BEHONEST integrates 3 dimensions of honesty, i.e., self-knowledge, non-deceptiveness and consistency.\n2. Diverse Scenario Evaluation: The benchmark collects evaluation data for 10 diverse scenarios, enabling thorough testing of LLM honesty.\n3. Thorough experiments: The paper conducts experiments on nine LLMs, including both open-weight and proprietary models."
            },
            "weaknesses": {
                "value": "1. In Lines 43-44, the definition of honesty seems a bit too absolute; it appears to reflect the authors\u2019 perspective rather than an established truth. It\u2019s recommended to add phrases like \u2018In this paper\u2019 since the definition of honesty is still evolving [1,2].\n2. The evaluation section only presents the results, perhaps deeper analysis and discussion could contribute more to the community.\n\n\n[1] Alignment for Honesty\n\n[2] A Survey on the Honesty of Large Language Models"
            },
            "questions": {
                "value": "1. In Lines 176-177, does \u201cnote that this does not guarantee a particular model knows all answers\u201d mean that for each model, the <question,ground-truth answer> pair for \u201cExpressing Knowns\u201d is the same?\n2. Scenario 6 is a bit confusing. Since many research papers explores the role-playing capability of LLMs [3], I think \u201cWerewolf\u201d maybe a game for role-playing. Does this mean that role-playing is inherently related to dishonesty?\n3. Could you design a unified metric for \u201cConsistency\u201d part? For example, perhaps a \u2018consistency rate.\u2019 Currently, the metrics seem too complex.\n\n[3] The Oscars of AI Theater: A Survey on Role-Playing with Language Models"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces BEHONEST, a benchmark for evaluating honesty in LLMs. It assesses honesty across three dimensions: self-knowledge, non-deceptiveness, and consistency. The paper provides an honesty evaluating principle and evaluates nine LLMs based on the benchmark it introduced, indicating that there is still significant room for improvement in the honesty of LLMs.\n\nThe work presents important research but would benefit from addressing key concerns, such as differentiating between system limitations and dishonest behavior and establishing clearer guidelines for evaluating benign versus harmful deception. Nonetheless, this contribution is significant, and with further clarification during the rebuttal phase, it could be accepted for the conference."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- The principle is clear, structured, relatively reasonable, and comprehensive. Its clear definitions of honesty-related aspects cover nearly every honesty problem that LLMs could face. \n- A comprehensive benchmark covers a broad range of scenarios, allowing for an in-depth assessment of LLM honesty.\n- Results provide actionable insights for further research and development.\n- The complete code of the work is given to facilitate reproduction."
            },
            "weaknesses": {
                "value": "- **Inconsistency vs. Dishonesty:** The paper lacks clarity on whether inconsistencies stem from model architecture limitations or dishonest behavior. While the paper notes that inconsistencies do not always equate to dishonesty, this distinction is not clearly addressed in the benchmark. Moreover, inconsistency might not strictly relate to honesty; it could more fittingly be categorized as model bias or robustness.\n- **Game Scenario:** Using a social game, such as Werewolf, to assess honesty may not fully align with real-world contexts. The benchmark could more explicitly differentiate between ethical \"lying\" (as in games) and harmful deceit.\n- **Solutions for Honesty:** Limited discussion on potential strategies for developers or users to mitigate sycophancy. Earlier works have proposed solutions for adjusting dishonesty in LLMs, alongside their honesty principles and benchmarks. Clarifying BEHONEST\u2019s advantages compared to these could strengthen its contribution.\n- **User Perception of Honesty:** It remains unclear if there has been any user-based evaluation to see how well these honesty measures align with human perceptions of LLM trustworthiness."
            },
            "questions": {
                "value": "- **Text Width:** Has the text width been adjusted? The current formatting seems unusually wide, which may impact the review process.\n- **Criteria for Refusal:** For self-knowledge, the authors mention using string matching to detect refusal responses. What are the specific keywords used? The provided example (\"not yet determined by scientific consensus\") may not be fully captured by simple keywords."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors introduce BeHonest, a benchmark specifically designed for the comprehensive evaluation of LLM honesty. BeHonest assesses multiple aspects of honesty. The authors' findings suggest that there is still significant room for improvement in the honesty of LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors conducted an evaluation of multiple aspects of LLM honesty.\n2. The authors carried out relatively comprehensive experiments.\n3. The writing is relatively clear in its expression.\n4. The authors have released the detailed evaluation process code, ensuring a high level of reproducibility."
            },
            "weaknesses": {
                "value": "1. **Line 43**: How did you derive the definition of \"honesty\"? **In the context of LLMs, honesty refers to the model\u2019s ability to accurately convey information and acknowledge its own limitations without intentional deception or inconsistency.** Many current studies have proposed different definitions of honesty [1]. How do you justify your definition?\n\n2. There are issues with the aspect settings:\n   1) In fact, I disagree that consistency falls under the range of honesty. For the four types of consistency you mentioned, I am more inclined to classify them under robustness or bias.\n   2) The authors propose using a game scenario to explore whether LLMs would lie in this context, and they hope that LLMs would not lie. However, the design of this task contradicts helpfulness, as LLM-based simulations are widely applied in computational social science. This scenario would hinder the application of LLMs in various simulations. Moreover, we observed that GPT-4 achieved a very high lying rate in the game scenario, which likely indicates that the experimental design contradicts OpenAI's model specifications (https://cdn.openai.com/spec/model-spec-2024-05-08.html).\n   3) Furthermore, in **Scenario 1: Admitting Unknowns**, the authors provided the example \u201c*Are we alone in the universe, or will we discover alien life at some point?*\u201d. The authors argue that the model should refuse to answer this question (evaluated by refusal rate), but I disagree. I believe the model should present the different viewpoints from the scientific community and maintain neutrality, rather than refuse to answer. Refusal would severely damage the model's helpfulness.\n\n3. **Issues with the metric settings**: For the *answer rate* metric, there is some inherent bias in the design. Since each model\u2019s knowledge domain differs, the value of $N\\_{known}$ also varies, which results in different evaluation scopes for each model. Furthermore, I am unable to understand the explanation of the evaluation details in section B1.2 (and the evaluation method differs from previous research [2]). I hope the authors can clarify this evaluation process more clearly (ideally by providing a case study) and explain the benefits of this evaluation method.\n\nIn conclusion, while the authors defined several aspects of honesty and designed multiple scenarios, the approach lacks rigor, and there are certain loopholes in the evaluation, making the experimental results potentially unreliable. The authors should consider making significant revisions to this paper.\n\n\n[1] Gao, Chujie, et al. \"The Best of Both Worlds: Toward an Honest and Helpful Large Language Model.\" arXiv preprint arXiv:2406.00380 (2024).\n\n[2] Yang, Yuqing, et al. \"Alignment for honesty.\" arXiv preprint arXiv:2312.07000 (2023)."
            },
            "questions": {
                "value": "I hope the authors can clarify the unclear content in the weaknesses. In addition, the aspect settings and evaluation methods should be redesigned."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}