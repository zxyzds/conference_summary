{
    "id": "TLmibuPMyi",
    "title": "An Efficient Algorithm For Computing Optimal Wasserstein Ball Center",
    "abstract": "Wasserstein Barycenter (WB) is a fundamental problem in machine learning, whose objective is to find a representative probability measure that minimizes the sum of its Wasserstein distance to given distributions. WB has a number of applications in various areas.  However, in some applications like model ensembling, where it aggregates predictions of different models on the label space, WB may lead to unfair outcome towards underrepresented groups (e.g., a \"minority'' distribution may be far away from the obtained WB under Wasserstein distance). To address this issue, we propose an alternative objective called  ``Wasserstein Ball Center (WBC)''. Specifically, WBC is a distribution that encompasses all input distributions within the minimum Wasserstein distance, which can be formulated as a minmax optimization problem. We show that the WBC problem with fixed support is equivalent to solving a large-scale linear programming (LP) instance, which is quite different from the previous LP model for WB. By incorporating some novel observations on the induced normal equation, we propose an efficient algorithm that accelerates the interior point method by $O(Nm)$ times ($N$ is the number of distributions and $m$ is the support size).  Finally, we conduct a set of experiments on  both synthetic and real-world datasets. We demonstrate the computational efficiency of our algorithm, and showcase its better accuracy on model ensembling under heterogeneous data distributions.",
    "keywords": [
        "Wasserstein barycenter",
        "model ensembling",
        "fairness"
    ],
    "primary_area": "optimization",
    "TLDR": "We introduce and solve a new type of probability center that enhences the fairness performance of WB.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TLmibuPMyi",
    "pdf_link": "https://openreview.net/pdf?id=TLmibuPMyi",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes the WBC-problem, which is a variant of the well-known Wasserstein-ball problem. Here, given a set of discrete distributions $D_1, D_2, \\dots, D_N$  the goal is to compute a distribution (with discrete support over a set of size $m$) that has minimal maximum wasserstein distance from any of these. The main modification from the typical WB-problem is that we seek to minimize the maximum distance, rather than minimize sum weighted sum of the distances. The motivation for doing so is fairness concerns, where averaging a loss may unfairly penalize distributions that correspond to underrepresented or marginalized classes. \n\nThis problem can be relatively straightforwardly be framed as a linear program -- in particular, the wasserstein distance between two discrete distributions can be written as the dot product between 2 appropriately chosen matrices, and thus by using a slack variable we obtain a typical linear program. However, the main technical difficulty in this program is the size -- $N$ and $m$ both can be fairly large, and a naive implementation of the interior point method will achieve an $O(N^3m^4)$-running time, which is infeasible. \n\nThus, the main technical innovation of this paper is a modified interior point method that achieves an $O(N^2m^3)$ running time for this problem. The high level idea is to exploit the shared structure amongst all of the constraint matrices corresponding to the $N$ distributions. Essentailly, interior point methods rely on inverting a matrix, and the matrix being inverted has a block structure that the authors exploit to significantly reduce the running time. \n\nFinally, this work demonstrates that their algorithm achieves better performance on the Fairfaces dataset than naively using the WB-formulation would."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper offers a fundamental optimization problem and gives a fairly elegant and non-trivial solution to it. In particular, the gains in their running time enable this algorithm to scale to relatively large sets of distributions."
            },
            "weaknesses": {
                "value": "I found the claim that concerns about fairness motivate this problem to be somewhat tenuous. First, while the relevance of fairness for tasks such as the fairface dataset makes sense, it is unclear to me that for downstream tasks over more complex distributions that the WBC method will significantly improve fairness. In particular, arguing for a lower maximum distance does not ensure fairness in any of the more rigorous definitions of fairness. \n\nSecond, while it is intuitive to me that the WBC method offers a reasonable solution in ensuring no class distribution is too far from the mean, it is unclear to me why simply changing the distribution weights over WB is insufficient. In particular, if it appears as though one distribution is significantly underrepresented, we could simply apply a larger weight to that distribution and re-run the original optimization problem. While this is more computationally expensive, it could nevertheless be better suited for downstream tasks where the freedom to choose the weights of each distribution allows the practitioner to achieve some kind of objective measure of fairness."
            },
            "questions": {
                "value": "See weaknesses -- could you address my concerns about the connection between this problem and fairness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method to solve the Wasserstein Ball Center problem in case of distributions with discrete support by rewriting it as a Linear Programming problem. Finally several experiments are conducted to demonstrate the efficiency of the proposed algorithm  as well as the enforced fairness of a solution."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The LP formulation of the WBC problem in the discrete support case as well as the proposed iterative algorithm are interesting."
            },
            "weaknesses": {
                "value": "$\\textbf{Experiments}$:\n\nThe overall experiments were supposed to demonstrate two points: (i) the effectiveness of the proposed algorithm in solving the WBC problem, (ii) the advantage of WBC over the classical WB in fairness. Both points were not convincingly justified by the chosen experiments.\n\n$\\textbf{i}$: For the first point, it would be preferable to see the comparison to some other existing optimization algorithms to solve the problem (6), for example the IPM without preconditioning. Additionally, could you clarify if the results in Figure 2 are averaged over multiple runs, and if not, could you include error bars or standard deviations to show variability.\n\n$\\textbf{ii}$:  For the second point, the fairness argument was not very well justified. The fact that a solution to the minmax probelm (2) (WBC) is more sensitive to outliers than a solution to (1) (WB) was straightforward from the fitness function. Are there any quantitative metrics that can justify this claim?\n\n$\\textbf{iii}$:  The conclusion was a bit sloppy, would be valuable to list some suggestions for a future work.  For example, you could propose exploring theoretical guarantees, applications to other domains, or comparisons to additional fairness metrics.\n\n$\\textbf{Small typos}$:\n\n- Page 3 : missing reference to Woodbury\u2019s equality\n- Page 4: missing reference to \"fixed-support WB\""
            },
            "questions": {
                "value": "$\\textbf{Extensions}$:\n- How does the program 6 changes when we lift the assumption of the fixed support distributions ? Since all the distributions are assumed to be of a discrete support, couldn't we just define the common support as a union and lift the assumption ?\n\n$\\textbf{Experiments}$:\n- How do you explain the non-monotonic behavior of the accuracy as a function of noise ratio that we observe in the table 1. \n- Same question for the results at u = 100%\n- What happens in the lower noise regime (i.u for u<50%?)\n- Have you experimented with the entries following other distribution than uniform.\n\nCould you provide possible explanations or hypotheses for these observations? Additionally, reporting results for the lower noise regime (u < 50%) would provide a more complete picture of the method's performance across different noise levels."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method for computing the centre of the _Wasserstein Ball_, that is, the minimum-radius ball in Wasserstein space that contains a given set of distributions. The aim of this Wasserstein Ball Center (WBC) is to be used as an alternative to the Wasserstein Barycentre, as it is expected that WBC will be more robust to outliers and thus be promising in fairness applications. \n\nThough some authors have considered the WBC in the past, the main contribution of the article seems to be the formulation of the min-max problem and the proposed solution."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well motivated. The WBC is certainly an attractive alternative to the Wasserstein barycentre. \n\nThe formulation of the optimisation problem is provided in a detailed manner, and it seems to be the main contribution of the article (Thm 3.2). However, this reviewer is not an expert in optimisation and this cannot assess the correctness (other than the experimental evidence provided) or the novelty of the proposed solution\n\nThe experimental validation *for the computational complexity of the proposed method* is convincing"
            },
            "weaknesses": {
                "value": "My main reservation with this article is that it makes a number of claims about the proposed method being suitable for *fairness*, however, the method is only either tested synthetically on data with outliers. \n\nFairness in ML is much more than robustness to outliers. There are defined quantitative indicators for fairness (e.g., disparity impact) and the notion of sensitive/private variables in a learning setting. Modifying an average is *far* from a subset of distributions is not solving a fairness problem. Therefore, saying that *WBC shows better fairness than WB* is a jumping conclusion as i) no fairness problems have been presented in the paper, and ii) no fairness indicators have been measured.\n\nWhy not leave the contribution just as a gain in computational complexity? (again, this is not my area of expertise)\n\nThere are some English problems, e.g., _we uses_  or \"who >> which\" but nothing too important. There are also some double parenthesis in the references and missing references (?)\n\nCaption of Fig 1: what is \"t\"?\n\nThere are other types of barycenters that were not considered, e.g., the weak Wasserstein barycenter, and also some unbalanced OT techniques that can help with the outlier detection problem"
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The weighted Wasserstein barycenter problem seeks to find a probability distribution $\\mu^*$ that minimizes the sum of Wasserstein distances to a given set of probability distributions $\\mu_1,\\ldots,\\mu_N$ with respective weights $w_1,\\ldots,w_N$, i.e., $\\mu^* = \\arg \\min_{\\mu} \\sum_{i=1}^N w_k W_p(\\mu, \\mu_k)^p$, where $W_p(\\cdot, \\cdot)$ denotes the $p-Wasserstein distance. The Wasserstein barycenter problem is often used in applications such as image processing, natural language processing, machine learning, and computer graphics.\n\nThis paper notes that this formulation of the problem may lead to unfair outcomes towards specific distributions that could represent \"protected\" subpopulations and thus proposes an alternative formulation to minimize the objective $\\min_{\\mu} \\max_{k=1 \\in [N]} w_i W_p(\\mu, \\mu_k)$. They show that the problem can be formulated as a linear program when the support of the probability distributions is discrete and then describe interior point methods to improve the efficiency of solving the linear program. Finally, the paper includes a number of experiments on synthetic and real-world datasets, evaluating the algorithmic performance for model ensembling on imbalanced data distributions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The main technical claims of the paper are supported with mathematically rigorous statements\n+ Both the Wasserstein barycenter problem and the notion of fairness are important to the ML community\n+ The experiments act as a small-scale demonstration that reinforces the theoretical guarantees provided in the paper"
            },
            "weaknesses": {
                "value": "- The variant of WBC introduced in this paper seems to be the standard socially fair objective, which has been extensively studied for the closely related problem of clustering, often using linear programming techniques, e.g., see [MV21] below\n- The discussion on fairness is lacking, given that the main focus of the paper is a problem motivated by fairness\n- The main techniques of the paper are for improving interior point methods on the linear program corresponding to the problem, which does not necessarily introduce insightful combinatorial properties about the problem\n- The experiments on real-world datasets are not sufficiently comprehensive to convincingly demonstrate a large difference in WBC and the proposed variant (though the motivation is clear) or the practicality of the algorithm across all use cases\n- There is a small number of presentation issues, e.g., broken references, extraneous equation markers, missing line breaks, missing theorem statements, substandard proof presentations, etc.\n\n[MV21] Yury Makarychev, Ali Vakilian: Approximation Algorithms for Socially Fair Clustering. COLT 2021: 3246-3264"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Motivated by fairness applications, this paper proposed an alternative notion of the Wasserstein barycenter (WB). The new \"center\" for probability measures is called the Wasserstein Ball Center (WBC), which aims to minimize the farthest distance from the center to the input measures. The WBC is solved by a linear programming (LP) formulation via an accelerated interior point method tailored to the special low-rank structure of the constraint matrix. Some numeric advantages are demonstrated through an off-the-shelf commercial LP solver Gurobi."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Wasserstein barycenter is a timely and important research objective. Minimal distance from the farthest input measure formulation seems to be a new formulation to account for fairness. The proposed interior point method (IPM) uses an observation that the constraint matrix can be expressed as the sum of a block diagonal matrix and a low-rank matrix, which allows faster matrix inversion. The proposal IPM reduces the time complexity of a vanilla IPM from $O(N^3 m^4)$ to $O(N^2 m^3)$, where $N$ is the number of input measures and $m$ is the discretized support size. This is an acceleration of order $O(N m)$."
            },
            "weaknesses": {
                "value": "${\\bf Experiment.}$ The numerical experiment scales in Section 4 are not large. For example, Fig. 2 shows up to $N=6000$ input measures supported on small size $m=50$, and $N=10$ input measures with each support up to $m=6000$ points. Either setup has limited applicability (e.g., low-resolution images, or a very small number of input point clouds). Due to the scaling $O(N^2 m^3)$, I see no reason why the algorithm can be scaled up for a reasonable experimental setup.\n\n${\\bf Convergence.}$ The paper claims super-linear convergence for the WBC objective function value (page 8, lines 427-429). However, I cannot see why the super-linear convergence rate in the objective value plot Fig. 3. A log-scale plot of objective value v.s. iteration number should be shown. Claim quadratic convergence consistent with (Ye, Guler, Tapia, Zhang, 1993) is unclear. A quadratic rate should be proved rigorous under certain assumptions in the WBC problem.\n\nWritings of the paper seem to be rushed and many typos are present. Some examples:\n\n-- page 3, line 108: m seems to be the support of WBC and m_i support of the i-th distribution.\n\n-- page 3, line 117: Woodbury's equality (?) [reference link broken].\n\n-- page 5, line 249: primal-duel -> primal-dual\n\n-- page 5, line 258: duel -> dual"
            },
            "questions": {
                "value": "I don't understand the equivalence between the WBC (2) and linear programming (LP) formulation (4). WBC solves an optimization problem $\\min_{w, \\Omega} \\max_{t} \\min_{\\Pi^{(t)}} [...]$, where $\\Pi^{(t)}$ is the coupling between the $t$-th input measure and the Wasserstein center. However, the LP solves a problem $\\min_{w, \\Omega} \\min_{\\Pi^{(t)}} \\max_{t} [...]$. How was this exchange of the two inner optimization sub-problems achieved?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}