{
    "id": "Z9Odi09Rv9",
    "title": "Fast and Noise-Robust Diffusion Solvers for Inverse Problems: A Frequentist Approach",
    "abstract": "Diffusion models have been firmly established as principled zero-shot solvers for linear and nonlinear inverse problems, owing to their powerful image prior and ease of formulation as Bayesian posterior samplers. However, many existing solvers struggle in the noisy measurement regime, either overfitting or underfitting to the measurement constraint, resulting in poor sample quality and inconsistent performance across noise levels. Moreover, existing solvers rely on an approximation of Tweedie's formula, where an intractable $\\textit{conditional}$ score is replaced by an $\\textit{unconditional}$ score network, introducing a fundamental source of error in the resulting solution. In this work, we propose a novel frequentist's approach to diffusion-based inverse solvers, where each diffusion step can be seen as the maximum likelihood solution to a simple single-parameter conditional likelihood model, derived by an adjusted application of Tweedie's formula to the forward measurement model. We demonstrate that this perspective is not only scalable and fast, but also allows for a noise-aware maximization scheme with a likelihood-based stopping criterion that promotes the proper noise-adapted fit given knowledge of the measurement noise $\\sigma_\\mathbf{y}$. Finally, we demonstrate comparable or improved performance against a wide selection of contemporary inverse solvers across multiple datasets, tasks, and noise levels.",
    "keywords": [
        "diffusion models",
        "inverse problems",
        "maximum likelihood"
    ],
    "primary_area": "generative models",
    "TLDR": "We propose an improved solver for inverse problems by adjusting a known intractable term in Tweedie's formula and applying a noise-aware MLE framework. The resulting algorithm is fast, avoiding costly backpropagations through the score network.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Z9Odi09Rv9",
    "pdf_link": "https://openreview.net/pdf?id=Z9Odi09Rv9",
    "comments": [
        {
            "summary": {
                "value": "This paper proposed a diffusion posterior sampling method which made a correction to the score function by a Maximum Likelihood Estimation (MLE). While the MLE is usually ill-defined since in many cases the measurement operator is underdetermined, this paper proposed a remedy by running gradient descent from a good initialization and introducing an early stopping criterion to compute MLE."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed algorithm is simple and has a better performance than DPS."
            },
            "weaknesses": {
                "value": "1. The experimental results are confusing. It is possible that some of the baselines were not implemented correctly. \n- In the experiments, many of the more recent algorithms, including PSLD, ReSample, had worse performance than DPS in most settings. This is apparently different than what was reported in the literature. I also have some personal experience of implementing these algorithms, and they all demonstrated clear advantage over DPS in my setting. \n- The paper used Stable Diffusion v1.5 for some of the baselines while using a specialized ImageNet score network for the proposed algorithm. This is clearly unfair. \n- Appendix C.4 seems to have some serious concerns about ReSample. However, not enough evidence was provided to support their arguments. \n\n2. While one of the major advantages of the paper was claimed to be less computation as it did not require computing Jacobian, this did not take into account possible variants of previous algorithms like DPS, LGD-MC. There are easy ways to get rid of Jacobian in these algorithms, and there should be thorough comparison with them."
            },
            "questions": {
                "value": "Please refer to the Weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a likelihood-based algorithm for solving inverse problems using diffusion models as regularizers.\nThe approach involves solving a sequence of maximum likelihood problems with an adaptive early stopping criterion, followed by a backward diffusion step.\nIn the maximum likelihood stage, the authors apply Tweedie\u2019s formula to estimate a clean sample and then regress over its corresponding residual.\nTo prevent overfitting the noisy measurement, the authors employ an early stopping criterion based on hypothesis testing.\nThe authors validate their method through extensive experiments on linear inverse problems with both pixel-space and latent-space diffusion models across different noise levels.\nAlso, they conduct ablation studies to assess the algorithm's sensitivity to hyperparameter settings."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Extensive experimental suite\n- Discussion of the disparity between the paper and the released code in ReSample algorithm"
            },
            "weaknesses": {
                "value": "**Technical concerns**\n\n- The paper present problematic discussion of Tweedie's formula, particularly in Lemma 3.1 and Theorem 3.2.\nIn diffusion models, Tweedie\u2019s formula is valid because the transition kernels are Gaussian; see [1], Section 2.3, for a detailed proof of\n\n$$\nE(X_0 | X_t) = \\frac{1}{\\sqrt{\\alpha_t}} (x_t + (1 - \\alpha_t) \\nabla \\log p_t(X_t))\n$$\n\nHence conditioning on $x_0$ and restating Tweedie's formula is irrelevant.\n\n- Equation (17) raises concerns as $x_0$ appears on both sides of the equation.\nAs $x_0$ is inferred from $x_t$, it can never be accessed directly; only an estimate of its expected value is obtainable.\n\n- The claim in lines 303\u2013304, is problematic. The transition kernel $p_{t|0}(x_t | x_0)$ is an isotropic Gaussian for any $t$, namely\n\n$$\np_{t|0}(x_t | x_0) = N(\\sqrt{\\alpha}_t x_0, (1 - \\alpha_t) I)\n$$\n\nhence when t is very close to zero, the kernel is almost a dirac around $x_0$\n\n**Mistakes**\n\nThe paper contains several substantial errors that affect the technical clarity and accuracy of the proposed methodology:\n\n- In Equations (5) and (7), $x_t$ is omitted from the drift term of the SDE, which is necessary for a correct formulation of Diffusion Models; see Equation 11 in [2]\n- In line 106 (footnote), there is an inconsistency: the score and epsilon terms are swapped. The correct expression should be $\\epsilon_\\theta = -\\sigma_t s_\\theta$\n- Equation (9) does not align with the sampling scheme proposed in DDPM [3]. In DDPM, sampling is performed by recursively applying the bridge kernel $q(x_{t-1} | x_t, x_0)$; however, here the authors apply the forward kernel $p_{t|0}(x_t | x_0)$ which differs fundamentally.\n- In Line 5 of Algorithm 2, the gradient should be taken with respect to $\\log p_t$\n- In Line 400, the correct term should refer to the Jacobian of the score, not the gradient of the log of the score.\n\n\n**Irrelevant comparisons in experiments**\n\nThe experimental comparisons presented in the paper has inconsistences leading to potentially misleading conclusions:\n\n- In Table 1, the authors compare multiple algorithms that utilize different types of priors (pixel space models and latent space models). However, changing the prior (the regularizer) change the problem being solved. Specifically, Latent DPS, PSLD, and ReSample employ latent diffusion priors, while other algorithms use pixel-space models.\nNoteworthy, the use of latent diffusion models introduces significant nonlinearities into the inverse problem due to the auto-encoder.\n- In the experiments, it is unclear whether the results for the authors' algorithm are based on a latent-space or pixel-space diffusion model.\nThis ambiguity also applies to the results reported in Tables 3 and 4 in the appendix\n\n---\n\n.. [1] Meng, C., Song, Y., Li, W., & Ermon, S. (2021). Estimating high order gradients of the data distribution by denoising. Advances in Neural Information Processing Systems, 34, 25359-25369.\n\n.. [2] Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2020). Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456.\n\n.. [3] Ho, Jonathan, Ajay Jain, and Pieter Abbeel. \"Denoising diffusion probabilistic models.\" Advances in neural information processing systems 33 (2020): 6840-6851."
            },
            "questions": {
                "value": "I find using \"hypothesis testing\" for the early stopping criterion misleading.\nHypothesis testing traditionally assesses the statistical significance of a hypothesis based on multiple samples, whereas, in the current setup, only one sample of the residual is available at each iteration of the likelihood minimization.\nThis make hypothesis testing less irrelevant. \nAdditionally, setting $\\sigma_t$ as the critical probability $p_{critic}$ lacks sufficient justification.\nWhile $\\sigma_t$ values remain in [0, 1] in this setup and can thus be interpreted as probabilities, this approach may not extend to Variance Exploding (VE) diffusion models, where $\\sigma$ takes values beyond the interval [0, 1]."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a novel frequentist's approach to score-based diffusion inverse solvers by directly sampling with a data-conditional score. Each diffusion step can be seen as the maximum likelihood solution to a single-parameter conditional likelihood model. This model is derived through an adjusted application of Tweedie\u2019s formula to the forward measurement model. It allows for a noise-aware maximization scheme."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The approach is scalable, and allows for a noise-aware maximization scheme with a likelihood-based stopping criterion that promotes the noise-adapted fit given knowledge of the measurement noise. The authors extensively demonstrate the performance on a variety of tasks from inverse problems."
            },
            "weaknesses": {
                "value": "1. The paper makes strong assumptions about the measurement operator $A$. Theorem A.3 assumes $A$ can be decomposed into a linear projection and a surjective function, which, while covering quite many operators, is still restrictive.\n\n2. Although Diffusion Conditional Sampling (DCS) claims computational efficiency by avoiding score function gradients, it depends on a noise-aware maximization (NAM) scheme that adds its own complexity. The paper lacks a thorough analysis of NAM\u2019s computational complexity.\n\n3. The ablation study in Section 5.2 indicates that DCS\u2019s performance depends on the optimizer used in NAM, introducing a sensitive hyperparameter. This sensitivity may reduce the method's computational advantages.\n\n4. Despite recognizing limitations in Tweedie\u2019s formula, the paper relies on it to estimate $x_0$, arguing that the Gaussian assumption in the reverse process justifies this. However, this justification hinges on the accuracy of the score estimate from NAM; any inaccuracies could propagate through Tweedie\u2019s formula and affect reconstruction quality.\n\n5. While Theorem A.3 affirms the sufficiency of the score estimate, I am not sure if the paper provides theoretical guarantees on DCS\u2019s convergence or overall accuracy.\n\n6. Qualitative comparisons in Figures 10-19 reveal subtle differences between DCS and other methods, often with minimal resemblance to the ground truth. This makes it hard to assert DCS\u2019s advantage in image quality or fidelity.\n\n7. DCS\u2019s frequentist approach may limit its use in applications needing uncertainty quantification. Figures 10-19 show none of the methods fully recover the ground truth, but some existing methods enable posterior sampling to quantify uncertainty - a capability that DCS seems to lack.\n\n8. It would have been beneficial to elaborate on a comparison with Y. Sun, Z. Wu, Y. Chen, B. T. Feng, and K. L. Bouman. Provable probabilistic imaging using score-based generative priors."
            },
            "questions": {
                "value": "Could the authors clarify Theorem A.3 and its proof? for example, concerning guarantees on the DCS's convergence.\n\nCould the authors comment on point 4. under perceived Weaknesses?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates solving inverse problems with score-based diffusion models. Particularly, they address the problem that comes with noisy data and the inconsistency between generated image and noisy measurement common algorithms encounter. To this end, the authors propose a novel algorithm that overcomes this problem. The algorithm is theoretically analyzed and extensively investigated numerically on various tasks and noise-levels."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "I really like this paper. The idea is to the best of my knowledge novel and solves a very important problem in the field. The clarity of the argument is excellent as is the presentation. The numerical results are very comprehensive giving a transparent assessment of the potential of the method for common computer vision tasks. I also really like that they test their method on high-noise levels showing that it may indeed have practical relevance, even for severely ill-posed inverse problems (which they haven't tested on)."
            },
            "weaknesses": {
                "value": "In my view there are no important weaknesses.\n\nThe writing is at times a little bit off:\n- Introduction: \"Generally, A is assumed to be non-invertible, meaning that any solution x satisfying A(x) = y ...\". This statement is false. Many inverse problems are invertible: Gaussian and motion deblurring, X-ray tomography. The former is even considered in this paper. This then has consequences for the argument that follows.\n- line 50: they refer to \"smoothness\" as a function, similar to total variation. I guess they mean the squared H1-seminorm ||\\grad u||_2^2?\n- line 68: \"While already effective, this approach suffers from a unique problem where the explicit form of the consistency error ||A(x) \u2212 y|| only exists for x = x0 (Chung et al., 2022a).\" The error of course exists everywhere but cannot be easily or readily evaluated at the correct location.\n- line 214: \"Of course, this strategy is only correct when two conditions simultaneously hold true: (1) the measurement operator A is linear, and (2) the inverse problem is noiseless, i.e, \u03b7 is identically 0.\" I don't follow their argument. I would be happy to say that (1) and (2) are sufficient for this to be correct (but may not be necessary in general).\n- line 260: \"Inverse problem solvers (Section 2.2) face a fundamental ...\" This of course is only true for score-based diffusion method. Most algorithms for inverse problems do not have this problem."
            },
            "questions": {
                "value": "I am fully sold on this paper. Of course the authors may want to reply to the comments as described above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}