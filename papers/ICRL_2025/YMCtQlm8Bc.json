{
    "id": "YMCtQlm8Bc",
    "title": "Critical Influence of Overparameterization on Sharpness-aware Minimization",
    "abstract": "Training overparameterized neural networks often yields solutions with varying generalization capabilities, even when achieving similar training losses. Recent evidence indicates a strong correlation between the sharpness of a minimum and its generalization error, leading to increased interest in optimization methods that explicitly seek flatter minima for improved generalization. Despite its contemporary relevance to overparameterization, however, this sharpness-aware minimization (SAM) strategy has not been studied much yet as to exactly how it is affected by overparameterization. In this work, we analyze SAM under varying degrees of overparameterization, presenting both empirical and theoretical findings that reveal its critical influence on SAM's effectiveness. First, we conduct extensive numerical experiments across diverse domains and show that SAM consistently improves with overparameterization. Next, we attribute this phenomenon to the interplay between the enlarged solution space and increased implicit bias resulting from overparameterization. Furthermore, we show that this effect is particularly pronounced in practical settings involving label noise and sparsity, and yet, sufficient regularization is necessary. Last but not least, we provide other theoretical insights into how overparameterization helps SAM achieve minima with more uniform Hessian moments compared to SGD, and much faster convergence at a linear rate.",
    "keywords": [
        "sharpness-aware minimization",
        "overparameterization"
    ],
    "primary_area": "optimization",
    "TLDR": "We uncover both empirical and theoretical results that indicate a critical influence of overparameterization on SAM.",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=YMCtQlm8Bc",
    "pdf_link": "https://openreview.net/pdf?id=YMCtQlm8Bc",
    "comments": [
        {
            "title": {
                "value": "Response to 18YL (2/2)"
            },
            "comment": {
                "value": "**Q: Adequate theoretical support for Section 3?**\n\n> Could the authors provide adequate theoretical support for the main findings? e.g., an analysis on diagonal linear networks.\n\nWe sincerely appreciate the reviewer for providing constructive feedback. However, we find this task, even with simplified models, unmanageable. Specifically, it is not possible to increase the number of parameters in diagonal linear networks since they are entirely determined by the dimensionality of the data (i.e., input and output dimensions). Also, while we could attempt to make it somewhat possible for a similar variant of deep linear networks [5], it won\u2019t affect the model capacity to fit more complex data and overfit, which is a crucial property when analyzing the generalization characteristics.\n\nNonetheless, we can develop connections between Sections 3 and 6 in the following sense:\n- Section 6.1: As explained in **Section 6.1** above, connecting the observation in Section 4.2 and  Equation (6) suggests larger models can lead to flatter minima, which has been observed to correlate well with good generalization [6-9].\n- Section 6.2: We can also connect this with Equation (6), since unlike the under-parameterized scenario where SAM needs diminishing step-size [10], SAM can use larger constant step-sizes when the model is overparameterized, which also tightens the upper bound on the maximum Hessian eigenvalue and lead to flatter minima.\n\nAlso, we remind that Section 4 directly supports the empirical phenomenon in Section 3. If there is any concern with the explanations in Section 4, please point us to it.\n\n\n&nbsp;\n\n**Closing remarks**\n\nWe sincerely thank the reviewer for taking the time to review our work. While we hope that our response has addressed the reviewer\u2019s concerns reasonably well should there remain any remaining concerns, please let us know so we can fix them further during the discussion period. Otherwise, given the reviewer\u2019s positive assessment on soundness, presentation, and contribution (2/3/2), we find the overall rating score of 3 is quite discouraging, and hence, we sincerely hope that the reviewer could give it a reconsideration.\n\n&nbsp;\n\n**References** \\\n[1] Ma et al. The power of interpolation: Understanding the effectiveness of SGD in modern over-parametrized learning. ICML, 2018. \\\n[2] Liu and Belkin. Accelerating SGD with momentum for over-parameterized learning. ICLR, 2020. \\\n[3] Meng et al. Fast and Furious Convergence: Stochastic Second-Order Methods under Interpolation. AISTATS, 2020. \\\n[4] Vaswani et al. Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates. NeurIPS, 2019. \\\n[5] Ji and Telgarsky, Gradient descent aligns the layers of deep linear networks, ICLR, 2019. \\\n[6] Keskar et al. On large-batch training for deep learning: Generalization gap and sharp minima. ICLR, 2017. \\\n[7] Chaudhari et al. Entropy-SGD: Biasing gradient descent into wide valleys. ICLR, 2017. \\\n[8] Jiang et al. Fantastic generalization measures and where to find them. ICLR, 2020. \\\n[9] Neyshabur et al. Exploring generalization in deep learning. NeurIPS, 2017. \\\n[10] Andriushchenko and Flammarion. Towards understanding sharpness-aware minimization. ICML, 2022."
            }
        },
        {
            "title": {
                "value": "Response to 18YL (1/2)"
            },
            "comment": {
                "value": "We appreciate the reviewer for taking the time to review our work. While we respond to the reviewer\u2019s specific comments as below, we would be keen to engage in any further discussion.\n\n&nbsp;\n\n**Theoretical support**\n\n> My primary concern is that the theoretical analysis can not support the main claim/findings (e.g., Figure 1). \n\nWe clarify that the purpose of Section 6 is to provide diverse theoretical benefits that overparameterization has on SAM, rather than to support the observations in Section 3 (e.g., Figure 1) as already mentioned in Section 6 (see Line 359). We provide our accounts of these findings in Section 4, which might not be entirely \u201ctheoretical\u201d, yet plausible and reasonable. Please let us know if there is any issue with it.\n\nOn a slightly different note, we believe that Section 6 itself, independently of its connection to Section 3, is noteworthy in that the results have not been explicitly developed in the literature, rendering its unique contributions. We address specific points in detail below.\n\n&nbsp;\n\n**Section 6.1**\n\n> The linear stability analysis in Section 6.1 can not demonstrate how overparameterization affects SAM (i.e., that \u201cgreater overparameterization leads SAM to find flatter minima\u201d).\n\nWe respectfully disagree with this assertion. Equation (6) shows that a larger $\\rho$ provides a tighter upper bound on the maximum Hessian eigenvalue, which leads SAM to find flatter minima. We emphasize that this property not only holds when overparameterization (i.e., interpolation) is achieved (see Line 392), but in conjunction with the findings in Section 4.2 that larger models tend to have higher optimal $\\rho$ values, suggesting that greater overparameterization can guide SAM toward flatter minima.\n\n&nbsp;\n\n**Section 6.2**\n\n> (i) it lacks relevance to generalization, the core focus of the paper\n\nAs clarified at the beginning, the purpose of Section 6.2 is not to address generalization properties, but to present the influence of overparameterization on the convergence of SAM. We believe these results are non-trivial in that for the first time, SAM is shown explicitly to converge at a linear convergence rate under overparameterization, which is much faster than the previously known sublinear rate, implying one critical influence of overparameterization on SAM (i.e., improved convergence).\n\n&nbsp;\n\n> (ii) it does not clarify how the degree of overparameterization influences the convergence speed within the interpolation regime\n\nOur finding indicates that with the existence of overparameterization, the convergence speed improves from sublinear to linear rate. To the best of our knowledge, we are unaware of any theories or analyses that capture varying degrees of overparameterization applicable to our study. We conjecture that formulating such a notion in the context of interpolation is complex, and our understanding as a society is still limited. Nonetheless, we would be keen to find any such work, so please let us know if the reviewer could suggest any. We are eager to pursue further study in this direction.\n\n&nbsp;\n\n> (iii) The proof for SAM appears to be a minor extension from SGD\n\nWe respectfully disagree with this assessment, as nontrivial efforts were made in the proof. Specifically, for Theorem 6.6, Lemma I.1 and I.2 were developed to characterize the similarity between SAM gradient and SGD gradient without relying on any special assumptions that might be infeasible in practice. This precise characterization, to the best of our knowledge, has not been presented in any previous works. We remark that it is a standard procedure to build upon existing results in optimization, and indeed, many prior works on extending the result for SGD to other optimizers [2-3] have been well-received by the community. In this regard, we believe that our efforts were far from \u201cminor\u201d, and we are optimistic that this proof will serve as a valuable contribution to the subject of study.\n\n&nbsp;\n\n**SAM without normalization**\n\n> The theoretical analysis in Section 6.1/6.2 focus on a variant of SAM (SAM without normalization) for simplification, rather than the original SAM. While not a major issue, the formulation should be clearly presented in Section 6 to prevent potential reader confusion.\n\nWe thank the reviewer for the suggestion and relate to the reviewer\u2019s concern. While we have mentioned this already in Line 376 and Line 478, we will make this more clear by providing the explicit formulation of the un-normalized SAM in the main text."
            }
        },
        {
            "summary": {
                "value": "The authors perform experiments to \nmeasure the effect of overparameterization in SAM for a diverse set of tasks (Section 3). The goal is to observe how overparameterization\naffects SAM under various conditions, e.g.,  label noise, sparsity, and regularization. \nFurthermore, they prove that stable minima of SAM are flatter and have more uniform Hessian moments (if compared with SGD), and stochastic SAM can also converge at a linear. The overall contribution is that they empirically and theoretically proved that overparameterization critically affects SAM."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- interesting and well-motivated problem\n - very well written"
            },
            "weaknesses": {
                "value": "- discussion on higher moments of Hessian is missing"
            },
            "questions": {
                "value": "This is an interesting paper. I have a question: how does the convergence of higher-order moments of Hessian in your result compare with the other approaches in the literature, e.g., [1]? Can you provide a literature review on the previous works considering higher-order moments of Hessian to define flatness?\n\n[1] Tahmasebi, Behrooz, et al. \"A Universal Class of Sharpness-Aware Minimization Algorithms.\" Forty-first International Conference on Machine Learning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the critical influence of overparameterization on SAM."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The experiments in this paper are insightful, showing that overparameterization can increase the performance gap between SAM and SGD, while also highlighting the role of factors such as label noise, sparsity, weight decay, and early stopping in this phenomenon."
            },
            "weaknesses": {
                "value": "- It is widely recognized that overparameterization improves the generalization performance of SGD (e.g., ResNet-152 outperforms ResNet-18 on Cifar10 using SGD). And it is unsurprising that overparameterization can also improve the generalization performance of SAM. However, the non-trivial aspect is that overparameterization increases *the gap between SAM and SGD*, as shown in Figure 1. To avoid trivialization, reconsidering the title and abstract might be beneficial.\n\n- My primary concern is that the theoretical analysis can not support the main claim/findings (e.g., Figure 1). \n  - The linear stability analysis in Section 6.1 can not demonstrate how overparameterization affects SAM (i.e., that \u201cgreater overparameterization leads SAM to find flatter minima\u201d).\n  - In the convergence analysis in Section 6.2: (i) it lacks relevance to generalization, the core focus of the paper; (ii) it does not clarify how the degree of overparameterization influences the convergence speed within the interpolation regime; (iii) in the interpolation regime, the exponential convergence result also holds for SGD (Bassily et al., 2018). The proof for SAM appears to be a minor extension from SGD, which treats SAM as SGD + small permutation.\n  - The theoretical analysis in Section 6.1/6.2 focus on a variant of SAM (SAM without normalization) for simplification, rather than the original SAM. While not a major issue, the formulation should be clearly presented in Section 6 to prevent potential reader confusion."
            },
            "questions": {
                "value": "Could the authors provide adequate theoretical support for the main findings? e.g., an analysis on diagonal linear networks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper claimed that SAM benefits from the over-parameterization. They find that SAM could find different (e.g. simpler) solutions than SGD in the over-parameterization regime. In addition, they find that SAM is robust to label noise and sparse parameterization. They also include some theoretical analyses, thought these analyses are irrelevant to their main findings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The authors picked up an interesting perspective to investigate the mechanism of SAM. This research topic is significant.\n2. The experiments are extensive, across various datasets and architectures.\n3. The writing structure is clear, and the paper is easy to follow."
            },
            "weaknesses": {
                "value": "Firstly, for the empirical findings,\n\n1. It is unclear that whether over-parameterization consistently helps SAM find more generalizable solutions as the model parameters scale up. As shown in Fig.1 and Fig. 7 (I think the Fig. 7 is clearer), for MLP on MNIST, ResNet-18 on CIFAR-10, LSTM on SST2 and CNN on Atari, when model get more over-parameterized, the generalization improvement does not always increase; sometimes, a noticeable decline is observed. Moreover, as noted by the authors, when there is no sufficient regularization, SAM might not benefit from the over-parameterization (as shown in Fig. 5). In addition, I notice that the authors conducted experiments for ViT on CIFAR-10 in Fig.5 but not in Fig. 1. I wish the authors could include the results for ViT on CIFAR-10 in Fig.1.\n2. Increasing depth of models yields different conclusions than increasing width. In Fig. 1, authors increase the number of neurons in each layer, leading to a seemingly consistent conclusion that over-parameterization helps SAM find generalizable solutions. However, in Fig. 17, when increasing the number of layers, SAM might not benefit from the over-parameterization (ResNet on CIFAR-10). Clearly, the increase of both depth and width demonstrates the over-parameterization, however, the conclusion differs. \n3. It is not surprising that the marginal improvement of SAM over SGD increases with higher label noise. In Fig. 12, in the over-parameterization regime, the performance of SGD drops significantly with higher label noise rate; however, SAM remains good generalization ability. Indeed, it is well known that SAM is robust to label noise, while SGD not. Thus, it is not surprising for us to observe such phenomenon. Also, it seems that the authors over-claimed that over-parameterization secures the robustness of SAM to label noise. Indeed, the observed increase of margin improvement of SAM over SGD with larger model parameters is primarily due to the pronounced decline in SGD's performance as the model parameters scale up under high label noise conditions.\n4. The conclusion on the effect of sparsity contradicts with the main argument that SAM benefits from the over-parameterization. In Fig. 5, it is observed that the generalization improvement tends to increase as the model becomes sparser. However, increased sparsity indicates that the model is becoming more under-parameterized. The authors are suggested to explain such contradiction.\n5. In Fig. 2 and 3, the authors compare GD and SAM, however, a comparison between SGD and SAM is more preferred.\n6. In section 4.2, \u201cimplicit bias of SAM increase with over-parameterization\u201d is over-stated. In this section, the authors demonstrated that we could use larger perturbation radius when scaling up the model parameters, however, this doesn\u2019t indicate that a larger scale of model parameters induces stronger implicit bias towards flatter minima. \n\nIn short, SAM doesn\u2019t always benefit from the over-parameterization in practice. A more thorough justification is needed to clarify whether over-parameterization is advantageous for SAM and under what specific conditions it may provide benefits. Indeed, it is quite expectable that SAM would benefit from over-parameterization sometimes. Because in the under-parameterization regime, the model capacity is low (say only one solution in the solution space), thus both SGD and SAM achieve similar solutions. Once scaling up the parameters, the solution space is getting larger, thereby SAM could differ from SGD and find more generalizable solutions. However, SAM might not always from the over-parameterization. \n\nSecond, for the theoretical analysis,\n\n1. As noted by the authors, the theoretical analysis cannot support the findings in Section 3 and 4, and thus the significance of the theoretical analysis is limited. The section 6 is more like a collection of possible theoretical analyses that could be done for SAM. \n2. Indeed, there is no clear theoretical support for the main discovery of this paper, as noted by the authors in the limitations part in Section 7. This is another critical issue of this paper.\n\nOverall, the major issue of this paper is the novelty. The empirical findings are largely anticipated, and the theoretical analysis closely follows prior works without introducing any new proof techniques.  Also, some statements are clearly over-claimed. The contribution of this paper is marginal, and thus I lean to rejection."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work attempts to disclose the critical influence of overparamatrisation on  Sharpness-Aware Minimization  (SAM) from experimental and theoretical perspectives. It starts with an extensive evaluation to display a  highly consistent view showing the generalization benefit of SAM increases with overparametrisation. Without such overparametrisation  SAM may not work. This leads authors to consider the benefit in terms of more solutions and implicit bias. Finally, they developed theoretical advantages and advantages of overparametrization for SAM on linear stability, convergence and generalization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Nice empirical and theoretical work on overparametrisation on sharpness-aware minimization. It is interesting and useful for ICLR communauty"
            },
            "weaknesses": {
                "value": "See questions below."
            },
            "questions": {
                "value": "(1) Please clarify the large curve surrounding the \"real\" curve in Fig . 1 (and possibly in other Figs). Specify if the shaded regions represent confidence intervals, standard deviations or soe other measures of uncertainty. \n(2) How can SAM improvements be compared to SGD improvements via overparametrisation? Hoa SAM and SGD performance changes with increasing model size (may be you can add provide (in Annex) plots showing relative improvement over a baseline for both optimizers across different nodel sizes)\n(3) Min Max optimization has some limitations (High complexity; Oscillatory behavior, sensitivity to initialization, etc.), how these problems are addressed here ? Can equation (3) (or some of its properties) help mitigate these issues ? Did you observed these specific issues in your experiments and if so, how have you addressed them ?\n(4) Can we use Elliptic Loss Regularisation for SAM instead of Min Max? Please comment on whether Elliptic Loss Regularization could be a viable alternative to the min-max formulation in SAM, and what potential advantages or disadvantages it might have in this context.\n(5) Is there a sort of \"optimal\" number of parameters for which the computation price is acceptable ?Did you  observed any diminishing returns in performance improvement as model size increased. Please provide guidance on balancing computational cost with performance gains when using SAM in practice"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}