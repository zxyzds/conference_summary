{
    "id": "1DEEVAl5QX",
    "title": "Mini-batch Submodular Maximization",
    "abstract": "We present the first *mini-batch* algorithm for maximizing a non-negative monotone *decomposable* submodular function, $F=\\sum_{i=1}^N f^i$, under a set of constraints. \nWe consider two sampling approaches: uniform and weighted. We show that mini-batch with weighted sampling improves over the state of the art sparsifier based approach both in theory and in practice. Surprisingly, we experimentally observe that uniform sampling achieves superior results to weighted sampling. However, it is *impossible* to explain this using worst-case analysis. Our main contribution is using *smoothed analysis* to provide a theoretical foundation for our experimental results. We show that, under *very mild* assumptions, uniform sampling is superior for both the mini-batch and the sparsifier approaches. We empirically verify that these assumptions hold for our datasets. Uniform sampling is simple to implement and has complexity independent of $N$, making it the perfect candidate to tackle massive real-world datasets.",
    "keywords": [
        "smoothed analysis",
        "submodular maximization"
    ],
    "primary_area": "optimization",
    "TLDR": "Present the first mini-batch algorithm for submodular maximization; use smoothed analysis to justify performance",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1DEEVAl5QX",
    "pdf_link": "https://openreview.net/pdf?id=1DEEVAl5QX",
    "comments": [
        {
            "comment": {
                "value": "Another point we would like to address:\n\"It is unclear if ICLR is an appropriate venue for this work. The non-exhaustive list of topics in the Call for Papers includes \"optimization\", but submodular maximization in its raw form seems one hop away from the target areas of ICLR (deep learning)\"\n\nSubmodular optimization papers are often published in ICLR / Neurips / ICML. While it is true that usually there are only a few submissions dealing with the raw form of submodular optimization, they are accepted quite positively. See for example this submission for ICLR 2025 which received some very positive reviews - https://openreview.net/forum?id=EPHsIa0Ytg"
            }
        },
        {
            "comment": {
                "value": "Thank you for the review. We address your comments below.\n\nWeaknesses:\n- \"The lunch menu optimization example, while a clear illustration, does not really motivate the problem from a practioner's perspective\"\n\nAgreed, we chose this for simplicity. However, utility maximization is an extremely\u00a0natural problem. Other examples for utility maximization include: adding medical services to a healthcare package as to maximize the welfare of all patients, adding features to a website to maximize user engagement, etc...\n\n- \"There are no p-system experiments\"\n\nIndeed, we couldn't find a real-world dataset for this problem. Previous papers seems to either be completely theoretical (no experiments), or run experiments just under a cardinality constraint.\n\nQuestions:\n\nQ1) It is quite natural in welfare maximizations (e.g., many people with different preferences). Another example is finding a representative set of images (e.g., thumbnails for a video). Here N can be very large (the number of frames in the video), clearly there is plenty of redundancy, so our approach is very natural here.\n\nQ2) It is defined just above Model 1. It is the set $\\{f^i(e)\\}_{i\\in [N]}$ and in our models we assume that every $f^i(e)$ (the value of the i-th func on e) is a random variable, not the set $A_e$.\n\nQ3) We roughly followed the paper of Rafiey and Yoshida which introduced\u00a0this set. They simply say that they select a set of \"popular pickup locations in the dataset\". We used k-means to pick \"popular locations\"."
            }
        },
        {
            "comment": {
                "value": "Thank you for your review. \n\nAbout the figure, basically all algorithms achieve almost the same quality when we sample enough elements.\nWe will move Section 4 before Section 3 if accepted."
            }
        },
        {
            "title": {
                "value": "Our main contribution is the uniform alg + smoothed analysis."
            },
            "comment": {
                "value": "Thank you for your review.\u00a0\n\nWe would like to emphasize that our main contribution is the introduction of the *uniform* sampling algorithm, observing that it outperforms other approaches empirically, and using smoothed analysis to bridge the gap between theory (no worst case analysis possible) and practice. We believe that this algorithm can be used as the first line of attack for many real-world massive datasets. The improved weighted sampling is \"nice to have\" and lays the groundwork\u00a0for the smoothed analysis of the uniform sampling algorithm, but this is not our main contribution.\n\nWe address your questions below.\u00a0\n\nQ1) An upper bound is not sufficient. This is because our proofs require a *multiplicative* error bound for the minibatch approach to work. Consider the following example: all functions except one are always zero ($f^i \\equiv 0, i\\neq j$), and one function, $f^j$, is upper bounded by 1. Clearly both the minibatch and the sparsifier algorithms can't optimize the sum as they will keep sampling functions that are always 0. The above example is unlikely to appear in real world applications, but it illustrates that worst-case analysis is simply not the right tool here. This is why we use smoothed analysis to explain the superior performance of uniform sampling in practice.\n\nQ2) Yes, specifically Model 2. The assumptions of Model 2 are *extremely* mild and we verify\u00a0empirically that they hold for *all* of our datasets. We would like to emphasize that we only introduced smoothed analysis in this revision of the paper, while we used the same datasets in previous revisions. That is, we did not simply pick datasets where our models apply (and indeed Model 1 does not apply to all datasets)."
            }
        },
        {
            "summary": {
                "value": "This paper considers maximization of decomposable monotone submodular functions over a ground set of size $n$, meaning that the objective function $f$ is a sum of $N$ monotone submodular functions $f_1,...,f_N$. If $N$ is large, then evaluations of $f$ may be computationally demanding. Previous work on the topic (Rafiey & Yoshida, 2022; Kenneth & Krauthgamer 2023) proposes constructing a random sparsified version of $f$ that is a weighted sum of some subset of the functions, and is within a multiplicative $\\epsilon$ factor approximation on all sets. A sparsifier such as those mentioned could be constructed as a preprocessing step for an algorithm, and then the algorithm would be run using the sparsifier in place of the original function. The state of the art is that of Kenneth & Krauthgamer, where a sparsifier of $O(k^2n\\epsilon^{-2})$ functions is constructed using $O(Nn)$ oracle calls. The sparsifier is constructed by iterating over the functions, computing a probability $p_i$ for each function $f_i$ to be included, and then sampling that function with probability $p_i$ (which takes a total of $O(Nn)$ queries). Then querying the sparsifier takes $O(k^2n\\epsilon^{-2})$ function evaluations, compared to $O(N)$ function evaluations to query the original $f$. If $N$ is relatively large, the sparsifier is more efficient.\n\nInstead of computing a sparsifier as a preprocessing step for an algorithm, this paper proposes a \"mini-batch method\" (which have been used in other areas of ML) for this problem (Algorithm 3). That is, a new sparsifier is sampled every iteration of the greedy algorithm. The approach in this paper uses the same sampling probabilities $p_i$ as Kenneth & Krauthgamer, and therefore still needs the $O(Nn)$ queries as a preprocessing step to compute the $p_i$. In order to prove some of the results in their paper, they make additional assumptions on the problem setting (Models 1 and 2). Several analyses are done on the number of function queries needed for their algorithm. Finally, they include an experimental comparison of their algorithm and related works."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Exploring submodular optimization algorithms that do not view the function $f$ as simply a black box is an interesting research direction that I think deserves attention.\n- They explained their results clearly and the paper was easy to understand."
            },
            "weaknesses": {
                "value": "- It seems a lot of the difficulty of these sparsification approaches is because the sampling of the $f_i$ is non-uniform, but it is still unclear to me that this is so much better than uniform sampling. According to this paper, uniform sampling does better in practice, and requires no preprocessing to compute the $p_i$ since they would be uniform. It is also stated that no theoretical bound can be gotten for uniform sampling. But if we assume that all the $f_i$ are bounded by some value $R$, why can't concentration inequalities be used to get a theoretical guarantee for the uniform approach?\n- Some of the results are dependent on assuming Models 1 or 2 (see Table 1), but it isn't clear to me that these models are realistic for applications of the problem.\n- Improvements over Kenneth and Krauthgamer mainly include the curvature of the function in the bound on the number of function queries, so the bounds are instance dependent.\n- The bounded curvature results (which don't depend on Models 1 and 2) don't use ideas that are that novel compared to related work. It seems the biggest difference from Kenneth and Krauthgamer is computing the sparsifier at each round of the greedy algorithm, and only relatively minor changes are needed to the argument of Kenneth and Krauthgamer."
            },
            "questions": {
                "value": "* If the $f_i$ are all bounded by a value $R$, could theoretical guarantees be gotten for uniform sampling?\n* Do you expect Models 1 and 2 would hold widely in applications of decomposable functions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the problem of maximizing a non-negative, monotone, decomposable submodular function under the cardinality constraint and $p$-system constraint. It introduces the first mini-batch algorithm with weighted sampling for this problem, demonstrating that it outperforms the sparsifier-based approach both theoretically and empirically. Additionally, the authors observe that, in experiments, uniform sampling outperforms weighted sampling. To explain this outcome, they define two smoothing models. The first model provides theoretical guarantees for both the mini-batch and sparsifier algorithms on some datasets, while the second model applies only to the mini-batch algorithm but is effective across all datasets tested."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Overall, the paper is well-structured and easy to understand. The definitions and explanations are clear, and related work is discussed in sufficient detail. \n\nThe discussion on uniform and weighted sampling, along with the smoothing model, helps bridge the gap between theoretical results and the empirical performance of the algorithms. It provides insights into why an algorithm without a worst-case guarantee can still perform well in experiments."
            },
            "weaknesses": {
                "value": "The algorithm is simple, and the analysis is quite straightforward. The technical contribution is limited. \n\nWith 12 indistinguishable lines in Figure 1, it is hard to see which algorithm with $\\beta=10^{-2}$ achieves the best performance."
            },
            "questions": {
                "value": "It might be better to put Section 4 before Section 3 to ensure the continuity of the analysis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies a sampling-based algorithm for faster non-negative monotone *decomposable* submodular maximization subject to\ncardinality or $p$-system constraints. In particular, it builds on work of\n[Kenneth-Krauthgamer, ICALP 2024] (please update reference in paper), which sparsifies\nand reweights the set of functions $f^{(i)}(S)$ for the input function $F(S) = \\sum_{i=1}^N f^{(i)}(S)$.\nThe goal of this paper is to eliminate the dependence on $N$, which the authors do under mild assumptions\nvia *smoothed analysis*. They also show that this is not possible in the general case with a simple pathological example.\nIn short, the main idea is to sample a subset of $f^{(i)}(S)$ functions at each step to form a\n\"mini-batch\" for approximating the full $F(S)$. The algorithm then greedily\nselect the next element based on the sampled funciton (which changes in each iteration), not $F$ itself.\n\nFurther, under the mild realistic assumptions, they prove why uniform sampling is a competitive approach,\nwhich helps explain initially surprising experimental observations.\nLastly, this work provides a clean set of experiments comparing their mini-batch sampling-based methods to\na full lazy greedy algorithm and the sparsification idea in [Kenneth-Krauthgamer, ICALP 2024]."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Uses smoothed analysis to more accurately study realistic inputs\n- Table 1 cleanyl describes the results, including a comparison with [Kenneth-Krauthgamer, ICALP 2024]\n- Draws connections to the lazier-than-lazy greedy algorithm of [Mirzasoleiman et al., AAAI 2015]\n  and explains how the two ideas can be combined to reduce query complexity by a factor of $\\Theta(k)$\n- Good comprehensive set of experiments for cardinality constraints, though the\n  values of $k \\le 20$ are quite small. It would be nicer to increase $k$ to see\n  how fast the different algorithms converge (relatively) to lazy greedy"
            },
            "weaknesses": {
                "value": "- The lunch menu optimization example, while a clear illustration, does not\n  really motivate the problem from a practioner's perspective\n- There are no $p$-system experiments\n- It is unclear if ICLR is an appropriate venue for this work. The\n  non-exhaustive list of topics in the Call for Papers includes \"optimization\",\n  but submodular maximization in its raw form seems one hop away from the\n  target areas of ICLR (deep learning)"
            },
            "questions": {
                "value": "- In the introduction, you claim that \"in many of the above applications, $N$\n  (the number of underlying submodular functions) is extremely large, making the\n  evaluation of $F$ prohibitively slow.\" Are there realistic examples where $N\n  \\gg 1000$? It's not clear to me how often we really encounter $N$ *distinct*\n  personalized submodular functions.\n- What exactly is the quantity $A_e$ when you first introduce it on page 3?\n  This should be made more clear. Initially, I thought it was a vector of all\n  marginal values, but then in model 1 you say it's a random variable.\n- For the Uber pickups experiment, why do you use Llyod's algorithm to find\n  centers instead of a data-indepedndent grid?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}