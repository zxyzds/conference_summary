{
    "id": "XA9A8mkFqa",
    "title": "CSP: An Efficient Baseline for Learning on Large-Scale Structured Data",
    "abstract": "Last decade has seen the emergence of numerous methods for learning on graphs, particularly Graph Neural Networks (GNNs). These methods, however, are often not directly applicable to more complex structures like bipartite graphs (equivalent to hypergraphs), which represent interactions among two entity types (e.g., a user liking a movie). This paper proposes Convolutional Signal Propagation (CSP), a non-parametric simple and scalable method that natively operates on bipartite graphs (hypergraphs) and can be implemented with just a few lines of code. After defining CSP, we demonstrate its relationship with well-established methods like label propagation, Naive Bayes, and Hypergraph Convolutional Networks. We evaluate CSP against several reference methods on real-world datasets from multiple domains, focusing on retrieval and classification tasks. Our results show that CSP offers competitive performance while maintaining low computational complexity, making it an ideal first choice as a baseline for hypergraph node classification and retrieval. Moreover, despite operating on hypergraphs, CSP achieves good results in tasks typically not associated with hypergraphs, such as natural language processing.",
    "keywords": [
        "Hypergraph representation learning",
        "Hypergraph convolution",
        "Label propagation",
        "Model complexity",
        "Naive Bayes"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "This paper proposes Convolutional Signal Propagation (CSP), a non-parametric simple and scalable method that natively operates on hypergraphs and can be implemented with just a few lines of code.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=XA9A8mkFqa",
    "pdf_link": "https://openreview.net/pdf?id=XA9A8mkFqa",
    "comments": [
        {
            "comment": {
                "value": "Thank you for your review, summary and suggestions. We would like to add a few comments regarding the points that you mention:\n\nTL;DR: CSP is simpler than Naive Bayes. CSP outperforms Naive Bayes on the movie-RA and movie-TA datasets. In more detail:\n\n## Comparison with the Naive Bayes classifier\nCSP is actually even simpler than the Naive Bayes classifier, both conceptually as well as when measured by computational complexity and cost (see Table 4). As you note, CSP does not consistently outperform Naive Bayes in classification tasks, except for the movie-RA and movie-TA datasets. However, the situation reverses in retrieval tasks, where CSP outperforms Naive Bayes in 5 out of 7 setups (see Table 3). This difference between scenarios is expected as Naive Bayes is utilizing prior information in contrast to CSP, which is discussed in Section 4.5 (as well as lines 472-475 in the experimental section). Naive Bayes requires both positive and negative class fitting, adding to its complexity, while CSP only propagates positive signals \u2014 making it computationally lighter while still performing comparably in many tasks.\n\n## Effectiveness on Large Datasets \nWe would like to note that CSP actually achieves better results than Naive Bayes on large datasets like movies-RA and movies-TA for both classification and retrieval tasks, sometimes by a significant margin (see Tables 2 and 3). While Naive Bayes performs better on the DBLP dataset (where it even beats HGCN in the retrieval setting), CSP\u2019s primary advantage is its efficiency across datasets \u2014 offering competitive performance while requiring fewer computations (see Table 4). Combined with the relative simplicity of CSP even when compared to Naive Bayes, it is still an attractive baseline algorithm.\n\n## Exclusion of Simpler GNNs\nGNNs do not naturally operate on hypergraphs, so converting a hypergraph to a unipartite graph introduces complexity, ambiguity and an additional processing step, which could complicate comparisons. Given that HGCN directly applies to hypergraphs, it provides a more relevant benchmark. We believe that HGCN's structure captures the benefits of simpler GNNs applied to hypergraphs, which better aligns with CSP\u2019s design focus. At the same time, we welcome suggestions for simple models that operate on hypergraphs and could serve as additional comparisons.\n\n## Omission of CSP Variants in Experiments\nOur main contribution and message is the comparative performance of CSP despite its radical simplicity, achieving 'good enough' performance as a low-cost baseline. While we briefly discuss extensions, our experiments focus on demonstrating CSP\u2019s core value. We would like to study and experimentally verify the extensions more in future work, however, in this paper, we focused on our primary message: that CSP is a highly efficient, simple alternative that performs comparably with more complex methods.\n\nIn summary, we thank you for your review, and would like to highlight a few key facts:\n\n- CSP is the only optimization-free method compared\n- CSP is the only parameter-free method compared\n- CSP doesn't aim to achieve the best performance\n- CSP is almost always competitive in performance\n- CSP offers the best performance per unit of computational time\n  \nWe are aware that these points could be explained more clearly in the paper itself and will try to address that. We're looking forward to continuing the discussion."
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors introduce Convolutional Signal Propagation (CSP), which is a simple and efficient algorithm for learning on large-scale structured data represented as hypergraphs or bipartite graphs. CSP is formulated as an iterative averaging process that propagates signals over hypergraph structure. Authors also analyse the proposed CSP, establishing connections between CSP and label propagation, niave bayes, and one-layer HNN. Experimental results for the tasks of node classification, and retrieval tasks are produced which show that CSP maintains very low computational complexity while achieving decent success. Authors present CSP as a important first choice baseline for learning on hypergraphs and large-scale structured data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The CSP algorithm is simple and can be implemented with just a few lines of code, making it computationally efficient. \n2. The paper evaluates CSP on a diverse set of real world datasets related to various domains, which show the broad applicability of the proposed method.\n3. Despite simplicity, the empirical results show that CSP achieves good performance compared to more complex baseline methods on tasks node classification, and retrieval. \n4. Authors also discussed several possible extensions for the basic CSP method."
            },
            "weaknesses": {
                "value": "1. The proposed CSP is simple, but it does not show any advantages over a simpler Naive-bayes classifier. For node classification task, Naive Bayes is better suited as a baseline than CSP.\n2. A simpler baseline is more beneficial on large scale datasets, because running a complex algorithm on small datasets is not that expensive. Among the datasets considered for evaluation DBLP, movie-RA and movie-TA are large-scale. CSP does not achieve better results on large scale datasets for both classification and retrieval tasks. CSP only achieves better results on small scale datasets, this raises the question of whether CSP is needed at all while a simpler Naive Bayes might just do the job. \n3. Authors also do not compare with simpler GNNs, which I think should be added in the evaluation.\n4. Authors discuss about various possible extensions to the basic CSP, but are not included in experimentation. I do not completely understand the reasoning behind this, but including them in experimental evaluation will strengthen the paper."
            },
            "questions": {
                "value": "I have addressed my concerns in the weakness section, I do not have any specific questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper extends classic Zhu Gharmani (2003) label propagation to hyper-graphs in a fairly straightforward manner.  Experiments are performed on the quality of the proposed algorithm on some small (and contrieved) hypergraph datasets.  Although they don't contain many recent baselines, the proposed method is still beat by baselines frequently (including naive bayes)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ well principled extension of classic idea\n+ clearly written"
            },
            "weaknesses": {
                "value": "- experimental results are on simple datasets and use very simple baselines.  still the proposed method doesn't seem that compelling\n- the hypergraph formalization seems excessively complicated for simple recommendation problems studied\n- zhu and gharmani is so old that it has doubtlessly been extended to hyper-graphs;  there absolutely must be more related work in this area"
            },
            "questions": {
                "value": "See weaknesses.  Unfortunately there realistically isn't much that could convince me of this paper's novelty or merits.\n\nThe related work, discussion, and baselines should focus more on the existing work on LP and hypergraphs (Henne 2015, Villian, etc).  Many more papers in this vein are not compared with, including:\n\n- Hypergraph Propagation and Community Selection for Objects Retrieval (2021)\n- Hypergraph Label Propagation Network (2020)\n- Wasserstein Soft Label Propagation on Hypergraphs (2018)\n\n.... and doubtlessly many more."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Convolutional Signal Processing (CSP), a simple, nonparametric, and scalable baseline for hypergraphs. It further examines the relationship between CSP and previously established baselines, such as Hypergraph Convolution, Label Propagation, and Naive Bayes. The method is validated on both classification and retrieval tasks."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* Paper is well written and easy to follow.\n\n* The proposed method is simple and scalable."
            },
            "weaknesses": {
                "value": "* The authors observe that their proposed method resembles a simplified form of Hypergraph Convolution, where learnable weights are replaced by identity matrices. This extension is straightforward and lacks novelty. To strengthen the contribution, a clean empirical analysis or theoretical analysis could clarify the advantages of this design choice over standard hypergraph convolutional methods. Such an addition would enhance the novelty of the approach and provide insights into the specific benefits and trade-offs introduced by this simplification.\n\n\n* Inconsistent evaluation compared to prior work: previous studies [1, 2] typically employ a 50%/25%/25% train/validation/test split, whereas CSP uses a 90%/10% train/test split. The authors should either align their evaluation with a similar evaluation split in [1, 2] or provide a clear justification for their choice of the 90%/10% split. Additionally, it would be beneficial for the authors to specify whether the recommended hyper-parameters in Hypergraph Convolution were used for the baseline methods and detail the specific settings employed for each baseline. \n\n\n* Retrieval tasks report only P@100; however, it would be more informative to also report NDCG@K (K=1/10/20). The authors should provide a rationale for their choice of P@100, explaining why this metric was selected over others.\nCSP can only support binary labels. What are some implications of this limitation for real-world applications? Were any extensions considered to support multi-class problems?\n\n\n* Missing baseline comparisons with MLP and Label Propagation. Including these baselines might offer additional insights into performance and advantages of CSP.\n\n---\nReferences\n\n[1] Equivariant Hypergraph Diffusion Neural Operators, ICLR 2023 \n\n[2] From Hypergraph Energy Functions to Hypergraph Neural Networks, ICML 2023"
            },
            "questions": {
                "value": "See weaknesses.\n\nTypo: Line 157: consists in a  \u2192 consists of a"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Convolutional Signal Propagation (CSP), a simple non-parametric method designed for learning on hypergraphs and bipartite graphs. \n\nThe authors present CSP as an efficient baseline for hypergraph node classification and retrieval tasks, achieving competitive performance with lower computational complexity.\n\nCSP operates natively on these complex structures and is compared with well-known methods such as label propagation, Naive Bayes, and Hypergraph Convolutional Networks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The motivation to propose a simple non-parametric method as a robust baseline for learning on hypergraphs, both with and without vertex features is strong.\n2. The paper is well-structured and provides intuitive explanations of the problem, the method, and its applications. The formal presentation of CSP is enhanced with toy examples and implementation details."
            },
            "weaknesses": {
                "value": "1. The paper lacks a detailed discussion and in-depth comparison with several existing methods [a, b, c]. The significance of CSP as a baseline can be greatly enhanced through empirical evaluation against existing methods. Such an evaluation considers multiple metrics, such as accuracy, training times, and memory usage, as well as the tradeoffs among these across all datasets.\n[a] proposes a scalable method on large graphs.\n[b] and [c] propose generalisations of neural networks on graphs and hypergraphs.\n2. The core idea of CSP is already present [d] in the literature on hypergraph learning. Adding a unique enhancement, such as theoretical analysis, into the non-parametric CSP would strengthen the originality of the idea.  For instance, it could be the case that the non-parametric nature of CSP makes it more robust to overfitting and hence easier to generalise to unseen data.\n3. The academic datasets used in the paper (Cora, Citeseer, Pubmed, DBLP) include vertex features suitable for processing with (hyper)graph neural networks. Therefore, it is necessary to include hypergraph neural networks as competitors to demonstrate the effectiveness of CSP. The method HGCN is used as a competitor but including more recent methods such as [b] and [c] would strengthen the significance of CSP.\n4. In datasets lacking vertex features, it is necessary to compare against non-neural methods for hypergraph learning [e, f]. Specifically, both [e] and [f] propose non-trivial methods for classifying nodes on hypergraphs, making them essential baselines for the task of node classification. \n5. The paper briefly introduces several potential extensions of CSP, such as alternative normalisations and generalisations of label propagation, but does not evaluate these variants experimentally. Including experimental results for these extensions would strengthen the paper by demonstrating the practical value of the proposed variations and providing a more comprehensive understanding of CSP's potential. \n\n\n\nReferences:\n* [a] LightHGNN: Distilling Hypergraph Neural Networks into MLPs for 100x Faster Inference, In ICLR'24,\n* [b] You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks, In ICLR'22,\n* [c] UniGNN: a Unified Framework for Graph and Hypergraph Neural Networks, In IJCAI'21,\n* [d] Hypergraph Label Propagation Network, In AAAI'20,\n* [e] The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited, In NeurIPS'13,\n* [f] Re-revisiting Learning on Hypergraphs: Confidence Interval and Subgradient Method, In ICML'17."
            },
            "questions": {
                "value": "1. Was an analysis conducted on the tradeoffs between accuracy, training time, and memory usage? How did these tradeoffs compare to existing methods, such as LightHGNN [a]? Understanding these aspects would clarify the efficiency and practicality of CSP in different scenarios.\n2. How does CSP offer a unique contribution compared to the existing work on hypergraph learning [d]? Were there unique aspects that were not covered in the existing literature?\n3. Was the inclusion of recent generalisations of hypergraph neural networks [b, c] considered in the experiments as competitors? How did these methods perform in comparison to CSP within the context of the datasets utilised with vertex features?\n4. For datasets lacking vertex features, how did CSP compare to non-neural hypergraph learning methods [e, f]?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}