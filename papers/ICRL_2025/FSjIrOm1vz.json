{
    "id": "FSjIrOm1vz",
    "title": "Inference Scaling for Long-Context Retrieval Augmented Generation",
    "abstract": "The scaling of inference computation has unlocked the potential of long-contextlarge language models (LLMs) across diverse settings. For knowledge-intensivetasks, the increased compute is often allocated to incorporate more external knowl-edge.  However, without effectively utilizing such knowledge, solely expandingcontext does not always enhance performance. In this work, we investigate infer-ence scaling for retrieval augmented generation (RAG), exploring strategies beyondsimply increasing the quantity of knowledge. We focus on two inference scalingstrategies: in-context learning and iterative prompting. These strategies provideadditional flexibility to scale test-time computation (e.g., by increasing retrieveddocuments or generation steps), thereby enhancing LLMs\u2019 ability to effectivelyacquire and utilize contextual information. We address two key questions: (1) Howdoes RAG performance benefit from thescaling of inference computationwhenoptimally configured? (2) Can we predict the optimaltest-time compute allocationfor a given budget by modeling the relationship between RAG performance andinference parameters? Our observations reveal that increasing inference computa-tion leads to nearly linear gains in RAG performance when optimally allocated, arelationship we describe as theinference scaling laws for RAG. Building on this,we further develop thecomputation allocation modelto estimate RAG performanceacross different inference configurations.  The model predicts optimal inferenceparameters under various computation constraints, which align closely with theexperimental results. By applying these optimal configurations, we demonstratethat scaling inference compute on long-context LLMs achieves up to 58.9% gainson benchmark datasets compared to standard RAG.",
    "keywords": [
        "inference scaling",
        "long-context LLM",
        "retrieval augmented generation"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=FSjIrOm1vz",
    "pdf_link": "https://openreview.net/pdf?id=FSjIrOm1vz",
    "comments": [
        {
            "summary": {
                "value": "This paper studies the inference scaling behaviors of two retrieval augmented generation (RAG) methods, demonstration-based RAG (DRAG) and iterative demonstration-based RAG (IterDRAG). The inference computation can be scaled in multiple ways, including increasing the number of retrieved documents, in-context examples, or introducing additional generation steps in IterDRAG. Experimental results show DRAG and IterDRAG achieve scaling properties with the proposed configurations, and demonstrate the performance of DRAG and IterDRAG can scale almost linearly with an increasing computation budget. Besides, the paper also learns a computational allocation model that could provides configuration guidance for DRAG and IterDRAG."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper studies two interesting research questions including the scaling behavior and the prediction of test-time computation allocation long-context RAG methods. The paper conducts systematical experiments on inference scaling of long-context RAG models, and reveals the scaling properties of DRAG and IterDRAG, i.e., the performance improves almost linearly with optimal configuration. Besides, the computational allocation model generalizes well across domains and context lengths, which potentially helps the community to better configure RAGs."
            },
            "weaknesses": {
                "value": "I have a question on the application of the computational allocation model. When pretraining LLMs, computational allocation models are crucial since pretraining is extremely resource-intensive. However, inference is typically much less costly by comparison. So, why not determine the best configuration by simply searching it?"
            },
            "questions": {
                "value": "Please see the question above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores inference scaling in long-context RAG; it analyses downstream QA results in different configurations, showing that RAG performance improves ~linearly with increasing test-time compute under optimal inference parameters. Furthermore, based on these observations, authors derive a set of inference scaling laws for RAG and a computation allocation model.\n\nMy only concern is to what extent the current model generalises -- from Fig. 4b, it seems like the considered models might suffer from \"lost in the middle\" (https://arxiv.org/abs/2307.03172) problems, while more recent long-context models seem to suffer from this significantly less (e.g., https://arxiv.org/abs/2404.16811)"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Inference-time scaling laws for RAG systems -- extremely interesting, and the community really needs an analysis like this one."
            },
            "weaknesses": {
                "value": "It is not clear whether the current analysis may generalise to future SFT/RLHF regimens."
            },
            "questions": {
                "value": "More recent models may suffer less from \"lost in the middle\" issues -- does the current analysis still hold?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores inference scaling for retrieval augmented generation (RAG) in long-context large language models (LLMs), focusing on strategies beyond simply expanding the knowledge base. The authors investigate how in-context learning and iterative prompting can optimize the use of additional compute resources during inference. They address two primary questions: the benefit of inference computation scaling for RAG and the prediction of optimal compute allocation within a given budget. Their findings reveal that optimal allocation of inference computation results in nearly linear performance gains for RAG, a phenomenon described as inference scaling laws. The authors also develop a computation allocation model that accurately predicts the optimal inference parameters, with experimental results showing up to 58.9% performance improvements on benchmark datasets compared to standard RAG configurations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The research question is quite interesting, as there is not much work on inference time scaling for RAG; this study systematically explores this area and may draw some attention."
            },
            "weaknesses": {
                "value": "I am concerned that this work is more suitable as a technical report rather than a research-oriented study. There is considerable related work combining long-context LLMs and RAG, and the main contribution of this work is mainly the proposed RAG inference scaling law. However, this conclusion is method-specific and may not apply to other methods."
            },
            "questions": {
                "value": "1. Can you provide a straightforward explanation of your findings and how they guide the use of long-context LLMs for RAG?\n2. If other prompts or methods are used, is your computation allocation model still applicable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper systematically investigates the performance of RAG systems as inference computation resources scale up, demonstrating an almost linear improvement in RAG performance with optimal test-time compute allocation. Furthermore, the authors derive an empirical scaling law that can predict the optimal inference parameter configuration for RAG systems under various computational budgets."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper studies a significant issue in the LLM community.\n2. The authors conducts extensive experiments across various datasets and evaluation metrics, yielding convincing results.\n3. This paper is well structured, clearly written, and easy to understand."
            },
            "weaknesses": {
                "value": "1. Novelty of the proposed methods is somewhat limited. Both DRAG and IterDRAG are conventional approaches in academia and industry, similar to some classic methods like ITER-RETGEN, IRCoT, and Active-RAG.\n\n2. The experiments were only conducted with Gemini-1.5-Flash and Gecko retrieval. Gemini-1.5-Flash is a relatively small LLM, and different conclusions might be drawn on larger, more powerful LLMs. Moreover, if the scaling laws derived could be generalized to other LLMs and retrieval methods, it would add greater value to this work.\n\n3. Some experimental phenomena lack more in-depth discussion. For example, on line 371, the impact of few-shot examples on IterDRAG should be assessable, at least through ablation studies to determine whether it is the in-context query decomposition or the knowledge extraction capability that is more influential.\n\n4. While increasing the context size can improve RAG performance, it also leads to greater inference time and token consumption, especially when using iterative retrieval. The authors did not discuss this trade-off."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}