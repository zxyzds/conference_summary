{
    "id": "2XBPdPIcFK",
    "title": "Steering Language Models with Activation Engineering",
    "abstract": "Prompt engineering and finetuning aim to maximize language model performance on a given metric, like toxicity reduction. However, these methods do not fully elicit a model\u2019s capabilities. To reduce this gap, we introduce _activation engineering_: the inference-time modification of activations in order to control (or _steer_) model outputs. Specifically, we introduce the _Activation Addition_ (ActAdd) technique, which contrasts the intermediate activations on prompt pairs (such as \u201cLove\u201d versus \u201cHate\u201d) to compute a _steering vector_. By tactically adding in e.g. the \u201cLove\u201d\u2212\u201cHate\u201d steering vector during the forward pass, we achieve SOTA on negative-to-positive sentiment shift and detoxification using models including LLaMA-3 and OPT. ActAdd yields inference-time control over high-level properties of output (like topic and sentiment) while preserving performance on off-target tasks. ActAdd is lightweight: it does not require any machine optimization and works with a single pair of data points, which enables rapid iteration over steering. ActAdd demonstrates the power of activation engineering.",
    "keywords": [
        "interpretability",
        "steering",
        "alignment",
        "safety",
        "sentiment"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We steer LLMs by adding a bias term computed from the model's representations of simple prompt pairs.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=2XBPdPIcFK",
    "pdf_link": "https://openreview.net/pdf?id=2XBPdPIcFK",
    "comments": [
        {
            "comment": {
                "value": "I thank the authors for their reply. However, I still disagree with the points they make.\n\nBefore going into specific points made by the authors, let me reiterate the main argument: The paper includes references to work published over a year ago stating they are contemporary. While these papers use a very similar method as the authors present, a  comparison (e.g., in the experiments) is not presented in the paper. This is specifically problematic since one of those papers outperforms the given method for at least one application.\n\n**We agree that revisions are important. However, your speculation about no \u201cmajor modifications\u201d is incorrect. For example, we rewrote the entire paper for ICLR.**  \nApologies for my mistaken assumption. My assumption was mainly based on the lack of recent citations, the use of outdated models, and the use of outdated baselines. This is in my opinion the most important part to address in an updated paper, and I believe this has not happened to a sufficient degree in this paper.\n\n**Furthermore, works from 2023 would have been submitted to ICLR 2024, which makes comparison in ICLR 2025 more reasonable.**  \nNo, it does not. The fact remains that these works are well-known, contain many of the same ideas, and are not compared against. One year should have given the authors plenty of time to compare against these baselines and improve upon their ideas. If a paper contained similar ideas to a paper published in ICLR 2024, but did not compare against it, this paper should be rejected.\n\n**We did not mean to claim that we directly inspired that work (although our ActAdd paper has, in fact, inspired a range of follow-on work). What we said was that those papers \"followed\" ours, meaning \"followed\" in a temporal sense. We see how this was unclear and will instead state that those papers \"came after\" this work.**  \n\"Followed\" is a very strange word to use in the temporal sense if you do not compare against the work. We are reviewing your paper against the current state-of-the-art, which now includes these papers.\n\n**We also observe that their approach outperformed ActAdd on a different task - TruthfulQA - which is not part of our paper. Those two facts do not invalidate our findings or the scientific contribution of this paper.**  \nBut it does! Unless you show that for the tasks on which you evaluate ActAdd, it outperforms their method, I have to assume your method is strictly worse.\n\n**We made a scientific discovery (in line with some past evidence from e.g. GANs): LLMs are steerable via linear manipulations of their activation space.**  \nUnfortunately, this scientific discovery has now been made in other papers as well. Please, do not understand me wrong: I do believe this idea is very interesting and worth noting. However, I cannot imagine this be presented at ICLR 2025 as \"novel\" since it has been already known for a year. In order to be accepted, your paper should now improve upon prior works that use this idea.\n\n**Consulting ICLR\u2019s reviewer guidelines:**  \nThe guidelines clearly state that papers should contain new knowledge. This is not the case anymore. To cite another part of those guidelines:\n\nQ: Are authors expected to cite and compare with very recent work? What about non peer-reviewed (e.g., ArXiv) papers? (updated on 7 November 2022)  \nA: We consider papers contemporaneous if they are published within the last four months. That means, since our full paper deadline is October 1, if a paper was published (i.e., at a peer-reviewed venue) on or after July 1, 2024, authors are not required to compare their own work to that paper. \n\nThis part of the guidelines clearly states that you should compare against all works before July 1, 2024.\n\n**For another angle on the issue, suppose paper X makes discovery Y. ...**    \nHowever, in this case Paper X' makes the discovery \"Y+\" since it is outperforming your method and I have not seen evidence to the contrary. \n\nBased on the provided information from the authors, I will almost certainly not change opinions on this topic anymore. If the authors want to consult with the AC to discuss the issue, I would be happy to contribute to that discussion. If, after that discussion, the AC instructs me to ignore this point in my review, I will of course do so. However, please note that the remaining weaknesses would still lead me to reject the paper, although those points are much more addressable in a rebuttal and I therefore expect that the situation might improve there."
            }
        },
        {
            "title": {
                "value": "Addressing two major claims"
            },
            "comment": {
                "value": "We want to clarify two major concerns you raised.\n\n> The primary issue with the paper is that it is outdated. The paper refers to several works published in 2023 as \"contemporary,\" implying that they are based on the presented work. This suggests that the paper may have been rejected in previous conferences and is now being resubmitted to ICLR without any major modifications. However, works from 2023 cannot be referred to as contemporary in a submission to ICLR 2025.\n\nWe agree that revisions are important. However, your speculation about no \u201cmajor modifications\u201d is incorrect. For example, we rewrote the entire paper for ICLR.\n\nFurthermore, works from 2023 would have been submitted to ICLR 2024, which makes comparison in ICLR 2025 more reasonable. \n\n> Moreover, the claim that both Liu et al. (2023) and Zou et al. (2023) are based on this work is questionable. \n\nWe did not mean to claim that we directly inspired that work (although our ActAdd paper has, in fact, inspired a range of follow-on work). What we said was that those papers \"followed\" ours, meaning \"followed\" in a _temporal_ sense. We see how this was unclear and will instead state that those papers \"came after\" this work. \n\n> A quick review of these papers reveals that Liu et al. (2023) merely cites ActAdd as related work, and Zou et al. (2023) actually outperforms ActAdd on one of the tasks. Therefore, I do not believe ActAdd presents any novel idea or result. This undermines the relevance of the method, and I believe this alone is sufficient for rejection. However, if I have misunderstood this point, the authors could clarify their claims.\n\nWe made a scientific discovery (in line with some past evidence from e.g. GANs): LLMs are steerable via linear manipulations of their activation space. We want our discovery to be scientifically validated by the peer review process. Zou et al explicitly noted that their approach is a \u201cvariant of ActAdd.\" We also observe that their approach outperformed ActAdd on a different task - TruthfulQA - which is not part of our paper. Those two facts do not invalidate our findings or the scientific contribution of this paper.\n\nConsulting ICLR\u2019s reviewer guidelines:\n\n> Q: If a submission does not achieve state-of-the-art results, is that grounds for rejection?\n>\n> A: No, a lack of state-of-the-art results does not by itself constitute grounds for rejection. Submissions bring value to the ICLR community when they convincingly demonstrate new, relevant, impactful knowledge. Submissions can achieve this without achieving state-of-the-art results.\n\nFor another angle on the issue, suppose paper X makes discovery Y. Paper X\u2019 (published substantially later) makes a further discovery Y\u2019. If paper X is not immediately published, and instead is being reviewed one year later for its scientific contributions, should it be considered to \u201cnot present any novel idea or result\u201d because people already know Y\u2019 (and also, therefore, a subset of X's discoveries about Y)? We think the answer is \u201cno, the original contribution is still valuable.\u201d If you disagree, we are happy to consult with the area chair to come to a mutual understanding of this issue.\n\nWe will address the rest of your helpful feedback and concerns in a follow-up comment. We think many of your concerns are reasonable and fixable."
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors introduce a paradigm of controlling model outputs/behavior which they term activation engineering. In activation engineering, a user controls model behavior by editing intermediate activations/hidden states of the model during the forward pass. They propose and focus on a specific method in the class of activation engineering called Activation Addition (ActAdd), in which a vector encoding an axis (e.g. love vs hate) can be added to the intermediate activations to make the model shift along that axis, e.g., in sentiment from negative to positive. They compute this vector by taking the difference along a single paired example (e.g. a love vs hate example) and demonstrate effectiveness in experiments on sentiment analysis and toxicity reduction."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Originality: The idea of activation engineering as \u201cperturbing activations during the forward pass\u201d is an important and simple idea. While it seems that much concurrent or previous work has also worked with this idea of editing activations, e.g. the ROME paper (Meng et al 2022), adding steering vectors (ActAdd) to control model outputs is to my knowledge original (and the authors do well to cite concurrent work in Li et al 2023b). \n\nQuality: Experiments are overall fairly thorough and demonstrate that ActAdd is a promising, intuitive, and simple approach to control model outputs.\n\nClarity: The overall flow of the paper is clear and well written.\n\nSignificance: This is an important contribution to interpretability and control of models using activation-level edits. The idea that you can controllably transform model behavior by adding a vector to the residual stream is important."
            },
            "weaknesses": {
                "value": "The biggest weaknesses in my read are a lack of clarity in the algorithm and some of the experiment setup and results. I leave specific questions/suggestions on this point for the Questions section of the review.\n\nAlso, the authors should be careful to clarify their definitions and contributions. In the intro/abstract, they define activation engineering as \u201cthe inference time modification of activations in order to control model outputs\u201d. However, section 2 states \u201cActivation engineering invovles creating vectors of activation which cause desired changes to output text when added to the forward passes of a frozen LLM\u201d. This latter definition sounds more specific than the original one; there are many works which fall under the first definition but not necessarily the second one. From my read, I would be careful to claim that you are introducing activation engineering and might instead recommend stating it as highlighting AE as a class of methods to control behavior, under which ActAdd (your primary contribution) falls."
            },
            "questions": {
                "value": "* Can you elaborate on how you search for injection coefficient c and injection layer l? How expensive is this process?\n* In Figure 2, what is the x axis? Why should we expect perplexity to go down when x axis increases?\n* In Figure 3, how is \u201cP(steered completion contains wedding related words)\u201d determined? Can you be more explicit about this in the paper? \n* Can you elaborate on what the p value in table 3 and 4 is? That is, what is the null hypotheses you are testing (and the corresponding alternative hypothesis)?\n* In Figure 5/S4.5, referring to the model\u2019s behavior as \u201coff-target answer probabilities\u201d is rather misleading. That phrase reads as the model\u2019s distribution over the answers for non-target tokens, whereas it seems that the actual probabilities being referred to is the P@K.\n* How do you determine which example to use to determine the steering vector? Did you do any studies on variance across the effectiveness for vectors derived from different examples?\n* Are there any experiments to support the claim in the intro that activation engineering can enable composition of multiple traits, e.g. speech eloquence and mathematical content? If not, I would remove this to avoid overclaiming.\n* The notation in Algorithm 1 could use some improved clarity. For example, what is @? In code it can refer to a matmul; even though this seems like an indexing operation the ambiguity is confusing for the reader."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes ActAdd, a method to _steer_ a Language Model's generation in a particular direction. ActAdd is lightweight and merely involves using contrasting prompts (related to the direction you want to steer the LM in). These contrasting prompts are used to compute a steering vector that can be applied at inference time to change the model's behavior.\nThe authors experimented with various tasks such as steering the topic of the LM's generation, steering to reduce toxicity, and steering to change sentiment. \nThe authors also show that ActAdd preserves the model's knowledge by showing that when the model's accuracy remains unchanged on ConceptNet when asked to steer towards a certain topic."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed approach is straightforward, lightweight, and demonstrates effectiveness on certain benchmarks. However, the experiments conducted only partially support the claims made in the paper (see more details under weaknesses).\n\nThe algorithm is well-presented, though some aspects of the experiments could benefit from further clarification."
            },
            "weaknesses": {
                "value": "The paper\u2019s experiments are interesting but could benefit from further depth and clarity. In some cases, it\u2019s challenging to fully understand the conclusions drawn from certain experiments. Additionally, some benchmarks created by the authors are quite small, which makes the results appear more anecdotal than empirical. There are also a few discrepancies with the baselines, as well as cases where only portions of larger benchmarks are used (eg. why use only a subset of RealToxicityPrompts and Sentiment? The current experimentation is performed on ~10% of the test split)\n\nThe paper would greatly benefit from demonstrating how ActAdd performs on larger benchmarks specifically designed for steering and alignment, such as HelpSteer (1 and 2)[1,2]. Also comparisons to methods that involve alignment training might give some indication on if ActAdd can be used instead of or in tandem with some these approaches in practice [3]. \n\nI've summarized my concerns as questions for certain parts of the experiments section\n\nQuestions\n1. ACTADD CAN CONTROL WHAT THE MODEL TALKS ABOUT\n- Which dataset serves as the starting point for the prompts? Is the experiment based on a single prompt with 100 generations? If so, **using a single prompt might make it difficult to fully verify the claim that \"ActAdd can steer the model to talk about a topic.\"**\n- Why does ActAdd perform well for certain topics but not others (e.g., Art)? Is it effective only for steering toward specific topics? Additionally, it is unclear what accounts for the drop at c=0.5 for weddings? This might indicate some experiments on how reliable ActAdd is. \n\n2. ACTADD CAN REDUCE TOXICITY\n- The results in this section could be clearer. The only baseline models are the unsteered model, prompting, and PREADD, while other comparisons, such as FUDGE and AirDecoding, are tested on GPT-2, making direct comparison difficult given the model-dependent nature of the task.\n- Some discrepancies in results are also notable\u2014for instance, the paper draws baselines from this paper ((https://aclanthology.org/2023.findings-acl.636.pdf)), but there are differences in the results for the unsteered OPT (0.152 vs. 0.134 toxicity, 49.9 vs. 8.9 fluency). Such large changes in fluency might suggest a difference in experimental setups, which could potentially affect the interpretation of ActAdd's fluency improvements.\n- Regarding the other results there seem to be a lot of discrepancies -- The authors pick most of their baselines from (https://aclanthology.org/2023.findings-acl.636.pdf). However, the unsteered OPT result is very different. (0.152 vs 0.134 toxicity and 49.9 vs 8.9 for fluency). With such a large change in fluency, it seems there might be a difference in the experimental setup of the two papers. This throws some doubt if the ActAdds better fluency comes from a different experimental setup. \n\n3. ACTADD PRESERVES THE MODEL\u2019S GENERAL KNOWLEDGE\n\nThere are some concerns regarding the setup here. ConceptNet, as a knowledge base, typically requires single-word answer predictions. Showing that the model performs similarly with and without ActAdd doesn\u2019t entirely demonstrate that ActAdd avoids side effects on the model\u2019s factual accuracy. Perhaps this could be bolstered with verifying if the factuality of longer form generations remain unaffected. The FactScore benchmark [4] might be a good place to start. \n\nFinally, while I attempted to review the provided code for further insights, it was challenging to navigate, and the links listed in tab 5 of the appendix did not seem to work.\n\n\nOverall I believe the approach has potential and the paper could heavily benefit from more thorough and comprehensive experimentation.\n\n\nRefs\n[1]https://arxiv.org/abs/2311.09528\n[2] https://arxiv.org/pdf/2406.08673\n[3] https://arxiv.org/abs/2310.05344\n[4] https://arxiv.org/abs/2305.14251"
            },
            "questions": {
                "value": "Questions added in Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Paper proposed \u201cAdd Act\u201d, a type of activation engineering that, when applied to language models (LMs), can \u201csteer\u201d the model the output during inference. \u201cSteering\u201d an LM, in this context, would mean enabling the user to enhance or control some high-level property of the generated text such as topic or sentiment of the text."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed activation engineering method can be applied during inference and does not require gradient-based optimization (thus making it computationally fast to compute and apply).\n2. The proposed activation engineering method does not modify the original LM\u2019s weights, and therefore would not change the model\u2019s performance on tasks if the activation engineering method wasn\u2019t applied. This is a unique advantage, as many related \u201csteering\u201d methods that modify a LM\u2019s weights may harm model performance on tasks unrelated to the \u201csteering\u201d-related tasks.\n3. Paper provides many compelling examples of where \u201cAddAct\u201d has been able to successfully steer an LM output (i.e., sentiment, topic, reducing toxicity) across many model architectures."
            },
            "weaknesses": {
                "value": "The authors have missed some related work in the area of activation engineering, and their paper may benefit from further comparing and contrasting the proposed \u201cAddAct\u201d method to these works: \n\n[a] Sakarvadia, Mansi, et al. \"Memory injections: Correcting multi-hop reasoning failures during inference in transformer-based language models.\" arXiv preprint arXiv:2309.05605 (2023).\n\n[b] Heimersheim, Stefan, and Neel Nanda. \"How to use and interpret activation patching.\" arXiv preprint arXiv:2404.15255 (2024).\n\n[c] Vig, Jesse, et al. \"Investigating gender bias in language models using causal mediation analysis.\" Advances in neural information processing systems 33 (2020): 12388-12401.\n\nSpecifically, I would like the authors to discuss the computational cost of computing the steering vector, especially if one must test multiple steering vectors for multiple target layers (as it is not obvious which layers/vectors would work best for a specific \u201csteering\u201d goal, and thus a user may need to do (costly) experimentation. Specifically, the \u201cAddAct\u201d method relies on generating the \u201csteering vector\u201d by doing two partial forward passes for the steering prompt pair. This itself is computationally expensive compared to a recent related work [a] which demonstrated that one could compute a \u201csteering vector\u201d simply using the model\u2019s (un)embedding matrix, rather than running the steering prompts through the top N layers of an LM.\n\nFurther, the \u201cAddAct\u201d \u201csteering vector\u201d is layer-specific within a given LM. For example, if a steering vector is generated for layer N, it is not clear if the same vector can be applied to layer N+1. This is a drawback of the method as it may not be apparent to the user which layer would be best for an \"AddAct\" injection. Again, I would be interested if the authors could discuss how their proposed layer-specific steering vector generation strategy compares to related work [a] which proposed a steering vector that is layer-agnostic."
            },
            "questions": {
                "value": "n/a"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces ActAdd, a controlled text generation technique that modifies the inner activations of an LLMduring forward passes to guide text generation towards a specific property. These modifications are applied using steering vectors, computed by taking the difference between the activations of a positive and negative prompt at a specific layer. The results demonstrate that ActAdd outperforms the baselines on tasks such as toxicity reduction and sentiment control."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow.\n- Activation addition is an intuitive and powerful technique that enables fine-grained control over model outputs.\n- The results convincingly show that activation addition outperforms included baselines in both sentiment control and toxicity reduction tasks."
            },
            "weaknesses": {
                "value": "The primary issue with the paper is that it is outdated. The paper refers to several works published in 2023 as \"contemporary,\" implying that they are based on the presented work. This suggests that the paper may have been rejected in previous conferences and is now being resubmitted to ICLR without any major modifications. However, works from 2023 cannot be referred to as contemporary in a submission to ICLR 2025.\n\nMoreover, the claim that both Liu et al. (2023) and Zou et al. (2023) are based on this work is questionable. A quick review of these papers reveals that Liu et al. (2023) merely cites ActAdd as related work, and Zou et al. (2023) actually outperforms ActAdd on one of the tasks. Therefore, I do not believe ActAdd presents any novel idea or result. This undermines the relevance of the method, and I believe this alone is sufficient for rejection. However, if I have misunderstood this point, the authors could clarify their claims.\n\nAdditional (and significant) weaknesses include:\n- Outdated Models: Most of the experiments were conducted on outdated models (OPT, GPT2-xl, and Llama-2). While a few experiments were rerun on Llama-3, there were no baseline comparisons for these models.\n- Inconsistent Baselines: The models used in the baselines do not match. For example, in Table 3, various models are used without a clear pattern. Ideally, all models should be run for every baseline to ensure fair comparison.\n- Outdated Baselines: Baselines such as Fudge and PreAdd have been surpassed by newer techniques (e.g., [1]). Additionally, the paper does not include any baselines that use white-box transformations to control model behavior, despite several relevant works from 2023 (Liu et al. (2023) and Zou et al. (2023)).\n- Inconsistent Perplexity Measurements: Perplexity for the included models was measured using Davinci 002, an old and less effective model. Furthermore, Lines 503-505 state that PreAdd's perplexity was measured using Davinci 001, making direct comparisons between the two methods problematic.\n- Omission of Fudge: In Lines 378-380, Fudge is omitted, despite performing better on certain aspects and only slightly worse on others. This is a strange misrepresentation of the results.\n- Redundant Experiments: The experiments in Sections 4.1 and 4.2 add little to the discussion, as they merely confirm that activation addition works. Furthermore, Tables 3 and 4 essentially present the same findings, but in a more interesting and applicable setting.\n- Basic Metrics: Perplexity and cosine similarity are insufficient metrics to fully capture fluency and relevance. Since controlled text generation methods edit the model's internals, they can yield unintuitive results that these metrics may not fully capture. The authors should include human or LLM-based evaluations to assess the outputs in Tables 3 and 4 and compare them with baselines.\n- Insufficient Code: The provided code lacks essential instructions and does not include scripts to reproduce the experiments. It only includes some notebooks for experimenting with activation addition, which overlooks the most important reason for providing the code. Additionally, the link to the GitHub repository that is present in the included code (playground.ipynb, top) violates the double-blind review process, as it is not anonymized.\n- Unconvincing Experiment in Section 4.5: Evaluating a model with activation addition on one or more recent, open-form reasoning benchmarks (such as GSM8k, MixEval, or MMLU-Pro) would be much more convincing than the benchmark with perplexity measurements.\n- Different Hyperparameters Across Experiments: If I am correct, the results for activation addition were generated using different values for top_p and temperature compared to some baselines (e.g., PreAdd), which undermines the validity of the comparisons. All non-critical hyperparameters should be kept consistent across baselines.\n\n[1] Dekoninck, Jasper, et al. \"Controlled text generation via language model arithmetic.\" arXiv preprint arXiv:2311.14479 (2023)."
            },
            "questions": {
                "value": "- What is meant by \"this section does not have complete statistics\" in Line 533?\n- How was grid search performed for ActAdd's hyperparameters? Were the results reported for the best set of parameters? If so, was a similar hyperparameter search conducted for the baselines to ensure accurate comparisons?\n- Could you clarify the hyperparameter \u201ca\u201d discussed in Appendix C and explain its function?\n- For which experiments are the prompts mentioned in Lines 1218-1223 used? Appendix C presents a collection of unrelated details, making it difficult to follow and understand how it fits into the overall context of the paper. Could the authors clarify the connection to the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}