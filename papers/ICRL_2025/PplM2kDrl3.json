{
    "id": "PplM2kDrl3",
    "title": "Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model",
    "abstract": "Recent advancements in diffusion models have revolutionized generative modeling. However, the impressive and vivid outputs they produce often come at the cost of significant model scaling and increased computational demands. Consequently, building personalized diffusion models based on off-the-shelf models has emerged as an appealing alternative. In this paper, we introduce a novel perspective on conditional generation for transferring a pre-trained model. From this viewpoint, we propose \\textit{Domain Guidance}, a straightforward transfer approach that leverages pre-trained knowledge to guide the sampling process toward the target domain. Domain Guidance shares a formulation similar to advanced classifier-free guidance, facilitating better domain alignment and higher-quality generations. We provide both empirical and theoretical analyses of the mechanisms behind Domain Guidance. Our experimental results demonstrate its substantial effectiveness across various transfer benchmarks, achieving over a 19.6\\% improvement in FID and a 20.6\\% improvement in FD$_\\text{DINOv2}$ compared to standard fine-tuning. Notably, existing fine-tuned models can seamlessly integrate Domain Guidance to leverage these benefits, without additional training.",
    "keywords": [
        "transfer learning",
        "diffusion models",
        "fine-tuning",
        "guidance"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "This paper proposes a conditional generation perspective for the transfer of diffusion models and derives a simple approach named Domain Guidance to enhance transfer learning.",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PplM2kDrl3",
    "pdf_link": "https://openreview.net/pdf?id=PplM2kDrl3",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces \"Domain Guidance\" (DoG), a new transfer approach for pre-trained diffusion models aimed at enhancing domain alignment and generating high-quality outputs. DoG builds on classifier-free guidance (CFG) but adapts it to transfer learning by incorporating pre-trained model knowledge to guide the generative process. Through empirical and theoretical analysis, the authors demonstrate DoG\u2019s effectiveness in maintaining pre-trained knowledge, improving image fidelity, and reducing out-of-distribution errors. The results indicate substantial improvements in several metrics (FID and FDDINOv2) across various downstream tasks compared to standard fine-tuning and CFG."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. DoG\u2019s design as a CFG variant is well-motivated, with clear delineation of the benefits over standard CFG. The illustrations of DoG\u2019s guidance process effectively highlight its domain-alignment benefits.\n2. Quantitative results show that DoG significantly outperforms CFG, particularly on datasets with substantial domain shifts. Qualitative results are also compelling, with DoG generating visually consistent images even with increased guidance weights.\n3. The paper provides a theoretical foundation for DoG, showing how it maintains domain alignment and mitigates OOD sampling errors, supported by an illustrative analysis using Gaussian mixtures."
            },
            "weaknesses": {
                "value": "1. The paper could benefit from more discussion on DoG\u2019s adaptability, particularly in domains vastly different from the pre-trained model\u2019s source domain.\n2. The influence of guidance weights is explored, but further sensitivity analysis could strengthen the understanding of DoG\u2019s stability across diverse settings.\n3. While diverse datasets are used, further tests on varying resolutions and different architectures could better illustrate DoG\u2019s flexibility."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This submission proposed a new conditioning method to transfer a diffusion model\n to a new domain named Domain Guidance (DoG).\nThe idea behind DoG is to pull the generation process toward the target domain\nfrom the pre-trained domain.\nGiven a pretrained model and fine-tuned version of it, domain guidance guides the process\nto the direction that the fine-tuned one indicates and against the pretrained ones's direction.\nTheoretical analyses show that DoG mitigates sampling from out-of-distribution area of the target domain.\nExperiments show that DoG outperforms CFG in combination with diffusion models finetuned in relatively small\ndatasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- The motivation and idea of DoG is clearly presented, and the paper is overall easy to follow.\n- Theoretical analyses and toy examples well depict the advantage of DoG.\n- DoG outperformed CFG, a de facto standard method for diffusion guidance with a margin."
            },
            "weaknesses": {
                "value": "- Range of applicable models is not discussed well:\nThe experiments are conducted with ImageNet-pretrained  DiT-XL/2 and \nbut is it mandatory to use the same architecture and initialization weights to perform DoG?\nFurthermore, transfering diffusion models to small datasets are often done with LoRA [a] rather than full finetuning.\nIs DoG applicable to LoRA-based transfer?\n[a] LoRA: Low-Rank Adaptation of Large Language Models \n\n\n- Relationship with Autoguidance:\nA similar idea is presented in a preprint [b] which uses less-trained version of the model for guidance, \ninstead of unfinetuned version in DoG.\nDiscussing this may be useful, while it it not mandatory due to its unpublished state and missing it should not be penalized.\n[b] Guiding a Diffusion Model with a Bad Version of Itself, NeurIPS 2024 to appear."
            },
            "questions": {
                "value": "Please see Weaknesses 1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work leverages pre-trained knowledge as the domain guidance to guide the model toward the target domain. This work thinks that domain guidance facilitates better domain alignment and higher-quality generations, and provides correlation theoretical analyses."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality: The idea of using the pre-trained model as a domain guidance to guide transfer learning is innovative.\n\nQuality and Clarity: The quality of this is satisfactory and the analysis is thoughtful. The paper is well-written, with a thorough and comprehensive ablation study.\n\nSignificance. This work provides a contribution to the transfer learning of diffusion learning. In particular, this work can be seamlessly integrated into the existing fine-tuning models without additional training."
            },
            "weaknesses": {
                "value": "1. Is there a domain gap between personalized scenarios and general scenarios?\n2. During the transfer learning process, does all the knowledge from the pre-trained model need to be effectively utilized?\n3. How can the impact of the knowledge in the pre-trained model be assessed in the target domain?\n4. For DF-20M (which has no overlap with ImageNet) and ArtBench-10 (whose feature distribution is completely distinct from ImageNet), why can guidance from the model pre-trained on ImageNet lead to better generation?\n5. Is the distribution of the downstream domain a subset of the distribution of the pre-trained domain?"
            },
            "questions": {
                "value": "it is the same as Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}