{
    "id": "g6fYDGKeyB",
    "title": "Addressing Misspecification in Simulation-based Inference through Data-driven Calibration",
    "abstract": "Driven by steady progress in generative modeling, simulation-based inference (SBI) has enabled inference over stochastic simulators. However, recent work has demonstrated that model misspecification can harm SBI's reliability, preventing its adoption in important applications where only misspecified simulators are available.\nThis work introduces robust posterior estimation (RoPE), a framework that overcomes model misspecification with a small real-world calibration set of ground truth parameter measurements.\nWe formalize the misspecification gap as the solution of an optimal transport problem between learned representations of real-world and simulated observations, allowing the method to learn a model of the misspecification without placing additional assumptions on its nature. The method shows how a small calibration set can be leveraged to offer a controllable balance between calibrated uncertainty and informative inference even under severely misspecified simulators. Our empirical results on four synthetic tasks and two real-world problems with ground-truth labels demonstrate that RoPE outperforms baselines and consistently returns informative and calibrated credible intervals.",
    "keywords": [
        "Simulation-based inference",
        "SBI",
        "Bayesian Inference",
        "Misspecification",
        "Likelihood-free",
        "Robust Inference",
        "Physics-informed Machine Learning"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "We introduce ROPE a simulation-based inference algorithm that relies on Neural Posterior Estimation and Optimal Transport to handle model misspecification with a small calibration set.",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=g6fYDGKeyB",
    "pdf_link": "https://openreview.net/pdf?id=g6fYDGKeyB",
    "comments": [
        {
            "title": {
                "value": "Authors' response [cont.]"
            },
            "comment": {
                "value": "**I also don\u2019t  ... you explain this?**\n\nIn section 3.1, we explain how we use the real-world calibration dataset, denoted $\\mathcal{C}$ and composed of pairs of observations and labels (i.e., $\\mathcal{C}:=\\{(\\theta^i, x_o^i)\\}$ defined on line 128), to finetune the neural statistics. In particular the loss defined by equation (6) depends explicitly on $x_o^i$ and the $\\theta^i$.\n\n**In EQ 3 ... when would it not be justified?**\n\nThe assumption will likely not hold in many situations, e.g, in the real-world experiment in task E (the light tunnel). To see this, look closely at the synthetic and real images in task E, figure 2: in the synthetic data, the dimming effect of the polarizers affects only the colour of the hexagon. However, in the real data, the frames holding the polarizers (see [[1](https://arxiv.org/pdf/2404.11341), Figure 2] for a detailed diagram), introduce a reflection (the diffuse circular bands around the image edges), which carries information about the actual polarizer positions and can be used to infer the dimming effect $\\alpha$. Thus, there is additional information about the parameters $\\theta$ in the real observations $x_o$ that is not captured by the synthetic observations $x_s$, and thus the assumption in equation 3 does not hold. In other words, there is more signal in the real data than in the simulator. However, this does not prevent RoPE from working (see the results for task E in Figure 2) as it can still use the dependence modelled by the simulator to infer the value of the parameter. Moreover, as we elaborate in the paragraph below equation (3), this assumption can also have some benefits when the simulator encodes properties that the practitioner / domain expert believes are invariant across application environments and we want to avoid the predictive model to use dependence patterns (i.e., shortcuts) between the parameter and the observation that are not stable over other environments or distribution shifts.\n\n[1] Gamella, Juan L., Jonas Peters, and Peter B\u00fchlmann. \"The Causal Chambers: Real Physical Systems as a Testbed for AI Methodology.\" [arXiv preprint arXiv:2404.11341 (2024)](https://arxiv.org/pdf/2404.11341)\n\n**In equation 6 ... Can you explain this to me in more detail?**\n\nThanks for raising this point, it seems we left a typo as $\\mathcal{L}(\\phi, \\mathcal{X})$ should be $\\mathcal{L}(\\varphi, \\mathcal{X})$. The loss enforces that the neural statistics on real data, $\\mathbf{g}_{\\varphi}(x_o)$ for a certain $\\theta$ match with the expected neural statistics predicted for the simulation that corresponds to that $\\theta$, $\\mathbf{h}\\_{\\omega^\\star} (S(\\theta, \\epsilon)$. \n\n**I don\u2019t quite understand  the results in relation to the LPP and ACAUC metrics...**\n\nTL;DR: As we explain in line 387, the SBI baseline is an artificial (oracle) baseline that represents an upper bound on the LPP, i.e. the best performance that could be achieved if there was no misspecification. Therefore, it is just a reference and always performs better than the other methods\u2014except MLP (in theory but not in our experiments), which does not use the simulator and directly exploits the signal in the real observations, see our point about EQ3 above.\n\nIn more detail, here is how we compute the SBI baseline:\nWe take the ground-truth labels $\\{(\\theta^I\\}\\_{i=1}^N$ from the test set $\\{(\\theta^i,x^i_o)\\}\\_{i=1}^N$ on which we compute all the metrics for Figure 2; for each label $\\theta^i$, we simulate an synthetic observation $x^i_s := \\mathcal{S}(\\theta^i)$, and collect them into a \u201csynthetic\u201d test set $\\{(\\theta^i,x^I\\_s)\\}\\_{i=1}^N$; then, we apply to it the NSE+NPE pipeline (simulated posterior in Figure 1, right) to obtain the posterior estimates which we then evaluate. In this way, the baseline represents the performance we would hope to achieve if there was no misspecification and the simulator perfectly replicated the real observations (up to the stochasticity of the simulator itself)."
            }
        },
        {
            "title": {
                "value": "Authors' response"
            },
            "comment": {
                "value": "We thank you for the very valuable feedback. We hope our answers below address the major concerns you have.\n\n**The main weakness ... consider recommending acceptance.**\n\nWe would like to point you to the second paragraph in the introduction, which motivates the use of a calibration set and explains in what real-world situations we can expect to have labeled real-world observations, together with examples. As we wrote, for such real-world applications, where the result of the inference must be accurate, there will most likely be a labeled dataset so the practitioner can verify that the pipeline works and build trust in the result. \n\n**As an expert in SBI ... OT to understand what is going on.**\n\nThank you for raising this point - we will further expand this section to make it easier for non-expert readers. Do you have any more specific suggestions of what background is missing for you to better understand this part?\n\n**The applied metrics in the experiments are reasonable ... SD or some other easy to understand quantities.**\nOnce we acknowledge model misspecification, we lose access to a simulator of the \u201ctrue\u201d generative model of the real-world data and thus also to the \u201ctrue\u201d posterior distribution. For instance, for the Task E and F, relying on a limited set of real-world data, we do not have access to the \u201ctrue\u201d posterior. Thus we chose to rely on two complementary metrics that can be used in practical settings, have clear interpretations and are only optimal for the \u201ctrue\u201d posterior, if we would know it. These metrics, the ACAUC and LPP, highlight if the credible intervals cover the true parameter value at the designated frequency and the informativeness (i.e., how much uncertainty reduction do we get by using the learnt posterior distribution) to infer the parameter of the posterior respectively. To contextualise the LPP numbers, we provide the \u201cSBI\u201d and \u201cprior\u201d baselines that represent an upper bound and a lower bound on the performance of well-calibrated models, when the independence assumption made in equation 3 holds.\nNevertheless, we appreciate the suggestion for synthetic benchmarks, for which it is possible to compute the mean and SD of \u201ctrue\u201d posterior distribution. We are planning to add those results as a table in the appendix.\n\n**Curse of dimensionality ... in more detail here if possible.**\n\nIt appears non-trivial to make any formal statement on the scalability of the method as N, the number of parameters of interest, increases in general. We believe this will depend strongly on the exact simulator and real-world data considered. Nevertheless, we can reason about some corner cases.\nIn case the parameters are really independent, the worst we can do is to apply the method N times and it should lead to reasonably good results (as a function of the degree of misspecification and C, the size of the calibration set). We could also apply RoPE jointly and hope to gain data efficiency as the sources of misspecification could be common to multiple parameters and the finetuning step would thus rely on strictly more information than if it only considered one parameter at a time. A toy example would be $\\theta := \\mu \\in \\mathbb{R}^k$ and $x_s \\in \\mathbb{R}^{k\\times N} \\sim \\mathcal{N}(\\mu, 1) \\times \\dots \\times \\mathcal{N}(\\mu, 1)$ while   $x_o \\in \\mathbb{R}^{k\\times N} \\sim \\mathcal{N}(\\mu + c\\mathbb{I}, 1) \\times \\dots \\times \\mathcal{N}(\\mu + c\\mathbb{I}, 1)$. This example highlights that, in certain cases, increasing the dimensionality of the parameters can help as this also implies that the calibration set is strictly more informative on the misspecification.\n\nUnfortunately, in other interesting cases there could be dependence between parameters (given the observation) that we would like to model as well as possible. In such cases, if these dependencies are poorly described by the simulator, we would expect the curse of dimensionality to hurt us and recovering these dependencies might necessitate exponentially larger calibration sets. Nevertheless, if the dependencies are well described by the simulator, we might again benefit from the more informative calibration set."
            }
        },
        {
            "summary": {
                "value": "The paper presents a robust posterior estimation method (RoPE) for simulation-based inference (SBI) when the model is misspecified but the the underlying parameters of interest can be physically measured via (possibly expensive) experiments. RoPE uses such a calibration dataset to better quantify the posterior uncertainty under model misspecification scenarios."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper addresses a relevant practical problem in the field of SBI. The paper is largely well-written, and the experimental evaluation seems thorough. The idea of using the optimal transport coupling for learning statistics is neat (approach is similar to the work of Huang et al. (2023) who also learn statistics to be robust to model misspecification, without the calibration set of course)."
            },
            "weaknesses": {
                "value": "I have concerns regarding the soundness of the proposed method. The main claim is that RoPE provides well-calibrated uncertainty quantification, which I am not convinced of. The self-calibration property discussed in Appendix F is under the assumption that the NPE posterio is well-calibrated, which itself is not guaranteed and is often overconfident as pointed out by [1] (and therefore making NPE well-calibrated is an active area of research in SBI [2]). Moreover, the expectation in equation 10 in the proof is estimated using $N_o$ Monte Carlo samples from the test set, which I am assuming, would be limited (unless having a large test set is one of the assumptions/requirements of RoPE, in which case it should be mentioned clearly since the other methods do not have such a requirement). Basically I do not see why RoPE solves the problem being addressed. Please let me know if I am missing something.\n\n[1] Hermans et al. (2022). A Crisis In Simulation-Based Inference? Beware, Your Posterior Approximations Can Be Unfaithful. TMLR.\n[2] Falkiewicz et al. (2023). Calibrating Neural Simulation-Based Inference with Differentiable Coverage Probability. NeurIPS.\n\nLooking at how the authors define the model misspecification problem, it seems like the problem is of posterior calibration. Calling it \"model misspecification\" only makes it confusing since the definition is in terms of posteriors, and is not necessarily a property of the model (the example in Appendix A focuses on posterior bias rather than calibration). I do not see what is gained by calling this a \"model misspecification problem\", as a well-specified model may still produce mis-calibrated posteriors (mentioned in lines 92-93). I really think that the story becomes straightforward if the problem being addressed is of obtaining calibrated SBI posteriors (i.e. a \"calibration problem\"), both in cases when the model is well- or mis-specified (right?), which can be done using some additional information (in the form of calibration dataset), which is available in scenarios such as the cardiovascular system and industrial process. If the authors agree with this statement, then the more appropriate baseline for RoPE is the method of [2], instead of NPE-RS and NNPE.\n\nThe equation in line 190 is the same as equation 3 from the Ward et al. (2023) paper (NNPE method). Seems like the primary difference between the two is that RoPE uses OT to learn $p(\\mathbf x_s | \\mathbf x_o)$, while NNPE uses a spike and slab model. Please elaborate on the key differences between the two approaches, as it would help gauge the originality of this work. Also, it is not clear to me what is the role of the assumption in equation 3 then, as NNPE does not seem to have that assumption but still arrives at the same equation (correct me if I am wrong). I suppose lines 193-194 states why this assumption is needed, but it is not clear to me. Plus, there needs to be more discussion on how limiting is this assumption, in which cases is it satisfied and not satisfied, and the implications for when it does not hold.\n\nThe notation is unclear at times. For instance, what is the difference between $\\mathbb{P}^\\star$ and $p^\\star(\\mathbf x_o)$? Sometimes the author say $p^\\star(\\theta)$ is the true distribution of parameters in the real-world, but then they use $p^\\star(\\theta | \\mathbf x_o)$ which is not introduced.\n\nIn section 2.2, it would be great if you could explain what an OT coupling is and what is the general idea (in simple words) before going into the OT details. This would help the readers not familiar with OT to get a sense of what this does without needing to understand every detail.\n\nMinor comments:\n* In equation 6, I think there should be $\\varphi$ in the left-hand side instead of $\\phi$, right?\n* typo in line 817: \"i.d.d.\" instead of \"i.i.d\"."
            },
            "questions": {
                "value": "In section 3.1, the authors say that \"it is reasonable to learn a sufficient statistic $\\mathbf h_o$ by fine-tuning $\\mathbf h_{\\omega^\\star}$\". How do we know that such a fine-tuning will produce sufficient statistics? Are there any guarantees?\n\nIn most of the experiments, the performance of RoPE does not var as the calibration set size increases. Is there a reason why the performance of RoPE saturates in those cases even with $N_c = 10$? Are these very simple problems?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work contributes another contender to the family of \u201crobust\u201d methods for neural SBI, which is known to be unreliable for out-of-distribution (OOD) data. The basic idea is to use a \u201ccalibration set\u201d of observables and ground-truth measurements as an additional training signal to inform the neural network responsible for embedding the observables into a lower-dimensional representation about distribution shifts and reduce its susceptibility to OOD observables. During inference, an optimal transport (OT) cost matrix is used to derive weights for approximating the posterior over observables as a weighted average over an ensemble of posteriors obtained from the training simulations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper contributes an interesting approach to a hard problem in SBI and scientific modeling in general.\n- The OT formulation and ensemble-based correction are well motivated and the assumptions are clearly stated.\n- The evaluation in the setting of low-dimensional models is comprehensive and features good comparisons. The results are encouraging even with a very small calibration set."
            },
            "weaknesses": {
                "value": "- The basic premise of the work is that \u201cground truth\u201d parameters or latent quantities are available during training. In other words, it assumes that the simulator is parameterized in the same way as reality. This severely limits the applicability of the proposed method beyond certain domains of physics physics, as \u201ctrue\u201d latent quantities are not available or not even sought-for in a majority of applications of Bayesian inference outside of physics. Nevertheless, the authors openly acknowledge this limitation and I don\u2019t think that it is an unsurmountable dealbreaker. On a related note, this limitation extends to the formal definition of misspecification assuming there is a \u201ctrue prior\u201d, and correspondingly, a \u201ctrue posterior\u201d that represents the updated prior which is hardly actionable - the marginal likelihood $p(x_0)$ is a model-implied quantity and de Finetti\u2019s representation theorem about the existence of a factorization apples only to infinitely exchangeable sequences without any guarantees that $dim(\\theta)$ is finite. \n- The authors are quite selective in their choice of cited related work. For instance, the authors can find extensive discussions on model misspecification in likelihood-free inference in [1, 2, 3, 4, see also references therein]; [5] also discuss an alternative definition of model misspecification through the scope of the marginal likelihood, which is also explicitly discussed in [6]. [7] propose to view misspecification as latent-space distortion and correct it during inference using the reverse KL. This goes on to show that the proposed paper can profit from a slightly more in-depth discussion on conceptual and empirical approaches to dealing with model misspecification and perhaps offer a categorization of methods designed to *detect* vs. methods designed to *fix* potential errors. Additionally, reference [8] regarding universal density approximation is misleading; see [9 and references therein] for a comprehensive treatment of the university of coupling-based normalizing flows.\n- I have two concerns regarding the scalability of the method. First, it seems that the transport matrix can grow very large for settings with millions of observations and millions of simulations. This, coupled with the need to optimize the optimize the $\\gamma$ trade-off parameter makes the method attractive only for small-data, small-simulation-budget settings. Second, the empirical evaluation only considers models with extremely low parameter dimensionality $\\theta$ (i.e., the largest parameter space has 4 parameters), which is a serious limitation and leaves the question of generalizability to more complex models completely open. As such, I believe the claims of the paper to be a bit over the top. I do appreciate that the evaluations were performed on fairly large tests sets of 2000 simulations and that recent contenders and good ablations were considered.\n- The presentation of the results is rather hard to follow. I believe some of the model details (e.g., example data sets) could have been delegated to the appendix and sushi plots summarized more succinctly. I am a bit concerned by the lack of sensitivity regarding the size of the calibration set (apart from task E).\n\n[1] Nott, D. J., Drovandi, C., & Frazier, D. T. (2023). Bayesian inference for misspecified generative models. Annual Review of Statistics and Its Application, 11.\n\n[2] Frazier, D. T., Robert, C. P., & Rousseau, J. (2020). Model misspecification in approximate Bayesian computation: consequences and diagnostics. Journal of the Royal Statistical Society Series B: Statistical Methodology, 82(2), 421-444.\n\n[3] Frazier, D. T., Kohn, R., Drovandi, C., & Gunawan, D. (2023). Reliable Bayesian inference in misspecified models. arXiv preprint arXiv:2302.06031.\n\n[4] Kelly, R., Nott, D. J., Frazier, D. T., Warne, D., & Drovandi, C. (2024). Misspecification-robust sequential neural likelihood for simulation-based inference. Transactions on Machine Learning Research.\n\n[5] Schmitt, M., B\u00fcrkner, P. C., K\u00f6the, U., & Radev, S. T. (2023, September). Detecting model misspecification in amortized Bayesian inference with neural networks. In DAGM German Conference on Pattern Recognition.\n\n[6] Masegosa, A. (2020). Learning under model misspecification: Applications to variational and ensemble methods. Advances in Neural Information Processing Systems, 33, 5479-5491.\n\n[7] Siahkoohi, A., Rizzuti, G., Orozco, R., & Herrmann, F. J. (2023). Reliable amortized variational inference with physics-based latent distribution correction. Geophysics, 88(3), R297-R322.\n\n[8] Wehenkel, A., & Louppe, G. (2019). Unconstrained monotonic neural networks. Advances in neural information processing systems, 32.\n\n[9] Draxler, F., Wahl, S., Schn\u00f6rr, C., & K\u00f6the, U. (2024). On the universality of coupling-based normalizing flows. arXiv preprint arXiv:2402.06578."
            },
            "questions": {
                "value": "Can the authors explain the insensitivity of performance to the size of the calibration set?\nCan the method be extended to incorporate a less-than-perfect calibration set (e.g., as coming from a well-calibrated posterior estimator from another data set)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a method for addressing model misspecification in simulation-based\ninference (SBI)  settings. Common SBI algorithms usually assume that the simulator (the\nmodel) is well-specified, i.e., that it can simulate data close to the observed data.\nHowever, if this is not the case, the resulting approximated posterior distributions can\nbe substantially biased. This paper proposes a procedure to mitigate this problem for\nscenarios where multiple observed data points are available.\n\nThe authors propose to first follow a standard SBI approach of learning a neural-network\nbased embedding on the simulated data, followed by neural posterior estimation (NPE) on\nthe embedded data. Their key contributions consists of three additional steps. First,\nthey fine-tune the neural embedding using the set of observed data points. Second, they\nthen solve the optimal transport problem between the embedding simulated data and the\nembedded observed data. Finally, at inference time, they use the optimal transport\nsolution between the embeddings to obtain the posterior given the observed data as a\nweighted sum of the posteriors given simulated data.\n\nThe proposed method is evaluated on six benchmarking tasks for misspecification, two\nexisting tasks and four new tasks. It is compared to baselines and competing algorithms\nand performs well in most settings. Additionally, the authors show results for ablations\nand the method's behaviour for different hyperparameter settings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The paper is well-written and well-structured. The first two sections\ngive a good introduction to the topic, accurately cite previous work and clearly state\nthe contributions and the specific SBI setting the method is designed to solve. The\npaper uses two existing benchmarks and introduces four new model misspecification\nbenchmarks, which is a valuable contribution for the community. The performance of the\nproposed method is promising and the ablation and hyperparameter sensitivity studies\nhelp understanding the method and tend to make it easier for practitioners to choose\nparameter in practice."
            },
            "weaknesses": {
                "value": "The new definition of model misspecification in SBI is a bit unclear\n(see questions below) and should be clarified. Some of the benchmarking results of the\nproposed method are unclear and concerning and need clarification. In general, the\nfigures in the experiments sections are difficult to read and make the overall results\nunclear (see suggestions below).\n\n**Initial recommendation:**\n\nOverall, I believe this could be a valuable contribution for the field and I am\ntentatively recommending to accept this paper. Model misspecification is an urgent\nproblem in SBI and methods for addressing it have been scarce so far. The proposed\nmethod and additionally the proposed benchmarking tasks are a promising next step.\nHowever, there are several questions that should be clarified and several improvements\nto the visualizations of the results that should be made. The performace of RoPE on\nseveral benchmarking tasks seems problematic to me, but this might be due to a\nmisunderstanding of the ACAUC metric and needs clarification (see question 11 below)."
            },
            "questions": {
                "value": "1) Definition of misspecification in SBI: There are several parts in the definition that\nI find challenging to follow. In line 090 it says\n\n> our simulator models the relationship between the real observations xo and the\n> parameters of interest as they appear in the calibration set\n\nand for that sentence it is followed, that the current definition is insufficient\nbecause a model may be well-specified by not well-calibrated. I cannot follow this line\nof argument. Are you defining mispecification based on posterior calibration here? Can\nyou please clarify? (I have seen the example in Appendix B, but it shown only that the\nposterior is biased, not badly calibrated).\n\n2) Related to my first question, I am wondering how your definition differs from the one\n   given in https://arxiv.org/abs/1904.04551, who introduced a synthetic likelihood\n   approach for misspecified simulators. \n\n3) In line 201, $\\pi$ is introduced as a joint distribution. However, below in equation\n   (4) it is then used as conditional distribution. Can you clarify this change please? \n\n4) In line 257, RoPE$^*$ is defined with $\\tau < 1$, but how much smaller than 1 does it\n   have to be? Further below, it is set to $\\tau=0.9$. How is chosen in practice? \n\n5) In line 409, it is claimed that RoPE significantly reduces the uncertainty compared\n   to the prior. How is this measured? Is LPP your proxy for this? If so, please clarify\n   in the text. \n\n6) How is ACAUC related to the more common posterior calibration metrics like SBC or\n   expected coverage? In line 381, it says that it is zero for a perfectly specified\n   prior. Shouldn't it refer to the calibration performance of the posterior instead? \n\n7) The **SBI** defined in line 387 is unclear to me. How is it obtained precisely and\n   how can it be the upper bound on the performance in misspecified simulators? How can\n   it below the prior baseline in the ACAUC plots for some tasks? \n\n8) How can the MLP baseline work so well when it assumes a Gaussian posterior? Are the\n   benchmarking task posteriors all close to Gaussian? \n\n9) The choice of $\\gamma$ can have a substantial effect on the accuracy of the\n   posterior, tending towards the prior for too large values of $\\gamma$. This would be\n   highly undesirable in practice. How can the practitioner tune $\\gamma$ for real-world\n   applications, i.e., outside the benchmarking tasks where it seems clear what the upper\n   and lower bound of the metric are?\n\n10) General remark on the figures: Figure 1 is great, but figure 2-4 are packed quite\n    challenging to read. The dotted lines are barely visible and quite different than\n    shown in the legend. Some lines have markers, others do not. Especially in the ACAUC\n    plots it is almost impossible to tell apart the different lines. Smaller line width\n    would be a start, but a general improvement in the choice of color code and markers\n    would be even better. \n\n11) Some questions on the RoPE performance: Why does it perform so badly compared to NPE\n    on the SIR example? You mentioned that the misspecification in SIR is weaker. How\n    does RoPE perform on well-specified simulators in general then? Could this be a\n    general weakness of the method?\n    Also, RoPE seems to be generally underconfident in terms of ACAUC and this does not\n    improve with calibration set size. This is concerning as well. I think it would be\n    important to check and compare the posterior predictive distributions as well.\n\n\n**Additional feedback**\n\n- The reference to figures from the main text is a bit off. For example, figure 1\n  appears on page 2, but is referenced only on page 5. Figures 3 and 4 are referenced\n  before figure 2. \n- A lot of content is moved to the appendix. It would be better to move parts of it back\n  to main text, e.g., algorithm 1. Or give more context in the main text. For example,\n  to me it became clear only by looking at the algoritm, that at inference time, one\n  needs to run additional simulations to match the test set of observed data for\n  computing the weighted posterior."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper details with the important issue of making neural posterior estimation methods more robust against model misspecification. The authors suggest to use a small set of labeled real-world data to calibrate the posterior inference in the face of a misspecified simulator. At the moment, I see some important weakness that make me cautios at recommending acceptance at this stage. But I am open and happy to change my mind during the discussion period."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper investigates an important issue.\n- The goals of this paper are clearly explained\n- Detailed examples and ablation studies are performed."
            },
            "weaknesses": {
                "value": "- The main weakness I see is the following: How do we gain access to the *labeled* real-world observations? This seems like quite a strong assumption. I would need to understand if and when this assumption is justified before I would consider recommending acceptance.\n- As an expert in SBI, I struggle to understand the introduction of optimal transport in Section 2.2. Adding some more details there might help readers which are not already experts in OT to understand what is going on.\n- The applied metrics in the experiments are reasonable but still make it hard to see in which way we actually deviate from the targeted true posterior. The authors could perhaps also show something like bias in posterior mean and SD or some other easy to understand quantities.\n- Curse of dimensionality: These scaling issues are mentioned by the authors themselves and seem to be severe in the number of model parameters(?) I would like to see the scaling laws discussed in more detail here if possible."
            },
            "questions": {
                "value": "- I also don\u2019t really see in Section 3, where exactly the real-world labels (the parameters underlying the real data) are actually used. Can you explain this?\n- In EQ 3, you make a central assumption. Can you explain why you think this assumption is usually justified? Asked differently, when would it not be justified?\n- In equation 6: I don\u2019t fully understand the logic of why 6 is a valid approach to learn a summary network of the real data. Can you explain this to me in more detail?\n- I don\u2019t quite understand the results in relation to the LPP and ACAUC metrics:\n    - So 0 would be optimal for ACAUC, right?\n    - And for LPP higher values would be better, right?\n    - If so, SBI looks better than ROPE. I know this is probably not how these metrics should be read, so I am merely asking to clarify this point for me."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}