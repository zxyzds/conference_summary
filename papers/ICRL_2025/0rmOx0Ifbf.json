{
    "id": "0rmOx0Ifbf",
    "title": "Controlled Generation of Natural Adversarial Documents for Stealthy Retrieval Poisoning",
    "abstract": "Recent work showed that retrieval based on embedding similarity (e.g., for retrieval-augmented generation) is vulnerable to poisoning: an adversary can craft malicious documents that are retrieved in response to broad classes of queries.  We demonstrate that previous, HotFlip-based techniques produce documents that are very easy to detect using perplexity filtering.  Even if generation is constrained to produce low-perplexity text, the resulting documents are recognized as unnatural by LLMs and can be automatically filtered from the retrieval corpus.\nWe design, implement, and evaluate a new controlled generation technique that combines an adversarial objective (embedding similarity) with a \"naturalness\" objective based on soft scores computed using an open-source, surrogate LLM.  The resulting adversarial documents (1) cannot be automatically detected using perplexity filtering and/or other LLMs, except at the cost of significant false positives in the retrieval corpus, yet (2) achieve similar poisoning efficacy to easily-detectable documents generated using HotFlip, and (3) are significantly more effective than prior methods for energy-guided generation, such as COLD.",
    "keywords": [
        "Dense Retrieval",
        "Corpus Poisoning",
        "Adversarial Attack"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We design, implement, and evaluate a new controlled generation technique that combines an adversarial objective (embedding similarity) with a \"naturalness\" objective based on soft scores computed using an open-source, surrogate LLM.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=0rmOx0Ifbf",
    "pdf_link": "https://openreview.net/pdf?id=0rmOx0Ifbf",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a beam-search-based adversarial attack method for RAG, designed to produce fluent text with sentence embeddings closely matching a target embedding. Experimental results demonstrate that this approach effectively bypasses perplexity-based filtering and achieves a comparable attack success rate to HotFlip baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The proposed method is straightforward and easy to follow. \n\nExperiments are conducted on a recent dataset and compared against current baselines."
            },
            "weaknesses": {
                "value": "(1) The proposed attack method relies on simple prompts like \u201cTell me a story about\u201d to generate documents from scratch. This approach raises concerns about practical applicability, as real-world malicious users typically aim to inject misinformation. It is unclear how the proposed method would effectively introduce misinformation in a RAG setting.\n\n(2) The paper lacks experiments demonstrating how the proposed method impacts the end-to-end performance of RAG systems, such as downstream QA performance.\n\n(3) The novelty of this work is limited, as similar approaches have been applied in slightly different contexts. For example, beam search algorithms have been widely used in adversarial attacks [1][2]. The paper should discuss these related works and emphasize its unique contributions beyond altering the beam search optimization objective.\n\n> [1] Zhao et. al., Generating Natural Language Adversarial Examples through An Improved Beam Search Algorithm\n\n> [2] Liu et. al., A More Context-Aware Approach for Textual Adversarial Attacks Using Probability Difference-Guided Beam Search\n\n(4) The paper claims black-box access to the embedding encoder. However, given the assumption that embeddings can be accessed repeatedly, one could calculate gradients numerically, making the black-box claim somewhat overstated.\n\n(5) Some other minor issues:\n- Please use `\\citep` and `\\citet` in Latex properly.\n- The paper uses the gendered pronoun \"his\" for attackers (Line 109), which could be avoided.\n- The paper contains several grammatical mistakes\n- Notation definitions lack precision and could be simplified. For example, `P_n`\u200b is defined as a retrieval corpus but actually represents benign documents. The subscript `n` could be omitted."
            },
            "questions": {
                "value": "(6) Why not use a weighted sum of embedding similarity and perplexity instead of introducing an extra model?\n\n(7) Why are only true positives and false positives considered for defense? Would false negatives not be equally important?\n\n(8) Is the LLM naturalness evaluator used during the attack aligned with the one used in the evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on the problem of retrieval poisoning in modern retrieval systems. Firstly, it points out the limitations of existing methods (such as HotFlip and COLD) in generating adversarial documents. The documents generated by HotFlip have a relatively high perplexity and are easily detected; while COLD fails to generate useful texts under adversarial constraints. Then, this paper proposes a new controlled generation technique, which combines an adversarial objective (embedding similarity) with a \"naturalness\" objective calculated based on an open-source surrogate language model (LLM). The generated adversarial documents are difficult to be detected by perplexity filtering or other LLMs without generating a large number of false positives. This method has been evaluated in different scenarios such as trigger attacks and no-trigger attacks, using the MS MARCO dataset. In terms of poisoning efficacy and the naturalness of generated documents, it is superior to previous methods, but still has some limitations, such as poor transferability across encoders and the need for more research on defenses."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed adversarial decoding method is a novel controlled generation technique that comprehensively considers embedding similarity and naturalness and effectively addresses the deficiencies of existing methods.\n2. Experiments were conducted using the large-scale MS MARCO dataset, comparing different generation methods and considering two scenarios: trigger attacks and no-trigger attacks."
            },
            "weaknesses": {
                "value": "1. The transferability of adversarial documents between different encoders is poor, which limits the universality of the method.\n2. It depends on LLM and does not consider the situation of LLM hallucination. In addition, the use of LLM needs to consider the efficiency and effectiveness of the attack.\n3. The experiments are not sufficient. The experiment only considers one retriever, contriver, and other retrievers need to be compared. At the same time, the baselines need to be increased (for example, PoisonedRAG in the references)."
            },
            "questions": {
                "value": "1. How to further enhance the attack effect while improving the naturalness of adversarial documents?\n2. For different types of retrieval systems and application scenarios, does this method need to be specifically adjusted?\n3. How to better understand and quantify the \"naturalness\" indicator in order to more accurately evaluate the generated adversarial documents? Is it reasonable to rely solely on perplexity?\n4. How to consider the hallucination and efficiency problems caused by the auxiliary LLM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work points to an issue in previous retrieval poisoning attacks\u2014detectability via fluency-based defenses\u2014and addresses it by proposing a new method. Specifically, it introduces a black-box attack that uses beam search to generate adversarial passages following both the retrieval objective, and text perplexity and naturalness (i.e., the level of naturalness as judged by an auxiliary LLM) penalties. The attack shows comparable performance with prior work, while it is arguably harder to detect by standard fluency-based defenses."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The work identifies\u2013and clearly states\u2013a limitation in existing retrieval attacks, and proposes a method to address it. \n\n* As the evaluation shows, the proposed attack is harder to detect through the proposed fluency-based detectors (including perplexity and naturalness), while attaining comparable attack success to prior attacks, which further emphasizes the vulnerability of retrieval."
            },
            "weaknesses": {
                "value": "**Novelty.** The work\u2019s novelty focuses on the naturalness of adversarial documents generated by the new method. However:\n\n* The main novelty of the method\u2014enriching  the objective with LLM logit-based naturalness score (Sec. 5.2)\u2014lacks a convincing presentation (see more details below) and its current evaluation might be biased (more below), especially in light of the repetitive text shown in qualitative samples.\n* It was previously shown that the discrete text optimization\u2019s (e.g., in LLM jailbreaks) trade-off between attack success and text fluency [1] can be mitigated (e.g., [2], [3]). Specifically, similarly to this work, Sadasivan et al. [3] show LM-logit-based beam search to produce fluent text attacks. Thus, it is unclear whether this work offers a significant delta w.r.t. to previous work. \n\n**Method.** Since it is introduced as a core contribution, it would be helpful to elaborate on the soft naturalness score component in the method. This, for example, could be done by reporting on a study of alternative scores (if such were considered), or exploring the correlation between the soft naturalness score with naturalness of text.\n\n**Threat Model.** It is unclear why an attacker would aim to generate unconstrained documents (potentially meaningless) and promote their retrieval. For example, In the trigger attack is motivated by the potential of \u201cspreading adversarial content\u201d (line 112), although, to my understanding, such content is not necessarily contained in the generated documents.\n\n**Evaluation.** As the work\u2019s contribution focuses on the \u201cnaturalness\u201d of the generated documents, it would be helpful to strengthen the evaluation:\n\n* **Perplexity Filtering (Sec 7.2).** As GPT2 is a relatively dated and weak LLM (line 329), it would be helpful to additionally calculate documents\u2019 perplexity using stronger LLMs (e.g., Gemma2-2B or others), and show that the method is robust to such filtering.\n* **Naturalness Filtering (Sec. 7.3).** It seems that the naturalness evaluation for the non-basic attack (\u201cAdv\u201d) is largely done using the same LLM (Llama) used in the attack. A stronger result would be to show the generated documents are robust to naturalness filtering of different, strong, LLMs. Alternatively, one could ask LLMs for a score in a large range (e.g., 1-10), as the current prompt (asking for binary score) could possibly bias the LLM\u2019s answer Another option is reporting on a user study of their naturalness.\n* **Evaluated Model(s).** The paper evaluates the attacks against a __single__ retrieval model (namely, Contriever [4]). It should be noted that the evaluated dataset (MS-MARCO) is out-of-training-distribution for this model (Contriver was not trained on MS-MARCO [4], as opposed to most text encoders), and it was previously observed to be exceptionally vulnerable to such retrieval attacks [5]. Thus, it would be helpful to validate the results on additional models.\n\n**Presentation.** Some presentation-related comment and nits:\n* Sec. 7.3: It would be helpful to state in the text (besides the table caption) that the evaluated attack is trigger attack.\n* Fig. 1: The figure would be easier to interpret if the y-axis ticks would match the (pre-log) values from text.\n* Algorithm 1: As LLM_{logits},LLM_{naturalness} and \\lambda are all part of the algorithm parametrization, it would be clearer if these were included in the Input.\n* Algorithm 1, line 23: Shouldn\u2019t `k` be `m` (in the comment)?\n\n**References:**\n\n[1] Baseline Defenses for Adversarial Attacks Against Aligned Language Models; Jain et al. 2023.\n\n[2] FLRT: Fluent Student-Teacher Redteaming; Thompson & Sklar, 2024.\n\n[3] Fast Adversarial Attacks on Language Models in One GPU Minute; Sadasivan et al., ICML 2024.\n\n[4] Unsupervised Dense Information Retrieval with Contrastive Learning; Izacard et al., TMLR 2022.\n\n[5] Poisoning Retrieval Corpora by Injecting Adversarial Passages; Zhong et al., EMNLP 2023."
            },
            "questions": {
                "value": "* It seems that the attack algorithm is given a prefix prompt (per line 186), however, unless I missed it, it is not mentioned in the text (Sec. 5). Could you clarify what is the role of this prefix and how it is chosen?\n* Results in Sec 7.4, Table 3 (e.g., HotFlip, Top-20, 0.01; with 500 passages), seem to contradict those originally reported by Zhong et al. 2023 [5] on the same dataset and model (e.g., Top-20, 98% with 50 passages). It would be helpful to clarify this discrepancy.\n* In line 243, it is mentioned that the no-trigger attack is tested against 1K queries. Are these disjoint from the 50K attacked set (similar to the trigger attack\u2019s evaluation)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the vulnerability of modern retrieval systems that rely on embedding similarity. The authors highlight that existing adversarial methods, such as HotFlip, generate malicious documents with high perplexity, making them easily detectable through perplexity filtering and large language model (LLM) evaluations. To address this, the paper introduces a novel controlled generation technique called AdversarialDecoding, which simultaneously optimizes for embedding similarity and naturalness using a surrogate LLM. This approach produces adversarial documents that maintain low perplexity and appear natural, effectively evading both perplexity-based and LLM-based detection mechanisms. Experimental results on the MS MARCO dataset demonstrate that AdversarialDecoding achieves high poisoning success rates comparable to traditional methods while significantly reducing the likelihood of detection. Additionally, the study explores the limited transferability of these adversarial documents across different encoders, suggesting potential avenues for developing robust defenses. The research underscores the importance of advancing defensive strategies to safeguard retrieval systems against sophisticated adversarial attacks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper introduces a novel controlled generation method named AdversarialDecoding, which uniquely integrates embedding similarity with a \"naturalness\" constraint. By leveraging a surrogate large language model (LLM) to compute soft scores, the method simultaneously optimizes for semantic relevance and textual naturalness.\n\n- The methodology presented in the paper is robust and meticulously designed. The authors conduct comprehensive experiments using the MS MARCO datasets, and give a lot of ablation studies."
            },
            "weaknesses": {
                "value": "- The writing in this paper could be **improved a lot**. Firstly, each formula lacks numbering. Additionally, the citation format in lines 251-258 seems off. Moreover, line 123 ends with a comma, and line 130 lacks a period. These issues are quite frequent in the article, which suggests a need for more attention to detail.\n\n- In some experiments, using llama3.1-8b as both the adversarial decoding model and the naturalness evaluation model could raise concerns about fairness. This is because llama3.1-8b might be biased towards the data it generates itself. Besides, could you explain why you use GPT2 to measure the perplexity of generated adversarial documents rather than GPT3 or other LLMs?\n\n- The selected baselines are limited. Hotflip is an early character-level adversarial attack method, but since then, many more effective attack algorithms[1] have been developed, whether at the word, character, or sentence level. These newer methods often result in much higher fluency.\n\n- Adding some additional human evaluations would be valuable.\n\n**References:**\n\n[1] https://github.com/thunlp/TAADpapers"
            },
            "questions": {
                "value": "See the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses the vulnerability of retrieval systems based on embedding similarity to poisoning attacks. The authors demonstrate that previous techniques, such as HotFlip, produce documents that can be easily detected using perplexity filtering. They introduce a new controlled generation technique that combines an adversarial objective (embedding similarity) with a \"naturalness\" objective based on soft scores from an LLM. The proposed method aims to generate adversarial documents that cannot be automatically detected using perplexity filtering or LLM-based \"naturalness\" filtering without incurring significant false positives, while maintaining similar poisoning efficacy. The authors evaluate their approach on the MS MARCO dataset and show that their method outperforms prior techniques like energy-guided decoding (COLD) and is more effective than HotFlip in generating stealthy adversarial documents."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper presents a novel approach to generating natural adversarial documents for retrieval poisoning. Combining an adversarial objective with a \u201cnaturalness\u201d objective based on soft scores from a surrogate LLM is novel. This addresses the limitations of previous methods that produced easily detectable adversarial documents.\n- The methodology section explains the proposed adversarial decoding method in detail and the \"Algorithm 1 Adversarial Decoding\" is clear. The experimental setup and results are also presented in a clear and organized manner.\n- The work is significant as it addresses an important issue in modern retrieval systems. The ability to generate stealthy adversarial documents has implications for the security and integrity of retrieval-augmented generation and other applications that rely on retrieval systems."
            },
            "weaknesses": {
                "value": "Methodology:\n\n- **Dependence on Surrogate LLM**: The proposed method's reliance on a surrogate LLM for computing the naturalness score has a drawback. It significantly raises the computational cost because computing $s_{natural}$ demands the calculation of the LLMs' output logits, which is more costly than computing the similarity score. This could limit the method's practical application, especially when dealing with large datasets. I would expect a runtime comparison between their method and baselines, or to discuss potential optimizations to reduce computational cost.\n- **Single Prompt Optimization**: Optimizing adversarial documents based on only a single prompt (\u201cIs this text unintelligible?\u201d) restricts their robustness.\n- **Insufficient Evaluation of LLM Detection Evasion**: One of the three \u201cnaturalness\u201d filtering prompts (\u201cIs this text unintelligible?\u201d) is identical to the attacker's prompt, and the other two are semantically similar. This resembles a \"data leakage\" situation, in my opinion. The perplexity-based filtering is also the case (the attacker and defender both use GPT-2). I expect a more comprehensive evaluation using a wider variety of prompts and different LLMs to accurately determine the method's ability to evade detection. \n- **Generalizability across Different Retrievers**: Given the relatively low transferability of adversarial decoding across different retrievers, more experiments on different retrievers are needed to verify the effectiveness of the proposed method.\n\nPresentation:\n\n- Figure 1 is too large, in my opinion. It might be better to present two figures (e.g., one for trigger attack and one for non-trigger attack) horizontally. \n- The table's caption should be placed before the table."
            },
            "questions": {
                "value": "- Q1: In line 378, The author stated \"Table 2 shows that evasion of detection transfers to two other prompts and another LLM\". It is confusing as table 2 does not seem to include the results for other prompts and LLMs. So where is the result for evasion of detection on other prompts and LLMs?\n\n- Q2: In experiment setup, The author said \"To evaluate 'naturalness' of adversarial and real documents, we prompt GPT-4o and LLaMA-3.1-8B with these prompts\". But where is the result of GPT-4o filtering?\n\n- Q3: In table 2, at the same threshold, increasing the width of the beam search actually increases the true positive rate of LLM-based naturalness filtering (0.5 -> 0.7), which means more adversarial document is filtered. This is very strange to me. In my opinion, increasing the width of beam search should be able to find better solutions (i.e., more stealthy adversarial documents) and therefore less likely to be detected."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}