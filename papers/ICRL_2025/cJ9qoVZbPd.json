{
    "id": "cJ9qoVZbPd",
    "title": "Locate-then-Unlearn: An Effective Method of Multi-Task Continuous Learning for Large Language Models",
    "abstract": "Nowadays large language models (LLMs) have achieved remarkable success in various NLP tasks. However, they often misinterpret human instructions and generate incorrect or outdated responses, highlighting the need for more effective continual learning techniques. While recent efforts have introduced unlearning methods to remove erroneous knowledge, existing approaches still struggle in multi-task learning scenarios.\n\nTo overcome these limitations, we propose \\textbf{locate-then-unlearn}, a novel framework that identifies and selectively unlearns task-specific neurons to enable efficient multi-task learning. We hypothesize that LLM neurons can be broadly categorized into task-specific neurons for handling individual tasks, and general neurons to maintain the model\u2019s foundational capabilities. To accurately identify task-specific neurons, we utilize a two-stage locating process: (1) ranking task-related neurons based on their importance to each task, and (2) identifying task-specific neurons by applying intervention to assess how neuron activity impacts task performance, isolating those most critical to each task. We conduct comprehensive evaluations in two experimental setups: single-task specialization and multi-task generalization. The results show that our method significantly improves performance across both settings. This indicates that our method effectively balances model efficiency and accuracy in multi-task continual learning.",
    "keywords": [
        "Machine unlearning",
        "Continue learning",
        "Model editing"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cJ9qoVZbPd",
    "pdf_link": "https://openreview.net/pdf?id=cJ9qoVZbPd",
    "comments": [
        {
            "title": {
                "value": "This paper violates the double-blind review process and is questionable"
            },
            "comment": {
                "value": "**Dear reviewers and chairs,**\n\nThere are some serious concerns about the academic integrity of this paper under review.\n\nSome people on Twitter have raised issues about the authors violating the double-blind review process by revealing their submission ID and author list on a Chinese social media: https://x.com/saulgoodma1995/status/1856616262039957581?s=12.\n\nUpon reading the paper, I believe it exhibits more substantial academic issues. Below, I outline my arguments:\n\n> 1. Lack of Originality in Neuron Locating Method\n\nThe paper claims to propose a \"two-stage locating method\" for identifying task-specific neurons (Line 90). However, this approach is very similar to the method presented by Chen et al. [1], which also involves a two-stage process\u2014first identifying **knowledge-related** neurons, and then filtering those that store factual knowledge **transcending** language. The authors of this paper first locate \u201c**task-related**\u201d neurons, then filter task-specific neurons, because \u201csome task-related neurons may exhibit high logit scores **across multiple tasks**. (Line 70)\u201d The authors replace the concept of \"language\" with \"task\" without citing Chen et al.\n\nFurthermore, the neuron identification method (Eq.2) in this paper is **identical** to Eq.4 of Tang et al. [2]. The authors replace \"language\" in the original paper with \"task\" once again. Here is Equation 4 from Tang et al.:\n\n$ p_{i,j}^k = \\mathbb{E} \\left( \\mathbb{I}(\\text{act fn}(\\tilde{h}^i W_1^i)_j > 0) \\mid \\text{language } k \\right), $\n\nThe authors state, \"the definition of a neuron follows Tang et al. (Line 143),\" but they do not attribute Eq.2 to Tang et al.'s contribution. Such wording seems intended to mislead reviewers unfamiliar with Tang et al.\n\n> 2. Continual Learning vs Lifelong Editing\n\nAt first, I did not understand why they called this model editing method continual learning. However, upon closer inspection, particularly of Eq.5, it becomes clear that the subtraction between parameters of two models is a common practice in lifelong model editing (see Eq.3 in Wang et al. [4]). Nevertheless, the authors do not cite any relevant lifelong editing literature, raising concerns about proper attribution.\n\n> 3. The Style of Figures Demonstrates That the Authors Have Read the Papers I Listed\n\nThis paper does not relate to attention modules at all, yet the authors still include attention modules in Figure 1. Why? This figure closely resembles Figure 1 from Meng et al. [5], with the authors largely following the figure style from this classic paper on model editing. Additionally, the top-right part of Figure 1 in this paper uses elements similar to Figure 1 of Tang et al., and the structure of Figure 1(a) and (b) in this paper is similar to that of Figure 2 in Chen et al. (Figure 1(a) and (b) illustrate the two-stage neuron identification and recap that this two-stage process was proposed by Chen et al.)\n\nWhile reusing classic figures is not inherently problematic, the resemblance suggests that the authors have read these papers but still underplayed their contributions in this paper.\n\n> Confusing Equations\n\nEquation 3 in the paper uses a superscript in the summation notation that is unclear. I wonder whether the reviewers understand this superscript\u2014is it n or k?\n\nIf it is k, as k represents a cardinal task index, the meaning of \"i < k\" is ambiguous.\n\nIf it is n, the summation term is a constant! It is strange to subtract a constant in this metric. However, based on the related works cited, I believe this formulation may have been borrowed from work in mechanistic interpretability, such as the circuit discovery methods in Lv et al. cited in this paper, which calculate the logit difference and normalize it with the average logits over the vocabulary to measure activation differences between two inputs. It is an odd mistake to reuse such a measure for probability differences in this paper because averaging the probability difference over the distribution results in a constant.\n\nIn summary, I believe that the paper demonstrates substantial overlap with prior works in both methodology and presentation, with some clear mistakes and without proper citations or acknowledgment. \n\nI appreciate the reviewers' efforts, but for the integrity of the ICLR, I hope the reviewers and chairs will consider the issues I have raised and re-evaluate the paper. \n\n**Given the violation of the double-blind process, this paper should perhaps be desk-rejected directly.**\n\n[1] Chen et al. Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons\n\n[2] Tang et al. Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models\n\n[3] Geva et al. Transformer Feed-Forward Layers Are Key-Value Memories\n\n[4] Wang et al. LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models\n\n[5] Meng et al. Locating and Editing Factual Associations in GPT"
            }
        },
        {
            "summary": {
                "value": "This work proposes to unlearn task-specific neurons to enable more efficient multi-task learning. The proposed approach, locate-then-unlearn, first locates task-relevant neurons, deduplicates the neurons that are relevant for multiple tasks, subtracts the corresponding parameters to unlearn false answers, and relearns the objective tasks.\n\nThe proposed approach shows higher performance than five baseline methods on five datasets in the multi-task learning scenario, and improvements on three out of five datasets in the single-task setting (with the remaining two getting close performance with direct full fun-tuning)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* This work presents solid experimental results and ablation studies, showing the effectiveness of each component.\n\n* The main novelty lies in finding task-specific neurons for more efficient re-learning."
            },
            "weaknesses": {
                "value": "* The causal effect between the neurons and different tasks requires more careful examination. Although the approach shows empirical improvement,  the assumption that neurons are task-specific might require further evidence.\n\n* This approach is dependent on the selected data used to locate neurons (section 3.2.) and unlearn tasks (section 3.3). The authors provide several ablation studies on the method components, while it may be worthwhile to conduct ablations on the choice of data."
            },
            "questions": {
                "value": "* What is the definition of \u201ctasks\u201d in this work? The distribution of data, even under the same tasks, seems to affect the selection of task-specific neurons. Thus, whether improvement will transfer to different datasets from the same tasks is unexamined.\n\n* How are the toy datasets constructed in section 3.2? Are they a subsample of the underlying task data?\n\n* Regarding the design of false answers in section 4.3., the false knowledge can be presented in multiple forms than what\u2019s already in the dataset, which could thus not capture the potential wrong knowledge and influence the effect of unlearning. \n\n*Typos*\n\n* Title: Continuous -> Continual\n\n* L409-410: implement -> implementing; this sentence is difficult to parse."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes locate-then-unlearn, to identify and unlearn task-specific neurons in transformer models for multi-task learning. Specifically, the authors propose to identify task-related neurons from top-k important scores for each task, and identify task-specific neurons by removing overlapping neurons shared by different tasks. Finally, the authors fine-tune the selected task-specific parameters with unlearning data, and remove the parameter offset (from fine-tuning) from the original parameters. Experiment results demonstrate that the proposed locate-then-unlearn outperform other knowledge editing baselines on five datasets. They also conduct extensive analysis to demonstrate that the setting of unlearning hyperparameter is important, task-specific neurons work the best on corresponding related tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The method is intuitively reasonable. By detecting task-related and task-specific neurons, the methods make multi-task learning more parameter-efficient.\n- The experiment results show that the proposed method can outperform knowledge editing methods such as ROME and MEMIT on five datasets.\n- The authors conduct extensive analysis. The unlearning hyperparameter has a best performing peak, while the threshold is rather robust. The task-specific neurons are important, and tuning them on unrelated tasks lead to worse results. Visualization shows that task-specific neurons are well-separated.\n- The writing of the paper is quite clear."
            },
            "weaknesses": {
                "value": "- Since the authors claim that their methods contribute to parameter-efficient multi-task learning, it would be better to compare with methods such as prompt-tuning, lora-hub.\n- It would be better if the authors could release their code upon acceptance."
            },
            "questions": {
                "value": "- How does the method compare with other PEFT methods, such as prompt-tuning, lora-hub?\n- It would be better if the authors can discuss for each new task, how many parameters need to be stored (the percentage of the whole transformer model)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns are found."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Locate-then-Unlearn framework to improve LLM continual learning by identifying and selectively unlearning task-specific neurons. By localizing neurons specific to individual tasks and unlearning them, the approach reduces interference between tasks and enhances model efficiency. Experiment results and analyses across multiple datasets demonstrate good performance in both single-task and multi-task settings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed approach is novel.  \n2. The proposed approach is well-motivated.  \n3. The results show good performance."
            },
            "weaknesses": {
                "value": "1. One problem of LLM continual learning is catastrophic forgetting, it would be better if the authors can further discuss how the proposed approach handle the issue.  \n2. For LLM continual learning, it would be better to compare model performance on the benchmark datasets before and after training, so the readers will learn the effect of the learning process on previously learnt task. Currently, the authors just report the performance after training. It would be a plus if the authors can include the model performance before training.  \n3. The paper claims many times that the proposed approach \"significantly\" outperforms existing approaches, but it seems the significance test is missing. So, it is hard to justify whether the improvement is significant."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}