{
    "id": "YcML3rJl0N",
    "title": "Calibrating LLMs with Information-Theoretic Evidential Deep Learning",
    "abstract": "Fine-tuned large language models (LLMs) often exhibit overconfidence, particularly when trained on small datasets, resulting in poor calibration and inaccurate uncertainty estimates. \nEvidential Deep Learning (EDL), an uncertainty-aware approach, enables uncertainty estimation in a single forward pass, making it a promising method for calibrating fine-tuned LLMs. However, despite its computational efficiency, EDL is prone to overfitting, as its training objective can result in overly concentrated probability distributions. \nTo mitigate this, we propose regularizing EDL by incorporating an information bottleneck (IB). Our approach IB-EDL suppresses spurious information in the evidence generated by the model and encourages truly predictive information to influence both the predictions and uncertainty estimates. Extensive experiments across various fine-tuned LLMs and tasks demonstrate that IB-EDL outperforms both existing EDL and non-EDL approaches. \nBy improving the trustworthiness of LLMs, IB-EDL facilitates their broader adoption in domains requiring high levels of confidence calibration.",
    "keywords": [
        "evidential deep learning; information bottleneck; calibration; large language models"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "This paper introduces an information-theoretic regularization for evidential deep learning, substantially enhancing the calibration of fine-tuned large language models.",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=YcML3rJl0N",
    "pdf_link": "https://openreview.net/pdf?id=YcML3rJl0N",
    "comments": [
        {
            "summary": {
                "value": "The paper present a method to effectively apply Evidential Deep Learning (EDL), a parametric uncertainty estimation approach, to LLMs. The approach introduces two features to standard EDL that make it practical and effective for application to LLMs; i) the introduction of an information bottleneck cost function, which mitigates over-fitting and ii) the use of the LLM outputs as the intermediate \"layer\" Z where to impose the bottleneck, which simplifies direct computation of certain terms in the mutual information lower bounds, part of the objective function."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Relevance: The paper addresses an important problem; the uncertainty calibration of LLM outputs (at the token level), which is an important component for providing safe deployment and error estimation for LLMs.\n\n- Theoretical Soundness: The paper takes a very principled approach, identifying precise issues in the application of EDL to LLMs and addressing them with mathematically derived solutions based on reasonable starting assumptions.\n\n- Experimental Soundness: The experiments are more than sufficiently extensive: they target the right settings and metrics to validate the claims of the paper and comprehend great breadth of models, tasks and mode of evaluation (acc+ECE, OOD detection). The claims of improved uncertainty calibration are strongly backed by the experiments."
            },
            "weaknesses": {
                "value": "The only main weakness of the paper is the clarity of scope:\n\nFrom the experiments, it is clear that this is a method for uncertainty calibration for a specific task at the fine-tuning stage of an LLM. e.g., an LLM is fine-tuned for summarisation and the proposed method offers a way to obtain superior uncertainty calibration *for the fine-tuned task*. This is indeed very useful, but needs to be stated more clearly in the abstract and intro, as from all sections up until the experiments it is unclear if this is a method for pre-training and/or fine-tuning. In principle, I can see the method may be applied to pre-training too, but since this is not tested in the experiments it should not be included in the claims."
            },
            "questions": {
                "value": "See and clarify the weakness point above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes IB-EDL, combining Evidential Deep Learning (EDL) with information bottleneck (IB) for uncertainty estimation in large language models (LLMs). This approach aims to enhance model calibration by reducing overconfidence, which is a common issue in fine-tuned LLMs. The experimental results show that IB-EDL achieves improved calibration with minimal computational overhead."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-presented and easy to follow.\n\nThe paper proposes a unified perspective on EDL methods by framing several existing EDL methods as special cases within the IB-EDL framework.\n\nThe experimental results are strong and covers multiple LLMs, showing that IB-EDL addresses the overconfidence problem well."
            },
            "weaknesses": {
                "value": "The pipeline of applying IB to EDL ($x \u2192 f(x; \u03b8) \u2192 e\u02dc \u2192 e \u2192\n\u03b1 \u2192 \u03c0 \u2192 y$) seems computational heavy, it would be informative to include an inference time comparison of IB-EDL against other methods. \n\nAlthough the training overhead of IB-EDL is relatively small compared with the pretraining step, the paper lacks comparison of training time compared with LoRA, which is the fine-tuning backbone of IB-EDL.\n\nFor out-of-distribution (OOD) scenarios, the paper only compares the OOD detection capabilities with AUROC. The expected calibration error (ECE) is missing, which is also an indicator of OOD performance as the calibrated model should maintain relatively low ECE score on OOD datasets.\n\nThe novelty is not clear for combining IB with EDL, which are two well established methods."
            },
            "questions": {
                "value": "Please see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this work, authors propose a new loss regularization term to reduce overconfidence in LLMs when training via the evidential deep learning (EDL) framework. This regularization is based on the information bottleneck loss, which maximizes mutual information between token logits and ground truth prediction distribution while minimizing M.I. between predictions and inputs. This is achieved by minimizing a KL divergence between a learned conditional logit distribution and a Gaussian prior. This work demonstrates that fine-tuning via this loss produces better calibration (ECE, AUC) with equivalent accuracy to baselines. Authors also illustrate this method is robust to noise in the training data and distribution shifts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* This work is well-motivated theoretically, pointing out deficiencies with previous EDL losses (e.g. paragraph *Challenges when applying IB to an internal layer of an LLM*). \n* The final regularization term $\\mathcal{L}_{IB-Info}$ seems relatively simple, although I do have some questions about practical implementation (see questions)\n* Strong experimental results; the OOD detection and noise injection experiments (tables 3,4) in particular seem support the argument for this additional regularization during calibration"
            },
            "weaknesses": {
                "value": "* This work seems only applicable for a discrete set of mutually exclusive classes. I'm not sure how this extends to the case of open-ended generation where multiple responses may be semantically equivalent or have different logical relationships.\n* While experiments show improvements in OOD detection when moving from OBQA--> ARC,CSQA datasets, I'd be interested to see the performance when the datasets are 'futher apart' semantically (e.g. moving from a reading comprehension to a math task)."
            },
            "questions": {
                "value": "* While it appears the training methods/baselines tested are normalized for number of training steps, I am curious about comparison in terms of the number of training flops. Do the new mean and covariance prediction heads require separate training? \n* Is there a benefit to having two parameters estimating the distributional shift (mean and variance) of the token predictions vs. just one?\n* What happens to calibration if we finetune on multiple datasets simultaneously?\n* How does this fine-tuning procedure effect LLM performance on open-ended generation tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents the Information Bottleneck (IB) - Evidential Deep Learning (EDL) method, designed to regularize EDL for better calibration of large language models (LLMs). The empirical experimental results demonstrate that IB-EDL enhances calibration in fine-tuned LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-written and easy to understand, trying to address the important task of calibrating fine-tuned large language models. The empirical experiments are comprehensive, testing across multiple LLMs including Llama2-7B, Llama3-8B, and Mistral-7B. The paper covers most of the baselines. The LoRA-based Mixture of Experts idea [1] could also be relevant and serve as a baseline for comparison. The empirical performance appears promising.\n\n[1] Li, Dengchun, et al. \"Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts.\""
            },
            "weaknesses": {
                "value": "The novelty of the paper is limited, as both EDL and the IB method are well studied in the literature. The primary contribution of the paper appears to be the implement IB on EDL. The authors explain that the IB objective allows the latent variables to retain the most predictive information about the target variable while discarding irrelevant information from X. However, a more detailed discussion on how this relates to improved calibration of LLMs is missing. Additionally, there is a lack of theoretical guarantees supporting this approach. It is beneficial to provide a theoretical bound on calibration error or a proof of convergence.\n\nIn terms of the empirical analysis, the paper highlights computational efficiency as a key contribution of the proposed method, yet it lacks experimental comparisons of computational time and memory costs to validating this claim. The paper also lacks the calibration performance analysis in out-of-distribution scenarios."
            },
            "questions": {
                "value": "Please refer to the strengths and weaknesses. Additionally, how many bins did the authors use to compute the ECE? It is beneficial to conduct a sensitivity analysis on the number of bins."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}