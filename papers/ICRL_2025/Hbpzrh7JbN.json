{
    "id": "Hbpzrh7JbN",
    "title": "Saturn: Sample-efficient Generative Molecular Design using Memory Manipulation",
    "abstract": "Generative molecular design for drug discovery has very recently achieved a wave\nof experimental validation, with language-based backbones being the most common\narchitectures employed. The most important factor for downstream success is\nwhether an *in silico* oracle is well correlated with the desired end-point. To this end,\ncurrent methods use cheaper proxy oracles with higher throughput before evaluating\nthe most promising subset with high-fidelity oracles. The ability to directly optimize\nhigh-fidelity oracles would greatly enhance generative design and be expected to\nimprove hit rates. However, current models are not efficient enough to consider such\na prospect, exemplifying the sample efficiency problem. In this work, we introduce\n**Saturn**, which leverages the Augmented Memory algorithm and demonstrates the\nfirst application of the Mamba architecture for generative molecular design. We\nelucidate *how* experience replay with data augmentation improves sample efficiency\nand *how* Mamba synergistically exploits this mechanism. Saturn outperforms 22\nmodels on multi-parameter optimization tasks relevant to drug discovery and may\npossess sufficient sample efficiency to consider the prospect of directly optimizing\nhigh-fidelity oracles.",
    "keywords": [
        "generative design",
        "drug discovery",
        "sample efficiency",
        "language models",
        "reinforcement learning",
        "scaling"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "state-of-the-art sample efficiency using mamba for generative molecular design",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Hbpzrh7JbN",
    "pdf_link": "https://openreview.net/pdf?id=Hbpzrh7JbN",
    "comments": [
        {
            "title": {
                "value": "Author Response (1/1)"
            },
            "comment": {
                "value": "We thank the reviewer for their feedback and the detailed questions around areas of insufficient clarity. For questions that we address in the General Response, we will explicitly note this. \n\n## **Q1: Math notation and terminology lacks clarity**\n\nWe apologize for the insufficient clarity. We have added explanations in the main text around \u201clocal sampling\u201d and \u201caugmentation rounds\u201d which we have highlighted in the updated manuscript. Regarding the math notation for the optimization problem, Saturn operates in a reinforcement learning framework where the optimization goal is to maximize the expected reward (equation 2 in the main text). In Appendix B.4, we show that maximizing the reward is equivalent to minimizing equation 4 in the main text. We could not include this full section in the main text due to space constraints.\n\n## **Q2: Mamba network details**\n\nWe will include this information in the Appendix in the next version of the manuscript (next few days). We wanted to first release initial responses.\n\n## **Q3: What is the novelty of this work?**\n\nPlease see **General Response 4.**\n\n## **Q4: Why are oracle calls deterministic? Why is Mamba better than RNN/decoder transformer?**\n\nPhysico-chemical property oracles are inherently deterministic. For example, in the QED oracle which includes molecular weight and number of hydrogen bond donors, this never changes. These are intrinsic molecular properties. Where there can be stochasticity is in simulation oracles, such as docking, as used in the work. In docking, a search over conformation space is done which can have stochasticity. We minimized this by fixing the seed and we have added a passage to explicitly say this in the updated manuscript. For a discussion on Mamba compared to RNN and decoder transformer, please see **General Response 4 Sub-response 2.**\n\n## **Q5: What are we trying to convey in the paper? Computational consumption of Mamba vs. RNN/decoder transformer**\n\nFor the findings in this paper, please see General Response 4. There is essentially no difference in the computational consumption of these models because we operate in such small model sizes. The only difference is pre-training, for which Mamba and Decoder transformer can take 1-2 hours longer but this is done once. For sampling, it is true that RNN samples faster than Mamba and Decoder transformer but the difference is milliseconds so practically negligible. This is true in our setting because we are modeling small molecule SMILES sequences (< 80 tokens typically). These are extremely short sequences when put in the context of LLM modeling.\n\n## **Q6: Messy math notation**\n\nIn the preceding text to equation 1, we define x as a SMILES sequence and theta as the parameters of the Agent (the generative model).\n\n\n## **Q7: Better description of Augmented Memory**\nPlease see **General Response 4 Sub-response 1.** We have added this extra clarity in the Appendix.\n\n## **Q8: Saturn performs worse than GEAM and then uses dissimilar data fine-tuning but does not do the same for GEAM**\n\nPlease see **General Response 2.**\n\n\\\n\\\nWe are thankful for the reviewer\u2019s time and apologize for any insufficient clarity. We hope to have answered their questions. We kindly ask the reviewer to consider raising their evaluation of our work.\n\nPlease let us know if you have any questions!"
            }
        },
        {
            "title": {
                "value": "Author Response (1/1)"
            },
            "comment": {
                "value": "We thank the reviewer for their feedback and their interest in the prospect of optimizing MD oracles. This is also exciting for us and achieving this is a goal for us! We will answer each question individually. For questions that we address in the General Response, we will explicitly note this. \n\n## **Q1: Trade-off in diversity may limit capability**\nPlease see **General Response 3.**\n\n## **Q2: Innovation of the model is limited**\nPlease see **General Response 4** regarding the contribution of this work.\n\n## **Q3: Report actual generation time.**\n\nThe benchmarking experiments are in Part 3 of the paper. The strongest baseline we compare to is GEAM [1] and the benchmark experiment itself is based on GEAM\u2019s case study. The optimization task is docking against 5 targets and we ran each target across 10 seeds (0-9 inclusive). The total wall time for Saturn across all 50 experiments (5 targets, 10 seeds each) which were run sequentially, was ~41.5 hours on an NVIDIA A6000 GPU. ***This information is in section F.1. in the Appendix.*** Therefore, each experiment was, on average < 1 hour.\n\nFor GEAM, we now add the wall times below (***we also newly add this information to section F.2 in the Appendix***):\n\n**braf:** 3.28 \u00b1 0.04 hours\n**jak2:** 3.26 \u00b1 0.05 hours\n**fa7:** 3.38 \u00b1 0.04 hours\n**5ht1b:** 3.17 \u00b1 0.08 hours\n**parp1:** 3.02 \u00b1 0.19 hours\n\nThe mean and standard deviation are reported across 10 seeds, except for parp1 which is across 7 seeds. The remaining 3 seeds were run on CPU due to insufficient GPU resources. CPU times are much longer so we did not include these, as that would be unfair. For more transparency, In GEAM, we used NVIDIA V100 GPUs due to cuda compatibility with their codebase. NVIDIA V100 GPUs are slower than A6000 GPUs. According to this benchmark: https://www.aime.info/blog/en/deep-learning-gpu-benchmarks/, A6000 was about 50% faster. It is hard to say exactly the difference in speed for this task, though. ***However, regardless, actual generation time for Saturn should still be faster than GEAM.***\n\n## **Q4: Why did we store the top-100 molecules in the buffer? Is lower diversity a problem?**\n\nIn the original Augmented Memory [2] work, the authors found minimal difference from decreasing and increasing the buffer size. Decreased diversity during generation is ***by design*** because it can improve sample efficiency (we refer to **General Response 4 Sub-response 1**). We emphasize that we have done extensive ablations in Appendix C so if more diversity were desired, one can increase batch size and decrease augmentation rounds. The consequence of decreased diversity, however, is that Saturn may generate repeat molecules, but we introduced oracle caching such that only \u201cnew\u201d generated molecules need to be sent to the oracle.\n\n## **Q5: Report diversity metrics. Is low diversity a problem?**\n\nFor a discussion on diversity, we refer to **General Response 3**. We want to emphasize that we are very transparent that Saturn trades off diversity for sample efficiency. Throughout the paper, we have always reported diversity metrics, specifically IntDiv1 (internal fingerprint similarity) and #Circles (packing number). For a qualitative analysis of the generated molecules, we also show examples in Fig. F7 in the Appendix.\n\nWe are grateful for the reviewer\u2019s time and hope we have answered their questions. We are especially thankful for the comments around generation time as we believe this is a very practical question. We kindly ask the reviewer to consider raising their evaluation of our work.\n\nPlease let us know if you have any questions!\n\n\\\n\\\n[1] GEAM: https://arxiv.org/abs/2310.00841\n\n[2] Augmented Memory: https://pubs.acs.org/doi/10.1021/jacsau.4c00066"
            }
        },
        {
            "title": {
                "value": "Author Response (2/2)"
            },
            "comment": {
                "value": "## **Q4: Saturn-Tanimoto/GEAM-Tanimoto**\n\nWe refer to **General Response 2.**\n\n## **Q5: Why is Saturn sample-efficient? How is data augmentation performed?**\n\nRegarding why Saturn is sample-efficient, we refer to **General Response 4.**\n\nWe apologize for the insufficient clarity around data augmentation. In Saturn, data augmentation is SMILES augmentation. SMILES are string-based representations of molecules and Saturn, using a Mamba backbone, learns a token distribution of these strings. Given a molecular graph, where the nodes are atoms and edges are bonds, a corresponding SMILES string can be obtained by designating a \u201cstart node (atom)\u201d and performing a depth-first search (DFS). The node traversal order yields a SMILES representation. If the starting node were changed, a different SMILES representation results. This is SMILES augmentation. From an implementation perspective, we use RDKit to shuffle atom numbers of the Mol object to return different SMILES representations of the same molecular graph. ***We have included this excerpt in the newly uploaded manuscript highlighted in Appendix C.2.***\n\n\n## **Q6: Which contributions are ours?**\nPlease see **General Response 4.** From the methods section, the Mamba backbone, oracle caching, and genetic algorithm are new. However, we believe that the important contribution from our work is understanding optimization dynamics in generative design and how to predictably tune Saturn to yield different sampling behavior/control the types of chemistry being generated.\n\n\nWe are grateful for the reviewer\u2019s time and hope we have answered their questions. We hope to have conveyed that Saturn is a valuable step towards high-fidelity oracle optimization and kindly ask the reviewer to consider raising their evaluation of our work.\n\nPlease let us know if you have any questions!\n\n\\\n\\\n[1] GEAM: https://arxiv.org/abs/2310.00841\n\n[2] Review Paper: https://www.nature.com/articles/s42256-024-00843-5\n\n[3] TNIK Phase 2: https://www.nature.com/articles/s41587-024-02143-0\n\n[4] gnina: https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00522-2"
            }
        },
        {
            "title": {
                "value": "Author Response (1/2)"
            },
            "comment": {
                "value": "We thank the reviewer for their feedback and their interest in the prospect of optimizing MD oracles. This is also exciting for us and achieving this is a goal for us! We will answer each question individually. For questions that we address in the General Response, we will explicitly note this. \n\n## **Q1: Limited technical novelty**\n\nPlease see **General Response 4.** We believe the insights into underlying mechanisms of molecular optimization and its effect on the generated molecules are valuable to the field. Many of the models we compared to were published in recent years at NeurIPS, ICLR, and ICML. **GEAM [1] (the strongest baseline) was published at ICML 2024.** In the general response, we hope to have conveyed the contributions of Saturn. We appreciate that the reviewer believes the work is mostly empirical. We only want to comment that in the end, developing generative models should be based on their utility in real world tasks. The most important empirical result is that Saturn can optimize multi-parameter optimization tasks under minimal oracle calls (**we refer to the Strict Hit Ratio results, where GEAM performs considerably worse**), which is a very important practical problem. We hope this, together with showing direct DFT optimization, is convincing to the reviewer that strong empirical results are a valuable contribution as we work towards making generative models more capable for real life applications.\n\n## **Q2: No high-fidelity oracle experiments**\n\nPlease see **General Response 1** for an example of direct high-fidelity oracle optimization. We emphasize that moving to DFT/MD oracles is an enormous jump (going from CPU/GPU minutes to ***hours***). We also do not think docking is a toy example as it is used in every single structure-based drug discovery generative campaign that has achieved experimental validation [2]. In fact, in commercial drug discovery, a generated molecule has just (***as of this week***), achieved good results in Phase IIa clinical trials. The design of this molecule used docking [3]. Moreover, certain parameters in docking can be relatively computationally expensive, such as allowing for some flexible residues and performing more exhaustive pose searching. A concrete example is gnina [4] docking. The developers recently achieved very good results on the CACHE challenge (given a target, propose molecules that the CACHE organization synthesizes and tests). ***We give these specific examples to reinforce that docking is not a toy example, and they are actually used in real life.***\n\n## **Q3: Saturn has similar performance to GEAM and why hyperparameter tuning of GEAM was not performed**\n\nWe refer to **General Response 2** regarding the performance comparison between Saturn and GEAM and that Saturn optimizes the multi-parameter optimization objective to a much greater degree (Strict Hit Ratio in the main text). ***Importantly, GEAM computed the oracle values for the entire ZINC 250k dataset to pre-train. This imposes an up-front cost of 250k oracle calls.***\n\nRegarding hyperparameter tuning of GEAM [1], in the GEAM paper, **their main results** are the docking tasks against **parp1**, **fa7**, **5ht1b**, **braf**, **jak2**. While not explicitly stated in their paper, it is reasonable to believe the authors already tuned their model on this task. In our Part 3 experiments, we mimicked GEAM\u2019s experiments **exactly** (the pre-training data and oracle code was directly taken from their codebase). ***We want to strictly emphasize that we did not tune Saturn on this task.*** We only investigated Saturn\u2019s hyperparameters in the Part 1 toy task and then fixed them for all docking experiments in the paper (Part 2 and Part 3 results). We explicitly note this at the end of Part 1 and we copy the text here:\n\n***\u201cFrom here on, this model configuration will be referred to as Saturn and hyperparameters are fixed such that all performance metrics in the following sections are out-of-the-box\u201d***\n\nWe now want to comment on hyperparameter tuning. This is not feasbile in real life. In real life, given an optimization objective, generative models are directly applied to generate suitable molecules. In commercial drug discovery, chemists will not wait for a model to be tuned for every single optimization task. This is even more problematic when the oracle is computationally expensive. It is highly impractical to tune hyperparameters for every objective. We want to end this response by saying that we believe developing generative models should be focused on improving downstream real life applications, and not necessarily for getting bolded numbers by tuning hyperparameters on the exact task. Assuming GEAM did not tune their hyperparameters on ***their*** task, tuning them would likely improve performance but the same could be said for Saturn."
            }
        },
        {
            "title": {
                "value": "Author Response (1/1)"
            },
            "comment": {
                "value": "We thank the reviewer for their feedback and their interest in the prospect of optimizing MD oracles. This is also exciting for us and achieving this is a goal for us! We will answer each question individually. For questions that we address in the General Response, we will explicitly note this. \n\n## **Q1: Optimizing MD simulations**\n\nPlease see **General Response 1** which also shares preliminary results on direct Density Functional Theory (DFT) optimization, which can sometimes be even more expensive than MD (depending on the type). We would just like to take this opportunity to emphasize that moving from docking to MD is an enormous jump, and proper experimentation takes time (***and also access to sufficient GPU compute resources***). We have made a genuine effort in thoroughly benchmarking Saturn (***we ran every single experiment across 10 seeds***) and compared to > 20 models, including extremely strong baselines such as GEAM [1].\n\n## **Q2: Does setting the reward to 0 for over-represented scaffolds limit the generator\u2019s learning?**\n\nGiven the local sampling behavior of Saturn, where generated molecules across similar Generator states can differ only by a few atoms, removing instances of over-represented scaffolds in the buffer is necessary. ***The purpose of this purging is to prevent detrimental mode collapse.*** If this is not done, the Generator can get stuck and only generate the same molecules repeatedly. In the original Augmented Memory [2] work, this was shown to a lesser extent. In that work, when the authors did not perform this purging, mode collapse was observed and due to no oracle caching mechanism (Saturn uses caching), this made Augmented Memory intolerable at higher augmentation rounds (how many times to learn from the augmented SMILES). In the original work, it was problematic to move beyond 2 augmentation rounds. The addition of oracle caching (store and re-use oracle evaluations), though straightforward, enables Saturn to drastically increase the number of augmentation rounds and still not lead to detrimental mode collapse. In Appendix C, we performed hyperparameter studies with augmentation rounds up to 20. \n\nNext, we want to comment on \u201clearning from useful scaffolds\u201d. In Saturn, Bemis-Murcko scaffolds are used which consider heavy atoms. Given the same scaffold, swapping any heavy atom with another heavy atom results in a different Bemis-Murcko scaffold. Therefore, purging over-represented scaffolds in the buffer does not prevent Saturn from learning from very similar scaffolds. The choice to use Bemis-Murcko scaffolds is to have this leniency. \n\nWe are grateful for the reviewer\u2019s time and hope we have answered their questions. We would also like to take this opportunity to highlight the DFT experiment we showed in **General Response 1.** All works that have used DFT as an oracle, use single-point DFT, which does not optimize the geometry of the molecule at the DFT level of theory. In our experiment, we showed for the first time, the direct optimization of full geometry in generative design. We hope to have conveyed that Saturn is a valuable step towards high-fidelity oracle optimization and kindly ask the reviewer to consider raising their evaluation of our work.\n\nPlease let us know if you have any questions!\n\n\\\n\\\n[1] GEAM: https://arxiv.org/abs/2310.00841\n\n[2] Augmented Memory: https://pubs.acs.org/doi/10.1021/jacsau.4c00066"
            }
        },
        {
            "title": {
                "value": "Author Response (3/3)"
            },
            "comment": {
                "value": "We are really grateful for the reviewer\u2019s time and their feedback. In particular, their suggestion for a 2D plot to visualize the diversity/sample efficiency trade-off was particularly helpful for us to better convey our results. We would also like to take this opportunity to highlight the DFT experiment we showed in **General Response 1**. Existing works that optimize a notion of DFT in generative design, use single-point DFT, which does not optimize the geometry of the molecule. In our experiment, we showed for the first time, the direct optimization of full DFT geometry in generative design. We hope to have conveyed that Saturn is a valuable contribution towards high-fidelity oracle optimization and understanding optimization dynamics. \n\nPlease let us know if you have any questions and we would be grateful if the reviewer would consider raising their evaluation of our work!\n\n\\\n\\\n[1] Diversity Filter: https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00473-0\n\n[2] REINVENT 1.0: https://jcheminf.biomedcentral.com/articles/10.1186/s13321-017-0235-x\n\n[3] REINVENT 2.0: https://pubs.acs.org/doi/10.1021/acs.jcim.0c00915\n\n[4] Augmented Memory: https://pubs.acs.org/doi/10.1021/jacsau.4c00066\n\n[5] REINVENT 3.2: https://github.com/MolecularAI/Reinvent\n\n[6] GraphGA: https://pubs.rsc.org/en/content/articlelanding/2019/sc/c8sc05372c\n\n[7] GEAM: https://arxiv.org/abs/2310.00841\n\n[8] DrugEx2: https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00561-9\n\n[9] RetMol: https://arxiv.org/abs/2208.11126\n\n[10] Fragment Retrieval at NeurIPS 2024: https://f-rag.github.io/"
            }
        },
        {
            "title": {
                "value": "Author Response (2/3)"
            },
            "comment": {
                "value": "## **Q8: UMAP trajectories**\n\nWe performed the UMAP embeddings on the Mamba and RNN generated molecules sets separately because they start from (slightly) different pre-trained distributions (based on stochasticity of pre-training - the same dataset was used). The plot is meant to qualitatively depict how similar generated molecules are, as the generative experiment progresses. In Fig. 2e, we quantitatively show that the Mamba generated molecules are indeed more similar. We also show the UMAP for RNN with 10 augmentation rounds in Fig. D5 in the Appendix. With increased augmentation rounds, the RNN sampling trajectory looks a lot more like Mamba. However, we additionally quantitatively assess the similarity in the same Fig. D5 in a heatmap and show that Mamba is still more similar. This is in agreement with the ability of \u201coverfit\u201d distribution more, being advantageous for sample efficiency in the settings in the paper. \n\n## **Q9: is GraphGA impairing the quality of the replay buffer molecules?**\n\nThank you for this question - it was very interesting for us to study GraphGA\u2019s effect during Saturn development! We would like to first discuss why we even used GraphGA. Saturn\u2019s mechanism of optimization involves SMILES augmentation on the Replay Buffer molecules. In our view, we want to populate the Replay Buffer as fast as possible, because they store the highest reward molecules. As Saturn performs local exploration, we want to find high reward molecules as fast as possible to facilitate this local exploration. We provided our perspective on diversity in **General Response 3** that we think too much onus is placed on it sometimes, simply because it is a metric reported in literature. However, we do believe diversity is important in many cases and we used GraphGA to jointly interrogate diversity recovering (which in the end, is the proposed use in Saturn) and its effect on Replay Buffer populating. GraphGA [6], first proposed in 2019 is actually used as is, as a component in many generative models. ***In fact, GEAM [7] directly uses GraphGA.*** Interestingly, in 2021, a generative model using RL explicitly mentions using an evolutionary algorithm for the purpose of promoting diversity [8]. It is expected that a genetic algorithm can promote diversity, given the crossover and mutation operations, but we were also interested in its effect on sample efficiency, given an already sample-efficient model. ***We now answer how GraphGA affects the Replay Buffer.*** In Tables 24 and 25 in the Appendix, we report sample efficiency and diversity metrics when activating GraphGA in Saturn. We investigated how to select the offspring (random or Tanimoto dissimilarity to the Replay Buffer). Selecting by dissimilarity (to the Replay molecules) may promote more diversity, which was our hypothesis. The summary of the results is that choosing randomly outperforms choosing by dissimilarity. This was somewhat expected given that similar molecules, on average, have similar properties. Molecules more dissimilar to the Replay Buffer molecules would be less likely to possess good properties. In the tables, we report **\u201cBuffer Replace\u201d** (how many GraphGA molecules replaced molecules in the buffer? Which means how many were better than at least the top-100 molecules by reward.) and **\u201cBuffer Best\u201d** (how many times were GraphGA molecules the best in the buffer throughout the run?). ***The summary of the findings is that GraphGA generated molecules do indeed replace the buffer molecules but rarely are they the best.*** In the end, activating GraphGA leverages its crossover and mutation operations to recover diversity but does not improve sample efficiency. We now answer the reviewer\u2019s question regarding higher fidelity methods to breed Replay Buffer molecules. Recent work [9, 10] showed that fragment retrieval can be beneficial for sample efficiency. This would entail choosing similar molecules or fragments to those present in the Replay Buffer and using these as a higher-fidelity breeding method/replacement method. This would require a priori known fragments though. This being said, this is an area we are also interested in as future work and we thank the reviewer again for this comment."
            }
        },
        {
            "title": {
                "value": "Author Response (1/3)"
            },
            "comment": {
                "value": "We thank the reviewer for their feedback and questions on interrogating diversity. We will answer each question individually. For questions that we answer in the General Response, we will explicitly note this. \n\n## **Q1: Diversity reduction is a weakness**\n\nPlease see **General Response 3** for a discussion around diversity in generative molecular design.\n\n## **Q2: Bemis-Murcko filter does not mitigate diversity reduction**\n\nBased on **General Response 3**, the diversity reduction is by design, but if more diversity is desired, one can increase batch size and decrease augmentation rounds. We comment more specifically on what the Bemis-Murcko filter is doing. The Bemis-Murcko is based on the \u201cDiversity Filter\u201d, first introduced in 2020 [1] as part of the REINVENT [2, 3] generative model. At the time, what this filter did was keep track of the scaffold generated in the run and if a scaffold is generated too many times (by some threshold), all future generated molecules possessing such scaffold have its reward truncated to 0. This heuristic helped REINVENT generate more diverse molecules. Augmented Memory [4] builds on REINVENT 3.2 [5] and in the original work, the authors found that this Diversity Filter is insufficient to prevent mode collapse. In **General Response 4 Sub-Response 1**, we describe in detail what Augmented Memory is doing. By learning from the same molecule too many times, mode collapse can occur which causes the generative model to generate the same molecules repeatedly. When this happens, the generative model is no longer generative. Therefore, in the Augmented Memory work, the authors introduced \u201cSelective Memory Purge\u201d which also purges the Replay Buffer of all molecules possessing penalized scaffolds. This was enough to rescue mode collapse. In Saturn, we adopt this mechanism and ***the purpose of the Bemis-Murcko filter is to prevent detrimental mode collapse.*** In Table 2 of the main text, we report the IntDiv1 on the molecules, which shows, again by design, that Saturn generates more high reward molecules possessing unique Bemis-Murcko scaffolds (Scaffolds metrics) than Augmented Memory, at a trade-off of diversity (IntDiv1 metric).\n\n## **Q3: Mamba is overfitting. Can diversity be recovered to RNN level?**\n\nOur argument and results at around Line 292 is that Mamba overfits the distribution of augmented SMILES. This is done by design because it improves sample efficiency. We refer to **General Response 4 Sub-Response 1** again. Similar to our response to the previous question, increasing batch size and lowering augmentation rounds will recover diversity at the expense of sample efficiency (in all the case studies in the paper). For quantitative metrics, we cross-reference Appendix C for the tables on pages 29-30, where we report the trade-off between **Yield** and **Oracle Burden** sample efficiency metrics with **IntDiv1** and **Scaffolds** metrics for diversity.\n\n## **Q4: Introduction should include QSAR models**\n \nWe have added an additional sentence to introduce QSAR models:\n\n***\u201cWe note that QSAR models are often used, which can have great predictive accuracy, but may suffer from a narrow domain of applicability (within their training data)\u201d***\n\n## **Q5: MD oracle**\nPlease see **General Response 1.**\n\n## **Q6: 2D summary plot of sample efficiency-diversity trade-off**\n\nThank you for this suggestion. We have added a new 2D plot for Mamba batch size 16 (since these are the parameters of Saturn) in the Appendix (Fig. C3).\n\n## **Q7: What does squeezing the likelihood of augmented SMILES mean?**\n\nThe likelihood of a SMILES string is the negative log-likelihood (NLL) of its token sequence. Please see **General Response 4 Sub-response 1** for more details on Saturn\u2019s optimization mechanism. Here, we will focus on what \u201csqueezing\u201d means. Given a SMILES, augmentation means generating a set of augmented SMILES. Every single augmented SMILES equates to a different token sequence and hence, different likelihood. In Fig. 2c, we show that executing Augmented Memory [4] makes the augmented SMILES more likely. This is shown by the delta NLL. In the main text, we expressed that the softmax function saturates, such that already ***likely*** SMILES, even if they exhibit a large loss, do not become ***much more likely***. Squeezing the likelihood means all the ***unlikely*** augmented SMILES converge towards the most likely one."
            }
        },
        {
            "title": {
                "value": "General Response (6/6)"
            },
            "comment": {
                "value": "### **3. Local sampling and optimization landscapes encountered in drug discovery**\n\nBuilding on the last point in the previous sub-response: Saturn performs well via local sampling - is beneficial for all optimization tasks? We have shown how strategic overfitting and model scaling leads to local sampling behavior that can improve sample efficiency for all docking case studies in the paper. We approach this question in 2 scenarios:\n\n***(1) Provided that local sampling is desired, what do our results suggest for interesting future work?***\n\n***(2) Provided that local sampling is not desired, what do our results suggest for interesting future work?***\n\nIf local sampling leads to high sample efficiency as is the case in this work, then paralleling LLMs, scaling up pre-training can yield benefits because we can start from a better initial distribution. A better initial distribution can allow the model to find a small set of \u201cgood\u201d molecules as fast as possible, and then locally explore. Indeed, recent work has shown that moving beyond the common ZINC 250k (about 250k molecules) and ChEMBL (about 2 million molecules) to PubChem (about 100 million molecules) for pre-training can directly lead to higher sample efficiency, **all else fixed** [15, 16]. While this may seem obvious in light of LLMs, chemistry data can be quite different and often you should be prudent in combining data from different sources [17]. We pre-train because we believe that the pre-training data is useful for our task. For a counter-example, consider a recent ICLR 2024 paper [18] which shows that pre-training on more suitable, but less data, is better than just more data. This is expected and PubChem, containing many bioactive molecules, is often suitable, as shown by the cited works [15, 16]. In this case, future work can investigate pre-training for generative design, for instance work at ICLR 2024 [19]\n\nWhat if local sampling is not desired? Through all the ablation experiments performed in our work, we have shown ***how*** batch size and augmentation directly control the exploration-exploitation trade-off and we have also shown how these hyperparameters affect the type of chemistry generated (as discussed in sub-reponse 1, we cross-reference Fig. D6 in the Appendix). We can ***predictably control*** the sampling behavior of Saturn to control for the types of chemistry (local atom changes or not) we want to generate.\n\nTo conclude, we believe that the development of Saturn has led to valuable insights into what components can lead to a sample-efficient model. We have studied the effect of architecture scaling and its implications on sample efficiency and the diversity of results. Beyond this, we have proposed a model that achieves state-of-the-art sample efficiency when compared to **GEAM (ICML 2024)** which is the strongest baseline and on their optimization task **and without a pre-training scheme that requires 250k oracle calls.** Throughout the entire study, we have made an effort to be rigorous, performing every experiment for 10 replicates (10 seeds, 0-9 inclusive so no cherry-picking seeds). We hope our general response is useful in clarifying Saturn\u2019s contributions.\n\nPlease let us know if you have any further questions!\n\nSincerely,\n\nThe Authors\n\n\\\n\\\n[1] MFBind: https://arxiv.org/abs/2402.10387\n\n[2] REINVENT + MD surrogate: https://pubs.acs.org/doi/10.1021/acs.jctc.4c00576\n\n[3] Review Paper: https://www.nature.com/articles/s42256-024-00843-5\n\n[4] AQFEP: https://pubs.acs.org/doi/10.1021/acs.jctc.4c00399\n\n[5] HOMO-LUMO: https://pmc.ncbi.nlm.nih.gov/articles/PMC10569544/#pone.0283271.ref056\n\n[6] D3 dispersion: https://pubs.aip.org/aip/jcp/article/132/15/154104/926936/A-consistent-and-accurate-ab-initio\n\n[7] B3LYP: https://journals.aps.org/prb/abstract/10.1103/PhysRevB.37.785\n\n[8] B3LYP: https://pubs.aip.org/aip/jcp/article/98/7/5648/842114/Density-functional-thermochemistry-III-The-role-of\n\n[9] def2-TZVP: https://pubs.rsc.org/en/content/articlelanding/2005/cp/b508541a\n\n[10] def2-TZVP: https://pubs.rsc.org/en/content/articlelanding/2006/cp/b515623h\n\n[11] GEAM: https://arxiv.org/abs/2310.00841\n\n[12] Metabolite similarity: https://link.springer.com/article/10.1007/s11306-014-0733-z\n\n[13] Orphan drug similarity: https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-5\n\n[14] Augmented Memory: https://pubs.acs.org/doi/10.1021/jacsau.4c00066\n\n[15] Mol2Mol: https://www.nature.com/articles/s41467-024-51672-4\n\n[16] REINVENT Transformer: https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00887-0\n\n[17] Combining chemistry data: https://pubs.acs.org/doi/10.1021/acs.jcim.4c00049\n\n[18] Pre-training data in RL: https://openreview.net/forum?id=nqlymMx42E\n\n[19] JMP pre-training: https://openreview.net/forum?id=PfPnugdxup"
            }
        },
        {
            "title": {
                "value": "General Reponse (5/6)"
            },
            "comment": {
                "value": "### **2. Model-intrinsic and scaling properties that lead to improved sample efficiency**\n\nBuilding on the last sentence in the previous section, \u201cintentional overfitting can benefit sample efficiency\u201d, we perform deeper investigations to answer 2 questions:\n\n***(1) Is Mamba intrinsically better than RNN and decoder Transformer?***\n\n***(2) How does scaling improve architectures?***\n\nWe begin our response by cross-referencing Table 1 in the main text which contrasts RNN (5.8M), Decoder transformer (6.4M), and Mamba (5.2M). The key metric we want to highlight is **\u201cRepeats\u201d**. This tracks how many repeat molecules were generated throughout the run. Note that Repeat molecules do not need to be scored by the Oracle. **Why is Mamba generating so many more repeat molecules compared to RNN and decoder Transformer?** This is because it \u201coverfits\u201d the distribution when Augmented Memory is applied. This is also what is being shown in Fig. 2a which we also referenced in the previous section: Mamba approaches mode collapse. This sub-response so far only conveys the observation that under the current model sizes, Mamba can capitalize on \u201coverfitting\u201d for sample efficiency. ***But is this Mamba intrinsic?*** To answer this question, we scaled up the RNN (from 5.8M to 24.7M) and decoder Transformer (from 6.4M to 25.3M) for use in Part 3 experiments. ***Does scaling up the other architectures close the gap to Mamba?*** Yes, and we show this in two ways. First, all models need to be pre-trained (on ZINC 250k). The loss with larger model sizes converges to lower NLL (maybe not exactly surprising, but we report the numbers for completeness):\n\n* **RNN (5.8M):** NLL = 30.88\n\n* **Mamba (5.2M):** NLL = 28.10\n\n* **RNN (24.7M):** NLL = 29.32\n\n* **Decoder Transformer (25.3M):** NLL = 26.96\n\n***How does this translate to sample efficiency?*** Using these larger models, we ran Part 3 experiments again across 10 seeds (0-9 inclusive). ***All the results are in Appendix F.6 Saturn: Architecture Scaling.*** The results show that the larger models have notably better sample efficiency with RNN (24.7M) still worse than Mamba (5.2M) but Decoder Transformer (25.3M) about the same - these larger models once again trade off diversity. This is the expected behavior. As we have shown that local sampling can improve sample efficiency, a model that can fit distributions better directly translates to such sampling behavior. Does this guarantee that this local sampling behavior is beneficial for all optimization tasks? No, but in all generative literature to date, docking tasks with physicochemical property modulators are used and we have shown through 8 docking targets that this indeed is beneficial. We believe these insights are interesting and directly lead to our next sub-response. We end this sub-response by asking: ***if larger RNN and Decoder Transformer can close the gap between Mamba, is Mamba still advantageous?*** We believe so because Mamba can use just 5.2M parameters, thus requiring less GPU memory. This is practically useful, especially if other oracles require GPU memory (GPU docking being a prime example). ***Mamba (5.2M) requires only ~2GB GPU memory to run so it fits on basically any GPU.*** Light-weight models that can be tuned quickly are practically advantageous. Finally, we note that one could scale up the model to even larger but we want to highlight that the pre-training data only contains ~250k molecules (ZINC 250k), unlike the classic LLM paradigm. Larger models require more compute resources and we chose to allocate more compute resources to run the Part 3 experiments across 10 seeds for thoroughness."
            }
        },
        {
            "title": {
                "value": "General Response (4/6)"
            },
            "comment": {
                "value": "## **Novelty: What is the contribution of our work?**\n\n* **Reviewer XhFy:** Saturn just applies Augmented Memory but with Mamba\n\n* **Reviewer f1gd:** The novelty of this paper seems to be limited to the application of Mamba\n\nWhile to the best of our knowledge, we are the first to apply the Mamba architecture for molecular generation, we do not believe this is the main contribution of our work. In the Introduction, we stated the following three additional contributions:\n\n* Elucidate the mechanism of Augmented Memory [14] \n\n* Comprehensively evaluate language model backbones to show model-intrinsic and scaling properties that lead to improved sample efficiency.\n\n* Demonstrate local sampling in chemical space can be a key component for sample efficiency. Our results provide discourse on the nature of optimization landscapes commonly encountered in drug discovery.\n\nWe omit discussion here on the empirical results as the reviewers above cite this as a strength. **Instead, we focus our discussion here on the deeper architectural and optimization insights in the context of molecular design.**\n\n### **1. Elucidating the mechanism of Augmented Memory and showing its effect at both the RL and chemistry levels**\n\nIn the original Augmented Memory work, the authors only showed its empirical benefits. In this sub-response, we will first discuss the steps of Augmented Memory, and then discuss its effect during RL, and finally show that by understanding this mechanism, we can have granular control over the \u201ctype\u201d of chemistry that is generated. \n\n**Augmented Memory.** Given a molecular graph, a SMILES representation is generated by picking a node (atom) and performing a depth-first search (as is done in RDKit). The traversal order yields **a specific SMILES form**. Starting from another node results in an alternative SMILES form. **This is known as SMILES augmentation** and the key fact is that **all augmented SMILES map to the same molecular graph.** This non-injective property means that if many SMILES forms of the same molecular graph are generated, **the same reward can be assigned to them.** This is what allows Augmented Memory to \u201clearn from the same molecule many times\u201d. In the original Augmented Memory work, the authors showed that the act of SMILES augmentation is vital and that if only one single SMILES form is used and learned repeatedly from, detrimental mode collapse occurs. This results in the model generating only 1 molecule, thus losing generative ability. ***What does this have to do with Saturn?*** In Part 1, we studied the exact mechanism of what Augmented Memory is actually doing: by learning (via RL) from different SMILES forms, it becomes more likely to generate any SMILES representation of the same molecular graph. In Part 1, we did a sub-experiment where we took all the SMILES in the Replay Buffer and performed 10x SMILES augmentation on them, resulting in 1,000 SMILES. We ran Augmented Memory on these SMILES and then computed the difference in their likelihoods of being generated before and after Augmented Memory. We cross-reference Fig. 2c which shows that after 1 round of Augmented Memory, the NLL shift can be quite large, indicating increased likelihood of generating the augmented SMILES forms. ***What does this mean at the chemistry level?*** If it becomes likely to generate some SMILES representation of the same molecular graph, then small changes in the SMILES sequences (via token sampling stochasticity) results in only minor changes to the molecular graph. We cross-reference Fig. D6 in the Appendix which shows that unique molecules generated at similar Agent states share significant similarity, often differing only by a few atoms. **This means that Saturn performs local sampling around \u201cgood\u201d molecules and is the reason why there is a trade-off in diversity.** This local sampling behavior can even interrogate the neighborhood of \u201cgood\u201d molecules, which is important in drug discovery for structure-activity relationships (SAR). Recent work even trains models to do this task for use in commercial drug discovery [15]. Finally, this set of results shows the SMILES augmentation is actually quite an effective regularizer - despite learning from the same molecular graph so many times, the model does not suffer from detrimental mode collapse. We cross-reference Fig. 2a which takes Agent states across 20 generation epochs (at every epoch, Augmented Memory is run), and computes the average max token sampling probability: during generation, track the highest probability token at each step and then take the average. The Figure shows that Saturn ***approaches*** mode collapse (if the average generation probability = 1), but does not do so which is extremely important, as the generative ability of the model is retained. **The final message we convey in this sub-response is that Saturn intentionally overfits on augmented SMILES which directly leads to improved sample efficiency by trading off diversity.**"
            }
        },
        {
            "title": {
                "value": "General Response (3/6)"
            },
            "comment": {
                "value": "## **3. Clarifications around diversity**\n\n* **Reviewer SmbD:** Main weakness is diversity reduction\n\n* **Reviewer feFf:** Does removing over-represented scaffolds limit the generative ability?\n\n* **Reviewer XhFy:** Would tweaking the hyperparameters of GEAM be able to trade-off diversity for improved sample efficiency? If so, would it outperform Saturn?\n\n* **Reviewer u4yG:** Trading off diversity may affect the capability for exploratory molecular design\n\nFor the specific reviewer questions regarding diversity and its effect on generative ability/GEAM, we will respond in the individual responses. Here, we discuss diversity as a general metric.\n\nIn molecular design literature, it is common to report a notion of diversity, via IntDiv1 (internal pair-wise Tanimoto similarity) and/or #Circles (packing number), which we report both in our work. However, aside from reporting a metric, ***why exactly*** do we want diversity? Typically, in real life projects, generated sets of molecules are clustered and then representative molecules (with optimal properties) selected to be subjected for MD simulations and finally prioritized for experimental validation. Diversity is often desired because proxy oracles (like docking) can lead to many false positives. Having other \u201cgood\u201d molecules can essentially act as back-up plans. If we now consider the prospect of directly optimizing higher-fidelity oracles, they should in principle lead to less false positives. There is literature precedent for this [4 is an example]. In this scenario, \u201chigh\u201d diversity may not actually be required because we have much higher confidence in our oracle. Perhaps in such an event, we are interested in the number of oracle calls required to generate molecules with optimized oracle values. ***This is why we report Oracle Burden (# oracle calls required to generate N unique molecules above a reward threshold).*** Our sub-response so far is to offer some discourse on diversity and ***treating it as an actionable objective so it is aligned with real life use***, rather than just as a metric that is reported. \n\nNext, we discuss the diversity reduction and that it is ***by design.*** In Appendix C, we performed extensive ablation experiments to ultimately demonstrate that low batch size and high augmentation rounds leads to higher sample efficiency at the expense of diversity. ***What is the model doing?*** It is performing local exploration, generating molecules that may differ only by a few atoms. Please see Fig. D6 in the Appendix where we explicitly illustrate this with molecular structures. We find that for all docking case studies, purposely trading off diversity for sample efficiency is beneficial. ***If desired, one can increase batch size and decrease augmentation rounds to recover diversity.*** We show this explicitly in Appendix C, and reference the IntDiv1 values reported in the ablation experiments. \n\nAt a very high level, what do we want from a generative model? We want molecules that possess optimal properties. We then pose a question: ***Would it be more advantageous to generate less diverse molecules that optimize the multi-parameter optimization (MPO) objective to a greater extent, than generating more diverse molecules that only moderately optimize the MPO objective?*** We argue that optimizing the MPO objective is most important, because we want optimal molecules. This is exactly what our Part 3 experimental results are commenting on. In our comparison with GEAM [11], we followed their MPO objective (jointly optimize docking, SA score, QED). Following their Hit Ratio metric, they filter molecules with SA score < 5 (**optimal is 1**) and QED > 0.5 (**optimal is 1**). This means optimal molecules according to the MPO objective should have much lower SA score than 5 and much higher QED than 0.5. This is why we introduced the Strict Hit Ratio which filters SA score < 3 and QED > 0.7. In Table 5, we show that most of the molecules generated by GEAM actually do not pass these thresholds. ***Whether or not low SA and high QED translates to real life success is not the point of this strict filter. It is to highlight that a generative model should be able to generate optimal molecules according to the MPO objective function.***"
            }
        },
        {
            "title": {
                "value": "General Response (2/6)"
            },
            "comment": {
                "value": "## 2. **Saturn and GEAM: Performance and Tanimoto dissimilarity**\n\n* **Reviewer XhFy:** Adding Tanimoto dissimilarity to GEAM and make the comparison\n\n* **Reviewer f1gd:** Saturn performs worse than GEAM so Tanimoto dissimilarity was introduced. Why was this not done for GEAM?\n\nWe want to first clarify the results between Saturn and GEAM around performance. **Reviewer f1gd** states that Saturn performs really bad compared to GEAM. We first acknowledge that GEAM is a strong baseline and was the main reason we compared to them (although their final publication date is almost concurrent work, ICML 2024). \n\nWe want to begin this response by highlighting that GEAM proceeds by first constructing an initial fragment vocabulary, which is trained on ZINC 250k. ***Every single molecule in ZINC 250k was actually computed by the oracle to enable this step.*** If the oracle were particularly expensive, this makes \u201cpre-training\u201d GEAM also very expensive. This is very different to the unsupervised pre-training scheme of Saturn. We are only maximizing the likelihood of re-constructing ZINC 250k molecules during pre-training. ***This does not require any oracle calls.*** So while we evaluated Saturn and GEAM both on 3,000 oracle calls, GEAM\u2019s pre-training step actually technically incurs 250k oracle calls. ***We hope by this fact alone, that reviewers can view Saturn\u2019s sample efficiency more favourably.*** We continue with our response.\n\nTo compare exactly with GEAM, we followed their case study and used the provided pre-training data and oracle code in their codebase. GEAM reports results for 2 metrics: \n\n* **Hit Ratio:** better docking score than reference ligand, QED > 0.5, SA < 5\n\n* **Novel Hit Ratio:** Hit Ratio with the added constraint that the molecule < 0.4 Tanimoto similarity to the pre-training data (ZINC 250k)\n\nIn our work, we further defined:\n\n* **Strict Hit Ratio:** better docking score than reference ligand, QED > 0.7, SA < 3\n\n* **Strict Novel Hit Ratio:** Strict Hit Ratio with the Tanimoto similarity constraint\n\n***Why did we enforce stricter thresholds?*** Because jointly optimizing docking score, QED, and SA score is the objective.\n\nWe now discuss the **Hit Ratio** results. In Table 3, we show Saturn outperforms or matches GEAM. However, investigating further with the **Strict Hit Ratio** in Table 5 shows that GEAM does not generate optimal molecules according to the optimization objective. The majority of the generated molecules do not possess QED > 0.7 and SA < 3. ***But are we just defining an arbitrary metric to make Saturn look better?*** No, and we cross-reference GEAM\u2019s reward function, which we also write out in the Saturn paper as equation 5. The objective is simply to jointly optimize all these metrics. Molecules with 0.5 QED and 5 SA cannot give maximum reward.  ***Therefore, this set of results is showing that Saturn is optimizing the objective to a much greater extent. In generative design, we want optimal molecules.***\n\nNext, we discuss the **Novel Hit Ratio** results. **Reviewer f1gd** has expressed that Saturn performs much worse than GEAM. In Table 4, Saturn indeed has much lower **Novel Hit Ratios** than GEAM. ***We would first like to pose the question, why do we want generated molecules to possess less than 0.4 Tanimoto similarity to the pre-training data?*** The argument is that we want dissimilarity to generate \u201cnew\u201d molecules. We want to first motivate that this threshold is somewhat arbitrary. In 2014, a study found that > 90% FDA approved drugs have > 0.5 Tanimoto similarity to human metabolites [12]. An example of one of these metabolites is \u201ccarnosine\u201d which is present in ZINC (ZINC2040854 and ZINC8583964). We note that it is not present in ZINC 250k, but had the pre-training data been another fraction of ZINC, it could be present. This paper suggests that similarity to these molecules might actually be desired for bioactive safety. ***In this case, we may not even necessarily want particular dissimilarity.*** Conversely, though, another study in 2014 suggested that Tanimoto similarity < 0.5 could be used to distinguish \u201csimilar to non-similar\u201d pairs of molecules for orphan drug registration [13]. ***In this case, dissimilarity may be desired.*** We wanted to highlight this because similarity to some reference set of molecules is not necessarily bad, and that the metric is somewhat arbitrary. Now answering the question explicitly, we wanted to show that if this was a desired constraint, that it is straightforward to adapt Saturn to satisfy it by simply running a ***few minutes*** of optimizing dissimilarity to the pre-training data.\n\nThe reviewer suggested running \u201cGEAM Tanimoto\u201d but GEAM constructs their initial fragment vocabulary to be aligned with the objective ***based on all of ZINC 250k which needs to be computed by the oracle.*** Training it to be dissimilar to the pre-training data would prevent GEAM from constructing such a vocabulary that is aligned with the objective."
            }
        },
        {
            "title": {
                "value": "General Response (1/6)"
            },
            "comment": {
                "value": "We would like to express our gratitude to the reviewers for their constructive feedback and questions related to all aspects of the paper. There are some shared concerns/questions amongst reviewers which we wanted to first address. ***We wanted to provide a comprehensive answer and we would be grateful if the reviewers could consider the full text.***\n\n## **1. Showing explicit optimization of high-fidelity oracles**\n* **Reviewer SmbD:** What will it take to achieve direct MD optimization?\n* **Reviewer feFf:** Show MD results\n* **Reviewer XhFy:** Empirical paper but does not show direct high-fidelity oracle optimization \n\nWe are excited that reviewers are interested in the potential to directly optimize MD. **Reviewer feFf** suggests that good performance on comparatively low-fidelity (docking) oracles does not necessarily translate to higher-fidelity oracles. We agree with this statement, but want to emphasize that there exists no work that directly optimize for MD. The closest are MFBind [1] which uses multi-fidelity active learning to sometimes queries an MD oracle to update the surrogate predictor model and REINVENT + MD [2] which uses active learning with supercomputer initial dataset generation (via MD). All generative molecular design works use docking as the oracle to predict binding affinity and docking can be very useful. We want to reference this recent review paper [3] which compiles all experimentally validated examples of generative models for drug candidate design. **Every single structure-based design case study used docking.** Therefore, showing Saturn is sample efficient on docking oracles is a valuable contribution.\n\nGoing from docking to MD imposes some engineering challenges (data transfer/computation on a cluster) and **access to sufficient compute resources. MD requires GPUs and each simulation imposes GPU hours.** Consider a real life case study where a model generates a batch of molecules. It would not make sense to sequentially use an MD oracle, as the wall time would be # molecules * MD time per generation epoch. This is prohibitively time expensive. Instead, we want to compute the entire batch in parallel, which would require 1 GPU per molecule or multiple molecules parallelized on a single GPU (the compute/speed trade-off will be case dependent). This is also why we use a batch size of 16 because while <= 16 GPUs **may be possible in academic labs**, access to more GPUs is quite prohibitive. Therefore, we respectively suggest that it is not fair to suggest we show MD results as there are no works that can do this and most works are very far from this prospect. It is an **enormous** jump from docking to MD. This is why there is so much work on making MD faster [4]. Our progress on optimizing docking is a valuable contribution.\n\nHowever, to show some evidence that our results can transfer to higher-fidelity oracle settings, ***we will show preliminary results of direct optimization of Density Functional Theory (DFT) oracles.*** DFT also requires parallelization but is possible on lots of CPUs. We will show direct optimization of the HOMO-LUMO gap which can be useful for molecular stability [5]. There are no papers that directly optimize DFT. There are works that use both xTB (lower-fidelity) and DFT on the most promising molecules (or only single-point DFT), but not DFT alone. Here, we will show, **for the first time**, full geometry optimization (**this is the expensive part**) with DFT and the direct minimization of the HOMO-LUMO gap using the B3LYP functional with D3 dispersion correction and def2-TZVP basis set [6-10]. We used Saturn (out-of-the-box, batch size 16, 10 augmentation rounds) with an **oracle budget of 300**. Every molecule in the batch is parallelized on 72 CPUs. 16 molecules = 16 nodes, each node with 72 CPUs. CPUs are more feasible for us to access and **we want to convey that had we used a larger batch size, this would have been prohibitive.** It is difficult to report an exact wall time as we had to use a shared compute cluster and jobs could be stalled in queue. In fact, if we had >> 16 molecules, this would\u2019ve been almost sequential due to cluster traffic, which would be prohibitively time expensive. Each molecule generally took 30 minutes to 2 hours depending on the size, as DFT scales $O(N^3)$. This is why DFT is expensive. ***Please see the attached supplementary file for the results.***\n\n***We hope by showing this explicitly, we convey to the reviewers that Saturn is a valuable step towards directly optimizing high-fidelity oracles and that they would consider raising their evaluation of our work.***"
            }
        },
        {
            "title": {
                "value": "Manuscript Changes"
            },
            "comment": {
                "value": "### **Manuscript Changes**\n\nAll changes are highlighted in the updated manuscript version.\n\n* Additional sentence introducing QSAR models in the Introduction **following Reviewer SmbD\u2019s feedback**\n\n* Fig. C3 in the Appendix showing a 2D heatmap of sample efficiency and diversity trade-off for Mamba with batch size 16 (Saturn uses this). **This follows Reviewer SmbD\u2019s feedback**\n\n* Contributions list previously mentioned performing > 5,000 experiments. **Following Reviewer f1gd\u2019s feedback**, we changed this to \u201c> 500 experiments, all across 10 seeds)\u201d\n\n* Appendix C.2 description of how SMILES augmentation is performed **following Reviewer XhFy\u2019s feedback**\n\n* Appendix F.2 wall times for reproducing GEAM **following Reviewer u4yG\u2019s feedback**. Saturn\u2019s wall times were previously already reported in Appendix F.1. Saturn\u2019s generation time is faster. \n\n* Page 4, clarity on what \u201caugmentation round\u201d means **following Reviewer f1gd\u2019s feedback**.\n\n* Page 4, added sentence about fixing the docking oracle seed which to support our statement about deterministic oracles **following Reviewer f1gd\u2019s feedback**\n\n* Page 5, clarity on what \u201clocal sampling\u201d means **following Reviewer f1gd\u2019s feedback**\n\n### **New Results (attached zip file)**\n\n* Showing direct high-fidelity oracle optimization. Density Functional Theory (DFT) oracle optimization - plots and discussion attached as a supplementary file\n\n* Further analysis between Saturn and GEAM results showing that GEAM generates molecules with lower ligand efficiency and are larger, which can inflate docking scores. This is due to GEAM not optimizing QED to the same extent as Saturn."
            }
        },
        {
            "summary": {
                "value": "The authors present Saturn, an RL-approach using a Mamba backbone for generative molecular design. Design choices surrounding implementation are thoroughly explored and motivated towards increasing sample efficiency to enable the use of higher fidelity (and more computationally expensive) objective functions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors focus on a weakness of the current trend in generative chemistry to couple an optimization algorithm with an insilico oracle function, suggesting two avenues for improvement: (i) increase the fidelity of the fast oracle, or (ii) increase the sample efficiency of the generative process to enable more costly oracles at reasonable throughput. In this paper they focus on (ii) which is a valuable contribution to the field.\n\nThe authors previously described the RL paradigm and here focus on substituting the language model backbone within the constraints that it is sufficiently simple to fine-tune on the fly. Given the recent success of Mamba as a linear time sequence modelling architecture this choice seems well motivated and suitable.\n\nThe reinforcement learning paradigm including SMILES augmentation, and experience replay buffer update with a genetic algorithm is intricately described along with its effect on the diversity of generated outputs."
            },
            "weaknesses": {
                "value": "Major:\n- The main weakness of the method seems to be the diversity reduction exhibited upon moving to Mamba from an RNN backbone. Therefore the majority of my main questions are surrounding this observation.\n  - The authors describe a logical diversity filter on the augmented memory buffer that uses bemis murcko as a filter. I am surprised that this addition does not seem to mitigate the reduction in diversity. Could the authors comment on this, and possibly show the murcko scaffold diversity along with their results in Table 2.\n  - Following the argument at Line 292 that Mamba generates repeat SMILES because its loss is lower during pre-training, this argument would suggest overfitting. If the authors use the Mamba checkpoint with a loss matched to the RNN, is the performance restored to the RNN levels of diversity? At what sample efficiency cost?\n\nMinor:\n- The introduction should mention the use of QSAR predictive oracles since these are very common in addition to docking.\n- The paper is motivated by an intent to use molecular dynamics as an oracle but does not achieve this goal, could the authors add a comment on what it would take to achieve this in the discussion section.\n- The authors discuss the trade off of sample efficiency and diversity, this is similar to the common fidelity vs diversity tradeoff among image generative models. The authors might consider a 2D summary plot showing the tradeoff of these while varying batch size and n augmentation rounds since its a lot of information to digest in a tabular form.\n- I am not clear on the logic presented in the section \u201csqueezing the likelihood of Augmented SMILES\u201d the authors might consider clarifying.\n- Figure 2d compares the trajectory of generation throughout augmentation rounds. It is unclear that the Mamba path is more linear from these plots in my opinion. (i) The UMAP should be fit to the same set in order to compare and (ii) the RNN should also have 10 rounds."
            },
            "questions": {
                "value": "- Is there a possibility that the molecules generated by GraphGA impair the quality of the replay buffer molecules as this is an imperfect generative approach itself, would it be better to use a higher fidelity method to breed the members of the buffer."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "- This paper tackles the problem of sample efficiency in chemistry generative models.\n- It proposes to use experience replay and data augmentation to train a Mamba model and show its sample efficiency compared to other state-of-the-art molecular generative models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The manuscript is well written and was easy to follow. The appendix is extensive and provides enough details on each component of the method. \n- The experiments are well thought and extensively demonstrate most of the laid out claims."
            },
            "weaknesses": {
                "value": "- The authors have claimed that promised candidates from docking are subjected to MD simulation which is a higher fidelity oracle and direct optimization for higher fidelity oracle may be possible with sample efficient generative models (Saturn). However, this paper remains short of demostrating the utility of Saturn on MD simulations and leave it as future work. It is not clear if the sample efficiency gains observed in case of a comparatively lower-fidelity oracle (docking) would translate to high-fidelity oracle (MD). For the sake of completeness and justification of laid out claim, I would suggest that the authors provide some preliminary results on application of Saturn for MD."
            },
            "questions": {
                "value": "- On the effect of diversity filter in augmented memory: Does setting the reward to 0 and removing all instance of over-represented scaffolds in the buffer, limits the generator from learning from potentially useful scaffold, if the removed scaffold had high reward before being penalized for redundancy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Motivated by the need to optimize expensive oracle functions, the authors propose Saturn, a framework for sample-efficient molecular optimization. Saturn consists of the Mamba architecture for generating SMILES, and RL with Augmented Memory for optimization. Extensive experiments show promising results on optimizing docking score compared to baselines in a sample-efficient manner."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* The motivation of the work is very clear and convincing. Inaccurate in silico oracles is one of the major limitations of generative models in practice, and so being able to utilize highly accurate but costly oracles in generative models would be very useful.\n* Clear and well-written paper, including methods and results.\n* Very extensive results. Comparison with ~20 state-of-the-art molecular generative models on the docking task, where Saturn appears to be significantly superior to all except GEAM.\n* Meticulous ablations of each model component and architecture hyperparameters. The appendix is also very in-depth."
            },
            "weaknesses": {
                "value": "* Very limited technical novelty. Saturn seems to be the application of the previously developed Augmented Memory approach, except swapping the backbone for the Mamba architecture. Thus, while the paper contains strong empirical results, it is probably not the right fit for ICLR.\n* The paper is motivated by the need to optimize the scores from high fidelity oracles, but no experiments are conducted for this setting. The results on docking are promising, but it is not clear that the superiority of Saturn will transfer to the more complex case of optimizing high fidelity oracles. For a paper that is mostly empirical, it seems important to have results on the motivating example instead of a toy example.\n* The most competitive baseline, GEAM, seems to reach about similar performance to Saturn for most tasks. While the authors argue that GEAM achieves more diversity at the cost of less sample efficiency, it appears the authors did not explore tweaking the hyperparameters of GEAM. Is it possible some hyperparameters of GEAM might encourage less diversity and increase sample efficiency?"
            },
            "questions": {
                "value": "* The authors state that they add a Tanimoto dissimilarity objective to Saturn to make Saturn-Tanimoto, but I don\u2019t see a similar GEAM-Tanimoto. Wouldn\u2019t it be possible to add the same optimization to GEAM? If so, would it be more fair to compare Saturn-Tanimoto with GEAM-Tanimoto?\n* What causes the improved sample efficiency of Saturn? It\u2019s not clear that the Mamba architecture would be particularly sample efficient, so it seems to be the SMILES data augmentation method. However, I don\u2019t see this explained in depth anywhere in the main text, particularly how the augmentation is actually done.\n* It is not clear to me in the methods which contributions are yours and which come from previous works. Could this be clarified?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a novel framework that introduces the Mamba architecture and enhances sample efficiency through augmented memory and experience replay mechanisms, demonstrating superior performance on multi-parameter optimization tasks relevant to drug discovery compared to the baseline models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper presents the first application of the Mamba architecture in generative molecular design. It uses augmented memory and experience replay to significantly enhance sample efficiency, enabling faster optimization of high-fidelity oracles. The authors conducted sufficient experiments to demonstrate the model's generative capabilities."
            },
            "weaknesses": {
                "value": "1. While Saturn excels in identifying high-reward molecules, there is a trade-off in diversity, which may affect its capability for exploratory molecular design.\n2. Although this paper used Mamba for the first time in the molecular generation task, the innovation of the model is limited."
            },
            "questions": {
                "value": "1. While the authors have demonstrated the efficiency of the Mamba through the comparison of different models under the Oracle Burden (OB) condition, it would be great to evaluate and report the actual generation time for each model. Could the authors provide any insights or results regarding the runtime performance of the different models?\n2. The authors should clarify why they specifically chose to maintain only the top 100 molecules in replay buffer. Additionally, does the replay buff lead to the potential issue of storing very similar molecules in the buffer, which could subsequently reduce the overall diversity of the generated samples?\n3. In part 3, the authors only compare the metrics related to Hit Ratio. Is there a problem that the generated molecules may be similar? Some metrics related to the diversity of generated molecules should be added for comparison."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a molecular generation framework for drug discovery by combining language model and Mamba neural network. The author experimentally show that a better generative performance can be achieved by incorporating Mamba network into the language model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The empirical evaluation is comprehensive. The author also provide open-source implementation, which enhance the credibility of the paper."
            },
            "weaknesses": {
                "value": "1. The presentation of the paper should be significantly improved. The description of the proposed framework is hard to understand. The author should at least provide a description of how the molecular generation problem is formulated as an optimization problem using clear math notation in Section 3. Many terminologies, such as \"local sampling\", \"augmentation round\", lacks clear description. The author should also include a clear description of Mamba network as most of the reader may not be familiar with it. \n2. It is unclear what the novelty of this paper is. The novelty of this paper seems to be limited to the application of Mamba network to molecular generation. \n3. Many model choices/claims of the proposed framework lack justification. For example, the author assume \"oracle evaluations are near deterministic\" without any justification. The author also need to discuss why Mamba can be potentially better compared to transformer/RNN. \n4. The experiment part of this paper seems to be simply Methods X Metrics. It is unclear what the author want to show other than the empirical superiority of Mamba compared to RNN/transformer. The author claim more than 5000 experiments by taking different random seeds into account, which is quite misleading. The author also need to compare the computational consumption of Mamba compared to RNN/transformer. \n5. The math notation is messy. For example, in Eq. (1): what is $x$? what is $\\theta$?"
            },
            "questions": {
                "value": "1. Could the author provide a more principled description of \"augmented memory\"? The explanation in Section 3 is hard to understand. \n2. The Saturn perform really bad compared to GEAM and the author improve Saturn by finetune the model with dissimilar data. Do the authors do the same for GEAM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}