{
    "id": "wQkERVYqui",
    "title": "Embedding Safety into RL: A New Take on Trust Region Methods",
    "abstract": "Reinforcement Learning (RL) agents are able to solve a wide variety of tasks but are prone to producing unsafe behaviors.\nConstrained Markov Decision Processes (CMDPs) provide a popular framework for incorporating safety constraints. \nHowever, common solution methods often compromise reward maximization by being overly conservative or allow unsafe behavior during training.\nWe propose Constrained Trust Region Policy Optimization (C-TRPO), a novel approach that modifies the geometry of the policy space based on the safety constraints and yields trust regions composed exclusively of safe policies, ensuring constraint satisfaction throughout training.\nWe theoretically study the convergence and update properties of C-TRPO and highlight connections to TRPO, Natural Policy Gradient (NPG), and Constrained Policy Optimization (CPO).\nFinally, we demonstrate experimentally that C-TRPO significantly reduces constraint violations while achieving competitive reward maximization compared to state-of-the-art CMDP algorithms.",
    "keywords": [
        "reinforcement learning",
        "safety",
        "information geometry"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wQkERVYqui",
    "pdf_link": "https://openreview.net/pdf?id=wQkERVYqui",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a novel method for safe RL, i.e. solving CMDPs while ensuring safety during training. The method is based on modifying trust region methods (i.e. TRPO and its constrained variant CPO) to yield trust regions that are contained in the set of safe policies. This is achieved by incorporating a barrier-like function to the trust region divergence, that approaches infinity as the expected cost of the updated policy approaches the threshold. The modified constrained objective is then approximately solved similarly to TRPO, with an additional recovery step in the case that an unfeasible point is reached. The authors provide a detailed theoretical analysis of their approach, and demonstrate the effectiveness of their method compared to other safe RL algorithms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper investigates an important problem and draws interesting connections to prior works on trust region methods.\n- While the idea of using barrier functions for safe RL has been explored before in a number of works, the present paper provides an original and interesting theoretical connection based on modifying the Bregman divergence of trust region methods.\n- The authors propose a simple yet effective recovery scheme for unfeasible policies.\n- The main part of the paper is well-structured. Ideas are introduced clearly and it is explicitly shown how they relate to previous works.\n- A further strength of the paper is the sound theoretical analysis of the proposed method, which is based on similar investigations for other trust region methods.\n- The experimental results are promising: the method is shown to achieve competitive return with lower cost regret compared to the presented baselines."
            },
            "weaknesses": {
                "value": "While the paper provides worthwhile contributions, in my view there are several improvements that could be made. These mainly concern experimental results and exposition. If these concerns are accounted for, I am happy to increase my score.\n\nExposition\n- The comparison to related work could be improved. In particular, while the related work section in the introduction summarises relevant approaches, it does not explicitly contrast them to the proposed method.\n- For example, in the discussion of penalty methods it is stated that they introduce bias and produce suboptimal policies. Why does introducing a logarithmic barrier function to the Lagrangian (as in e.g. IPO) introduce more bias than modifying the trust region divergence?\n- It would also be interesting to compare the theoretical bounds of the proposed method to those of other baselines besides CPO, e.g. IPO and the work by Ni et al. (2024).\n- The discussion of relevant material in the background section focuses on the setting of discrete state and action spaces. However, one of the primary appeals of policy gradient methods is their applicability to the continuous setting (and this is indeed where the proposed method is evaluated). A discussion of how this relates to the introduced background would be appreciated.\n- Furthermore, a brief discussion of Bregman divergences (possibly in the Appendix) would increase readability of the paper for readers not familiar with the topic.\n- The experiments section is missing a (brief) discussion of the environments and associated constraints.\n\nExperiments:\n- The experimental evaluation does not include other approaches (e.g. P3O), particularly those also based on log-barriers (e.g. IPO, Ni et al. (2024)), which are relevant baselines.\n- The ablation study on the hysteresis parameter shows that it is an important component of the achieved cost regret. The same idea can equally be applied to CPO. An ablation study comparing the proposed approach to CPO with hysteresis would highlight the effect of the main contribution of the paper, which is the modified trust region.\n\nMinor remarks:\n- The citation for IPO is wrong, this should be Liu et al. (2020) (line 71).\n- $V_r^\\pi(s)$ in Eq. 31 should be $V_{c_i}^\\pi(s)$ (line 737).\n- The definition of $L_\\theta$ is missing in Eq. 40 (line 792).\n- The presentation of Table 1 could be improved. Please highlight the best achieved cost in each row (e.g. bold or underline) and add standard deviations if possible. In the CarButton1 line, no return is bold."
            },
            "questions": {
                "value": "- How does the proposed approach compare to relevant baselines (e.g. IPO, Ni et al. (2024)), both in terms of theoretical bounds and empirical performance?\n- Why does introducing a logarithmic barrier function to the Lagrangian (as in e.g. IPO) introduce more bias than modifying the trust region divergence?\n- Can you provide an ablation study in which the proposed approach is compared to CPO with the same hysteresis scheme?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Constrained Trust Region Policy Optimization (C-TRPO), an approach that maintains safety constraints throughout reinforcement learning by shaping policy space trust regions to contain only safe policies. C-TRPO achieves competitive rewards and constraint satisfaction compared to leading CMDP algorithms, with theoretical convergence guarantees and experimental success in reducing constraint violations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The idea of incorporating safety constraints into the trust region in TRPO is very reasonable and novel compared with penalty-based methods. \n2. Both theoretical explanation of the C-TRPO and intuitive visualization in the toy MDP as in Figure 2 help to understand the effectiveness of C-TRPO, that it tries to behave safely in the trust region part instead of the target part."
            },
            "weaknesses": {
                "value": "1. One concern is the learning of the cost value V_c if this term is unknown. Since CPO suffers from estimation errors, C-TRPO has exactly the same problem. The theoretic analysis builds on the assumption that this function is accurate. \n2. From the experimental results, the improvement over certain baselines is limited. For example, TRPO-Lag achieves smaller costs by the end of training and similar reward performance. Also in Table 1, CPO outperforms C-TRPO in many tasks."
            },
            "questions": {
                "value": "1. Is the action dimension two for the toy MDP used in Figure 2? Then the y-axis should represent a_2? \n2. Line 167, D_k is not consistent with the previous Bregman divergence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a constrained optimization method called Constrained Trust Region Policy Optimization (C-TRPO), which modifies the geometry of the policy space based on safety constraints to create trust regions comprised solely of safe policies. They also provide an approximate implementation of C-TRPO. The main contribution is integrating safety constraints into policy divergence without introducing additional computational complexity or bias. The theoretical analysis of convergence is also provided. Experimental results show that C-TRPO achieves comparable policy performance and smaller constraint violations compared to common safe optimization methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "## Originality and Significance\n\n1) Safe RL is a crucial direction in reinforcement learning, which has significant implications for the application of reinforcement learning in real-world scenarios.\n\n2) This paper proposed the approach C-TRPO to address the constrained optimization problem by modifying policy divergence, which appears to be novel.\n\n## Quality and Clarity\n1) This paper provides mathematical formulations for the main concepts needed to understand the approach. They also provide relevant theoretical results. \n\n2) The paper includes  a number of Figures which are helpful in understanding the main concepts in the paper. The use of figures (such as Figure 1 to illustrate the constrained KL - divergence in policy space) and examples (like the description of the optimization trajectories in Figure 2) enhances the clarity of the explanations.\n\n3) The optimization implementation and approximation process is provided in detail."
            },
            "weaknesses": {
                "value": "1) The motivation and impact of integrating safety constraints into policy divergence are not sufficiently clear.\n\n2) The core idea of this paper is to incorporate the constraints into the policy divergence, but according to the definition in equation (15), the divergence approaches $\\infty$ when the policy approaches the constraint boundary, which results in the new divergence $D_c$ failing to satisfy the constraints, potentially leading to the absence of a solution.\n\n3) The paper does not provide sufficient evidence to prove that the improved effectiveness of C-TRPO is solely due to the new policy divergence. It states that the enhanced constraint satisfaction compared to CPO is attributed to a slowdown and reduction in the frequency of oscillations around the cost threshold. This effect may also be partially due to the hysteresis-based recovery mechanism. However, the paper does not demonstrate whether introducing the same hysteresis-based recovery mechanism to CPOs would yield similar improvements.\n\n4) Some of the theoretical explanations in the paper are not clear.\n\n## Experiments\n1) The paper does not include state-of-the-art baselines. It would be beneficial to compare C-TRPO with some of the latest safe RL algorithms to verify its effectiveness.\n\n2) No ablation studies have been conducted to assess the roles of the core components in C-TRPO.\n\n3) The observed results improvement is limited. The experimental results in the appendix indicate that the constraints in C-TRPO appear to be at the same level as in CPO, showing no smaller constraint violations (e.g., in safetycarbutton1 and safetyracecarcircle1).\n\n4) No code is provided, raising concerns about reproducibility."
            },
            "questions": {
                "value": "1) In Proposition 3, when $\\beta = 0$, $D_C = D_{KL}$ according to equation (9), why does C-TRPO approach CPO but not TRPO in this case\uff1f\n\n2) In Proposition 4, according to the proof in the appendix,  $\\mathbb{A}_c < \\Psi^{-1}$, Why is the upper bound of C-TRPO smaller than that of CPO? Could the authors provide a more detailed explanation of this upper bound?\n\n3) As the policy approaches the constraint boundary, $D_\\phi$ in equation 15 will approach infinity, which may make Equation (14) unsatisfiable and results in no solution. How is this situation addressed in the proposed framework?\n\nI am willing to raise my score if the authors can address my concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes Constrained Trust Region Policy Optimization (C-TRPO) that aims to ensure safe exploration (always being safe during the training) without sacrificing performance in reward. Inspired by TRPO, the main idea of this work is to incorporate cost constraints into divergence to create safer trust regions. The divergence is obtained by mapping the policy onto its occupancy measure on the state-action polytope, where a safe geometry can be defined using standard tools from convex optimization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Overall, this paper is well-written and well-presented.\n\nThe authors honestly point out and discuss the similarities and differences from the existing literature, and cite the paper correctly.\n\nSome figures in the paper are intuitive such as Figure 2.\n\nOverall, the mathematical proofs are sound.\n\nI indeed have several concerns regarding this work, and I hope some of them can be answered or addressed after rebuttal."
            },
            "weaknesses": {
                "value": "Line 43. Does \" without sacrificing performance\" mean C-TRPO can achieve exactly the same performance as that of TRPO (unconstrained RL)? The experiment does not support this. Indeed, Figure 3 shows that C-TRPO is even a bit worse than CPO. (TRPO should be have even much higher return as it is unconstrained.)\n\n\nLine 60-62, please provide more details that why model-based safe RL is less general, and what kind of stricter guarantees they provided.\n\n\nLine 74-75. I am not sure if I argree with this. In the convex problems (which I understand may not hold in RL) and the problems where the policy parameterization is \"perfect\", there is no \"bias\". Solving the langragian-weighted objective is as good as solving the constrained RL. See the reference below\n\n\"Paternain, S., Chamon, L., Calvo-Fullana, M., & Ribeiro, A. (2019). Constrained reinforcement learning has zero duality gap. Advances in Neural Information Processing Systems, 32.\"\n\n\nLine 76-82. The dicussion of trust region methods is too short. It is even shorter than the Penalty methods, while the paper focuses on the trust region methods.\n\n\nLine 86-87. I don't understand. C-TRPO is an approximation of C-TRPO itself?\n\n\nLine 112-120. Please make it clear in the formula that the expectation is w.r.t. initial state distribution, policy, and the transition function. \"the expectations are taken over trajectorie\" is too brief and not clear.\n\n\nLine 188-189. If I remember correctly, doesn't CPO already inherit TRPO's update guarantees for reward and constraints?\n\n\nLine 188. Refer to Figure 1 too early. There is no enough explaination in the main text or the caption of Figure, e.g., what is \\beta, etc.\n\nFrom Figure 1, why the proposed method is better than CPO? One is a clipped policy space, and the other one is a newly constructed policy space. It is hard to see which one is better intuitively. It also seems like C-TRPO has the same bounds as that of CPO (on page 7). The novelty is a bit limited in this sense.\n\nTo be honest, it is hard to tell if C-TRPO is better than baselines from Figure 3. Especially that it has lower return than CPO.\n\nIn general, I am a bit worried about the novelty of this work. It seems to me that there is not too much change compared to TRPO and CPO. Especially that Figure 1 does not clearly explain the difference. Why the fourth is better? Also, are these figures just hand-drawn intuition illustration? Are they true in practice?\n\nEnhance the writing and fix typos, e.g., Line 63, Line 142,"
            },
            "questions": {
                "value": "Please see my questions in the \"Weaknesses\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel policy optimization method for safe RL by constructing trust region of each iteration within the safe policy set for update."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "By constructing trust region within the safe policies set, this method maintains competitive returns with less constraint violations during training. Since only construction of trust region is altered, this method still preserve convergence and policy improvement guarantee of original TRPO. This paper is technically solid and well-written. In the analysis part, author also provides a thorough explanation on connection between C-TRPO and CPO on policy update and constraint violation."
            },
            "weaknesses": {
                "value": "1, line 142 CMPD &rarr; CMDP\\\n2, line 354 Proposition 1 refers to Theorem 1?"
            },
            "questions": {
                "value": "1, For safety during training and convergence of constrained natural policy gradient, what kind of initial set assumptions are needed?\n2, It would be interesting to see some comparison with hard constraints based approaches such as control barrier function based method[1, 2, 3], since similar notion of invariance seems to be brought up in section 4.2 to ensure safety during training. \n\n\n[1]Charles Dawson, Sicun Gao, and Chuchu Fan. Safe control with learned certificates: A survey of neural lyapunov, barrier, and contraction methods for robotics and control. IEEE Transactions on Robotics, 2023.\\\n[2]Yixuan Wang, Simon Sinong Zhan, Ruochen Jiao, Zhilu Wang, Wanxin Jin, Zhuoran Yang, Zhaoran Wang, Chao Huang, and Qi Zhu. Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments. In International Conference on Machine Learning, pages 36593\u201336604. PMLR, 2023b.\n[3]Jason Choi, Fernando Castaneda, Claire J Tomlin, and Koushil Sreenath. Reinforcement learning for safety-critical control under model uncertainty, using control lyapunov functions and control barrier functions. arXiv preprint arXiv:2004.07584, 2020."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}