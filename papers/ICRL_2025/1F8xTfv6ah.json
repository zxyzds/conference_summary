{
    "id": "1F8xTfv6ah",
    "title": "Advancing Out-of-Distribution Detection via Local Neuroplasticity",
    "abstract": "In the domain of machine learning, the assumption that training and test data share the same distribution is often violated in real-world scenarios, requiring effective out-of-distribution (OOD) detection. \nThis paper presents a novel OOD detection method that leverages the unique local neuroplasticity property of Kolmogorov-Arnold Networks (KANs). \nUnlike traditional multilayer perceptrons, KANs exhibit local plasticity, allowing them to preserve learned information while adapting to new tasks. \nOur method compares the activation patterns of a trained KAN against its untrained counterpart to detect OOD samples. \nWe validate our approach on benchmarks from image and medical domains, demonstrating superior performance and robustness compared to state-of-the-art techniques. \nThese results underscore the potential of KANs in enhancing the reliability of machine learning systems in diverse environments.",
    "keywords": [
        "Out-of-Distribution Detection",
        "Local Neuroplasticity",
        "Kolmogorov-Arnold Networks"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "A novel method leveraging the local neuroplasticity of Kolmogorov-Arnold Networks for OOD detection",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1F8xTfv6ah",
    "pdf_link": "https://openreview.net/pdf?id=1F8xTfv6ah",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a new out-of-distribution (OOD) detection method leveraging Kolmogorov-Arnold Networks (KANs), which utilize \u201clocal neuroplasticity\u201d to differentiate in-distribution (InD) data from OOD data via comparing the activation patterns of a trained KAN against an untrained counterpart. KANs stand out due to their spline-based architecture, which preserve specific network regions during training, aiding in the OOD detection."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The described method is clearly defined and is easy to reproduce.\n\nThe method is validated across image and tabular medical data benchmarks, demonstrating improved performance and robustness compared to other state-of-the-art OOD detectors. \n\nThe findings highlight KANs' potential in enhancing model reliability across diverse environments by maintaining high detection accuracy, even with a relatively small training dataset.\n\nThe results (although not on all datasets) look promising in terms of different OOD detection accuracy especially for the case of low number of training samples."
            },
            "weaknesses": {
                "value": "Despite the clarity, some steps of the approach implementation look like ad-hoc tricks for improving the method\u2019s performance without developing a deep intuition why a particular step is better than alternatives (please, see questions below for details).\n\nThe fact that not all datasets (leaderboards) from the OpenOOD were used for testing the approach, along with the obtained not perfect results on CIFAR-100, suggest that the datasets were selected manually. The authors need to prove absence of any selection bias. \n\nI am strongly concerned about the scalability of the proposed method, which requires splitting the training dataset into a number of subsets and fitting a model per a subset (see comments below).\n\nThe method resembles feature-preprocessor (backbone-dependent), being not applicable to the case where a good feature extractor is not known."
            },
            "questions": {
                "value": "Questions and suggestions:\n\nMajor: \nTesting of approach on other large-scale datasets would be beneficial, consider other leaderboards from openOOD like ImageNet-200, 1K.\nThe choice of the K-means clustering approach looks quite arbitrary for initial data splitting. Why not use other clustering approaches like DBScan, Spectral, Agglomerative or even Gausian mixture? I believe, K-means choice should be justified here.\n\nOne can assume a dataset with a lot of natural clusters (like ImageNet-1K) will require a lot of time for training KANs. Show that the approach is actually scalable, robust, and not computationally burdensome in case of a large number of clusters.\n\nThe robustness of clustering approach is not evident for the case of regression task due to the poor internal separability of data clusters. I suggest adding one example of OOD detection where the training dataset is directly related to the regression task. \n\nThe method looks strongly backbone dependent and may be poorly working for the plethora of practical tasks where the good backbone feature extractor is not known. Is it possible to exemplify the method robustness for the case of the absence of backbone preprocessor? \nProbably, some classic ML tabular datasets (e.g. from sklearn) could be useful here.\n\n\u201cImportantly, our experiments show that the previous methods suffer from a non-optimal InD dataset size\u201d - this statement requires more experimental support. Currently, the method superiority was shown only for the CIFAR-10 dataset. \n\nMinor:\nLine 183 (figure caption): \u201c- \u201c(e) InD score S(x)\u2200x \u2208 [\u22121,1] \u201c - why the InD score can take negative values? The original formula (5) contains absolute value operation brackets. Is this the typo? \n\nLine 187: \u201cA simple, yet effective approach is to split the dataset based on class labels.\u201d  - It is not obvious how to train KANs in case of such splitting. One can imagine a situation where positive class is OOD for a KAN trained on samples of negative class, and the maximization scoring procedure identifies positive class as an OOD. This point should be clarified or rephrased.\n\nI\u2019m interested if the method will be robust for the case of NaN-enriched data samples? It is not a request for an additional analysis but rather an interesting point for the discussion of method limitations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel OOD detection method that leverages the unique local neuroplasticity of Kolmogorov-Arnold Networks (KANs). By comparing the activation patterns of a trained KAN against its untrained counterpart, the method identifies OOD samples across diverse benchmarks, including computer vision and tabular medical data. Experimental results demonstrate that the KAN-based approach outperforms existing methods and shows resilience to variations in in-distribution dataset sizes. This robust, adaptable approach makes KANs a promising tool for enhancing the reliability of ML systems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. It introduces an innovative approach to OOD detection, offering fresh ideas and a unique viewpoint that advances the current understanding of OOD detection techniques.\n2. The paper effectively harness the neuroplasticity characteristic of KANs, ensuring that learning new tasks only affects the network regions activated by the training data, effective motivation for OOD detection.\n3. The paper includes thorough experiments on standard benchmarks."
            },
            "weaknesses": {
                "value": "1. While the core idea is clear, the method appears loosely structured. Specifically, the role of multiplying location-specific information with regions activated by InD samples to achieve the delta function (used in the score function) is unclear (e.g., Eqn 5). Additionally, no study is provided to analyze these aspects, leaving parts of the methodology unexplored.\n2. The paper does not present or discuss the generalization performance of models when KANs are incorporated into the training scheme. \n3. Results on CIFAR-100 indicate minimal advantage over existing methods, as the improvements in detection performance appear statistically insignificant.\n4. Including a discussion on the computational cost of the proposed method would strengthen the paper. Given that the approach involves dividing the dataset into different groups, insights into computational efficiency would enhance understanding of the method\u2019s practicality."
            },
            "questions": {
                "value": "Please answer the points raised in the questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Authors utilize Kolmogorov Arnold Networks (KAN) for out of\ndistribution detection. The main idea is to leverage the fact that KAN\nuses BSplines as non-linear functions. Feature values that appear\nwithin InD, if they are concentrated in certain part of the feature\nspace - which is $\\mathbb{R}$ in this case, will only modify certain\nBSpline coefficients. In this scenario when a feature value that is\ndifferent than the InD comes, the BSpline coefficients at those\nlocations will not have been modified during training. Hence, the\ndifference in activation between trained and untrained network will be\nlow. Experiments with benchmark datasets and comparisons with large\nset of alternatives are presented."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ The topic is very relevant.\n+ The idea is novel and quite intuitive.\n+ The results are motivating. Even though this is not the best\n  performing all around, it is one of the top algorithms.\n+ Authors do a great job explaining the method as well as motivating\n  the approach.\n+ Large set of experiments."
            },
            "weaknesses": {
                "value": "- The model - due to KANs - is heavily univariate. While authors do\n  dataset partitioning to alleviate the problem, I do not see how they\n  can actually do so. Unsupervised combinations of features are\n  mentioned, however, their applicability also raises questions.\n- Partitioning the dataset requires having multiple trained models,\n  which limits the applicability of the approach for large scale\n  problems.\n- KANs are interesting but most recent work do not use these\n  networks. This naturally limits the applicability of the approach."
            },
            "questions": {
                "value": "- It is not clear how different KAN$_i$'s are trained. It would be\n  good to explain this a bit more in depth.\n- Authors state that the method can be seamlessly integrated with any\n  pre-trained model. I do not really understand this. Doesn't one need\n  to use KAN model for this?\n- How are the pre-trained backbones used for KAN? Does one use the\n  features extracted from these networks and build classifiers and\n  regressors with KAN architecture?\n- Authors state that hyperparameters are tuned using a validation\n  set. How much do the trained hyperparameters generalize to OOD types\n  unseen in the validation set?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose to use Kolmogorov-Arnold Networks (KAN) for out-of-distribution detection. The key advantage of KANs is their plasticity which results in avoiding catastrophic forgetting. The authors show that this property can be leveraged to detect OOD samples. \n\nThe method demonstrates good performance on small datasets, but the proposed method does not properly address the shortcomings of the KAN architecture, and the method was not validated in terms of scalability to realistic problems. Overall I rate weak reject."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Originality: Given that KANs are a novel type of architecture the research is a very current  \n- The method is evaluated on image and tabular data, demonstrating feasibility across different domains. \n- Performance: The performance on the benchmarks is convincing and demonstrates superiority over a vast set of previous methods \n- Exhaustive experimentation on toy datasets including multiple important ablations that erase questions (such as stochasticity)"
            },
            "weaknesses": {
                "value": "Major: \n- Scalability: No experiments demonstrate the method's scalability to larger images or real-world problems. \n- Insufficient capturing of joint distribution: I believe the partitioning problem of KANs is very severe. While the problem is mentioned I believe it is not properly addressed. Essentially, by partitioning the dataset you are just scaling the problem down to subclasses. What if the l-shaped differences, that you mention in Table 2, appear on an intra-class level instead of a class level? While this may work for toy data if the data is sufficiently separable using k-means or class labels directly, I doubt it will work for more difficult problems such as MVTech. \n- The influence of Model capacity is unclear: KANs are known for their improvements in lack of catastrophic forgetting. How does the model size influence this. Additionally, if KANs treat features individually, the difficulty of the problem and the necessary capacity of the method scales drastically with the image size."
            },
            "questions": {
                "value": "Line 43 has wrong citation \n\nYou mention that the hyperpareter search can be quite challenging. How did you decide for the parameter space especially regarding number of epochs, learning rate, partitionings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}