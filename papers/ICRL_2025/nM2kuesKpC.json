{
    "id": "nM2kuesKpC",
    "title": "D2P2-SGD: Dynamically Differentially Private Projected Stochastic Gradient Descent",
    "abstract": "Stochastic optimization is a key enabler in modern machine learning, producing effective models for various tasks. However, several researchers have shown that model parameters and gradient information are susceptible to privacy leakage. Although, Differentially Private SGD (DPSGD) addresses privacy concerns, its static noise mechanism impacts the error bounds for model performance. Additionally, with the exponential increase in model parameters, efficient learning of these models using stochastic optimizers has become more challenging. To address these concerns, we introduce the Dynamically Differentially Private Projected Stochastic Gradient Descent (D2P2-SGD) optimizer. In D2P2-SGD, we combine two important ideas: (i) dynamic differential privacy (DDP) with automatic gradient clipping and (ii) random projection with SGD, allowing dynamic adjustment of the tradeoff between utility and privacy of the model. It demonstrates provably tighter error bounds compared to DPSGD across different behavior (i.e. convex and non-convex) of the objective function. The theoretical analysis further suggests that DDP leads to better utility at the cost of privacy, while random projection enables more efficient model learning. Extensive experiments across diverse datasets show that D2P2-SGD significantly enhances accuracy while maintaining privacy. Our code is available here.",
    "keywords": [
        "Differential privacy",
        "SGD",
        "random projection",
        "convergence",
        "stochastic optimization"
    ],
    "primary_area": "optimization",
    "TLDR": "We design an efficient differentially private optimizer called D2P2-SGD to allow small model performance reduction gap and maintaining privacy.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=nM2kuesKpC",
    "pdf_link": "https://openreview.net/pdf?id=nM2kuesKpC",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes D2P2-SGD, a differentially private optimizer which combines dynamic DP (i.e., time-varying noise variance) with automatic clipping and random projection. The authors provide a theoretical analysis which demonstrates that the error rates for D2PD-SGD are tighter than for DP-SGD."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* D2P2-SGD nicely consolidates several existing methods and I appreciated the \u201cunified framework\u201d angle (line 247).\n* The theoretical analysis applies to a general setting, with utility bounds and convergence rates for both convex and non-convex functions.\n* The authors provide a very detailed overview of related work and explain how their proposed algorithm differs from previous methods."
            },
            "weaknesses": {
                "value": "* The paper\u2019s contribution is debatable. While I do think it\u2019s valuable to consolidate existing methods into a unified framework, I wasn\u2019t convinced that D2P2-SGD is greater than the sum of its parts. I also didn\u2019t see much technical novelty in the thoeretical analysis.\n* I wasn\u2019t convinced by the empirical evaluation. It looks like the algorithm is only validated on a fairly small number of image datasets. The experimental set-up and the presentation of the results was also confusing for me. For example, I don\u2019t see a \u201cprivacy vs utility\u201d plot among the experimental results, which I think is crucial for validating the effectivness of D2P2-SGD compared to existing methods. \n* The writing could use polishing; I was a bit distracted by some of the language and I think there are many sentences that could be reworded in order to make them easier to parse."
            },
            "questions": {
                "value": "* Is there a reason for not including a privacy vs utility plot in the experimental results? For Figures 1 and 2, D2P2-SGD has higher testing accuracy but also higher privacy loss, so I don\u2019t think we can conclude from the figures that D2P2-SGD performs better than other algorithms at a fixed level of privacy. Indeed, from Figures 3 and 4, D2P2-SGD has both a larger privacy loss and lower testing accuracy for $\\sigma_{\\epsilon} > 6$ and so would appear to be indisputably worse than the baselines at stronger levels of privacy.\n\n* In Figures 1 and 2, the plots on the far right show \"epoch vs privacy loss\u201d for static and dynamic mechanisms. Why does the dynamic DP privacy loss increase so much more quickly than the static DP privacy loss?\n\n* Minor detail: the colors in Figure 4 don\u2019t seem consistent with the rest of the figures."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new differentially private gradient-based optimization algorithm. Specifically,\nthe proposed algorithm introduces two variations to the existing DP-SGD framework. First, it\napplies the per-sample gradient normalization to bound the sensitivity of gradient computation.\nSecond, it projects the normalized gradient to a lower dimensional space using the Gaussian random\nprojection and the noise is also added in the low-dimensional space. The performance of proposed\nalgorithm is evaluated on two datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper proposes a new differentially private algorithm, called D2P2-SGD, that combines\ntwo techniques: per-sample gradient normalization and random projection.\n- The paper analyzes the privacy property and utility of the proposed algorithm and provides\ntheorems."
            },
            "weaknesses": {
                "value": "- The proposed algorithm is largely based on existing algorithms and appears to make only\nincremental contributions. Specifically, the idea of combining the recently proposed autoclip\ntechniques with classical random projection approach is interesting but it\u2019s unclear what the\nmotivation is. Are there any reasons why this combination would outperform existing methods? Besides, recent work [1] empirically demonstrated that the belief that a network with fewer parameters would perform better than a parameter-heavy network is a misconception.\n\n  * [1] De, Soham, Leonard Berrada, Jamie Hayes, Samuel L. Smith, and Borja Balle. \u201cUnlocking High-Accuracy Differentially Private Image Classification through Scale,\u201d arXiv 2022\n\n- The provided proof of privacy property is straightforward due to the use of existing differentially\nprivate techniques. While convergence analysis is provided for both convex and non-convex functions, it neither demonstrates an improvement over prior work nor appears particularly more interesting than those presented in prior work. It seems to follow the same procedure used for analyzing the convergence rate of DP optimizers.\n- The empirical results are limited in scope and not well explained. The performance evaluation was done by training a 4-layer CNN on two small datasets FashionMNIST and SVHN. To demonstrate the effectiveness of the proposed approach, more extensive experimental evaluations should be conducted on a variety datasets and architectures."
            },
            "questions": {
                "value": "- What is the significance of the discussion presented in lines 172 - 174. Isn\u2019t \u03b3 a constant used\nto ensure the numerical stability of clipping for the gradient with norms close to 0?\n- Since the proposed algorithm projects the gradient into a lower dimensional subspace and\nadds noise in the projected space, it is natural to include other subspace projection-based\napproaches as baselines in the experiments. For example, GEP [1] and PDP-SGD [2]. Why\nare they excluded?\n\n  1. Yu, Da, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. \u201cDo Not Let Privacy Overbill Utility: Gradient Embedding Perturbation for Private Learning,\u201d ICLR 2020.\n  2. Zhou, Yingxue, Steven Wu, and Arindam Banerjee. \u201cBypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification,\u201d ICLR 2021.\n\n- I am not sure how to interpret the graph on the right in Figure 1. It seems that the dynamic\nmechanism incurs larger privacy loss and ends up with large epsilon values.\n- In Figure 3, why does the performance of DP-SGD and D2P-SGD remain almost the same\neven when $\\sigma_{\\epsilon}$ is increased?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper combines two variants of DP-SGD, namely Dynamic DP-SGD (Du et al., 2022) and DP-SGD with auto-clipping (Bu et al., 2024), to propose a privacy-preserving algorithm called D2P2-SGD. The authors claim that the proposed D2P2-SGD achieves a refined privacy-utility trade-off. Experiments are conducted to validate the results."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed D2P2-SGD is new to me, even though it is simply a hybrid of two existing methods. Experiments show that the proposed D2P2-SGD achieves higher accuracy on the FashionMNIST and SVHN datasets."
            },
            "weaknesses": {
                "value": "1. The theory seems confusing and is not self-contained. In fact, to achieve a rate of $O(1/\\sqrt{K} + \\log^2K/K^{1.5})$, according to Theorem 3 in this paper, it requires that $\\sigma_\\epsilon^2 = O(\\log K)$. However, to achieve differential privacy, in the privacy analysis (Theorem 1), it requires that $\\sigma_\\epsilon^2 = O(K^2)$. If so, the bound in Theorem 3 will diverge. Additionally, this rate is intuitively confusing. If I understand the four terms in Theorem 3 correctly, the last term should be caused by the auto-clipping techniques, while the remaining terms are from dynamic DP-SGD. Since the dynamic DP-SGD converges at an order of $O(1/\\sqrt{K})$, it is unclear why the proposed D2P2-SGD achieves a better utility guarantee. Please clarify this in the rebuttal section.\n\n\n2. The paper is not well-written, and there are numerous significant typos. Although this is not a reason for my rating, I believe it should be thoroughly polished."
            },
            "questions": {
                "value": "See the weaknesses part"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper combines several ideas -- automatic clipping, non-uniform privacy budget allocation and JL lemma, on DP-SGD and demonstrates that those improvements can improve the performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The idea and motivations behind are clearly presented. Proofs look solid and the prior work review is comprehensive. The authors also improved the clipping bias analysis."
            },
            "weaknesses": {
                "value": "1. Technically, my major concern is that both dimension reduction and automatic clipping are not new ideas. In particular, for DP-SGD with low-dimensional embedded gradients, it has been proved that such technique cannot improve (at least asymptotically) the utility-privacy tradeoff due to the projection bias. As also derived in this paper, D2P2-SGD in Table 1 does not improve the convergence rate but even leads to a worse one. \n\n2. Another mismatch is that the authors argue to use dynamic privacy budget rather than the classic uniform or static selection, while the theorems did not characterize or provide instruction on how to select the budget across the iterations. In the experiments, empirically the authors argue to use a decaying noise but there is no formal analysis to support such selection. A possibly reasonable explanation is maybe the initial stage is more robust to the noise. Also, the authors may want to carefully think about the dynamic protocol: if you want to further adaptively determine the decaying rate based on gradient features, additional privacy budget may be consumed. Anyway, much more work is needed for this dynamic proposal.\n\n3. Only toy examples are presented in this paper. Even from an empirical perspective, the advantage of D2P2-SGD is not very convincing. For example, can the author show comparisons to some SOTA work such as \"Unlocking High-Accuracy Differentially Private Image Classification through Scale\"."
            },
            "questions": {
                "value": "Please see the above comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}