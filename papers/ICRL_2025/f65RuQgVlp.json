{
    "id": "f65RuQgVlp",
    "title": "Federated Continual Learning Goes Online: Uncertainty-Aware Memory Management for Vision Tasks and Beyond",
    "abstract": "Given the ability to model more realistic and dynamic problems, Federated Continual Learning (FCL) has been increasingly investigated recently. A well-known problem encountered in this setting is the so-called catastrophic forgetting, for which the learning model is inclined to focus on more recent tasks while forgetting the previously learned knowledge. The majority of the current approaches in FCL propose generative-based solutions to solve said problem. However, this setting requires multiple training epochs over the data, implying an offline setting where datasets are stored locally and remain unchanged over time. Furthermore, the proposed solutions are tailored for vision tasks solely. To overcome these limitations, we propose a new approach to deal with different modalities in the online scenario where new data arrive in streams of mini-batches that can only be processed once. To solve catastrophic forgetting, we propose an uncertainty-aware memory-based approach.  Specifically, we suggest using an estimator based on the Bregman Information (BI) to compute the model's variance at the sample level. Through measures of predictive uncertainty, we retrieve samples with specific characteristics, and \u2013 by retraining the model on such samples \u2013 we demonstrate the potential of this approach to reduce the forgetting effect in realistic settings while maintaining data confidentiality and competitive communication efficiency compared to state-of-the-art approaches.",
    "keywords": [
        "Federated Continual Learning",
        "Catastrophic Forgetting",
        "Uncertainty Estimation"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=f65RuQgVlp",
    "pdf_link": "https://openreview.net/pdf?id=f65RuQgVlp",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a new federated continual learning setting to deal with different modalities in the online scenario. The proposed uncertainty-aware memory-based approach uses an estimate to measure of predictive uncertainty of samples, The extensive experiments demonstrate the effectiveness of reducing the forgetting effect in realistic settings while maintaining data confidentiality and competitive communication efficiency."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- A real-world online federated continual learning setting is proposed to adapt the practical scenario where new data arrive in streams of mini-batches that can only be processed once.\n- Using a bias-variance decomposition of the cross-entropy loss for classification tasks has some merits.\n- Experiments are comprehensive and well-designed."
            },
            "weaknesses": {
                "value": "- Some advanced memory replay methods are lacking.\n- Why do not consider some uncertainty-aware continual learning methods, such as [1] for comparison?\n- Why the number of $M$ is different from different datasets?\n- The improvement on the KC-Cell dataset is incremental compared to the other datasets. why?\n- On the left of Fig. 3, it is better to discuss the notable performance gap between the proposed method and the others on the last task.\n\n[1] NPCL: Neural Processes for Uncertainty-Aware Continual Learning."
            },
            "questions": {
                "value": "Please see the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an uncertainty aware-memory-based approach for federated continual learning in an online setting where an estimator based on Bregman Information is employed to compute model\u2019s variance at sample level. The proposed method includes predictive uncertainty-aware updated coupled with random replays."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe problem statement is meaningful and not widely talked about.\n\n2.\tThe proposed method works considering memory management in an uncertainty sampling setting, and handles class imbalance and data scarcity well. \n\n3.\tThe performance looks promising and overall the paper is easy to read."
            },
            "weaknesses": {
                "value": "1.\tThe fundamental idea of memory management is based on predictive uncertainty which is highly model-dependent and widely used.\n\n2.\tWhile federated continual learning in online settings is not very common, the paper proposed uncertainty estimator and random sampling for replay sets are not novel.\n\n3.\tA scalability issue may arise for large datasets such as ImageNet.\n\n4.\tIt\u2019d be interesting to see how the proposed method would work if task numbers were increased (>20)."
            },
            "questions": {
                "value": "In table 5, for BI method, why do you think the last forgetting(F) score is lower than FedCIL for CIFAR100? Is it related to the increased number of classes?\n\nPlease refer to weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a federated learning with end points doing online continual learning (CL), called online-FCL. To address the new task, the paper proposes a memory based CL method that manage samples based on its uncertainty score using Bregman Information (BI). The proposed method is evaluated on CIFAR10/100. On vision, medical and textual datasets, the proposed method does not show much of gain but in multi-modal data, the proposed method improves the performance over the state of the arts by significant gains."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Proposing a new federated learning with **online** continual learning.\n* Proposed method improves the accuracy significantly over the state of the arts in multi-modal (vision-and-textual task) setup"
            },
            "weaknesses": {
                "value": "* The proposed method is a straightforward application of Gruber & Buettner (2023), thus the technical novelty is limited.\n* Empirical gain seems marginal (also considering the standard deviation) compared to prior arts (Table 1-4). But in Table 5, the empirical gain in vision and textual task seems significant. Any reasoning for this?\n* Empirical validation is limited due to the size of the dataset. Although CIFAR-10/100 are popularly used datasets in CL literature, they are quite small sized. ImageNet-1K would be the minimum large scale dataset to validate the value of the proposed method.\n* Some results are not clear \n  * In Fig. 3, why the BI and MFCL improve the last accuracy significantly only at the last accuracy?\n* In L291-292, it is claimed that \"the BI-based estimation of the epistemic uncertainty is meaningful also under distribu- tion shift and able to identify robust and representative samples\". But is this fact used in your method? If so, can you analyze that this is true in the Fed CL context?\n* Presentation can be improved.\n  * In several lines, line break has been removed -- L056, L271, L290 to name a few\n  * Some abbreviation is not used without definition -- L072 CF -> catastrophic forgetting (this should be defined in L048)\n  * Figure 1 could be larger and clearer. Currently it is drawn with too small fonts, making visibilty bad.\n* Misc.\n  * Similar previous work that uses CE loss for uncertainty of a sample is missing: Koh et al., **Online Boundary-Free Continual Learning by Scheduled Data Prior**, *ICLR 2023* (the method called CLIB)"
            },
            "questions": {
                "value": "Please refer to my comments in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses Federated Continual Learning under the challenges of catastrophic forgetting and online learning, where new data arrives in sequential mini-batches and can only be processed once. The authors introduce an uncertainty-aware memory management strategy that leverages Bregman Information to measure predictive uncertainty. This approach selectively retains samples in memory based on their uncertainty, aiming to improve learning retention while handling various data modalities beyond vision tasks. The paper also showcases the method\u2019s performance on vision and text data, demonstrating its capability to mitigate forgetting across diverse, real-world settings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper presents a novel framework for Federated Continual Learning in an online setting, effectively addressing the practical limitations faced in scenarios where clients continuously receive new data without the ability to revisit previous data.\n\n2. The innovative use of Bregman Information for uncertainty estimation allows for selective sample retention based on epistemic uncertainty.\n\n3. The authors conduct extensive experiments with detailed analyses, demonstrating the effectiveness and depth of the proposed approach across various data types and scenarios."
            },
            "weaknesses": {
                "value": "see questions."
            },
            "questions": {
                "value": "1. The performance improvements on the CRC-Tissue and KC-Cell datasets are marginal. Given that these datasets contain more samples than others, could you provide a justification for this outcome?\n\n2. How does the Bregman Information approach to uncertainty estimation compare with recent probabilistic methods in Federated Continual Learning, particularly regarding computational efficiency and predictive accuracy?\n\n3. Most results are primarily compared to other uncertainty measurements, while comparisons with previous continual learning or federated learning methods appear limited."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}