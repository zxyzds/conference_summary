{
    "id": "DblHBgD0GR",
    "title": "Rethinking and Defending Protective Perturbation in Personalized Diffusion Models",
    "abstract": "Personalized diffusion models (PDMs) have become prominent for adapting pretrained text-to-image models to generate images of specific subjects using minimal training data. However, PDMs are susceptible to minor adversarial perturbations, leading to significant degradation when fine-tuned on corrupted datasets. These vulnerabilities are exploited to create protective perturbations that prevent unauthorized image generation. Existing purification methods attempt to mitigate this issue but often over-purify images, resulting in information loss. In this work, we conduct an in-depth analysis of the fine-tuning process of PDMs through the lens of shortcut learning. We hypothesize and empirically demonstrate that adversarial perturbations induce a latent-space misalignment between images and their text prompts in the CLIP embedding space. This misalignment causes the model to erroneously associate noisy patterns with unique identifiers during fine-tuning, resulting in poor generalization. Based on these insights, we propose a systematic defense framework that includes data purification and contrastive decoupling learning. We first employ off-the-shelf image restoration techniques to realign images with their original semantic meanings in latent space. Then, we introduce contrastive decoupling learning with noise tokens to decouple the learning of personalized concepts from spurious noise patterns. Our study not only uncovers fundamental shortcut learning vulnerabilities in PDMs but also provides a comprehensive evaluation framework for developing stronger protection. Our extensive evaluation demonstrates its superiority over existing purification methods and stronger robustness against adaptive perturbation.",
    "keywords": [
        "Protective Perturbations",
        "Imperceptible Perturbations",
        "Adversarial Purification",
        "Diffusion-based Generative Models"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We analyze protective perturbation via shortcut learning, proposing a systematic defense framework with restoration-based purification and contrastive decoupling learning that outperforms previous approaches.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=DblHBgD0GR",
    "pdf_link": "https://openreview.net/pdf?id=DblHBgD0GR",
    "comments": [
        {
            "summary": {
                "value": "The paper aims to improve the personalization performance of Diffusion Models on images with protective perturbation, a kind of noise avoiding images to be learned by models. The authors fist empirically analyze the latent mismatch between the perturbed and original images, finding that perturbation significantly alternate the latent representations of images. The authors believe that the mismatch causes shortcut learning and therefore fail the personalization of diffusion models on such perturbed data. Therefore, a novel method is proposed to improve the personalization training by contrastive learning and super resolution."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed contrastive learning method is well motivated by the empirical finding on the latent mismatch of perturbed images.\n2. In multiple domains, the method presents better fine-tuning performance than baselines given protective perturbation on images.\n3. Comprehensive experiments are conducted to understand and evaluate the method."
            },
            "weaknesses": {
                "value": "1. It is not clear to me the connection between the latent mismatch and the shortcut learning. Why does the existence of latent mismatch lead to shortcut learning? \n2. I don't think the word \"defending\" (in the title) should be used against a good technique, protective perturbation. The paper is a good red-teaming paper that explored a stronger threat model for protective perturbation. Unfortunately, many description of the method is defined as a mitigation method, which could mislead the readers about the negative impacts of the methods. The authors should discuss how this method can break the existing protective perturbation. It would be appreciated if the authors can discuss potential solutions toward better copyright protection via protective perturbation or other alternatives."
            },
            "questions": {
                "value": "* It is not clear to me the connection between the latent mismatch and the shortcut learning. Why does the existence of latent mismatch lead to shortcut learning? \n* What are the potential mitigation against to the proposed method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)",
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The proposed method can put the copyright of artists' work at risk. The method can void the protective perturbation in protecting images from being used for training diffusion models. The authors did not discuss the potential negative impacts."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper uncovers and validates the underlying mechanism by which adversarial perturbations disturb the fine-tuning of personalized diffusion models by latent-space image-text misalignment. Then, it introduces a systematic defense framework that mitigates the misalignment with data purification and contrastive decoupled learning and sampling."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper finds that adversarial perturbation leads to latent image-text mismatch and provides an explanation from the perspective of shortcut learning. Their analysis contributes to the further development of protective perturbation in personalized diffusion models.\n  \n- The proposed framework provides a system-level defense covering data purification, model training, and sampling strategy. Compared with previous data transformation and diffusion-based methods, the proposed method achieves the best semantic and image quality restoration."
            },
            "weaknesses": {
                "value": "- In Table I, the authors would better add a setting that the clean images are processed by the proposed and baseline methods.\n  \n- In Table II, why only calculate the time for data purification? Will CDL incur additional time costs?"
            },
            "questions": {
                "value": "Please help to check weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This is a promising method to destroy nearly all SOTA defense study on personalized diffusion model.  As it mentioned, it provides a valuable evaluation framework.  However, how to protect is still unsolved."
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper conduct a comprehensive analysis to show that perturbations induce a latent-space misalignment between images and their text prompts in the CLIP embedding space, which leads to association between the noise patterns and the identifiers. Based on this observation, the paper introduces contrastive decoupling learning with noise tokens to decouple the learning of personalized concepts from spurious noise patterns."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The observation that adversarial perturbations induce a latent-space misalignment between images and their text prompts in the CLIP embedding space is interesting and insightful.\n\n2. The paper is well-organized and easy-to-follow.\n\n3. The paper conducts an extensive array of experiments and also considers adaptive perturbation."
            },
            "weaknesses": {
                "value": "1. The paper does not provide strong theoretical analysis to support the conclusions.\n\n2. The technical contribution is a little limited since Decoupled Contrastive Learning is not a new technique proposed by the paper."
            },
            "questions": {
                "value": "I am wondering if the noisy images generated by DM without any defense can be denoised?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes viewing the fine-tuning process of Personalized Diffusion Models (PDMs) through the lens of shortcut learning, using causal analysis as motivation. The authors then introduce a defense framework designed to enable the model to correctly associate images with their original semantic meanings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper provides preliminary experiments on CLIP, which help demonstrate the authors' ideas.\nPersonalized diffusion models present an interesting area for further exploration."
            },
            "weaknesses": {
                "value": "1. The paper lacks overall coherence, with some sections difficult to follow and, in some cases, contradictory. Additionally, several terms and graphs are missing clear definitions and explanations.\n\n    1. Are \"adversarial perturbations\" and \"protective perturbations\" intended to be the same concept? The author seems to use these terms interchangeably; if they differ, please clarify each term carefully.\n    2. In the introduction, the author presents multiple related works. It may be helpful to focus on those most relevant to the paper\u2019s main motivation. Additionally, certain terms, such as \"purification studies,\" would benefit from brief explanations\u2014similar to the way \"image purifications\" is introduced on line 142.\n    3. Several equations need further explanation, such as those on lines 178-179, regarding the function of an instance dataset and a class dataset. Additionally, the meaning of \"r\" on line 208 is unclear.\n\n2.After reading the entire paper, I found it challenging to identify the specific question the author aims to address and the associated motivations. While the introduction attempts to outline these points, it is difficult to discern the relationship between the motivation and the problem being addressed. Additionally, there appears to be a disconnect between the problem definition in the introduction and the methods presented. Here are some specific suggestions for clarification:\n\n    1. The introduction states, \u201cThe model trained on perturbed data will generate images that are poor in quality, and thus, unauthorized fine-tuning fails.\u201d Does this imply that generating low-quality images of private content protects copyright and privacy? If so, why does the proposed method focus on enhancing image clarity for private content while defining it as a defense?\n    2.The author mentions that shortcuts are key to avoiding the generation of private personal images. Given this, why does the method seem to eliminate these shortcuts?\n    3.On line 46, adversarial perturbations are suggested as a means to protect users\u2019 images from unauthorized personalized synthesis. However, line 100 describes an intention to \"defend against\" this. Could you clarify?\n    4.Additionally, the highlighted question in the introduction, \u201cHow to design an effective, efficient, and faithful purification approach is still an open question,\u201d lacks context. Although there is a mention of \u201cMoreover, purification studies are also purposed to further break those protections\u201d in the following sentence, there are no subsequent explanations, particularly concerning how this question connects with the paragraph's earlier discussion.\n    5. In the end of introduction, it seems that the authors propose a new purify methods, \"Our approach conducts comprehensive purification from three perspectives, including input image purification, contrastive decoupling learning with the negative token, and quality-enhanced sampling....\". However, in the methods, the author says they propose a method to address the short cut learning...., which is a little bit confusing.\n\n3. Minor: Although viewing fine-tuning from a causal effect and shortcut learning perspective is novel, it shares similarities with backdoor attacks. In the backdoor attack literature, several papers have employed causal graphs to analyze shortcut mechanisms.[1-3]\n\n4. The causal graph is underexplained and possibly contains ambiguities. For example, the definitions of $\\bar{C}$ and $\\bar{x_o}$ are missing. While a brief introduction to the construction of the graph is provided, explanations of each node\u2019s meaning and the meaning of the arrows are absent. Given that the causal graph is a key contribution, adding a paragraph to introduce and explain it in detail would be beneficial. The term \"spurious path\" may also be misapplied; in causal inference, this usually refers to a backdoor path between treatment and outcome. Since this doesn\u2019t apply here, either avoid the term or define it within the paper\u2019s context.\n\n5. The causal graph may need structural revision. In causal inference, an arrow between A and B signifies that A causes B. However, in this graph, it seems that an arrow signifies containment rather than causation. I would suggest adhering closely to causal inference conventions and adjusting the graph accordingly.\n\n[1]Zhang Z, Liu Q, Wang Z, et al. Backdoor defense via deconfounded representation learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 12228-12238.\n\n[2]Liu Y, Xu X, Hou Z, et al. Causality Based Front-door Defense Against Backdoor Attack on Language Models[C]//Forty-first International Conference on Machine Learning.\n\n[3]Hu M, Guan Z, Zhou Z, et al. Causality-Based Black-Box Backdoor Detection[J]."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}