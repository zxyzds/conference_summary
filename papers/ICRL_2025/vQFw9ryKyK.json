{
    "id": "vQFw9ryKyK",
    "title": "ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination",
    "abstract": "Visual navigation is an essential skill for home-assistance robots, providing the object-searching ability to accomplish long-horizon daily tasks. Many recent approaches use Large Language Models (LLMs) for commonsense inference to improve exploration efficiency. However, the planning process of LLMs is limited within texts and it is difficult to represent the spatial occupancy and geometry layout only by texts. Both are important for making rational navigation decisions. In this work, we seek to unleash the spatial perception and planning ability of Vision-Language Models (VLMs), and explore whether the VLM, with only on-board camera captured RGB/RGB-D stream inputs, can efficiently finish the visual navigation tasks in a mapless manner. We achieve this by developing the imagination-powered navigation framework ImagineNav, which imagines the future observation images at valuable robot views and translates the complex navigation planning process into a rather simple best-view image selection problem for VLM. To generate appropriate candidate robot views for imagination, we introduce the Where2Imagine module, which is distilled to align with human navigation habits. Finally, to reach the VLM preferred views, an off-the-shelf point-goal navigation policy is utilized. Empirical experiments on the challenging open-vocabulary object navigation benchmarks demonstrates the superiority of our proposed system.",
    "keywords": [
        "Robotics",
        "Visual Navigation",
        "Vision-Language Model",
        "Scene Imagination"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "We propose a mapless visual navigation system by proposing a imagination-based visual prompting for pre-trained large vision-language models.",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vQFw9ryKyK",
    "pdf_link": "https://openreview.net/pdf?id=vQFw9ryKyK",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes \"ImagineNav\", a mapless navigation framework for robots using vision-language models (VLMs) to perform object search in open environments. It replaces traditional perception + mapping and text-based LLM-based planning with a pipeline where future scene views are \"imagined\" using novel view synthesis and analyzed by a VLM to choose optimal next waypoint. The candidate waypoints are generated by the Where2Imagine module, which is learned from human-like navigation patterns. Tested on benchmarks like HM3D and HSSD, ImagineNav significantly outperformed baselines in terms of success rate and success rate weighted by path length."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The system design significantly reduces complexity of an open-world object-goal navigation pipeline, and better integrates sensor observation with large models."
            },
            "weaknesses": {
                "value": "There are 3 fundamental limitations of this approach\n- First, there is no way to guarantee that the diffusion model used to synthesize the novel views understands object permanence. How often does it hallucinate non-existing object or leave out objects that should be there? How does the rest of the pipeline deal with this?\n- There is no way to ensure the waypoints generated by Where2Imagine is reachable, especially in out-of-distribution scenarios. The method still needs sine kind of local map for low-level path planning. Some tests on real robot or new environments will erase this concern.\n- The method does not deal with the ambiguity of object reference. For example, navigate to the chair (example used in Fig. 2) is very ambiguous as there might be many chairs in the environment."
            },
            "questions": {
                "value": "- Some details are very unclear, especially on the novel view synthesis (i.e. future view imagination). What data is used to train the diffusion model? Is it in or out of distribution for indoor navigation? How about resolution and the speed? \n- More qualitative visualizations of the environments, imagined views, waypoint distribution would be nice.\n- The structure of the VLM analysis output in Fig. 3 and Fig. 4 are inconsistent. Which one is used in evaluation?\n- Why is the success rate lower with NVS model added in Table. 2 (row 3 vs 5)? More explanation is needed.\n- How does this method compare with end-to-end approaches like SPOC [1]?\n\n[1] Ehsani, Kiana, et al. \"SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a framework called ImagineNav, designed to enable effective visual navigation for robots in a mapless, open-vocabulary setting. ImagineNav leverages Vision-Language Models (VLMs) with on-board RGB/RGB-D camera inputs, without complex mapping and localization procedures. Instead of traditional planning approaches, it translates the navigation task into a series of \"best-view\" image selection problems, where the VLM selects optimal viewpoints based on imagined future observations. This approach views navigation as a simplified selection problem and showcases the potential of imagination-driven guidance for robotic systems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper proposes a relatively novel navigation method: using novel view synthesis as a form of \"imagination\" for indoor navigation.\n2. Due to the inclusion of a VLM and diffusion model, this work achieves a higher success rate in open-ended tasks.\n3. The writing in this paper is clear, the illustrative figures are accurate, and the explanations of the proposed framework are well-articulated."
            },
            "weaknesses": {
                "value": "1. There are some concerns about the imagination module. The ImagineNav framework relies on the diffusion models for novel view synthesis. However, these could lead to some mistakes e.g. non-existing objects in the scene.  This could lead to incorrect navigation decisions by the VLM and reduce the overall success rate.\n2. A small concern about this approach is its performance in multi-room or occluded scenarios. The use of human habits without interactive perception could cause the robot to become trapped in local optima, preventing it from locating the target.\n3. The framework seems to only rely on immediate views for navigation decisions, without fully utilizing historical information from the navigation process. This lack of memory may limit the robot's ability to explore effectively and plan global paths over long distances or in intricate environments.\n\nIn summary, the method is novel (only to me, but if other reviewers show some related work, I will defer it). But the quality of the generated novel view (especially the emergence of non-existing objects) is concerned. If the author can explain more about this, I would increase the score."
            },
            "questions": {
                "value": "No extra problems."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents an innovative framework for mapless visual navigation that utilizes Vision-Language Models (VLMs) to streamline open-vocabulary navigation. Unlike conventional approaches that depend on mapping and text-based planning, this method relies solely on RGB/RGB-D inputs, redefining navigation as a task of selecting the best view image. Using the Where2Imagine module, it generates potential viewpoints based on human-like navigation tendencies, enabling the VLM to select the most suitable view to locate target objects. The NVS module then generates potential views of the next waypoint. This approach results in efficient paths without mapping and enhances success rates on standard benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This work introduces mapless navigation through the prediction of future 3D waypoints, generating possible image observations at these waypoints, and selecting the most promising next waypoint with VLM. The method is well-motivated and thoroughly described, making it feasible for the community to replicate the results.\n2. The paper includes an extensive experimental evaluation of the HM3D and HSSD benchmarks, with the proposed method achieving notable improvements over baselines on the challenging HM3D and HSSD benchmarks.\n3. A comprehensive ablation study is provided in Tables 2, 3, 4, and 5, highlighting the significance of different components within the proposed pipeline. Showing the effectiveness of using visual prompting and novel view thesis, as well as the waypoint prediction.\n5. Figure 5 also offers some failure cases to help readers understand the method\u2019s limitations."
            },
            "weaknesses": {
                "value": "1. The paper claims that object detection, segmentation, and mapping modules increase computational demand for robots. However, the proposed method introduces a computationally intensive novel view synthesis module to generate imagined observations. A comparison of computational load would strengthen this claim.\n\n2. The paper\u2019s organization could be improved for clarity. The text references to Tables and Figures are sometimes distant from the actual tables or figures; repositioning these elements could improve the flow and clarity of the paper.\n\n3. Although GPT4o-mini is a robust VLM model, comparisons with recent open-source VLMs featuring 3D spatial understanding would enhance this work, such as:\n\n* Spatial VLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n\n* SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models"
            },
            "questions": {
                "value": "1. While the provided failure case intuitively demonstrates a failure mode of the method, it would be valuable to include an approximate distribution of failure modes, such as how many failures are due to inaccurate imagined novel views."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces ImagineNav, a novel framework that leverages VLMs for open-vocabulary object navigation without relying on explicit mapping or localization. The framework acts on a discrete action space spanning different views, then predicts candidate relative poses from the current observation, then uses NVS to generate images of those poses, and lastly uses a VLM for planning. The problem is posed as a best-view image selection problem for the VLM. Empirical experiments on challenging open-vocabulary object navigation benchmarks demonstrate that ImagineNav outperforms existing methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is mostly clear and effective in conveying its points. There is an effective set of baselines indicating thorough experimental evidence but the addition of error bars would be helpful to determine significance. There are also effective sections discussing the failed and successful trajectories and clear ablations of the imagination module and the high-level planning module. The originality of having NVS methods apply to scene-level settings is interesting."
            },
            "weaknesses": {
                "value": "I think the main novelty within the method is the where2imagine module because prior work has used VLMs for high-level planning and NVS methods. I think the underlying claim is that waypoint as an intermediate representation (to predict poses and images with NVS) is a better reasoning intermediate than other intermediate representations and better than learning an end-to-end model. I think further investigating this claim could further add novelty to the paper as it would tackle a more fundamental question of are modular systems better than end-to-end models for object search and navigation. I would also try to rewrite the conclusion and portions of the introduction for clarity (paragraph 2). Lastly, the discussion of successful and failed trajectories are useful but I would like to see how to address those failed trajectories within the framework."
            },
            "questions": {
                "value": "Why do you think that waypoints are the correct intermediate representation for navigation and object search?\nWhat is the point of having images from NVS as it limits performance and rather uses the embeddings to train a navigation policy?\nWhat do you think are the limitations of the VLM high-level planning?\nHow can you get the VLM to learn online from additional data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}