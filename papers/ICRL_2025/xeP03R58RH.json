{
    "id": "xeP03R58RH",
    "title": "Rethinking Uncertainty Estimation in Natural Language Generation",
    "abstract": "Large language models (LLMs) are increasingly employed in real-world applications, driving a need to determine when their generated text can be trusted or should be questioned. To assess the trustworthiness of the generated text, reliable uncertainty estimation is essential. Current LLMs generate text through a stochastic process that can lead to different output sequences for the same prompt. Consequently, leading uncertainty measures require generating multiple output sequences to estimate the LLM\u2019s uncertainty. However, generating additional output sequences is computationally expensive, making these uncertainty estimates impractical at scale. In this work, we challenge the theoretical foundations of the leading measures and derive an alternative measure that eliminates the need for generating multiple output sequences. Our new measure is based solely on the negative log-likelihood of the most likely output sequence. This vastly simplifies uncertainty estimation while maintaining theoretical rigor. Empirical results demonstrate that our new measure achieves state-of-the-art performance across various models and tasks. Our work lays the foundation for reliable and efficient uncertainty estimation in LLMs, challenging the necessity of the more complicated methods currently leading the field.",
    "keywords": [
        "llm",
        "nlg",
        "uncertainty estimation",
        "uncertainty measures",
        "proper scoring rules"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "We propose a theoretically grounded uncertainty measure for LLMs that significantly reduces computational costs while maintaining state-of-the-art performance.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=xeP03R58RH",
    "pdf_link": "https://openreview.net/pdf?id=xeP03R58RH",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a novel approach to uncertainty estimation in natural language generation (NLG) models. The authors propose using the negative log-likelihood (NLL) of the generated sequence as a surrogate for uncertainty estimation. By leveraging the theoretical framework of proper scoring rules, they demonstrate that NLL can serve as an effective uncertainty metric. This approach simplifies the estimation process because it only requires the likelihood of the generated sequence under the model, avoiding the need for multiple samples. The theoretical foundation is well-established within the framework of proper scoring rules, and the empirical results demonstrate the method's superiority over existing metrics across various models and tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper introduces a new uncertainty estimation metric based on NLL that avoids the need for multiple sequence generations, which is a common bottleneck in existing methods.\n- By eliminating the need to generate multiple output sequences, the proposed method significantly reduces computational overhead, making it more practical for large-scale applications.\n- The method achieves or surpasses the performance of existing state-of-the-art uncertainty estimation methods across different models and tasks.\n- The approach shows strong performance across various model architectures, sizes, and training stages, demonstrating its broad applicability."
            },
            "weaknesses": {
                "value": "- The proposed metric focuses on statistical uncertainty derived from model probabilities but does not explicitly account for the semantic aspects of generated text. Incorporating semantic uncertainty would provide a more holistic estimation, capturing discrepancies between the generated content and the underlying meaning or intent. While the authors briefly discuss this limitation in the conclusion, it remains a significant issue that warrants deeper exploration, possibly through additional methods or combined metrics.\n- While the experiments are extensive, they focus primarily on free-form question-answering tasks. Additional experiments on other types of NLG tasks (e.g., dialogue generation, story generation) would strengthen the claims.\n- The paper spans 7 pages, whereas the conference allows submissions up to 10 pages. This unused space represents an opportunity to expand on key areas such as additional experiments, detailed analyses, or discussions that could further strengthen the paper's contributions."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a MAP-based approach to estimating uncertainty in large language models (LLMs) to improve the reliability of generated text. Traditional Monte-Carlo uncertainty estimation methods rely on generating multiple output sequences, a process that is computationally intensive and inefficient at scale. This study introduces a streamlined method that estimates uncertainty using only the negative log-likelihood of the most probable output sequence, eliminating the need for multiple sequences. The proposed approach maintains theoretical rigor and outperforms or matches existing methods across a range of tasks and models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Computational Efficiency: The proposed uncertainty measure requires only a single output sequence, significantly reducing computational costs compared to methods that generate multiple sequences, making it highly scalable for real-world applications.\n- Theoretical Soundness: Using MAP as the metric of uncertainty is grounded in established principles of proper scoring rules, ensuring theoretical robustness while simplifying the complexity of uncertainty estimation for natural language generation models\u200b."
            },
            "weaknesses": {
                "value": "- Questionable Efficiency: It seems to me that obtaining the MAP sequence (argmax) is non-trivial. While seemingly at the end of the day we only get one sequence, taking efforts to approximate it to be the MAP could be no computationally cheaper than sampling a lot of candidates, which is exactly what the paper is claiming to avoid. It would make the paper more compelling, if the authors can briefly study how well the argmax sequence is approximated, and if the approximation of obtaining argmax is not quite good, what is the worst-case performance of the proposed method."
            },
            "questions": {
                "value": "Please refer to Weaknesses)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Traditional uncertainty estimation relies on sampling-based methods, which inevitably incurs additional computation cost. This work addresses this limitation and proposes to measure the uncertainty solely based on the negative log-likelihood of the most likely sequence. Empirical results demonstrate the performance of the proposed metric in distinguishing between correct and incorrect answers."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed metric alleviates the need for sampling to estimate the uncertainty in natural language generation. \n\n2. The derivation of the different uncertainty terms and defining aleatoric and epistemic uncertainty is helpful.\n\n3. The presented experiments cover a few backbone models and representative tasks."
            },
            "weaknesses": {
                "value": "1. The only metric proposed by this work is the zero-one score, which is one minus the predictive distribution for the most likely output sequence. Therefore, I find this is actually equivalent to propose $p(y=y\u2019|x)$ as the confidence estimation, which has been widely applied in the machine learning community, whereas uncertainty is simply derived by 1-confidence. Consequently, this metric lacks technical novelty.\n\n2. Though the proposed NLL metric seems to be superior to baselines, this work lacks justification and insights on why the NLL is a better metric than the variants using sampling. \n\n3. Verbal explanations have been widely implemented in estimating the confidence level of LLMs. The author includes relevant discussion in Line 249. However, there is no empirical comparison to this type of baseline."
            },
            "questions": {
                "value": "1. What\u2019s $\\mathcal{D}$ in Line 115?\n\n2. Could you provide more justifications on why NLL may be better than other sampling-based baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to quantify the uncertainty of a language model for a specific prompt using the log-likelihood of the most probable sequence. Empirical results show that this new measure is effective in quantifying the uncertainty of the model without having to generate multiple times."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper studies an important topic, which is crucial for many applications (e.g. improving trustworthiness of LMs).\n\n2. The experiment results are good, which is surprising given the simplicity of the proposed method."
            },
            "weaknesses": {
                "value": "1. The idea to use a single generation to \u201capproximate the most likely output sequence (line 223)\u201d is concerning - the motivation is to avoid generating multiple sentences, and yet in order for beam search to find the most probable sentence (even in a toy setting), it requires multiple samples (Appendix A, Figure 2). Practically, I don\u2019t know how close a greedy sampled / top-k sampled sequence is close to the most likely sequence even of the same length. \n\n2. The contribution (using log-likelihood of one generation) is somewhat limited to empirical findings without any theoretical guarantees that one generation is able to find a sequence that is close to the most probable sequence. \n\n2. The paper is not very well written, making it difficult to understand what the authors want to convey. See the questions section."
            },
            "questions": {
                "value": "083: What is \\mathcal{V} here? You need to introduce the vocabulary. \n\n094: \u201csince \\mathcal{Y} scales exponentially with the sequence length T.\u201d Here you defined \\mathcal{Y} as all possible sequences, which is an infinite set so it shouldn\u2019t be growing. If you want to make this claim, you can define \\mathcal{Y}_t as the subset of all sequences with length <t. \n\n098-100: \u201cWe consider uncertainty for a given LMs, \u2026 a valid assumption\u201d I am a bit confused, what is the assumption here?\n\n111: Here if you are sampling y\u2019 from p(\\cdot |x, \\cdot) it is better to write it explicitly \u201cy\u2019 \\sim \u2026\u201d It can be a bit confusing here, and I am not sure how the discussion on \u201cProper Scoring Rules for Uncertainty Measures.\u201d advances your main claims - if this is only about evaluation, you can defer this to later sections.\n\n127: Again, I am not sure how \u201cAleatoric and Epistemic Uncertainty\u201d relates to your proposed method. The purpose of \u201crelated works\u201d or \u201cpreliminaries\u201d is to make people ground your work to existing literature, if you believe that your proposed method is connected with this literature, make it more explicit.\n\n898: \u201cThe reference answer sampled using beam search with a size of 20 is considered for assessing overall correctness, as it represents the most likely answer generated by the language model\u201d - why is beam search of 20 = most probable answer? Do you have any guarantees that a beam size of x makes the generated sequence log-prob close (difference bounded) to the most likely sequence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the uncertainty quantification for LLM. The paper first defines uncertainty as the expected score (which needs to be designed) of LLM prediction with respect to all possible parameters fitting the data. It then defines the scoring function as zero-one indicator of whether the generated sequence reaches maximal likelihood. The paper claims the estimation of the final uncertainty quantity only requires max-decoding (such as beam search). The paper evaluates the proposed method against prior baselines on 3 tasks for 6 LLMs. The paper used AUROC to measure accuracy and claims the proposed method achieves the best."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The uncertainty quantification for LLM is an important topic. \n2. The derivation of aleatoric and epistemic entropy is valid. \n3. The introduction of zero-one score is valid."
            },
            "weaknesses": {
                "value": "1. The writing of the paper is very blurred. It is not clear which part is from prior paper, which part is the original contribution in this paper. Eq(7-8) are proposed in this paper. But Eq(1-6) are unclear. \n2. The definition of uncertainty in Eq (1) seems to suggest there is a groundtruth y generation. The definition in Eq(2) is questionable. It is unclear what is the posterior distribution of parameter w. Does it need to have a distribution of parameter? What if the parameter is fixed. \n3. The actual estimation algorithm is not described. In particular, Eq(8) needs to estimate an expectation term. It is unclear how to estimate this part. There is no description in the paper. \n4. The evaluation approach and the metric used are quite questionable. It is unclear why this particular F1 threshold-based correctness is used. But the details of estimation such correctness is also not described well. The use of LLaMA 70B as the evaluator is also quite questionable."
            },
            "questions": {
                "value": "1. Are eq (1-6) developed by you or prior work? \n2. what is the exact step to calculate Eq(8).\n3. Do you have real groundtruth measurement for generation correctness?\n4. How many generations do you need to estimate uncertainty for one sequence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}