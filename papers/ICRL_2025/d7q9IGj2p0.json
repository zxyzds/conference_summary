{
    "id": "d7q9IGj2p0",
    "title": "MORPHING TOKENS DRAW STRONG MASKED IMAGE MODELS",
    "abstract": "Masked image modeling (MIM) has emerged as a promising approach for training Vision Transformers (ViTs). The essence of MIM lies in the token-wise prediction of masked tokens, which aims to predict targets tokenized from images or generated by pre-trained models like vision-language models. While using tokenizers or pre-trained models are plausible MIM targets, they often offer spatially inconsistent targets even for neighboring tokens, complicating models to learn unified and discriminative representations. Our pilot study identifies spatial inconsistencies and suggests that resolving them can accelerate representation learning. Building upon this insight, we introduce a novel self-supervision signal called Dynamic Token Morphing (DTM), which dynamically aggregates contextually related tokens to yield contextualized targets, thereby mitigating spatial inconsistency. DTM is compatible with various SSL frameworks; we showcase improved MIM results by employing DTM, barely introducing extra training costs. Our method facilitates training by using consistent targets, resulting in 1) faster training and 2) reduced losses. Experiments on  ImageNet-1K and ADE20K demonstrate the superiority of our method compared with state-of-the-art, complex MIM methods. Furthermore, the comparative evaluation of the iNaturalists and fine-grained visual classification datasets further validates the transferability of our method on various downstream tasks.",
    "keywords": [
        "Self-supervised learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We identify spatial inconsistencies of Masked Image Modeling targets tokenized from pre-trained models and resolve the issue through our proposed Dynamic Token Morphing",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=d7q9IGj2p0",
    "pdf_link": "https://openreview.net/pdf?id=d7q9IGj2p0",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a novel masked image modeling method. The paper identified the issue of the existing pre-training models: the spatial consistency in target representations despite proximity and contextual similarity. Then the author proposed a context-aware token aggregation method called Dynamic Token Morphing (DTM). The experiments show that the proposed DTM method is general enough to help improve various SSL frameworks and achieve SOTA ImageNet-1k performance. In addition, the computation complexity add-on of the proposed method is not large."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper achieves SOTA results on ImageNet-1k. \n- The proposed method is applicable to various SSL frameworks such as MAE, BEiT and BYOL.\n- The paper is very well written and has extensive experiments and visualizations.\n- The \"Pilot Study\" section gives very good narrative to the paper and provides intuitive motivations."
            },
            "weaknesses": {
                "value": "The paper is very well-written and achieves SOTA results, I don't have weakness to add."
            },
            "questions": {
                "value": "- For the Token encoding section of \"4.1 Preliminary\" section, consider adding more introduction/references to the concept introduced, for example \"online encoder\" and \"target encoder.\n- It will be easier for readers to follow if there is an additional figure about how to obtain the morphing matrix M."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the issue of spatially inconsistent target representations during pre-training. The authors conducted quantitative and qualitative experimental observations of this phenomenon to discuss the impact of such spatial inconsistencies, particularly on the performance of downstream classification tasks. To address this issue, based on the MIM baseline, they propose an advanced token aggregation method termed Dynamic Token Morphing (DTM), which integrates tokens with contextual associations. The paper reports the classification results on ImageNet-1k and the segmentation performance on ADE20k, demonstrating consistent improvements with DTM across the SSL framework and the SLIP model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.This paper provided an extensive discussion and demonstration of the spatial inconsistency in pre-trained representations, illustrating the significance of this issue in the domain of self-supervised learning.\n2. DTMs re-characterize visual features, mitigating the inflexibility associated with patch-based representations.\n3. This paper integrated DTM with multiple baseline methods, such as MAE, BEiT v2, BYOL, etc., demonstrating the effectiveness and convenience of the proposed approach."
            },
            "weaknesses": {
                "value": "1. Although DTM presents an intuitive feasibility in addressing the spatial inconsistency of visual representations, the explanation in the methods section is not very intuitive or detailed, which may lead to comprehension errors. For instance, the process of how to perform k iterations based on bipartite matching to obtain the final token morphing matrix M, and how to derive the aggregated results from M, is not clear. Perhaps an algorithm flowchart would be beneficial.\n2. Some details in the writing of the article could be further optimized. For example, on line 264, whether generating vi with xiM should not include masked tokens, as there is an inconsistency with Figure 4 (the input of the target encoder does not include masked tokens). Additionally, in Figure 4, the lines connecting the Morphing Matrix to Vi and Ui are different; does this indicate different operations?\n3. The authors emphasize in the abstract that DTM brings about faster training, but when comparing with other methods, especially the baseline method, the best results are achieved using the same number of epochs. Does this imply that the overall training time was reduced?"
            },
            "questions": {
                "value": "In addition to weakness, there are also some questions:\n1. In line 239, \"we do not employ a superpixel-based clustering for our method, which is inefficient as well.\" Is the decision to avoid superpixel-based clustering solely due to its low performance? Could the reasons be analyzed further?\n2. Please explain why the absence of a dynamic mechanism results in lower performance than the baseline in Table 7."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Dynamic Token Morphing (DTM) to mitigate the identified spatial inconsistencies discriminative problem. This method can be used fot both masked image modeling methods and contrastive methods and accelerating the training. The achieved results look good and I think the novelty is also good. But I concern several weaknesses as I noted below. I may change the rate after reviewing other reviewers' review."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.A interesting insight on masked image modeling pre-training methods and good results are achieved across all scale models including ViT-S/B/L.\n2.The proposed method can be used for both MIM and contrastive learning mothods"
            },
            "weaknesses": {
                "value": "1.Why not compare your method with hierarchical ViTs, such as ConvMAE [1], HiViT [2], iTPN [3], GreenMIM [4]. Is your method still effective on these methods? I suggest conducting an experiment on maybe one Small scale model.\n2.The authors argue that the current methods meets the trouble of spatial inconsistencies, but existing ones actually have achieved very good results, such as EVA [5], EVA02 [6], Fast-iTPN [7]. How the authors expain that? If you think the proposed method can still improve their performance, I think conducting experiments is necessary.\n3.Missing COCO experiments\n\n[1]ConvMAE: Masked Convolution Meets Masked Autoencoders, [NeurIPS 2022]\n[2]HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer, [ICLR2023]\n[3]Integrally Pretrained Transformer Pyramid Networks, [CVPR2023]\n[4]Green Hierarchical Vision Transformer for Masked Image Modeling, [NeurIPS 2022]\n[5]EVA: Exploring the Limits of Masked Visual Representation Learning at Scale [CVPR2023]\n[6]EVA-02: A Visual Representation for Neon Genesis\n[7]Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration [TPAMI2024]"
            },
            "questions": {
                "value": "See the second point of Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}