{
    "id": "92GUJzTRXs",
    "title": "ConDS: Context Distribution Shift for Robust In-Context Learning",
    "abstract": "In-context Learning (ICL) is a popular approach to filling Large Language Models (LLMs) with the context without fine-tuning. ICL works by feeding the test input along with the context information selected from the candidate dataset as examples of explaining the target task and getting the answer. In real-world applications, noisy samples are easily to be included in the datasets, so it is unavoidable that the candidate set might contain noise caused by human or measurement errors. The effectiveness of ICL is highly dependent on the quality of the selected ICL samples. Thus the noise in the candidate set can severely mislead the query answer and degrade the ICL performance. However, the noise ICL problem is largely overlooked. To tackle this challenge, in this paper, we propose Context Distribution Shift (ConDS), which iteratively revises the distribution of the candidate dataset so that the retrieved ICL samples are emphasized to improve the robustness of ICL. Specifically, we first identify the informative samples based on the retriever ranking score and the feedback from the LLMs, and then augment the identified informative samples. A subsampling strategy is also adopted to emphasize the importance of informative samples and decrease the size of noisy samples. Thus, ICL's reliability can be improved by reducing the catastrophic impact of noisy samples on almost all test queries to a small percentage. Our ConDS can be easily combined with existing off-the-shelf and fine-tuned retrievers. An analysis is also provided to reveal the relationship between ConDS and retrievers. Experimental results show that ConDS outperforms baselines on various tasks under the influence of noise by a large margin of 8.12\\%.",
    "keywords": [
        "In-context learning",
        "Distribution shift",
        "Robustness"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=92GUJzTRXs",
    "pdf_link": "https://openreview.net/pdf?id=92GUJzTRXs",
    "comments": [
        {},
        {
            "summary": {
                "value": "This paper proposes **ConDS (Context Distribution Shift)** to enhance the robustness of **In-Context Learning (ICL)** when dealing with noisy samples. The core idea is to modify the distribution of the candidate sample set to amplify informative samples and reduce the impact of misleading samples. The ConDS method primarily consists of the following steps: 1. Identifying Informative Samples: Using feedback from large language models (LLMs) and ranking scores from retrievers to identify information-rich samples within the candidate set. 2. Enhancing Informative Samples: Amplifying Informative samples by duplicating or paraphrasing them, thereby increasing their presence in the candidate set. 3. Subsampling: Conducting subsampling on the enhanced candidate set to control its size and further increase the probability of selecting Informative samples. The paper validates the effectiveness of the ConDS method through experiments on various text classification tasks, with results indicating that ConDS improves ICL performance in the presence of noisy samples. Additionally, the paper analyzes the effectiveness of combining ConDS with different retrievers, and finds that ConDS can be effectively combined with various retrievers."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper addresses the impact of noisy samples in In-Context Learning (ICL), which is a practical and important issue.\n2. The experiment results seem promising. Experimental results show that the ConDS method achieves performance recovery across various text classification tasks, consistently outperforming pure retrieval baselines, in both off-the-shelf and fine-tuned retrievers setting.\n3. The paper conducts extensive experiments to validate the effectiveness of the ConDS method, providing detailed analyses of the impact of different parameters."
            },
            "weaknesses": {
                "value": "1. **Lack of comparison with other denoising methods**: The paper would benefit from comparing ConDS with other dataset denoising methods.\n2. **Insufficient explanations in some places**:  \n   - The definitions of \"informative samples\" and \"misleading samples\" are vague, lacking a thorough discussion regarding their relationships with clean and noisy samples.\n   - The authors introduce the mixed score and assert that it enhances the retriever's ability to select clean samples. However, there is no experimental evidence provided to support this claim. It would be beneficial for the authors to design experiments comparing the impacts of different scoring mechanisms (e.g., using only retriever ranking scores, only sampling probabilities, and using mixed scores) on ICL performance to validate the effectiveness of the mixed score.\n3. **Lack of discussion on mathematical assumptions**:  The conditions for applying the hypergeometric distribution in line 273 may need more discussion. ConDS utilizes enhancement and subsampling to modify the size and distribution of the candidate sample set, which does not strictly meet the conditions for sampling without replacement. Furthermore, the retriever does not make binary decisions but instead ranks and selects samples based on scores.\n4. **Lack of case studies**:  The paper would benefit from the inclusion of case studies that illustrate the application and effectiveness of the ConDS method in experiment datasets.\n5. **Lacks results of larger and more advanced LLMs**:  The experimental conclusions do not encompass larger or more advanced language models. Given that models with varying parameter sizes and training methodologies may yield different ICL results, it would be valuable for the authors to conduct further experiments involving these models to provide a more comprehensive evaluation."
            },
            "questions": {
                "value": "1. **Alternative indicators for sample selection**:  Besides LLM answer consistency, what other methods can guide the selection of samples? Have you validated the effectiveness of any other indicators in this context?\n2. **Limitations observed in figure 4d**:  In Figure 4d, there are nearly 500 test queries where the clean sample ratio is 0 after applying ConDS. Does this indicate some limitations of the ConDS method? How do you plan to address and overcome these limitations in future work?\n\nOther questions please see above weakness for reference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies in-context learning (ICL) where the pool of examples includes noisy examples. To address this challenge, the paper proposes ConDS, which focuses on improving ICL robustness. ConDS identifies clean and informative samples based on the validation set, and then removes noisy examples that contribute to negative performance. Experimental results on nine datasets show ConDS's robustness on noisy ICL examples."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The strengths of the paper are outlined below:\n\n- S1)  The paper examines the robustness of ICL, offering new insight for various LLM-based applications.\n- S2) ConDS significantly outperforms competing baselines in noisy conditions.\n- S3) The motivation is clear, and the paper is easy to follow."
            },
            "weaknesses": {
                "value": "The weaknesses of the paper are outlined below:\n\n-  W1) I have some concerns regarding the methodology. ConDS relies on the validation set to classify examples as clean or noisy. However, since the validation set itself may contain noise, this could lead to inaccurate predictions. How do the authors ensure that the feedback from the validation set is reliable?\n\n- W2) The experimental setup and results are unconvincing. The default noise ratio is set to $p=0.6$, which results in the majority of the pool being noisy. In this scenario, it would be reasonable to conduct zero-shot inference using advanced LLMs, such as LLaMA-3, and disregard the noisy pool entirely. However, the authors only test smaller, outdated models like GPT-Neo-2.7B, which do not provide meaningful insights into zero-shot performance. Could the authors present zero-shot results for more advanced models of various sizes?\n\n- W2) ConDS seems to be an extension of PromptPG, which may limit its broader applicability (although ConDS can be combined with other retrievers, its performance is suboptimal). Could the authors elaborate on the unique contributions of this work?"
            },
            "questions": {
                "value": "Some additional questions/comments are outlined below:\n\n- Q1) Could you further clarify the differences between Sections 3.1 and 3.2? In Section 3.1, you utilize a paraphrasing model, while in Section 3.2, you employ a fine-tuned retriever to define $E_{shift}$. Is this correct and how do the two approaches compare in terms of performance?\n\n- Q2) The paraphrasing model is a T5 model trained on ChatGPT responses. Could you augment the baselines with this model and achieve better performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces ConDS, an approach designed to filter noisy in-context examples from a candidate set using LLM feedback\u2014in this case, the prediction of the LLM on a held-out split of the candidate set\u2014to distinguish between noisy and non-noisy examples. The method is straightforward and effective, demonstrating notable improvements over the strongest baseline, PromptPG, evaluated in this study. \n\nHowever, it is worth noting that the paper does not address why existing LLM feedback-based filtering methods, which employ similar entropy/perplexity based feedback mechanisms, cannot be directly applied in noisy settings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Demonstrates a significant performance improvement over baselines in noisy settings, showing proposed approach's effectiveness in filtering noisy in-context examples.\n2. An additional strength lies in the static approach's simplicity, as it can be seamlessly applied to any in-context pipeline with minimal modifications."
            },
            "weaknesses": {
                "value": "1. There is lack of clarity in contextualizing this work against prior studies on filtering in-context demonstrations. Although these existing methods operate in non-noisy settings, many rely on LLM feedback [1,2,3], often in the form of entropy or perplexity, similar to ConDS. Clarifying why such methods are not discussed would be beneficial. [3] originally is applied to find the best order of the prompt but it can potentially be used to provide the weighting of each in-context example in the noisy setting.\n\n2. UDR [4], mentioned in related work, also fine-tunes a retriever based on LLM feedback, yet it is unclear why training a UDR-style model on feedback is not included."
            },
            "questions": {
                "value": "**Questions and Comments**\n\n1. Consider ConE: ConE [3] appears to be applicable for re-weighting the candidate set $C^{\\text{train}}$ based on the informativeness of retrieved examples, as different prompts of in-context examples would have higher perplexity. ConDS and ConE share similarities in this respect, but there is no discussion on these parallels.\n2. Comparing PromptPG + ConDS with UDR: Based on Lemma 1, how does the combination of PromptPG + ConDS differ from training a UDR-style model on the target task? Since PromptPG + ConDS also requires retriever training on the target task, it would seem that a UDR-like method, which incorporates LLM feedback directly into the retriever's fine-tuning, would serve as a useful baseline.\n3. Noise Ratio in Figure 5(a): Why is the maximum noise ratio capped at 0.6? It would be insightful to know if ConDS can filter noise effectively at even higher noise levels, which may align with noisy samples in the validation split of $C^{\\text{train}}$.\n4. Definition of SCORE($\\cdot$): It is not specified what SCORE($\\cdot$) represents. Are these similarity scores from the retriever?\n5. Static Augmentation with ConDS: Are there results on applying ConDS to PromptPG in the static augmentation scenario? I am assuming that all other values in Table 2 are from the static setting.\n6. The term \u2018augmentation time\u2019 is confusing, as it actually refers to the number of augmentations after upsampling. Consider renaming it to \u2018augmentation size\u2019 or an equivalent term for clarity.\n\n**Typos**\n- Line 131: \"concatination\" \u2192 \"concatenation\"\n- Line 533: \"as followings\" \u2192 \"as follows\"\n- Algorithm 1, Lines L6 and L7: Should $q_i$ be $x_i$?\n\n**References**\n\n[1] Demystifying Prompts in Language Models via Perplexity Estimation (Gonen et al., EMNLP Findings 2023)\n\n[2] Revisiting Demonstration Selection Strategies in In-Context Learning (Peng et al., ACL 2024)\n\n[3] Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity (Lu et al., ACL 2022)\n\n[4] Unified Demonstration Retriever for In-Context Learning (Li et al., ACL 2023)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces ConDS to handle noisy ICL examples which could be misleading and result in degraded ICL performance. ConDS tackles this by adjusting the distribution of the candidate pool \u2014identifying clean, informative examples through retriever scores and LLM feedback, then boosting them while downplaying noisy ones. The paper also mathematically proved that this process is equivalent to dynamically fine-tuning a retriever. Rather than developing a new retriever, ConDS enhances the data for existing retrievers like BM25, KNN, and fine-tuned ones like PromptPG. The paper\u2019s experiments show ConDS improves performance significantly\u2014by about 8.12%\u2014across various tasks like sentiment analysis and topic classification, particularly in noisy conditions. The key takeaway is ConDS boosts ICL\u2019s reliability by ensuring cleaner samples are used during learning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper introduces a practical approach to improve ICL run-time robustness by adaptively adjusting the distribution of the demonstration pool. This approach is not limited to the choice of example retriever, i.e. off-the-shelf and fine-tuned retrievers can both be integrated, making the system flexible. It also shows overall promising results on several benchmarks (8.1% average performance boost), and especially in noisy data environments."
            },
            "weaknesses": {
                "value": "Although the paper recognize a real-world problem - contamination in ICL example pool and developed a practical mitigation strategy, it is still somewhat incremental and I am questionable about it's generalizability. There are lots of other real-world complexities that have not been considered.\n1. The datasets assessed here are not very challenging, mostly classification. It's uncertain how this behaves on more challenging use-cases such as text2sql, RAG, plus the binary signal used to distinguish between noisy / informative examples can be hard to generalize on other tasks.\n2.  The inference model used here is a fairly out-dated model GPT-neo-2.7B., and whether such method will still be effective towards a more powerful llm is unclear.\n3. The \"training\" stage is not very scalable as the pool size increases and the queries are long.\n4. The definition of noise: looking at the noise example provided, the labels are completely irrelevant with the ground truths. In real world scenario, noise can be more nuanced and there lacks of discussion how to handle borderline cases (e.g. when examples are ambiguous)"
            },
            "questions": {
                "value": "1. Have you tested the transferability across other tasks?\n2. How would this approach over-penalize borderline examples that may actually hold some useful contextual information? In complex tasks such as function calling, code generation, maybe the label contents do not match exactly with ground truth, but the formatting can be useful? Also what would happen when the query is challenging and hard to achieve good performance by adding :relevant good examples and those good examples are marked as \"problematic\" ones?\n3. The paper mentions using simple duplication or paraphrasing for augmenting clean examples. Although it might not be the focus of this paper - have you considered other augmentation methods such as adversarial example generation, i.e. adding noise to $x_i^{k}$ in the retrieved example (not $y_i^k$), to not only reduce noise examples but enhance the quality of the informative examples?\n4. Regarding scalability, any profiling of training time regarding dataset size and LLM size?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}