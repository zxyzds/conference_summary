{
    "id": "WkhlUyJcOJ",
    "title": "Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient Finetuning of LLMs",
    "abstract": "Fine-tuning Large Language Models (LLMs) has become a crucial technique for adapting pre-trained models to downstream tasks. However, the enormous size of LLMs poses significant challenges in terms of computational complexity and resource requirements. Low-Rank Adaptation (LoRA) has emerged as a promising solution. However, there exists a gap between the practical performance of low-rank adaptations and its theoretical optimum. In this work, we propose eXtreme Gradient Boosting LoRA (XGBLoRA), a novel framework that bridges this gap by leveraging the power of ensemble learning. Inspired by gradient boosting, XGBLoRA iteratively learns and merges a sequence of LoRA adaptations to refine model predictions. It achieves better performance than the standard LoRA, while enjoying the computational efficiency of rank-1 adaptations. We provide theoretical analysis to show the convergence and optimality of our approach, and conduct extensive experiments on a range of natural language processing tasks. The results demonstrate that XGBLoRA consistently outperforms standard LoRA and achieves performance comparable to full fine-tuning with significantly fewer trainable parameters. This work advances parameter-efficient fine-tuning for LLMs, and offers a promising solution for adapting LLMs to downstream tasks while optimizing performance and efficiency.",
    "keywords": [
        "LLM",
        "PEFT",
        "LoRA"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=WkhlUyJcOJ",
    "pdf_link": "https://openreview.net/pdf?id=WkhlUyJcOJ",
    "comments": [
        {
            "summary": {
                "value": "This paper gives a novel framework that enhances LoRA through ensemble learning, bringing it closer to theoretical optimality. The proposed method\u2019s convergence is demonstrated through detailed theoretical analysis. Comprehensive experiments reveal the relationship between the number of weak learners and model performance. Additionally, XGBLoRA outperforms traditional LoRA and many baseline methods across multiple tasks while significantly reducing the number of trainable parameters and memory usage."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. It is a good idea to use gradient boosting in LoRA finetuning, and the results showed that the idea is valid.\n\n2. The concepts in the paper are clear and understandable. This paper has a well-organized structure that effectively conveys the authors\u2019 ideas.\n\n3. This paper provides detailed theoretical analysis and proofs."
            },
            "weaknesses": {
                "value": "1. In Lines 251-256, could frequent merging introduce additional overhead? This part lacks quantitative analysis.\n\n2. Line 413 mentioned that the hyperparameters refer to LoRA[1], but to my knowledge, multiple sets of hyperparameters were provided, which seems ambiguous. Moreover, I am uncertain if the hyperparameters for RoBERTa and GPT-3 are suitable for the current models.\n\n3. In Table 5, the performance improves as the rank decreases. Could this be due to the simplicity of the data, which fails to allow for sufficient fitting?\n\n4. Alpaca is an excellent project; however, the dataset is relatively simple (despite being regenerated with GPT-4). Meanwhile, base models such as LLaMA3-8B and Mistral exhibit strong performance. Based on my experience, fine-tuning a highly capable base model with a lower-quality dataset can lead to model degradation.\n\n\t(a) From the existing leaderboards[2], LLaMA3-8B has achieved 66.49% accuracy on MMLU, it is even higher than the best value in Table 4. To avoid discrepancies due to evaluation methods and code versions, could you provide the performance of base models like LLaMA3-8B and Mistral on 8 tasks?\n\n\t(b) The Alpaca GPT-4(en) dataset differs significantly from the currently popular instruction fine-tuning datasets. Could you share results from other datasets like WizardLM[3], Infinity-Instruct [4] ?\n\n[1] https://arxiv.org/pdf/2106.09685\n\n[2] https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard\n\n[3] https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_70k\n\n[4] https://huggingface.co/datasets/BAAI/Infinity-Instruct"
            },
            "questions": {
                "value": "Please refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel, efficient fine-tuning method for LLMs, XGBLoRA, inspired by gradient boosting. XGBLoRA randomly samples a subset of layers for rank-1 adaptation at each training step, significantly reducing the parameters required per step. XGBLoRA outperforms standard LoRA while using less GPU memory."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is clear and easy to understand, effectively explaining the ideas.\n- The paper also presents a straightforward, efficient fine-tuning method that achieves state-of-the-art results with fewer parameters across various benchmarks.\n- The theoretical analysis provided by the authors supports the method's expressiveness power."
            },
            "weaknesses": {
                "value": "- A significant limitation of the paper is that the reduction in trainable parameters does not significantly decrease GPU memory usage and training time. Furthermore, the frequent merging operations compromise LoRA's plug-and-play nature (efficient model storage), making the \"Params\" axis in Figure 1 misleadingly optimistic. Specific limitations include:\n  - In Table 1, the computational cost for the base model $\\beta$ appears higher than $\\alpha$, suggesting that XGBLoRA does not significantly reduce the total cost. As shown in Figure 5, the difference in wall-clock time between 0.64s and 0.62s is minimal.\n  - The optimization of an unstable subset of parameters in XGBLoRA adds complexity to the training pipeline, potentially slowing convergence. It would be beneficial to compare different total training steps $K$.\n\n- There are minor errors in the Figures and Tables:\n  - In Figure 4, the x-axis should be labeled $\\kappa$ instead of $K$.\n  - In Figure 5, the x-axis should be labeled as XGBLoRA, not GBLoRA.\n  - In Table 5, bolded entries under Other, SIQA, and ARC-e columns are not optimal.\n  - In Table 6, bolded entries under Social and Other columns are not optimal."
            },
            "questions": {
                "value": "- **Related work**: The paper omits a significant related work, COLA [r1], which also employs residual learning. Please provide a brief comparison of the key methodological differences between XGBLoRA and COLA, as well as to include COLA in their experimental comparisons if feasible.\n\n    [r1] Xia, Wenhan, Chengwei Qin, and Elad Hazan. \"Chain of lora: Efficient fine-tuning of language models via residual learning.\"\n\n- **Theoretical justification**: The paper mentions that for LoRA, \"To fit any target matrix, the rank of the adaptation must satisfy (r \u2265 embedding_size/2).\" For XGBLoRA, the paper presents Theorem 2. Please provide a more detailed explanation of how Theorem 2 demonstrates the expressiveness advantages of XGBLoRA over LoRA, particularly in relation to the trade-off between rank r and iterations T. And how to ensure consistent total training times between XGBLoRA and LoRA in the theoretical analysis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes XGBLoRA, a low-rank adaptation method based on ensemble learning. It trains LoRA Adapters iteratively."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Interesting idea.\n2. Comprehensive experiment."
            },
            "weaknesses": {
                "value": "I will raise my score if the authors could address my concerns, especially weakness 3.\n1. Figure 1 is not mentioned in the manuscript.\n2. The motivation is poor and insufficient. In other words, any method could be considered a product of this motivation. Moreover, the motivations in the introduction (Line 66-69) and the abstract (15-16) are not consistent.\n3. I believe the parameter comparison of XGBLoRA with other methods is unfair. While it\u2019s true that XGBLoRA requires fewer GPU resources, this does not mean the number of parameters it trains is significantly smaller than other methods. In some cases, the number of parameters required by XGBLoRA should actually exceed that of LoRA. Compared to a method, XGBLoRA is more like a training approach. In other words, one could also train LoRA\u2019s parameters in stages, which would also allow it to \u201cuse a minimal number of parameters\u201d.\nNote that this weakness is related to Question 3."
            },
            "questions": {
                "value": "1. Line 58, what are rank_r and embedding_size? What does \u201cmuch smaller ranks\u201d in Line 59 refer to?\n2. Actually, I don\u2019t quite understand what is meant by \u201ctheoretical optimum\u201d (Line 61). Does this refer to better performance corresponding to a higher rank? And what is the \u201cperformance gap\u201d (Line 68)? Does it refer to the difference in performance between low-rank and high-rank?\n3. How to store the trained weights of XGBLoRA?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a method to improve LoRA, by posing the fine-tuning as a gradient boosting where randomly selected rank-1 LoRA matrices are used as weak learners. This method achieves better results than LoRA, with fewer parameters, less memory footprint and better speed per batch."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "* The authors tackle an important and meaningful topic that would be of interest to the community. PEFT is gaining more and more attention as the size of language models continues to grow.\n\n* The results are impressive.\n\n* Memory footprint is better, less trainable parameters, and better speed."
            },
            "weaknesses": {
                "value": "* Equation (7): In the first part of the equation, all components are fixed: the label y_i and the previous iteration of the model. Where is the current iteration prediction? Where is the residual part?\n\n* In line 252, double \u2018the\u2019.\n\n* Line 383, is it \u20181000%\u2019 or \u2018100%\u2019? Same for line 447.\n\n* Line 497, is it \u2018GBLoRA\u2019 or \u2018XGBLoRA\u2019?"
            },
            "questions": {
                "value": "* Line 324, it is called here \u2018Lemma 1\u2019, while later on in the appendices it is called \u2018Lemma 4\u2019?\n\n* In line 327, I assume the definition of L() is the loss of the model, given the weights? Furthermore, matrices A,B are defined earlier on with superscript and subscript. Here they are with two superscripts. Can you add a definition explanation?\n\n* Line 347, where is L* defined?*"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes XGBLoRA to bridge the gap between LoRA and FT by leveraging gradient boosting. This approach achieves high performance and low complexity, as shown in Figure 1. The authors demonstrate the empirical performance through extensive experiments, including various benchmarks and comparison baselines. The convergence guarantees and approximation error bounds are also justified."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The presentation is clear and easy to follow.\n2. The approach refines the pre-trained weight matrix progressively, with the original LoRA as a special case, which makes sense and is novel to me.\n3. The proposed approach is justified both empirically and theoretically.\n4. The influence of each hyperparameter, such as $k$ and rank, is illustrated in the experiments."
            },
            "weaknesses": {
                "value": "1.  The idea of ensemble learning has been used for LoRA before, like MELoRA, which the authors have already included in the experiments. I encourage the authors to include a discussion about ensemble learning for LoRA in the Related Work section and highlight the differences.\n2. Is the hyperparameter setting robust across different base models and datasets? For example, Table 6 shows that $L_s = 11$ gives the best performance for LLaMA3-8B on the MMLU benchmark; does it hold for other setups? The same question applies to $k$. If the performance significantly depends on the hyperparameters, extra effort would be required for hyperparameter search, which could be a weakness of XGBLoRA.\n3. The authors state, \"...leading to better generalization performance without explicitly adding regularization terms to the loss function...\" on page 6, line 300. However, in equation (7), regularization terms on the norms of $A$ and $B$ are explicitly used. Whether these regularization terms have an influence is not verified in the experiments.\n\n**Minor comments**\n\n1. Page 5, Line 252: \"set the the number of\" -> \"set the number of\"\n2. Page 5, Line 269: \"the the expressiveness\" -> \"the expressiveness\""
            },
            "questions": {
                "value": "1. Does the learning rate $\\alpha_t$ used in equation (8) have a big influence? It seems to me that a constant $\\alpha_t = 1$ may not be the best choice."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}