{
    "id": "vL9t9tpKli",
    "title": "Latent Radiance Fields with 3D-aware 2D Representations",
    "abstract": "Latent 3D reconstruction has shown great promise in empowering 3D semantic understanding and 3D generation by distilling 2D features into the 3D space. However, existing approaches struggle with the domain gap between 2D feature space and 3D representations, resulting in degraded rendering performance. To address this challenge, we propose a novel framework that integrates 3D awareness into the 2D latent space. The framework consists of three stages: (1) a correspondence-aware autoencoding method that enhances the 3D consistency of 2D latent representations, (2) a latent radiance field (LRF) that lifts these 3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field (VAE-RF) alignment strategy that improves image decoding from the rendered 2D representations. Extensive experiments demonstrate that our method outperforms the state-of-the-art latent 3D reconstruction approaches in terms of synthesis performance and cross-dataset generalizability across diverse indoor and outdoor scenes. To our knowledge, this is the first work showing the radiance field representations constructed from 2D latent representations can yield photorealistic 3D reconstruction performance.",
    "keywords": [
        "3D Gaussian Splatting",
        "3D-aware Representation"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "To our knowledge, this is the first work demonstrating that radiance field representations in the latent space can achieve decent 3D reconstruction performance across various settings including indoor and unbounded outdoor scenes.",
    "creation_date": "2024-09-21",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vL9t9tpKli",
    "pdf_link": "https://openreview.net/pdf?id=vL9t9tpKli",
    "comments": [
        {
            "summary": {
                "value": "This submission tries to resolve the problem of 3D reconstruction in the latent space via a 3-stage idea. The first stage focuses on improving the 3D awareness of the VAE's encoder via a correspondence-aware constraint on the latent space; the second stage builds a latent radiance field (LRF) to represent 3D scenes from the 3D-aware 2D representations; the last stage further introduces a VAE-Radiance Field (VAE-RF) alignment method to boost the reconstruction performance. The results generated by this pipeline out-perform the results generated by many state-of-the-arts."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "There are many innovations in this work, but I think the best part is the introduction of the 3d awareness into the 2D representation training. In this part, especially the correspondence aware autoencoding is the key to the success of this overall idea."
            },
            "weaknesses": {
                "value": "There are still some weaknesses prevented me from giving a higher score, especially, the details of how to compute each component of the pipeline. Please see my questions below. In addition, some related references are missing."
            },
            "questions": {
                "value": "I'm willing to raise my score if questions below is answered:\n\n1. How is \\lambda_{ij} computed in equation (6), section 4.1? Basically, how to compute the average pose error and how it contributes to the weight \\lambda_{ij}?\n2. Since equation (6) becomes multi-objective optimization, does this change largely increase the training convergence time? Did you experience any convergence issues?\n3. How is the inference speed of this pipeline?\n4. The idea is kind of similar to CaesarNeRF: Calibrated Semantic Representation for Few-shot Generalizable Neural Rendering: https://haidongz-usc.github.io/project/caesarnerf, which also uses calibrated image feature in each 2d latent, could you cite and compare?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The author introduces pixel-to-pixel correspondences across different viewpoints to help the VAE learn a 2D latent space with 3D awareness. Using 3D Gaussian Splatting (3DGS), they perform 3D reconstruction and rendering in the latent space to obtain 2D latent representations from specified camera poses. The rendered results are then decoded back into image space by the decoder to obtain RGB images.\n\nExperimental results demonstrate that the resulting 2D latent space possesses a certain level of 3D perception capability and outperforms existing methods when decoding to higher-resolution images."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The author is committed to integrating 3D awareness into the 2D latent space, and the results show a significant degree of success in this endeavor. Additionally, using 3D Gaussian Splatting (3DGS) in modeling the latent space is an intriguing idea."
            },
            "weaknesses": {
                "value": "The motivation of this paper is somewhat unclear. Is the author aiming to improve reconstruction accuracy, enhance rendering speed, reduce storage space, or achieve some other application? It appears that none of these goals have been fully addressed.\n\n**Reconstruction Accuracy**: When training the comparison methods, the author down-scaled the RGB images to the same resolution as the latent representation before training, which may be considered unfair. The VAE used by the author has been exposed to high-resolution images, while the comparison methods have not. This discrepancy could reduce the reconstruction performance of the comparison methods and impact the paper's credibility.\n\n**Rendering Speed**: In Section 5.1, the author reports the training times for Stage 1 and Stage 3, but not for Stage 2 or for inference time. Therefore, it is challenging to conclude that the proposed method has a faster rendering or training speed compared to other methods.\n\n**Storage Space Reduction**: In Section 5.1, the author mentions the need to \"train the same number of latent 3D Gaussian splatting scenes... for Stage-III,\" indicating that Stage 2 is scene-specific. Compared to 3DGS, this does not seem to save much storage space.\n\n**Other Applications**: In Section 5.3, the author suggests that their work can be used for text-to-3D generation. However, the two methods used for comparison are relatively outdated. It is recommended to compare the method with more recent approaches, such as IPDREAMER [1], to make the claim more convincing.\n\n[1] Zeng, Bohan, et al. \"Ipdreamer: Appearance-controllable 3d object generation with image prompts.\" arXiv preprint arXiv:2310.05375 (2023)."
            },
            "questions": {
                "value": "- Could the author please provide further clarification on the motivation and applicable scenarios for this work? \n- In the experimental section, it would be beneficial to employ fairer comparison methods; using low-resolution training for comparison models is not advisable. \n- Please consider using more recent models to compare the text-to-3D generation capabilities of this work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a framework for constructing radiance field representations in latent space, aiming to bridge the domain gap between 2D feature space and 3D representations. The authors propose a three-stage pipeline: (1) a correspondence-aware autoencoding method that enforces 3D consistency in latent space through correspondence constraints, (2) a latent radiance field (LRF) that lifts these 3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field alignment strategy that improves image decoding from rendered 2D representations.\n\nThe key technical contribution is the integration of 3D awareness into 2D representation learning without requiring additional per-scene refinement modules. The authors adapt the 3D Gaussian Splatting framework to operate in latent space, using spherical harmonics to model view-dependent effects. They demonstrate their method's effectiveness on both novel view synthesis and text-to-3D generation tasks across various datasets including MVImgNet, NeRF-LLFF, MipNeRF360, and DL3DV-10K. The authors claim their approach is the first to achieve photorealistic 3D reconstruction performance directly from latent representations while maintaining cross-dataset generalizability.\n\nThe work represents an attempt to make latent 3D reconstruction more practical by addressing the geometric consistency issues in existing approaches. The framework is designed to be compatible with existing novel view synthesis and 3D generation pipelines without requiring additional fine-tuning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper follows a standard pipeline structure addressing latent space 3D reconstruction. The method section breaks down into three components: correspondence-aware encoding, latent radiance field construction, and VAE alignment. The ablation study provides basic validation of these components, though more comprehensive analysis would be beneficial.\n- While building heavily on existing techniques, the paper demonstrates competent engineering in combining different elements into a working system. The adaptation of correspondence constraints and 3DGS to latent space shows reasonable technical implementation. The provided implementation details outline the basic approach.\n- The evaluation includes tests on multiple datasets (MVImgNet, NeRF-LLFF, MipNeRF360, DL3DV-10K), attempting to demonstrate applicability across different scenarios. While the cross-dataset evaluation has limitations, it provides basic evidence of generalization capability. The inclusion of both novel view synthesis and text-to-3D generation shows the method's potential utility, though more thorough evaluations are needed.\n- The method functions without per-scene refinement modules, which could be advantageous compared to some previous approaches."
            },
            "weaknesses": {
                "value": "- The paper fails to provide compelling justification for operating in latent space. While previous works like Latent-NeRF (for text-to-3D generation) established initial groundwork, this paper does not clearly demonstrate additional benefits of its approach. The motivation for operating in latent space remains questionable. The paper shows modest improvements in PSNR/SSIM metrics but doesn't address fundamental questions: What are the computational advantages over image-space methods? How does memory consumption compare? Why is the added complexity of latent space operations justified? The authors should conduct a thorough efficiency analysis, measuring training time, inference speed, and memory usage against image-space baselines. Without such evidence, the practical value of the latent space approach is hard to justify.\n- Section 4.1's correspondence mechanism has several fundamental issues. Most critically, the paper fails to address the scale mismatch between COLMAP's pixel-level correspondences and the VAE's latent space. Given that the VAE operates at a lower resolution (likely 8x or 16x downsampled) with larger receptive fields, how are pixel-level correspondences meaningfully mapped to latent features? This mapping is non-trivial: a single latent code typically corresponds to a large receptive field in pixel space, making precise correspondence matching questionable. The paper should answer: How are multiple pixel correspondences within one latent cell handled? How does the receptive field size affect correspondence accuracy? Additionally, basic details are missing: the correspondence filtering criteria, quality metrics, and robustness to matching errors. The use of L1 distance for latent features (Eq. 6) needs justification, especially given the coarse nature of latent correspondences. These technical gaps raise serious concerns about the method's fundamental soundness.\n- The use of spherical harmonics in latent space (Eq. 8) is puzzling. Given that the features are already in a learned latent space, why introduce SH basis functions? A direct learnable decoder or simpler view-dependent representation might suffice. Similarly, the VAE-RF alignment stage seems unnecessarily complex - the authors may quantify the alleged distribution shift and explore simpler alternatives. These design choices add complexity without clear benefits.\n- The experimental setup has a fundamental flaw: image-space methods are handicapped by low-resolution inputs while the proposed method has access to high-resolution data. This creates an artificial advantage for the proposed method. A fair comparison requires either: testing at matched resolutions, or demonstrating specific benefits under computational constraints. The ablation study skips crucial experiments on correspondence quality, loss function components, and architectural variations. These missing comparisons make it difficult to assess the true value of each component.\n- The paper sidesteps important practical concerns. Where are the failure cases? How does the method handle challenging scenes with varying illumination or complex geometry? The text-to-3D generation results lack comparisons with current state-of-the-art methods. The claim of \"photorealistic reconstruction\" needs validation through proper user studies or established perceptual metrics. Testing on more diverse, challenging scenarios would better demonstrate real-world applicability."
            },
            "questions": {
                "value": "- How to handle the scale mismatch between COLMAP and latent space? Please clarify: 1) the exact mapping strategy from pixel to latent correspondences; 2) how multiple pixel correspondences within one latent cell are aggregated\n- What's the correspondence filtering pipeline? In particular: 1) thresholds used for COLMAP matching 2) any additional filtering criteria in latent space 3) how outlier correspondences are handled.\n- During VAE-RF alignment (Section 4.3), how to: 1) balance the training/novel view losses 2) prevent overfitting during decoder fine-tuning.\n- Regarding the resolution setup, why choose this specific resolution comparison protocol? Would the method's advantages hold at equal resolutions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper target at Latent 3D reconstruction, and address the domain gap between 2D feature space and 3D representations. They proposed propose a novel framework that comprise (1) a correspondence-aware autoencoding method, (2) a latent radiance field (LRF), and (3) a VAE-Radiance Field (VAE-RF) alignment strategy."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Strengths*:\nWith the proposeld framework, this paper enhances the 3D consistency of 2D latent representations as well as effectively mitigated the gap between the 2D latent space and the natural 3D space."
            },
            "weaknesses": {
                "value": "Weaknesses*:\n\n1. Compared with feature-GS, it is a good improvement to add a correspondence-aware constraint during VAE encoder finetuning to improve its 3D awareness. However, this approach still cannot guarantee strict multi-view consistency of the encoded multiview features. As a result, after constructing the LRF, there may be a blurred radiance field with significant detail losses. Although the LRF is 3D consistent, the final decoded features may still exhibit noticeable flickering effects due to the lack of view consistency of decoder.\n\n\n2. I think this paper still needs optimization during the LRF stage instead of using a total feed forward method. When compared with 3DGS and Mip-Splatting, the authors only train them in latent space resolution (8 times lower than the image resolution), which yields very bad visual results. I suggest that the authors consider training the competing methods at full resolution. This paper may not work better than Mip-Splatting when trained with full resolution and very dense view, but it would be interesting to see if it outperforms Mip-Splatting in a sparse-view setting. This might be achieved because of generation capability of Stable Diffusion VAE used in this paper.\n\n3. The author didn't provide a detailed explanation of experiment settings. For example, I wonder how views are sampled in the experiment during training. And how many views are used as input and evaluation repectively. This is important for me to fully evaluate this paper. \n\n4. Object change in final rendering. As shown in Fig 1, the building in the final rendering is different from the ground truth image. One is in red, the other one is in white. This lead to my concern of identity preserving capability of purposed method. I think this is a problem that needs to be addressed."
            },
            "questions": {
                "value": "I think the strict view-inconsistency can not be solved fundamentally due to the emplyment of VAE Decoder. But it coule be possible to showcase more view consistent results and evlaute the 3D consistency of proposeld methods. \n\nThe author provide more details about the experiment settings, especially the view sampling strategy and the number of views used in the experiment. I hope see whether the proposed method can be applied to a sparse-view setting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}