{
    "id": "SfTy1ac4OX",
    "title": "Exploring Image-Text Discrepancy for Universal Fake Image Detection",
    "abstract": "With the rapid development of generative models, detecting generated images to prevent their malicious use has become a critical issue recently. Existing methods frame this challenge as a binary image classification task. However, such methods focus only on visual space, yielding trained detectors susceptible to overfitting specific image patterns and incapable of generalizing to unseen models. In this paper, we address this issue from a multi-modal perspective and find that fake images exhibit more distinct discrepancies with corresponding captions compared to real images. Upon this observation, we propose to leverage the \\textbf{I}mage-\\textbf{T}ext \\textbf{D}iscrepanc\\textbf{y}~(\\textbf{TIDY}) in joint visual-language space for \\textit{universal fake image detection}. Specifically, we first measure the distance of the images and corresponding captions in the latent spaces of CLIP, and then tune an MLP head to perform the usual detection task. Since there usually exists local artifacts in fake images, we further propose a global-to-local discrepancy scheme that first explores the discrepancy on the whole image and then each semantic object described in the caption, which can explore more fine-grained local semantic clues. Extensive experiments demonstrate the superiority of our method against other state-of-the-art competitors with impressive generalization and robustness on various recent generative models.",
    "keywords": [
        "vision-language model",
        "image-text discrepancy",
        "fake image detection"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=SfTy1ac4OX",
    "pdf_link": "https://openreview.net/pdf?id=SfTy1ac4OX",
    "comments": [
        {
            "withdrawal_confirmation": {
                "value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."
            }
        },
        {
            "summary": {
                "value": "This paper proposes to tackle the fake image detection problem via utilizing the discrepancy in the vision-language space (based on pre-trained CLIP model), in which fake image-text pairs exhibit higher discrepancies than the real ones thus helping the discrimination among them. Basically, given an image to be tested, the off-the-shelf image captioning model is adopted to first generate the corresponding text description of the input image, followed by feeding the image-text pair (as well as the object proposals found in the image and the objects described in the text) to the CLIP model for computing the discrepancy (between vision and language representations). The proposed method demonstrates better performance than baselines, verifying its efficacy and generalizability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The proposed method (named as TIDY) experimentally shown to have superior performance and better generalization ability (across various datasets produced by face-swap, GAN-based generative models, and the diffusion-based generative models) with respect to several baselines/SOTAs.\n+ The thorough ablation studies are provided to better verify the design choices and the properties of the proposed method. \n+ The organization and writing of this paper is clear and easy to follow."
            },
            "weaknesses": {
                "value": "- The novelty of the proposed method seems to be limited: \n1. The utilization of the joint representation (e.g. CLIP-based) over vision and language domains has been proposed in [Sha et al., DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models, CCS'23], in which their method (implicitly) also leverages the discrepancy in the vision-language space to perform the deepfake detection. However, such important work is not included in this submission.\n2. Furthermore, although the proposed method in this submission also considers the local discrepancy (in which the the object proposals found in the image and the objects described in the text are used), such design does not significantly boost the overall performance (according to the ablation study) and it needs to additionally utilize the object detector (where such object detector could be also limited to the object classes it has learn). \n- The image-only model variant (cf. Figure.4) shows comparable performance with respect to the full model (and that is why the proposed method has only slightly better performance than the UniFD baseline, and such performance improvement could even disappear when the local discrepancy is excluded), there hence should be more investigation into better leveraging the complementary properties across vision and language domains. \n-"
            },
            "questions": {
                "value": "The authors should carefully address the concerns as listed in the weaknesses (e.g. limited novelty, insignificant performance improvement with respect to the baselines)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a method that detects AI-generated fake images by leveraging discrepancies between images and their corresponding textual descriptions in a shared vision-language embedding space using CLIP. Unlike traditional detectors that focus solely on visual features and may overfit to specific artifacts, this approach captures differences more pronounced in fake images by analyzing both global image-level discrepancies and local discrepancies at the level of individual semantic objects. By combining these discrepancies, the method trains a simple MLP classifier to distinguish between real and fake images. Extensive experiments demonstrate that this method outperforms some other detectors in accuracy, and generalization to unseen generative models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- This paper  introduces a unique method that exploits image-text discrepancies in a joint embedding space, moving beyond traditional visual-only detection methods.\n- The inclusion of both global and local discrepancies allows for capturing fine-grained forgery clues."
            },
            "weaknesses": {
                "value": "- The method relies on captions for images. Inaccurate or misleading captions could negatively impact detection performance, especially for complex scenes or low-quality images.\n- The approach assumes that a caption can be accessed for every image, which may not hold in all cases, particularly for images with abstract content or minimal semantic information.\n- Errors in object detection and caption generation stages can propagate and affect the final detection performance.\n- The method is only evaluated on out-of-date models, results on more challenge models like FLUX, SD3, DALLE3 etc will make it more convincing."
            },
            "questions": {
                "value": "See weaknesses above.\nHow to detect fake images without any caption?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper detects fake images from different generative models by leveraging image-text discrepancy between real and fake images. In light of this, authors explored the discrepancy on both global and local aspects. Experiments on two datasets show outstanding performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tLeveraging the cross-modal discrepancy in global and local is novel to some extent. And the method is simple yet effective.\n2.\tThe paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1.\tLack comparison with recent works, such as FatFormer[1], FreqNet[2].\n2.\tThis work shares a similar motivation with DE-FAKE[3]. The author is encouraged to provide a comparison with DE-FAKE.\n3.\tAccording to Figure 3, the performance under unseen perturbations is not outstanding. The author is suggested to provide a discussion.\n\n[1] Forgery-aware Adaptive Transformer for Generalizable Synthetic Image Detection. CVPR 2024\n[2] Frequency-Aware Deepfake Detection: Improving Generalizability through Frequency Space Learning. AAAI 2024\n[3] DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models. ACM CCS 2023"
            },
            "questions": {
                "value": "1.\tWould the object detection performance affect subsequent learning? How to avoid the mismatch between \u201cobject patch\u201d and text embeddings?\n2.\tWhy the fake images show more distinct discrepancies with the caption than real images. The author is encouraged to provide some discussion in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a universal fake image detection method called \"TIDY,\" which leverages discrepancies between images and their corresponding text descriptions within a joint vision-language space to identify fake images. The study demonstrates that fake images exhibit greater discrepancies in image-text alignment compared to real images. The proposed method calculates the distance between images and text within the latent space of CLIP and further enhances detection by exploring both global and local discrepancies. Experimental results show that TIDY outperforms state-of-the-art methods in terms of generalization and robustness across various generative models"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Good Global-to-Local Discrepancy Analysis: The proposed global-to-local discrepancy analysis enables fine-grained detection, addressing both overall image and localized semantic discrepancies, which enhances detection accuracy.\nExtensive Experimental Validation: The authors conducted extensive experiments across diverse generative models, including GANs, diffusion models, and Deepfakes, demonstrating the method\u2019s robustness and adaptability to new and unseen generative models."
            },
            "weaknesses": {
                "value": "The inference process for TIDY relies on both image and caption data, which introduces potential inefficiencies. Specifically, detection requires generating a caption for the manipulated image, processing it through a pre-trained model, identifying corresponding objects, and then computing the overall distance score between image and text. This reliance on caption generation and object detection may hinder the model\u2019s efficiency and scalability in real-world scenarios where computational resources are limited or response time is critical. It would be beneficial for the authors to consider an image-only inference approach, which could streamline the process and make the model more lightweight and practical for broader applications without compromising detection accuracy.\n\nAfter the object detector extracts objects, formula (4) calculates the distance between each object and its corresponding object token only, rather than between a single object and all object tokens. However, Figure 2 illustrates that the distance is large between an object and unrelated tokens. Could this description be inaccurate?"
            },
            "questions": {
                "value": "See the weaknesses, my key concerns lie in the effiiency of proposed method and the complex pipeline, a sysmetic evaluation regarding the efficiency is required."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}