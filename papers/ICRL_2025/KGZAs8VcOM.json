{
    "id": "KGZAs8VcOM",
    "title": "MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers",
    "abstract": "Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement. However, this potential is largely unrealized because these assets always need to be converted to meshes for 3D industry applications, and the meshes produced by current mesh extraction methods are significantly inferior to Artist-Created Meshes (AMs), i.e., meshes created by human artists. \n\nSpecifically, current mesh extraction methods rely on dense faces and ignore geometric features, leading to inefficiencies, complicated post-processing, and lower representation quality.\nTo address these issues, we introduce MeshAnything, a model that treats mesh extraction as a generation problem, producing AMs aligned with specified shapes.\n\nBy converting 3D assets in any 3D representation into AMs, MeshAnything can be integrated with various 3D asset production methods, thereby enhancing their application across the 3D industry.\nThe architecture of MeshAnything comprises a VQ-VAE and a shape-conditioned decoder-only transformer. We first learn a mesh vocabulary using the VQ-VAE, then train the shape-conditioned decoder-only transformer on this vocabulary for shape-conditioned autoregressive mesh generation. Our extensive experiments show that our method generates AMs with hundreds of times fewer faces, significantly improving storage, rendering, and simulation efficiencies, while achieving precision comparable to previous methods.",
    "keywords": [
        "3D Generation",
        "Transformers",
        "Sequence Learning"
    ],
    "primary_area": "generative models",
    "TLDR": "Generate Artist-Created Mesh aligned to given shapes",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=KGZAs8VcOM",
    "pdf_link": "https://openreview.net/pdf?id=KGZAs8VcOM",
    "comments": [
        {
            "summary": {
                "value": "This work introduces a method for conditional \"artist-created\" mesh generation. \nThe key idea is employing auto-regressive transformer on top of a VQ-VAE pre-trained latent space, with additional conditioning for VQ-VAE and noise tricks to improve generalization. Quantitative results on toy datasets suggest that the method outperforming recent baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The paper is well-written and is easy to follow. \n+ The overall architecture of the method makes sense, and the problem of conditional mesh generation is important. \n+ There are several interesting techniques that could be useful for practitioners: e.g. conditioning VQ-VAE on the shape to improve reconstruction, and the gumbel noise trick to improve VQ-VAE's decoder robustness.\n+ Quantitative results (Table 2) suggest that the method outperforms competitors."
            },
            "weaknesses": {
                "value": "- The motivation for focus on artist-generated meshes is not very clear - would exactly the same method not work on a collection of reconstruction-based meshes? My assumption would be that the reason is that the method cannot scale to any realistic number of vertices. \n- The method is a combination of existing architectures (VQ-VAE based on BERT + OPT transformers), whereas the encoding scheme is the same as in polygen / meshgpt. \n- The scalability of the method is very questionable: both in terms of training and inference. If my undertstanding is correct, memory scales quadratically with the number of mesh faces, and authors explicitly mention that they filter out meshes with less than 800 faces. There is discussion of this in the appendix, but it is worth providing more information (memory and/or runtime numbers) which would be critical to get a complete impression of how usable the method actually is.\n- Comparison to diffusion-based methods (e.g. point-e) would be interesting (converting point clouds to meshes should be supported by publicly available code)."
            },
            "questions": {
                "value": "- Can you provide details on the scalability of the method - i.e. what is the max number of faces/vertices the model can handle?\n- Is there anything specific about architecture why artist-generatedness of the meshes is critical?\n- How does this method stack against diffusion-based methods for mesh/point-cloud based generation (e.g. point-e)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Meshanything for shape-conditioned AM generation. Meshanything first trains a VQ-VAE to obtain a set of mesh tokens and employs a noise-resistant decoder to enhance mesh generation quality. Subsequently, a shape-conditioned autoregressive transformer is trained to generate artist-created meshes. Experiments show that Methanything outperforms previous mesh generation methods with fewer faces."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Mesh generation toward artist creation has been seldom explored in previous research. This work could serve as an improved remeshing tool, facilitating a more rational topology for the generated 3D assets.\n\n- The visualization is good. The final mesh reconstructions are nice looking, and the method outperforms previous methods in the experiments.\n\n- This work constructs coarse meshes for sampling point clouds as inputs, thereby narrowing the gap between training and inference. This approach is reasonable."
            },
            "weaknesses": {
                "value": "- This paper mainly focuses on the task of point cloud -> mesh, which is usually referred to as mesh reconstruction, rather than mesh generation. Though a transformer-based autoregressive architecture is applied, I still don't think it is appropriate to name the paper a mesh generation paper. \n\n- It is unfair to use meshes generated by Rodin as the shape conditioning for comparison with MeshGPT. From my understanding, the Rodin engine largely (or purely) provides the generation ability while MeshAnything only acts as a remesher component.\n\n- Another concern is that the number of AM mesh faces generated by MeshAnything is limited to a maximum of 800. Using such a limited number of faces makes it insufficient for representing meshes with more complex structures.\n\n- This work appears to have stringent data requirements, as the authors were only able to filter 56k high-quality artist-created meshes from the Objaverse and ShapeNet datasets for training. So it may be challenging to scale this method up to more data and modeling parameters (but it might be sufficient to train a point cloud reconstructor/mesh remesher)."
            },
            "questions": {
                "value": "My suggestion is to change the description of mesh generation to mesh reconstruction/remeshing. \n\nI regard the proposed method as a novel learning-based remesher (AR applied), but I don't think the paper could be classified into a generation paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tackles the important question of converting various 3D representations to explicit mesh representations similar to what artists create. For this, the authors adopt the idea of MeshGPT and inject shape-conditioned information to make the model focus on learning topology. To make the model agnostic to specific 3D representations, e.g., NeRF or Gaussian Splats, the authors choose to convert all representations to point clouds and generate the mesh based on the features from the point cloud. Further, the authors develop a fine-tuning strategy for the VQ-VAE's decoder to create a noise-resistant one to reduce the gap between the data in training and real usages. A user study demonstrates the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- originality-wise: the choice of converting all representations to a point cloud and using the point cloud to generate corresponding mesh is interesting;\n- quality-wise: qualitative results are of high quality;\n- clarify-wise: the paper is well-written and easy to follow;\n- significance: to produce a mesh similar to what artists create is of great importance to bridge the current 3D generation research into real-world usage."
            },
            "weaknesses": {
                "value": "## Concerns about the user study\n\nSince the goal of the project is to produce artist-created meshes, it is important to make sure that the model's output is aligned with real artists' preferences. However, there is no information about whether the 41 users who participated in the study (L517) are qualified especially Tab. 1 is the main results reported in the paper. For \"qualified\" users, I mean people who are artists with enough experience in the field of 3D creation or editing, etc.\n\nCan authors clarify? If the users are not qualified, I would not be convinced by the result.\n\n## Concerns about comparison with mesh generation pipeline\n\nIn L508, the authors state:\n> MeshAnything is a shape-conditioned mesh generation method, we sampled shapes randomly from the evaluation set of Objaverse as inputs for MeshAnything, while for the baseline methods, we performed random sampling directly.\n\nFurther, in L862, the authors state:\n> We quantitatively evaluate mesh quality by uniformly sampling 100K points from the faces of both the ground truth meshes and the predicted meshes, and then computing a set of metrics to assess various aspects of the reconstruction.\n\nIf I understand correctly:\n- the procedure for evaluating MeshAnything is: 1) sample a mesh and then sample points from the evaluation set; 2) run MeshAnything on the sampled point cloud to obtain a predicted mesh; and 3) sample points from the predicted mesh.\n- the procedure for the baselines are 1) directly run the generation; and 2) sample points from the generated mesh.\n\nSuch a setup is quite unfair in my opinion. Essentially, MeshAnything generates meshes aligned with the evaluation dataset. I think it will surely perform well in terms of those metrics for evaluating generation qualities in Tab. 2. Especially on Objaverse, which has tons of various categorical objects, the unfairness will be amplified.\n\nI also agree that it is not easy to compare with baselines as MeshAnything needs to have a point cloud conditioning. ShapeNet may be more suitable due to the constrained categories.\n\n## Questions about generation procedure\n\nCan the authors clarify how the generation is conducted? As a generative framework, should the model output various plausible meshes based on the input point cloud? However, in the paper, there is always one mesh corresponding to the input. Does this mean that the authors always use **max** logits during the sampling, i.e., greedy search? Can authors provide more generation variants, e.g., with beam search? I would like to know how diverse the mesh generation could be.\n\nFurther, a question related to the evaluation on Tab. 2 is whether the authors only use **max** sampling to evaluate. \n\n## Insufficient qualitative results\n\nThere are actually no qualitative comparisons to baselines, e.g., MeshGPT and PolyGen in the paper. Please provide. \n\n## Citation format\n\nPlease refer to the template instructions and use the correct citation commands. Currently, all citations are with `\\citet` instead of `\\citep`. They look so weird as those citations appearing without parenthesis break the sentences."
            },
            "questions": {
                "value": "See \"weakness\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed.",
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents MeshAnything, a model framing mesh extraction as a generative task, resulting in artist-crafted meshes that align with specified shapes. The approach first establishes a mesh vocabulary through VQ-VAE, followed by training a shape-conditioned, decoder-only transformer with a noise-resistant decoder design on the learned vocabulary, enabling shape-conditioned autoregressive mesh generation. This paper is well-crafted, and I anticipate that it will positively influence the field of 3D content generation though the pipeline appears somewhat straightforward. The performance of the method seems promising."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "This is a well-written paper that clearly conveys the authors' contributions. The topic is highly relevant to the field, as it addresses the gap between generated shapes and practical applications - I expect the authors will consider open-sourcing their code to facilitate further research. The effectiveness of the method is demonstrated in the experimental sections."
            },
            "weaknesses": {
                "value": "I recommend that the authors showcase real-world application examples to highlight the significance of their research. For example, the authors could demonstrate how Artist-Created Meshes (AMs) improve shape manipulation for artist-driven shape modifications or offer enhanced rendering performance (e.g., efficiency) compared to conventional dense meshes.\n\n## Related Works\nAdditionally, recent advancements in 3D reconstruction and generation could be acknowledged, such as:\n- Part123: Part-aware 3D Reconstruction from a Single-view Image (SIGGRAPH 2024)\n- SyncDreamer: Generating Multiview-consistent Images from a Single-view Image ICLR 2024.\n- CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets (SIGGRAPH 2024)\n\n\n## Exposition\n\n- \"MeshAnything converts any 3D representation into Artist-Created Meshes (AMs), i.e., meshes created by human artists.\" Consider rephrasing \"any 3D representation\" to \"different 3D representations.\"\n\n- \"This is because all meshes produced by these methods (Lorensen & Cline, 1987; Chernyaev, 1995; Lorensen & Cline, 1998; Shen et al., 2021b; Chen et al., 2022; Shen et al., 2023) exhibit significantly poorer topology quality compared to AMs. As shown in Fig. 2, these methods rely on dense faces to reconstruct 3D shapes, completely ignoring geometric characteristics.\" The logic here seems unclear. Generally, mesh topology pertains to the connectivity of vertices, while geometric features relate to vertex positions. Could the authors clarify how \"poorer topology quality (relying on dense faces)\" affects \"geometric characteristics\"?"
            },
            "questions": {
                "value": "1. Will the data and code be available for reproducibility?\n2. Can the limitations and potential future work be expanded in the main paper? Are there any failure cases the method cannot handle? Additionally, what would happen if watertightness is required during generation? Including these discussions could enhance the paper.\n3. \"Due to its generative nature, our method is not as stable as reconstruction-based mesh extraction methods.\" Could the authors clarify the meaning of \"stable\" in this context?\n4. How does the method perform on open surfaces?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}