{
    "id": "EbOhZyxIzQ",
    "title": "Overcoming Knowledge Barriers: Online Imitation Learning from Visual Observation with Pretrained World Models",
    "abstract": "Pretraining and finetuning models has become increasingly popular in decision-making. But there are still serious impediments in Imitation Learning from Observation (ILfO) with pretrained models. This study identifies two primary obstacles: the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB). The EKB emerges due to the pretrained models' limitations in handling novel observations, which leads to inaccurate action inference. Conversely, the DKB stems from the reliance on limited demonstration datasets, restricting the model's adaptability across diverse scenarios. \nWe propose separate solutions to overcome each barrier and apply them to Action Inference by Maximising Evidence (AIME), a state-of-the-art algorithm.\nThis new algorithm, AIME-NoB, integrates online interactions and a data-driven regulariser to mitigate the EKB. Additionally, it uses a surrogate reward function to broaden the policy's supported states, addressing the DKB. Our experiments on vision-based control tasks from the DeepMind Control Suite and MetaWorld benchmarks show that AIME-NoB significantly improves sample efficiency and converged performance, presenting a robust framework for overcoming the challenges in ILfO with pretrained models.",
    "keywords": [
        "World Models",
        "Foundation Models",
        "Pretraining",
        "Imitation Learning from Observation",
        "Decision-making"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We identify two knowledge barriers for pretrained-model-based Imitation Learning from Observation and propose AIME-NoB to overcome these barriers, which showcases supreme performance on common benchmarks.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=EbOhZyxIzQ",
    "pdf_link": "https://openreview.net/pdf?id=EbOhZyxIzQ",
    "comments": [
        {
            "title": {
                "value": "Rebuttal (Part 3 / 3)"
            },
            "comment": {
                "value": "> I appreciate that the experiments are on visual observations, though they would be stronger if they included more complex tasks such as dextrous tasks, longer horizon tasks, or tasks with greater diversity/generalization\n\nThanks for the suggestions. We have conducted new experiments on more complex tasks as also suggested by reviewer SkmK. As from previous benchmark results, baselines BCO($\\alpha$), OT and PatchAIL are not well-performed, so we are not expecting them to be good on these even harder tasks. Thus, we currently mainly compare between AIME-NoB with AIME. If time permitted during the rebuttal phase, we will add these results later. All the results are available in Appendix L of the revised paper.\n\nFor MetaWorld, we include four additional tasks, namely pick-place, shelf-place, stick-pull and stick-push. The results are shown in Figure 15. From the results, AIME-NoB reliably outperforms AIME. \nEven in the very hard tasks stick-pull and stick-push, AIME-NoB manages around 60% success rates. However, AIME-NoB doesn't performs so well on pick-place and shelf-place. We conjecture it is due to the visual difficulties of the tasks. It is known that the world models based on reconstruction loss struggle at modeling small objects, which is the small cube we need to pick-up in these two tasks. Improving the world models' ability of modeling small objects or increase the resolutions of the observations will likely improve the performance. \n\nFor DMC, we plan to conduct additional experiments on the three humanoid tasks, namely humanoid-stand, humanoid-walk and humanoid-run. Since the humanoid embodiment is not in the initial list of the study, we essentially need to recollect the datasets on the embodiment and pretrain the world model. Thus, the experiments will take longer time to finish. The experiments are still running and we will report the results as soon as they are available."
            }
        },
        {
            "title": {
                "value": "Rebuttal (Part 2 / 3)"
            },
            "comment": {
                "value": "> EKB and DKB are defined quite informally and then are used extensively, sometimes in a hand-wavy way. I\u2019m not sure if introducing them is helpful for understanding. I think it would be better to either remove them or define them more formally and measure the extent to which they contribute to poor performance.\n\nThanks for the suggestions. We add formal definitions of EKB and DKB as:\n- $\\text{EKB} = \\mathcal{R}(\\pi_{\\omega^*}) - \\mathcal{R}(\\pi_{\\psi^*})$\n- $\\text{DKB} = \\mathcal{R}(\\pi_{\\mathrm{demo}}) - \\mathcal{R}(\\pi_{\\omega^*})$\n\nwhere:\n- $\\mathcal{R}(\\pi) = \\mathbb{E}_{a \\sim \\pi} [\\sum_t r_t]$ is the expected accumulate reward under policy $\\pi$.\n- $J_{\\mathrm{policy}}(q(a_t|o_{1:T}), \\pi(a_t|o_{\\leq t}), D_{\\mathrm{demo}}, D_{\\mathrm{body}}, D_{\\mathrm{online}})$ is the learning objective for the policy depending on an action-inference model $q(a_t|o_{1:T})$. For behaviour cloning based methods like BCO and AIME, it is essentially equivalent to $- \\sum_{o_{1:T} \\in D_{demo}} \\sum_{t} D_{\\text{KL}}(q(a_t|o_{1:T}) \\,|\\, \\pi(a_t|o_{\\leq t}))$. In this paper, we use Eq. 3 for AIME and Eq. 8 for AIME-NoB.\n- $\\phi^* = \\arg \\max_\\phi J_{\\mathrm{model}}(D_{\\mathrm{body}}, D_{\\mathrm{online}}, \\phi)$ is the optimal parameter for maximising the model learning objective $J_{\\mathrm{model}}$, e.g. Eq. 2 for AIME and Eq. 4 for AIME-NoB.\n- $\\hat p_{\\pi_{\\mathrm{demo}}}(a_t|o_{1:T})$ is the ground truth of empirical distribution of the demonstration data that serves as an oracle. \n- $\\omega^* = \\arg \\max_\\omega J_{\\mathrm{policy}}(\\hat p_{\\pi_{\\mathrm{demo}}}(a_t|o_{1:T}), \\pi_{\\omega}(a_t|o_{\\leq t}), D_{\\mathrm{demo}}, D_{\\mathrm{body}}, D_{\\mathrm{online}})$ represents the optimal policy parameters for maximising $J_{\\mathrm{policy}}$ with the oracle.\n- $\\psi^* = \\arg \\max_\\psi J_{\\mathrm{policy}}(q_{\\phi^*}(a_t|o_{1:T}), \\pi_{\\psi}(a_t|o_{\\leq t}), D_{\\mathrm{demo}}, D_{\\mathrm{body}}, D_{\\mathrm{online}})$ represents the optimal policy parameters for maximising $J_{\\mathrm{policy}}$ with the learned model.\n\nBased on these definitions, it is clear that:\n- To reduce the EKB, we need to bring the learned inference model closer to the oracle, i.e. to minimise $\\sum_{o_{1:T} \\in D_{\\text{demo}}} D_{\\text{KL}}(\\hat p(a_{1:T-1}|o_{1:T}) \\,| \\, q_{\\phi^*}(a_{1:T-1}|o_{1:T}))$. At iteration $k$, The online interactions we proposed allow the agent to rollout the current policy $\\pi_{\\psi^*_{k}}$ and collect a new trajectory. This new trajectory together with previously collected trajectories are used to train the model, i.e. improve the model parameter to $\\phi^*_{k+1}$ by maximising $J_{\\mathrm{model}}$. This parameter update serves to reduce the gap between the oracle and the learned model. Then the model parameter $\\phi^*_{k+1}$ is used to improve the policy parameter to $\\psi^*_{k+1}$ by maximising $J_{\\mathrm{policy}}$. Thus, through this iterative process, the EKB can be reduced.\n- To reduce the DKB without increasing the number of demonstrations, we modified the policy learning objective $J_{\\mathrm{policy}}$ by adding the value gradient loss based on the surrogate reward. The surrogate reward improves the policy $\\pi_{\\psi^*}$ on states not covered by the demonstrations. Thus, the DKB can be reduced."
            }
        },
        {
            "title": {
                "value": "Rebuttal (Part 1 / 3)"
            },
            "comment": {
                "value": "We would like to thank for your time to review our paper and giving constructive feedback. Please find our replies to your concerns below.\n\n> Overall, the technical contribution seems like a fairly basic combination of prior works. While many methods build upon prior works, this particular combination seems closer to an implementation choice than a significant technical contribution. The method is also more complex than prior methods as it introduces one hyperparameters and combines multiple objectives.\n\nWe respectfully disagree with the characterization of AIME-NoB as merely an implementation variant of AIME for several reasons:\n\n1. **Principled Design**: Our combination of techniques is specifically motivated by the analysis of the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB), as acknowledged by reviewers Yddj and SkmK. This makes AIME-NoB a principled solution rather than an arbitrary combination of prior works.\n2. **Novel Integration**: We are the first to integrate these techniques with pretrained world models for Imitation Learning from Observation (ILfO). This integration required significant adaptations, such as leveraging the pretrained image encoder's embeddings for discriminator training, which substantially stabilizes the adversarial training process.\n\nRegarding complexity:\n\n- While we do introduce new hyperparameters (replay ratio $\\alpha$ and value gradient loss weight $\\beta$), these additions are justified by their clear purposes.\n- We provide comprehensive ablation studies and analysis that illuminate their roles and practical tuning guidelines.\n\n> It\u2019s a bit unclear how this connects to a real-world problem. In robotics, you typically either collected data from a robot, where you can get actions or something close to them, or videos of humans/animals, which lacks actions but also has significant domain shift. Connecting the problem statement to a real world problem would be helpful to motivate the paper\u2019s significance.\n\nOur work has both immediate and long-term practical relevance:\n\n1. **Current Applications**:\n    - Action Space Mismatch: Even when robot data includes actions, the action space during data collection may differ from deployment. For example, open-x datasets recorded in end-effector Cartesian space may need to be applied to systems with torque-space controllers.\n    - Manual Demonstrations: When demonstrations are collected by physically moving the robot's end-effector, no explicit action information is available, necessitating observation-only learning.\n2. **Path Toward Cross-Embodiment Learning**: While our primary focus is on efficient imitation learning, we acknowledge that the ultimate goal is to learn from human or animal demonstrations that have different embodiments from our robots. Our work represents an important step in this direction, complementing parallel research exploring the cross-embodiment aspect [1]. By integrating AIME-NoB with such cross-embodiment methods in future work, we aim to address the complete challenge of efficient learning from demonstrations across diverse embodiments.\n\n> Some relevant related works to consider citing/discussing\n\nThanks for your suggestions of the related works. We have cited them in the revised version.\n\n[1] Mazzaglia *et al.*, GenRL: Multimodal-foundation world models for generalization in embodied agents, NeurIPS 2024"
            }
        },
        {
            "comment": {
                "value": "We would like to thank for your time to review our paper and giving constructive feedback. Please find our replies to your concerns below.\n\n> I appreciate that the authors conduct experiments on both tasks from DMControl and Meta-World, and consider tasks with varying difficulty in the case of Meta-World. However, I would have liked to see a few examples of tasks that are more challenging, i.e., where the proposed method (along with baselines) struggle a bit more. For DMControl this could be e.g. any of the Humanoid or Dog tasks, and for Meta-World this could be e.g. Stick Pull / Push or Pick Place (Shelf).\n\nThanks for your suggestions of the harder tasks. As from previous benchmark results, baselines BCO($\\alpha$), OT and PatchAIL are not well-performed, so we are not expecting them to be good on these even harder tasks. Thus, we currently mainly compare between AIME-NoB with AIME. If time permitted during the rebuttal phase, we will add these results later. All the results are available in Appendix L of the revised paper.\n\nFor MetaWorld, we include four additional tasks, namely pick-place, shelf-place, stick-pull and stick-push. The results are shown in Figure 15. From the results, AIME-NoB reliably outperforms AIME. \nEven in the very hard tasks stick-pull and stick-push, AIME-NoB manages around 60% success rates. However, AIME-NoB doesn't performs so well on pick-place and shelf-place. We conjecture it is due to the visual difficulties of the tasks. It is known that the world models based on reconstruction loss struggle at modeling small objects, which is the small cube we need to pick-up in these two tasks. Improving the world models' ability of modeling small objects or increase the resolutions of the observations will likely improve the performance. \n\nFor DMC, we plan to conduct additional experiments on the three humanoid tasks, namely humanoid-stand, humanoid-walk and humanoid-run. Since the humanoid embodiment is not in the initial list of the study, we essentially need to recollect the datasets on the embodiment and pretrain the world model. Thus, the experiments will take longer time to finish. The experiments are still running and we will report the results as soon as they are available.\n\n> I find that some of the claims / conclusions are a bit exaggerated relative to what the experimental results show. For example, the authors claim that \" the model pretrained on MW-mt50 offers much better results\" (L424) while the results in Figure 4(c) show a fairly small difference between the two curves (absolute ~15% increase in success rate on avg it seems). I would prefer claims to accurately reflect the evidence.\n\nWe thank you for this careful observation. We agree that our language should more precisely reflect the quantitative results. We have revised the paper accordingly, with changes highlighted in blue to better align our claims with the experimental evidence.\n\n> Figure 10 indicates that the proposed method succeeds in all of the considered tasks except Cartpole Swingup. I would have expected this task to be the easiest of them all; can the authors please comment on why the method fails on this particular task?\n\nThanks for the question. We were also very surprised by the poor result on CartPole during experiments. Thus, we have conducted a separate analysis on the CartPole at Appendix J. In a nutshell, we think it is a problem of the environment design -- the camera position cannot cover the entire moving space of the CartPole which makes the most important behaviour, the swingup, not visible from the image observation. This makes the AIME loss less effective and even harmful. Thus, we tune the hyper-parameters to lower the effect of the AIME loss and we have shown in this way AIME-NoB can solve CartPole swingup. Please refer to Appendix J for more details."
            }
        },
        {
            "comment": {
                "value": "Thanks for spending your time to offer feedback to our paper. We address your concerns below:\n\n> The logic is confusing. The authors state that in order to alleviate EKB, they additionally use online interactions. It is obvious that online trajectories will bring more benefits since the behavior policy to collect offline datasets differs from the current policy. So the improvement compared to AIME might be largely caused by the setting difference rather than algorithm improvement.\n\nWe appreciate this concern but respectfully disagree for several reasons:\n\n1. **Novel Setting as Contribution**: Extending algorithms to new settings with demonstrated improvements is a recognized contribution in reinforcement learning research. For instance, DrQ [1] extended SAC [2] to handle image inputs, while Cal-QL [3] adapted CQL [4] to online settings. Both works were considered significant contributions to the field.\n2. **Non-trivial Algorithm Design**: While online trajectories may intuitively seem beneficial, proper algorithm design is crucial for realizing these benefits. Our experiments demonstrate this: the BCO(\u03b1) baseline, which also utilizes online data for fine-tuning, shows no improvement or even degraded performance (Figure 2). This highlights that simply having access to online data is insufficient without proper algorithmic design.\n3. **Comprehensive Benchmarking**: While we compare against AIME to demonstrate resolution of knowledge barriers, our evaluation extends beyond this single comparison. AIME-NoB significantly outperforms state-of-the-art online ILfO algorithms like PatchAIL and OT, validating our approach's effectiveness.\n4. **Fundamental Solution to EKB**: We contend that online interaction is the most scalable solution to EKB. This mirrors human learning - many complex skills (e.g., athletic movements) cannot be mastered through observation alone but require practical experience. While performance improvements in purely offline settings are possible through specialized architectures or regularization, such knowledge engineering approaches are inherently less scalable than allowing online interaction.\n\n> The setting is very complicated. To my understanding, AIME's setting includes an offline dataset\u00a0($s_{off}$,$a_{off}$)\u00a0for world model training, an expert dataset\u00a0($s_{expert}$)\u00a0for imitation learning. And AIME-NoB additionally assumes access to the environment to collect an online dataset\u00a0($s_{on}$,$a_{on}$). The complexity of the setting makes it hard to make fair experimental comparisons as baselines might only assume access to a part of the datasets.\n\nWe acknowledge this complexity but would like to contextualize it:\n\n1. The apparent complexity largely stems from the current state of the field, where pre-trained world models are not yet readily available. Once such models become standard resources, the training pipeline will be significantly streamlined, as the current state in NLP and CV.\n2. Our setting is fundamentally comparable to standard online RL in terms of required components - the expert dataset effectively replaces the reward function in traditional RL. The additional offline dataset aligns with the growing paradigm of pre-training and fine-tuning in machine learning.\n3. While some comparisons (e.g., with PatchAIL and OT) may not utilize all available data, we argue that the ability to leverage diverse data sources is increasingly important in modern machine learning. Our method's ability to effectively utilize multiple data sources should be viewed as a feature rather than a limitation.\n\n[1] Yarats *et al.*, Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels, ICLR 2021 Spotlight\n\n[2] Haarnoja *et al.*, Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor, ICML 2018\n\n[3] Nakamoto *et al.*, Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning, NeurIPS 2023\n\n[4] Kumar *et al.*, Conservative Q-Learning for Offline Reinforcement Learning, NeurIPS 2020"
            }
        },
        {
            "comment": {
                "value": "We would like to thank for your time to review our paper and giving constructive feedback. Please find our replies to your concerns below.\n\n> How do you differentiate from previous work that uses online interactions and surrogate rewards?\n\nThe key innovation of AIME-NoB compared to previous online ILfO approaches like PatchAIL and OT is its ability to leverage pre-trained world models. While previous methods require training models from scratch, AIME-NoB can effectively utilize models pretrained on large datasets. As demonstrated in Figure 6(c), this capability is crucial for AIME-NoB's performance, showing much better sample-efficiency over training from scratch. This aligns with the broader paradigm shift in AI toward pre-training and fine-tuning approaches, which have proven highly effective across multiple domains.\n\n> How does it compare with RL with some hand-designed reward or a learned surrogate reward?\n\nFirst we would like to point out that one of the most important motivations to study ILfO is that we can remove the needs of reward. The reward engineering is a notorious problem in the RL field, which prevents RL from scaling to hundreds or even thousands of different tasks. So in our setup, we deliberately remove the reward to make the algorithm more applicable in a broader scope of problems.\n\nTo address your question directly, we compare AIME-NoB with Dreamer with the hand-designed reward from the environments. The results are shown in Appendix L of the revised paper. \n\nFor DMC, as we can see from the results in Figure 16, AIME-NoB achieve better sample efficiency on 8 out of 9 tasks excepted the problematic environment cartpole-swingup as we shown in Appendix J. On hopper-hop and quadruped-run, AIME-NoB is surpassed by Dreamer in the end. This is because as an imitation learning algorithm, AIME-NoB's performance is limited by the quality of the expert.\n\nFor the MetaWorld, since DreamerV3 is not officially benchmarked on it, we have to produce the results ourselves. we report the results at Figure 17. As the manipulation tasks typically impose challenges for exploration, we see that Dreamer with or without the help of the pretrained model struggles to accomplish the task within the 500k environment steps. In the same time, with the help of the demonstrations, it is much easier for AIME-NoB to explore the related regions in the observation which results in a better sample efficient.  This marks an advantage of using demonstrations over using a scalar reward to define the task.\n\n> Does the method generalize to real-world environments, such as learning from videos?\n\nIn this paper, we run all our experiments on image observations, i.e. videos as demonstrations. But indeed, we only run our experiments on the simulators. We believe AIME-NoB is well-suited for real-world applications, given its ability to leverage pre-trained models is particularly valuable for real-world scenarios where data collection is costly. Do you have some suggested real-world environments that we can test on?  Would these align with your expectations for real-world validation?"
            }
        },
        {
            "summary": {
                "value": "This paper studies two barriers in imitation learning from observations, namely the Embodiment Knowledge Barrier (EKB) and Demonstration Knowledge Barrier (DKB). EKB refers to the gap when generalizing to new observations and DKB refers to the gap when generalizing from limited number of expert demonstrations.\n\nIt proposes to use online interaction to reduce EKB and introduces a weighted loss between new interactions and pre=training data. For DKB, it proposes to use a surrogate reward, such as a discriminator in AIL.\n\nExperimental results suggest AIME-NoB outperforms baselines with significant margin in DMC and MetaWorld."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Clearly identify the shortcomings of existing IFO methods and propose reasonable solutions to reduce the gaps.\n- Extensive experiments on the different design choices, e.g. reward functions, and comparisons with baselines demonstrate solid improvement.\n- Paper writing is clear and easy to follow."
            },
            "weaknesses": {
                "value": "- The ideas to reduce EKB and DKB seems a simple combination of previous methods, such as using online interaction with weighted sampling, adversarial training, etc. It's unclear the distinctions from previous methods (such as AIL, e.g.) is significant enough.\n- These environments aren't hard to come up with a hand-designed reward or learning a surrogate reward. Lack of comparisons with Dreamer-like methods using hand-designed or surrogate reward."
            },
            "questions": {
                "value": "- How do you differentiate from previous work that uses online interactions and surrogate rewards?\n- How does it compare with RL with some hand-designed reward or a learned surrogate reward?\n- Does the method generalize to real-world environments, such as learning from videos?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper extends AIME to AIME-NoBarries, which addresses EKB with online interaction and DKB with online trajectories labeled with surrogate rewards. The authors evaluate the effectiveness of AIME-NoB in DMC and MetaWorld."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- AIME-NoB alleviates EKB through online interaction. To address DKB, the most natural way is to collect more expert demonstrations, which is expensive in terms of robot manipulation. Therefore, AIME-NoB circumvents this burden by providing reward signals using surrogate models and optimizes the policy with reward-labeled online trajectories.\n- Experiments in DMC and MetaWorld show that AIME-NoB can bring significant improvement to AIME."
            },
            "weaknesses": {
                "value": "- The logic is confusing. The authors state that in order to alleviate EDK, they additionally use online interactions. It is obvious that online trajectories will bring more benefits since the behavior policy to collect offline datasets differs from the current policy. So the improvement compared to AIME might be largely caused by the setting difference rather than algorithm improvement.\n- The setting is very complicated. To my understanding, AIME's setting includes an offline dataset $(s_{\\mathbf{off}},a_{\\mathbf{off}})$ for world model training, an expert dataset $(s_{\\mathrm{expert}})$ for imitation learning. And AIME-NoB additionally assumes access to the environment to collect an online dataset $(s_{\\mathrm{on}},a_\\mathrm{on})$. The complexity of the setting makes it hard to make fair experimental comparisons as baselines might only assume access to a part of the datasets."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the problem of imitation learning (IL) from (visual) observation, i.e., demonstrations do not contain action information using pretrained world models. The paper identifies two key bottlenecks in current IL algorithms, namely (1) OOD observations, and (2) OOD task configurations, which the authors refer to as the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB), respectively. The key technical contribution is an algorithmic extension of the method Action Inference by Maximizing Evidence (AIME); the core idea is to finetune the learned IL policy via limited online interaction and a RL objective, with surrogate rewards derived from the demonstration dataset. Experiments are conducted on DMControl and Meta-World from visual observations, and the proposed method compares favorably to both AIME without finetuning as well as other IL methods that can be finetuned online (BCO, OT, PatchAIL)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is generally well written and easy to follow. Illustrations (Figure 1 in particular) are useful for understanding the proposed method, and Section 2 provides sufficient background for an unfamiliar reader to appreciate the contributions.\n- Experiments are reasonably extensive, covering a variety of tasks from DMControl and Meta-World, as well as several (what appears to be) baselines appropriate for the problem setting. Empirical results are strong compared to baselines.\n- It is a purely empirical paper, but there are sufficient ablations to understand how the algorithm behaves in different settings (e.g. number of demos), and which components are the main drivers of performance (e.g. choice of surrogate reward)."
            },
            "weaknesses": {
                "value": "- I appreciate that the authors conduct experiments on both tasks from DMControl and Meta-World, and consider tasks with varying difficulty in the case of Meta-World. However, I would have liked to see a few examples of tasks that are more challenging, i.e., where the proposed method (along with baselines) struggle a bit more. For DMControl this could be e.g. any of the Humanoid or Dog tasks, and for Meta-World this could be e.g. Stick Pull / Push or Pick Place (Shelf).\n- I find that some of the claims / conclusions are a bit exaggerated relative to what the experimental results show. For example, the authors claim that \" the model pretrained on MW-mt50 offers much better results\" (L424) while the results in Figure 4(c) show a fairly small difference between the two curves (absolute ~15% increase in success rate on avg it seems). I would prefer claims to accurately reflect the evidence."
            },
            "questions": {
                "value": "I would like the authors to address my comments listed in \"weaknesses\" above during the rebuttal. I have one additional question:\n\n- Figure 10 indicates that the proposed method succeeds in all of the considered tasks except Cartpole Swingup. I would have expected this task to be the easiest of them all; can the authors please comment on why the method fails on this particular task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "**Problem setting**: have access to a demonstration data set without actions, an offline data set of suboptimal experience, and the ability to sample experience online without reward feedback\n\n**Proposed approach**: (1) balance the offline and online datasets when updating the model, (2) combine AIME and AIL objectives (though also considers other alternatives to AIL)\n\n**Experiments**: comparisons to relevant prior methods on DMC and MetaWorld tasks with visual observations"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Writing is fairly straightforward to understand \n- Results show clear improvements over relevant methods\n- The paper includes fairly extensive ablations/empirical analysis"
            },
            "weaknesses": {
                "value": "Overall, the technical contribution seems like a fairly basic combination of prior works. While many methods build upon prior works, this particular combination seems closer to an implementation choice than a significant technical contribution. The method is also more complex than prior methods as it introduces one hyperparameters and combines multiple objectives. \n\nOther weaknesses\n- It\u2019s a bit unclear how this connects to a real-world problem. In robotics, you typically either collected data from a robot, where you can get actions or something close to them, or videos of humans/animals, which lacks actions but also has significant domain shift. Connecting the problem statement to a real world problem would be helpful to motivate the paper\u2019s significance.\n- EKB and DKB are defined quite informally and then are used extensively, sometimes in a hand-wavy way. I\u2019m not sure if introducing them is helpful for understanding. I think it would be better to either remove them or define them more formally and measure the extent to which they contribute to poor performance.\n- I appreciate that the experiments are on visual observations, though they would be stronger if they included more complex tasks such as dextrous tasks, longer horizon tasks, or tasks with greater diversity/generalization\n\nSome relevant related works to consider citing/discussing:\n- Mobile (https://arxiv.org/abs/2102.10769), though uses low-dim observations\n- VMAIL (https://arxiv.org/abs/2107.08829), though assumes actions in demos"
            },
            "questions": {
                "value": "See suggestions in the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}