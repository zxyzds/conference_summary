{
    "id": "dIZB7jeSUv",
    "title": "CamI2V: Camera-Controlled Image-to-Video Diffusion Model",
    "abstract": "Recently, camera pose, as a user-friendly and physics-related condition, has been introduced into text-to-video diffusion model for camera control. However, existing methods simply inject camera conditions through a side input (like T2I-Adapter). These approaches neglect the inherent physical knowledge of camera pose, resulting in imprecise camera control, inconsistencies, and also poor interpretability.\nIn this paper, we emphasize the necessity of integrating explicit physical constraints into model design. We propose to apply epipolar attention for modeling all cross-frame relationships from a novel perspective of noised condition. This ensures that features are aggregated from corresponding epipolar lines in all noised frames, which overcomes the limitations of current attention mechanisms in tracking displaced features across frames, especially when features move significantly with the camera and become obscured by noise.\nAdditionally, we introduce register tokens to handle cases without intersections between frames, commonly caused by rapid camera movements, dynamic objects, or occlusions. To support image-to-video, we propose the multiple guidance scale to allow for precise control for image, text, and camera, respectively. Furthermore, we establish a more robust and reproducible evaluation pipeline to solve the inaccuracy and instability of existing camera control measurement caused by the limitations of structure-from-motion (SfM). Experimental results on RealEstate10K and out-of-domain datasets demonstrate that our approach achieves state-of-the-art performance. Checkpoints and training/evaluation codes are planned for release.",
    "keywords": [
        "Camera Control",
        "Image-to-video",
        "Video Diffusion Model"
    ],
    "primary_area": "generative models",
    "TLDR": "a Camera-Controlled Image-to-Video Diffusion Model enhanced by explicit geometry constraint.",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=dIZB7jeSUv",
    "pdf_link": "https://openreview.net/pdf?id=dIZB7jeSUv",
    "comments": [
        {
            "summary": {
                "value": "This paper extends an existing I2V method (DynamiCrafter) to accept additional camera viewpoint control. The input cameras are parameterized as Plu\u0308cker rays, serving as a position encoding. The authors additionally inset learnable Epipolar attention layers before temporal attn, which explicitly model cross-view geometry to further enhance the adherence to the input camera."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well written with a few nicely created figures, e.g., Fig. 1 and Fig. 2. The ideas of (1) clean vs. noisy condition & (2) register tokens are neat (but they are also related to the weakness and questions below). Despite tuned with static/rigid scene dataset -- RealEstate10k, from the few generated videos in the supplementary, the motion dynamic of foreground is not lost too much. The discussion in L457-479 is good and clearly supports the design choices. For fair comparison, the authors spend effort reproducing the recipes of baselines (MotionCtrl and CameraCtrl) in their framework (instead of applying the released ckpt out of the box) such that they all stand on a common ground."
            },
            "weaknesses": {
                "value": "* Contribution is clearly articulated in L.135-139 but not verified in the experiment: using Plu\u0308cker embedding and/or epipolar attention has become a norm in the 3D generation literature, e.g. [1]. Applying one of them, if not both, in the video generation field has also been done, e.g., CameraCtrl, CamCo etc. Therefore, the biggest technical contribution I see in this work, to my knowledge, is applying the idea of register tokens to account for occlusion, zero epipolar scenarios, etc. Despite simple, I find this idea neat, but I don't see any experiments ablating this key idea to analyze the effect. A contribution/novelty has be verified by the experiments. If this concern can be addressed, I am happy to raise the rating.\n\n* Method description is a bit insufficient. I need to read Sec. 5.3 to realize Plu\u0308cker rays are also used as global positional encoding similar to CameraCtrl. This is also part of the final method so it has to be described in Section 3.\u00a0\n\n\n\n[1] Kant et al., SPAD : Spatially Aware Multiview Diffusers, CVPR24. https://yashkant.github.io/spad/"
            },
            "questions": {
                "value": "1. The concept of clean vs. noisy conditions is also neat. The authors even make a figure to illustrate it (Fig. 1), but the knowledge of clean vs. noisy conditions seems not fully exploited in the method? For example, Fig. 1 points out that text and RT are clean conditions, but I don't see this new insight leads to new architectural designs? Adding camera information through Plu\u0308cker embedding and epipolar attention is a natural choice due to the nature of multi-view geometry, not because it is a clean condition. I wonder if making such separation is really necessary in the exposition.\u00a0 (This is more of a question for presentation, not technical details.)\u00a0\n\n2. More generated video results. All video examples in the supplementary material show only zooming-out camera motion, which, in my experience, is the easiest one to be learnt by the network. Please provide other examples, such as panning left/right, moving up/down, etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new method to inject camera pose conditions for controlled video generation using a diffusion model. The method uses Pl\u00fccker coordinates to represent the camera rays and introduces epi-polar mask attention for cross-view attention, which is more efficient than the full 3D attention counterpart. The proposed method is evaluated on the RealEstate10K dataset and achieves state-of-the-art performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The epi-polar mask attention layer proposed in the paper helps to enhance the camera control ability for video diffusion models. Also, it is plug-and-play \u2013 we don't need to retrain other modules of the original pretrained VDM.\n\n+ It is an interesting idea to include the register token to handle cases where the epipolar constraint on correspondences fails, although more discussion and evaluation on it are needed (see the weakness section).\n\n+ The paper addresses the inaccuracy in the SfM for robust evaluation metrics. Specifically, GLOMAP, a more state-of-the-art dense SfM pipeline, is used to validate the camera pose consistency."
            },
            "weaknesses": {
                "value": "+ Discussion and experiment missing for a key statement in the paper: While the paper states (in L112) that register tokens are included to handle rapid camera movements, occlusions, and dynamic objects, this contribution (also the key difference from CamCo) is not discussed in more detail. For example, how does this additional token help to deal with the non-epipolar constrained correspondences? Can the image-level register token handle pixel-level dense correspondences across frames (like moving arms of people)? In addition, the register token is not ablated, so it\u2019s unclear if it actually helps for dynamic scene generation.\n\nFurthermore, since this is the key difference from CamCo, more discussion and evaluation could help to distinguish this submission from CamCo.\n\n+ Some parts of the presentation are confusing: While the discussion in Fig. 1 and Sec. 1 on different types of conditions based on how much uncertainty the condition can reduce is interesting, how is that related to the epipolar condition? Does the epipolar condition reduce more uncertainty, hence making it a good condition? How does this view of condition serve as a principle to introduce the epipolar condition in the paper? More clarification is needed.\n\n+ For the quantitative results, cross-view consistency is missing, although it has been highlighted in the qualitative subsection (Fig. 7). Is there any reason why it is ignored in the tables as a metric? Cross-frame consistency can be another useful metric to show how the epipolar attention helps, in addition to camera controllability.\n\n+ In Fig. 5, the multi-resolution epipolar mask is shown but not discussed in the text. What is the motivation for using a multi-resolution epipolar mask?"
            },
            "questions": {
                "value": "Please refer to the weakness section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces CAMI2V, a camera controlled I2V model that integrates several camera-based components, such as epipolar attention and plucker embeddings, image conditions and text prompts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is easy to follow, figures are intuitive and look good. \n\nExperiments shows the model outperforms all available state-of-the-art works."
            },
            "weaknesses": {
                "value": "Novelty of the paper: The contribution of this paper highly resembles CamCo, which is released in June (3.5 months prior to the submission deadline). Both methods use plucker embedding, epipolar lines, and are aimed for image-2-video model. The paper mentioned that CamCo does not supports video trajectories with non-overlapping frames, and the introduction of register token alleviate this problem. Yet this can be  considered as a relatively minor improvement, and is not well supported by experiments. While CamCo does not release its code for reproduction, an ablation study on the token (comparing to other trivial techniques, such as averaging all K/V together), might be helpful to show its effectiveness. Aside from CamCo, CVD (Kuang et.al. 2024) also applies the epipolar attention in its cross-view modules, and CameraCtrl/CVD both use plucker embedding. All these methods challenge the novelty of this work. \n\nWhile many of the components proposed by the paper are originated from other prior works, the paper spent a huge portion of space to explain these components, overcomplicating the model itself. For example, in Figure 1/2 the authors try to analyze the differences between image/text conditions and noisy latents and show the importance of the epipolar attention, and later provides very detailed calculation on how to compute the attention maps. These contents are somehow redundant since they are already been proposed in prior works. \n\nThe author also claims the improvement of the evaluation protocol as one of the major contributions, yet the changes are rather trivial. It only replaces COLMAP with GLOMAP, and fixing the GT camera parameters to the registration. \n\nAll of the examples shown in the paper are in relatively small camera changes, which does not support the claims that CAMI2V can handle large camera movements with non-overlapped area. One of the example in Ours_I2V_on_unseen_image_3.gif also shows strong inconsistency across frames (5\u2019th one from the left)"
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper retrofits the text-guided video diffusion model with a camera control module, enhancing the camera trajectory controllability and 3D consistency for image animation. \n\nBy representing a pinhole camera model as a bundle of ray-level 3D embeddings through Pl&uuml;cker coordinates, the denoising process is conditioned on camera poses in a _clean_ and expressive manner. To facilitate multi-view 3D reasoning and efficiency, the epipolar attention mechanism is developed, which only aggregates cross-view information within a limited region along the epipolar line. \n\nThe authors test the method on RealEstate10K and out-of-domain datasets using an improved evaluation pipeline, and demonstrate its superiority over other alternatives."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The idea of introducing epipolar geometry to the temporal attention is straightforward and well-motivated, which allows the model to explicitly reason about the 3D world.\n- It is interesting and technically sound to configure register tokens for degenerate cases where no corresponding epipolar lines are detected.\n- Experiments on RealEstate10K and in the wild have shown that the proposed method improves the camera motion controllability and stability."
            },
            "weaknesses": {
                "value": "- One major concern is that the proposed method is somewhat lack of novelty. The epipolar attention mechanism is not new [1], while the Pl&uuml;cker embedding is adopted from [2].  There are some impressive innovations, e.g. the concept of _clean/noised conditions_ and the register tokens, but related exposition and analysis could be more thorough. Please see Q1-5 in the question section for details.\n- In the conclusion, the authors announce that their method _significantly_ improves the controllability and stability (Line 534-535), which may be an overstatement. In Tab.1, the advantage of CamI2V (the proposed method) over CameraCtrl is kind of minor, and in Tab.2, the introduction of the epipolar attention quantitatively appears to bring in only limited improvement. It would be nice to include additional (qualitative) results that demonstrate the effectiveness of each contribution. Also see Q6.\n- One contribution that the authors adapt GLOMAP for more robust evaluation pipeline raises another concern. It is not convincing that GLOMAP can effectively address limitations such as low resolution and dynamic scenarios. More details and experimental comparisons would be helpful to demonstrate the merits of the proposed evaluation protocol.\n\n[1] Hung-Yu Tseng, et al. \"Consistent View Synthesis with Pose-Guided Diffusion Models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023.\n\n[2] Hao He, et al. \"CameraCtrl: Enabling Camera Control for Text-to-Video Generation.\" arXiv preprint arXiv:2404.02101. 2024."
            },
            "questions": {
                "value": "1. Elaboration on the insight of the _clean/noised conditions_. The Pl&uuml;cker embeddings and the proposed epiplolar attention are regarded as clean conditions, but no further clarification is given. It would be preferable to include some mathematical analysis on how clean conditions can reduce uncertainty more effectively than noisy conditions, as the current perspective seems just intuitive and empirical rather than theoretically supported.\n2. I appreciate the idea of reserving register tokens for special cases, but the authors fail to demonstrate its effectiveness through essential ablation studies. \n3. In the epipolar attention part, a hard mask strategy is employed, which filters out pixels far from epipolar lines, while [1] proposes the epipolar weight matrix (a form of soft masks). It would be interesting to draw a comparison between them. The hard mask strategy might be computationally more efficient, so for example, more specific details about the computational complexity or running time would be beneficial.\n4. The epipolar geometry is based on the assumption that observations are all static. Wouldn't the proposed epipolar attention incur degradation in cases involving dynamic objects?\n5. The multiple guidance scale is not only similar to [2] but also lacks explanation and experimental support. The authors should reconsider whether to count it as one contribution (Line 133-134).\n6. Limited qualitative results. Camera trajectories used in supplementary materials are similar and relatively simple. Diverse camera trajectories for evaluating are in great need. \n\n- The authors may have to check the citation in Line 208-209: we follow _(Tseng et al., 2023)_ to represent cameras as ray bundles. \n- In the top middle of Fig. 4, what do the \"Project\" module and its learnable weights refer to? \n- Please check the format of notations. Make sure they are consistent throughout the paper. For example, the latent code should be in bold: $\\mathbf{z}_t$.\n\n[1] Hung-Yu Tseng, et al. \"Consistent View Synthesis with Pose-Guided Diffusion Models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023.\n\n[2] Jinbo Xing, et al. \"DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors.\" Proceedings of the 18th European Conference on Computer Vision (ECCV). 2024"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}