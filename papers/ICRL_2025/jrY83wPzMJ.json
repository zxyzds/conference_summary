{
    "id": "jrY83wPzMJ",
    "title": "Synchronous Scene Text Spotting and Translating",
    "abstract": "Text image machine translation aims to translate the content of textual regions in images from a source language to a target language. Compared with traditional document, images captured in natural scenes have more diverse text and more complex layout, posing challenges in recognizing text content and predicting reading order within each text region. Current methods mainly adopt pipeline pattern, in which models for text spotting and translating are trained separately. In this pattern, translation performance is affected by propagation of mispredicted reading order and text recognition errors. In this paper, we propose a scene text image machine translation approach by implementation of synchronous text spotting and translating. A bridge and fusion module is introduced to make better use of multi-modal feature. Besides, we create datasets for both Chinese-to-English and English-to-Chinese image translation. Experimental results substantiate that our method achieves state-of-the-art translation performance in scene text field, proving the effectiveness of joint learning and multi-modal feature fusion.",
    "keywords": [
        "multimodal machine translation"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "a novel scene text understanding and translating framework keep layout information",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=jrY83wPzMJ",
    "pdf_link": "https://openreview.net/pdf?id=jrY83wPzMJ",
    "comments": [
        {
            "summary": {
                "value": "In this work, the authors propose a scene text image machine translation method, which can detect, recognize and translate text in natural scene images. They also create image datasets for both Chinese-to-English and English-to-Chinese image translation tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The curated dataset STST800K can be beneficial to the community."
            },
            "weaknesses": {
                "value": "1. Considering the architecture and pipeline of the proposed model, the novelty of it is limited.\n2. The details of casting existing baselines (such as ABCNetv2, SPTSv2 and UNITS) as text spotter and translator are very crucial, but they are absent in the paper.\n3. The comparison in Tab. 5 is unfair. The proposed model is pre-trained with data from various sources (such as STST800K and WMT22) and fine-tuned on down-stream datasets, while the Qwen-VL model is not."
            },
            "questions": {
                "value": "Please address the questions and concerns in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method for synchronous scene text recognition and translation, which introduces a Bridge & Fusion module to integrate text and image modalities. Additionally, the paper collects multiple datasets to construct a scene text image machine translation (TIMT) dataset. The real data includes manually annotated test sets from ReCTS and HierText, while other datasets are annotated using the Qianwen API, followed by manual correction. The synthetic datasets are created based on existing tools. Experimental results demonstrate that the method in this paper outperforms existing text image translation models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tA method is proposed that can perform text spotting and text image machine translation, achieving better performance than previous methods.\n2.\tA Chinese-to-English and English-to-Chinese TIMT dataset is constructed based on multiple existing datasets, which includes annotating real data and synthesizing data using existing tools. This dataset is beneficial to the development of the TIMT field."
            },
            "weaknesses": {
                "value": "1.\tThe related work section needs adjustment. This paper mainly focuses on TIMT, yet the authors provide very little introduction to this field in related work, instead offering a large amount of introduction on text spotting.\n2.\tThe paper is somewhat difficult to read and does not provide some key experimental settings. For example, the setting of $\\alpha$ in Equation 2 and the size of the Qwen-VL model are not specified. Additionally, all models labeled as 'ours' in the experimental results are bolded, yet they are not the best, which is quite confusing. This result does not align with what the paper claims as SOTA.\n3.\tThe novelty is limited. The Bridge & Fusion module essentially extracts visual features based on the predicted text region's coordinates and then obtains multimodal features through cross-attention. This approach is very common in multimodal machine translation.\n4.\tThe paper claims that its model is end-to-end, however, actually the model still requires autoregressively generating coordinates and recognition output first, and then combines them with the image to autoregressively generate the translation. Therefore, it is not fully end-to-end.\n5.\tThe paper illustrates the issue of incorrect reading order in the pipeline method shown in Figure 1. However, the proposed method does not address this problem but merely provides such training data. Given this type of data, can the text spotting model in the pipeline also solve this issue?\n\n[1] PEIT: Bridging the Modality Gap with Pre-trained Models for End-to-End Image Translation"
            },
            "questions": {
                "value": "1.\tWhat is the size of the Qwen-VL model being compared, and what are the differences in settings with AnyTrans [1]? It would be better if compared with AnyTrans.\n2.\tWhat is the specific setting of alpha in Equation 2? Is the model training sensitive to this parameter?\n3.\tWill the data and code be open source?\n\n[1] AnyTrans: Translate AnyText in the Image with Large Scale Models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a synchronous text spotting and translating approach for Scene TIMT by jointly training the model with three sub-tasks, detecting text regions in the image, recognizing the source language text content, and translating it into the target language.\nIn order to solve the problem of mis-ordered text recognition and translation that occurs in existing pipelined methods, the paper introduces a \u201cBridging and Fusion (BAF)\u201d module to more effectively utilize visual and textual features to achieve efficient scene text translation. The method is evaluated on the self-constructed STST800K dataset and compared with existing methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The paper proposes that the synchronized training approach can improve the sequential reading accuracy of translation compared to the traditional pipeline model. \n2) The proposed BAF module is creative and helpful in fusing visual and textual features.\n3) The paper creates a large-scale dataset containing 800,000 samples of Chinese-English translations with real and synthetic data annotations, providing a new benchmark for scenario-based text translation tasks."
            },
            "weaknesses": {
                "value": "1) For text recognition and translation of complex scenes, especially fine-grained and layout complexity is not reflected in the paper. \n2) The way of synchronized training is not clearly described. The model's training data is large, and the resources consumed are not mentioned. Large datasets lead to more resources consumed for training models."
            },
            "questions": {
                "value": "1) How does the BAF module affect the reliability of the translated output in cases where errors occur in detection or recognition? Could more error analysis be provided to help understand the effects of the fusion of visual and textual features?\n2) How about the recognition and translation performance of the model in more complex scenarios?\n3) Can the model be applied to translations in languages other than Chinese and English?\n4) Can there be a more detailed explanation of the construction of the dataset\uff1f"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a unified framework for scene text image spotting and translation. A BAF Module is proposed to connect and integrate visual and textual features. Additionally, the authors have created the Scene TIMT (Scene Text Image Multilingual Translation) datasets for both Chinese-to-English and English-to-Chinese translations. Experimental results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper proposes a unified framework designed to enhance the performance of text image machine translation.\n2. This paper introduces a new Scene TIMT (Scene Text Image Multilingual Translation) dataset for this field."
            },
            "weaknesses": {
                "value": "1. The proposed method involves a larger set of parameters compared to other methods.\n2. When comparing with multimodal large models, the analysis lacks a comparison against the most recent large multimodal models, such as mplug-owl (CVPR 24), Monkey (CVPR 24), and InternVL (CVPR 24). It would be beneficial to have the performance results of the large multimodal models after they have been fine-tuned using the corresponding dataset.\n3. The authors claim that \u201cIn this pattern, translation performance is affected by propagation of mispredicted reading order and text recognition errors.\u201d However, there is a lack of experiments or evidence to verify this claim.\n4. Lack of experiment to verify the effectiveness of proposed BAF Module using different text spotter and Translation module.\n5. There is a lack of experimental validation to show how a unified framework improves different modules. Does it provide a greater improvement for end-to-end text spotting or for translation?"
            },
            "questions": {
                "value": "Why is it necessary to unify end-to-end spotting and translation? The author did not emphasize the necessity of unification in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}