{
    "id": "r8tMECbxOl",
    "title": "Language Models Are Good Tabular Learners",
    "abstract": "Transformer-based language models have become the de facto standard in natural language processing. However, they underperform in the tabular data domain compared to traditional tree-based methods. We posit that current models fail to achieve the full potential of language models due to (i) heterogeneity of tabular data; and  (2) challenges faced by the model in interpreting numerical values. Based on this hypothesis, we propose a method titled Tabular Domain Transformer (TDTransformer). TDTransformer has distinct embedding processes for different types of columns. The alignment layers for different types of columns transform column embeddings to a common embedding space. Besides, TDTransformer adapts piece-wise linear encoding for numerical values in transformer-based architectures. We examine the proposed method on 76 real-world tabular classification datasets from the standard OpenML benchmark. Extensive experiments indicate that TDTransformer significantly improves the state-of-the-art methods.",
    "keywords": [
        "Language Model; Tabular Data; Natural Language Processing"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=r8tMECbxOl",
    "pdf_link": "https://openreview.net/pdf?id=r8tMECbxOl",
    "comments": [
        {
            "summary": {
                "value": "The paper is about modifications to the transformer architecture and training objectives for solving [tabular classification tasks](https://huggingface.co/tasks/tabular-classification) using transformers. The authors propose techniques for linearizing tabular data and encoding it into a pre-trained transformer model like BERT or Roberta models. Features are encoded differently depending on whether the feature is categorical, numerical, or binary, with column-type aware position encodings. Further, the paper includes a pre-training strategy based on a self-supervised and a supervised contrastive learning strategy. The authors compare their proposed method, named TDTransformer, with strong baselines used for tabular classification tasks, e.g., XGBoost, and demonstrate reasonable accuracy improvements over these baselines."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper explores an important area -- tabular classification -- where traditional ML methods continue to outperform pre-trained transformers.\n- The experiments in the paper seem to be very thorough, covering 76 tabular classification tasks in the openml benchmark and ablations of the key design choices used in the proposed method."
            },
            "weaknesses": {
                "value": "Despite its strengths, I feel that the paper needs significant improvements. \n\nMajor concern: Clarity of the Proposed Method.\n\n- Sections 3.1 and 3.2 introduce an embedding E, but Section 3.3 refers to an embedding z without clarifying if or how z is derived from E. Line 261 states, \"z_i is the hidden representation for the i-th table row,\" yet Sections 3.1 and 3.2 only discuss column embeddings, not row embeddings. It appears that \"column embedding\" might actually refer to \"feature embeddings.\" Therefore, clearer terminology and more consistent naming of components would enhance understanding.\n- Section 3.3 (Training Pipeline) begins by mentioning both training and fine-tuning but only elaborates on pre-training objectives (SSCL and SCL). The absence of detail on the fine-tuning objective leaves it unclear how the model was fine-tuned after pre-training.\n\nMinor concerns:\n- Line 33: The reference to \"tree-based methods\" is not clear. Providing specific examples or references would improve clarity.\n- Line 110: There appears to be a typo -- should \"j_i\" be replaced with \"y_i\"?"
            },
            "questions": {
                "value": "- How does the proposed method handle columns with string datatype?\n- How does the proposed method handle missing values in a table?\n- Does the proposed method encode multiple rows in a table, or is it only one row at a time?\n- Line 357: Why is subset selection required?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates the application of transformer-based language models to tabular data, highlighting their limitations due to the heterogeneous nature of tables and the interpretting of numerical values. To tackle the challenges, the paper proposes Tabular Domain Transformer (TDTransformer), which utilizes distinct embedding processes for different column types and incorporates piece-wise linear encoding (PLE) for numerical columns. This approach aims to enhance the semantic understanding of language models when interpreting tabular data. Extensive experiments on 76 real-world datasets demonstrate that TDTransformer outperforms existing methods, suggesting a promising direction for leveraging language models in the tabular data domain."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Introduces a novel framework (TDTransformer) specifically designed for tabular data, addressing the limitations of traditional transformer models.\n- Employs distinct embedding processes for different column types, improving the model's ability to capture semantic information.\n- Employs piece-wise linear encoding (PLE) for numerical columns, improving the model's ability to capture numerical values.\n- Demonstrates strong performance across a wide range of real-world datasets, indicating the framework's robustness and applicability.\n- The paper is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "- The complexity of the TDTransformer architecture may lead to increased computational costs compared to simpler models.\n- The reliance on specific embedding techniques may limit the model's generalizability to other types of data or tasks."
            },
            "questions": {
                "value": "Have you ever considered using a different LM as the table encoder (for example, llama or mistral)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents the Tabular Domain Transformer (TDTransformer) framework, designed to enhance transformer-based models' performance on tabular data. This framework uses distinct embedding processes for categorical, numerical, and binary columns. It also includes a positional encoding. After fine-tuned the model on the data from OpenML benchmark. It outperforms some traditional tree-based and transformer based methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper provides a comprehensive experiments and evaluations on the traditional methods on tabular data."
            },
            "weaknesses": {
                "value": "This proposed framework is lack of novelty,  it largely adapts existing techniques without proposing fundamentally new methodologies, which may limit its impact. For example TAPAS from google is also a transformer-based table parser.\n\nI think authors can follow the paper I posted in the review as well as some other papers related to more up-to-date LLM-based methods to address the concern of out dated baselines, for details please refer to the paper I posted below.\n\nBy using transformer-based models with specialized encoding and pre-training techniques, the computational complexity and training cost increase. In many cases, traditional tree-based models like XGBoost or CatBoost achieve comparable results with far less computation, making TDTransformer potentially less attractive for practitioners dealing with tabular data in resource-constrained environments.\nMost of the baseline methods are out dated. I'd like to see more comparison with up-to-date methods like the LLM-based. Please provide runtime and resource usage comparisons between TDTransformer and tree-based models, or to discuss scenarios where the increased computational cost might be justified by performance gains."
            },
            "questions": {
                "value": "Can you add some experiments on LLM based methods like the ones mentioned in Fang, Xi, et al. \"Large language models (LLMs) on tabular data: Prediction, generation, and understanding-a survey.\" (2024).?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Transformer language models excel at natural language processing but underperform in tabular data compared to traditional algorithms. Authors claim that this is due to 2 factors: a) heterogenous data in tables, and b) handling of numerical values. Authors propose TDTransformer to fix these issues.\nTo address a), TDTransformer uses different embedding processes for different types of columns. To address b), TDTransformer adapts piece-wise linear encoding.\n\nThey evaluate the model on 76 datasets, improving on SOTA methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The main strength of this paper is empirical (results are strong). Additionally, the architectural modifications introduced are well-motivated.\n\nThe paper itself is well-written and well-presented, and the code, open-source, looks good at a first glance. I haven't attempted it myself, but I assign high success ratios for reproducing this work."
            },
            "weaknesses": {
                "value": "While the motivations for the architectural motivations are well-principled, and they do work empirically, I still think that the paper has some claims that in my view are not backed up by the paper itself or by existing literature. For example, \"overcome the transformer-based architectures\u2019 incapability of interpreting heterogeneous data\". This is a very strong statement. It's not clear to me whether Transformers have any intrinsic incapability of interpreting heterogeneous data, especially given the success of Transformers handling multimodal data in other domains. Perhaps these statements should be more cautious."
            },
            "questions": {
                "value": "Say that I have a computational budget C and I want to obtain the best possible results on a given tabular dataset with some features (e.g., certain number of rows). Should I choose XGBoost or TDTransformer? I'm not necessarily asking for scaling laws (which would be ideal), but I do think readers would appreciate some pointers on this.\n\nSimilarly, \"PLE introduces an inductive bias that is beneficial to the training process\". Is this an inductive bias that is going to be less relevant at scale?\n\nContrastive losses are sensitive to batch sizes, how does batch size affect TDTransformer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors present a method called TDTransformer to handle tabular data. They use different embedding methods for categorical, numeric and binary columns: categorical is embedded as text, numeric uses an extension of the PLE method to encode numbers in the [-1, 1] range using quantiles, and binary columns are just 0 or 1. They explore removing positional embeddings.\nTDTransformer outperforms XDGBoost on the benchmark they consider."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Strong performance on OpenML against XGBoost/CatBoost"
            },
            "weaknesses": {
                "value": "- The presentation is quite lacking, also because of janky notation. For example, in Eq. (5) it seems like the linear transformation is applied to the concatenation of PLE features for all cells in the columns which seems to be absurd, because there should a variable number of cells for each table. All equations that present the architecture are similarly unclear.\n- It's also unclear what is the difference between SCL and SSCL in Eq (11) and Eq (12). In what sense one uses labels and the other one doesn't?\n- I am not an expert in the field, but a cursory look at arXiv pointed out a missing comparison system: https://arxiv.org/pdf/2403.01841, published at ICLR last year."
            },
            "questions": {
                "value": "- It's not clear whether any tuning has been performed on XGBoost or default hyperparameters have been used which can have a dramatic effect on performances: see Table 1 in https://arxiv.org/pdf/2403.01841. Can you clarify this issue?\n- How does CTA work? Tables are (arguably) permutation invariant wrt rows, but removing positional embeddings altogether would make the two following tables have the same representations:\n\n(1)\n| a | b |\n| - | - | \n| 0 | 0 |\n| 1 | 1 |\n\n(2)\n| a | b |\n| - | - | \n| 0 | 1 |\n| 1 | 0 |\n\n- Given that you use 0 to represent `false` would your approach be able to distinguish the two following tables?\n\n(3)\n| a | b |\n| - | - | \n| 1 | 0 |\n| 1 | 0 |\n\n(4)\n| a | c |\n| - | - | \n| 1 | 0 |\n| 1 | 0 |\n\n- Is PLE invariant to scaling?\n- Have the authors tried a simple LLM baseline? This would be more to contextualise the practical implications of the paper, rather than its scientific validity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}