{
    "id": "pQoD5MtZve",
    "title": "Iterative Vectors: Boost In-Context Learning within Activations",
    "abstract": "In-context learning (ICL) has emerged as a standard paradigm for utilizing language models. Although ICL is convenient due to the absence of backpropagation, selecting and processing appropriate demonstration examples can be difficult and time-consuming, particularly when the number of examples is large. We propose to explore the potential of activation space through Iterative Vectors (IVs), a technique designed to enhance in-context performance and necessitating only forward inference passes. IVs are employed by first extracting and iteratively steering activations within a language model, then applying them during inference with minimal computational and memory overhead. We evaluate IVs across numerous tasks using three popular models and observe significant improvements. Our findings suggest that activation steering can serve as a promising direction for in-context learning, thereby opening new avenues for future research.",
    "keywords": [
        "Large Language Model",
        "In-context Learning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "Iterative Vectors (IVs) enhance in-context learning by steering activations in language models through forward inference passes, leading to significant performance improvements without backpropagation.",
    "creation_date": "2024-09-21",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=pQoD5MtZve",
    "pdf_link": "https://openreview.net/pdf?id=pQoD5MtZve",
    "comments": [
        {
            "title": {
                "value": "Sorry, it should be FV"
            },
            "comment": {
                "value": "Sorry, it was a typo. I was referring to the paper that proposed the FV method."
            }
        },
        {
            "comment": {
                "value": "Thank you for your insightful review! Before we address your concerns, could you please specify which \"FT paper\" you are referring to (in the second question)?"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an interesting idea for improving the efficiency of in-context learning (ICL). Building on former observations like Function and Task Vectors, this work invented an iterative procedure that mirrors gradient-based training. The authors compare this method with the two predecessors and standard ICL on multiple models and datasets, showing accuracy improvement. They also study the effect of the number of shots and extraction episodes on this method."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* This paper has performed experiments in a principled way, using multiple datasets and base models, and conducted hyperparameter searches for the proposed method and previous method alike.\n\n* This paper is well-written. The motivation and most of the details of the method are well explained."
            },
            "weaknesses": {
                "value": "* I'm unsure how to interpret some experiment results; more details in the questions.\n\n* The functionality of this method (IV) overlaps with PEFT, but it lacks comparison with any of the PEFT methods. Both IV and PEFT take a small number of training examples, spend some amount of upfront computation (training in the case of PEFT, iterative extraction in the case of IV), produce a small number of additional states (parameters in the case of PEFT, V vectors in the case of IV), and use these states to specify a model more effective on a downstream task. PEFT has already been well adopted and understood. So, if we are to prove this new approach is worth exploring, we need to show that it has unique advantages or better outcomes. (e.g., measuring or analyzing the memory requirement could be helpful.)"
            },
            "questions": {
                "value": "* When we extract the vector iteratively, are the examples encountered in the new iteration the same as the first iteration or resampled every time?\n\n* The FT paper also performed experiments on agnews and sst5, with GPT 1.3B and GPT2.7B, which should be weaker than the models in this paper. But they had sst5 = 39.5, agnews = 65.3 for GPT1.3B, and sst5 = 39.1, agnews = 65.7 for GPT2.7B, significantly higher than the numbers in Table 1. I would guess there are some differences in your experiment setting. I wonder if you could test your method with their setting and report the numbers.\n\n* If we want to test the inference time reduction, why do we need to introduce a brand new dataset? I would guess it will be more informative to use established datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Iterative Vectors (IVs), a method designed to enhance in-context learning (ICL) within large language models (LLMs) by leveraging activation steering in the activation space rather than in the discrete prompt space. The authors propose extracting activation vectors from the difference in model activations for queries with and without prior context examples, and iteratively reapplying these vectors during inference to improve ICL performance. This approach is evaluated on multiple models and diverse classification tasks, demonstrating performance gains over traditional ICL and other activation vector methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The concept behind IVs is simple and straightforward, and the approach of leveraging activation vectors in the model\u2019s activation space intuitively makes sense."
            },
            "weaknesses": {
                "value": "1. The paper suffers from poor clarity in both writing and figures. For instance, the contributions listed in the Introduction are not clearly differentiated, making it difficult to identify the unique impact of the proposed approach. Additionally, Figure 1 is difficult to interpret and does not clearly explain the IV process. Both the writing and presentation need substantial refinement for the paper to be clear and accessible.\n2. The approach is evaluated exclusively on classification tasks, which restricts its generalizability. No experiments are conducted for tasks involving multi-token responses or other types of reasoning, limiting the broader applicability of the findings.\n3. The theoretical section does not provide sufficient insight into why IVs improve ICL performance. Although the paper claims that meta-gradients from in-context examples may not fully capture the task, it lacks an explanation of how IVs solve this limitation. \n4. More thorough justification and explanation would be necessary for the approach to be convincing."
            },
            "questions": {
                "value": "1. Could the authors add a written-out algorithm to clarify the procedure for extracting and applying IVs? This would improve reproducibility and understanding of the method.\n2. In the theory section, what specific mechanisms in IVs address the purported limitations of ICL in the discrete prompt space? A more detailed explanation here would add to the paper\u2019s depth.\n3. Given the observed performance drops with a low number of extraction episodes, are there any strategies the authors could suggest to improve stability in such contexts?\n4. The paper notes that extraction and inference strength values play a critical role in performance. Could the authors explain the role of each hyperparameter in a clearer manner? Detailed guidance on tuning these values would improve the usability of the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce iterative vectors (IV), a method designed to enhance the in-context learning (ICL) performance of language models during inference. Their main comparison is with the standard ICL approach, where $k$ examples are included in the prompt, leading to increased performance and computation costs as $k$ increases. In contrast, IV captures the meta-gradients made by in-context examples in a condensed format, which can be applied during inference. The computational cost of these updates is incurred mostly during the creation of IV and is amortized over multiple inference rounds. The authors conduct several experiments to showcase the advantages of using IV in various ICL tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The framing of iterative vectors as a way to capture the meta-gradients that occur during ICL is well-done and well-motivated (Section 3.1).\n2. This paper addresses an important problem: efficiently boosting LM performance without relying on longer prompts."
            },
            "weaknesses": {
                "value": "1. Presentation: Section 3.3 is difficult to follow. In particular, lines 298-317 are meant to describe how the iterative component of IV works, but I could not understand it even after multiple readings. As currently written, this section heavily relies on formulas to communicate how IV works. There is nothing necessarily wrong with including the formulas, but the surrounding text lacks clarity. Consider adding a new figure, pseudocode, or examples.\n2. Missing experiment: Line 36 states, \u201cThis finding is also corroborated by our experiments, wherein adding more in-context examples does not always result in improvements. Instead, it introduces uncertainty, which compromises LMs\u2019 reliability and usability. I could be mistaken, but do not see any experiments that support this claim.\n3. Missing zero-shot experiments: Given the framing in Section 3.1, it seems natural to include zero-shot experiments. I recommend that the authors either include them or provide an explanation in the paper justifying why these experiments were left out. The authors allude to a reason in line 482, but I did not find it compelling."
            },
            "questions": {
                "value": "1. Please clarify how the iterative component of IV works.\n2. Which experiment supports the claim in line 36?\n3. Where is C in formula 17 defined?\n4. Please clarify why only the one-shot setting was considered for Table 1. I do not follow the reasoning in lines 350-351.\n5. How are iterative vectors different from task and function vectors? A short section comparing their differences will help me better understand the the technical novelty of iterative vectors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Iterative Vectors (IVs) for enhancing in-context learning (ICL) by exploring the potential of activation space without the need of backpropagating and with minimal computation overhead.\n\nIVs are generated by extracting the difference of attention activations from queries with and without preceding examples during the inference process, with the goal of capturing the insights the model learns from the input examples. These IVs are then iteratively reintroduced into the model, facilitating the formation of more stable and effective vectors while continuously incorporating information from subsequent examples.\n\nThe authors evaluate IVs across three models and different classification tasks, reporting improvements over previous  vector-based ICL methods Function Vectors and Task Vectors. They also demonstrate how IVs reduce inference time and scale with the number of demonstrations and extraction episodes. An ablation study is provided to examine the impact of hyperparameters on ICL stability and performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This work has several notable strengths regarding its topic and results:\n- S1 -  The paper addresses an interesting topic of in-context learning (ICL), where authors are specifically focusing on task vectors that enable fast and robust ICL performance. The method Iterative Vectors (IV) contributes to the ongoing research in this direction by further supporting the use of task vectors instead of standard ICL.\n- S2 - The authors demonstrate that their method, Iterative Vectors (IV), achieves strong performance over 3 different models and 13 tasks, outperforming previous vector-related approaches without requiring additional memory or backpropagation."
            },
            "weaknesses": {
                "value": "The paper has several areas that could benefit from improvement in terms of clarity, novelty, evaluation, and formatting:\n- **W1** - The introduction lacks a complete motivation for the use of task vectors, making it challenging to understand their necessity in in-context learning. \nThe writing appears unpolished and the paper should be improved structural-wise. For example, Section 3 discusses the theory of transformers as meta-optimizers, but its connection to the main contributions and approach of IVs is unclear. I would suggest either integrating this better with more related works on this topic, or focusing more directly on the proposed method. \nNext, the paper lacks clarity in some parts. For example, the method description could be clearer, especially regarding the hyperparameters, which are mentioned, but not explained properly. Figure 2 also seems to be redundant. \nMoreover, there are formatting issues, such as excessive empty space around Figure 3 and within the related work section.  Reformatting the text and refining the introduction would improve the overall presentation and clarity.\n- **W2** - The related work section misses important studies on in-context learning, including research on demonstration sensitivity, brittleness, and pretraining dynamics [1, 2, 3] - areas relevant to task vectors. Additionally, it does not reference recent work on task vectors in visual models [4], multi-modal models [5], and VQA [6]. Including these would provide a more comprehensive overview of the field and the paper would be more complete.\n- **W3** - Figure 3 is difficult to interpret due to small font sizes and a lot of details and information. I suggest the authors simplify this figure or move it to the appendix while summarizing key findings in the main text to enhance comprehension and clarity.\n- **W4** - The proposed method appears to be an extension of existing task-vector based approaches, with slight modifications such as aggregation and different prompt designs applied to new tasks. Further, the method relies on several hyperparameters that require tuning, which might not be practical in few-shot in-context learning situations where minimal parameter adjustment is preferred. Have the authors conducted some additional analysis of the feature space or the underlying mechanisms that could strengthen the paper\u2019s contributions? \n- **W5** - The experiments are limited to classification tasks and do not extend to more complex tasks that reflect real-world scenarios or involve multi-modal models. Moreover, the paper does not include evaluations on newer large language models like Llama 3. Have the authors tested different model sizes? If so, how much does the performance differ in such scenarios? \n- **W6** - The finding that performance improves with more iterations and demonstrations is somewhat expected in small-data scenarios and few-shot learning. Prior research on multi-modal [5] and visual task vectors [4] has reported similar results. Have the authors conducted more analysis in this direction that can bring new insights for this direction? \n\n[1] https://proceedings.mlr.press/v139/zhao21c.html\n\n[2] https://arxiv.org/abs/2202.12837\n\n[3] https://arxiv.org/abs/2205.05055\n\n[4] https://arxiv.org/abs/2404.05729 \n\n[5] https://arxiv.org/abs/2406.15334\n\n[6] https://arxiv.org/abs/2406.13185"
            },
            "questions": {
                "value": "While the paper introduces an interesting approach with Iterative Vectors (IVs) that shows performance improvements over existing methods for vector-based in-context learning methods, there are several areas that require improvement. Strengthening the motivation, refining the method description, expanding the related work to include recent studies, and providing more comprehensive evaluations on a variety of tasks and models would significantly improve the paper. With these revisions, the work could become a strong contribution to the community. \n\nMy further questions to the authors are:\n- Could you evaluate your method on more challenging tasks that are closer to real-world applications to demonstrate its broader applicability?\n- Could you evaluate your method on multi-modal models?\n- How does your method perform on newer language models, and is there a significant difference in performance between smaller and larger models?\n- Have you analyzed what more iterations and demonstration do to the feature space of the task vectors? \n- Have you observed compositionality with Iterative Task Vectors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper discusses the use of Iterative Vectors (IVs) to enhance In-Context Learning (ICL) in language models. IVs are shown to improve performance in various tasks, particularly in few-shot learning scenarios. The paper compares IVs with other vector methods (TV, FV) and demonstrates its effectiveness in reducing inference time and scaling with the number of demonstration shots. The results suggest that IVs offer advantages in improving model performance and can potentially be applied to more advanced applications."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The paper shows rigorous process of the research methodology, experimental design, and result analysis. The study rigorously evaluates IVs across multiple models and diverse tasks, providing a comprehensive assessment of their performance.\n\nThe clarity of the paper is evident in the detailed explanation of IV generation, their iterative reintroduction into the model, and their impact on ICL performance. The use of clear and concise language, along with illustrative examples, enhances the readability and understanding of the paper. Additionally, the comparison with other methods and the discussion of practical applicability contribute to the overall clarity of the research.\n\nBy demonstrating the effectiveness of IVs in improving ICL performance, the paper addresses a critical aspect of language models related to few-shot learning and adaptation to real-world datasets. The practical implications of IVs in enhancing model performance, reducing inference time, and scaling with the number of demonstration shots underscore their significance in real-world applications."
            },
            "weaknesses": {
                "value": "Clarity could be improved in certain figures and tables. \n\nTable 1: Include brief reminders of the terms FV and TV directly within the table, as they are first defined in Section 2.1, which is distant from their use here. Providing concise definitions and a comparison with IV will improve comprehension.\n\nFigure 3: To improve readability, round the score to one decimal place, which should make the figure appear less dense and easier for readers to interpret at a glance.\n\nTable 4: Converting the results to a plot could visually emphasize the effectiveness of IVs in extracting and utilizing a greater number of examples. This format may better showcase the comparative strengths of IVs.\n\nTo make the experiment more comprehensive, consider expanding the experiment in Inference Time Experiment to evaluate FV and TV on the emoji dataset. Assessing their respective performance and inference times would offer a clearer picture of each method's efficiency in various contexts."
            },
            "questions": {
                "value": "Could you provide further details on the emoji dataset? Additional examples and explanations might help clarify its structure and significance.\n\nThe paper primarily evaluates models with relatively small parameters (6B, 7B, and 13B). Would extending experiments to include larger models, such as 70B, offer additional insights or strengthen the findings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}