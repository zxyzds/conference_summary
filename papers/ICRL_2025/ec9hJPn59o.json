{
    "id": "ec9hJPn59o",
    "title": "BiEnhancer: Bi-Level Feature Enhancement in the Dark",
    "abstract": "The remarkable achievements of high-level vision tasks (e.g., object detection, semantic segmentation) under favorable lighting conditions highlight the persistent challenges faced in low-light vision. Previous studies have mainly focused on enhancing low-light images to create visual-friendly representations, often neglecting the differences between machine vision and human vision. This oversight has led to limited performance improvements for high-level tasks. Furthermore, many approaches rely on synthetic paired datasets for training, which can result in limited generalization to real-world images with diverse illumination levels. To address these issues, we propose a new module called BiEnhancer, which is designed to enhance the representation of low-light images by optimizing the loss function of high-level tasks to improve performance. BiEnhancer decomposes low-light images into low-level and high-level components and performs feature enhancement. Then, it adopts an attentional feature fusion strategy and a pixel-wise iterative estimation strategy to effectively enhance and restore the details and semantic information of low-light images and improve the machine-readable representation ability of low-light images. As a versatile plug-in module, BiEnhancer supports end-to-end joint training with diverse high-level tasks. Extensive experimental results demonstrate that the BiEnhancer framework outperforms state-of-the-art methods in both speed and accuracy.",
    "keywords": [
        "Low-light image enhancement; low-light object detection; night-time semantic segmentation"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "BiEnhancer, a versatile plug-in module, enables end-to-end joint training with various ligh-level vision tasks to boost performance under low-light conditions.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ec9hJPn59o",
    "pdf_link": "https://openreview.net/pdf?id=ec9hJPn59o",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a new module called BiEnhancer designed to enhance the representation of low-light images by optimizing the loss function of high-level tasks to improve performance. BiEnhancer decomposes low-light images into low-level and high-level components, adopting an attentional feature fusion strategy and a pixel-wise iterative estimation strategy to effectively enhance and restore the details and semantic information."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.  The proposed BiEnhancer module presents a novel approach to enhancing the representation of low-light images by optimizing the loss function of high-level tasks, which is a unique perspective compared to previous methods that focus on image enhancement for human visualization. \n2. The authors have conducted extensive experiments to validate the effectiveness of the BiEnhancer framework.  \n3. Improving the performance of high-level vision tasks in low-light conditions is a practical problem."
            },
            "weaknesses": {
                "value": "1. The innovation is not sufficient. The key issue is that the proposed architecture is similar to the previously published FeatEnHancer, including the use of similar network components.\n\n2. The goal of constructing the BiEnhancer network module is unclear. It is not understood why the output images of the BiEnhancer network module suffered from severe quality degradation.\n\n3. The quantitative metrics do not show significant improvement, especially in Table 4. Obvious image quality degradation can be observed in the visualized Figure 4.\n\n4. The writing is not smooth and is difficult to understand. Particularly, the implementation details of the fusion strategy in Section 3.3 of the methodology are hard to comprehend."
            },
            "questions": {
                "value": "See the above Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a BiEnhancer structure for low-light image enhancement that can be cascaded with a variety of HIGH-level tasks to improve performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Originality: This paper introduces a novel module designed to address existing challenges in the field.\nQuality: The writing logic is fairly clear, and the quality is acceptable.\nClarity: The writing is overall clear.\nSignificance\uff1aGeneral significance."
            },
            "weaknesses": {
                "value": "1. the proposed design cannot match the problem to be solved. This paper proposes BiEnhancer for low-light image enhancement, but from its design, there is nothing specific to low-light. In other words, it can be used for input images under any conditions.\n2. lack of innovation. The main contribution of the paper is the design of the BiEnhancer module for low-light enhancement placed at the front end of multiple high-level tasks. However, the following doubts exist: firstly, as mentioned in the previous question, there is no reflection anywhere that the module is specific to low illumination. Secondly, the design of the module is not particularly attractive, and even some of the design justifications remain to be demonstrated, as shown in the following Questions for the authors."
            },
            "questions": {
                "value": "1. The innovation is insufficient. Regarding the proposed BIENHANCER, it is unclear how its design specifically caters to low-light conditions; the paper does not provide any explanation. In fact, based on the content, BIENHANCER could be understood as a generic module that, like any other module, could be integrated into any network as a component, lacking any distinctive characteristics. It could even be added to networks intended for non-low-light scenarios. \n2. In section 2.1 RELATED WORK, no references are provided for the last three years.\n3. For the \u201cIh\u201d obtained in the proposed module, what is its role?\n4. The description of Figure 2 is unclear: for the inputs of the IEM, three are given outside the blue rectangle but only two inside; how are the two outputs of the BFFM combined into one? The dimension of \u201cIh\u201d is not consistent with the text.\n5. The description of the proposed module in lines 173-179 does not align with the corresponding Figure 2.\n6. In Figure 3, the output of (a) should be the input of (d), but the dimensions are labelled differently.\n7. Figure 3(a) has only one input, but the corresponding position in Figure 2 shows two.\n8. In Section 3.3, what is the function of \"split\"?\n9. Unclear description in Section 3.4\u3002\n10. In Section 4.1, please give references for all compared methods.\n11. In Section 4.2, the common method of face detection should be used instead of the method of target detection.\n12. For face detection, visual results are missing.\n13. Which method is \u201cXue et al.\u201d in Table 4? There is no explanation or reference. In addition, why is Retinexformer used in the first two tasks and Xue et al. used in the last task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This manuscript proposed a bi-level feature enhancer to boost the performance of low-light object detection. Experiments show the superiority over previous feature enhancement methods for both performance and processing speed."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "+ The proposed method performs better than the previous method, FeatEnhancer."
            },
            "weaknesses": {
                "value": "+ Motivation. BiEnhancer proposed a \"plug-and-play\" method for low-light images. However, the proposed BiEnhancer still requires training on the specific detector and low-light image dataset. It can't be applied to other detectors/normal-light images without fine-tuning. However, the low-light enhancers can be applied to various pre-trained detectors. Besides, I think the concept of \"feature enhancement\" makes no sense. If you assign more parameters and multi-scale mechanisms to the backbone, the performance will gain, but one will never call it a **low-light enhancer**.  The community is not **\"neglecting\"** the difference between machine vision and human vision, it is just off-topic for enhancement methods to feed the detector up.\n+ Weak baselines.  As time flies, numerous stronger methods have been proposed in the literature, some of them are even cited by the authors in the related work part of the manuscript. I believe some of them can perform better than these 2 baselines.\n+ More modern detectors. The proposed method only examines RetinaNet and Sparse R-CNN. However, more modern detection frameworks typically involve stronger augmentation techniques and better pretrain (Object365, for instance). I wonder if the proposed method still works under these new settings.\n+ Novelty. What is the difference between the proposed method and FeatEnhancer? This part should be highlighted.\n+ Unfair comparison. It seems that the parameters of image enhancers are fixed while the Bi-Enhancer can learn from high-level loss. This is unfair, I wonder if you can unlock the parameters of these enhancers to see what happens."
            },
            "questions": {
                "value": "Please, refer to the weakness part where I have stated my concerns about this work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to enhance the performance of low-light image enhancement for downstream high-level tasks. The proposed method is called BiEnhancer, which optimize the loss function of high-level tasks to improve performance. Moreover,  BiEnhancer employs an attentional feature fusion and a pixelwise iterative estimation strategy to effectively restore the details which can help the semantic information."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper optimizes the high-level tasks' loss besides the low-light enhancement target, and the improvement on the downstream high-level tasks is achieved."
            },
            "weaknesses": {
                "value": "1. Although this paper is designed as a bi-level framework, this paper mainly consider the improvement on the high-level tasks, and ignore the visual improvement for the human vision. As we can see in the Fig.2 and Fig.4, the enhanced image is difficult to see without suitable color and illuminations. The bi-level target should be enhance the low-light image which is both human- and machine-friendly.\n\n2. Although this framework has sacrificed the visual quality, the improvement on the downstream tasks is not very obvious, and improvements are almost 1-2 points.\n\n3. There is not sufficient novelty or novel motivation for the network design, e.g., the skip connections, RepConv, and DWConv are all common network architectures in the computer vision field.\n\n4. The presentation of this paper can be further improved. For example, the tables of 3 and 4 are of over-large size, while Figures 4 and 5 are not large enough for visualization."
            },
            "questions": {
                "value": "1. What is the visual quality for the enhanced images with BiEnhancer? Show the quantitative results.\n\n2. The comparison with more SOTA low-light image enhancement methods should be provided."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the problem of low-light image enhancement to support high-level vision tasks. Its key idea is to design a plug-and-play module called BiEnhancer that consists of feature extraction, feature fusion, and iterative estimation. The proposed approach was shown to outperform FeatEnHancer (ICCV'2023) the previous SOTA benchmark. Performance improvements (based on Tables 2-4) are modest - .e.g., 0.3-1.1 in terms of mAP and mIoU."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ Originality: The idea presented in Sec. 3.4 (Iterative estimation) is novel and interesting. It borrows the idea from diffusion model to iteratively optimize the feature representation of low-light images.\n+Significance: The reported experimental results have shown consistent improvement over all benchmark methods including Zero-DCE and FeaEnHancer. In some situation, the performance gain can be over 1 point (e.g., Table 3)."
            },
            "weaknesses": {
                "value": "-Quality: I have found the literary presentation of this work significantly lacking. The main body of this work (Sec. 3) is difficult to read, especially Sec. 3.2 and 3.3. Those sections contain lots of technical details (how we do it?) without discussing motivation or new insight. The interesting idea inspired by DDPM was only briefly mentioned in Sec. 3.4, which makes it difficult to appreciate its elegance. \n-Clarity: I have been struggling all the way with identifying the authors' contribution from the standard practice in the literature. For example, Fig. 3 shows the details of three architectures but did not say anything about their relationship, which one or which part represents the authors' new design. The feature fusion formula (i.e., Eq. (3) in Sec. 3.3) is also presented without any substantial justification. \n-Technical contribution: Fig. 2, to me, is still an open-loop design. Such open-loop design has fundamental limitations because it lacks the feedback from high-level vision tasks. A more fruitful approach, in my biased opinion, is a closed-loop design that jointly optimize low-level and high-level vision tasks. Note that in an open-loop design, any errors made by BiEnhancer can not be corrected  later - e.g., the amplified noise as shown in Fig. 2 has been totally overlooked by the authors.\n-Experimental results. I took a look at the mAP results reported by FeatEnHancer (https://github.com/khurramHashmi/FeatEnHancer). On the ExDark and Dark Face datasets, they are very different from those reported in this paper. I suspect that there are some parameter setting issues. But the missing of reference for FeatEnHancer (Line 307) seems unforgivable because this is the most important reference in benchmark methods."
            },
            "questions": {
                "value": "1) Have you investigated the issue of noise amplification in your study? From Fig. 2, one cannot help wondering if the amplified noise, without proper guidance, might have a negative impact on the high-level vision tasks.\n2) What could cause the result inconsistency between ICCV'2023 and your experiments? In FeatEnHancer + Featurized Query R-CNN, mAP of 86.3 and 69.0 were reported for ExDark and Dark Face datasets. Why did you have 49.3 and 25.5 (according to Tables 2 and 3)?\n3) Can you elaborate on the design of iterative estimation in Eq. (4)? I think this equation might contain some error because the terms within the parenthesis appear inconsistent (i.e., the last term contains I_n).\n4) What is the rationale for visual comparison in Fig. 4? It seems to me that visual quality is not the purpose of BiEnhancer, right? Visually, we should not expect any meaningful results from enhanced images."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}