{
    "id": "nHmaQf2wJC",
    "title": "Aligning Large Language Models With Preference Privacy",
    "abstract": "Alignment is a crucial part in the implementation pipeline of Large Language Models (LLMs) that utilizes human feedback to ensure that LLMs adhere to human values and societal norms. This introduces privacy threats associated with the identity and preferences of the labelers responsible for creating the human feedback data. Several recent works have explored using differential privacy (DP) as a notion to protect the privacy of human labeled data; primarily relying on DP-SGD based solutions, which privatize the gradients during fine-tuning and alignment. Human preferences, however are only associated with the labels of the (prompt, response) tuples; therefore DP-SGD based approaches can be superfluous, providing more privacy than necessary and can degrade model utility. In this work, we focus on the problem of aligning LLMs with preference level privacy, which only preserve the privacy of preferences provided by humans. We build and expand upon the concept of label DP for this problem, and present a series of increasingly sophisticated, yet practical privacy preserving mechanisms for alignment. Specifically, starting from a standard randomized response (RR) mechanism which randomly flips human preferences, and it's corresponding \\textit{unbiased} RR mechanism (which ensures an unbiased loss during alignment), we propose a new mechanism, PROPS (PROgressively Private Self-alignment). PROPS works in multiple stages as follows: in each stage, the privately trained and partially aligned model from the previous stage to act as a labeler for the training data for the next stage and combine it with RR which is repeated across multiple stages. Motivation for PROPS comes from the following critical observations: a) learning to label correct preferences might be an easier problem than generating responsible content; b) progressively combining RR with partially aligned models for labeling preferences significantly reduces the amount of necessary perturbation needed for privacy and also shows the potential of possibly reducing the number of human labeled preference samples. We present proof-of-concept experiments that demonstrate the feasibility and effectiveness of our proposed approach and show that preference privacy based alignment can still attain a comparable utility to their non-privately aligned counterparts.",
    "keywords": [
        "LLMs",
        "Differential Privacy",
        "Alignment"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We propose LLM-based alignment techniques that ensure differential privacy for the labelers of the preference datasets typically used in alignment..",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=nHmaQf2wJC",
    "pdf_link": "https://openreview.net/pdf?id=nHmaQf2wJC",
    "comments": [
        {
            "summary": {
                "value": "This work addresses privacy in aligning large language models (LLMs) with human values by protecting the privacy of human preferences in feedback data, using a refined approach beyond traditional DP-SGD methods. It introduces PROPS, a mechanism that progressively privatizes model alignment by combining randomized response with partial self-alignment over multiple stages. This method aims to protect preference-level privacy with minimal degradation to model utility, showing promise for reducing reliance on human-labeled samples while maintaining alignment efficacy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Work seems interesting and seems to focus on a problem that is largely overlooked. Algorithm is novel, and in the setting appears to do well. See Questions."
            },
            "weaknesses": {
                "value": "I remain unconvinced of the prevalence of the label-privacy setting. Further, the method itself is presented well, but I feel that more understanding of how the parameters are selected is needed. Lastly, I\u2019d like a more thorough evaluation. See Questions."
            },
            "questions": {
                "value": "* You write \u201cWe note that in typical alignment scenarios, the prompts and responses are not generated by humans (the two responses (y1 , y2 ) are usually obtained from the fine-tuned LLM)\u201d Why are human preferences only on the labels, and not associated with the prompts? Since humans often are generating the prompts. What setting are humans only voting on prompt-responses? For example, knowing that someone is voting on \u201cwhat to do if someone has disease A\u201d is revealing.  \n\n* I think you should consider looking to PATE, where you have a private dataset that you train with ML, and then ensembling the labels on a public dataset with DP. The setting is slightly different, as you have different labelers label the same data (privately), but it\u2019s quite related, as the labels are private, but the data is public.\n\n* While not as relevant, I\u2019ll point you to PMiXeD (https://arxiv.org/abs/2403.15638) which applies the PATE framework to the predictions of LLMs.\n\n* I\u2019d like more examples of label-privacy machine learning methods in your related works.\n\n* Regarding going from preference-level to labeler-level DP, you should consider looking at research on personalized DP, such as: Individualized PATE (https://arxiv.org/abs/2202.10517). Specifically in Individualized PATE, different teachers can produce different numbers of labels because they have different privacy budgets. Other related works are Individual Privacy Accounting for DPSGD (https://arxiv.org/abs/2206.02617), and (less relevant) Individualized DPSGD (https://arxiv.org/abs/2303.17046)\n\n* Can you make the computational cost of this method more explicit?\n\n* How do you pick the number of rounds you wish to do, and how to split your privacy budget?\n\n**Evaluation:** \n* Why is this win-tie-loss rate for this specific dataset the right measure here?\n*  It\u2019s hard to tell what the non private baseline is, and the existing baseline\n* You seem to focus on \u201cquality\u201d but it seems like you have 2 axes that you care about: quality and helpfulness/relevance. It does not seem like these two measures should be combined into 1 (e.g. \u201ci don\u2019t have a physical location\u201d is given as a high-quality answer, but it is clearly not helpful, as you state yourself). How should we interpret the performance of your model on these two axes?\n* For figure 5, it\u2019s hard to tell if your results in the figure are repreesntative, or just those 2 examples.\n\n\n**Typo:**\nAbstract uses \u201cit\u2019s\u201d not \u201cits\u201d - \u201cmechanism which randomly flips human preferences, and it\u2019s corresponding unbiased\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed the notion of preference privacy for LLM alignment as an alternative to using DPSGD to protect the privacy of the annotators. Their motivation stems from the fact that DPSGD-based approaches can be excessive, providing more privacy than necessary, and can degrade model utility. Their contributions are in two folds, leveraging DPO as the base alignment algorithm; first, they proposed an unbiased loss for DPO for randomized response method. Secondly, they proposed PROPS, a multi-stage alignment algorithm with preference privacy. In their experiments, they showed that preference privacy based alignment can attain preference a comparable utility to their non-privately aligned counterparts."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors made two major contribution; the introduction of unbiased estimation loss to DPO based on randomized response and PROPS.\nPROPS is a multi-stage algorithm which trains models to generate labels / preferences for the next batch. The preferences and the noisy groundtruth are combined with MLE and are used for the alignment. \nI believe that the problem of the privacy risks of alignment is important, and this paper does a fine job in showing how important the problem is."
            },
            "weaknesses": {
                "value": "The authors did a good job in highlighting the importance of the problem. However, there are a number of claims in the paper that are not experimentally grounded. For instance, one of the major claim of the paper is that DPSGD-based method hurts utility. I don't see such experiments in comparing DPSGD-based method with their proposed preference privacy.\nSecondly, a claim is that PROPS can be used for RLHF-based alignment. No experiment in this direction either. While there is a \"PROPS Algorithm & Remarks\" section, it does not do justice to actually varying the claims.\nLastly, only one LLM (GPT-2 Large) was used for their experiment"
            },
            "questions": {
                "value": "I have the following questions for the authors. I am willing to increase my score if the authors can address my concerns:\n\n- To verify one major claim of the paper, could you please show experimentally that DPSGD indeed hurts privacy compared to the proposed preference privacy? One way would be to compare the DPSGD-based method with the preference privacy under the same privacy budget\n\n- While the proposed PROPS is clever, I see a very high similarity to the idea from PATE [a], where models are trained on disjoint datasets and a noisy label is obtained from the aggregation of the teacher ensembles in a DP fashion? Can you comment on why PATE approach cannot be adapted here?\nIn fact, I would rather say that PROPS inherently follows the private-public data setup of PATE. Here, what is public is the (prompt, responses) while the label is private.\n\n- Can the authors comment on the need / rationale for using fresh batch of samples for PROPS?\n\n- One major problem is in the experimental setting. While the proof of concept is appreciated, using other language models like the Pythia suite of models [b], Gemma [c], or Llama3 [d] that are current used in practice and showing how different models behave with PROPS alignment would have been a better approach.\n\n- A problem of DPO which PROPS use as a base method is in its generalization to out of distribution. Now that RR is added, how does it affect the performance of PROPS?\n\n- Can the authors provide an experiment on using different alignment like the RLHF. It would make the paper stronger to see such transferability and if there are any potential issues with those alignment approaches.\n\n- In figure 4, why is the win-tie-loss rate of $\\epsilon=\\infty$ lower than $\\epsilon=2$?\n\n- In figure 5, what I can deduce is that training with no privacy ($epsilon=\\infty$) leads to somewhat arbitrary response while training with privacy ($\\epsilon=1$) can give some reasonable response (Prompt 2). I find this somewhat interesting. To better understand what is going on, can the authors also present the result of DPO without any privacy. Just normal DPO.\n\n- While the motivation sounds plausible, I don't see a real threat here. I understand why DP could be used as a blanket covering for the privacy of the annotators, but the motivation for an annotator being exposed for their preference is somewhat unrealistic to me. Can the authors point to a reference in which this is practical?\n\n[a] Scalable Private Learning with PATE https://arxiv.org/abs/1802.08908\n\n[b] Pythia: A suite for analyzing large language models across training and scaling. https://arxiv.org/abs/2304.01373\n\n[c] Gemma: Open models based on gemini research and technology. https://arxiv.org/abs/2403.08295\n\n[d] llama 3 herd of models. https://arxiv.org/abs/2407.21783"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the problem of aligning large language models with preference level DP, where the binary human preferences for fixed (prompt, response) pairs are the only sensitive information.  The authors propose a randomized-response style mechanisms and variants thereof to solve this problem.  Experimentally, they show how the utility of their mechanism improves with the privacy budget, and that their more sophisticated PROPS algorithm outperforms a simple randomized response approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Principled approach to a timely problem.\n2. Demonstrated better-than-baseline performance in one experimental setting. \n3. Well written and mostly easy to follow.\n4. The illustrative example in Figure 5 is pretty nice."
            },
            "weaknesses": {
                "value": "1. Experiments are pretty thin and could benefit from more thorough treatment.  Are there any baselines or other prior work you could compare against (e.g., mechanisms for label-DP)?\n2. The problem considered is fairly narrow.  Is the core algorithm you proposed more broadly applicable to other settings?  How does your work compare algorithmically to other prior work for label-DP?\n3. Technical contribution is fairly small, although I think the simplicity is a good thing here.  But related to (1) above simple approaches would benefit from a more thorough empirical treatment."
            },
            "questions": {
                "value": "1. In experiments, can you compare against optimizing the biased loss function?\n2. RR provides local DP, are there any benefits to only preserving central DP?  I'm not quite sure what that mechanism would look like in this setting, but maybe a discussion somewhere in the paper could be good."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies alignment in LLMs, focusing on using differential privacy (DP) to protect the privacy of human labeled data. Previous works used full DP for the training, which is superfluous when only the preferences of the labelers need to be protected, rather than the prompt and response pairs. The authors propose improvements by considering randomized response (RR) for Label DP, and also present a new algorithm PROPS for doing a multi-staged training with the noised labels. They run experiments and show that their new methods have good utility compared to nonprivate baselines even at high privacy regimes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors adapt existing techniques to a new setting for aligning LLMs, and propose a new multistage mechanism (PROPS) for training the model.\n- PROPS improves over RR for some settings of parameters."
            },
            "weaknesses": {
                "value": "- The technique in Lemma 1 for unbiasing the loss function is not new (see e.g. the paper \u201cDeep Learning with Label Differential Privacy\u201d by Ghazi et al., in NeurIPS 2021).\n- It would be great if the authors included more results about Labeler-level DP, since it is more relevant for applications and it is unclear how much the model utility will be affected.\n- The experimental results were only on a single dataset, and it would be good to have results for multiple types of datasets.\n- The authors propose their new method as an improvement over existing works with full DP, but do not compare against it in their experiments."
            },
            "questions": {
                "value": "- How does the number of stages in PROPS affect the utility?\n- Why does PROPS perform worse than RR at $\\epsilon=0.1$ in Figure 3? Is there a standard deviation for the results?\n- Does it improve utility to run PROPS for multiple epochs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}