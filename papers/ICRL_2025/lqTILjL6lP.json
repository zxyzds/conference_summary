{
    "id": "lqTILjL6lP",
    "title": "RESuM: A Rare Event Surrogate Model for  Physics Detector Design",
    "abstract": "The experimental discovery of neutrinoless double-beta decay (NLDBD) would answer one of the most important questions in physics: Why is there more matter than antimatter in our universe? To maximize the chances of discovery, NLDBD experiments must optimize their detector designs to minimize the probability of background events contaminating the detector. Given that this probability is inherently low, design optimization either requires extremely costly simulations to generate sufficient background counts or contending with significant variance. In this work, we formalize this dilemma as a Rare Event Design (RED) problem: identifying optimal design parameters when the design metric to be minimized is inherently small. We then designed the Rare Event Surrogate Model (RESuM) for physics detector design optimization under RED conditions. RESuM uses a pre-trained Conditional Neural Process (CNP) model to incorporate additional prior knowledge into a Multi-Fidelity Gaussian Process model. We applied RESuM to optimize neutron shielding designs for the LEGEND NLDBD experiment, identifying an optimal design that reduces the neutron background by $(66.5 \\pm 3.5)$% while using only 3.3% of the computational resources compared to traditional methods. Given the prevalence of RED problems in other fields of physical sciences, especially in rare-event searches, the RESuM algorithm has broad potential for accelerating simulation-intensive applications.",
    "keywords": [
        "surrogate model",
        "simulation",
        "rare event search",
        "AI4Sci",
        "AI for physics",
        "conditional neural process",
        "Bayesian methods",
        "emulator",
        "Multi-Fidelity Gaussian Process"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "We developed a Rare Event Surrogate Model (RESuM) powered by Conditional Neural Process to optimize particle physics detector design under high-variance design metrics.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=lqTILjL6lP",
    "pdf_link": "https://openreview.net/pdf?id=lqTILjL6lP",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a surrogate model for rare events in physics.  The model is a hybrid model combining a conditional neural process (CNP) model and a multi-fidelity Gaussian process (MFGP).  The CNP is pre-trained on both low-fidelity (LF) and high-fidelity (HF) simulation data.  Subsequently the CNP model is used to calculate additional design data (averaged CNP scores for LF and HF data).  The MFGP model is initially trained using HF, HF CNP and LF CNP data, and further tuned using Bayesian Optimization (BO) equipped with the integrated variance reduction acquisition function to minimize the total variance of the model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The RESuM model is well-motivated and the application in physics is quite interesting.  While I am not a statistician nor familiar with the CNP model, I am very familiar with GP models and BO design, and can see no obvious mistakes in the paper.  Moreover the results are thorough and I can see a clear application for this model outside of that considered here."
            },
            "weaknesses": {
                "value": "It appears that the real goal of this paper is to optimise particle detector design.  As such, it strikes me that the surrogate model need only be accurate in those regions of design space that are \"good\" - ie minimise background noise.  However the RESuM model is designed to model the entire design space, including \"bad\" designs (eg it may request HF simulations for regions whose prior confidence bounds are such that we know whp will not be appropriate in practice).\n\nAs such perhaps a more efficient approach would be to replace the integrated variance reduction acquisition function used in the BO portion of the design with a more goal-oriented acquisition function like expected improvement (EI) or GP-UCB, which would naturally focus computational effort on optimizing the detector design (ie the real underlying goal) rather than modeling all possible detectors (essentially the first step in the current design procedure).\n\n(Note I intend this more as a point for discussion rather than a major criticism - the current approach is valid, but perhaps the strong focus on modeling is obscuring the real goal)."
            },
            "questions": {
                "value": "In addition to the above:\n\n- I am a little unclear regarding the role of CNP here.  Is it primarily a means to include the LF simulations in the overall model in a way that takes advantage of the physical insight in the combined LF/HF data?\n- in the active learning / Bayesian optimization phase you only run HF simulations.  Have you considered using multi-fidelity BO [1,2] to take advantage of the cheaper LF simulations as well?\n- As a minor note, on line 53 you say that N is $\\mathcal{O} (10^4)$.  Do you mean $N$ is of order $10^4$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The article presents a study on the optimal design of a neutrinoless double-beta decay detector, a Rare Event Design. The authors introduce a surrogate model, RESuM, to solve this optimization problem and gain computational cost. This algorithm is based on Conditional Neural Process, incorporating prior information. The authors applied their work to the LEGEND experiment, a physics detector for neutrinoless double-beta decay.\n\nThe Rare event Design problem is mainly well stated and very clear.  The Large and Small N scenarios provide a very good understanding of the problem.\n\nBayesian prior knowledge with Conditional Neural Process (CNP) is clear in the text. CNP is explained in Appendix 11. The details shows the CNP is rich : by the use of surrogate modelling it generates data as fast simulator. It is more efficient for exploration of parameter space, uncertainty quantification and model validation.\n\nThe simulations, results and validation of RESuM are well explained.\n\nRESuM reduces LEGEND neutron background by (66.5 \\pm 3.5)% using only 3.3% of the computational power of traditional methods. This is a very good achievement.\n\nAll in all, this is a very good paper."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The study is well conducted, very clear and concise, very interesting. The stated results are also very impressive and I hope this will lead to a larger breakthrough regarding neutrinoless double-beta decay community. The code is well presented and commented.\n\nThe work can be applied to Astronomy or Material Science. There are many other application possibilities in physics.\n\nThe work is motivated by the experimental discovery of neutrinoless double-beta decay (NLDBD), leading to answering the question \u2018Why is there more matter than antimatter in our universe?\u2019. Which makes a link with one of the opening questions of Javier Duarte talks at ICML24 \u2018What is our universe made of?'. Thus, this work is very relevant for the community, and source of enthusiasm since the potential discovery of NLDBD would  lead to a Nobel-Prize-level breakthrough in physics."
            },
            "weaknesses": {
                "value": "The authors indicates in the limitations and applications part that due to limitation of ressources available they computed only 100 High Fidelity simulation, maybe the publication of the article will help to convince to gain more ressources.\n\nThe authors also add that the active learning functions used un RESuM are simplistic, they will consider more sophisticated one in future work.\n\nMinor comments that do not impact the score :\n\nThey are many inconsistencies in the notations, here are what I noted while reading, some might be redundant\n\nLine 95, independent of the other events\n\nLine 97, \\mathbf{Phi} is not defined. Is it for conciseness?\n\nLine 97,  dollar i dollar-th\n\nLine 133, dollar { y \\in  blablabla } dollar should make it appear on the same line\n\nLine 153-154, Why highlight +1 and +0? Should be in dollar 1 dollar, same for dollar 0 dollar\n\nLine 186, \\mathbf{\\phi}\n\nLine 187, the same\n\nLine 188,  \\mathbf for theta\n\nLine 172, The use of the nuisance parameters is unclear to me. The vertical bar followed by X_ki, phi_ki, theta_k means conditionally, if possible this should be improved.\n\nLine 183, Figure 1 title is \u2018Overview of the RESuM framework\u2019 which is not the case of Figure 3. I assume the authors mean Figure 1 then.\n\nIn Figure 1, if the MFGP combines prediction from LF and HF simulations, why are the arrows pointing at them and not starting from them?\n\nIn the section 3, the vector of design parameters is \\mathbf{\\theta} thus it should be the same in all the study.\n\nIn the introduction, line 49, 51, 52. According to section 4 it should be in \\mathbf?\n\nIn the section 3,\n\nLine 92-93, is it in \\mathbf?\n\nLine 96, in \\mathbf\n\nLine 108-109, the same\n\nLine 113-114, the same for \\theta and \\phi\n\nLine 120,  the same for \\theta, N should be in  dollar N dollar\n\nLine 124-125, the same for \\theta\n\nLine 126, the same for \\theta twice in the eq\n\nLine 126-127, \\infty should be used and same for \\theta\n\nIn the section 4,\n\nLine 140, mathbf for \\theta\n\nLine 151, the same for \\theta_k the k-th simulation trial\n\nLine 152, the same for \\theta_k and \\phi_ki\n\nLine 153, it should be  dollar N dollar,  dollar y dollar also\n\nLine 160-161, \\mathbf for \\theta_k and \\phi_ki\n\nLine 187-188, \\mathbf for \\phi and \\theta\n\nLine 204, the same for \\theta\n\nLine 209, the same for \\theta_k and \\phi_ki\n\nLine 212-213, the same for \\theta and \\phi_ki\n\nIn the section 5,\n\nLine 261-263, the parameters of design should be the same as in the figure 2 (\\varphi in particular). Figure 2 represents explicitly 4 parameters, n is missing\n\nLine 265, \\mathbf for the design parameters and the corresponding space\n\nLine 289-290, \\mathbf for \\phi\n\nLine 298, the same for \\phi\n\nLine 360, I suggest \u2018plotted against the five parameters in Fig3.\u2019 Since you only present five.\n\nLine 370-371, the same for \\theta\n\nLine 395-400, parameters should be consistent with line 261-263\n\nLine 404-405, parameters should be consistent with line 261-263\n\nLine 409, also\n\nFigure 4 and Table 1, should be consistent with line 261-263\n\nLine 441-446, the same for \\theta and the corresponding space\n\nIn section 6,\n\nLine 491 and 492, the same for \\theta and \\phi\n\nIn section 2,\n\nLine 93-93, I would have introduced the stochastic process X_1, \u2026, X_N and specify after that\n\nLine 213, \\mathbf for theta and phi\n\nLine 222, ^{HF} is necessary for y_CNP\n\nLine 224, ^{LF} is necessary for y_CNP\n\nIn the supplementary materials\n\nLine 738, \\mathcal{L}_k appears two times\n\nA.9 must be improved:\n\nAre \\theta and \\theta\u2019 vectors as in the manuscript?\n\nWhat kind of matrix is K? What are K_LL, K_\\delta?\n\nCould you give a reference or improve the theory, please?\n\nA.10 \\Theta must be in bold\n\nIs \\theta a scalar or a vector?\n\nCould you give a ref or a proof for the approximation of I(\\theta) for GP model? It seems simple, but I would like to be convinced this is right."
            },
            "questions": {
                "value": "Line 53, this is interesting, where does this order of magnitude 'N needs to be extremely large (O(10^4))'come from? Could you give a quick explanation or a reference, please?\n\nIn part 5.0.2. why are relationship dependencies clear for R and N and not for the thickness and the angle? What does this implies on the model's interpretation or performance?\n\nLine 171, the nuisance parameters are introduced quickly. Can you provide a brief explanation and give a reference, please?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 10
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses a key question in physics: the matter-antimatter asymmetry in the universe, focusing on neutrinoless double-beta decay (NLDBD). The authors tackle the challenge of optimizing detector designs to minimize background event contamination, framing it as a Rare Event Design (RED) problem.\nThey introduce the Rare Event Surrogate Model (RESuM), which combines a pre-trained Conditional Neural Process with a Multi-Fidelity Gaussian Process to optimize detector designs. Applied to neutron moderator designs for the LEGEND NLDBD experiment, RESuM achieves a 66.5\u00b13.5% reduction in neutron background while using only 3.3% of the computational resources of traditional methods. This innovative approach has broad implications for similar challenges in physical sciences."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors did a great job narrating each step of the procedure and model description. However, since I am not in the field, I got a little lost in the setup of the Experiment section, which involves applications. The results are convincing, though."
            },
            "weaknesses": {
                "value": "- It would be better if the authors provided more detailed explanations in the Experiment section.\n- Several mathematical symbols are not in math mode."
            },
            "questions": {
                "value": "Are there no other works attempting to solve the same problem that we can use to benchmark your work against?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents a generative approach for optimizing physics detector design in rare event scenarios by leveraging a Conditional Neural Process (CNP). By incorporating techniques like simple co-kriging, data augmentation, and multi-fidelity modeling, the authors demonstrate that their surrogate model effectively addresses the computational demands of rare event detection while maintaining the accuracy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The authors provide a solid background introduction to the NLDBD problem and offer clear, detailed explanations of the technical methods used in this work, including co-kriging and Conditional Neural Processes (CNP), in the appendix."
            },
            "weaknesses": {
                "value": "Some of the critical weaknesses: \n\n- While the authors provide a solid introduction to NLDBD and its motivation, the paper lacks a literature review on rare event simulation/modeling. Rare event simulation is a well-established field in engineering, including techniques such as the first/second-order reliability methods (FORM/SORM) [1, 2, 3], polynomial-based surrogate modeling [4, 5], adaptive and sequential importance sampling [6, 7], and ensemble Kalman filters [8]. These approaches have also been extended to multi-fidelity settings, as seen in [9, 10, 11]. Although it\u2019s not necessary to cite all related works, the authors should clarify why they focus on deep generative modeling, highlighting its advantages over traditional models.\n\n- The computational results of the proposed method appear to lack a baseline, making it challenging to assess performance without a fair comparison.\n\n- Some notations are not statistically rigorous. Specific comments on notations are provided in the questions section. Also, the variables $\\phi$ and $\\theta$ are inconsistently bolded, which can cause confusion. \n\nOverall, we find that this work falls below the acceptance threshold for ICLR. We encourage the authors to: (1) expand the literature review to include rare event simulation/probability estimation as outlined here, (2) refine the statistical notations, and (3) add baselines for experiments. With these adjustments, this work would be in a better shape.\n\n[1] A. M. Hasofer, An exact and invarient first order reliability format, J. Eng. Mech. Div., Proc. ASCE 100 (1974) 111\u2013121.\n\n[2] A. Der Kiureghian, et al., First-and second-order reliability methods, Engineering design reliability handbook 14 (2005).\n\n[3] B. Fiessler, H.-J. Neumann, R. Rackwitz, Quadratic limit states in structural reliability, Journal of the Engineering Mechanics Division 105 (1979) 661\u2013676.\n\n[4] J. Li, D. Xiu, Evaluation of failure probability via surrogate models, Journal of Computational Physics 229 (2010) 8966\u20138980.\n\n[5] J. Li, J. Li, D. Xiu, An efficient surrogate-based method for computing rare failure probability, Journal of Computational Physics 230 (2011) 8683\u20138697.\n\n[6] C. G. Bucher, Adaptive sampling\u2014an iterative fast monte carlo procedure, Structural safety 5 (1988) 119\u2013126.\n\n[7] I. Papaioannou, C. Papadimitriou, D. Straub, Sequential importance sampling for structural reliability analysis, Structural safety 62 (2016) 66\u201375.\n\n[8] F. Wagner, I. Papaioannou, E. Ullmann, The ensemble kalman filter for rare event estimation, SIAM/ASA Journal on Uncertainty Quantification 10 (2022) 317\u2013349.\n\n[9] B. Peherstorfer, T. Cui, Y. Marzouk, K. Willcox, Multifidelity importance sampling, Computer Methods in Applied Mechanics and Engineering 300 (2016) 490\u2013509.\n\n[10] B. Peherstorfer, B. Kramer, K. Willcox, Multifidelity preconditioning of the crossentropy method for rare event simulation and failure probability estimation, SIAM/ASA Journal on Uncertainty Quantification 6 (2018) 737\u2013761.\n\n[11] F. Wagner, J. Latz, I. Papaioannou, E. Ullmann, Multilevel sequential importance sampling for rare event estimation, SIAM Journal on Scientific Computing 42 (2020) A2062\u2013A2087."
            },
            "questions": {
                "value": "- This works uses CNP as the major tool for rare event modeling. Is it model originally proposed by the authors? If not, please provide the corresponding citations. Also, please give the motivation here. Why CNP is more advantageous than VAE/GAN in this setting? \n\n- In line 51, the authors said \"If $m_1/N_1 < m_2/N_2$, it suggests that the design $\\theta_1$ is better than $\\theta_2$.\" This statement holds only if $N_1, N_2$ are sufficiently large, right? Otherwise, since it is in a rare event scenario, a bad $\\theta$ may also lead to zero values when $N$ is not large enough. \n\n- It looks like $X_{ki}$ is equivalent with $t(\\theta_k, \\phi_{ki})$, why use two separate notations? Please correct me if I am wrong.\n\n- Formula (4): I don't understand what does Bernoulli($p=t(\\theta_k, \\phi_{ki})$) mean here. Do you mean Bernoulli($p=\\bar{t}(\\theta_k)$)? Also, the letter $p$ has been used as density function. \n\n- The variable $\\beta$ is applied in different versions without clear definitions. In line 160, the authors introduce $\\beta_{ki}$, while $\\beta$ appears in formula (5) without clearly showing its dependency. My guess is that $\\beta_{ki} = \\beta(\\theta_k,\\phi_{ki})=\\beta\\vert\\theta_k, \\phi_{ki}=\\beta$. Please clarify. \n\n- Line 182, typo: \"RESuM as shown in Figure 3.\" Here it is supposed to be Figure 1.\n\n- Line 230: you don't need to give the fourmulation of the acquisition function, since $\\boldsymbol{x}$ is not introduced here.\n\n- Line 299 and line 308: The density $g(\\phi)$ for LF and HF models are different, right? If so, please use different notations for them.\n\n- Line 517-520: \"Based on the statistical formulation and ... for accelerating simulations. \" I don't see the comparison between the proposed method with VAE/GANs, could you please clarify how you draw this conclusion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces RESuM (Rare Event Surrogate Model), designed to optimize physics detector design, specifically for reducing background events in neutrinoless double-beta decay (NLDBD) detection. The authors frame the challenge as a Rare Event Design (RED) problem, where background events are so rare that traditional simulation methods become computationally prohibitive. RESuM employs a multi-fidelity approach, using a Conditional Neural Process (CNP) model to learn from limited data and a Gaussian Process (GP) model to efficiently explore the design space. The authors apply RESuM to the LEGEND experiment\u2019s neutron moderator design and achieve an  efficient performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The model targets a critical challenge in physic.\n2. The integration of CNP and MFGP is a novel approach for handling rare event problems.\n3. The framework\u2019s formulation makes it generalizable to other simulation-heavy domains."
            },
            "weaknesses": {
                "value": "I do not find major concerns.\nMinor issues:\n1. The success of RESuM\u2019s multi-fidelity modeling approach relies on access to both high-fidelity and low-fidelity simulation data. In situations where these data sources are unavailable or highly dissimilar, would the model's effectiveness be reduced?\n2. It seems to me that the RESuM does not embed physics constraints or validation check to the model, thus it might be possible that RESuM would suggest some invalid designs. In practice, do we need to first define the valid range of parameters for RESuM? And thus for different application contexts, we need to modify the model accordingly?\n3. Section 5 uses subsubsections without subsections.\n4. Can RESuM scale to applications with larger or more complex detector designs? How to estimate the relationship between the dimension of design space and model performance? Also, can RESuM incorporate expert knowledge on the design space to make it more efficient?"
            },
            "questions": {
                "value": "See Weaknesses for questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}