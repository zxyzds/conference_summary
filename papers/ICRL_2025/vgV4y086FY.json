{
    "id": "vgV4y086FY",
    "title": "Differentially Private Bilevel Optimization",
    "abstract": "We present differentially private (DP) algorithms for bilevel optimization, a problem class that received significant attention lately in various machine learning applications.\nThese are the first DP algorithms for this task that are able to provide any desired privacy, while also avoiding Hessian computations which are prohibitive in large-scale settings.\nUnder the well-studied setting in which the upper-level is not necessarily convex and the lower-level problem is strongly-convex, our proposed gradient-based $(\\epsilon,\\delta)$-DP algorithm returns a point with hypergradient norm at most $\\widetilde{\\mathcal{O}}\\left((\\sqrt{d_\\mathrm{up}}/\\epsilon n)^{1/2}+(\\sqrt{d_\\mathrm{low}}/\\epsilon n)^{1/3}\\right)$ where $n$ is the dataset size, and $d_\\mathrm{up}/d_\\mathrm{low}$ are the upper/lower level dimensions.\nOur analysis covers constrained and unconstrained problems alike, accounts for mini-batch gradients, and applies to both empirical and population losses.",
    "keywords": [
        "Bilevel optimization",
        "differential privacy",
        "nonconvex optimization",
        "first-order methods"
    ],
    "primary_area": "optimization",
    "TLDR": "We present the first differentially private first-order algorithms for bilevel optimization.",
    "creation_date": "2024-09-14",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vgV4y086FY",
    "pdf_link": "https://openreview.net/pdf?id=vgV4y086FY",
    "comments": [
        {
            "summary": {
                "value": "This paper studies bilevel optimization under the central DP model. The authors leverage recent advancements in (non-private) first-order bilevel optimization and propose algorithms that cover both ERM and population loss.\nThe proposed algorithm avoids computing Hessian and uses only gradients, finding approximate solutions under certain conditions. Authors also show the mini-batch variant has similar convergence properties."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper studies bilevel optimization under central DP, establishing first results in the area.\n- It provides a mini-batch variant and addresses both ERM and population risks. \n- The paper has a well organized structure."
            },
            "weaknesses": {
                "value": "See questions below"
            },
            "questions": {
                "value": "Since the paper is built on recent advancements in (non-private) first-order bilevel optimization, what are the technical challenges when moving from non-private to private case?\nWhat are the technical difficulties of the analysis of the algorithm compared to the ones for non-private bilevel optimization?\n\nI hope to see some empircal results if possible."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents DP algorithms for bilevel optimization problems, where upper-level objectives are smoot and the lower-level problems are smooth and strongly convex. The proposed gradient-based DP algorithms can avoid Hessian computations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This framework can work with different inner algorithms with only dependency on its convergence rate and DP parameters."
            },
            "weaknesses": {
                "value": "While the methods outlined in the paper appear innovative, they lack a clear comparative analysis with existing methods. Fully first-order methods have already been established in non-DP settings; however, it's not apparent whether the DP version introduces significant additional complexities. \n\nThe paper lacks empirical evaluation, which is noted as future work. This omission is unconventional and limits the ability to gauge practical effectiveness. Exploring the interaction between outer and inner algorithms through experiments could yield insightful results regarding their actual performances.\n\nThe \"any desired privacy\" mentioned in the contributions does not have a clear meaning because:\nAdjusting a parameter to achieve a specific \\epsilon,\\delta value is almost always possible in all DP algorithms. The algorithm can meet any pair of \\epsilon,\\delta pair. However, through naive application of gaussian noise. While being correct, it does not exactly produce the desired privacy. Furthermore, meeting any privacy specification doesn't necessarily imply efficiency."
            },
            "questions": {
                "value": "The closest existing result is from Chen (2024), but due to the difference in the DP frameworks (central DP vs. local DP) it is hard to draw a direct comparation. Since both DP mechanisms are achieved by adding Gaussian noise, a question remains: When the scale of noise is identical, can the performance between these methods be effectively compared?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a method for differentially private bilevel optimization. In bilevel optimization, the constraint set itself is given as another optimization problem. This submission aims to produce a value with a small gradient norm (we do not assume the \"upper\" objective is convex).\n\nThis problem has received a lot of attention recently because of new approaches, based on penalizing/smoothing the objective, that only require first-order information. One recent paper considered bilevel optimization in the local model and assumed access to second-order information. This submission operates in the central model and only uses gradients.\n\nThe submission provides theoretical guarantees for the minimizing the norm of the empirical gradient and for the population term. It also analyzes a minibatch variant with roughly similar guarantees."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This submission provides a clear contribution. Private bilevel optimization is certainly worthy of study. The paper is written well."
            },
            "weaknesses": {
                "value": "The submission is not very deep: once the problem is stated and we've decided to following the non-private first-order penalty methods, the analysis strikes me as essentially a process of assembling the right tools and carefully applying them and tracking the error. (I don't mean to imply that this is trivial, just that the paper would appeal to a wider audience if it had new ideas for private optimization. Maybe it does, and I wasn't able to pick them up?)"
            },
            "questions": {
                "value": "We get guarantees for the gradient norm but the paper calls it \"ERM.\" Is this a standard terminology for the non-convex problem?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a novel algorithm to solve differential private bilevel optimization problems, assuming that (1) the upper-level objective is smooth and Lipschitz and (2) the lower-level function is strongly convex and locally Lipschitz around optima. \nCompared to existing approaches, the proposed method is fully first-order and doesn\u2019t need assumptions on privacy parameter $(\\varepsilon, \\delta)$"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "According to the author, the proposed method is the first fully first-order DP optimization method that solves the bilevel optimization problem. The proof seems correct to me and the paper is well-organized."
            },
            "weaknesses": {
                "value": "As the authors pointed out in the discussion section, the error rate for bilevel ERM, as well as the additive factor on the inverse batch size that appeared in minibatch bilevel ERM, could potentially be improved."
            },
            "questions": {
                "value": "The proof technique assumes that the lower-level objective $g(x, y)$ is strongly convex in $y$. Can this assumption be weakened so that only the convexity of $g(x,y)$ is required?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}