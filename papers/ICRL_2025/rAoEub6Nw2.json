{
    "id": "rAoEub6Nw2",
    "title": "A Statistical Framework for Ranking LLM-based Chatbots",
    "abstract": "Evaluating large language models (LLMs) effectively is essential for advancing their development and ensuring alignment with human preferences. Platforms like Chatbot Arena have made significant strides by gathering millions of votes through crowdsourced pairwise comparisons to rank LLMs, offering valuable data for assessing model performance. However, the statistical methods employed rely on simplistic approaches, such as the Elo rating system, which inadequately handles ties in competitions and overlooks the underlying relationships between competing models. In this paper, we introduce a more rigorous statistical framework that builds upon the data from Chatbot Arena while correcting these methodological shortcomings. We apply well-established statistical models to properly account for ties within an axiomatic framework. Additionally, we introduce a novel factor analysis that captures the complexity of ties across pairs of competitors, significantly improving the overall model performance. These improvements not only enhance the handling of ties but also increase the accuracy of win and loss predictions compared to previous methods. Additionally, we incorporate Thurstonian representations to model covariance structures between competitors, allowing for deeper insights beyond rankings. We also address previously unrecognized symmetries in the likelihood function that can hinder optimization and propose constraints to ensure stable parameter estimation. Finally, we provide a Python package, leaderbot, to facilitate reproducibility. Our experiments demonstrate significant improvements in accuracy for both ties and win-loss outcomes, offering a robust alternative to existing methods.",
    "keywords": [
        "Large Language Models (LLMs)",
        "Paired Comparison",
        "Statistical Ranking",
        "Human Preferences",
        "Chatbot Arena",
        "Logistic Regression"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We introduce a rigorous statistical framework for ranking large language models (LLMs) using crowdsourced comparisons, improving accuracy for ties, wins, and losses beyond current methods like Elo.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=rAoEub6Nw2",
    "pdf_link": "https://openreview.net/pdf?id=rAoEub6Nw2",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a statistical framework for ranking LLM-based chatbots. It addresses the limitations of existing methods like the Elo rating system by incorporating ties and covariance structures. The proposed apply well-established statistical models to properly account for ties within an axiomatic framework and introducing factor analysis. Experiments on the Chatbot Arena dataset show improved accuracy and insights into chatbot rankings and relationships."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The proposed method can account for ties in an axiomatic framework. This improves not only tie prediction but also enhances win-loss inference.\n2. The mathematical analysis is comprehensive and thorough.\n3. This work has an open source python package which is easy to use."
            },
            "weaknesses": {
                "value": "1. For model ranking, it has no \"ground truth\". Therefore, it is hard to convince others under this method, the ranking is more accurate.\n2. The evaluation is highly dependent on the Chatbot Arena dataset which makes this work a slight improvement on chatbot arena. Therefore, the impact of this work is limited.\n3. As for chatbot arena, a simple enough ranking rule is more important if the users are common users. This work will make the rule too complicated for them to understand and then reduce the impact of chatbot arena."
            },
            "questions": {
                "value": "1. Is this method only useful to chatbot arena or it is useful to all Elo ratings?\n2. We need another figure to see the ranking difference of original chatbot arena and this methods. Also the figure 1 showing the result of PCA analysis is confusing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose an improved statistical model for ranking large language models (LLMs) on the Chatbot Arena dataset to enhance the traditional Elo rating system. Paper points out that the Elo system has limitations in handling ties and capturing relationships between models. To address these issues, paper introduces the Rao & Kupper and Davidson models, as well as a novel factor model to better capture the complexity of ties between different models, thereby improving prediction performance. Finally, they provide a Python package, \u201cLeaderbot,\u201d to reproduce the statistical model and support further experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The authors identify symmetry issues in the likelihood function of traditional models, which could lead to instability in parameter estimation. To address this, they propose symmetry constraints that ensure stable parameter estimation, thereby enhancing the model\u2019s optimization performance and interpretability;\n2. They effectively address the issue of ties, which is a limitation of the existing Elo system, and make optimizations to handle this;\n3. They provide a Python package that allows for the reproduction of the paper's results and supports further research."
            },
            "weaknesses": {
                "value": "1. The authors use multiple models to rank models, showing that high-ranking models exhibit greater consistency than lower-ranking ones. However, they do not provide further analysis, such as examining the specific characteristics of models that initially show ranking inconsistencies;\n2. The authors\u2019 work focuses on optimizing the ranking model but lacks subsequent analysis. Additional insights, such as a more in-depth examination of correlations between LLMs or an analysis of the differences between Leaderbot rankings and Chatbot Arena Elo rankings in relation to model characteristics, would enhance this work."
            },
            "questions": {
                "value": "1. In the treatment of pairs and unique pairs, have the authors considered introducing semantic clustering to their model, as opposed to solely using statistical techniques? Additionally, could they clarify the reasoning behind not exploring semantic relationships within pairs?\n2. Have the authors considered methods other than PCA to further analyze the correlations between models in the results? The current approach does not sufficiently reveal the underlying correlations between LLMs in an intuitive way."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses a novel problem in the evaluation of large language model (LLM)-based chatbots by proposing an advanced statistical framework that builds on existing methods used in the Chatbot Arena setting. The framework integrates well-established models, such as Rao-Kupper and Davidson, to account for ties in a rigorous axiomatic framework, and introduces Thurstonian representations to model covariance structures between competitors. These additions allow for more nuanced insights into chatbot rankings and performance consistency. A Python package, leaderbot, is also provided for reproducibility and ease of experimentation."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. **Pioneering Approach**: This paper tackles a novel and important problem in LLM evaluation, presenting a unique perspective on using advanced statistical models to refine the ranking process in chatbot comparisons. Its approach to systematically modeling ties and latent structures sets a new precedent for evaluating LLM-based chatbots and offers fresh insights that go beyond traditional ranking methods.\n2. **Innovative Use of Thurstonian Representations**: By integrating Thurstonian models and introducing covariance structures, the paper offers a groundbreaking method for capturing relationships among LLMs. This enables a deeper analysis of model consistency and performance that extends beyond simple rankings, marking a notable advancement in chatbot evaluation techniques."
            },
            "weaknesses": {
                "value": "1. While the paper introduces an alternative framework, it does not clearly discuss why Arena\u2019s Elo-based approach is insufficient beyond the issue of ties. Additional insight into Arena\u2019s limitations in capturing competitive dynamics or certain statistical shortcomings would strengthen the argument for this new model.\n2. The authors mention consistency in high-ranking models and variability in lower-ranking models, but do not explore further distinctions within these groups. For example, identifying specific characteristics (e.g., model size) associated with high consistency could provide more actionable insights."
            },
            "questions": {
                "value": "1. The paper mentions that high-ranking models show greater consistency than lower-ranked ones. Could you expand on this observation? For example, are high-ranking models generally larger or more sophisticated in architecture? And at what ranking position does this shift in consistency typically occur?\n2. How is the effectiveness of handling ties quantified across different statistical models? Is there an analysis of which models are more favorable to certain LLMs, and if so, what might explain these differences?\n\nIf these questions can be addressed, the paper could potentially be rated one point higher."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}