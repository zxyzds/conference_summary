{
    "id": "WJaUkwci9o",
    "title": "Self-Improvement in Language Models: The Sharpening Mechanism",
    "abstract": "Recent work in language modeling has raised the possibility of \u201cself-improvement,\u201d where an LLM evaluates and refines its own generations to achieve higher performance without external feedback. It is impossible for this self-improvement to create information that is not already in the model, so why should we expect that this will lead to improved capabilities? We offer a new theoretical perspective on the capabilities of self-improvement through a lens we refer to as \u201csharpening.\u201d Motivated by the observation that language models are often better at verifying response quality than they are at generating correct responses, we formalize self-improvement as using the model itself as a verifier during post-training in order to \u2018sharpen\u2019 the model to one placing large mass on high-quality sequences, thereby amortizing the expensive inference-time computation of generating good sequences. We begin by introducing a new statistical framework for sharpening in which the learner has sample access to a pre-trained base policy. Then, we analyze two natural families of self improvement algorithms based on SFT and RLHF. We find that (i) the SFT-based approach is minimax optimal whenever the initial model has sufficient coverage, but (ii) the RLHF-based approach can improve over SFT-based self- improvement by leveraging online exploration, bypassing the need for coverage. We view these findings as a starting point toward a foundational understanding that can guide the design and evaluation of self-improvement algorithms.",
    "keywords": [
        "Learning theory",
        "Sample complexity",
        "Self-Improvement",
        "Language Models"
    ],
    "primary_area": "learning theory",
    "TLDR": "We offer a new theoretical perspective on the possibility of self-improvement in language models.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=WJaUkwci9o",
    "pdf_link": "https://openreview.net/pdf?id=WJaUkwci9o",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a theoretical framework for analyzing language model self-improvement through the lens of \"sharpening\"\u2014defined as the process of tilting a model's probability mass toward sequences with high self-reward (which crucially does not rely on any external information). The paper focuses mostly on the case where self-reward = log \u03c0(y|x), which has been used in previous self-improvement literature. It also focuses mostly on sharpening via self-training (as opposed to sharpening via only inference-time computation), which can be viewed as amortizing expensive inference-time computation/sharpening. With this in mind, the authors present two families of algorithms for sharpening via self-training: SFT-sharpening and RLHF-sharpening. SFT-sharpening uses Best-of-N with self-reward to generate a new SFT dataset to fine-tune the model, while RLHF-sharpening uses RLHF with self-reward as the reward function for preference scoring (the paper looks at DPO-style algorithms specifically). These families of algorithms come from previous self-improvement literature.\n\nTo analyze the sample complexity of these algorithm families, the authors introduce a \"sample-and-evaluate\" framework where sample complexity is measured in terms of m = n * N sample-and-evaluate queries (n sampled prompts with N generations sampled per prompt). They prove a lower bound on the sample complexity for any sharpening algorithm. Then, they prove that both SFT- and RLHF-sharpening can learn sharpened models under the maximum likelihood objective (ie, maximizing self-reward), and they prove the sample complexity of each (with certain assumptions about model coverage). Finally, they demonstrate that adding online exploration to RLHF-sharpening can replace the dependence of sample complexity on model coverage to a dependency on the complexity of exploration."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- The paper is well written and easy to follow, with a detailed appendix containing proofs and additional results like guarantees for purely inference-time sharpening.\n- The work is relevant to the scope of ICLR and introduces a novel theoretical perspective and statistical framework to analyze language model self-improvement algorithms. It provides a much needed theoretical understanding of recently popularized self-improvement methods for language models.\n- The SFT-sharpening and RLHF-sharpening algorithm families are widely used in self-improvement literature, making the theoretical results immediately relevant and valuable as a foundation for future work.\n- The result showing that using online exploration with RLHF-sharpening shifts dependency from model coverage to complexity of exploration is a particularly interesting and significant result. Future works can analyze how different online exploration strategies affect sample complexity.\n- The choice to focus on log \u03c0(y|x) as self-reward is simple yet well-motivated given its use in prior self-improvement literature. The framework established by the authors provides a foundation for future work to analyze the effect of using other self-reward functions on sharpening guarantees and sample complexity."
            },
            "weaknesses": {
                "value": "- On lines 187 and 199, the paper states that the corresponding algorithm scheme converges to a sharpened model / sharpening objective in the limit. However, it does not seem to provide a justification or proof for these two statements (please let me know if I have missed it).\n- Although focusing on log-probability as self-reward is justified, the paper would benefit from including a survey of other self-reward functions used in prior self-improvement works. This context would provide valuable guidance for others to build on the sharpening perspective by looking at alternative self-rewards.\n- While the paper does mention the prior literature on self-distillation, it would benefit from a more complete summary of this prior line of work, including key findings and making it very clear how this work is different/novel.\n- While emperical results are shown for inference-time sharpening, there are no experiments for self-training methods (SFT-sharpening, RLHF-sharpening) which are the main focus of the theoretical analysis. The paper would greatly benefit from including experimental results for SFT-sharpening and/or RLHF-sharpening, **with a focus on whether the theoretical results can be used to correctly predict something about the emperical results.** For example, can the theoretical results be used to predict the difference between running an RLHF-sharpening setup with and without online exploration? Or between SFT- and RLHF-sharpening? I would be curious to hear the authors' views on this.\n- The conclusion focuses on future work but lacks a summary of the paper's key contributions and findings. The final manuscript would benefit from a summary of key takeaways in the conclusion. (No need to include it now in the rebuttal.)"
            },
            "questions": {
                "value": "- What should experimentalists take away from this work? For example, researchers working on inference-time computation scaling, or those doing self-training. Can this work help them in some way?\n- Do the authors see this work as being helpful for investigations into scaling laws for self-improvement? If so, how?\n- In the RLHF-sharpening formulation, my understanding is that the KL-term ensures that the finetuned policy stays close to the base-policy, which is required since self-reward is defined using the base-policy (i.e., to keep generations in-distribution for the reward function). Is this correct? Furthermore, I am wondering whether the self-reward can be defined using the non-stationary policy that is being fine-tuned. What would be the effect of this change?\n- Can you provide a more detailed explanation on the meaning of the coverage coefficient and how it can be problem dependent?\n- [The questions mentioned in \"Weaknesses\"]"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper provides an answer to why self-improvement methods can improve model performance even if they do not increase the information the mode has access to.\nIt defines sharpening as the greedification of a model, i.e. outputting its most likely answer, which is typically hard to compute.\nSharpening can be applied at inference time (e.g. with Best of N) or at training time, and the paper identifies self-improvement as a training-time sharpening method which allows to replace or amortize inference-time sharpening.\nIt defines a framework for theoretically evaluating the sample complexity of sharpening methods and provides results for the sample complexity of SFT and RLHF sharpening/self-improvement (using the model's likelihood as a filter or a reward), giving sample complexity bounds for each method to achieve sharpening at training time."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "### Contribution\n- The identification of self-improvement methods as sharpening methods that trade training time for inference-time search is novel and important in the discussion of self-improvement methods.\n- The sample complexity results provided for SFT-Sharpening and RLHF-Sharpening are novel and relevant to the community. The results on adding exploration to RLHF methods are relevant to the online vs offline discussion of post-training.\n- The additional results in the Appendix, such as adaptive sampling for SFT-Sharpening to improve its sampling complexity, are appreciated and can drive the design of novel practical algorithms.\n\n### Soundness \n- The sharpening objective and definition and the sample-and-evaluate framework are well-motivated and provide results that are relevant to practical settings.\n- The assumptions made in the paper seem reasonable.\n\n### Presentation\n- The paper is very well written and easy to follow. The claims and their associated evidence/results are easy to identify.\n- The remarks discussing the assumptions in depth, such as Remark 4.1, are well appreciated."
            },
            "weaknesses": {
                "value": "### Contribution:\n- Line 256: It's appreciated and essential that the paragraph \"Empirical validation of maximum-likelihood sharpening\" verifies that sharpening provides downstream task improvement; however, in my view, this is a fairly known fact which could be presented in a more concise way, or acknowledged as a general fact.\n- Definition 3.2: the generation-verification operation seems a bit restricted, but in a self-improvement context, this seems enough to me.\n- The results seem limited to the specific algorithms chosen IN RLHF-sharpening mainly REBEL and XPO. \n\n### Presentation:\n- It can be misunderstood from the paper that self-improvement methods should exclusively be seen as sharpening methods, from the strong questions raised in the abstract and the introduction. I suggest rephrasing those as motivation to see self-improvement as sharpening.\n- Line 194 typo: the expectation's long definition should be over $\\pi$ instead of $\\pi_{base}$.\n- Proposition 3.1 introduces an important result, which is then not discussed. The transition to the next section is too abrupt."
            },
            "questions": {
                "value": "- Could the authors elaborate on their choice of REBEL and XPO as the algorithms studied?\n- To what extent are the results in the paper limited to these algorithms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper analyses self-improvement in language models (LLMs) through a mechanism called \"sharpening.\" The authors focus on self-improvement where log-probabilities (of sequences of tokens) are used as the reward signal. They analyze two algorithms: SFT-Sharpening, which is optimal under sufficient model coverage, and RLHF-Sharpening, which can surpass SFT by utilizing online exploration to overcome coverage limitations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This work develops a theoretical framework for studying and understanding self-improvement. \nThey compare two approaches, SFT-Sharpening and RLHF-Sharpening, and prove their convergence."
            },
            "weaknesses": {
                "value": "1. While the authors empirically show there is useful signal in Best-of-N Sharpening (by sampling at inference), they do not empirically verify that SFT or RLHF training with this signal works.\n2. The authors focus their analysis on the maximum likelihood self-reward. While this objective is simple, it has not been empirically explored a lot in self-improvement literature (as the authors themselves note).\n3. The sentence in the abstract \"*Motivated by the observation that language models are often better at verifying response quality than they are at generating correct responses*\" is misleading in the context of this paper. Here, the reward is not assigned by asking the model to verify or critique its own solutions but by having access the log-probabilities of generated solutions. In general (relevant for Introduction and Related Work sections), claims of self-improvement in LLMs should be handled with more nuance.  While there are papers that LLMs can self-improve without external feedback, there is also recent evidence of the contrary:\n    -  Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. 2024. *When can LLMs actually correct their own mistakes?\nA critical survey of self-correction of LLMs*\n    -  Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. 2024. *On the self-verification limitations of large language models on reasoning and planning tasks*\n    -  Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. *Large language models cannot self-correct reasoning yet.*\n    -  Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. *Gpt-4 doesn\u2019t know it\u2019s wrong: An analysis of iterative prompting for reasoning problems.*\n    -  Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023. *Can large language models really improve by self-critiquing their own plans?*"
            },
            "questions": {
                "value": "While theoretically sound, can your work provide any guidelines for the practical deployment of sharpening techniques?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a theoretical framework to understand self-improvement. The authors define the sharpening as the best sequence according to the model likelihood which is different than greedy decoding or temperature sampling (the theory implies greedy decoding only covers the best sequence under some constraints). They further define self-improvement via SFT and RLHF using BoN and DPO, respectively. The authors show sample complexity bounds for SFT and RLHF using a model and data dependent coverage and concentrability coefficients. For example, sample complexity for SFT is proportional to coverage coefficient and logarithmically scales with size of the policy class. Finally, by adding exploration, the dependence on the coverage coefficient is replaced by sequential exploration coefficient."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed theoretical framework captures a very significant class of post-training improvements, ranging from SFT to RLHF. In particular, it covers BoN and DPO that are two very popular inference-time and post-training methods. The paper is also easy to follow."
            },
            "weaknesses": {
                "value": "My main concern is the lack of implications of the theoretical results for practical use.\n\n1. While the theory covers many popular methods, such as BoN and DPO, there is not much connection to what this theory implies in practice.\n\nA. For example, what predictions does it make about BoN, can we choose \u201cN\u201d based on your theory?\n\nB. What is the minimal experimental setup to cover both SFT and RLHF?\n\nC. How should we interpret Figure-1 based on your theory? Such as, would convergence rate of BoN be explained by your method?\n\n2. While I understand that using the same context \u201cx\u201d for both policy and reward function is meaningful, in practice it is generally different. Can you discuss if an extension of your theory explains the sample complexity when using different contexts that are related by a function? In euclidean space, this could simply be defined as a bound on the distance between two context."
            },
            "questions": {
                "value": "Please see above for specific questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}