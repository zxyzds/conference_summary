{
    "id": "FjZcwQJX8D",
    "title": "Towards Scalable Topological Regularizers",
    "abstract": "Latent space matching, which consists of matching distributions of features in latent space, is a crucial component for tasks such as adversarial attacks and defenses, domain adaptation, and generative modelling.\n    Metrics for probability measures, such as Wasserstein and maximum mean discrepancy, are commonly used to quantify the differences between such distributions.\n    However, these are often costly to compute, or do not appropriately take the geometric and topological features of the distributions into consideration.\n    Persistent homology is a tool from topological data analysis which quantifies the multi-scale topological structure of point clouds, and has recently been used as a topological regularizer in learning tasks.\n    However, computation costs preclude larger scale computations, and discontinuities in the gradient lead to unstable training behavior such as in adversarial tasks. \n    We propose the use of principal persistence measures, based on computing the persistent homology of a large number of small subsamples, as a topological regularizer.\n    We provide a parallelized GPU implementation of this regularizer, and prove that gradients are continuous for smooth densities.\n    Furthermore, we demonstrate the efficacy of this regularizer on shape matching, image generation, and semi-supervised learning tasks, opening the door towards a scalable regularizer for topological features.",
    "keywords": [
        "topological data analysis",
        "persistent homology",
        "generative adversarial network",
        "latent space matching"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We use a subsampling to approximate persistent homology, which is a topological summary of point clouds, to obtain a parallelizable and smooth topological regularizer.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=FjZcwQJX8D",
    "pdf_link": "https://openreview.net/pdf?id=FjZcwQJX8D",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a regularizer to be used in Generative Adversarial Networks (GANs) in addition to the \u201cstandard\u201d distance between distributions (e.g. Wasserstein distance). The regularizer consists in computing the probability of persistent homology diagrams through Principal Persistent Measures (PPM) on both real and generated data and comparing them through MMD. The paper provides a few theorems proving the smoothness of the MMD applied to compare PPM under mild conditions and experimentally shows the benefits of this regularizer on both generative metrics and semi-supervised classification tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper pairs the proposed regularizer with a solid theoretical analysis of the smoothness of the resulting loss function, which justifies its use as an additional term to the (W)GAN loss.\n\nExperimental results show that the proposed regularizer provides a significant advantage in training GANs, especially for self-supervised tasks. \n\nAlthough the paper is theoretically dense, it can still be followed by non-experts in the PH fields. Nevertheless, it would be helpful to remark the purpose of each theorem, or sequence of theorems, before introducing them."
            },
            "weaknesses": {
                "value": "The main weakness of the paper is the introduction and motivation of the problem and the relation of the proposed method to the current literature. From the introduction (and also the title), it isn\u2019t really clear what problem the paper is going to tackle. While the introduction talks about general regularizers for latent space representations, most of the methodological development and the experimental section tackle specifically the problem of regularizing GANs. I would be clearer about this from the beginning of the paper or show applications beyond GANs (beyond the shape-matching toy example).\n \nI would also better frame the work in the context of GANs, and regularization strategies for GANs. A few methods are mentioned in the literature, but none is discussed in depth. For instance, PHom-GeM seems to share objectives and strategies similar to those of the proposed work. It would be ideal to compare also with some of these methods.\n\nAlso, it is not clear what theorems are novel, and which ones are just reported or adapted from results in the existing literature. Maybe they are all novel, but specifying the current state of the literature and which gaps need to be filled (from a theoretical point of view) would help to better understand the contribution."
            },
            "questions": {
                "value": "I would like the authors to clarify the points I have highlighted on the weaknesses, especially the contribution and the relation with other GAN regularizers.\n\nMoreover, the method proposes the use of MMD rather than WD to compare the PPMs. Is there any theoretical and/or practical reason? Would still be possible to use WD and experimentally compare the performance of WD and MMD?\n\nMinor comments:\n- The citation format is weird and interferes with the reading.\n- At row 66, what would be the classic PH pipeline?\n- In row 155, you provide 5 introductory references to PH, these are far too many, and this does not really help the reader.\n- Row 383: \u201cconsider provide\u201d\n- Table 1: since you computed averages over 10 runs, it would make sense to report also the standard deviation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to introduce a topological regularization term to the objective function used in the context of GAN. \n\nAs standard topological descriptors (known as persistence diagrams) are typically too expensive to compute, they propose to rely on the (recently introduced) notion of \"principal persistence measure\". Losely speaking, the principal persistence measure is a distribution in the space of persistence diagrams, but its elements have (at most) one point, and can thus be (roughly speaking) identified with distribution on the open half-plane $\\{b < d,\\ (b,d) \\in R^2\\}$ augmented with an additionnal virtual point $\\star$. \n\nThe work then introduce a notion of MMD distance between PPM. The MMD has several benefits: it is fairly easy to compute and induce smooth gradients. \n\nThe authors showcase their method in a series of experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Good introduction / preliminaries (section 1 and 2). \n- The work mixes results from different ideas (mix of MMD, persistence measures, etc.), and may be useful beyond the scope covered by this paper (improving GAN training). It showcases the use of PPM which are interesting objects in their own, introduce novel possibly useful metric between these topological descriptors, etc. \n- interesting experiments, mixing pedagogical Proof-of-Concept and more advanced experiments. \n- Nice animation in the supplementary material!"
            },
            "weaknesses": {
                "value": "1. While well written, the introduction and section 2 somewhat fail to motivate the need to account for the geometry/topology when comparing distributions in the latent space. This is just given as a fact (line 142-143), but basically, why should one care about such information? Is there some situations in which this clearly lacks in GAN models? I understand that this needs is empirically confirmed in the experiment sections, but is there a good _a priori_ reason? \n\n2. Somewhat in the same vein: Wasserstein-like distances between topological descriptors are discarded because \"gradients are not smooth\", but is it a real problem in practice? I would expect stochasting GD to not actually bother which such details. \n\n3. One may argue that GAN tend to be outdated in comparison to more modern generative models (DDPM, etc.). I nonetheless believe that this does not significantly impede the contributions of the work, whose interest is not limited to GAN only. \n\n### Minor remarks (not real weaknesses)\n\n- I believe that the weighting by a factor $\\ell$ in the \"induced kernel\" from $R^2$ to $\\Omega$ resemble the PWK kernel of Kusano et al., and more generally the kind of weighting needed in linear representation of persistence diagrams (see Divol&Lacombe, along with Obayashi et al., \"Persistence diagrams with linear machine learning models\", and related works)."
            },
            "questions": {
                "value": "See question 1. and 2. in the Weaknesses section. \n\nAdditionnally : \n- I did not have time to read the appendix in details so I may be wrong, but regarding the Remark 1: in theory, PDs may have infinite total mass (here in the work, everything simplify as one consider PDs with at most 1 point), in which case $n,m, N = +\\infty$. And even if one restrict to finite diagrams, without putting a uniform bound on their cardinalitty, one may still have $n,m \\to \\infty$ and (I guess) peculiar behavior of the metric. Does the remark implicitely consider a space of diagrams with uniformly bounded mass/number of points? From a quick glance, $M_{lin}$ only incorporate a standard integrability constraint but nothing about the cardinality of the diagrams. \n\n- The animations show that optimization let few \"leftovers\", can you comment on this? Is it due to a mix of (gaussian-kernel based) MMD which indeed tends to let leftovers behind + the subsampling in the PPM which needs to be \"lucky\" to put some gradient on this points? Would using the Energy-distance MMD $k(x,y) = - |x-y|$ improve on this particular aspect? (note: gradient may no longer be smooth using this MMD)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "One of the prohibitive factors of Topological Data Analysis are computational\nconstraints imposed on the calculation of Persistent Homology (PH). While\nhighly optimized CPU-GPU algorithms exist, the computation already become \nunfeasible for small point clouds in low dimensional settings. A remedy is\nproposed through the use of a GPU implementation of the Principal Persistence\nMeasure (PPM) (G\u00f3mez & M\u00e9moli 2024). This addresses both difficulties related\nto smoothing as well as scalability. Given the scalable backbone the authors\nthen provide a theoretical framework to turn the PPM into a regularizer that\ncan be used in generative tasks where the topology of the dataset is of\nimportance. They provide a strong theoretical foundation for their regularizer\nand show that the gradients are continuous.\n\nThe authors provide two experiments as empirical evidence of the efficacy of\ntheir method. First is an optimization task where a point cloud is\nbackpropagated to match a reference shape. Faster convergence to the final\nsolution is shown, thereby showing topological regularization to be helpful in\nconvergence. In a final suite of experiments the authors show that the\nregularization significantly increases the generative quality of the trained\nmodels and this is demonstrated in both unsupervised and semi-supervised\nlearning tasks. The computational advantages are shown to have a significant\nincrease in training time over the traditional PH calculation and not suffer from the\nexponential cost."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "In general it is great to see a push towards scalable topological methods and\nthis entails the core strengths of the paper, as it an impactful and relevant\nproblem to work on. Often in TDA, the focal point lies in theoretical\njustification of the methods with only limited emperical evidence, mostly on\nsmall datasets. Lack of scalability of PH being the main driver of this\nphenomenon and therefore this paper is of importance in building the bridge in\napplying TDA in large scale applications.\n\nThe theoretical justification is both thorough and sound and adopts a novel\nview on using PH in machine learning applications with the empirical evidence\nshowing good improvements when using the regularization term."
            },
            "weaknesses": {
                "value": "Where the theoretical contributions have good exposition, the implementation\ndetails are less thoroughly addressed. As one of the two cornerstones of the\ncontributions, it would be nice to see where the authors differ in their\nimplementation from previous work that allowed them to compute the PH on the\nGPU. The details seem to be lacking in the paper and together with the release\nof the code, would strengthen the paper considerably.\n\nFor the experimental section the optimization of point clouds is not\nparticularly convincing. In the figure the results could be improved by showing\nhow the final output has converged and in two dimensions, which would be\nnatural as other metrics are not provided. Especially since the method (I\nassume) optimizes only a subset of the whole point cloud at a time I would like\nthe doubt of only a subset of the point cloud converging to the reference point\ncloud to be taken away. Second, only the loss of the training set over the\niterations is shown. An independent set distance to quantify the results would\nstrengthen the results. The Wasserstein distance between the point clouds or\nthe Chamfer Distance would be great examples. Finally, an argument why the\nnon-zero convergence is correct would also be in place, since none of the loss\nfunctions converge to zero.\n\nThe experimental section also heavily relies on the Cramer loss, and although\nthe paper presenting it was rejected at ICLR 2018, although the paper is\nwell-cited. The reviewer is not an expert on training GAN's and evaluating\nthem, hence it would be nice to provide an argument as to why this is the main\nloss metric to use and also the main model to use and compare to.\n\nIf the reviewer would have to show to the reader that a certain regularization\nterm is effective, a good strategy would be to take a variety of datasets and a\nvariety of models (perhaps with certain properties) and show that the\nregularization consistently improves the scores across the board. As the paper\nis currently presented, the scope of the experiments is rather limited.\nMoreover, there is also comparable work on topological regularizers for\nclassification problems which would also provide a nice comparison partner. A\nlast potential point of improvement is the connection with other topological\nmethods and regularization terms. \nThe application of persistent homology is not new and a comparison with previous work, \nboth in terms of computational performance and increase in accuracy would also be \nvery nice. \n\nAlthough the reviewer commends the line of work the authors have decided to\npursue, a considerable amount of work would have to be done to create a logical\nand convincing experimental suite. Multiple reference architectures of GAN's\nwould have to be implemented with more standard metrics. Perhaps other tasks\nwould also benefit from this regularization term, such as classification and\nregression tasks. The non-standard set up for the point cloud optimization\nexperiment would also have to be convincingly revised. Once these elements are\naddressed, the work has great potential impact on the Topological Data Analysis\ncommunity and machine learning in general."
            },
            "questions": {
                "value": "- A question that would also be interesting to consider is how the method\n    scales with dimension, based on the paper it looks like it might scale\n    exponentially in the dimension. \n- What causes loss not to converge to zero in the point cloud optimization\n    task? \n\nRemark:\nDuring the review, some typo's were found. Please give the paper another read\nand correct them."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}