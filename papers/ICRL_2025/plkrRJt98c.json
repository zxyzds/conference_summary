{
    "id": "plkrRJt98c",
    "title": "Explaining Vision-Language Similarities in Dual Encoders with Feature-Pair Attributions",
    "abstract": "Dual encoder architectures like CLIP models map two types of inputs into a shared embedding space and learn similarities between them.\n  However, it is not understood how such models compare the two inputs.\n  We first derive a method to attribute predictions of any differentiable dual encoder onto feature-pair interactions between its inputs. \n  Second, we apply our method to CLIP models and show that they learn fine-grained correspondences between parts of captions and regions in images. They match objects across input modes and also account for mismatches. However, this visual-linguistic grounding ability heavily varies between object classes, depends on the training data distribution, and largely improves upon in-domain training.\n  Using our method we can identify individual failure cases and knowledge gaps about specific object classes.",
    "keywords": [
        "Explainability",
        "Attribution",
        "Dual Encoder",
        "Similarity",
        "Vision-Language",
        "CLIP"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We derive a method to attribute similarity predictions of dual encoders onto feature-pair interactions of inputs and can show detailed correspondence between caption parts and image regions in CLIP models.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=plkrRJt98c",
    "pdf_link": "https://openreview.net/pdf?id=plkrRJt98c",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates the mechanisms by which dual encoder architectures, such as CLIP models, compare two types of inputs (e.g., text and images) by mapping them into a shared embedding space and learning their similarities. To address the lack of understanding of how these models compare inputs, the authors present two main contributions:\n\nDerivation of Feature-Pair Attribution Method: The authors derive a method to attribute the predictions of any differentiable dual encoder onto the feature-pair interactions between its inputs. This method can explain interactions between inputs of any differentiable dual encoder model without requiring modifications to the trained model.\nApplication to CLIP Models: The authors apply their attribution method to CLIP-type models and demonstrate that these models can learn fine-grained correspondences between parts of captions and regions in images. They show that the models can match objects across input modes and account for mismatches. The visual-linguistic grounding ability of these models varies significantly between object classes, depends on the training data distribution, and improves with in-domain training.\n\nThe paper includes various experiments to validate the proposed method, such as:\nEvaluating object bounding-box attributions to systematically assess the visual-linguistic grounding abilities of dual encoders.\nAnalyzing attributions to other objects to identify negative attributions for mismatched objects.\nCreating and evaluating hard negative captions to test the model's response to errors in captions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-written and clearly structured.\n\n2. The mathematical derivations and equations are clearly presented (But I did't check carefully)\n\n3. The proposed feature-pair attribution method has the potential to significantly advance the understanding of how dual encoder models, particularly vision-language models, compare inputs and make predictions. This understanding is crucial for improving model interpretability and trustworthiness."
            },
            "weaknesses": {
                "value": "1. The authors acknowledge that their feature-pair attribution method is an approximation and may not fully capture the exact contributions of feature interactions. This approximation introduces potential inaccuracies in the interpretation of attributions.\n\n2. The experiments and evaluations are primarily focused on CLIP models and specific datasets (e.g., COCO, Flickr30k, HNC). Conducting experiments on a broader range of dual-encoder models and datasets from different domains (e.g., medical imaging, audio-visual data) could demonstrate the generalizability and versatility of the method.\n\n3. While the paper introduces a promising analysis method for attributing predictions of dual encoder models to feature-pair interactions, it lacks a straightforward way to quantify the performance of existing models using this method.\nThe absence of a standardized metric, similar to FID in image generation or BLEU [1] in machine translation, limits the broader applicability and impact of the method. Having such a metric could significantly enhance the influence and usability of the proposed method.\n\n4. The paper primarily focuses on analyzing and explaining existing phenomena within CLIP models rather than providing solutions or improvements to the training process. While this analysis is valuable, it leaves several important aspects of CLIP training unexplained.\nFor example, the paper does not address why CLIP models require an extremely large batch size during training, which is a critical aspect of their performance and efficiency.\n\n[2] [Bleu: a Method for Automatic Evaluation of Machine Translation](https://aclanthology.org/P02-1040) (Papineni et al., ACL 2002)"
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a method to interpret CLIP models' cross-modal alignment mechanisms by attributing similarity predictions to specific feature-pair interactions across input modalities (image-text, text-text, image-image). The proposed method leverages integrated gradients to match parts of captions with corresponding regions in images. The authors also show that the model\u2019s grounding abilities vary by object class and are sensitive to the training data distribution, with in-domain fine-tuning improving performance. The paper includes evaluations on datasets with object bounding-box annotations (3.5k image-caption pairs from COCO, 8k pairs from Flickr30k, and 500 pairs from HNC)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper is easy to read, the proposed algorithm/methodology is sound, and the technical details are well explained (including a general description for code implementation in Section 7).\n\n- This paper is an extension of previous work for language-only Siamese models in NLP (M\u00f6ller et al., 2023; 2024) -- it provides a principled implementation for assessing the visual-linguistic grounding abilities of CLIP models, showing that the proposed approach in [1, 2] generalizes to vision and language models (particularly, image-text and image-image matching).\n\n- The proposed generalization of integrated gradients for object attribution is simple yet effective, and seems to work well for CLIP models.\n\n\n[1] Lucas M\u00f6ller, Dmitry Nikolaev, and Sebastian Pad\u00f3. An attribution method for siamese encoders. In Proceedings of EMNLP, Singapore, 2023.\n\n[2] Lucas M\u00f6ller, Dmitry Nikolaev, and Sebastian Pad\u00f3. Approximate attributions for off-the-shelf\nSiamese transformers. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th\nConference of the European Chapter of the Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 2059\u20132071, St. Julian\u2019s, Malta, March 2024. Association for Computational\nLinguistics."
            },
            "weaknesses": {
                "value": "*\"One may expect CLIP models must learn object correspondence between\nvision and language modes. But to our knowledge, our evaluation is the first piece of evidence\nindicating that this is actually the case.\"* -- this is a strong claim that does not hold. A large number of prior works has systematically explored gradient-based methods that uncover the grounding capabilities of CLIP (and other visual-language models) [3, 4, 5, 6]. \n\n- Limited evaluation: gradient-based explanations are typically evaluated in pointing game accuracy, segmentation masks, object detection and qualitative comparison against prior work. This work only compares the grounding ability in bounding-box evaluation. It is unclear how the proposed approach fares against prior relevant work.\n\n- Due to limited benchmarks and testbeds, it is difficult to determine if the proposed method is competitive against prior work, and to what extent it provides significant new contributions.\n\n[3] Chenyang ZHAO, Kun Wang, Xingyu Zeng, Rui Zhao, & Antoni B. Chan (2024). Gradient-based Visual Explanation for Transformer-based CLIP. In Forty-first International Conference on Machine Learning.\n\n[4] Yossi Gandelsman, Alexei A Efros, & Jacob Steinhardt (2024). Interpreting CLIP's Image Representation via Text-Based Decomposition. In The Twelfth International Conference on Learning Representations.\n\n[5] We\u0133ie Tu, Weijian Deng, & Tom Gedeon (2023). A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP). In Thirty-seventh Conference on Neural Information Processing Systems.\n\n[6] Bousselham, Walid, et al. \"LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity.\" arXiv preprint arXiv:2404.03214 (2024)."
            },
            "questions": {
                "value": "- As an extension of previous work, the contribution seems incremental due to the limited experimental analysis. Is it possible to show experimental results with other VLM architectures?\n\n- Results in Figure 5 are difficult to parse. Is it possible to separate the results into different tables per model (to show in the Appendix)?\n\n_____\n\nNot questions to answer, but suggestions that don't need to be addressed: \n- What are the results after finetuning with out-of-domain data? -- this might give a better picture and analysis of the models' performance and explainable approach\n\n- In Related Work (Sec. 2), there is a rather extensive and detailed explanation of vision-language models and their training objectives, however,  this work only focuses on CLIP models -- it might be interesting to correlate how different learning objectives/models correlate with the proposed post-hoc evaluation, and how they impact the grounding/explanation capabilities exposed by prior work and the proposed approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper is mainly focused on how dual encoder architectures map two types of inputs into one shared embedding space. To this end, authors propose to attribute predictions of any differentiable dual encoder onto feature-pair interactions between inputs. The proposed method shows that CLIP models learn fine-grained correspondences between parts of captions and regions in images. In-domain finetuning can largely improve the visual-linguistic grounding ability."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well written and easy to follow.\n2. The authors provide extensive illustrations to demonstrate the visual-linguistic alignment results."
            },
            "weaknesses": {
                "value": "1. While authors demonstrate the improvements by applying the proposed method to CLIP modes, the results seem to be straightforward: (1) visual-linguistic grounding ability heavily varies between object classes and the training data distribution; (2) in-domain finetuning can largely improve such ability.\n\n2. Regarding the results in Table 1, there is one missing baseline which is directly finetuning CLIP models without the proposed method. Such ablated experiment can further show the effectiveness of the proposed method.\n\n3. Why are there some missing results for the HNC dataset in Table 1?"
            },
            "questions": {
                "value": "See the above weakness. \n\n1. The main concern is that there is one missing baseline in Table 1. This baseline should finetune CLIP models without the proposed method to ablate the proposed method.\n\n2. It seems straightforward to get the conclusions that in-domain finetuning can improve the visual-linguistic ability. It is a little bit unclear about the main contributions of this paper.\n\n3. Have you ever compared the T2I and I2T retrieval results w/ and w/o finetuning? Does better visual grounding lead to better retrieval ability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper extends the Integrated Gradients algorithm to explain similarity in CLIP models which consist of two vision-language encoders, via attributing feature pairs (language-vision attribution). This allows capturing of feature interactions and correspondances, rather than having 2 separate attribution entities that are disconnected. Experiments are conducted on a variety of CLIP models from OpenAI and OpenCLIP."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors tackle an important topic of interpretation of feature interactions, which is not that well-explored in the literature\n- Wide range of CLIP models considered in experiments\n- Nice extension of the work \"An Attribution Method for Siamese Encoders to CLIP\" to the vision-language setting."
            },
            "weaknesses": {
                "value": "- [W1] There is an abundance of related work that the authors miss. [R1, R2, R3] are papers that are doing exactly the same thing. R1 proposes several baseline methods and evaluation metrics. R3 is also applied to CLIP and shows object-word interactions. The authors did not mention these works, their drawbacks and how their method is better, and did not compare with them. The authors are proposing a method without comparing it to existing methods in the literature. Comparison with baselines (and mentioning how the proposed method is different and what advantages it offers) is an integral part of any research work, which is missing. \n\n- [W2] Related to W1, the authors do not mention neither prove how methods which provide individual feature attributions (e.g., Grad-CAM, Integrated Gradients, LIME...etc) are bad and do not work, and that a new formulation of feature interactions is needed. Simply stating this statement without proving it, is not valid. \n\n- [W3] Poor evaluation: all I see is two graphs in Figure 5 (table 1 is an ablation experiments, not main results). There is no wide enough evaluation metrics and tasks. The only evaluation in Figure 5 is based on bounding box overlaps. However, it is well-agreed upon in XAI that bounding boxes are a very bad way of evaluating explanations, because they assume the model reasoning process aligns with the annotations.  If a model is biased and detects a \"hand\" rather than a \"dumbell\", it will be penalized (because the annotated box is for the dumbell), while it should not. Similarly, assume the model can identify a dog solely by its \"tail\". In this scenario, the model focuses only on physical features of the tail, and disregards other features like the dog's body, fur, or face. A good explanation in this case would highlight the tail. But, the overlap between the bounding box of that explanation (the dog's tail) and the overall bounding box of the dog would be small and penalised (while it should not). These attribution methods act as a way to perform applications such as zero-shot object detection or segmentation (e.g., [R4]), but not as ways to evaluate interpretations. \n\n- [W4] How did the authors arrive to Eq.2? How is is formulated? Based on what? It suddenly appears in this form. Did the authors create this starting point? If so, based on what?\n\nMinor (do not affect my decision):\n- The authors mention: \"One may expect CLIP models must learn object correspondence between vision and language modes. But to our knowledge, our evaluation is the first piece of evidence indicating that this is actually the case\". This is not surprising at all. None of the various applications of CLIP would actually work if this was not the case. \n- Not sure what is (top: selections in yellow, bottom: saliencies as above) in Figure 2. Are yellow and red supposed to be corresponding? Or is red the negative? \n\nReferences\\\n[R1] Visualizing and Understanding Contrastive Learning, TIP 2023\\\n[R2] Visualizing deep similarity networks, WACV 2019\\\n[R3] Model-Agnostic Visual Explanations via Approximate Bilinear Models, ICIP 2023\\\n[R4] Interpreting CLIP's Image Representation via Text-Based Decomposition, ICLR 2024\\"
            },
            "questions": {
                "value": "In general, I feel this paper is not ready for ICLR. The weaknesses greatly outweighs the strength, Therefore, my decision will be, sadly enough, to reject this paper. W1, W2, W3 and W4 are major issues which should be addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}