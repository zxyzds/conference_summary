{
    "id": "mkDam1xIzW",
    "title": "Probabilistic Geometric Principal Component Analysis",
    "abstract": "Dimensionality reduction is critical across various domains of science including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a prominent dimensionality reduction method that provides a probabilistic approach unlike the deterministic approach of PCA and serves as a connection between PCA and Factor Analysis (FA). Despite their power, PPCA and its extensions are mainly based on linear models and can only describe the data in a Euclidean coordinate system around the mean of data. However, in many applications, data may be distributed around a nonlinear manifold rather than lying in the Euclidean space around the mean. We develop Probabilistic Geometric Principal Component Analysis (PGPCA) as a new dimensionality reduction algorithm that can explicitly incorporate knowledge about a given nonlinear manifold that is first fitted from data. Further, we show how in addition to the Euclidean coordinate system, a geometric coordinate system can be derived for the manifold to capture the deviations of data from the manifold and noise. We also derive a data-driven EM algorithm for learning the PGPCA model parameters. As such, PGPCA generalizes PPCA to better describe data distributions by incorporating a nonlinear manifold geometry. In simulations and brain data analyses, we show that PGPCA can effectively model the data distribution around various given manifolds and outperforms PPCA for such data. Moreover, PGPCA provides the capability to test whether the new geometric coordinate system better describes the data than the Euclidean one. Finally, PGPCA can perform dimensionality reduction and learn the data distribution both around and on the manifold. These capabilities make PGPCA valuable for enhancing the efficacy of dimensionality reduction for analysis of high-dimensional data such as neural activity that exhibit noise and are distributed around a nonlinear manifold.",
    "keywords": [
        "geometry",
        "nonlinear manifold",
        "factor analysis",
        "dimensionality reduction",
        "neural population activity"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "PGPCA generalizes PPCA by incorporating any nonlinear manifold with various distribution coordinates in its probabilistic model for high-dimensional brain data that exhibit noise.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mkDam1xIzW",
    "pdf_link": "https://openreview.net/pdf?id=mkDam1xIzW",
    "comments": [
        {
            "summary": {
                "value": "In this work, the author proposes a Probabilistic Geometric Principal Component Analysis (PGPCA) method which can be seen as an extension of PPCA with a better description of data distributions by incorporating a nonlinear manifold geometry. A data-driven EM algorithm is also proposed to solve the PGPCA problem. Experimental results verify the performance of the PGPCA is better than that of PPCA."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The method and algorithms are technically sound, with reasonable insights and solutions."
            },
            "weaknesses": {
                "value": "Comparison with other nonlinear PPCA methods is not provided."
            },
            "questions": {
                "value": "1. As some works focus on nonlinear PPCA, such as \n\nLawrence, Neil, and Aapo Hyv\u00e4rinen. \"Probabilistic non-linear principal component analysis with Gaussian process latent variable models.\" Journal of Machine Learning Research 6.11 (2005).\n\nZhang, Jingxin, et al. \"An improved mixture of probabilistic PCA for nonlinear data-driven process monitoring.\" IEEE transactions on cybernetics 49.1 (2017): 198-210.\n\nhow is the performance of the proposed PGPCA compared with these nonlinear PPCA methods?\n\n\n2. How does PGPCA compare with other methods in terms of computational efficiency?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper's main idea is the PPCA extension containing nonlinear manifold terms. The reason for this assumption is not clear. \nBut the formulation and mathematical analysis are true.\nAlso, its performance should be seen in real data. However the experiments on real data are not clear and also they should apply it for more general data like well-known image data sets, or at least mention the class of real data that this method can work well for them in comparison with PPCA. By this version of experiments the importance and quality of the proposed method are not clear.\nAlso, they should report the complexity of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The writing is good and also the mathematics are true."
            },
            "weaknesses": {
                "value": "The paper's main idea is the PPCA extension containing nonlinear manifold terms. The reason for this assumption is not clear. \nBut the formulation and mathematical analysis are true.\nAlso, its performance should be seen in real data. However the experiments on real data are not clear and also they should apply it for more general data like well-known image data sets, or at least mention the class of real data that this method can work well for them in comparison with PPCA. By this version of experiments the importance and quality of the proposed method are not clear.\nAlso, they should report the complexity of the proposed method."
            },
            "questions": {
                "value": "_ experiments for real data should be done?\nwhat is the specific class of data that this idea works good?\nwhy they did not report the complexity?\nif they report one can compare them with methods like deep factor models"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "its ok"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a generalization of probabilistic principal component analysis (PPCA) to data that lies on a (nonlinear) manifold rather than in Euclidean space around the mean. \n\nThe key contributions of the paper are (i) the introduction of the PGPCA model, (ii) an expectation-maximization algorithm to fit the proposed model and (iii) empirical experiments with real and simulated data to compare PGPCA with vanilla PPCA."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The problem of dimensionality reduction for data on a manifold is well motivated, and the proposed method closes the gap between PPCA and data on manifolds. \n- The proposed extension PPCA is innovative and well explained.\n- The mathematical derivation of the EM-algorithm is concise and understandable, and the empirical experiments show promising results."
            },
            "weaknesses": {
                "value": "- The simulated data seems to be favorable for PGPCA compared to PPCA as it lies on non-linear manifolds. It remains open how well PGPCA compares to PPCA for data in other settings, e.g. in linear subspaces.\n- PGPCA is only compared to PPCA and not other - potentially stronger - baseline methods. Especially, comparisons with other dimension reduction methods for data on manifolds is missing.\n- The evaluation on real data is rather limited. Only brain signals of two mice were used."
            },
            "questions": {
                "value": "- Please add literature on dimensionality reduction for data on a manifold.\n- How well does PGPCA work with other choices of $K(z)$ than geometric and Euclidean?\n- What theoretical guarantees for the hypothesis test on EuCOV vs. GeCOV are there? (e.g. in terms of statistical power/or asymptotic level)\n- How does PGPCA compare to PPCA for data in linear subspaces?\n- How does PGPCA compare against stronger competitors?\n- How well does PGPCA work on other real data?\n- Can the i.i.d. assumptions in model (1) be relaxed? E.g. by allowing (temporal) dependence or slightly varying distributions?\n- Can the assumption of normality be relaxed to allow for non-normal errors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}