{
    "id": "9I6UOIfbwf",
    "title": "Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging",
    "abstract": "Video face re-aging deals with altering the apparent age of a person to the target age in videos. This problem is challenging due to the lack of paired video datasets maintaining temporal consistency in identity and age. Most re-aging methods process each image individually without considering the temporal consistency of videos. While some existing works address the issue of temporal coherence through video facial attribute manipulation in latent space, they often fail to deliver satisfactory performance in age transformation. To tackle the issues, we propose (1) a novel synthetic video dataset that features subjects across a diverse range of age groups; (2) a baseline architecture designed to validate the effectiveness of our proposed dataset, and (3) the development of novel metrics tailored explicitly for evaluating the temporal consistency of video re-aging techniques. Our comprehensive experiments on public datasets, including VFHQ and CelebV-HQ, show that our method outperforms existing approaches in age transformation accuracy and temporal consistency. Notably, in user studies, our method was preferred for temporal consistency by 48.1\\% of participants for the older direction and by 39.3\\% for the younger direction.",
    "keywords": [
        "Face Editing",
        "Face Re-Aging",
        "Video Editing"
    ],
    "primary_area": "generative models",
    "TLDR": "We propose a video re-aging framework that includes a synthetic video re-aging dataset, a temporal-based architecture, and suitable metrics for video re-aging.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=9I6UOIfbwf",
    "pdf_link": "https://openreview.net/pdf?id=9I6UOIfbwf",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a novel approach to video face re-aging, focusing on altering the apparent age of individuals in videos while maintaining temporal consistency. Key contributions include the creation of a synthetic video dataset, a baseline architecture leveraging recurrent blocks for temporal coherence, and the introduction of new metrics for evaluating age transformation quality."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tInnovative Data Generation Pipeline: The authors designed a comprehensive pipeline for generating a synthetic dataset specifically for model training in video face re-aging. This pipeline addresses the challenge of obtaining paired video data with consistent identities and varying ages, thereby enhancing the quality and applicability of the training data.\n\n2.\tIntroduction of New Evaluation Metrics: The development of two novel metrics, Time Region Wrinkle Consistency (TRWC) and Time-Age Preservation (T-Age), provides a more effective means of assessing the quality of age transformations in videos. These metrics focus on maintaining temporal coherence, offering a more nuanced evaluation compared to traditional methods, and contributing to the advancement of the field."
            },
            "weaknesses": {
                "value": "1.\tOne significant shortcoming of the paper lies in its experimental section, which lacks thoroughness and depth. Specifically, the evaluation of the proposed new metrics includes only three baselines, and the quantitative comparisons in the User Study Results are limited to just two baselines. While the paper presents qualitative comparisons with various methods, these are not sufficiently persuasive without robust quantitative backing. Furthermore, the authors do not demonstrate the performance of their architecture using their own dataset to evaluate past methods, which undermines claims of superiority for their network design. This lack of comprehensive evaluation limits the credibility of the results and the overall impact of the proposed approach.\n2.\tAnother weakness is the poor discussion of the related works. The authors merely list all current works without providing a comprehensive discussion on them. It is unclear how many methods currently exist in the video-based face re-aging area and why those methods perform poorly.\n3.\tThe motivational section of the paper requires enhancement to better articulate the significance of the proposed method. Currently, the paper does not provide a sufficiently detailed explanation of the practical benefits and implications of the technique. The authors should elaborate on the unique challenges in video face re-aging and how their method specifically addresses these issues, thereby clarifying the motivation behind the research and its importance in the field.\n4.\tThe paper does not explicitly address the rationale for comparing the proposed method with image-based methods. It would be beneficial for the authors to clearly state the reasons behind this comparison. The review suggests that the paper lacks an explanation of why the video approach is being contrasted with image-based techniques, and what specific advantages or insights are to be gained from this comparison. Providing this information would strengthen the paper\u2019s argument and help the reader understand the significance of the methodological choice."
            },
            "questions": {
                "value": "1.\tDid you conduct quantitative comparisons with methods such as Diffusion VAE, and did you train these methods using your own dataset to evaluate their performance? If so, could you provide the relevant experimental results and analysis?\n2.\tCan you add new experiments to demonstrate the effectiveness of your newly constructed dataset? For example, by using a currently common technique to conduct experiments on both the existing dataset and the dataset you provided, and using the corresponding metrics to show that your newly constructed dataset can achieve better training results.\n3.\tCan you clearly articulated the specific benefits of training on videos for the face age reset method, which is essential for understanding the motivation behind choosing video training over static image training? The paper seems to imply the importance of temporal consistency, but it does not explicitly state the advantages of this approach in the context of videos. Could you please elaborate on these benefits to strengthen the motivational aspect of the research and to clarify why this method is innovative and significant compared to traditional static image training methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors address the Temporal consistency issue in Video Face-Aging Approaches.\nTo tackle this issue, the authors introduce:\n(1) A video data generation pipeline to obtain a synthetic video dataset;\n(2) A video face aging framework with recurrent U-Net structure; and\n(3) Temporal Regional Wrinkle Consistency (TRWC) and Temporally Age Preservation metrics to validate the temporal consistency factor as well as age transformation over time.\n\nExperiments are employed on CelebV-HQ and VFHQ datasets to show the advantages of the proposed approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper addresses temporal consistency factor of video face aging. This is a challenging factor in this topic.\n- The paper has introduced both data generation; architecture and metrics for video face aging."
            },
            "weaknesses": {
                "value": "The novelty of the paper is limited as most sections are \"inspired\" or \"motivated\" from previous approaches. \nParticularly:\n- For data generation process, it relied on StyleGAN and SAM to generate aging results for single frames. Then OSFV technique is adopted to generate faces at different poses and expressions for key frames and motion generation for temporal smoothing.\n- For video aging architecture, it is not novel as it is just a recurrent U-Net with commonly used losses.\nThe structure of the paper is more on putting multiple (previous) approaches together in an engineering manner rather than emphasizing on the novelty.\n\n1. In line 475, \"SAM fails to preserve attributes such as identity, pose and expression\". However, the data generation relied on images generated by SAM (Eqn. (1)) and learn from that data. Moreover, this error will be further accumulated with OSFV method when poses and expressions presented. \n- What are technical details or modifications in the proposed approach that helps to mitigate the limitation of SAM in the data generation process? \n- Moreover, the authors should provide more discussions/ analysis on how the proposed approach can improve on attribute preservation. Quantitative comparison demonstrating these improvements over SAM is recommended.\n\n2. There is no explicit constraint for consistency between frames during learning process. How can the trained network achieve the consistency when generating age-progressed frames?\nParticularly:\n- How can temporal consistency be enforced in the proposed approach? The authors should discuss about the details on architecture/loss functions that maintain this factor during learning/inference stages?\n- Can we adopt TRWC metric as loss function for this ? \n\n3. In Eqn. (8), why do we need to validate on the generate image rather than Delta image? In other words, can Delta images be used directly to validate the similarity rather than compute that similarity on \\hat{I} and normalize with real image.\nThe authors should analyze on the choice of using generated images instead of delta images and its effect on the metric values. An ablation study to compare the similarity/difference between these choices is recommended."
            },
            "questions": {
                "value": "Please address the concerns in Weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a simple GAN-based approach to generate a video of a subject at the target age. To maintain temporal consistency, the generator employs a recurrent architecture with U-Net blocks. This structure leverages both previous hidden states and generated frames, ensuring smooth transitions between ages. The model is trained using a combination of image and video discriminators, enhancing realism and temporal coherence.  Furthermore, the authors develop a pipeline for generating synthetic aging datasets and propose two new metrics for evaluating the temporal consistency of video re-aging methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Establishes a Strong Baseline: It introduces a new baseline for video re-aging, with novel contributions to architecture, dataset creation, and evaluation metrics. This provides a valuable foundation for future research in this area.\n2. Demonstrates the Effectiveness of Synthetic Data: The proposed approach, while architecturally simple, effectively leverages synthetic video datasets to achieve compelling results. This highlights the potential of synthetic data for training re-aging models.\n3. Provides Comprehensive Evaluation: Through extensive experiments, the authors convincingly demonstrate the realism and temporal coherence of their framework, using both qualitative and quantitative analysis."
            },
            "weaknesses": {
                "value": "1. Lack of Detail Regarding the Synthetic Dataset: The authors provide insufficient information about their synthetic dataset. To enable a comprehensive evaluation, the authors should provide detailed information about the dataset's size, diversity (including the range of ages, facial features, and other relevant attributes), and visual samples. This would allow reviewers to assess the dataset's quality and its potential impact on the reported results.\n2. Missing Information on Motion Generation: Section 3.1.3 on motion generation lacks clarity regarding the stopping condition for generating intermediate frames. A more precise explanation of this process is necessary for readers to fully understand the method.\n3. Unclear Availability of Resources: The authors do not explicitly state their intentions regarding the availability of the proposed dataset, pipeline code, or trained models. To enhance reproducibility and facilitate further research, it is strongly recommended that the authors publicly release these resources. Providing a link to a project page or repository, even if it's currently empty, would provide a clear indication of their commitment to open science.\n4. Limited Scope of Age Progression: The generated videos primarily exhibit age-related changes in the facial area, neglecting other important regions like hair and neck skin. This inconsistency detracts from the overall realism, as subjects appear to have mismatched facial and other features.\nSee more detailed questions about the above weaknesses in the next section."
            },
            "questions": {
                "value": "1. Synthetic Dataset Details:\n    a. Could you please provide more information about the size of your synthetic dataset, specifically the number of videos it contains?\n    b. What is the average length of the generated videos in the dataset?\n2. Motion Generation:\n    a. In Section 3.1.3, you mention generating intermediate frames between keyframes. How many intermediate frames are typically generated?\n    b. Is there a specific criterion or stopping condition that determines when to stop generating intermediate frames?\n3. Spatial Masks:\n    a. What is the purpose of the spatial masks M^inp and M^tar?\n    b. Could you provide a visual example or description of these masks?\n    c. Are they the same size as the input image I_t, and do they have the same value for all pixels?\n4. Limitations in Age Progression: I noticed that the generated videos primarily show age-related changes in the facial area. Why does the proposed approach not generate changes in other regions, such as hair and neck skin? How might this limitation be addressed in future work?\n5. Typos: I came across a few typos in the text, such as on line 311 and some symbols in Figure 2. Please ensure a thorough proofread to correct these errors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, authors focus on video face re-aging task considering the temporal consistency. Most re-aging methods processed each image individually without integrating temporal dimension of videos due to the lack of paired video datasets for supervised training. Thus, an important contribution from authors is a novel synthesis video dataset created via proposed pipeline, it features many subjects with covering a diverse range of age groups.  Then, a baseline video face re-aging architecture is designed to validate the effectiveness of the proposed video dataset. Last but not least, two tailored novel metrics are developed for evaluating the temporal consistency of video face re-aging task."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "10 existing state-of-the-art re-aging methods are compared in order to validate the efficacy of proposed synthesis video dataset and baseline architecture on public datasets, such as VFHQ and CelebV-HQ, as well as necessary ablation experiments. The paper is overall well written."
            },
            "weaknesses": {
                "value": "Although, a new paired video face re-aging dataset is essential for enhancing face re-aging technique and motivating relevant community. Overall, lack of novelty is disadvantage of this manuscript. First, video face re-aging dataset is constructed by a pipeline with three stages. Each of them focuses on off-the-shelf method, such as Style-based Age Manipulation (SAM) is chosen for image-based face re-aging, OSFV is chosen for key frame generation and FILM is chosen for motion generation. It is a general pipeline for constructing video dataset. Second, the proposed baseline architecture of video face re-aging is composed of off-the-shelf building block stacks. Such as recurrent block (RB) and Unet-based Encoder-Decoder. Even the input fashion of the proposed architecture is borrowed from Zoss et al, such as 5 channels with age masks, let alone the discriminator with PatchGAN proposed by Isola et al. Last but not least, the proposed Temporal-Age (T-Age) metric measures the age difference between two adjacent frames utilizing an off-the-shelf age classifier from Rothe at al.  In a short, this manuscript can be considered as a regular technical report, it has a gap to meet the novelty requirement for acceptance."
            },
            "questions": {
                "value": "There are some questions need to be clarified from authors.\n1. In line 292, for image and video discriminator loss , how to explain there is no ground truth in total objective function when updating the discriminator loss ? \n2. In Table 1,  three image-based face re-aging methods are compared, is there no comparison with SAM? and how about video-based method, such as diffusion autoencoders (Preechakul et al.) ?\n3. In Figure 4 (b), how to explain there is no CUSP results ?\n4. In Table 2, how to explain there is no video-based face re-aging method in user study ?\n5. In line 468, please give more detailed explanation about the sentence \u201c the significance of a 0.18 in TRWC in Table. 1 is evident by the user\u2019s choices\u201d\n6. Overall, I can\u2019t find more detailed meta information about the proposed video face re-aging dataset, such as how many identities or subjects, total duration of dataset and so on."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Although, StyleGAN is utilized to generate fake(not exist in real world) face image from a random noise, the proposed dataset may have the potential biases inherited from StyleGAN trained on FFHQ dataset."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}