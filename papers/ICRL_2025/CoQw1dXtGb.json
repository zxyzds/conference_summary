{
    "id": "CoQw1dXtGb",
    "title": "SPDIM: Source-Free Unsupervised Conditional and Label Shift Adaptation in EEG",
    "abstract": "The non-stationary nature of electroencephalography (EEG) introduces distribution\nshifts across domains (e.g., days and subjects), posing a significant challenge\nto EEG-based neurotechnology generalization. Without labeled calibration data\nfor target domains, the problem corresponds to a source-free unsupervised domain\nadaptation (SFUDA) problem. For scenarios with constant label distribution, Riemannian\ngeometry-aware statistical alignment frameworks on the symmetric positive\ndefinite (SPD) manifold are considered state-of-the-art. However, many practical\nscenarios, including EEG-based sleep staging, exhibit label shifts. Here, we\npropose a geometric deep learning framework for SFUDA problems under specific\ndistribution shifts including label shifts. We introduce a novel, realistic generative\nmodel and show that prior Riemannian statistical alignment methods on the SPD\nmanifold can compensate for specific covariate and conditional distribution shifts\nbut hurt generalization under label shifts. As a remedy, we propose a parameterefficient\nmanifold optimization strategy termed SPDIM. SPDIM uses the information\nmaximization principle to learn a single SPD-manifold-constrained parameter\nper target domain. In simulations, we demonstrate that SPDIM can compensate\nthe shifts under our generative model. Moreover, using public EEG-based braincomputer\ninterface and sleep staging datasets, we show that SPDIM outperforms\nprior approaches.",
    "keywords": [
        "geometric deep learning",
        "transfer learning",
        "source-free adaptation",
        "electroencephalography",
        "neurology",
        "brain-computer interfaces"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=CoQw1dXtGb",
    "pdf_link": "https://openreview.net/pdf?id=CoQw1dXtGb",
    "comments": [
        {
            "summary": {
                "value": "This study focuses on the realistic issue of label shifts in EEG across subjects and/or sessions (relative class proportions in target domains when source domains are class-balanced). Using theoretical analysis, it extends the SotA statistical alignment framework for handling distribution shifts in EEG to also include label shifts. The proposed SPDIM includes a domain-specific bias parameter estimated from unlabeled target data that reduces over-corrections done by the current SotA framework. Results on synthetic data and real-world EEGs demonstrate the value of SPDIM over baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Rigorous and clear presentation of technical details and full analytic workflow.\n- This work is a great example of theory-guided methods design for EEG.\n- Impactful choice of research problem - performance of EEG models under label shifts will remain a ubiquitous concern, both clinically and in the BCI space."
            },
            "weaknesses": {
                "value": "- (line 166) Q: Is the assumption of number of latent brain sources = number of observed scalp channels = P necessary or realistic?\n- No discussion of study limitations and/or future directions."
            },
            "questions": {
                "value": "- Q: Does this framework treat one subject or one EEG recording as one source/target domain containing both/multiple class labels?\n- Q: How does this framework for \"latent space alignment\" compare/relate to non-reimannian approaches for SFUDA for EEGs/multivariate timeseries? See [1] for a recent example. The \"test-time adaptation\" (Section 3.2) studies listed in [2] might also be relevant.\n- Q: What factors other than dataset size and label shifts could account for the high variability/stdev in Table 1? In most cases, handling label shift (either with RCT or SPDIM) decreases variability compared to \"w/o\", but its still seems high.\n- Minor comments: 1) pixel resolution of Figure 1 can be improved, 2) typo in citations at line 218 and 236., 3) line 443 remove \"standard-deviation in brackets\"\n- The anonymous code link is broken?\n\n[1] He, Huan, et al. \"Domain adaptation for time series under feature and label shifts.\" International Conference on Machine Learning. PMLR, 2023.\n\n[2] Garg, Saurabh, et al. \"Rlsbench: Domain adaptation under relaxed label shift.\" International Conference on Machine Learning. PMLR, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The study addresses a source-free unsupervised domain adaptation problem and proposes SPDIM, a framework based on the SPD manifold. \nSPDIM compensates for label shifts using proposed generative models, which prior Riemannian statistical alignment methods do not effectively handle. \nAdditionally, SPDIM applies the information maximization principle to learn domain-specific parameters. \nSimulation experiments demonstrate its superiority under various levels of label shift, and empirical analysis on real EEG datasets shows that it outperforms previous approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The motivation is clear and easy to follow. SPDIM aims to address adaptation under label shifts, a common challenge in real-world EEG datasets. \n  Theoretical analysis further explains the causes of deviations under label shifts.\n\n- Simulation experiments qualitatively validate the benefits of SPDIM in the presence of label shifts. \n   Cross-subject and cross-session experiments on motor and sleep-staging EEG datasets illustrate its superiority over existing alignment methods based on the SPD manifold."
            },
            "weaknesses": {
                "value": "- Some notations in equations seem confusing. For example, the index $j$ under $\\sum$ may need to be $i$ in Eq. (2). The invertible mapping $upper$ is defined on $S$, but $upper^{-1}$ appears in Eq.(10). \nAdditionally, $j_i$ and $j$ use the same letter but with different meanings, which could lead to ambiguity. The notation $Q$ in Eq.(15) seems to appear without prior introduction. \n\n- Some aspects of the method require further clarification. As mentioned in Line 249, the right-hand side of Eq. (15) is claimed to contain only domain-invariant terms. However, from my perspective, $C_i$ depends on the domain-specific \nmatrix $A_{j}$, as suggested by Eq. (13). According to Proposition 1, $ \\bar{C} _ {j(i)} $ converges to $I_P$. These indicate that $Q$ is linked to $A_{j}$, which may not be domain-invariant. Additionally, the relationship between the information maximization approach introduced in Section 3.3 and SPDIM (bias) / SPDIM (geodesic) is unclear. \n\n- To better demonstrate SPDIM\u2019s effectiveness, it would be beneficial to compare it with additional statistical alignment methods beyond those based on the SPD manifold. This would provide a more comprehensive evaluation against existing approaches."
            },
            "questions": {
                "value": "- Is the domain-specific formard model $A_{j}$ learned from features of a specific domain, or is it predefined? \n- How are domain specific parameters $\\Phi_{j(i)}$ and the geodesic step-size parameters $\\varphi_{j(i)}$ learned according to the proposed information maximization principle described in Section 3.3?\n- Is there any relationship between the adaptation performance and predefined hyperparameters, such as the rank of $A$ and the number of domains within $\\mathcal{D}_{s}$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The \"SPDIM\" paper introduces a novel geometric deep learning framework aimed at enhancing source-free unsupervised domain adaptation (SFUDA) for EEG data under both conditional and label shifts. By leveraging the symmetric positive definite (SPD) manifold and employing a parameter-efficient manifold optimization strategy, the proposed method, SPDIM, addresses significant generalization challenges in EEG data processing, especially where traditional Riemannian geometry methods fall short due to label shifts. SPDIM shows promising improvements across multiple EEG-based applications, including brain-computer interface tasks and sleep staging, demonstrating its efficacy over prior alignment frameworks"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe introduction of an SPD-manifold-constrained bias parameter is an advancement for tackling SFUDA in EEG.\n2.\tThe framework has been applied effectively across different tasks, showcasing broad applicability.\n3.\tSPDIM outperforms conventional methods, showing its resilience under varying label distributions."
            },
            "weaknesses": {
                "value": "1.\tThe motivation behind addressing label shifts and domain gaps with SPDIM is somewhat implicit, without clearly laying out why these challenges necessitate the proposed framework. \n2.\tThe paper contains an extensive number of equations and mathematical formulations in the main text, which can make the methodology difficult to follow. \n3.\tAlthough the paper compares SPDIM with several baselines, a broader set of comparisons, especially with newer unsupervised or semi-supervised EEG methods, could provide further insights into SPDIM\u2019s performance and robustness.\n4.\tWhile SPDIM improves accuracy under domain shifts, the model\u2019s interpretability remains limited. \n\nI will reconsider my assessment after checking rebuttual."
            },
            "questions": {
                "value": "Plz go and check weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces SPDIM, a framework for source-free unsupervised domain adaptation (SFUDA) in EEG-based applications, which are challenged by distribution shifts across sessions or subjects. SPDIM leverages the geometry of symmetric positive definite (SPD) matrices to handle conditional and label shifts, aligning EEG data across domains without requiring labeled target data. The approach introduces a domain-specific SPD-manifold bias to counteract label shifts, and optimizes alignment using an information maximization principle, which prevents mode collapse by ensuring class diversity and prediction confidence. Experimental results on EEG datasets for motor imagery and sleep staging show that SPDIM outperforms baseline SFUDA methods, demonstrating robust generalization across domains even under significant label distribution changes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well written.\n- The modelisation is original and insightful. I really enjoyed reading the modelisation part of the paper.\n- The developed methods are tested on 3 setups: synthetic data, Motor-Imagery and Sleep-Staging."
            },
            "weaknesses": {
                "value": "- At the time of reviewing the paper, the code is not available: \u201cThe repository is not found.\u201d is returned by anonymous.4open.science\n- A modelisation per domain of EEG data was proposed in [1] which could be worth citing in your introduction. Indeed, the authors mention there exists a linear mapping per domain to get domain-invariant tangent vectors (and without assumption on the mixing matrix (9)).\n- The experiment on motor imagery is limited since you artificialy unbalance the labels. Finding real world data which are naturally unbalanced would add value to the paper.\n- The mean accuracy of the 2 proposed methods are within the standard deviation of the recenter for the motor imagery application.\n- On the sleep-staging setup, you do not compare with adaptation methods expect recenter. You should compare at least to STMA or TMA (Spatio-Temporal Monge Alignment) which is presented in [2].\n- The presentation of the results is not homogeneous between the two applications. In particular, it is strange to me to call an \u201cablation study\u201d a comparison with other methods.\n\n[1] Collas, Antoine, R\u00e9mi Flamary, and Alexandre Gramfort. \"Weakly supervised covariance matrices alignment through Stiefel matrices estimation for MEG applications.\" arXiv preprint arXiv:2402.03345 (2024).\n\n[2] Gnassounou, T., Collas, A., Flamary, R., Lounici, K., & Gramfort, A. (2024). Multi-Source and Test-Time Domain Adaptation on Multivariate Signals using Spatio-Temporal Monge Alignment. arXiv preprint arXiv:2407.14303.\n\n\nI put a rating of 5 but I am open to increasing it."
            },
            "questions": {
                "value": "- You mention there are conditional shifts in EEG data (p_j(x|y) changes between domains). Can you relate this with your modelization?\n- What is D in the Remark 1?\n- Does the Propostion 2 still hold when M_j does not tend to the infinite?\n- You train your model on the target domain (in an unsupervised manner). Did you train/test split the target domain?\n- How easy to train are the methods you use? e.g. USleep is rarely used as a baseline in other sleep staging papers. Providing infos the lr scheduler, batch size, \u2026 would be valuable.\n- I am surprised that the spatial covariance is enough to classify sleep stages. Usually, the temporal information is used but not the spatial one. Can you comment on this?\n\nA few typos:\n- D and P are both used for the data dimension\n- There are \u201c?\u201d in lines 218 and 236.\n- Q and U are both used for domain-invariant par of the mixing matrix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Due to the difficulty of the SPD method in handling label shift issues, this paper proposes a geometric deep learning framework, SPDIM, for SFUDA problems under specific distribution shifts, including label shifts. SPDIM employs the information maximization principle to learn a single SPD-manifold-constrained parameter per target domain. Using public EEG-based brain-computer interface and sleep staging datasets, we demonstrate that SPDIM outperforms prior approaches."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper investigates the label shift problem in SFUDA, with a very strong motivation and significant practical relevance.\n\nThe method proposed in this paper has a certain theoretical foundation, and the derivation of some propositions may provide inspiration for solving the label shift problem."
            },
            "weaknesses": {
                "value": "Although this paper focuses on EEG SFUDA problems, the proposed method does not appear to be specifically designed for EEG but seems to be a more general approach applicable to any label shift scenario. From the perspective of EEG research, the method lacks specificity for EEG data, while from the perspective of SFUDA research, the paper only validates the method on EEG data, lacking more reliable experimental verification.\n\nThe experiments are not solid. The paper does not clearly present the experimental setup, such as the hyperparameters of the models, the partitioning method of the source and target domains, etc. Additionally, the EEG decoding methods compared in the experiments are not sufficiently strong. The paper does not compare some classic EEG decoding models, such as EEGNet and EEG Conformer, nor does it compare some sleep staging models, such as DeepSleepNet. The domain adaptation methods only compare Information Maximization (IM), and such insufficient comparisons are not enough to prove the superiority of the proposed method.\n\nThe writing of this paper still has some room for improvement. For example: Figure 1 has low resolution, and the four sub-figures in Figure 2 lack sub-titles."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}