{
    "id": "5V0f8igznO",
    "title": "Captured by Captions: On Memorization and its Mitigation in CLIP Models",
    "abstract": "Multi-modal models, such as CLIP, have demonstrated strong performance in aligning visual and textual representations, excelling in tasks like image retrieval and zero-shot classification. Despite this success, the mechanisms by which these models utilize training data, particularly the role of memorization, remain unclear. In uni-modal models, both supervised and self-supervised, memorization has been shown to be essential for generalization. However, it is not well understood how these findings would apply to CLIP, which incorporates elements from both supervised learning via captions that provide a supervisory signal similar to labels, and from self-supervised learning via the contrastive objective.\nTo bridge this gap in understanding, we propose a formal definition of memorization in CLIP (CLIPMem) and use it to quantify memorization in CLIP models. Our results indicate that CLIP\u2019s memorization behavior falls between the supervised and self-supervised paradigms, with \"mis-captioned\" samples exhibiting highest levels of memorization. \nAdditionally, we find that the text encoder contributes more to memorization than the image encoder, suggesting that mitigation strategies should focus on the text domain. \nBuilding on these insights, we propose multiple strategies to reduce memorization while at the same time improving utility---something that had not been shown before for traditional learning paradigms where reducing memorization typically results in utility decrease.",
    "keywords": [
        "memorization",
        "multi-modal",
        "clip",
        "vision language models"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We propose a metric to measure memorization in CLIP models and study the memorization behavior in the multi-modal setup.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5V0f8igznO",
    "pdf_link": "https://openreview.net/pdf?id=5V0f8igznO",
    "comments": [
        {
            "summary": {
                "value": "**Summary:** Understanding the memorization / generalization tradeoff is important to properly quantify modern ML models. This work focuses on CLIP which applies InfoNCE between image and text pairs and attempts to quantify the extent to which CLIP models memorize. The authors introduce CLIPMem to quantify memorization and find that the text encoder contributes more to memorization than the vision encoder. They also propose strategies to mitigate / remove memorized samples to improve performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**Stong points:**\n- Well highlighted literature on memorization.\n- Defines CLIPMem based on hold one out strategy (similar to Feldman et. al in supervised learning).\n- Interesting results on mis-captioned text labels, multi-caption and removal of memorized examples.\n- Reasonable pretraining datasets like CC3M\n- Clean separation of training and test splits for measuring memorization."
            },
            "weaknesses": {
                "value": "**Weak points:**\n- Missing ability for CLIPMem to be applicable to general off-the-shelf CLIP models. Currently if I understand correctly it requires retraining on specific splits. \n- Clarity of specifics of CLIPMem is used for vision only and joint vision + text can be improved.\n- The noising results (Table 5-b) are not very convincing. Almost all the results are within the same +/- std range.\n- The linear probe accuracy seems quite low (Table 1, 5-a, 5-b,  6-a/b).  \n\n**Nit:**\n- Text and images in Figure one are very hard to read. Suggest larger and fewer images and move rest to appendix. \n- Figure 3 can be make larger / more readable by doing share-y and increasing font sizes."
            },
            "questions": {
                "value": "**Questions:**\n\n- Is CLIPMem bounded? \n- What dimension is kept constant for Table 1? Are the total training samples [count] seen the same? \n- It would be interesting to evaluate infinite data regimes (i.e. no repeated data) rather than classical K-epoch  runs. Would the results for - memorization hold here? Just like in language this setting is becoming more and more common."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces CLIPMem, a novel metric to quantify memorization in CLIP, which combines elements of supervised and self-supervised learning. The paper shows that memorization within CLIP often arises from mis-captioned or atypical samples, particularly within the text modality rather than the image modality. They propose mitigation strategies that reduce memorization without sacrificing model utility, unlike traditional methods that often degrade performance when reducing memorization. It reports several interesting findings, including 1) CLIP's memorization lies between supervised and self-supervised paradigms, with high memorization for data with inaccurate or misaligned captions; 2) Text domain adjustments, such as using varied or augmented captions, reduce memorization and improve generalization, defying the usual trade-offs seen in other paradigms."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ It introduces a new metric -- CLIPMem to provide a new way for measuring memorization in multi-modal settings, a gap in previous research.\n+ It performs empirical analysis to show differences in memorization between the text and image modalities, providing actionable insights.\n+ It proposes techniques to successfully reduce memorization while preserving or even enhancing model utility, challenging established norms.\n+ By highlighting the risks of training with uncurated, potentially mis-captioned data, the paper suggests guidelines that can benefit real-world multi-modal model training practices."
            },
            "weaknesses": {
                "value": "- While tailored to CLIP, the metric and findings may need adaptation to apply effectively to other multi-modal models with different architectures.\n- The experiments focus on datasets like COCO and CC3M, so it\u2019s unclear how well these findings generalize to other large-scale or domain-specific datasets.\n- The mitigation strategies, such as augmenting captions or generating variations, may incur additional computational costs in training, which could limit practicality for some users."
            },
            "questions": {
                "value": "Would you please comment on how the metric adapt to other multi-modal models besides CLIP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "They study the memorization problem in the CLIP model unlike existing studies focusing on the unimodal memorization problem. \nThey propose a new metric to analyze it and find that memorization seems to be more significant in text encoder than in image encoder. Their analysis indicates that augmenting captions can be a key to mitigating memorization in the CLIP model. Their experiments confirm that augmenting captions can improve the quality of the image encoder's representations while reducing memorization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. They propose a new metric to measure memorization in the CLIP model. The design of the metric is reasonable. \n2. Their insight that memorization is more significant in the text encoder is new and might interest readers. \n3. They conduct analysis on the augmentation of the text and images, which might be also interesting to some readers. \n4. This paper is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "1. They do not discuss how model size can affect memorization. Although I am not very familiar with this topic, I guess the model size can affect their arguments. For example, if they utilize a larger image encoder, the memorization might be more significant on the image side. Therefore, I think their conclusion about which encoders suffer more from memorization can change by the size of the encoders, but they do not discuss much. \n\n2. Most of their findings sound a bit too reasonable and are not surprising. Their finding that augmenting text improves the CLIP model has already been observed in many previous papers though they probably did not discuss memorization. Also, it is not hard to imagine that augmenting datasets mitigates memorization. In these points, I think their findings are not impressive. \n\n3. They mention that their metric is effective in removing noisy samples. But, they compare their approach only with random replacement. I think they need to add more baseline, such as naive CLIP's similarity as done in many works. \n\n4. In Table 1, the authors augment images by using a diffusion model. But, as they imply, such augmentation can cause a distribution shift in the image side and does not give much intuition about image augmentation. \n\nOverall, I think this paper is well-organized and delivers a clear statement to readers, which I like. However, I think their findings are not very surprising and lack impact due to the reasons described in 1, 2. My rating is based on it."
            },
            "questions": {
                "value": "My rating is mainly based on 1 and 2. Please respond to those points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}