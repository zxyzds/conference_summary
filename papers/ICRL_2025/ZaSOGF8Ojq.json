{
    "id": "ZaSOGF8Ojq",
    "title": "TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration",
    "abstract": "Graph Neural Networks (GNNs) have shown remarkable performance in various scientific domains, but their lack of interpretability limits their applicability in critical decision-making processes. Recently, intrinsic interpretable GNNs have been studied to provide insights into model predictions by identifying rationale substructures in graphs. However, existing methods face challenges when the underlying rationale subgraphs are complicated and variable. To address this challenge,\nwe propose TopIng, a novel topological framework to interpretable GNNs that leverages persistent homology to identify persistent rationale subgraphs.\nOur method introduces a rationale filtration learning technique that models the generating procedure of rationale subgraphs, and enforces the persistence of topological gap between rationale subgraphs and complement random graphs by a novel self-adjusted topological constraint, topological discrepancy. We show that our topological discrepancy is a lower bound of a Wasserstein distance on graph distributions with Gromov-Hausdorff metric. \nWe provide theoretical guarantees showing that our loss is uniquely optimized by the ground truth under certain conditions.\nThrough extensive experiments on varaious synthetic and real datasets, we demonstrate that TopIng effectively addresses key challenges in interpretable GNNs including handling variiform rationale subgraphs, balancing performance with interpretability, and avoiding spurious correlations. \nExperimental results show that our approach improves state-of-the-art methods up to 20%+ on both predictive accuracy and interpretation quality.",
    "keywords": [
        "topological data analysis",
        "persistent homology",
        "graph neural network",
        "interpretability",
        "explainability",
        "filtration learning"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "A topologically interpretable GNN with a novel topological discrepancy loss is proved to be uniquely optimized by ground truth.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ZaSOGF8Ojq",
    "pdf_link": "https://openreview.net/pdf?id=ZaSOGF8Ojq",
    "comments": [
        {
            "summary": {
                "value": "In this paper, the authors propose a graph explanation method based on the persistent homology to identify stable rational subgraphs via persistent rationale filtration learning. Specifically, they propose a topological discrepancy loss to achieve the goal. Experimental results seem to outperform state-of-the-art models in most datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The idea of using topological data analysis for graph explanation is novel and interesting. \n\n- The derivation of the proposed method seems to be solid with theoretical insights."
            },
            "weaknesses": {
                "value": "- The preliminary, especially the TDA, is not very clear, making it hard for readers without the knowledge of persistent homology (I don\u2019t think it is necessarily a prerequisite knowledge for graph explainability). What is the difference of $H$ between the $p$-homology functor $H_p (\\mathcal{F} (G))$ and $H_p (G_{\\le t})$? Figure 1 is also not easy to understand with a clear explanation of the homology groups. The caption is over lengthy and not illustrating the intuitive idea very well. It will be better to segment the long caption into several parts to make it more readable.\n\n- In the introduction, they mention one challenge that exists in current graph explanation models: that different graphs may contain different core subgraphs even within the same category. Yet, I don\u2019t see why the proposed method can tackle this problem well. The training procedure involves learning a filtration function to separate the graph. How such a mechanism can tackle the challenge and why the separating method in other methods cannot are not well discussed. It is recommended to add more analysis (or examples) to demonstrate why the proposed method can address this challenge and others cannot.\n\n- The prior regularization seems to be empirical. The hyperparameters of the prior are not trivial. This raises concern whether the proposed method is sensitive to the selection of the prior and the corresponding parameters. More discussion is encouraged."
            },
            "questions": {
                "value": "- See the weakness above\n\n- The results of MUTAG are not impressive in terms of both interprebility and accuracy. Further analysis for these results are required for better understanding the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces TOPING, a novel intrinsically interpretable framework for GNNs that uses topological data analysis (TDA) to identify stable rationale subgraphs. TOPING addresses key limitations in existing interpretable GNN models by using a rationale filtration learning approach. Extensive experimentation shows improvements over state-of-the-art interpretability and predictive performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1. Using TDA for intrinsically interpretable GNNs is interesting. \n\nS2. The authors provide theoretical guarantees to show that topological discrepancy optimally captures rationale subgraphs. \n\nS3. The authors present extensive experiments across multiple datasets."
            },
            "weaknesses": {
                "value": "W1. The mathematical notation and presentation could be improved, as some definitions and descriptions are unclear or ambiguous. Clarifying these elements would enhance the paper\u2019s readability for readers less familiar with topological data analysis. \n\nW2. The paper would benefit from a detailed time complexity analysis and runtime study, particularly in comparison with baseline methods. This addition would clarify whether TOPING\u2019s performance improvements come at the cost of efficiency. \n\nW3. Since TOPING requires learning a filtration function that assigns values to each edge, this method may become impractical for large graphs due to the need to compute and store $|E|$ values. Further discussion on how TOPING might be adapted or approximated for larger graphs would be valuable."
            },
            "questions": {
                "value": "Q1. The explanation of TDA in Section 2.2 is unclear. In Figure 1, the subgraph $G_{t}$ appears to have more edges as $t$ decreases. However, in line 170, it seems that $G_{t}$ should contain more edges as $t$ increases. Could the authors clarify this apparent discrepancy?\n\nQ2. In line 271, the term \"topological invariants $\\tau$ \" is introduced without a clear mathematical definition. Could the authors provide a formal definition for $\\tau$ to improve understanding?\n\nQ3. In line 278, Equation (3) is not properly referenced. Besides, it appears disconnected from the context. Could the authors clarify its relevance here?\n\nQ4.  The authors state that $f_\\phi$ and $h_\\phi$ share parameters. However, $f_\\phi$ is defined as a function mapping to $[0,1]^{|E|}$, while $h_\\phi$ seems to map to $\\mathbb{R}^{|V|}$. Could the authors explain how parameter sharing is feasible between these two functions with distinct output spaces?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an interpretable GNN framework that leverages techniques from topological data analysis, specifically persistent homology. The method splits an input graph into a rationale subgraph (important subgraph) and a noise subgraph, where the rationale subgraph is used for both predicting the target and providing an explanation for the prediction. The authors introduce a novel loss function that quantifies the topological discrepancy between the rationale and noise subgraphs and jointly optimize this loss along with the prediction loss. The framework is evaluated using several synthetic datasets and two real-world molecular datasets, demonstrating its effectiveness."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThis paper introduces an innovative use of topological analysis to provide intrinsically explainable GNNs, offering a promising direction for future research.\n2.\tThe authors proved that the method can optimize to the true important subgraph under certain conditions, providing a solid mathematical foundation for the approach."
            },
            "weaknesses": {
                "value": "1.\tThe writing is difficult to follow due to unclear definitions of the mathematical symbols used (see questions below). The figures are not well-explained and can be confusing, especially for readers with limited knowledge of topological analysis.\n2.\tThe experimental datasets are limited, focusing only on synthetic and relatively simple molecular datasets. Additionally, qualitative visualizations for the MUTAG dataset are missing, which reduces the clarity of the results."
            },
            "questions": {
                "value": "1. What is $\\beta_1$ in Fig. 1?\n1. In Eq. 3, what is $\\mathcal R$?\n2. What is the difference between $f_{\\phi}, h_{\\phi}$? In Fig.2 they are all equal to $GNN_\\phi$ so they are the same? And is $\\phi$ the parameters of GNN?\n3. After reading Sec 3 I still have problem understanding how the model predicts. In Fig.2 it seems that the model first generates a subgraph $G_X$ according to the edge score from $f_\\phi(G)$, feed it into a GNN $h_\\phi$, somehow concatenate with $\\mathcal T_X, \\mathcal T_\\epsilon$, and use a classifier to get the final prediction. What is $\\mathcal T_X, \\mathcal T_\\epsilon$ and how are they concatenated with output of GNN $h_\\phi$? A more detailed explanation of Fig.2 is needed.\n4. Row 267-269:the defined $\\mathcal G, \\mathcal F(\\mathcal G)$ are never used.\n5. Does the measure of topological discrepancy also consider the node/edge features? It would be interesting to see how the method performs on graphs where not only topological structure but also node/edge features are relevant."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a framework TOPING for XGNN, which is similar to the GSAT pipeline. It introduces a rationale filtration learning technique and a topological discrepancy constraint to separate important subgraphs from random ones. The method offers theoretical guarantees and improves both accuracy and interpretability, outperforming existing approaches by over 20% in experiments. However, the comparison with other methods is potentially unfair due to differences in evaluation metrics or datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper introduces an existing method based on the assumption, \"either explicitly or implicitly that the subgraph rationales are nearly invariant across different instances within the same category of graphs.\" This is a reasonable and novel aspect of the problem. However, there are few experiments to support this idea.\n2. This paper introduces the $p$-homology functor to XGNN, which is a novel approach to addressing this problem."
            },
            "weaknesses": {
                "value": "1. Some expressions are confusing. For example, in Equation 3, the meaning of $\\mathcal{R}$ is unclear. In Figure 2, the meaning of $L_{topo}$ is also not explained.\n2. In subsection 3.2, the distribution of the prior is determined by the hyperparameters $\\mu$ and $r$. However, $\\mu$ is a pre-defined parameter, raising the question of how to set this parameter. Additionally, $r$ is a learnable parameter. Will this setting lead to a trivial solution, where the divergence between $\\mathcal{N}(\\mu_1,r_1)$ and $\\mathcal{N}(\\mu_2,r_2)$ is small?\n3. The comparison is potentially unfair. In Appendix C.2.1, all baselines are based on the GIN backbone, while TOPING uses the CINpp backbone, which could result in biased comparisons."
            },
            "questions": {
                "value": "1. In Figure 1, the meaning of the row lines is unknown."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}