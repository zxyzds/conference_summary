{
    "id": "GOjr2Ms5ID",
    "title": "Cascaded Learned Bloom filter for Optimal Model-Filter Size Balance and Fast Rejection",
    "abstract": "Recent studies have demonstrated that learned Bloom filters, which combine machine learning with the classical Bloom filter, can achieve superior memory efficiency. However, existing learned Bloom filters face two critical unresolved challenges: the balance between the machine learning model size and the Bloom filter size is not optimal, and the reject time cannot be minimized effectively. We propose the Cascaded Learned Bloom Filter (CLBF) to address these issues. Our optimization approach based on dynamic programming automatically selects configurations that achieve an optimal balance between the model and filter sizes while minimizing reject time. Experiments with real-world datasets show that CLBF reduces memory usage by up to 24% and decreases reject time by up to 14 times compared to the state-of-the-art learned Bloom filter.",
    "keywords": [
        "learned Bloom filter",
        "learned index",
        "membership query",
        "optimization",
        "dynamic programming"
    ],
    "primary_area": "optimization",
    "TLDR": "We propose a novel learned Bloom filter with a cascaded structure that optimizes model-filter size balance and minimizes rejection time, achieving up to 24% memory savings and 14x faster rejection than the state-of-the-art method.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=GOjr2Ms5ID",
    "pdf_link": "https://openreview.net/pdf?id=GOjr2Ms5ID",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes Cascaded Learned Bloom Filter (CLBF) to address the balance between the machine learning model size of learned Bloom filters and the Bloom filter size of classical Bloom filters.  The authors formulate an optimization approach based on dynamic programming to select configurations, with the reject time also part of the optimization objective. . Experiments using two benchmark datasets show that CLBF reduces memory usage by up to 24% and decreases reject time by up to 14 times compared to the state-of-the-art learned Bloom filter."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "A cascading (learned) Bloom filter (LBF)  architecture that combines both sandwiched LBFs and partitioned LBFs (PLBFs), with classical BFs (as branch BFs)."
            },
            "weaknesses": {
                "value": "The paper claims that the \"optimization approach based on dynamic programming automatically selects configurations\nthat achieve an optimal balance between the model and filter sizes while minimizing reject time.\"  This claim is not substantiated at all. \nYou aim to optimize eq.(4), which seems to make sense, but which is largely meaningless.  First of all, unlike the classical BF where one can derive a low bound on the relation between false positive rate and filter size, what is the relation between the false positive rate of a LBF, $f^{(t})_i$ and the model size?    Second, given the cascading architecture, $\\prod f^{(t})_i$ is not the overall false positive rate of the entire CLBF.  Most of section 3.3 makes little sense to me."
            },
            "questions": {
                "value": "See the weakness above.\n\n1.  What is the relation between the false positive rate of a LBF, $f^{(t})_i$ and the model size?  Without an explicit expression, how do you optimize eq.(4) using dynamic programing?  \n\n2. Please justify that  $\\prod f^{(t})_i$  yields the overall false positive rate of the entire CLBF. \n\n3. In Section 3.3, You never establish the optimality of your dynamic programming formulation. Please formally establish the optimality of your solution.\n\n4. How does the \"depth\" of your cascade, i.e., d, affect the overall balance between the model and filter sizes"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This article introduces a Cascaded Learned Bloom Filter (CLBF). CLBF combines the advantages of machine learning and traditional Bloom filters, aiming to address two critical issues in existing Learned Bloom Filters (LBF): the optimal balance between the size of the machine learning model and the Bloom filter, and the effective minimization of rejection time. CLBF employs a cascade structure with multiple machine learning models and multiple Bloom filters arranged alternately. The goal is to minimize memory usage and rejection time under the constraint that the false positive rate does not exceed F. CLBF uses dynamic programming optimization methods to automatically select configurations, achieving an optimal balance between model and filter sizes, and minimizing rejection time. Experiments show that CLBF reduces memory usage by 24% and rejection time by 14 times compared to the state-of-the-art Learned Bloom Filter (PLBF)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The article proposes a novel structure of CLBF that optimizes memory usage and query performance by cascading multiple machine learning models and Bloom filters. It utilizes dynamic programming to automatically select the optimal configuration, reducing the manual parameter tuning effort and potentially finding better solutions.\n2. By adjusting hyperparameters, CLBF can trade off between memory efficiency and rejection time to adapt to different application scenarios. The article conducts extensive experiments with real-world datasets, validating the effectiveness of CLBF.\n3. The paper provides a clear description of the CLBF's implementation, accompanied by a thorough theoretical analysis."
            },
            "weaknesses": {
                "value": "1. The article primarily focuses on the application of a single machine learning model, XGBoost, which limits the exploration of how different models might contribute to the performance of CLBF. A comparison with a variety of machine learning models could provide a more comprehensive understanding of CLBF's versatility and robustness across different algorithmic approaches.\n2. While the article effectively demonstrates the benefits of CLBF, it would be beneficial to explore whether fine-tuning the hyperparameters of the machine learning model or alternative constructions of cascaded Bloom filters could yield similar or superior optimization results. This discussion could offer additional insights into the factors that contribute to the overall performance of CLBF."
            },
            "questions": {
                "value": "1. Is it possible that, instead of employing a cascaded structure, optimizing the hyperparameters of the machine learning model within a Learned Bloom Filter could also achieve similar performance enhancements? \n\n2. It raises an interesting question whether embedding a multi-layer deep learning model within the initial machine learning framework could further enhance optimization. Could such a multi-layer structure, potentially through algorithms like Bayesian Optimization, or Adam, autonomously identify optimal network parameters to achieve enhanced model optimization?\n\n3. Is it possible to use a combination of a machine learning model with hierarchical Bloom filters for optimization? For example, using one Bloom filter to filter out most elements, and then using a second, more precise Bloom filter for final verification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents the Cascaded Learned Bloom Filter (CLBF), a learned Bloom filter design aiming to optimize the balance between memory usage and reject time. The CLBF achieves these goals using dynamic programming and by designing a cascaded structure combining machine learning models and Bloom filters. Experimental evaluations on real-world datasets demonstrate CLBF\u2019s improvements in reject time and memory efficiency compared to previous state-of-the-art methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1- The paper effectively summarizes and describes prior research on learned Bloom filters, providing a clear understanding of existing work and where CLBF differs. For example, CLBF introduces a unique combination of sandwiched BF and PLBF structures in a cascade, allowing a flexible balance between model and filter sizes and distinguishing it from prior approaches.\n\n2- The experiments cover multiple datasets and baselines, which provides a well-rounded analysis of CLBF's effectiveness and limitations across diverse configurations."
            },
            "weaknesses": {
                "value": "1- The paper\u2019s primary contribution seems minimal, as it largely combines established techniques from sandwiched Bloom filters and PLBF within a cascade structure. This approach suggests an incremental rather than a groundbreaking contribution.\n\n2- The emphasis on optimizing reject time raises applicability concerns. There is a lack of discussion about real-world applications where reduced reject time is crucial, as well as cases where it might be less relevant. More critically, the cascade structure may increase query time for elements in the set, impacting overall efficiency. A fair comparison would benefit from including average query time metrics across different query types.\n\n3- The dynamic programming approach, while effective, is computationally intensive. Any change in parameters would require re-optimization, and even without changes, the initial construction time is high (as illustrated in Figure 6). This may limit the scalability and practicality of CLBF in time-sensitive applications."
            },
            "questions": {
                "value": "1- Could the design be made general for any learning-augmented application, extending beyond Bloom filters?\n\n2- What are real-world applications where reduced reject time is prioritized over faster query times for present keys? How could the cascade depth be controlled based on the target query time for these applications?\n\n3- Given the variety of ML models with diverse performance profiles (e.g., different sizes or query times), how adaptable is the proposed design? Would the structure or configuration require major adjustments for different ML model types?\n\nMinor typos:\n\n(1) line 266 includes extra \""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper the authors present a new method for the set membership query that combines Bloom filters with machine learning models. Unlike the earlier definitions of learned Bloom filters (LBF) that typically combine a single learned model with a few Bloom filters, this paper uses an alternating sequence of learned models and Bloom filters to answer a query. The stated goals that the authors seek to achieve are the reduction of the memory footprint of the LBF and also reduction of the time to rejection of a negative query."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "S1. The key strength of the paper is that the ML models are assumed to be weak learners. Since the use case of Bloom filters is generally in distributed databases and filesystems, often there are no strong patterns in the keys and so it is difficult to expect high accuracy (this conclusion is based on some work that has been done in my own group over the years). This comment highlights the significance of this work under review."
            },
            "weaknesses": {
                "value": "W1. Perhaps the most important weakness here is that the introduction of a multi-layered structure with great coding complexity and many learning models adds so much complexity to the structure that there is little to no chance that such a proposal will ever find its way into an actual file system. A quick look at Fig 1 and Fig 2 that have been placed side to side will show why. \n\nW2. The tremendous increase in complexity of the solution is reflected in the construction time jump (see Fig 6). The authors need to ask themselves if such a massive increase over simple Bloom filters is justifiable. In the aftermath of Kraska's paper we seem to have forgotten that Bloom's idea was a lightweight and quick data structure to speed up set membership. In distributed filesystems there is a need to periodically rebuild all the set membership structures, once too many writes have been logged. I don't think the kind of construction times being shown here are acceptable.\n\nW3. Important baselines are missing. The authors have not compared with the following methods that are competitors to Vaidya's PLBF:\n- Zhenwei Dai and Anshumali Shrivastava. Adaptive Learned Bloom Filter\n(Ada-BF): Efficient Utilization of the Classifier with Application to Real-Time\nInformation Filtering on the Web, NeurIPS 2020\n- Arindam Bhattacharya, Chathur Gudesa, Amitabha Bagchi, Srikanta Bedathur.\nNew wine in an old bottle: Data-aware Hash Functions for Bloom Filters, VLDB 2022.\n\nBoth these papers measure memory used. While it can be argued that Vaidya et. al. have compared against Ada-BF already and beaten it on memory (but not comprehensively), the latter paper shows improved memory utilization over both Vaidya and Ada-BF. So there is no reason why it should not have been used as a baseline. The authors have cited the first paper but not the second. \n\nW4. Memory-learning tradeoff not explored adequately. The authors mention that there has to be a tradeoff between memory and learning in terms of model size, but there is no experiment that investigates this tradeoff.\n\nW5. Mathematica quantities not properly defined. For example, is Reject Time defined in terms of clock time?"
            },
            "questions": {
                "value": "Q1. How would your method perform in a dynamic case where the set membership structures need to be reconstructed periodically?\n\nQ2. How did you decide how much memory it to be assigned to Bloom filters and how much is to be kept for ML models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}