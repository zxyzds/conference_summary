{
    "id": "4GT9uTsAJE",
    "title": "AdaGrad under Anisotropic Smoothness:  A Fine-Grained Analysis",
    "abstract": "Adaptive gradient methods have been widely adopted in training large-scale deep neural networks, especially large foundation models. Despite the huge success in practice, their theoretical advantages over classical gradient methods with uniform step sizes across all coordinates (e.g. SGD) have not been fully understood, especially in the large batch-size setting commonly used in practice. This is because the only theoretical result that can demonstrate this benefit was obtained in the original paper of Adagrad for convex nonsmooth objective functions, which is insufficient for large batch algorithms. In this work, we attempt to resolve this gap between theory and practice by proposing a novel anisotropic generalized smoothness assumption and providing corresponding analysis of Adagrad. It is shown that under anisotropic smoothness and noise conditions, AdaGrad can achieve faster convergence guarantees in terms of better dimensional dependence than algorithms with uniform step sizes across all coordinates. Experiments in logistic regression and instruction following fine-tuning tasks provide strong evidence to support our novel assumption and theoretical analysis.",
    "keywords": [
        "Optimization theory",
        "Convergence analysis",
        "Stochastic optimization",
        "Adaptive gradient methods"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4GT9uTsAJE",
    "pdf_link": "https://openreview.net/pdf?id=4GT9uTsAJE",
    "comments": [
        {
            "summary": {
                "value": "This paper provides a detailed analysis of the AdaGrad optimization algorithm under anisotropic smoothness assumptions, addressing gaps in theoretical convergence for large-scale tasks. It introduces a new anisotropic smoothness framework that better explains AdaGrad\u2019s convergence speed, especially for large-batch training. Experiments on logistic regression and GPT-2 fine-tuning support these theoretical claims, showing AdaGrad\u2019s improved performance over SGD."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The main strength lies in its novel anisotropic assumptions, which align well with AdaGrad\u2019s observed performance in high-dimensional settings. The experiments effectively validate the theoretical benefits, highlighting AdaGrad\u2019s adaptability to large batch sizes and diverse data structures. For the rest it is a standard optimization analysis."
            },
            "weaknesses": {
                "value": "This kind of work always relies on assumptions which limits their applicability to the setting of interests, as neural networks. However, this is common and not really an issue. See also questions."
            },
            "questions": {
                "value": "Can please you better compare to Convergence Analysis of Adaptive Gradient Methods under Refined Smoothness and Noise Assumptions - D Maladkar, R Jiang, A Mokhtari\u00a0- arXiv preprint arXiv:2406.04592, 2024?\nAlso, is much work required to generalize to Adam?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The work provide analysis result for the convergence of Adagrad for training of machine learning models with large batch size, emphasizing the effects of anisotropic smoothness. The authors then compare the results with similar results for SGD and Adagrad-norm and point out the potential of Adagrad. In general, the work can be helpful to under stand the training process."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The work provides an analysis result which may be the first one for Adagrad. This can be helpful for others to understand the potential of Adagrad and select optimizers for training tasks."
            },
            "weaknesses": {
                "value": "The numerical results are not sufficient to verify the assumptions and analytic results."
            },
            "questions": {
                "value": "1. Convexity is used in a large part of the paper. In many machine learning models, there are more parameters than data. In this case, local minima w* may not be isolated points. Instead, it can be a manifold. What is the impact of overparametrization to the results in this work?\n\n2. In Table 2, the authors list the coefficients and norms in the analytical results. It is also important to see how well the convergence of the loss (or gradients) are controlled by these coefficients and norms.\n\n3. For Table 4, since the work mainly discusses the convergence rate of Adagrad, it is better to show how the loss converges and how do the  authors select hyperparameters."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the convergence of Adagrad under anisotropic smoothness conditions. The main contributions are:\n1. Defining the anisotropic smoothness condition.\n2. Studying the convergence of Adagrad for convex and nonconvex problem under the anisotropic condition.\n3. Further nonconvex results for relaxed smoothness conditions. \n\nStrength:\n\nI think the paper is presented in a very clean and organized manner. The key results and discussions are clear. \n\nUp to my knowledge, although anisotropic smoothness were hinted across different setups, there is no very systematic study prior to this work. Therefore, I think the results here can be a valid contribution to optimization theory.\n\nWeakness:\n\n-The results are not surprising, and hence I didn't find the analysis / statements to be novel.\n\n\n-In addition to reviewing adagrad analyses, it would be helpful to review anisotropic analysis. Several related works that I could think of : analysis on coordinate descent;  Nesterov's study on zero-order methods; adagrad's advantage on sparse gradients; Adam's convergence on infinity norm rather than l2 norm of gradients, etc.\n\nAlthough, the above results probably are not directly comparable, it would be good to summarize and discuss the differences.\n\nSome results that can make the work more impressive are listed below:\n\n-Lower bounds to justify when and why adaptive step,  diagonal / full matrix adaptivity are sufficient / insufficient would be very interesting. \n\n-Given the analysis, can faster methods be proposed for neural nets?"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "See summary"
            },
            "weaknesses": {
                "value": "See summary"
            },
            "questions": {
                "value": "See summary"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper analyzed the convergence rate of AdaGrad, showing that AdaGrad converges faster than SGD with respect to the dimension of parameters."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper analyzed the convergence rate of AdaGrad. Then, it showed that the convergence rate of SGD depends on $D_{\\infty}$, while the rate of AdaGrad depends on $D_2$. $D_2$ depends on the dimension of parameters, while $D_{\\infty}$ does not. Thus, this paper claimed that AdaGrad converges faster than SGD."
            },
            "weaknesses": {
                "value": "1. The authors compared the convergence rate of AdaGrad in Theorem 4.1 and the convergence rate of SGD, Eq. (6). The convergence rate in Eq. (6) depends on $D_2$, while the tighter convergence rate that depends on only $\\| x_0 - x^\\star\\|$ was more common. \n\n2. $D_\\infty$ does not depend on the dimension of the parameter. However, it is unclear whether $D_\\infty$ is smaller than  $\\| x_0 - x^\\star\\|$.\n\n2. Theorem 4.1 assumes that $L_1 = 0$, which sounds a bit strong assumption. The reviewer feels that it would be better to provide the intuition why this assumption is necessary, at least in the Appendix.\n\n2. It was confusing which term corresponds to \"bias term\" in line 272."
            },
            "questions": {
                "value": "See the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}