{
    "id": "sz7HdeVVHo",
    "title": "Structure-preserving contrastive learning for spatial time series",
    "abstract": "Informative representations enhance model performance and generalisability in downstream tasks. However, learning self-supervised representations for spatially characterised time series, like traffic interactions, poses challenges as it requires maintaining fine-grained similarity relations in the latent space. In this study, we extend time series contrastive learning by incorporating two structure-preserving regularisers: one preserves the topology of similarities between instances, and the other preserves the graph geometry of similarities across spatial and temporal dimensions. We conduct experiments on multivariate time series classification, as well as macroscopic and microscopic traffic prediction. For all three tasks, our method preserves the structures of similarity relations more effectively and improves state-of-the-art task performances. This extension can be applied to an arbitrary encoder and is particularly beneficial for time series with spatial or geographical features. Our code is attached as supplementary material, which will be made openly available with all resulting data after review.",
    "keywords": [
        "time series",
        "contrastive learning",
        "spatio-temporal data",
        "traffic interaction"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "Preserving certain structures of similarity relations in spatio-temporal data can improve downstream task performance via contrastive learning.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=sz7HdeVVHo",
    "pdf_link": "https://openreview.net/pdf?id=sz7HdeVVHo",
    "comments": [
        {
            "comment": {
                "value": "Thanks for your time and effort in reviewing our submission. We appreciate the feedback provided, but some parts of the review seem to refer to a different study from ours. We aim here to resolve possible misunderstandings and address relevant concerns.\n\n__1. Potential misalignment in the summary__\n\nThe summary describes a study focused on flow prediction across graphs employing a two-step, data-driven and simulation-based approach. While our study includes flow (macroscopic traffic) prediction as a downstream task, it is not across graphs and is not our focus. Simulation is not mentioned in our paper, nor is a two-step approach. Instead, we propose a self-supervised representation learning approach for spatial time series to improve the performance of downstream tasks such as classification and prediction. Our focus is on dynamically incorporating structure-preserving regularisation in contrastive learning. If there has been any misunderstanding or confusion with another study, we would appreciate your clarification.\n\n__2. Disconnected points in the feedback__\n\nSeveral specific points raised in the review appear to be unrelated to the content of our paper.\n- The emphasis on balancing \"between data-driven approaches and simulation-based methods\". Our study does not use simulation. The balance we seek is between contrastive learning and similarity structure preservation. \n- A novel model \"leveraging a combination of node-level and edge-level features\". Our study is not tailored to graph data or node/edge features. Instead, our approach is for spatial time series data, which can be encoded using graphs but not necessarily.\n- The model \"integrates various features such as topological, geographical, and temporal data to optimize flow predictions\". Our study utilises the topological and graph-geographical features of _the similarity relations between time series samples_. The utilisation is to preserve similarity information in the encoded representations.\n- \"Despite the improvements made, the overall approach does not significantly advance the field of flow prediction\". This is not our aim. Rather, we aim to draw attention to and preliminarily investigate the preservation of sample similarity structure in contrastive learning of spatial time series.\n- \"The authors make contributions to the understanding of flow prediction\". We do not contribute specifically to the field of flow prediction; macroscopic traffic prediction is one of the downstream tasks where our approach helps to enhance performance.\n\n__3. Less connected but useful points in the feedback__\n\nDespite the disconnected points above, we value any insights that help improve this study. Below are the points we find helpful.\n- \"The scope of the datasets used in the experiments is somewhat restricted\", which \"may impact the generalizability of the results\". In this study, we use 30 datasets in total: 28 from the UEA archive for classification, 1 for macroscopic traffic (flow) prediction, and 1 for microscopic traffic (trajectory) prediction. Half of the 28 UEA datasets are spatial time series and the other half are non-spatial. The wide range of data and variety of tasks have helped to demonstrate the applicability of our approach.\n- \"The novelty of this paper primarily lies in its enhancements to pre-existing methodologies\", \"lacking the introduction of new paradigms or theoretical frameworks\". There are two novelties in our study: one is adapting structure-preserving representation learning from images to time series, and the other is proposing a dynamic weighing method to combine structure-preserving regularisation and contrastive learning. The former builds on existing methodologies, while the latter is original and useful. We recognise that this study's novelty was not explicitly stated in the manuscript, and with this rebuttal we can make it clearer.\n\n__4. Clarification on the core contribution__\n\nFinally, we would like to clarify our core contribution in this study. We introduce an approach that adds structure-preserving regularizers in contrastive learning of time series. By dynamically integrating topology-preserving or graph-geometry-preserving regularisation, our approach can maintain finer-grained similarity relations in the latent space of sample representations, which improves state-of-the-art performance in a broad range of downstream tasks. This approach can be applied to an arbitrary encoder via pretraining, and is particularly suitable for time series data with spatial or geographical characteristics, such as in robotics, meteorology, remote sensing, urban planning, etc.\n\n__We hope this response clarifies our objectives and contribution, and we respectfully invite any further feedback.__"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a time series contrastive learning loss with topology-preserving or graph-geometry preserving regularization. I appreciate the sufficient experiment made by the authors, but the novelty and originality of this paper are unclear."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed method can enhance SOTA performance on various datasets.\n- The writing is good, and the paper is easy to follow."
            },
            "weaknesses": {
                "value": "- About novelty. The idea of measuring topology similarity or preserving graph geometry has been extensively studied in existing literatures. What is the key insight of this paper that is different from existing works?\n- About originality. As described in Sec3.2 and Sec3.3, the proposed method adopts many existing technics, including TS2Vec loss, SoftCLT loss and topology-preserving loss. What is the origin idea or content of this work?"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper extends time series contrastive learning by incorporating two structure-preserving regularisers: one preserves the topology of similarities between instances, and the other preserves the graph geometry of similarities across spatial and temporal dimensions. And, the proposed method preserves the structures of similarity relations more effectively and improves state-of-the-art task performances for all three tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The topic of time series analysis is important to the ICLR community.\n\n2. The proposed Structure-preserving contrastive learning is novel, which can enhance model performance and generalisability in downstream tasks.\n\n3. The presentation is good and the experimental evaluation is adequate."
            },
            "weaknesses": {
                "value": "1. In terms of experimental evaluation, this paper does not analyze the efficiency of the proposed method, making the evaluation of the model incomplete.\n\n2. In the experimental part, the ablation experiment of key modules in the model is not carried out, which makes the effectiveness of the designed module difficult to be verified.\n\n3. In traffic prediction evaluation, some important baseline models in the field of traffic prediction were not used, making the performance comparison experiments less convincing."
            },
            "questions": {
                "value": "1. The UEA datasets this paper used omit the two largest, InsectWingbeat and PenDigits, due to limited computation resources. Does this mean that the proposed method has limitations when dealing with large datasets?\n\n2. See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper aims to improve contrastive learning for spatiotemporal data. Building upon existing time series self-supervised representation learning methods, namely TS2Vec and SoftCLT, the paper adds two structure-preserving regularisers to the pre-training process. The topology-preserving loss \n$\\mathcal{L}\\_{topo}$\nis borrowed from Topological Autoencoders by Moor et al., 2020, while the graph-geometry-preserving loss \n$\\mathcal{L}\\_{GGeo}$\nis borrowed from Graph Geometry-Preserving Autoencoders by Lim et al., 2024.\n\nThe paper then applies six combinations of pre-training losses on the UEA datasets by Bagnall et al., 2018 and two traffic prediction datasets. In all datasets, it is evident that adding structure-preserving regularizers improves prediction performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The question that the paper is asking is very important. Applying self-supervised representation learning techniques to multivariate time series is relatively unexplored. Many previous works focus on improving encoder architecture, and the techniques introduced in this paper are general and applicable to all deep learning models in this field.\n\nThe paper brings concepts from autoencoder regularization techniques to time series contrastive learning, forcing the distance of samples in the input space and latent space to be similar. This bridge of ideas is creative and original. The paper also introduces the background information very well, allowing the readers to fully understand the motivation behind these regularizers and how they can be used in spatiotemporal data. The overall framework is intuitive.\n\nThe paper also runs extensive experiments on two datasets, comparing the effect of using only contrastive losses and using them with the structure-preserving regularizers."
            },
            "weaknesses": {
                "value": "* **Experimental design**: \n   * The paper lacks the \"No Pre-training\" baseline on the UEA datasets (this setting is included in the traffic prediction datasets). The baseline method should use only the vanilla models (i.e. models without constrastive loss or regularizers).\n   * The paper does not include the training configurations (e.g. hardware specs, the GPUs/CPUs used during training, the training time, and the training/validation/test data splits ratio\n    * The paper does not discuss if contrastive pre-training decreases overall training time, which is important in practice. To address this, the paper could include a plot with training time as the x-axis, test accuracy on the y-axis, and 2 curves showing 1. finetuning from scratch 2. finetuning from pre-training. For instance, see Figure 5. in Masked Autoencoders As Spatiotemporal Learners by Feichtenhofer et al., 2022.\n    * Since the paper is examining the effect of contrastive learning and structure regularizers in the latent space, more baseline models should be tested in combination such as DCRNN, STGCN, Graph WaveNet, etc. These models are important spatiotemporal forecasting models that can provide more evidence for the effectiveness of the method.\n\n* **Writing clarity**:\n    * The term $r\\_{\\eta}$ is only explained to \"regularisation against overfitting of the dynamic weights\", but is not further mentioned in the methods section or hyperparameter search. \n    * It is difficult to understand what the bold and underlining mean in Tables 5, 6, 7. Perhaps adding a detailed caption for the tables helps.\n    * There is no explanation on why \"No-Pretraining\" has the best structure preserving metrics on traffic prediction. This is counter-intuitive since the baseline does not force the distance between samples in the original and latent space to be similar.\n    * In Table 1, maybe explain the abbreviations of batch size and learning rate in the caption or footnote.\n    * Lines 468-472 are hard to understand without giving specific measures for improvement e.g. using GGeo + TS2Vec improved performance by x%. It is impossible to know in \"These methods are also those showing sub-optimal performances in prediction\", what \"These methods\" are specifically. The paper should provide rephrase the paragraph by giving specific improvement examples and analyzing the results in more detail. \n\n* **Typos**\n    * Line 84-85 SOTA abbreviation should be in the introduction at the first mention\n    * Line 423-424 whereas it does when together with preserving similarity structure >> whereas it does when used together with preserving similarity structure"
            },
            "questions": {
                "value": "* What are the details of the model used for fine-tuning, which model is used for traffic prediction?\n* For the evaluation metrics (e.g. dRMSE), how are the distances measured in datasets with spatial features vs. without spatial features? Do you account for the edge distances between connected nodes? Are they the same when calculating the structure regularizer losses (e.g. $\\boldsymbol{A}$ in equation 7)?\n* The graph structures in many of these datasets are static, i.e. the graph doesn\u2019t change over time, is it still appropriate to use structure-preserving regularizers from these graph autoencoders?\n* What do the colors of the nodes in Figure A1. represent? Are they the classes of the samples? Are the latent representations obtained during pre-training or after fine-tuning?\n* The paper leaves many questions unanswered. The goal here is to examine the effectiveness of structure-preserving regularizers in spatiotemporal data contrastive learning, yet the paper only shows prediction improvements without explaining why one regularize works better than the other. For example, if $\\mathcal{L}\\_{topo}$ performs better than $\\mathcal{L}\\_{GGeo}$ on macroscopic traffic prediction, what does this tell us about the dataset and the loss function? Under what circumstances would $\\mathcal{L}_\\{GGeo}$ do better than $\\mathcal{L}\\_{topo}$? This could be better answered by evaluating them on the same topology dataset such as the Sphere or the Swiss Roll dataset to interpret the results better and understand the self-supervised representations. There is also no explanation on why TS2Vec performs better than SoftCLT in certain scenarios."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This article investigates structure-preserving flow prediction models, emphasizing the balance between data-driven approaches and simulation-based methods. The authors present a novel model that aims to enhance the accuracy of flow predictions by leveraging a combination of node-level and edge-level features. The proposed method is built on existing frameworks but introduces modifications to improve performance in predicting flow across graphs.\nSpecifically, it describes the methodology in detail, outlining how the model integrates various features such as topological, geographical, and temporal data to optimize flow predictions. It employs a two-step approach: first, it analyzes the historical flow data to identify patterns, and second, it applies simulation techniques to predict future flows based on these identified patterns. The theoretical foundation is supported by mathematical formulations that establish the relationships between flow, costs, and node attributes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1 The paper provides a robust theoretical analysis, detailing the mechanics of flow prediction models. It includes mathematical derivations that clarify the relationships between various graph features and flow dynamics.\n2 The proposed model incorporates a unique combination of simulation techniques and feature extraction from node and edge data. This method aims to closely approximate actual flow while maintaining computational efficiency.\n3 The experiments are well-structured, comparing the proposed method against existing approaches. The results demonstrate the effectiveness of the model in various scenarios, showing improvements in prediction accuracy."
            },
            "weaknesses": {
                "value": "1 Despite the improvements made, the overall approach does not significantly advance the field of flow prediction. The methods employed largely build upon established techniques without introducing truly innovative concepts or frameworks.\n2 The scope of the datasets used in the experiments is somewhat restricted. This limitation may impact the generalizability of the results, raising questions about how well the method would perform in diverse real-world scenarios"
            },
            "questions": {
                "value": "The novelty of this paper primarily lies in its enhancements to pre-existing methodologies. While the authors make contributions to the understanding of flow prediction, these modifications do not represent a big enough step forward in the field. Many of the techniques discussed are already prevalent in the literature, lacking the introduction of new paradigms or theoretical frameworks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}