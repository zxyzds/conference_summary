{
    "id": "cnecLUNs6w",
    "title": "Adversarial Robustness of In-Context Learning in Transformers for Linear Regression",
    "abstract": "Transformers have demonstrated remarkable in-context learning capabilities across various domains, including statistical learning tasks. While previous work has shown that transformers can implement common learning algorithms, the adversarial robustness of these learned algorithms remains unexplored. This work investigates the vulnerability of in-context learning in transformers to _hijacking attacks_ focusing on the setting of linear regression tasks. Hijacking attacks are prompt-manipulation attacks in which the adversary's goal is to manipulate the prompt to force the transformer to generate a specific output. We first prove that single-layer linear transformers, known to implement gradient descent in-context, are non-robust and can be manipulated to output arbitrary predictions by perturbing\na single example in the in-context training set. While our experiments show these attacks succeed on linear transformers, we find they do not transfer to more complex transformers with GPT-2 architectures. Nonetheless, we show that these transformers can be hijacked using gradient-based adversarial attacks. We then demonstrate that adversarial training enhances transformers' robustness against hijacking attacks, even when just applied during finetuning.  Additionally, we find that in some settings, adversarial training against a weaker attack model can lead to robustness to a stronger attack model.  Lastly, we find that hijacking attacks against one transformer can only transfer to other transformers when they are small-scale, while attacks against larger transformers do not transfer even against transformers of the same architecture but trained with different random seeds.",
    "keywords": [
        "in-context learning",
        "transformers",
        "hijacking attacks",
        "linear regression",
        "linear transformers",
        "transfer of adversarial attacks"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cnecLUNs6w",
    "pdf_link": "https://openreview.net/pdf?id=cnecLUNs6w",
    "comments": [
        {
            "summary": {
                "value": "This paper explores the adversarial robustness of in-context learning in transformers, focusing on hijacking attacks in linear regression tasks. Single-layer linear transformers are shown to be easily manipulated, while complex models like GPT-2 resist simple attacks but remain vulnerable to gradient-based ones. Adversarial training enhances model robustness, especially during fine-tuning, and hijacking attacks transfer only among smaller models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is overal clear and well-written with sufficient experiments."
            },
            "weaknesses": {
                "value": "1. I find the use of x-attack, y-attack, z-attack to be confusing. The joint attack (z-attack) does not really corespond to a different dimension to attack. It is better to call it data, label, and joint attack, or something similar. \n2. From the motivation level, I am uncertain if hijacking attack is the main concern in the applications of LLM for transformers. These line of work along Garg et al. are used to demonstrate the in-context capability of transformers which is a simplified setting but sufficient. The typical adversarial learning considers the setting where the input is modified only slightly, but the adversary can have large impact on the output. I don't see how this classical analysis can easily transfer to the text domain. How to define the allowable small perturbation. If we naively apply on token-level, people can easily find it. If the text contains number, and we only make small modification on the number, will it actually lead to different output. So, I am not sure if the paper can provide many insights into understanding the hijack attack for LLM. I will raise my score if the author sufficiently addresses this issue.\n3. Since the authors mainly investigate the in-context learning setup, they should consider some context-specific attacks, for example within context or across-context. I don't think uniform attack on all possible entries are the only interesting scenario.\n4. I appreciate the empirical effort by the authors. In terms of novelty, I would like to see more novel algorithm design and potential implications, since incontext is still a different setting from the standard ML setting. \n5. I find the experiment section to be hard to follow. Even though the model choice is discussed in the setup section, it is unclear what architecture and training procedure that each figure corresponds to e.g. figure 2.3.4."
            },
            "questions": {
                "value": "1. Can the author provide why the adversarial examples fail to transfer? It is possible that the attack examples cannot be transferred from linear attention model to GPT2, but it can transfer from GPT2-small to GPT2-large. \n2. Has the author experiments with larger model than GPT2, such as llama 8B? I am wondering if the hijack attacks can be mitigated by the emergent ability of transfoermers. \n3. Did the author experiements with TRADES, which usually serves as a better defense than the original PGD."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper shows that linear transformers trained on linear regression aren't robust to hijacking attacks. Moreover, the authors show that these attacks don't transfer from small models to more complex GPT2-style ones (and even among them). The latter can be hijacked with gradient-based optimization. Finally, adversarial training (or fine-tuning) is shown to be promising to prevent these attacks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Methods and experiments are clearly explained and carried out. The work is original, but the scope could have been better stated.\n\n- Results are well presented and commented on. However, they're not very significant.\n\n- I personally like the part about adversarial training, as it is a promising method to make transformers more robust to attacks."
            },
            "weaknesses": {
                "value": "- I don't get the point of this work and why it has to be considered relevant for the community. I would say this better, possibly on the first page.\n\n- I would have tried to inspect better why attacks don't work on GPT2 style models. This would have been helpful in understanding better how these models perform linear regression."
            },
            "questions": {
                "value": "1. What's the meaning of these attacks in the context of linear regression? Is there a more practical interpretation?\n\n2. How do these results compare to when using OLS? Can the latter be used as a baseline?\n\n3. What does adversarial training on attacks mean from a linear regression perspective? (eg. can it be related to some form of regularization?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on studying the adversarial robustness of in-context learning for transformers. The paper first proves that a single-layer transformer can be manipulated to output arbitrary predictions, and also uses gradient-based attacks to demonstrate the vulnerability of GPT-2."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper has a solid theoretical analysis to show the vulnerability of single-layer transformers.\n\n2. Beyond single-layer transformers, the paper also considers GPT-2, and the experimental results appear to support the conclusions well.\n\n3. The paper is easy to follow and well-organized."
            },
            "weaknesses": {
                "value": "1. The novelty of this paper is limited because the phenomenon that single-layer transformers and GPT-2 are vulnerable to adversarial attacks is not surprising, and the paper does not have new and strong technical contribution compared with existing gradient-based attacks.\n\n2. The task that uses GPT-2 for linear regression is not well-motivated. We usually do not use GPT-2 to solve linear regression.\n\n3. The paper does not propose any new defense method beyond adversarial training, which is a standard baseline."
            },
            "questions": {
                "value": "No question, and please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}