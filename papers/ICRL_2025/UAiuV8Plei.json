{
    "id": "UAiuV8Plei",
    "title": "FBSVP: Video Prediction Based on Foreground-Background Separation",
    "abstract": "Video prediction is the process of learning necessary information from historical frames to predict future video frames. \nHow to focus and efficiently learn features from historical frames is a critical step in this process. For any sequence of video frames, \nthe background changes little or remains almost constant, while the foreground changes significantly and is the main focus of our video prediction learning. \nHowever, current known video prediction learning methods do not consider how to utilize the different characteristics of the foreground and background to further improve prediction accuracy. \nTo fully leverage the different characteristics of the foreground and background and enhance prediction accuracy, \nwe propose a Foreground-Background Separation Video Prediction (FBSVP) model in this paper. \nThrough the foreground and background separation module, historical video frames are separated into foreground and background frames. \nIn the video prediction module, the foreground and background frames are predicted and learned separately. \nFirst, the features of historical frames are fused into the current frame through a historical attention fusion module using an attention mechanism. \nThen, the complementary temporal and spatial features are fused through a spatio-temporal fusion module. \nFinally, the learned foreground and background features are fused in the foreground and background fusion module to predict the final video frame. \nExperimental results show that our proposed FBSVP model achieves the best performance on popular video prediction datasets, demonstrating its significant competitiveness in this field.",
    "keywords": [
        "Video Prediction",
        "Foreground-Background Separation"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-21",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=UAiuV8Plei",
    "pdf_link": "https://openreview.net/pdf?id=UAiuV8Plei",
    "comments": [
        {
            "summary": {
                "value": "The authors propose a  foreground-background separation video prediction. The proposed method named FBSVP consists of modules. First, an historical attention fusion module employs an attention mechanism. Second, temporal and spatial features are fused through a spatio-temporal fusion module. Third,a  foreground-background fusion module allows the authors to estimate the final video frame."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1) The paper is well organized and relatively well written.\n2) The proposed method FBSVP is detailed and reproducible.\n3) Experiments are conducted on various datasets such as KITTI and Caltech Pedestrian showing the superiority of FBSVP compared to ten previous methods."
            },
            "weaknesses": {
                "value": "1) There are missing references about key surveys in the field of background/foreground separation for novices.\n\nM. Cristani, et al., \u201cBackground Subtraction for Automated Multisensor Surveillance: A Comprehensive Review\u201d, EURASIP Journal on Advances in Signal Processing, 24 pages, Volume 2010, 2010.\n\nB. Garcia-Garcia, et al., \"Background Subtraction in Real Applications: Challenges, Current Models and Future Directions\",  Computer Science Review, Volume 35, February 2020.\n\n2) All over the paper, the authors cite two times as in the sentence \"Wang et al. Wang et al. (2017) argued...\". Please only let the second citation."
            },
            "questions": {
                "value": "It would be interesting to test the proposed method on the CDnet 2014 dataset. Indeed, it is a large-scale dataset used in MOD and it would be interesting if the method can help in this context. If yes, the authors have just to mention it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a video prediction model from the perspective of foreground and background separation. This practice may be novel but the motivation is not well related to the solution in this paper. The proposed solution is not novel in my view. The main reason for my rejection rating is the current version may be unfinished and needs major revisions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Divide the video prediction goal into foreground and background parts and process them separately before aggregation."
            },
            "weaknesses": {
                "value": "1. The presentation of the entire paper needs to be carefully improved. For example: \na) The latex citet and citep commands should be used properly.\nb) Fig 1 is too small.\nc) Figures 3, 4 and 5 express essentially the same process and should be illustrated with a clearer diagram. \nd) The visual comparisons are basically not distinguishable, especially in Figure 7 and 8.\ne) Tables 1 to 4 are arranged in an order that does not match the content.\nf) Writing and Grammar Issues.\n\n2. The motivation and solution of the paper are not well related and do not clearly illustrate the differences with existing video prediction methods. From my point of view, the contribution is also insufficient and the proposed model components are not novel."
            },
            "questions": {
                "value": "1. Fonts differ from the official template.\n2. In Table 2, we can find that compared to SimVPv2 the SSIM is better yet the PSNR is worse, which is an interesting observation that is worth analyzing.\n3. Why the LPIPS metric is not used in the comparison experiments, but only in the ablation experiments.\n4. Why is there no quantitative and qualitative comparison of training on the KITTI training set and testing on the KITTI test set? It has been done in existing methods including [1].\n\n[1] A Dynamic Multi-Scale Voxel Flow Network for Video Prediction, CVPR 2023"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "A proper Foreground-Background Separation Video Prediction (FBSVP) model, has been proposed\nin this paper, to enhance prediction accuracy in video prediction.\n\nThe features of historical (assumed to be past) frames are fused into the current frame through a historical attention fusion module using an attention mechanism. \n\nThen, the complementary temporal and spatial features are fused through a spatio-temporal fusion module. Finally, the learned foreground and background features are fused in the\nF/G-B/G fusion module to predict the final frame.\nExperimental results shows that the proposed FBSVP model achieves the\nbest performance on few benchmark video prediction datasets, demonstrating its\npower."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Good set of results on benchmark datasets; beats few SoA processes\nproper analytics given for the process.\n\nAblation studies also provided.\n\nMethod clearly explained."
            },
            "weaknesses": {
                "value": "State clearly the difference between the operations:\nhistorical attention fusion\nvs\nSpatio-temporal fusion.\n\nDifference between the terms information and feature (T & S) should be clarified - unless used interchangeably (specify so then).\n\nWhat about the use of any probabilistic model in prediction, future may be uncertain (although GT data may provide unique answer) ?\n\nFig. 1- text inside processing blocks barely legible\n\nRightmost column of Fig. 7 - is mostly dark - not sure, what the authors want to show to readers.\n\nOnly for Kitti D/S the number of test frames is larger than training 10 -> 40, well not actually so, as I am pointing out to.\n\nThe real challenge lies in predicting:\n(i)  the number of predicted frames is large enough (longer duration), compared to history/past;\n(ii) HR frames from LR ones - did not find any such results\n\n(i) is partly addressed, although better to show results on say:\ntrain: 10 --> 20\nand then\ntest 10--> 40 or 80/120 - what happens then.?\n\n\n\n \n\nF/G-B/G separation is well-studied topic with vast literature, which is often difficult to cover in a Conf. paper.\nHowever,\na few relevant papers which have not been cited, or performance not compared, are given below:\n\nY. Zhao, D. Luo, F. Wang, H. Gao, M. Ye and C. Zhu, \"End-To-End Compression for Surveillance Video With Unsupervised Foreground-Background Separation,\" in IEEE Transactions on Broadcasting, vol. 69, no. 4, pp. 966-978, Dec. 2023, doi: 10.1109/TBC.2023.3280039.\n\nMotion-aware Contrastive Video Representation Learning via Foreground-background Merging; Shuangrui Ding et. al; CVPR \u2013 2022\n\nCollaborative Video Object Segmentation by Foreground-Background Integration; Zongxin Yang, Yunchao Wei, Yi Yang; ECCV 2020; https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3385_ECCV_2020_paper.php\n(although a later T-PAMI paper, has been cited).\n\nRevisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization: A Clustering-based Approach; Qinying Liu, Zilei Wang, Shenghai Rong, Junjie Li, Yixin Zhang; Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 10433-10443\n\nZBS: Zero-Shot Background Subtraction via Instance-Level Background Modeling and Foreground Selection; Yongqi An, Xu Zhao, Tao Yu, Haiyun Guo, Chaoyang Zhao, Ming Tang, Jinqiao Wang; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 6355-6364\n\nMotion-Aware Contrastive Video Representation Learning via Foreground-Background Merging; Shuangrui Ding, Maomao Li, Tianyu Yang, Rui Qian, Haohang Xu, Qingyi Chen, Jue Wang, Hongkai Xiong; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 9716-9726\n\nMitigating and Evaluating Static Bias of Action Representations in the Background and the Foreground;  Haoxin Li, Yuan Liu, Hanwang Zhang, Boyang Li; Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 19911-23.\n\nA Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes;  Mazda Moayeri, Phillip Pope, Yogesh Balaji, Soheil Feizi; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 19087-19097"
            },
            "questions": {
                "value": "The term \"historical attention fusion\" needs more clarification.\nAnyway, one needs to obviously learn/gather information features only from past frames, in any problem of video analytics.\nSo what is the reason of explicitly stating that using this term ? - unless there is significant meaning of something else - \nsay, state change in past in latent space, say.\n\nLine 168  vs 182 - ...subscript \"s\" denotes parameters related to spatial features...\nvs\nM^st for temporal features, where \"s' is a superscript.\n\nthe SUM function - limits/range of indices (even if obvious) not specified in Eqns. 1 & 2.\n\nEqns. (4-5)  -  sigmoid of a feature vector (as argument to the function) ? - something missing there. Pl. clarify"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a video prediction method that trains by separating foreground and background. The method uses MOG2 to separate foreground from background and then predicts future frames by fusing historical frame information into the current frame. The authors validated the algorithm on datasets such as Moving MNIST and TrafficBJ."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors achieved some performance improvement on multiple datasets, such as TrafficBJ and KTH.\n2. The authors conducted both qualitative and quantitative analyses of the proposed method's performance, producing some reasonably good visual results."
            },
            "weaknesses": {
                "value": "1. The proposed idea of training by separating foreground and background has certain limitations. It is based on the assumption that backgrounds change infrequently while foregrounds change frequently (see Lines 014\u2013016), which holds mainly for simple scenes, such as those in datasets like Moving MNIST and KTH used in this paper. In complex scenes, however, both the background and foreground can undergo significant changes, and foreground objects can vary greatly in size and spatial position within the frame (e.g., appearing larger when close, smaller when far). For scenes with complex backgrounds, foreground-background separation itself becomes challenging, and performing video prediction on separately processed foreground and background could be difficult. It would be helpful if the authors could analyze the applicability of this method in complex scenes or test on more challenging datasets such as UCF101 and Kinetics400.\n2. Video prediction can be considered a subtask of mainstream video generation, where the goal is to condition on the first few frames of a video to generate future content. The methods compared by the authors are all based on supervised video prediction, lacking comparisons with mainstream video generation methods like Stable Video Diffusion, OpenSora, and VideoCrafter. Current mainstream video generation methods tend to generalize better in handling complex scenes, with a broader applicability than the supervised methods commonly used in video prediction. It would be beneficial if the authors could compare their approach with mainstream video generation models on datasets representing more general and complex scenes.\n3. Miscellaneous: Figure 1 is too small, and the text is difficult to read. It would be better to use a larger image with adjusted font size for clarity."
            },
            "questions": {
                "value": "1. The core selling point of this paper is the prediction of video content by separately processing foreground and background. If the foreground-background separation algorithm is inaccurate, how does this affect the final result? Have the authors conducted any quantitative or qualitative analysis on the performance of the foreground-background segmentation algorithm and its impact on subsequent video prediction? For example, the authors could perform an ablation study using different foreground-background separation algorithms of varying accuracy.\n2. I noticed that the captions for Table 1 and Table 2 indicate that video frame prediction is performed by conditioning on 10 frames to predict 10 frames (Table 1) or conditioning on 10 frames to predict 1 frame (Table 2). Have the authors tested results under other settings, such as conditioning on *n* frames to predict *m* frames, with various combinations of *n* and *m*? Testing with different combinations might provide a more comprehensive reflection of the method's effectiveness and robustness in video prediction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}