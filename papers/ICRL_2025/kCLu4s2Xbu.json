{
    "id": "kCLu4s2Xbu",
    "title": "Trajectory-level Data Generation with Better Alignment for Offline Imitation Learning",
    "abstract": "Offline reinforcement learning (RL) relies heavily on densely precise reward signals, which are labor-intensive and challenging to obtain in many real-world scenarios. To tackle this challenge, offline imitation learning (IL) extracts optimal policies from expert demonstrations and datasets without reward labels. However, the scarcity of expert data and the abundance of suboptimal trajectories within the dataset impede the application of supervised learning methods like behavior cloning (BC). While previous research has focused on learning importance weights for BC or reward functions to integrate with offline RL algorithms, these approaches often result in suboptimal policy performance due to training instabilities and inaccuracies in learned weights or rewards. To address this problem, we introduce Trajectory-level Data Generation with Better Alignment (TDGBA), an algorithm that leverages alignment measures between unlabeled trajectories and expert demonstrations to guide a diffusion model in generating highly aligned trajectories. With these trajectories, BC can be directly applied to extract optimal polices without the need for weight or reward learning. Moreover, to ensure high fidelity and diversity in the generated trajectories and to make the learning more stable, the implicit expert preference that can fully exploit the unlabeled data is employed in the training of the diffusion model. Experimental results on the D4RL benchmarks demonstrate that TDGBA significantly outperforms state-of-the-art offline IL methods. Additionally, the analysis of the generated trajectories shows the effectiveness of incorporating the diffusion model and implicit expert preference for trajectory-level data generation.",
    "keywords": [
        "offline imitation learning",
        "behavior cloning",
        "trajectory-level data generation",
        "high-alignment trajectories"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kCLu4s2Xbu",
    "pdf_link": "https://openreview.net/pdf?id=kCLu4s2Xbu",
    "comments": [
        {
            "title": {
                "value": "Official Comment by Authors (2)"
            },
            "comment": {
                "value": "**[Writing]**\n\n**We would like to clarify that our primary goal in writing is to convey our ideas with clarity and precision.** In response to some of the concerns you raised regarding the writing, we have provided a detailed explanation of our overall writing approach.\n\n[Introduction & Background]  \nThese two sections mainly focus on various aspects related to the TDGBA method. Specifically, Section 3.2 presents a description of the foundational background knowledge on diffusion models, which typically shares some similarities with other papers discussing diffusion like [1]. In contrast to FTB, we provide separate discussions on offline imitation learning and Wasserstein distance in the context of other background knowledge, all of which are developed based on the TDGBA framework.\n\n[Experiment]  \nIn the experimental section, we compared nearly all recent offline imitation learning methods, and we made two separate chapters comparing BC-based and reward-based methods. We list the baseline methods for comparison between TDGBA and FTB here:  \n\u25cf TDGBA: (BC, %10BC, SQIL, DEMODIC, SMODICE, LobsDICE)  \n\u25cf FTB: (%10BC, TD3+BC, IQL, OPRL, PT, IPL)\n\n**It is evident that our choice of baselines is quite different.**  \nIn Section 4.2, we also visualize the correlation between the implicit expert preferences we defined and the ground-truth returns, **further demonstrating that we can achieve results similar to those in FTB without using human preferences**. Additionally, we plotted the distribution differences between the rewards learned by various reward-based methods and the true rewards, which, in comparison to FTB, **not only compares the true reward distribution but also clearly shows the biases in reward estimation by reward-based methods.**\n\nIn the analysis of generated trajectories (Section 4.3), we compared TDGBA\u2019s performance across various aspects using both synthetic data under unconditional preferences and original data. **We also further improved the experiment demonstrating diversity. While FTB only used a standard Gaussian distribution to show diversity, we believe this method is incomplete, as increasing Gaussian noise also increases diversity. Therefore, we switched to scatter plots combined with dynamic error under different Gaussian noise enhancements to better demonstrate this property.**\n\n[Related Work]  \nIn this section, we first discussed the related works on offline imitation learning. In the application of generative model in RL, **apart from the classical works such as Decision Diffuser[1], we also supplemented with works on diffusion-QL[2], diffusion policy[3], MTdiff[4] and included FTB. We believe these works are necessary to present in the paper.**\n\n[Conclution & Limitations & Future Work]  \nIn this part, we further clarified the unique limitations of TDGBA and explicitly stated that it may not be suitable for handling multimodal expert trajectories.\n\nIn summary, in the design of the writing structure, we closely followed the presentation of TDGBA and included all the necessary related content. **We will make every effort to revise the areas you pointed out. Our primary goal is to avoid any potential concerns or misunderstandings.**\n\n**Finally, we would like to once again express our sincere gratitude to the reviewer for the time and effort invested in reviewing our work and rebuttal.  Most importantly, while we acknowledge that our work may not be fully mature or perfect and that there may be some areas with shortcomings, we want to emphasize that we have no intention of plagiarism. It would be unreasonable to label our efforts as plagiarism. We kindly ask you to reconsider our work in light of the points we have raised above. We will carefully revise and refine the work based on your valuable feedback.** We look forward to your response, as it is very important to us. \n\nThank you.\n\n\n\n[Reference]\n\n[1] Is conditional generative modeling all you need for decision-making. ICLR, 2023.\n\n[2] Diffusion policies as an expressive policy class for offline reinforcement learning. ICLR, 2023.\n\n[3] Diffusion policy: Visuomotor policy learning via action diffusion. RSS, 2023.\n\n[4] Diffusion model is an effective planner and data synthesizer for multi-task reinforcement learning. NeurIPS, 2023.\n\n[5] Flow to better: Offline preference-based reinforcement learning via preferred trajectory generation. ICLR, 2024."
            }
        },
        {
            "title": {
                "value": "Official Comment by Authors (1)"
            },
            "comment": {
                "value": "Dear reviewer:\n\nWe sincerely thank the reviewer for the time and effort in reviewing our work. We fully understand and respect your role as a reviewer in ensuring the quality of the work, but **we must clarify that we totally disagree with the conclusion regarding plagiarism.** **We assure that we have never had any intention of plagiarizing other works. We will make every effort to provide a detailed explanation to address your concerns, as this is crucial for both our current work and future endeavors.**\n\n**[Method]**\n\nFirst, we would like to provide a clearer explanation of the original motivation behind the proposed TDGBA method. As you noted, TDGBA is applied to the field of offline imitation learning, where **a key challenge is the lack of labeled data to distinguish good from bad trajectories, with only a small number of expert demonstrations, or even a single expert trajectory, being provided.** Through our extensive review of the literature, we found that previous works have mainly focused on inverse reinforcement learning or methods from the DICE family. However, these methods are often limited by the finite state space in the training set or inaccurate reward estimates. As a result, when encountering unseen states during inference, they tend to suffer from compound errors in the policy. **In response to this issue, we aim to address the problem from the perspective of data optimization and augmentation.**\n\n**To the best of our knowledge, there has been no prior work in offline imitation learning that directly tackles this challenge using data generation.** The primary reason for this gap is the scarcity of expert demonstrations and the difficulty in utilizing large volumes of unlabeled data. **We are grateful for the inspiration provided by the FTB method from offline preference-based RL,  particularly its use of human-preference learning in diffusion model to optimize trajectories.** However, the FTB approach requires an additional human preference dataset to train a task-specific score model, which is used to label each trajectory. We believe that collecting such human preference datasets could present several challenges: 1) the differences between preferences should not be too large; 2) individuals may have varying preferences for the same trajectories; 3) as task complexity increases or the collection of human preferences becomes more difficult, the accuracy of the score model may decrease, potentially impacting the quality of subsequent labeling.\n\n**Therefore, under the setting of offline imitation learning where only a limited number of expert trajectories are available, a key aspect of our method is to find an effective metric to label different trajectories.** This allows us to construct a form of preference that does not rely on human input, which we refer to as \"implict expert preference\" in our work. We believe that by applying the concept of preference learning in this context\u2014without relying on human preference datasets\u2014we can effectively tackle several challenges inherent in offline imitation learning. **This is a central focus of our work and represents a promising direction for future research.**\n\nWe would like to assure you that we did not attempt to plagiarize the FTB work. We have cited it in several other places throughout our article, including:\n\n+ \"Notably, while our approach shares similarities with FTB in utilizing diffusion models, we differ by not requiring additional human preference datasets or task-specific score models. This reduces model bias and complexity, offering a more efficient method for inferring implicit expert preferences, making it better suited for real-world applications.\" (Section 5: 510\u2013515)\n+ \"On the other hand, FTB (Zhang et al., 2023) utilizes an additional human preference dataset to train a task-specific score model, which calculates preferences for unlabeled trajectories. Yet, collecting extensive human preference data is labor-intensive and even challenging.\" (Section B.1: 770\u2013772)\n+ \"Recently, some research has compared the ground-truth returns or human preferences of different trajectories to train a generative model that can improve inferior trajectories into high-quality ones, effectively enhancing the quality of the training data. (Liu & Abbeel, 2023; Zhang et al., 2023).\" (Section 1: 062\u2013065)\n\n**We will carefully review and revise to address any potential issues and ensure appropriateness, including adding a dedicated section to discuss the similarities and differences between FTB and our approach.**"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method called Trajectory-level Data Generation with Better Alignment (TDGBA)  for offline imitation learning (IL). TDGBA leverages alignment measures between unlabeled trajectories and expert demonstrations to guide a diffusion model in generating highly aligned trajectories, which are then used for better behavior cloning. Experimental results on the D4RL demonstrate that TDGBA outperforms SOTA IL methods."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "N/A"
            },
            "weaknesses": {
                "value": "I have sufficient reason to believe that this paper has plagiarized another paper [1] (called flow-to-better (FTB)) in both its method and writing. I will explain this in detail:\n\n**Method:**\n\nThe method proposed in this paper, TDGBA, first uses an alignment measurement method to score the trajectories in the dataset, then applies a clustering method to divide all trajectories into several blocks. Next, two trajectories from two neighboring blocks are sampled, respectively labeled as high-alignment and low-alignment trajectories, and these are provided to a diffusion model for learning. Finally, the learned diffusion model is used to generate several high-alignment trajectories as augmented data for imitation learning. This entire process is identical to the FTB method, except that in FTB, human preferences are used as the measurement in the first step, whereas TDGBA proposes its own alignment measurement. However, the paper completely avoids discussing these similarities, only mentioning \u201cDrawing inspiration from previous works...\u201d in Section 3.2.\n\n**Writing:**\n\nThe overall structure of the paper is also highly similar to the FTB paper. For example, Section 3.2 is very similar to Section 3.1 in FTB. The experimental sections 4.2 and 4.3 are also similar to sections 4.2 and 4.3 in FTB, and the discussion of generative models in the related work section also closely resembles that of FTB.\n\nIn summary, while TDGBA and FTB are methods proposed in different fields\u2014one in offline imitation learning and the other in offline preference-based RL\u2014their method structure, details, and even parts of the writing are very similar. Therefore, I have reason to suspect that this paper may have plagiarized from FTB.\n\n[1] Zhang et al. \"Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation\" (ICLR'24)."
            },
            "questions": {
                "value": "I have no further questions because this paper is suspected of plagiarism."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel offline imitation learning method, Trajectory-level Data Generation with Better Alignment (TDGBA), designed to optimize policy learning when both expert and unlabeled datasets are available. TDGBA leverages the Wasserstein Distance to measure alignment between unlabeled and expert trajectories, constructing preference pairs from the initially unlabeled dataset. A trajectory diffusion model is then trained to generate higher-aligned trajectories conditioned on lower-aligned ones, progressively enhancing the top-aligned trajectories. The behavior cloning (BC) policy trained on this improved dataset demonstrates significant improvement over baseline algorithms, particularly in mixed-quality datasets such as medium-expert and medium-replay."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. TDGBA uses implicit preference labeling to guide trajectory generation, enhancing the quality of generated trajectories without requiring additional human labels or preference datasets.\n\n2. TDGBA avoids the complexities and computational costs associated with reward learning in offline settings, simplifying the imitation learning process.\n\n3. Experimental results show that TDGBA outperforms state-of-the-art offline IL methods, especially in mixed-quality datasets like medium-expert and medium-replay."
            },
            "weaknesses": {
                "value": "1. Some critical algorithm designs lack sufficient and reasonable explanations. 1) Why are low-preference trajectories used as conditions of the classifier-free guidance, and how about directly using labeled preference values as generation conditions? 2) According to Algorithm 1, the expert demonstrations were not used as training data for BC policy. What is the reason for this choice?\n\n2. Some important technical details are missing on the paper. As the authors did not provide their code implementation, reproducing the results could be challenging. Specifically, what is the length of the trajectories generated by the diffusion model? If it corresponds to the length of an episode, e.g., 1000 steps in MuJoCo, would this make the model\u2019s inference cost very high? Also, the authors did not mention the model architecture of the diffusion model. Even though the downsample rate in the parameter table suggests it might be a U-Net, the authors did not explicitly clarify this in the paper.\n\n3. (minor issues) In Table 6, the \"Top trajectories number\" is reported as 300, but in Table 7, it varies by environment. Line 47 is missing a space before the left parenthesis, line 197 is missing a closing parenthesis, \u201cconfirme\u201d on line 268 should be corrected to \"confirm\", and a space is missing after the closing parenthesis on line 318."
            },
            "questions": {
                "value": "1. In line 3 of Algorithm 1, how did the authors select the expert demonstration trajectory to pair with each unlabeled trajectory? If the selection was random, could this pairing approach create issues in environments where the initial state varies significantly, as the optimal trajectory distribution may differ widely depending on the initial state?\n\n2. Since the diffusion model is applied solely to improve the top trajectories, is it necessary to train on all neighbor blocks? For instance, training the model to generate $B_2$ from $B_1$ might be unnecessary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Offline reinforcement learning depends on precise reward signals, which are difficult to obtain. Offline imitation learning (IL) seeks to develop policies from expert demonstrations without rewards, but is limited by scarce expert data and numerous suboptimal trajectories, affecting methods like behavior cloning (BC). Traditional approaches using importance weights or reward functions for BC encounter instability and accuracy issues. To address this, we introduce Trajectory-level Data Generation with Better Alignment (TDGBA). This method aligns unlabeled trajectories with expert demonstrations to guide a diffusion model in generating well-aligned trajectories, enabling BC to extract optimal policies directly. It also uses implicit expert preferences to improve stability, fidelity, and diversity. Experiments on D4RL benchmarks demonstrate TDGBA's superior performance over other offline IL methods, confirming the effectiveness of diffusion models and expert preferences in trajectory data generation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "well-written and clear\n\nwell-motivated\n\nextensive comparisons and evaluations in experiments"
            },
            "weaknesses": {
                "value": "The grammar and presentation of some paragraphs need to be improved.\n\nThe visualization of the experimental results needs to be improved slightly."
            },
            "questions": {
                "value": "line 212, why using lower-alignment trajectories as the condition\uff1f\n\nIn Section 4.3, the visualization results were only presented on the hopper-medium-replay-v2 task, which left the reader somewhat confused and curious. How about the results on other tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a simple yet effective trajectory alignment approach that combines a W1-distance-based trajectory ranking method with the use of a diffusion policy. This approach demonstrates improved performance in the imitation learning (IL) domain, specifically within Gym-Mujoco.\n\nAdditionally, the authors provide extensive analytical experiments and analysis, which illustrate the limitations of previous methods' reward shaping. These experimental results are crucial for understanding the performance of current reward-shaping algorithms.\n\nHowever, the extent to which improvements result from algorithm innovation rather than model architecture still needs further verification."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper proposes a simple yet effective trajectory alignment approach that combines a W1-distance-based trajectory ranking method with a diffusion policy. It demonstrates improved performance in imitation learning (IL) tasks sourced from the Gym-Mujoco domain.\n\nAdditionally, the authors conducted extensive analytical experiments and analyses, which highlight the limitations of previous reward shaping methods. These experimental results are essential in helping us understand the constraints of current reward shaping-based IL algorithms."
            },
            "weaknesses": {
                "value": "See Question Section"
            },
            "questions": {
                "value": "**Q.1** How much improvement is brought by your trajectories ranking and sorted out? since the performance of Diffusion policy is considerate enoughy, especially in the long horizonal setting. Meanwhile, most of your baselines chosen are based on MLP policies.\n\n**Q.2**  From your experimental results, I observe that most of your setups focus on continuous control tasks, while there is a lack of long-horizon decision-making tasks. Could you please provide more comparisons across these related domains (such as kitchen, android, etc.)? Thank you.\n\n**Q.3** Could you clarify the advantages of your paper over other alignment and sequential modeling approaches? For instance, the paper [1] utilizes contextualized information to align demonstrations, showcasing improved performance on IL tasks. I also have a question about your **diffusion policy's optimizing objective**.\n\nSpecifically, we know diffusion policy can be optimized via ODE or SDE based method. ODE objective includes consistency model e.g.  and SDE methods encompass numerous related researches. Most of those methods optimize models via adding noising to the initial feature till uniform distribution followed by recovering the feature or predicting the added noise. Therefore, I am very confused about your diffusion objective. Why its a maximizing likelihood objective?\n\nIf you ignore your objective, I think it's not diffusion policy, I prefer naming it U-NET? Meanwhile, I can provide some method for you to check whether its diffusion policy:\n\n- Render and observe, whether your policy can learn multiple modes. For example, given a state, there are several clusters.\n- check your codebase, and observe, whether you just don't correctly write the objective.\n\nI won't directly reject this paper, since your contribution is independent with diffusion policy. I am looking forward to any improvements or corrections.\n\nReference\n[1] Z. Zhang, J. Xu, J. Liu, Z. Zhuang, D. Wang, M. Liu, S. Zhang, Context-Former: Stitching via Latent Conditioned Sequence Modeling"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "It introduces Trajectory-level Data Generation with Better Alignment (TDGBA), which addresses the challenges of scarce expert data and suboptimal trajectories by using a diffusion model guided by alignment measures between unlabeled trajectories and expert demonstrations. The method employs an alignment metric based on optimal transport theory and Wasserstein distance, which is used as an implicit expert preference to train the diffusion model. This approach generates high-quality, diverse trajectories that enable the direct application of Behavior Cloning methods to derive optimal policies, outperforming state-of-the-art offline IL methods on D4RL benchmarks\u2060."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper introduces a metric using Wasserstein distance to measure the alignment between unlabeled trajectories and expert demonstrations."
            },
            "weaknesses": {
                "value": "The conditional diffusion model conditioned on inferior data to generate better data is derived from previous work [1]. Therefore, it is essentially an adaptation to the offline imitation setting.\n\nThe defined metric has some practical limitations. Perfect demonstrations may be scarce, and the policy might be multi-modal, making it challenging to formally define the distance. However, practical methods to measure differences, such as training a discriminator, have been proposed in previous work [2].\nThus, the contribution of this work appears limited.\n\n[1] Zhang Z, Sun Y, Ye J, et al. Flow to better: Offline preference-based reinforcement learning via preferred trajectory generation.\n\n[2] Wang, Yunke, Chang Xu, and Bo Du. Robust Adversarial Imitation Learning via Adaptively-Selected Demonstrations."
            },
            "questions": {
                "value": "Can you visualize the improved generated data? Given that the original conditional diffusion might rely on a large number of human labels, does the expert-demonstration-shaped preference satisfy the data requirements? Ablation studies on the number of preference labels and trajectories are also needed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}