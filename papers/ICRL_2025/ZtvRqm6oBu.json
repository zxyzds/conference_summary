{
    "id": "ZtvRqm6oBu",
    "title": "Applying Sparse Autoencoders to Unlearn Knowledge in Language Models",
    "abstract": "We investigate whether sparse autoencoders (SAEs) can be used to remove knowledge from language models. We use the biology subset of the Weapons of Mass Destruction Proxy dataset and test on the gemma-2b-it and gemma-2-2b-it language models. We demonstrate that individual interpretable biology-related SAE features can be used to unlearn biology-related knowledge with minimal side-effects.\nOur results suggest that negative scaling of feature activations is necessary and that zero ablating features is ineffective. We find that intervening using multiple SAE features simultaneously can unlearn multiple different topics, but with similar or larger unwanted side-effects than the existing Representation Misdirection for Unlearning technique. Current SAE quality or intervention techniques would need to improve to make SAE-based unlearning comparable to the existing fine-tuning based techniques.",
    "keywords": [
        "mechanistic interpretability",
        "unlearning",
        "ai safety",
        "interpretability"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We investigate whether sparse autoencoders can be used to unlearn bioweapon-related knowledge in language models and compare performance to an existing fine-tuning based approach.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ZtvRqm6oBu",
    "pdf_link": "https://openreview.net/pdf?id=ZtvRqm6oBu",
    "comments": [
        {
            "summary": {
                "value": "The paper uses sparse autoencoders to identify features related to biology, which they isolate and clamp down to unlearn on a subset of the WMDP dataset. They use two versions of gemma-2b-it, intervening at an intermediate layer on specific bio-related features. They find that such an intervention can be successful in unlearning, while adding relatively small loss on a generic text dataset."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "I think applying SAEs to this task is useful \u2013 for us to do good unlearning we almost certainly want an interpretable method, so these are worthwhile first steps. I like the depth you went into in Section 3, as well as Figure 2. I think the methodology was clearly defined, as well as the metrics and tasks you were evaluating. I think there are some nice ablations as well, e.g., Section 4.2."
            },
            "weaknesses": {
                "value": "I think the messaging of the paper needs to change to increase the novelty by emphasizing how your work and existing work (RMU) differ. Specifically, how yours is more interpretable and why that\u2019s a good thing. I know the latter is mentioned in the intro but that\u2019s the most substantive discussion of this difference, which is the main reason right now people would want to use SAEs for unlearning.\n\nI think the experiments section should include both gemma models. Relatedly, I think Figure 4 is weak and makes the results hard to interpret. I also don\u2019t really know why the added loss would matter much when you show such a relatively large deterioration on MMLU.\n\nI\u2019d prefer you to show at least some results on even one other subset of WMDP just to see how this generalizes. I wonder if SAEs may be more helpful than fine-tuning if we are trying to unlearn a combination of separate tasks, e.g., both biology and chemistry.\n\nFinally, there is a lack of a related work section, which I feel is necessary as I think you could better contextualize your, e.g., by further showing how people are currently using SAEs to adjust features."
            },
            "questions": {
                "value": "-\tWhy did you select the hyperparameters you did for the clamped feature activations (1x, 10x, 50x, etc.)? Same for RMU hyperparameters? Specifically, the layers.\n-\tWhy did you use 300x in Figure 5 but not Figure 4?\n-\tOn page 2, \u201cWe trained SAEs at several intermediate layers of the residual stream\u201d \u2013 which layers? Sometimes you use layer 3, sometimes layer 9.\n-\tWhy do you use gemma-2b-it for Section 3, but don\u2019t present results for gemma-2b-it in the main body? There is a disconnect between this and Section 4, which uses gemma-2-2b-it.\n-\tOn page 5, there are experiments mentioned but I\u2019d like to see the results somewhere: \u201cTo investigate the importance of the particular feature that we selected, we performed the same ablation on a variety of features that activate on this prompt, chosen at random.\u201d\n-\tFigure 1 feels bare. You could consider including some results in half of this figure or some examples on the bio WMDP subset so it is more tailored to your goal instead of a somewhat bare depiction of a SAE.\n\nThese did not affect my review, but some smaller things to note:\n\n-\tCould you use different symbols to represent the feature number and question? You use \u201c#\u201d for both feature number and question number (e.g., \u201cfeature #9163\u201d and \u201cquestion #841\u201d), which I think is distracting.\n-\tIn the conclusion you say intervening using \u201c10-20\u201d features \u2013 I would change it to 10 or 20 because you don\u2019t use any intermediate values in the range.\n-\tOn page 4, \u201cThe model still provides \u201cA\u201d as the correct answer with probably\u201d -> \u201cprobability\u201d\n-\tOn page 6, I would explicitly define what $L0$ means in $L0\\approx 59$ for readers are less familiar with SAEs.\n-\tFigure 5 you should use a different color for the random decoder vector as in Figure 4 you use that some color for representing a 20 feature intervention.\n-\tOn page 8, you say \u201cWe propose four key directions for future research\u201d but only provide three."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tests the use of sparse autoencoders (SAEs) for unlearning specific knowledge in LLMs, their application of interest is the biosecurity-related Weapons of Mass Destruction Proxy dataset. The authors selectively suppresses SAE features that are highly used by the WMDP dataset (i.e. figure 2). Experiments compare SAE-based unlearning with Representation Misdirection for Unlearning methods, they test accuracy and side effect on the OpenWebText dataset. The authors report effective unlearning with some success in minimizing side effects, though challenges remain with both interpretability and the general applicability of SAEs for unlearning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Unlearning\n* Minimal side effects"
            },
            "weaknesses": {
                "value": "* Not concise: I still don't fully understand what a SAE is. The entire paper proposes a new methodological framework without a single math equation. It is very hard to follow and reads like a conversation between LLM software engineers moreso than a technical report on a new methodology.\n* Plots everywhere: why is figure 10 cited an entire page before figure 2? The figures should be placed in close proximity to the text in which it is being discussed\n* What causes the drop on OpenWebText? What datapoints do you lose performance on? How many datapoints do you misclassify? What type of questions are they?\n* It might just be unlearning biology related content. There should be more focus on testing biology related content."
            },
            "questions": {
                "value": "Figure 2: the figure says it's a distribution, but it has counts on the y-axis. Also, it does not seem to be normalized per dataset, i.e. the OpenWebText is a lot smaller, why is that? Also, why is figure 2 on page 2 instead of next to the text it's mentioned in. Please fix that it makes it hard to follow the storyline.\n\n\"Interestingly, the model modified using RMU answers option \u201cA\u201d on 62% of questions,\ncompared to 25% for the base model.\" I dont understand what that means.\n\n\"2.4 HOW TO SELECT RELEVANT SAE FEATURES\" by this point in the paper I still have no idea what a SAE feature is and this section is unintuitive to me. In general, until this point, the paper has been very verbose and has had limited conciseness.\n\nSection 3:\nThe entire section has heavy use of DL/LLM lingo and explains the methodology using \"math-intuition\". This is, in fact, quite unintuitive to me. Please rewrite and be concise about the method you are proposing. I am sure you can do it in much less space."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores the potential of using sparse autoencoders (SAEs) to remove specific types of knowledge from language models in an interpretable way. Specifically, it investigates whether SAEs can selectively \"unlearn\" harmful biology-related information from two language models, gemma-2b-it and gemma-2-2b-it, using a subset of Weapons of Mass Destruction Proxy dataset. The main findings suggest that adjusting (negatively scaling) certain biology-related feature activations is effective for unlearning this knowledge, whereas simply zeroing out features is not effective.\n\nKey insights include:\n1. Negative scaling of feature activations is necessary for unlearning specific topics, while zeroing features is ineffective.\n2. Multiple SAE features can be manipulated simultaneously to unlearn several topics, but this method has side effects similar to, or greater than, those of the existing fine-tuning-based Representation Misdirection for Unlearning technique."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper addresses an important and current issue in AI safety, focusing on controlled knowledge removal.\n- The authors present a thorough analysis of how individual SAE features can be targeted to unlearn specific knowledge, showcasing the possibility for precise, fine-grained control."
            },
            "weaknesses": {
                "value": "- Novelty: I am not sure about the difference between the paper\u2019s method and negative activation in [1] and [2].\n- Unlearning Performance: The submission underperforms relative to existing unlearning methods, notably RMU, as benchmarked by the WMDP. While it proposes an innovative approach, it fails to deliver superior results compared to RMU across several metrics, raising concerns about its effectiveness and relevance in high-stakes applications.\n- Helpfulness: Furthermore, the exact MMLU accuracy of the Gemma models in absolute terms is unclear, although it appears to be close to random (approximately 25%). The authors should clarify the selection criteria, the number of questions in the MMLU subset, and whether the overall performance of the model improves or worsens.\n- Validity of Evaluation Method: The evaluation is limited to small subset of the WMDP dataset (300 questions, or less than 8% of the full dataset), which diminishes the credibility of its results. Expanding the evaluation to other subsets, such as Chemistry and Cybersecurity, would provide a more robust measure of generalizability. The choice of \u201cSelected MMLU\u201d for assessing unlearning is also problematic; the subset itself is potentially biased toward knowledge that is resistant to permutation, which complicates unlearning and evaluation.\n- Explainability: The negative scaling approach in the submission should be explained more. According to the monosemanticity principle, zeroing the feature activation should be enough to suppress targeted knowledge, since features are believed to be untangled. It\u2019s unclear why stronger negative values would yield better suppression. The lack of transparency on what negative values signify and whether the feature activation is one-dimensional undermines the plausibility of the approach.\n\n\n[1] Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html\n\n[2] Scaling and evaluating sparse autoencoders https://cdn.openai.com/papers/sparse-autoencoders.pdf"
            },
            "questions": {
                "value": "- **On \"Perfectly Unlearned Models\"**: The paper should clarify why a perfectly unlearned model should achieve a score below 6, as this criterion feels arbitrary without supporting rationale.\n- **On the Negative Value in Feature Suppression**: A deeper analysis of the meaning and impact of negative scaling in feature suppression would add clarity to the method's robustness claims."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the use of Sparse Autoencoders (SAEs) to selectively unlearn specific knowledge within language models, using interpretable interventions. The study focuses on the Gemma-2b-it and Gemma-2-2b-it models, particularly on knowledge related to biosecurity from the Weapons of Mass Destruction Proxy Dataset (WMDP-bio)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Overall, applying SAE to unlearn specific knowledge in LLMs is an interesting and practical approach.\n- SAE interventions provide precise control over targeted knowledge, enhancing transparency.\n- This method avoids weight modification, offering a novel, activation-based approach to unlearning."
            },
            "weaknesses": {
                "value": "- It negatively impacts performance on unrelated domains.\n- The proposed method seems to require re-training each time a new domain needs to be unlearned, along with access to data from that domain.\n- More evaluation results beyond the WMDP-bio dataset would enhance the assessment."
            },
            "questions": {
                "value": "- The evaluation appears to be based on multiple-choice question answering. How would the method perform in open-ended question answering?\n- What are the additional training and inference costs of using SAE?\n- Can a trained SAE model be transferred to other large language models ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}