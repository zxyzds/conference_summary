{
    "id": "vgZDcUetWS",
    "title": "Neural Approximate Mirror Maps for Constrained Diffusion Models",
    "abstract": "Diffusion models excel at creating visually-convincing images, but they often struggle to meet subtle constraints inherent in the training data. Such constraints could be physics-based (e.g., satisfying a PDE), geometric (e.g., respecting symmetry), or semantic (e.g., including a particular number of objects). When the training data all satisfy a certain constraint, enforcing this constraint on a diffusion model makes it more reliable for generating valid synthetic data and solving constrained inverse problems. However, existing methods for constrained diffusion models are restricted in the constraints they can handle. For instance, recent work proposed to learn mirror diffusion models (MDMs), but analytical mirror maps only exist for convex constraints and can be challenging to derive. We propose *neural approximate mirror maps* (NAMMs) for general, possibly non-convex constraints. Our approach only requires a differentiable distance function from the constraint set. We learn an approximate mirror map that transforms data into an unconstrained space and a corresponding approximate inverse that maps data back to the constraint set. A generative model, such as an MDM, can then be trained in the learned mirror space and its samples restored to the constraint set by the inverse map. We validate our approach on a variety of constraints, showing that compared to an unconstrained diffusion model, a NAMM-based MDM substantially improves constraint satisfaction. We also demonstrate how existing diffusion-based inverse-problem solvers can be easily applied in the learned mirror space to solve constrained inverse problems.",
    "keywords": [
        "generative models",
        "diffusion models",
        "mirror maps",
        "constrained generation",
        "inverse problems"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vgZDcUetWS",
    "pdf_link": "https://openreview.net/pdf?id=vgZDcUetWS",
    "comments": [
        {
            "comment": {
                "value": "Thank you for the positive feedback. \n\nA hyperparameter sweep for $\\sigma_\\max$ and $\\lambda_\\text{constr}$ can be used to determine the values that would provide the best performance in terms of constraint distance and distribution-matching accuracy. As detailed in Appendix C.1, we did not extensively search the hyperparameter space and simply looked at $\\sigma_\\max=0.1, 0.5$ and $\\lambda_\\text{constr}=0.01, 1$.\n\nIntuitively, it would make sense that $\\sigma_\\max$ should be increased with a higher dimensionality: because data points are more separated in higher dimensional spaces, adding more noise would ensure the maps are trained accurately at regions between the data points. We did not find a great deal of hyperparameter sensitivity with the dimensionalities considered in our experiments. We believe that a related theoretical direction to pursue is to understand how the best possible constraint satisfaction depends on the complexity of the constraint and how that might inform the choices of $\\lambda_\\text{constr}$ and $\\sigma_\\max$.\n\nWe believe that the performance gap for the semantic constraint has to do with the gradient-based nature of our approach. One avenue we are exploring is the use of gradient-free methods to optimize the NAMM, which could better handle constraints with irregular or undefined derivatives.\n\nThank you for the suggestions for improving the text; we will revise the text accordingly."
            }
        },
        {
            "comment": {
                "value": "Thank you for your feedback and suggestions. \n\nRobustness of our method is enhanced by introducing noise in the mirror space. By robustness, we mean that the inverse mirror map is able to restore a wider region of $\\mathbb{R}^d$ to the constraint set. A robust inverse map works not just for points from the data distribution but also for points somewhat off the data manifold. Figure 6 demonstrates the robustness provided by introducing noise in the mirror space. When the noise level $\\sigma_\\max$ is too low (i.e., $\\sigma_\\max=0.001$), the constraint distances are higher. This indicates that training the NAMM with too little noise in the mirror space makes it difficult for the inverse mirror map to handle any errors introduced by the mirror diffusion model.\n\nWe performed ablation and inverse problem experiments on a subset of the considered constraints for the sake of presentation clarity. We chose to focus on the more complex physics-based constraints to highlight the applicability of our method to physics-constrained inverse problems. We would be happy to include experiments on all five constraints in an appendix.\n\nTo our knowledge, there is no other approach that is applicable to all the constraints we consider. Since there was no relevant baseline, we instead considered a modification of the DPS method to act as a baseline we call \u201cconstraint-guided DPS.\u201d\n\nWe do not consider finetuning to be an essential component of the method. As Figure 4 and Table 1 show, most of the performance is already achieved before finetuning. We suggest finetuning as an optional additional step to further boost the constraint satisfaction. We believe it is a strength of our method that the results are not so dependent on finetuning."
            }
        },
        {
            "comment": {
                "value": "Thank you for the positive feedback and clarifying questions. \n\nWe would like to clarify that all of our experiments are conducted on image datasets. For example, the divergence-free constraint is demonstrated on 128x256 images. The NAMM architecture can easily scale to even higher dimensions, and we expect the bottleneck to be the diffusion model itself, which is known to be slow for generating very large images. To overcome this bottleneck, it is possible to train a latent diffusion model in the mirror space, where the latent space would be lower dimensional and thus cheaper to sample in.\n\nOur method can be easily applied to the constraints considered in Reflected Diffusion Models [1], which are convex and have simpler analytical forms in comparison to the physics-based and semantic constraints we consider in the paper. For example, we can easily design a constraint distance for our method to enforce the constraint that pixel values are bounded between two values.\n\nThe regularization loss ensures uniqueness of the forward and inverse mirror maps. Just the fact that the forward map is invertible does not ensure uniqueness: as a simple example, the forward map $f(x) = x$ has the inverse $g(x) = x$, but it could be scaled to $f\u2019(x) = 2x$ and have the inverse $g\u2019(x) = 0.5x$. The regularization loss helps resolve this scale/shift degeneracy.\n\n---\nReferences:\n\n[1] Aaron Lou and Stefano Ermon. \u201cReflected Diffusion Models.\u201d ICML 2023."
            }
        },
        {
            "title": {
                "value": "Global Response"
            },
            "comment": {
                "value": "We thank the reviewers for their feedback and suggestions. NAMMs offer a way to flexibly incorporate more general constraints than possible with previous methods (y9Gb, iSD2), and we demonstrate this with promising results \u201con a wide array of problems\u201d (FTXm). All reviewers appreciated the broader impact of our approach, as it addresses the challenge of constrained generative modeling for \u201cimportant applications in engineering, physics and computer vision\u201d (FTXm). They also recognized the broad applicability of our approach to inverse problems (y9Gb, iSD2) and different generative models (iSD2). Overall, our proposed approach is \u201csound\u201d (y9Gb) and \u201cnovel\u201d (FTXm), and it is backed by a \u201cwell-written\u201d paper (FTXm) and \u201ccomprehensive\u201d ablation studies (y9Gb). We will address individual reviewers\u2019 questions in the individual responses."
            }
        },
        {
            "summary": {
                "value": "The paper proposes neural approximate mirror maps for constraint data generation with diffusion models. Compared to typical mirror diffusion models, the mirror map is parameterized by the gradient of ICNN and learned via penalizing the differentiable constraint distance, thereby being applicable to general non-convex constraints. The forward and inverse mirror maps are learned by a combination of cycle consistency loss, constraint loss and regularization loss. Experiments in several settings ranging from physics-based to semantic demonstrate the effectiveness on constraint satisfaction, training efficiency and constrained inverse problem solving."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The method is applicable to more general constraints than previous works.\n- The cycle-consistency loss tailored for mirror maps and diffusion models is sound.\n- The experiments are conducted on diverse settings, including constrained DPS.\n- The ablation studies are comprehensive."
            },
            "weaknesses": {
                "value": "- The experiments are primarily toy. It is not clear whether the proposed method can scale to high dimensions and apply to domains such as images."
            },
            "questions": {
                "value": "- The regularization loss is introduced to ensure a unique solution according to the paper. What is the meaning of unique? As the mirror map is parameterized as the gradient of ICNN, the reversibility is already ensured.\n- Is the method scalable? For example, can it be applied to the image settings in Reflected Diffusion Models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents neural approximate mirror maps (NAMMs) to enforce soft constraints for diffusion models (DMs). NAMMs employ two neural networks to learn a mirror map that maps constrained points into the mirror space, and its inverse that transforms data back to the constraint set. A mirror diffusion model (MDM) can be trained in the learned mirror space, and its generated samples can be mapped to the constraint set via the inverse map. This method is tested on five benchmark problems, ranging from physics-based, geometric to semantic constraints, and the results show the proposed method improve constraint satisfaction compared to a vanilla unconstrained DM. And, this paper also demonstrates NAMMs leads to less constraint violation when solving constrained inverse problems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. NAMMs generalize the concept of true mirror maps to learn approximate mirror maps to handle non-convex constraints.\n2. NAMMs can handle physics-based, geometric and semantic constraints, while existing methods are restricted in the types of constraints they can handle.\n3. NAMMs not only help diffusion models, but also help VAEs to improve constraint satisfaction, showing the potential to be compatible for other generative models. And NAMMs are also helpful to diffusion-based inverse-problem solvers for solving constrained inverse problems."
            },
            "weaknesses": {
                "value": "1. Theoretically, NAMMs lack the guarantee of the existence and uniqueness of the mirror maps when applied to non-convex problems.\n2. The proposed method is validated on five benchmark problems in the main experiments to show the superiority of applying NAMMs to diffusion models in generating constrained data. However, in the experiments to solve inverse problems, ablation experiments, and the experiments applied to VAE, this method is only carried out on partial problems and does not fully demonstrate its performance on the three types of constraints mentioned.\n3. Finetuning is an important part of the proposed method introduced in section 3. But, it is mentioned in subsection 4.1 that \u201cWe show results from a finetuned NAMM, but as shown in Section 4.3, finetuning is often not necessary\u201d. Moreover, in the ablation studies of constraint loss and mirror map parameterization, and experiments about the VAE, fine tuning is not used.\n4. The basic unconstrained model is used for comparison, but the comparison with another existing methods dealing with constraints is lacking."
            },
            "questions": {
                "value": "1.\tIn this paper, the robustness of the model is enhanced by introducing noise into the mirror space. What does robustness mean here, and are there any experimental results that support the robustness of the method?\n2.\tIn order to fully demonstrate the performance of the method, it is necessary to supplement its experiments on five benchmark problems and an additional baseline model.\n3.\tIf fine-tuning is considered to be one of the important components of the method and one of the contributions of this paper, more experimental support is needed.\n4.\tOn line 1097, there is a clerical error, \u201ca la\u201d."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a technique to learn a mirror map for general constrained diffusion generation. The resulting neural approximate mirror map transforms the constrained problem domain to an unconstrained space, where the diffusion model is trained. The training loss encourages that the inverse of the mirror map lies within the constraint set. Numerical experiments on various problems demonstrate the efficacy of the proposed technique in enforcing the constraints."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The method is well motivated. Proper constraint satisfaction is challenging in diffusion generation, limiting many important applications in engineering, physics and computer vision. \n\n- The proposed approach is sensible, and to the best of my knowledge novel. It improves the flexibility of mirror diffusion models by obviating the need for analytical mirror maps.\n\n- The experimental results are promising on a wide array of problems. \n\n- The paper overall is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- It is unclear how complex of a constraint the proposed method can handle, as the gap between NAMM and the baseline is less significant for the semantic problem. \n- It is unclear if there is a systematic way to tune the introduced hyperparameters, and how sensitive the performance is in higher dimensions to $\\sigma_{max}$."
            },
            "questions": {
                "value": "- It appears in Fig. 6 that constraint distance is fairly sensitive to $\\sigma_{max}$ and $\\lambda_{constr}$. Can authors propose systematic ways to tune the hyperparameters? How does the sensitivity depend on the dimensionality of the problem or the complexity of the constraint?\n- Minor comment: the citations in Section 2.1 take up a large portion of the paragraph somewhat reducing readability (top of page 3).\n- Minor comment 2: Fig. 2 is never directly referenced in the text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}