{
    "id": "HCJ7B6dhYK",
    "title": "Radon Implicit Field Transform (RIFT): Learning Scenes from Radar Signals",
    "abstract": "Data acquisition in array signal processing (ASP) is costly because achieving high angular and range resolutions necessitates large antenna apertures and wide frequency bandwidths, respectively. The data requirements for ASP problems grow multiplicatively with the number of viewpoints and frequencies, significantly increasing the burden of data collection, even for simulation. Implicit Neural Representations (INRs) \u2014 neural network-based models of 3D objects and scenes \u2014 offer compact and continuous representations with minimal ground truth data. They can interpolate to unseen viewpoints and potentially address the sampling cost in ASP problems. In this work, we select Synthetic Aperture Radar (SAR) as a case from ASP and propose the \\textit{\\textbf{R}adon \\textbf{I}mplicit \\textbf{F}ield \\textbf{T}ransform} (RIFT). RIFT consists of two components: a classical forward model for radar (Generalized Radon Transform, GRT), and an INR based scene representation learned from radar signals. This method can be extended to other ASP problems by replacing the GRT with appropriate algorithms corresponding to different data modalities. In our experiments, we first synthesize radar data using the GRT. We then train the INR model on this synthetic data by minimizing the reconstruction error of the radar signal. After training, we render the scene using the trained INR and evaluate our scene representation against the ground truth. Due to the lack of existing benchmarks, we introduce two main new error metrics: \\textit{\\textbf{p}hase-\\textbf{R}oot \\textbf{M}ean \\textbf{S}quare \\textbf{E}rror} (p-RMSE) for radar signal interpolation, and \\textit{\\textbf{m}agnitude-\\textbf{S}tructural \\textbf{S}imilarity \\textbf{I}ndex \\textbf{M}easure} (m-SSIM) for scene reconstruction. These metrics adapt traditional error measures to account for the complex nature of radar signals. Compared to traditional scene models in radar signal processing, with only 10\\% data footprint, our RIFT model achieves up to 188\\% improvement in scene reconstruction. Using the same amount of data, RIFT is $3\\times$ better at reconstruction and shows a 10\\% improvement generalizing to unseen viewpoints as shown in Figure 3 and Table 1.",
    "keywords": [
        "AI for Science",
        "Representation Learning",
        "Scene Rendering",
        "Implicit Neural Representation",
        "3D Reconstruction",
        "Inverse Problems"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We combine implicit neural representation and signal processing algorithms to create a model which can reconstruct scenes by learning radar signals.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=HCJ7B6dhYK",
    "pdf_link": "https://openreview.net/pdf?id=HCJ7B6dhYK",
    "comments": [
        {
            "summary": {
                "value": "The paper presents the Radon Implicit Field Transform (RIFT), a novel method for reconstructing scenes from for synthetic aperture radar (SAR) imaging using an Implicit Neural Representation (INR). By encoding scene reflectivity in an INR, RIFT can reconstruct scenes with high fidelity from limited data samples, addressing the high data acquisition costs typical in SAR. The authors introduce custom error metrics\u2014magnitude-SSIM (m-SSIM) for scene reconstruction and phase-RMSE (p-RMSE) for radar signal interpolation\u2014to evaluate performance. Experimental results show that RIFT significantly improves scene reconstruction and viewpoint interpolation with reduced data compared to baseline models, demonstrating its potential for data-efficient radar-based imaging applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed method works well and outperforms existing approaches.\n- Due to a lack of existing benchmarks, the authors created their own benchmark problems.  This can be a significant contribution if the authors make their benchmark open and easily accessible to the community.  I would encourage them to do so."
            },
            "weaknesses": {
                "value": "- Presentation of the paper could be improved.  Figures generally contains small plots which are sometimes difficult to inspect, some (e.g. Figure 5) are surrounded by a lot of white space, some are not discussed in the text.\n- 3D scenes considered are very simple."
            },
            "questions": {
                "value": "- It is commented that a non-standard approach is taken of \"accumulating gradients within an individual epoch across different views.... This gradient accumulation is specifically designed to mimic the physical motion inherent in synthetic aperture radar systems...\"  This comment is somewhat cryptic and I did not fully understand why a non-standard approach was needed and, if so, how it mimics motion in SAR systems.  Could this comment please be elaborated and clarified.\n- The abstract should stand alone and should not reference figures and tables in the paper.\n- Figure 3 is not discussed in the text, as far as I could tell.\n- Caption of Figure 4 is incomplete: \"The data is\"\n- Typo p4, line 178: \"scenarios, We\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenges of data acquisition in array signal processing (ASP), particularly in Synthetic Aperture Radar (SAR), where high angular and range resolutions require extensive data collection. To mitigate the costs associated with large antenna apertures and wide frequency bandwidths, the paper proposes the Radon Implicit Field Transform (RIFT), which integrates a classical forward model (Generalized Radon Transform) with an Implicit Neural Representation (INR) learned from radar signals. The method enables efficient scene representation and interpolation at unseen viewpoints, potentially reducing data collection burdens across various ASP applications. In addition, the paper introduces two novel error metrics to radar signal interpolation and evaluate performance. Experimental results demonstrate that RIFT achieves up to 188% improvement in scene reconstruction accuracy with only 10% of the data footprint compared to traditional models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper learn implicit scene representations directly from radar signals, which is a new perspective for radar signal processing.\n2. The proposed method can reconstruction and interpolate views using fewer measurements that non-deep learning methods.\n3. The proposed two metrics are meaningful for the community.\n4. The selected experiments show the efficiency of the proposed modules."
            },
            "weaknesses": {
                "value": "1. While the paper presents a novel method (RIFT) for radar signal processing, the experimental validation may not be comprehensive enough. Additional datasets or real-world scenarios could strengthen the claims regarding the method's effectiveness and robustness.\n2.. The novelty of integrating deep learning methods with Generalized Radon Transform (GRT) is not well illustrated, what is the main contribution, only a combination of them?  \n3. The criteria and methodology for this benchmarking may require further elaboration. A more detailed description of how the benchmarks were selected and validated could enhance the paper's credibility.\n4. The influence of hyperparameter selection on the performance of the INR model is not sufficiently explored."
            },
            "questions": {
                "value": "1. How do the data acquisition costs for radar imaging compare with those of other ASP applications, and what specific factors contribute to these differences?\n2. How does the proposed Radon Implicit Field Transform (RIFT) manage to achieve scene reconstruction with only 10% of the data footprint compared to traditional models, and what specific mechanisms facilitate this reduction?\n3. How does the joint benchmark for radar scene reconstruction and signal interpolation proposed in this study align with existing benchmarks in other imaging modalities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper describes an application of implicit neural representation with a radar forward model to recover the 3D complex reflectivity of a scene radar array observations. A bi-static forward model is introduced and used to supervise a SIREN network with recovery results shown on simulated test cases. Two evaluation metrics are designed to quantify success both in reflectivity reconstruction and novel radar view synthesis."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The strength of the paper is the introduction of a new forward model to the ICLR community and its integration with neural fields. In general this is an interesting inverse problem which could a) open up new areas for research with the ML community and b) use ideas from the ML community, such as neural fields, for improving reconstructions, uncertainty quantification, run-time and scalability etc."
            },
            "weaknesses": {
                "value": "Overall I like the idea of the paper, however, I think the execution is quite lacking and it is not quite ready for publication. The experimental design is quite simplistic and I am unable to verify basic things like the validity of the forward model or if any improvements from the inclusion of neural fields would translate to real world scenarios. Below I detail some of the bigger issues and some minor ones:\n\n1. The main issue is that the simulations are way too simplistic. SAR is a well established imaging modality and so I would expect to see real world data reconstructions. In the absence of real data reconstructions I would still expect to see more realistic simulations than the ones shown in the paper. I don\u2019t think we can learn anything from the cube experiments and how they translate to real world scenarios. \n\n2. The experimental setup is not adequately described, so it is hard to assess the methodology. Here is a small subset of questions to guide the authors to a more complete picture:\n  2.1. What is the sampling of the view angles? Is it sampled uniformly? Is it sampled according to a realistic path of a SAR instrument? \n  2.2. What noise, if any, is added to the data? Is this noise realistic and mimics real SAR instruments?\n  2.3. Is there any mismatch between the forward GRT model used to generate the observations and the one used to recover the reflectivities? \n  2.4. Is the data/radar assumed to have perfect calibration, is this assumption realistic? \n  2.5. Is the GRT model described in Line 146 adequate? Are there any underlying assumptions that would introduce noise (systematic or otherwise) to real radar observations? What is the noise model? \n  2.6. How does R_b (Line 149-150) take into account differences in transverse vs along line of sight movement of the SAR?\n  2.7. In sec 3.1 it is not clear what is granularity refers to when SIREN is a continuous representation\n  2.8. The entire paragraph and discussion given in lines 188-193 is completely unclear. Also matrix A was never defined in the text.\n3. I think that there are some over claims in the text that should be addressed. \n  3.1 For example lines 216-217 state: \u201cIt is crucial to note that during training, we employ a nonstandard approach of accumulating gradients within an individual epoch across different viewpoints.\u201d . This seems to me quite standard in multi-view setting, and I think even the original NeRF and any multi-view algorithm before nerf (e.g. bundle adjustment, multi view stereo, etc) uses rays from multiple views in each gradient calculation. Furthermore the highlighted claim: \u201cThis gradient accumulation is specifically designed to mimic the physical motion inherent in synthetic aperture radar systems\u201d is not entirely clear to me, how is that different to any other multi-view setup? Where is the motion taken explicitly into account for the SAR system. It would help the paper if you could explicitly clarify these statements that seem like a key contribution.\n  3.2. Line 485: \u201clays a cornerstone for research into the representation of INRs in less-explored data modalities.\u201d I think this is an over claim. This work certainly is in line with a recent trend but I wouldn\u2019t say it is laying the corner stone. Work in the past couple of years have demonstrated the use of nerf with wild and interesting forward models from biology, astronomy, transient imaging to name a few, see a handful of links below:\n            - https://arxiv.org/abs/2307.09555\n\t\t- https://arxiv.org/abs/1909.05215\n            - https://arxiv.org/abs/2204.03715\n           - https://arxiv.org/abs/2405.04662\n           - https://arxiv.org/abs/2309.04437\n           - https://ojs.aaai.org/index.php/AAAI/article/view/20171\n\n\nMinor comments:\n - Figure 1 should not appear in the first page where it\u2019s not referenced. \n - Could be useful to give equation numbers for referencing.\n - The use of \u201cground truth\u201d should be reserved for reflectivities and it is confusing to use it in the context of radar data, even if this \u201cdata\u201d was simulated from ground truth reflectivities. (e.g. lines 173, 231 etc).\n - Lines 418-424 describe the setup in a very complex way. I think this could be resolved with a single illustration figure."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents RIFT (Radon Implicit Field Transform), a novel method for learning scene representations directly from radar signals using implicit neural representations (INRs). The approach combines classical radar forward modeling (Generalized Radon Transform, GRT) with modern neural network techniques to address the data acquisition challenge in array signal processing (ASP), particularly in Synthetic Aperture Radar (SAR) applications.\n\nThe main contributions include:\n\n1. A framework that integrates INR with GRT to learn scene representations directly from radar signals, enabling scene reconstruction with reduced data requirements\n\n3. Introduction of new evaluation metrics specifically designed for radar signal processing:\n   - phase-Root Mean Square Error (p-RMSE) for radar signal interpolation\n   - magnitude-Structural Similarity Index Measure (m-SSIM) for scene reconstruction\n\nThe authors demonstrate their method's effectiveness through extensive experiments on synthetic data, showing improvements in both scene reconstruction quality and viewpoint interpolation capability compared to traditional methods. They also present a case study on weak target detection in far-field scenarios, demonstrating the method's potential practical applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Good Originality:\n\n- The paper presents a novel application of implicit neural representations to radar signal processing, particularly in SAR imaging\n- While similar to computer vision approaches, its adaptation to radar domain represents meaningful innovation\n\nGood Quality:\n\n- The technical development is thorough and well-grounded in both radar signal processing and deep learning principles\n- The paper introduces well-designed evaluation metrics specifically for radar signals:\n  * phase-Root Mean Square Error (p-RMSE) for signal interpolation\n  * magnitude-Structural Similarity Index Measure (m-SSIM) for scene reconstruction"
            },
            "weaknesses": {
                "value": "# Major Weaknesses:\n\n1. **Lack of Real-World Data Validation**: **The most critical concern**\n- The paper relies entirely on synthetic data for evaluation\n- Many public mmWave and SAR datasets are available but not utilized\n- The gap between synthetic and real data is not addressed:\n  * Real scenarios involve complex multipath effects\n  * The accuracy of GRT in real-world conditions is questionable\n  * Environmental factors affecting radar signals are not considered\n- The effectiveness on synthetic data is expected due to neural rendering, but this doesn't validate real-world applicability\n- Therefore, this significant omission raises serious doubts about the validation and practical value of the proposed method\n\n> I would significantly improve my assessment if the authors could demonstrate their method's effectiveness on real-world SAR or mmWave radar datasets.\n\n2. **Accuracy of GRT Forward Model**\n- The paper doesn't address GRT's limitations in complex real-world scenarios:\n  * Multipath effects\n  * Material properties\n  * Environmental interference\n- These limitations could severely impact the method's practical applicability\n- The validation of GRT accuracy is crucial but missing from the current work\n\n3. **Related Work Citiation and Discussion**\n- RadarFields (published May 2024) has already explored similar ideas\n- While concurrent development is understandable, proper citation and discussion are needed\n\n# Minor Weaknesses:\n\n- Inconsistent mathematical notation in Section 3.1:\n  * Variable x is used inconsistently\n  * Matrix A notation varies throughout the paper\n\n- Typos"
            },
            "questions": {
                "value": "Please see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}