{
    "id": "pjKdWj5NSR",
    "title": "ReFOCUS: Recurrent False Object Correction Using guidance Strategies in Object Detection",
    "abstract": "This work addresses the issue of recurrent false positive classification in object detection. We consider two experimental setups imitating real-world scenarios that lead to such errors: i) erroneous annotations, ii) non-objects that resemble actual objects. We show that resulting models can be corrected efficiently using a two-step protocol that leverages false positive annotations. For the first step, we present and compare two correction approaches that guide false positives toward true negatives, in either the latent or the logit space. The second step then consists in standard continuous fine-tuning on correct annotations. The latent guidance framework relies on a decoder that maps the bounding box of a given false positive to its target true negative embedding. The decoder is trained as part of an autoencoder, where appropriate true negative samples are generated by a learnable Gaussian mixture model in the latent space. By leveraging the properties of the Wasserstein distance, the mixture model is optimized through standard backpropagation. In both experimental setups, the two correction methods significantly outperform standard continuous fine-tuning on correct annotations and demonstrate competitive performance when compared to models retrained from scratch on correct annotations. In particular, in the second experimental setup, the latent guidance framework consistently outperforms these models, effectively enhancing detection performance at the cost of supplementary false positive annotations. Additionally, the proposed techniques prove effective in a few-shot learning context.",
    "keywords": [
        "Object Detection",
        "False Positive",
        "Computer Vision",
        "Recurrent Errors",
        "Correction"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=pjKdWj5NSR",
    "pdf_link": "https://openreview.net/pdf?id=pjKdWj5NSR",
    "comments": [
        {
            "summary": {
                "value": "In this paper, two strategies of improving object detectors are proposed to handle the issue caused by erroneous annotations and false positives. It is achieved by guiding the false positives toward true negatives in the latent or the logit space. By further fine-tuning on correct annotations, the proposed methods improve the detection performance in most cases."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Two strategies of improving object detectors are proposed.\n2. Training details are provided to ensure its reproducibility."
            },
            "weaknesses": {
                "value": "1. The writing is poor. The authors need to carefully revised the paper from aspects of logical structures and grammars.\n2. In the first sentence of abstract, \u201crecurrent false positive classification\u201d is required to be further explained. In the first paragraph of introduction, the explanation of \u201crecurrent errors\u201d is also needed. Are these two concepts have the same meaning?\n3. The motivation may be questionable. In introduction, the authors claim that \u201cthe model consistently detects an object that should not be identified, e.g. people on billboards as instances of real people.\u201d However, the phenomenon that people on billboards are detected may be reasonable in some situations. Classifying these samples to be background by force could increase the risk of model oscillation or overfitting.\n4. In introduction, the Motivation paragraph seems to be logically incorrect. It seems that D_{True} is used in all cases. Why not using the f_{True} directly? A detailed explanation of this paragraph is required. Furthermore, the proposed methods only slightly outperform f_{True}. In some cases, the performance is even dropped, which limits the contribution of this paper.\n5. The experiments are not convincing. For PASCAL VOC dataset, there are 20 classes of objects. However, the authors only uses a small part of them. The mAP on all classes should be reported."
            },
            "questions": {
                "value": "The motivation of this paper should be re-clarified. The complete experimental results should be reported. The overall contribution is limited. Furthermore, this paper needs a major revision by re-organizing and proofreading its paragraphs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The work focuses on guiding the false positive object predictions to true negative predictions. The paper shows that models with FP predictions can be efficiently corrected using FP annotations. The paper proposed two correction approaches that guide false positives toward true negatives, i.e., in latent space (LoGF) and in logit space (LaGF). Both two methods required a corrective dataset where all recurrent FPs are additionally annotated. LoGF only modifies the classification logits of FP. LaGF uses an autoencoder architecture to change FP latents (sampled in a trained GMM) to TN latents (decoded by a trained decoder). After the training of the decoder, the original detection model is guided with newly annotated FP labels and  the decoder to bridge the gap between FP and TN in latent space, therefore, avoiding the FP predictions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper explores a new area to show that models with FP predictions can be efficiently corrected using FP annotations with two proposed methods, LoGF and LaGF."
            },
            "weaknesses": {
                "value": "1. Unfair comparison. While these two proposed guidance frameworks both rely on a corrective dataset where all recurrent FPs are additionally annotated, a fair comparison should be a noisy model fine-tuned on the combination of corrective dataset D_c and correct dataset D_{True} instead of fined-tuned only on correct dataset D_{True}.\n2. The experiments only explore the situation when only one pair of classes are misannotated, however, in the real world, multiple misannotated pairs are more common.\n3. Some mistakes in writing. In Section 4.1, the authors use PASCAL VOC 2007, while in Section 5.1, the authors say to train the model on MS-COCO 2017. And no labels on the caption of tables to point out whether the results are done on the former or the latter dataset."
            },
            "questions": {
                "value": "1. Please refer to the weakness.\n2. Why contrastive learning is listed as one subsection of related works?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work addresses the issue of recurrent false positive (FP) classification in object detection. The paper proposes two innovative correction frameworks that guide FPs toward TNs in either the latent space or the logit space. The latent guidance framework leverages an autoencoder where a learnable Gaussian mixture model generates the embeddings of appropriate TNs, and a straightforward decoder retrieves the TN embedding given a bounding box. The paper utilizes the properties of the Wasserstein distance to train the Gaussian mixture model through standard backpropagation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The experiments show that the propsoed module improves the performance.\n2. The idea of introducing other spaces other than logit space to solve classification errors is valuable."
            },
            "weaknesses": {
                "value": "1. Figure 1 is the network diagram in DETR paper, without any information increment, so this content should not appear in the main text. In addition, although illustrations of motivation may not be necessary in the Introduction section, the description of the motivation section in this article should have an image which includes:\n(a) the case visualization and explanation of the problem of false positives in classification errors. \n(b) the comparison with previous practices, as said in 'Motivations' part in Line 68. Tell the reader how the proposed method addresses the problem compared to previous approaches.\n(c) the visualization of the noisy dataset D_{Noisy}, and well-annotated dataset D_{True}. And how they are used in the method.\n\n2. Line 062 said the method 'can be generalized across different datasets and detection frameworks.' However, there were no experiments conducted on additional datasets and detectors in the experimental section and supplementary materials. In addition, both the data volume and the number of categories of VOC07 are not representative enough. Can it be validated with a larger dataset  (e.g., COCO, OpenImages)  and extra detectors to  better demonstrate the method's generalizability? Also, could you provide the computational requirements for testing on larger datasets if that is limited in larger dataset experments.\n\n3.  The writing and paper organized (like the structure of method section) should be improved. For example:\n(a). Sec 3.3.1 has 'Definition 1' (Line 221), then what? There are no  'Definition 2' in the following part.\n(b). The relationship between sections in method. You can provide  an overview of  'LoGF' and 'LaGF' in 'CORE CONCEPT' part and tell us the structure of the following sub-sections.\n......\nand, there are too many colloquial words in the article, such as \"we\" appearing 107 times.\n\n4. More visual displays are necessary. You don not need to show the results here, but rather make the before-and-after comparisons of specific false positive cases, or visualizations of how the latent space changes with the proposed methods.  In addition, some qualitative experiments on false positives can be added, such as analyzing whether the proportion of false positives decreases after adding methods from multiple dimensions, and so on."
            },
            "questions": {
                "value": "See weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to correct false positives in object detection. Starting from erroneous annotations and non-bojects similar to the true samples, the model is corrected in the latent and logit spaces."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper provides a rich theoretical basis and source of motivation.\n2. The paper verifies the effectiveness of the proposed method in both general object detection and few-shot object detection scenarios."
            },
            "weaknesses": {
                "value": "1. The paper mentions corrections in two spaces in both the abstract and contributions, but lacks a description of logit.\n2. The paper mentions that \u2018we assume that we have access to a corrective dataset DC\u2019. It seems that the paper's method relies on clear FP data with the same data domain as the initial dataset to guide GMM generation and model fine-tuning, but such data is often not easy to obtain. We often cannot access the data after training a network, not to mention the FP information. How to solve this?\n3. The paper proposes multiple loss functions in the method stage. It is recommended that the paper mark the different stages and training parameters of each loss in the Figure to facilitate readers' understanding. For example, in the final loss L_decoder stage, the author's network contains two MLP_bbox modules. Which one does ybbox target?\n4. In Table 2, it seems that the author's method does not improve significantly compared to directly using f_Ture. Did the author compare the extra cost of this method compared to f_True? For example, a time comparison for such a two-stage process."
            },
            "questions": {
                "value": "Before the Definition 1, the paper set the TN \u2018only its position remains to be defined\u2019, but after Definition 1, the paper said the TN \u2018shares the same bounding box as the FP.\u2019 How does this information align?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}