{
    "id": "uDZ9d4UAUh",
    "title": "Exposing the Achilles' Heel: Evaluating LLMs Ability to Handle Mistakes in Mathematical Reasoning",
    "abstract": "Large Language Models (LLMs) have significantly impacted the field of Math Word Problems (MWPs), transforming how these problems are approached and solved, particularly in educational contexts.  However, existing evaluations often focus on final accuracy, neglecting the critical aspect of reasoning capabilities. This work addresses that gap by evaluating LLMs' abilities to detect and correct reasoning mistakes.  We present a novel dataset, MWP-MISTAKE, containing MWPs with both correct and incorrect reasoning steps generated through rule-based methods and smaller language models. Our comprehensive benchmarking of state-of-the-art models such as GPT-4o and GPT4 uncovers important insights into their strengths and limitations. While GPT-4o excels in mistake detection and rectification, gaps remain, particularly in handling complex datasets and novel problems. Additionally, we identify concerns with data contamination and memorization, which affect LLM reliability in real-world applications. While OpenAI's O1 model demonstrates 90\\% accuracy in reasoning and final answers on complex tasks, it still remains weak in mistake detection. Our findings highlight the need for improved reasoning evaluations and suggest ways to enhance LLM generalization and robustness in math problem-solving.",
    "keywords": [
        "math reasoning",
        "mistake detection",
        "dataset",
        "benchmarking"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "This study presents the MWP-MISTAKE dataset to assess LLMs' ability to detect and correct reasoning errors in MWPs. While GPT4o shows strong performance, all models overall struggle with mistake detection.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=uDZ9d4UAUh",
    "pdf_link": "https://openreview.net/pdf?id=uDZ9d4UAUh",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a dataset for evaluating LLMs\u2019 abilities in detecting and correcting mistakes. The dataset contains math word problems with both correct and wrong reasoning steps, sourced from various datasets including SVAMP, GSM-8K, MATH, MATHBENCH, and JEEBENCH. To systematically generate mistakes, the authors employ two approaches: (1) rule-based techniques, and (2) using small language models as bad reasoners. It provides a comprehensive benchmarking of various LLMs and SLMs, with insights into their capabilities and limitations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Originality**:\nThe proposed dataset is designed to evaluate models in identifying and correcting reasoning mistakes, a critical yet underexplored area in LLM performance. Its focus on these reasoning capabilities rather than final answer accuracy bridges the gap in understanding models\u2019 overall problem-solving capabilities.\n\n**Quality**:\nThe paper evaluates a wide range of models, from large to small models, from open to proprietary models. It also provides thorough analysis from various aspects, including question variations, memorization, etc.\n\n**Clarity**:\nDataset construction and experiment setups are generally clear. By defining different categories of mistakes (rule-based or SLM-based) and different types of reasoning failures (eg, wrong numerical values, shuffled steps), the paper provides a structured assessment that is easy to understand.\n\n**Significance**:\nUnderstanding reasoning errors and their correction is meanful for educational applications and the wider community. The paper also highlights potential data contamination and memorization issues, emphasizing the need for more sophisticated training and evaluation practices."
            },
            "weaknesses": {
                "value": "- While the authors claim that rule-based mistakes mimic human reasoning errors, my concern is that some of these transformations lack natural coherence. For example, shuffling reasoning steps or numeric values often results in errors that do not reflect authentic human errors in mathematical reasoning, which can be distracting to models. Could this be the reason why models struggle more with rule-based errors compared to SLM-generated mistakes? I would suggest refining the rule-based methods to better mimic typical student errors, which could give a more valid evaluation of models\u2019 ability to detect naturally occurring mistakes. \n\n- Additionally, deleting reasoning steps raises a question about whether such omissions genuinely impact the final answer\u2019s correctness or reasoning quality. In cases where reasoning steps are deleted but the answer remains correct, it would be insightful to investigate whether models still detect these partial solutions as errors and if such ambiguities lead to lower error detection rates.\n\n- In related work, the authors mention studies suggesting that LLMs struggle to detect their own mistakes. However, the paper focuses only on mistakes generated by smaller models, without examining those generated by models of similar or competitive capabilities. Exploring a model\u2019s ability to detect its own mistakes in its own reasoning would add substantial value to the study, particularly given its focus on evaluating reasoning capabilities. For example, the authors could investigate whether GPT-4 or GPT-4o can detect mistakes it generates, which would offer more insights into the limitations of LLMs\u2019 self-evaluation in more complex scenarios."
            },
            "questions": {
                "value": "Questions and Suggestions:\n- Figure 2 is not referenced anywhere in the paper.\n- Lines 208-209: Do you mean Table 2? What is the difference between Table 2 and Table 7?\n- Lines 242-243: Why is GPT-4o underperforming in both simpler and more complex datasets? A brief explanation of the possible challenges it faces with these datasets would be helpful.\n- A detailed analysis of mistake types would strengthen the study. For instance, it would be valuable to know which types of mistakes models consistently struggle to detect."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examines the capability of current LLMs to detect and correct errors in mathematical reasoning. It introduces a new benchmark, MWP-MISTAKE, featuring both correct and incorrect reasoning steps generated by rules or smaller models. Experiments reveal that LLMs still have room for improvement in consistently identifying mistakes across simple and complex datasets. The authors note that contamination issues and generalization challenges hinder reliable performance in real-world applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper meticulously develops six crafted rules to assess LLMs' ability to detect reasoning errors, providing valuable insights into their mathematical reasoning capabilities in realistic settings. The authors compare current models in terms of mistake detection and correction, while also exploring potential contamination and memorization issues, thereby deepening our understanding of these models."
            },
            "weaknesses": {
                "value": "1.\tIn the figure 1, GPT-3.5 Turbo fails to detect errors while GPT-4o succeeds. Can GPT-3.5 Turbo correctly solve this question?\n2.\tRegarding the metric (line 186), is the F1 score for step-level or path-level detection performance?\n3.\tI suggest presenting the results of model performance in solving questions alongside error detection. Are questions that models solve correctly easier for them to detect errors compared to more challenging questions?\n4.\tIn Table 3, is the average computed based on all data samples? The data sizes from different sources are unbalanced. Additionally, compared to GSM8K, why does detection performance decrease on D but increase on SM (e.g., JEEBENCH)?\n5.\tI disagree with the statement in lines 253-255 that \u201cGPT-3.5 Turbo performs similarly to GPT-4 and even surpasses it on certain datasets like GSM-8K.\u201d The results in Table 4 do not support this as well. In lines 258-262, the authors claim that the performance of Llama-3-8b-finetuned rivals GPT-4 due to domain-specific fine-tuning. Have you tested Llama-3-8b-finetuned for memorization?\n6.\tIn section 4.4, while GPT-4o shows high ROUGE-L, it also has the best performance in mistake detection and question answering. Could this high performance be attributed to contamination? Can you provide qualitative cases to compare these LLMs? I am also curious about the contamination scores for Llama-3-8b-finetuned and O1.\n7.\tCitation issue? (line 44)\n8. Some relevant work in evaluation is overlooked, such as \"MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation\"."
            },
            "questions": {
                "value": "Please review the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents a dataset MWP-MISTAKE to evaluate LLM\u2019s abilities to detect and correct reasoning mistakes, the experimental results show that current LLM remains weak in mistake correction."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed dataset is helpful to the relevant research community.\n- This work proposes potential directions for enhancing LLM capabilities in reasoning."
            },
            "weaknesses": {
                "value": "- The finding that LLM remains weak in mistake detection/correction [1][2] is not novel.\n- The reliability and accuracy of the MWP-MISTAKE dataset are questionable. The correct reasoning steps of MWP-MISTAKE are generated by GPT-4 and manually verified for correctness. However, there is a lack of detailed statistics on manual inspection, and it is not explained how the authors handle it to ensure data quality when the reasoning steps generated by GPT-4 are incorrect.\n- The evaluation setup is hard to follow.\n- There are some typos in the paper, e.g., \u201ccorect\u201d in line 105, \u201ctheir\u201d in line 154.\n\n[1] Large Language Models Cannot Self-Correct Reasoning Yet.\n\n[2] SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated Responses"
            },
            "questions": {
                "value": "- The paper mentions that the correct reasoning steps are generated by GPT-4 and manually verified for correctness. I am wondering what proportion of the data has been manually checked? Among these data, what is the proportion of cases where the reasoning steps generated by GPT-4 are completely correct?\n- What does \u201cSmaller model reasoning steps\u201d in table 3 mean? Does it mean the incorrect steps are generated by smaller language models?\n- The paper evaluates the ability of LLMs to detect the correctness of reasoning steps and generate correct answers, but it does not evaluate the accuracy rate of the generated reasoning steps?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to assess the reasoning capabilities of large language models (LLMs) when solving math word problems (MWPs). It emphasizes the importance of evaluating LLMs\u2019 ability not only to solve problems but also to detect and correct errors in their reasoning processes. The authors introduce a novel dataset called MWP-MISTAKE, which contains correct and incorrect reasoning steps generated through rule-based methods and smaller language models (SLMs). The study benchmarks several LLMs, including GPT-4o, GPT-3.5Turbo, Claude-3-Opus, and others, revealing insights about their strengths and limitations in handling MWPs, especially in error detection and correction."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper introduces a novel dataset (MWP-MISTAKE) that evaluates LLMs\u2019 ability to detect and correct errors in reasoning, filling a gap in prior research that focused mainly on final answer accuracy.\n* The research is comprehensive, benchmarking multiple state-of-the-art models on various tasks and datasets, revealing significant insights about their reasoning abilities.\n* The experiments are well-documented, with clear descriptions of tasks and metrics. The inclusion of both rule-based and model-generated errors is a thoughtful addition.\n* The findings are relevant to applications where reliable reasoning is crucial, such as educational tools, and provide a foundation for future improvements in LLMs."
            },
            "weaknesses": {
                "value": "* Overall, the primary contribution lies in evaluating the reasoning abilities of state-of-the-art (SOTA) LLMs in mathematical problem-solving, with a focus on mistake detection, etc. The contribution remains somewhat limited in scope.\n* The paper identifies data contamination as a limitation but could provide more detailed mitigation strategies. This weakens the reliability of some results.\n* While the study covers multiple datasets, the models show reduced performance on newer, complex datasets like JEEBENCH, suggesting limited generalization.\n* Although rectification abilities are discussed, the results indicate that even advanced models struggle with consistent correction, which highlights the need for stronger frameworks for error handling.\n* While the paper identifies several challenges, it offers limited practical solutions or recommendations for overcoming the observed weaknesses in mistake detection."
            },
            "questions": {
                "value": "Refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}