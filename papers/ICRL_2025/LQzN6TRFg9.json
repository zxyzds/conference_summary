{
    "id": "LQzN6TRFg9",
    "title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer",
    "abstract": "We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 $\\times$ 1360 pixels. \nPrevious video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. \nFirst, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, \\model is adept at producing coherent, long-duration, different shape videos characterized by significant motions. \nIn addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. \nResults show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. \nThe model weights of the 3D Causal VAE, the video caption model, and CogVideoX are open-source.",
    "keywords": [
        "Video Generation",
        "Diffusion model",
        "Pretraining"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=LQzN6TRFg9",
    "pdf_link": "https://openreview.net/pdf?id=LQzN6TRFg9",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces CogVideoX, an advanced text-to-video generation model built on a diffusion transformer. It produces 10-second, high-resolution videos (768\u00d71360) at 16 fps. To overcome challenges in video generation, the authors implement:\nA 3D Variational Autoencoder (VAE) for better compression and video quality.\nAn Expert Transformer with adaptive LayerNorm to enhance text-video alignment.\nProgressive Training and Multi-Resolution Frame Packing to create coherent, long-duration videos with significant motion."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. 3D-RoPE for Video Data: The adaptation of Rotary Position Embedding (RoPE) to 3D (3D-RoPE) is novel, effectively capturing spatiotemporal relationships and adding originality to positional encoding.\n2. The qualitative visualizations showcase various video domains, including scenes, single-object videos, and multi-object videos."
            },
            "weaknesses": {
                "value": "1. The paper highlights CogVideoX\u2019s performance but lacks a detailed analysis of computational efficiency, including memory usage and training/inference time.\n2. The paper does not discuss the scalability of CogVideoX to longer video durations beyond 10 seconds."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper is a text-to-video generation model using a diffusion transformer that generates 10-second, high-resolution videos aligned with text prompts without super-resolution or frame-interpolation. It improves upon previous models through a 3D causal VAE for text-video alignment, coherent, long-duration, and motion-rich videos. It achieves state-of-the-art performance and is open-source"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I believe this paper is good, an open-source and open-hyperparameters, will have a impact on community research.\n\n1. The paper is well written.\n2. This paper creates a well-defined dataset for text-to-video generation.\n3. This paper publicly release 5B and 2B models, including text-to-video and image-to-video versions.\n4. This paper achieves state-of-the-art performance compared with other text-to-video models.\n5. The generated high-resolution videos have very good quality."
            },
            "weaknesses": {
                "value": "1. The training process and model structure are somewhat unintuitive and slightly complex, raising some concerns about performance improvement in the future.\n2. There is a lack of detailed analysis on the ablation study. If there were a more detailed analysis on the ablation study, I would raise the score.\n3. Why is 2D + 1D attention unstable? In Figure 8, is the X-axis FVD and the Y-axis Training Steps?\n4. In 459 line, () is typo ?\n5. In 475 line, 17 frame is right ?"
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a text-to-video generation model focused on generating video with temporal consistency and rich motion in longer sequence. It proposes a 3D VAE for video compression, achieving high-quality reconstruction, and introduces a new Transformer architecture to enhance semantic alignment between text and video. Experimental results show that CogVideoX outperforms existing models, especially in complex dynamic scenes. With open-source release, CogVideoX has potential to advance research in video generation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Easy to follow \n* Effective apporach to text-to-video generation based on diffusion transformer \n* Demonstration of high-quality video"
            },
            "weaknesses": {
                "value": "* Overall, the performance improvement does not appear to be significant. For example, the CogVideoX-2B model outperforms only in the Dynamic Degree (compared to CogVideoX-5B). Additionally, CogVideoX-5B does not achieve the best performance across all models and metrics.\n* The computational cost (in both time and memory) and the complexity of data filtering and training seem high. Authors should specify these."
            },
            "questions": {
                "value": "* In the paper, ablation studies have been evaluated with only FVD scores. However, for Expert AdaLN, which focuses on alignment between text and video data, it would be reasonable to include other metrics, such as the CLIP score, to provide more robust validation of the ablation study results.\n\n* What causes the 2B model to underperform compared to other baselines and the 5B model except dynamic degree? \n\n* Can the model generate similar-quality videos without using the caption upsampler? The caption upsampler may hinder robust generalization performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduced CogVideoX, a large-scale text-to-video generation model based on diffusion transformers, addressing common issues in prior video generation models, such as limited motion and short duration. It introduces several designs like 3D VAE, which I think is novel.\n\nExperimental results demonstrate that CogVideoX achieves state-of-the-art performance across multiple machine metrics and human evaluations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.There are few diffusion transformer-based models. This paper provides a comprehensive approach, from training a 3D VAE to constructing a text-video data processing pipeline, along with a robust model architecture and training design. The model and code are well-implemented.\n\n2.The paper is well-structured, and very easy to follow.\n\n3.The ablation study is thorough, verifying the effects of 2D+1D attention and 3D attention design in video generation, as well as different positional encodings. The 2B and 5B model training partially validates the scaling law."
            },
            "weaknesses": {
                "value": "1. The novelty at the model level is relatively weak, with two expert adaptive layer norms being a fairly simple design.\n2. The authors use T5 as the encoder, but comparisons with mixed text encoders are missing.\n3. The 3D VAE design is primarily compared with its own variants, without comparison to other VAE designs, such as the spatial compression capability of SD\u2019s 2D VAE.\n4.Although the data processing pipeline is clearly outlined, the dataset is not publicly available.  It would be nice to open-source the data."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "CogVideoX is a large-scale text-to-video generation model using a diffusion transformer, capable of generating 10-second videos at 16 fps with a resolution of 768\u00d71360 pixels. It addresses previous models' limitations in movement and duration by introducing three components: a 3D Variational Autoencoder for improved video compression and fidelity, an expert transformer with adaptive LayerNorm for enhanced text-video alignment, and progressive training with multi-resolution frame techniques for coherent, long-duration videos with significant motions. The paper introduces a text-video data processing pipeline to enhance generation quality and semantic alignment. CogVideoX achieves state-of-the-art performance in text-to-video generation, and its components. The authors also make the model weights of the 3D Causal VAE, the video caption model, and CogVideoX publicly available."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The open-sourcing of the 3D Causal VAE, the video caption model, and the CogVideoX model significantly promotes future developments in video generation research.\n2. The method can generate videos with larger frames, better temporal consistency, and higher resolution.\n3. The proposed Multi-Resolution Frame Pack and Progressive Training techniques are interesting and meaningful.\n4. This paper introduces sufficient technical improvements to enhance the model's performance.\n5. Experimental results demonstrate that CogVideoX achieves superior performance compared to current text-to-video generation models."
            },
            "weaknesses": {
                "value": "1. Providing additional details in the methods section would enhance the paper\u2019s completeness. For example, further explanation on implementing videos of different durations (and resolutions) in the same batch would be helpful. Although it is mentioned that the method is inspired by Patch n\u2019Pack, a brief clarification would be beneficial.\n2. It would be better to discuss the additional computational costs associated with using 3D attention compared to the commonly used 2D+1D attention.\n3. The axis labels in Figure 8 appear to be mislabeled.\n4. It would be better to add a related work section to discuss video generation works and the differences from previous methods and architectures. Additionally, references for some methods are missing in Table 3."
            },
            "questions": {
                "value": "1. There is a typo in line 310: \"Patch\u2019n Pack.\"\n2. Line 459 seems to be missing a citation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}