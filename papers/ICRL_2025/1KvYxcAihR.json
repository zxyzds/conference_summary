{
    "id": "1KvYxcAihR",
    "title": "TMGBench: A Systematic Game Benchmark for Evaluating Strategic Reasoning Abilities of LLMs",
    "abstract": "The rapid advancement of large language models (LLMs) has accelerated their application in reasoning, with strategic reasoning drawing increasing attention.\nTo evaluate the strategic reasoning capabilities of LLMs, game theory, with its concise structure, has become the preferred approach for many researchers.\nHowever, current research typically focuses on a limited selection of games, resulting in low coverage of game types. \nAdditionally, classic game scenarios carry risks of data leakage, and the benchmarks used often lack extensibility, rendering them inadequate for evaluating state-of-the-art models.\nTo address these challenges, we propose TMGBench, a benchmark characterized by comprehensive game type coverage, novel and diverse scenarios, and flexible game organization. \nSpecifically, we incorporate all 144 game types summarized by the Robinson-Goforth topology of 2\u00d72 games, which are constructed as classic games in our benchmark. \nFurthermore, we employ synthetic data generation techniques to create diverse, higher-quality game scenarios through topic guidance and human inspection for each classic game, which we refer to as story-based games.\nLastly, to provide a sustainable evaluation framework adaptable to increasingly powerful LLMs, we treat the aforementioned games as atomic units and organize them into more complex forms through sequential, parallel, and nested structures.\nWe conducted a comprehensive evaluation of mainstream LLMs, covering tests on rational reasoning, reasoning robustness, Theory-of-Mind capabilities, and reasoning in complex game forms. \nThe results revealed that \nLLMs still have flaws in the accuracy and consistency of strategic reasoning processes, and their levels of mastery over Theory-of-Mind also vary.\nAdditionally, o1-mini, the latest reasoning model from OpenAI, was also evaluated across the sequential, parallel, and nested game structures and reached accuracy rates of 66.6\\%, 60.0\\%, and 70.0\\%, respectively, highlighting the challenges posed by TMGBench.",
    "keywords": [
        "Large Language Models; Benchmark; Strategic Reasoning; Game Theory; Theory of Mind"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1KvYxcAihR",
    "pdf_link": "https://openreview.net/pdf?id=1KvYxcAihR",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces TMGBENCH, a benchmark for systematically evaluating the strategic reasoning abilities of LLMs. By evaluating some LLMs on TMGBENCH, the paper identifies several flaws in LLMs\u2019 performance, such as low accuracy rates and unstable inconsistency."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well written and well organized.\n- The games included in TMGBENCH are comphrehensive."
            },
            "weaknesses": {
                "value": "- I am not fully convinced there exists the need for a benchmark fo evaluating strategic reasoning abilities of LLMs. In fact, there lacks an universal definition of the ability of strategic reasoning. In other words, what are the fundemental differences between tasks that require strategic reasoning and tasks that do not?\n\n\n- If there is a clear definition of strategic reasoning, I would expect a more systematic study of existing LLMs on strategic reasoning. Why some LLMs perform better than others in terms of strategic reasoning? What are the influencing factors of LLMs? Data, Architecture, Model Size, training objectives?"
            },
            "questions": {
                "value": "Regarding weakness 1:\n\n- Do you have a clear definition of tasks that require strategic reasoning, as used in this paper?\n\n- Could you explain more on how TMGBENCH addresses gaps in existing benchmarks for evaluating LLM reasoning capabilities?\n\n- What are the fundemental differences between tasks that require strategic reasoning and tasks that do not, perhaps with concrete examples?\n\nRegarding weakness 2:\n\n- Could you conduct an analysis of how different LLM characteristics (e.g., model size, architecture, training data, or objectives) correlate with performance on TMGBENCH? and why."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors create TMGBench, a game theory based benchmark for testing the strategic reasoning abilities of LLMs. They create a large number of games based on the \"Robinson-Goforth topology of 2x2 matrix games\" as well as utilizing synthetic data generation to build on top of said games for further game development. The games are then combined in a variety of ways, creating a complex structure for the LLMs to reason in. The authors then evaluate a selection of LLMs on the benchmark and report their results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Models are tested rigorously; 2,880 times for a single model in the single game tests, the complex games have a baseline of being tested 20 times, and there's testing for positional bias with the reFoToM / reSoToM prompts.\n- Extensibility: this is a great way of creating a difficult-to-overfit-to benchmark, using the synthetic data generated stories as additional \"games\" to play.\n- The metrics used (ID, BD, PAR) are comprehensive for evaluating a model's performance and good insight to how the models perform in these situations.\n- The tables and figures nicely present the findings of the experiments and are mostly given good descriptions."
            },
            "weaknesses": {
                "value": "- The paper can be hard to follow at times. It would be nice to have examples of the complex games to solidify the reader's understanding. The description given for sequential games doesn't quite make sense to me, even with two introductions. And because of that, I'm not sure how well it upholds the task of \"testing for strategic reasoning\".\n- I'm not convinced that parallel forms are actually a test of strategic reasoning either, this seems closer to measuring the model's \"working memory\" and being able to keep track of the different situations at a given time step. But, this may be based on a misunderstanding of what the form is describing; it's not clear to me based on the descriptions given.\n- The prompt given for `Example of classic game: classic/111` gives me pause for the rest of the prompt generation. \"Player A and Player B are playing a game. Either of them has two choices, namely A1, A2/B1, B2.\" Is this telling the model that the choices are {A1, A2} or {B1, B2}? I assume this, but that could lead to the model being confused about the task rather than being honestly judged on the difficulty of the task.\n\n- a number of simple proofreading errors:\n\t- \"sequential form, where LLMs are required to response multiple game tasks in a row\" --> \"to respond to multiple games\"\n\t- \"As explained in Section 2.2, our benchmark are perfectly suitable\" --> your benchmark what?\n\t- \"as for practical result provide by LLMs,\" --> results provided by\n\t- \"which we expect robuster LLMs\" --> \"more robust LLMs\", I'm not sure if \"robuster\" is a word, but if it is it's not commonly used.\n\t\t- \"using CoT prompting, which is robuster\"\n\t- \"We perform 4 independent tests on each data point, covering both the classic setting and the story-based setting. Basically, we conduct 2,880 tests to generally evaluate a certain model\"\n\t\t- this is weird, \"Basically, we conduct 2,880 tests...\" these should be combined to make flow better.\n\t- \"We setup the test by divided it into several types\" --> \"by dividing it\""
            },
            "questions": {
                "value": "- interesting that llama70B did worse on DA than 8B, why do you think this is?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a benchmark TMGBENCH. TMGBENCH incorporates 144 game types based on the Robinson-Goforth topology of 2\u00d72 games and provides three forms (sequential, parallel, and nested) to construct more complex games using those 144 game types. Several LLMs were compared on the benchmark using several quantified metrics to identify their strengths and weaknesses."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- The paper is very well-written.\n- Objectives are clear, and how those objectives are achieved by this work is well demonstrated.\n- Quantified metrics and visualisations have been used to compare LLMs on different tasks to assess their capabilities. \n- Extensive experiments were conducted to exam the failure cases and the effect of ToM. \n- Limitations were also discussed.\n- Generation pipeline was demonstrated in Appendix.\nOverall, the reviewer quite enjoyed reading this paper."
            },
            "weaknesses": {
                "value": "No particular weakness was identified by the reviewer. The reviewer is not an expert in game theory or reasoning. It is quite likely that the reviewer is unfamiliar with some pieces of related work or crucial part of this work."
            },
            "questions": {
                "value": "It is stated that \u201cTheoretically, using these atomic games, we can expand the framework to generate infinitely many increasingly complex game forms.\u201d However, standard answers are required to compute the inconsistency map. The reviewer wonders how to obtain the standard answers to newly generated games?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a benchmark for strategic reasoning comprised of all 2x2 game ordinal payoff arrangements. Additional evaluation capabilities include testing agents when reasoning on compositions of these games (in parallel, sequentially, or where one game influence the choices in a subsequent game) and reframing the games in story-based scenarios. Evaluations study open and closed source LLMs on this benchmark, assessing: how well they produce optimal choices, the extent to which they exhibit asymmetrically biased responses when payoff matrices are flipped, and using theory of mind to improve performance. The results demonstrate that existing LLMs do not saturate the benchmark, have varying degrees of bias based on the payoff structure and story framing, and struggle to leverage theory of mind to improve results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "# originality\nModest.\n\nEvaluating LLMs in strategic reasoning games is a thoroughly investigated topic (as attested by the related work). Examining anti-symmetric reasoning patterns is a question I have not seen probed before and important to consider for this setting in general.\n\n# quality\nModest.\n\nExperiments demonstrate the benchmark can find differences among LLMs. Models fail to saturate the success criteria, particularly for more stringent requirements like perfect answering or demonstrating theory of mind. Biases based on the generated stories show there is clear room for improving LLM context sensitivity, however it is not clear how much this could be mitigated by different prompts for the strategic reasoning (a dimension not explored in the paper).\n\n# clarity\nModest.\n\nThe introduction was vague and hard to follow without reading the rest of the paper. Experiments are documented well. Some figures were hard to parse or could use a different presentation (notes below).\n\n# significance\nModest.\n\nThere are numerous evaluations for strategic reasoning in game theoretic games. This focuses on 2x2 games, omitting multi-agent agents or repeated/multi-turn games (excepting the composite games tested). The paper will be of some interest to the community focusing on this subset of LLM capabilities."
            },
            "weaknesses": {
                "value": "Note: These weaknesses are phrased as questions to facilitate discussion.\n\n# originality\nHow do the games in this benchmark cover those not covered in the \"game theory\" subsection of the cited paper \"A Survey of Strategic Reasoning with Large Language Models\"? Or for the \"Societal Behavior\" examples that include theory of mind?\n\n\n# quality\n\nThe experiments should include statistical tests when claiming differences among model types. At least in the cases where multiple runs were possible or multiple scenarios are being aggregated (for example, in Table 1 and Figure 5). Many claims seem plausible, but the tests are there to provide rigor.\n\nThe paper would benefit from evaluating the concern stated in the introduction that there is scenario leakage of common game forms. Was there evidence of scenario leakage based on the games in Robinson-Goforth topology results? Do the games most likely to be leaked (like Prisoner's Dilemma) demonstrate substantial performance differences relative to other games?\n\n\n# clarity \n\nThe introduction could be clearer on details later explained in the paper. Examples: \n- \"performance variability marked by coefficients\" - Coefficients of what?\n- \"marked by an asymmetric pattern\" - What asymmetric pattern?\n\nFigure 6 is hard to read. It might be better plotted by showing the differences in scores between the classic and story-based settings instead.\n\n# significance\n\nWhat are the key insights we learn from TGMBench that were not revealed in prior benchmarks? This is not very clearly articulated in the paper and would help establish it's originality and significance. As TGMBench is a benchmark, the value it provides derives in exposing LLM capabilities that are not already apparent in alternatives."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}