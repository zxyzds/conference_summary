{
    "id": "DuyuAHBk1t",
    "title": "AIR: Zero-shot Generative Model Adaptation with Iterative Refinement",
    "abstract": "Zero-shot generative model adaptation (ZSGM) aims to adapt a pre-trained generator to a target domain using only text guidance and without any samples from the target domain.\nCentral to recent ZSGM approaches are *directional loss* which use the text guidance in the form of aligning the image offset with text offset in the embedding space of a vision-language model like CLIP.\nThis is similar to the analogical reasoning in NLP where the offset between one pair of words is used to identify a missing element in another pair by aligning the offset between these two pairs.\nHowever, a major limitation of existing ZSGM methods is that the learning objective assumes the complete alignment between image offset and text offset in the CLIP embedding space. \n**Our work** makes two main contribution.\nInspired by the offset misalignment studies in NLP, as our first contribution, we perform an empirical study to analyze the misalignment between text offset and image offset in CLIP embedding space for various large publicly available datasets.\nOur important finding is that offset misalignment in CLIP embedding space is correlated with concept distance, *i.e.*, close concepts have a less offset misalignment.\nTo address the limitations of the current approaches, as our second contribution, we propose Adaptaiotn with Iterative Refinement (AIR) which mitigates the offset misalignment issue in directional loss by iteratively selecting anchor points closer to the target domain.\nExtensive experimental results show that the proposed AIR approach achieves SOTA performance across various adaptation setups.",
    "keywords": [
        "Zero-shot Generative Model Adaptation",
        "Transfer Learning",
        "Prompt Learning",
        "Multi-modal Representation Space"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=DuyuAHBk1t",
    "pdf_link": "https://openreview.net/pdf?id=DuyuAHBk1t",
    "comments": [
        {
            "withdrawal_confirmation": {
                "value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."
            }
        },
        {
            "summary": {
                "value": "This paper introduces an innovative Adaptation with Iterative Refinement (AIR) method for addressing offset misalignment in CLIP embedding space within Zero-Shot Generative Modeling (ZSGM). Through a detailed empirical study, the authors analyze the offset misalignment between image and text offsets in CLIP embedding space, demonstrating that this misalignment intensifies with greater concept distance, yet is less impactful between closer domains. To counter this, the AIR method iteratively samples anchor points during adaptation, utilizing a novel prompt learning strategy to describe these anchor points without predefined textual descriptions. The proposed approach effectively mitigates offset misalignment, resulting in good performance in ZSGM for diffusion models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper introduces a novel approach to Zero-shot Generative Model Adaptation (ZSGM) by addressing the critical issue of offset misalignment between image and text representations in the CLIP embedding space, showcasing originality in its formulation and methodology."
            },
            "weaknesses": {
                "value": "1.\tThe references are somewhat disorganized and have formatting issues; for example, many citations should be formatted as (Smith et al., 2023) rather than Smith et al. (2023). Additionally, there is a lack of coherent context when citing references. \n2.\tThe writing of this paper could benefit from some improvement, as it contains several spelling errors (e.g., \"Adaptatoin\" instead of \"Adaptation\") and some grammatical inconsistencies.\n3.\tIn the Related Work section, this paper assumes that many methods default to the alignment of image and text offsets in CLIP space, which seems to warrant further consideration. For instance, some works, such as SVL, have already discussed T2I consistency. Furthermore, some studies have also addressed zero-shot content consistent T2I, such as Tewel, Yoad, et al. \"Training-free consistent text-to-image generation.\" \n4.  Lacking some more convincing qualitative and quantitative experiments (e.g., Figure 4, Figure 5), as well as a comparison of the diversity of entities."
            },
            "questions": {
                "value": "1.\tIn Figure 4 (left), there is no significant difference in qualitative results between AIR, NADA, and IPL. Could this indicate that, in single-descriptor, same-category adaptation, existing methods do not exhibit a significant T2I offset? Otherwise, please provide some more convincing qualitative experiments to support this.\n2.\tIn Table 3, please explain why aligning offsets significantly improves generation diversity without causing the generated content from the model to become too similar, resulting in a decrease in diversity.\n3.\tCompared to animals and humans, the qualitative and quantitative experiments in the paper seem to lack content-consistent generation for some objects and scenes, as seen in other works. Please provide more diverse experimental examples and results to enhance the paper's persuasiveness.\n4.\tThe paper seems to lack a qualitative ablation study. Please provide some specific experimental results to supplement it.\n5.\tIf the text description in a sentence (rather than a specific prompt) is inherently ambiguous, is the method of aligning offsets presented in this paper still useful?\n6.\tRegarding the results in Figure 5, for example, the eyes in the photo \u2192 cartoon transformation seem to move, and a similar issue appears in the dog example. This problem also seems to be present in Figure 6 of the supplementary materials. Please provide more compelling results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In the ZSGM field, previous work has simply aligned the image offset with text offset using directional loss. However, experiments show that these offsets are not merely aligned but often misaligned. Based on this finding, the authors propose Adaptation with Iterative Refinement (AIR) to alleviate this issue by iteratively selecting anchor points closer to the target domain. The anchor points are selected during adaption, coupled with a new prompt learning approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper is well-written and clearly introduces its motivation and research methods. First, it highlights the limitation of previous methods, which simply aligned image offsets with text offsets, and verifies this limitation through experiments. Next, it conducts experiments to validate the hypothesis that addressing these misalignments can lead to improved performance. Finally, based on this analysis, the paper presents its proposed research method.\n\n2. The paper conduct an analysis of the offset misalignment, and then the first to reveal the misalignment is larger for distance concepts and less for close concepts."
            },
            "weaknesses": {
                "value": "1. Table 1 and Table 2 present the results of the GAN model and the diffusion model, respectively. However, the evaluation metrics, comparison methods, and adaptations used for the two models are not consistent.\n\n2. Most of the experiments conducted involve adaptation between two concepts with similar images.\n\n3. Line 153 states, \"Previous works assume that for two different concepts, \u03b1 and \u03b2.\" However, Algorithm 1 and Algorithm 2 use two learning rates, also denoted as \u03b1 and \u03b2. This could lead to confusion."
            },
            "questions": {
                "value": "1. Why are Table 1 and Table 2 not consistent? Please explain the differences in the evaluation metrics, comparison methods, and adaptations used for the GAN model and the diffusion model.\n\n2. How does it perform on adaptation when there are significant differences between the source and target images? For example, in the NADA[1] experiment: Dog -> The Joker, Dog -> Nicolas Cage.\n\n3. Note that Algorithm 1 and Algorithm 2 have two learning rates, \u03b1 and \u03b2. Are these two learning rates consistent? How was the learning rate of 0.002 chosen in Supp. A.4?\n\n[1] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-guided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 41(4):1\u201313, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The authors discuss the limitations and ethical issues that we are concerned about in Supp. I and J."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores zero-shot adaptation for generative models, particularly diffusion models. It empirically examines the offset misalignment issue in previous methods and the impact of this issue on generative model adaptation. The paper proposes an iterative refinement method to mitigate the effects of offset misalignment."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper is well-organized and clearly presented.\n2. The study on offset misalignment is novel to me.\n3. The iterative refinement solution for misalignment is interesting and reasonable."
            },
            "weaknesses": {
                "value": "I did not find any remarkable flaws in this paper. However, I have one question: in the study in Section 3.1, the concept distance is measured between different classes, whereas in the impact study in Section 3.2, the concept distance is constructed using different hand-crafted prompts. Why are these setups misaligned?"
            },
            "questions": {
                "value": "Please refer to the Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel approach called AIR to address the zero-shot generative model adaptation (ZSGM) problem. First, it performs an empirical study to analyze the misalignment between text offsets and image offsets in CLIP. Second, it proposes AIR to address this issue."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.The paper conducts an empirical study on a large public dataset to analyze offset misalignment in the CLIP embedding space, finding that misalignment increases as the concepts become more distant\n\n2.Figures 2 and 3 vividly present the misalignment in the CLIP space and illustrate the impact of offset misalignment"
            },
            "weaknesses": {
                "value": "1. There is a concern about the paper lacks theoretical proof or experimental evidence that the after limited iterations of the adaptation, the adapted generator is already closer to the target domain than the pre-trained generator \n\n2. There is no sensitivity study conducted for the parameters t_int and t_thresh. Since these parameters play a critical role in introducing adaptive loss and updating the anchor points, their impact should be analyzed.\n\n3. This paper lacks comparative experiments involving ITI-GEN, as the design of its learning prompt is based on ITI-GEN (Lines 353-357)."
            },
            "questions": {
                "value": "Could you conduct an experiment or provide a theoretical proof for the statement: 'After a limited number of adaptation iterations using directional loss, the encoded concept in the adapted generator is already closer to the target domain than the encoded concept in the source generator'? This is an important assumption underlying your method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}