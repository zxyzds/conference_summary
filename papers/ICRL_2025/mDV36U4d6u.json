{
    "id": "mDV36U4d6u",
    "title": "Interpretable Table Question Answering via Plans of Atomic Table Transformations",
    "abstract": "Interpretability for Table Question Answering (Table QA) is critical, particularly in high-stakes domains like finance or healthcare.\nWhile recent Large Language Models (LLMs) have improved the accuracy of Table QA models, their explanations for how answers are derived may not be transparent, hindering user ability to trust, explain, and debug predicted answers, especially on complex queries.\nWe introduce Plan-of-SQLs (POS), a novel method specifically crafted to enhance interpretability by decomposing a query into simpler sub-queries that are sequentially translated into SQL commands to generate the final answer.\nUnlike existing approaches, \nPOS offers full transparency in Table QA by ensuring that every transformation of the table is traceable, allowing users to follow the reasoning process step-by-step.\nVia subjective and objective evaluations, we show that POS explanations significantly improve interpretability, enabling both human and LLM judges to predict model responses with 93.00% and 85.25% accuracy, respectively.\nPOS explanations also consistently rank highest in clarity, coherence, and helpfulness compared to state-of-the-art Table QA methods such as Chain-of-Table and DATER.\nFurthermore, POS demonstrates high accuracy on Table QA benchmarks (78.31% on TabFact and 54.80% on WikiTQ with GPT3.5), outperforming methods that rely solely on LLMs or programs for table transformations, while remaining competitive with hybrid approaches that often trade off interpretability for accuracy.",
    "keywords": [
        "Table QA; Interpretability; XAI; LLM-as-a-Judge"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We introduce POS, a Table QA method specifically designed for interpretability, decomposing complex queries into atomic natural-language sub-queries, which are then translated into SQL commands to sequentially transform input table into final answer.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mDV36U4d6u",
    "pdf_link": "https://openreview.net/pdf?id=mDV36U4d6u",
    "comments": [
        {
            "title": {
                "value": "Regarding ablation study for interpretability (continued)"
            },
            "comment": {
                "value": "As suggested by Reviewer `VQhj`, we conducted another ablation study on interpretability . \n\nIn our paper, explanations consist of attribution maps (highlights) combined with natural-language steps to (for POS) or functions (for DATER and CoTable) or SQL command (for Text-to-SQL). To assess the impact of attribution maps, we removed highlights in explanations and give to XAI judges.\n\nWe tested explanations with no highlights on three XAI tasks: Preference Ranking, Forward Simulation, and [Model Debugging](https://openreview.net/forum?id=mDV36U4d6u&noteId=6fDz8ayF1w) \n(an additional, more standard XAI evaluation). \nWe repeat the LLM-judge setups as described in our paper for GPT4o-mini \nand report the data below:\n\nTable: Evaluating Table QA explanations with no highlights (attribution maps).\n\n|     Task    | Preference Ranking (\u2193) | Forward Simulation (\u2191) | Model Debugging (\u2191) |\n|:-----------:|:----------------------:|:----------------------:|:-------------------:|\n| Text-to-SQL |          3.97          |         65.67%         |        55.37%       |\n|    DATER    |          2.64          |         73.86%         |        56.32%        |\n|   CoTable   |          1.76          |         75.94%         |        64.18%        |\n|  POS (Ours) |          1.62          |         81.81%         |        75.10%       |\n\nOur findings are:\n- Without highlights, `POS still performs best` across all three XAI tasks\u2014Preference Ranking, Forward Simulation, and Model Debugging.\n- The removal of highlights does not affect the accuracy/ratings for the explanation methods, suggesting that highlight-based explanations may not be important for interpretability in Table QA. This finding aligns with similar results in other domains, which indicate that highlights often add minimal value in explaining AI decisions [a, b]. This insight could guide future developments in XAI methods for Table QA.\n\nReferences:\n\n- [a] The effectiveness of feature attribution methods and its correlation with automatic evaluation scores, NeurIPS\n- [b] What i cannot predict, i do not understand: A human-centered evaluation framework for explainability methods, NeurIPS"
            }
        },
        {
            "title": {
                "value": "Regarding paper format"
            },
            "comment": {
                "value": "> The paper's presentation requires improvement, as some tables extend beyond the page margins\n\nThank you for bringing this to our attention! We will ensure that all tables are properly formatted within the page margins in the next revision.\n\nIf there are any further concerns, we would be more than happy to address them and continue the discussion. We are excited to engage in any way!"
            }
        },
        {
            "title": {
                "value": "Regarding the necessity of SQL-only constraints in the tasks"
            },
            "comment": {
                "value": "> The authors argue that it is better for interpretability to merely depend on SQL to process the table and answer the question. However, the necessity of this constraint is still questionable for the table qa in practical scenarios. The tables are of various formats, and the answer usually cannot be produced by a SQL query. Such limitation may explain the lower performance of PoS compared with many other baselines.\nAre there any cases where the answer cannot be generated by SQL queries?\n\nThank you for this great comment!\n\nYes, there are cases where the answer cannot be generated by SQL queries alone, particularly those involving long-form generation\u2014such as text summarization tasks in datasets like QTSumm [a]\u2014often relying on model \u201ccreativity\u201d to produce answers. For such datasets, relying solely on SQL is not optimal. We argue that combining the strengths of SQL for structured data handling with the creative reasoning capabilities of LLMs offer a more powerful approach. \n\nWe recognize that tables vary significantly in format, which can sometimes make SQL-based processing challenging [b]. However, by preprocessing samples into a SQL-compatible format (i.e., making them \"SQLifiable\"), we can achieve a significantly better interpretable decision-making process. For instance, the NormTab paper [c] demonstrates effective data cleaning techniques to prepare tables for SQL processing, showing that, after cleaning, applying naive methods like Text-to-SQL can boost accuracy on datasets like TabFact and WikiTQ by 5-10 points. This shows that POS can not only provide great interpretability but also accurate answers.\n\nWe also want to note that our primary focus in this work is on interpretability, which is why we emphasize SQL-based methods. For long-form generation, hybrid explanation methods (e.g. SQL + natural-language explanations) would be more promising and we leave this exploration for future work. We hope POS lays the groundwork for interpretability research in Table QA.\n- [a] QTSumm: Query-Focused Summarization over Tabular Data, EMNLP\n- [b] On the potential of lexico-logical alignments for semantic parsing to sql queries, EMNLP\n- [c] NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization, EMNLP"
            }
        },
        {
            "title": {
                "value": "Regarding the request to test POS on more LLM models"
            },
            "comment": {
                "value": "> Since this is an in-context learning method, the performance is greatly influenced by the choice of LLM backbones. The paper only uses gpt-3.5-turbo-16k-0613 which has already been deprecated by OpenAI. I recommend the authors choose more variety of models and use newer models. This would make the results more reliable and reproducible.\n\nThank you for the suggestion to test POS on the newer LLM backbones! \n\nThe reason we picked gpt-3.5-turbo-16k-0613 is because this LLM has \nbeen widely used by a long line of works (CoTable, Binder, DATER, \nTableSQLify) in Table QA. Using gpt-3.5-turbo-16k-0613 makes sure \nwe have a fair comparison between POS and baselines.\nGiven the deprecation of GPT3-5, we tested POS on GPT4o-mini and \nreport the numbers below:\n\nTable: Table QA accuracy of POS on TabFact and WikiTQ with GPT3-5 and GPT4o-mini\n\n|            |    TabFact    |        |     WikiTQ    |        |\n|:----------:|:-------------:|:------:|:-------------:|:------:|\n|   Method   | End-to-end QA |   POS  | End-to-end QA |   POS  |\n|   GPT3-5   |     70.45%    | 78.31% |     51.84%    | 54.80% |\n| GPT4o-mini |     71.17%    | 83.45% |     49.24%    | 54.95% |\n\nWe plan to add GPT4o and GPT4-turbo in the next version of the paper."
            }
        },
        {
            "title": {
                "value": "Regarding the effects in highlights in XAI comparison"
            },
            "comment": {
                "value": "> The XAI comparison is not very convincing. The highlight method is preferable for SQL-based methods such as PoS or Text-to-SQL. However, it is not explicit for other baselines without SQL, such as Dater and CoTable.\nDid you try to remove them in the experiments and see the results?\n\nWe find this comment very thought-provoking, and it indeed aided us in improving our XAI evaluation. \n\nWe understand that the highlight-based interpretability \nmethod may naturally align better with SQL-based approaches like POS and Text-to-SQL. \nFor non-SQL methods, such as Dater and CoTable, we agree that highlight-based interpretability \nmay be less advantageous compared to SQL-based methods.\n\nFollowing Reviewer `VQhj`'s suggestion, we removed highlights to assess which explanations \nperform best without the potential advantage that highlights may give to methods like \nText-to-SQL or POS.\n\nWe tested explanations with no highlights on three XAI tasks: Preference Ranking, Forward Simulation, and [Model Debugging](https://openreview.net/forum?id=mDV36U4d6u&noteId=3PACyF2oBr) \n(an additional, more standard XAI evaluation). \nWe repeat the LLM-judge setups as described in our paper for GPT4o-mini \nand report the data below:\n\nTable: Evaluating Table QA explanations with no highlights (attribution maps).\n\n|     Task    | Preference Ranking (\u2193) | Forward Simulation (\u2191) | Model Debugging (\u2191) |\n|:-----------:|:----------------------:|:----------------------:|:-------------------:|\n| Text-to-SQL |          3.97          |         65.67%         |        55.37%       |\n|    DATER    |          2.64          |         73.86%         |        56.32%        |\n|   CoTable   |          1.76          |         75.94%         |        64.18%        |\n|  POS (Ours) |          1.62          |         81.81%         |        75.10%       |\n\nOur findings are:\n- Without highlights, `POS still performs best` across all three XAI tasks\u2014Preference Ranking, Forward Simulation, and Model Debugging.\n- The removal of highlights does not affect the accuracy/ratings for the explanation methods, suggesting that highlight-based explanations may not be important for interpretability in Table QA. This finding aligns with similar results in other domains, which indicate that highlights often add minimal value in explaining AI decisions [a, b]. This insight could guide future developments in XAI methods for Table QA.\n\nReferences:\n\n- [a] The effectiveness of feature attribution methods and its correlation with automatic evaluation scores, NeurIPS\n- [b] What i cannot predict, i do not understand: A human-centered evaluation framework for explainability methods, NeurIPS"
            }
        },
        {
            "title": {
                "value": "Regarding the motivation of adding highlights in explanation"
            },
            "comment": {
                "value": "> What is the motivation to add highlights in the XAI experiments?\n\nThank you for this great question! \n\nHighlights (or attribution maps) are widely recognized as a key method for explaining \nAI decisions to humans across all domains, including images[a], text[b], and time series[c]. \nIn these domains, highlights effectively draw users\u2019 attention to the most relevant parts of \nthe data, helping them understand how the model arrived at its decisions.\n\n- [a] What i cannot predict, i do not understand: A human-centered evaluation framework for explainability methods, NeurIPS\n- [b] Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?, ACL\n- [c] Explainable AI for Time Series Classification, IEEE Access\n\nFor Table QA, there has been limited work visualizing model explanations in an interpretable way. \nTo address this gap, we adapted the highlight method from other domains for tabular data \nto help users understand how the model utilizes specific parts (features) of the input table \nto arrive at the final answer. By highlighting relevant rows, columns, or cells in the table, \nwe aim to provide a clear visual explanation of the model's reasoning process."
            }
        },
        {
            "title": {
                "value": "Regarding technical novelty of POS"
            },
            "comment": {
                "value": "> The novelty of the method is limited. Since the step-by-step execution has been already proposed in CoTable, this work seems a simple extension of CoTable with SQL queries and plan generation. And the final performance is even reduced with the added components.\n\nThank you for your comment!\nWhile POS shares similarities with CoTable in its step-by-step approach, \nwe believe our method introduces key innovations beyond CoTable. POS systematically decomposes \ncomplex queries into atomic, interpretable sub-steps and SQL-compatible transformations, \nproviding transparency that CoTable and other existing methods lack. Unlike traditional \nprogram-based approaches, which often struggle with unexplainable selections or function \narguments (see Fig. 1), POS allows each transformation to be fully traceable, making \nthe entire reasoning process interpretable.\n\nWe would like to emphasize that our goal is not to introduce a new Table QA method that \nmaximizes accuracy but rather to address critical interpretability gaps in current Table QA \nmethods. Interpretability in this domain has been largely overlooked, and POS aims \nto fill that gap.\n\nTo further support the interpretability frontier of POS, we conducted an additional evaluation where \nexplanation methods (Text-to-SQL, DATER, CoTable, and POS) help us identify whether the Table QA model \nis correct (model prediction debugging[c,d,e,f]). Model prediction debugging is the mainstream and established task in XAI and has \nmany real-world applications (e.g. healthcare[g]).\n\nUsing both the TabFact and WikiTQ datasets, we evaluated the following question: \nGiven the explanations, can users identify if the Table QA model is correct or wrong? \nWe leverage LLM-as-a-Judge as we did in Forward Simulation and Preference Ranking for this setup.\n\nHere is our prompt to the LLM judges:\n```\nThe Table Question Answering (Table QA) model is working on a tabular dataset, answering questions based on a given table.\n\nYou are given an HTML file containing a Question, Input Table, Prediction, and an Explanation clarifying the Prediction.\n\nYour task is to carefully analyze the explanation and determine whether the Prediction is correct or not.\n\nExplanation Method: [Text2SQL/DATER/CoTable/POS]\nAnswer with \u2018Correct\u2019 or \u2018Wrong\u2019 only.\n\nYou MUST ignore the order of the options and answer based on the correctness of the Prediction!\n```\n\nWe report the result of the evaluation below:\n\nTable: Model prediction debugging accuracy (&uarr;) on TabFact and WikiTQ with LLM judges using different Table QA explanations. \n\n|   Dataset   |   TabFact   |        |         |        | WikiTQ |        |\n|:-----------:|:-----------:|:------:|:-------:|:------:|:------:|:------:|\n|    Method   | Text-to-SQL | DATER  | CoTable |  POS   | DATER  |  POS   |\n| GPT-4o-mini |   55.37%    | 55.43% | 61.36%  | **76.74%** | 64.58% | **71.93%** |\n|    GPT-4o   |   55.97%    | 70.95% | 67.34%  | **72.85%** | 73.31% | **74.45%** |\n|    GPT-4    |   49.93%    | 57.56% | 60.38%  | **72.08%** | **73.5%**  | 72.38% |\n\nOur findings are:\n- POS significantly improves model prediction debugging accuracy compared to other Table QA baselines. \n- On the TabFact dataset, POS achieves a debugging accuracy of `76.74%` with GPT-4o-mini, `72.85%` with GPT-4o, and `72.08%` with GPT-4, outperforming other methods by margins of up to `21%`. \n- On WikiTQ, POS reaches the highest accuracy of `74.45` with GPT4-o. \n\nThese results again confirm that POS offers a distinctive interpretability advantage and usefulness compared to existing Table QA models.\n\nReferences:\n- [c] The effectiveness of feature attribution methods and its correlation with automatic evaluation scores, NeurIPS\n\n- [d] HIVE: Evaluating the Human Interpretability of Visual Explanations, ECCV.\n\n- [e] What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods, NeurIPS.\n\n- [f] Debugging Tests for Model Explanations, NeurIPS.\n\n- [g] Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation, ICLR."
            }
        },
        {
            "title": {
                "value": "We sincerely appreciate your reviews!"
            },
            "comment": {
                "value": "We sincerely appreciate Reviewer `VQhj`'s thoughtful reviews on our paper!\n\nYour comments, particularly regarding the `effect of highlights in XAI comparisons`, have provided us with valuable insights to improve our evaluation.\n\nBelow, we show how your insights helped us improve evaluation and humbly answer your questions as follows:"
            }
        },
        {
            "title": {
                "value": "Regarding Figure 6 issues"
            },
            "comment": {
                "value": "> 3. There appear to be issues with Figure 6. The function parameters in Step 1 are incorrect, and there is an unintended split in the image for Step 3.\n\nThank you for this detailed comment! \n\nIn Fig.6, we actually visualize exactly the row indices given by the Chain-of-Table method (i.e. row 1, row 2, row 3, row 4, row 8). In Chain-of-Table, it ignores the header row (column names), and start with index 0 (here refers to game 1). Please note that in human studies, we also explicitly told users that CoTable row indices start from 0 and we do not count the header row.\n\nHowever, we agree with the Reviewer that we can adjust these indices to make the visualization less confusing and aligned with other methods (e.g. DATER in Fig.5). Regarding the unintended split, this was an error in converting the visualization to PDF for insertion into Overleaf. We will address both issues in the next revision.\n\nThank you once again for the positive and constructive feedback!"
            }
        },
        {
            "title": {
                "value": "Regarding ablation study for interpretability"
            },
            "comment": {
                "value": "> 2. In Appendix C, the ablation study discusses changes in interpretability after removing different modules, but no quantitative metrics are provided to measure these changes. Could the authors include specific interpretability metrics to quantify the impact of each module?\n\nThank you for highlighting this point! \nWe would like to justify that the main goal of this ablation study is to study how different components affect POS QA accuracy, not interpretability. \nHowever, to address interpretability quantitatively, we can use the fallback rate as a proxy \nmetric. In our approach, fallback occurs when the model defaults to an end-to-end QA approach, \nwhich lacks interpretability (as shown in Fig. 1). Thus, a lower fallback rate indicates that \nPOS can solve more queries using SQL, providing a fully interpretable decision-making process.\n\nAs shown in Table 4, modules like NL Planning and Text-to-SQL significantly contribute to \ninterpretability, with their removal increasing the fallback rate to 40-50%. \nRemoving NL Planning results in SQL-only steps (in lieu of natural-language steps Fig. 7) will \ndiminish interpretability, as shown by lower performance of Text-to-SQL compared to POS in the Forward Simulation experiment (Table 1). \n\nSimilarly, without Text-to-SQL, LLM performs table transformations directly in natural language without using SQLs, which is less interpretable due to the inherent LLM black-box reasoning.\n\nWhen the Atomicity is ablated out, the fallback rate does not change much. To justify the change in interpretability, we visualized the plans generated by NL Planner in Fig.8 and Fig.9 and conducted a qualitative analysis (on 200 samples), which hinted that explanations were indeed less interpretable in this setup due to complex steps (see Fig.8 and 9\u2019s captions)."
            }
        },
        {
            "title": {
                "value": "Regarding POS efficiency analysis"
            },
            "comment": {
                "value": "> 1. Given POS\u2019s sequential nature and reliance on intermediate tables as inputs for each step, what are the efficiency implications?\n\nThank you for this valuable question!\n\nTo analyze the efficiency of POS, we followed the analysis used in the \nChain-of-Table paper (Sec 4.5), using the total number of LLM queries as the measure of efficiency.\n\nWhen comparing POS to other baselines, including Binder (Cheng et al., 2022), \nDater (Ye et al., 2023), and CoTable (Wang et al., 2024), we observe a distinct advantage: \nPOS does not rely on self-consistency (generating multiple candidate answers and selecting the most frequent or consistent one to improve accuracy) \nduring either the planning or SQL generation steps, \nwhereas all three baseline methods use self-consistency to boost accuracy. \nThis helps POS significantly reduce the number of LLM queries needed per sample (to only `4`), \nmaking POS the most efficient among these baselines. See the Table below.\n\nTable: Efficiency analysis on WikiTQ\n\n| Method                      | Self-consistency | LLM queries | Breakdown LLM queries                                                | Database queries |\n|-----------------------------|------------------|-------------|----------------------------------------------------------------------|------------------|\n| Binder (Cheng et al., 2022) | Yes              | 50          | Generate Neural-SQL: 50                                              | 50               |\n| Dater (Ye et al., 2023)     | Yes              | 100         | Decompose Table: 40; Generate Cloze: 20; Generate SQL: 20; Query: 20 | 20               |\n| CoTable (Wang et al., 2024) | Yes              | \u226425         | DynamicPlan: \u22645; GenerateArgs: \u226419; Query: 1                         | 5                |\n| Plan-of-SQLs (Ours)         | No               | 4           | Planning: 2 Generate SQL: 2                                          | 2                |\n\nAdditionally, we propose to compare the efficiency of these methods based on the number of \ntable transformations, as a higher count leads to increased workload on the local database \nwhen handling Table QA tasks (i.e., higher number of queries to the table database). \n\nIn this regard, POS also demonstrates a clear advantage over the baselines.\nIn particular, while methods like Chain-of-Table rely on a predefined set of `5` \noperations (e.g., add column, select column/row, group column, sort column) to maintain \nconsistency, POS requires only `2` database queries on average, making it the most efficient \napproach in terms of database queries. Similarly, Binder and Dater require \nsignificantly greater numbers of operations, with Binder doing table database queries `50` times \nand Dater sending slot-filling queries `20` times."
            }
        },
        {
            "title": {
                "value": "Thank You for Your Positive Feedback and Insightful Suggestions!"
            },
            "comment": {
                "value": "We sincerely thank the reviewer `mQDc` for the feedback on our work and for recognizing POS\u2019s contributions to interpretability in Table QA.\n\nWe particularly appreciate the suggestion regarding efficiency analysis, which has helped us better showcase the efficiency advantages of our method. \n\nBelow, we detail how your insights guided us in conducting this analysis and addressing your questions!"
            }
        },
        {
            "title": {
                "value": "Regarding the extension of evaluation benchmarks"
            },
            "comment": {
                "value": "> 4. Further experimentation across a broader range of datasets (ideally 5\u20136) would be needed to demonstrate POS\u2019s generalizability and robustness.\n\nThank you for this suggestion!\n\nWe would like to note that TabFact and WikiTQ are also the primary evaluation datasets \nin existing works, including our main baselines\u2014CoTable, Binder, Dater, and TableSQLify.\n\nDue to budget and time constraints, we are currently unable to extend POS testing to \nadditional datasets as suggested. The cost of running POS on TabFact and WikiTQ is \nparticularly high due to the large input tables and the in-context examples required \nfor each prompt, resulting in a high token usage. For instance, a single evaluation on \n\nTabFact or WikiTQ using step-by-step planning (in Question 1) incurred over `$600` in API costs. \nGiven these constraints, we prioritized \nTabFact and WikiTQ as they offer a robust and feasible evaluation within our available \nresources.\n\nIf there are any additional questions or further concerns, we would be more than happy to address them and continue the discussion. We are eager to engage in any way!\n\nThank you once again for your time and consideration."
            }
        },
        {
            "title": {
                "value": "Regarding the generalizability of in-context examples"
            },
            "comment": {
                "value": "> 3. The NL Atomic Planner in POS depends on in-context planning examples that are specifically tailored to each dataset. This raises questions about its adaptability to a variety of reasoning types and Table QA problems. It remains unclear whether the current prompting schema can generalize effectively across different Table QA tasks.\n\nThank you for the insightful comment!\n\nOur in-context planning examples were indeed crafted specifically for each dataset. \nTo explore the generalizability of our prompting schema, we modified the examples \nto remove any dataset-specific details. These revised, `general` in-context examples are \ndesigned to handle typical Table QA tasks, where the input to the NL Planner \nis simply the Query/Table and the output is the Plan.\n\nAn example of the new generalized in-context prompt is as follows:\n\n```markdown\nWe are working with a tabular dataset.\nYour task is to develop a step-by-step plan to answer a query on a given table.\n\n### Table: \n/* \ncol : id | name | score \nrow 1 : 1 | alice | 85 \nrow 2 : 2 | bob | 90 \nrow 3 : 3 | charlie | 75 \n*/ \nQuery: what is the score of alice? \nPlan: \n1. Select rows where the 'name' is 'alice'. \n2. Select the 'score' column.\n```\n\nWe ran POS again on both datasets using these generalized in-context examples. The results indicate that this change led to only a slight decrease in accuracy compared to dataset-specific examples, as shown in the table below:\n\nTable: Table QA accuracy (&uarr;) on TabFact and WikiTQ datasets using general in-context examples for planning.\n\n|                             Method                              | TabFact | WikiTQ |\n|:---------------------------------------------------------------:|:-------:|:------:|\n|                          End-to-end QA                          |  71.17% | 49.24% |\n|         POS with 5 general in-context planning examples         |  81.08% | 50.92% |\n|       POS with 30 (full) general in-context planning examples        |  81.92% | 52.00% |\n|   POS with 30 (full) dataset-specific in-context planning examples   |  83.45% | 54.95% |\n\nThese numbers confirm that POS remains competitive without a strong reliance on \ndataset-specific examples (`1.5%` \u2192 `3.0%` accuracy gaps). Additionally, we observed that the general in-context \nexamples led to low unprocessable rates, with TabFact rates changing from \n`3.16%` \u2192 `3.06%` and WikiTQ rates from `13.58%` \u2192 `14.48%`.\n\nFinally, it is also worth noting that reliance on tailored, dataset-specific examples is common across other methods, such as CoTable. \nTypically, general prompts (e.g., Meta-prompting[1]) yield slightly lower accuracy than dataset-specific prompts but improve generalizability, a trend that aligns with our findings.\n\n- [1] Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding"
            }
        },
        {
            "title": {
                "value": "Regarding POS technical contribution"
            },
            "comment": {
                "value": "> 2. The technical contribution of POS is limited, since the whole framework primarily involves prompt engineering. Essentially, compared to traditional program-based methods, POS simply adds an additional layer of prompts to generate natural language \u201cplans\u201d that function as pseudo-comments for SQL statements. This does not introduce substantial technical contribution beyond traditional program-based methods.\n\nThank you for your comment! While POS does indeed utilize prompt engineering, its design \nextends beyond simple prompt layering. The key innovation lies in its systematic decomposition \nof complex queries into atomic, **interpretable** sub-steps and table transformations\u2014a feature not found in \nexisting approaches. Unlike traditional program-based methods, which often struggle with unexplainable selections or function arguments (Fig. 1), \nPOS offers a fully transparent, step-by-step transformation process.\n\nWe would like to emphasize that our goal is not to introduce a new Table QA method that \nmaximizes accuracy but rather to address critical interpretability gaps in current Table QA \nmethods. Interpretability in this domain has been largely overlooked, and POS aims \nto fill that gap.\n\nTo further support the interpretability frontier of POS, we conducted an additional evaluation where \nexplanation methods (Text-to-SQL, DATER, CoTable, and POS) help us identify whether the Table QA model \nis correct (model prediction debugging[c,d,e,f]). Model prediction debugging is the mainstream and established task in XAI and has \nmany real-world applications (e.g. healthcare[g]).\n\nUsing both the TabFact and WikiTQ datasets, we evaluated the following question: \nGiven the explanations, can users identify if the Table QA model is correct or wrong? \nWe leverage LLM-as-a-Judge as we did in Forward Simulation and Preference Ranking for this setup.\n\nHere is our prompt to the LLM judges:\n```\nThe Table Question Answering (Table QA) model is working on a tabular dataset, answering questions based on a given table.\n\nYou are given an HTML file containing a Question, Input Table, Prediction, and an Explanation clarifying the Prediction.\n\nYour task is to carefully analyze the explanation and determine whether the Prediction is correct or not.\n\nExplanation Method: [Text2SQL/DATER/CoTable/POS]\nAnswer with \u2018Correct\u2019 or \u2018Wrong\u2019 only.\n\nYou MUST ignore the order of the options and answer based on the correctness of the Prediction!\n```\n\nWe report the result of the evaluation below:\n\nTable: Model prediction debugging accuracy (&uarr;) on TabFact and WikiTQ with LLM judges using different Table QA explanations. \n\n|   Dataset   |   TabFact   |        |         |        | WikiTQ |        |\n|:-----------:|:-----------:|:------:|:-------:|:------:|:------:|:------:|\n|    Method   | Text-to-SQL | DATER  | CoTable |  POS   | DATER  |  POS   |\n| GPT-4o-mini |   55.37%    | 55.43% | 61.36%  | **76.74%** | 64.58% | **71.93%** |\n|    GPT-4o   |   55.97%    | 70.95% | 67.34%  | **72.85%** | 73.31% | **74.45%** |\n|    GPT-4    |   49.93%    | 57.56% | 60.38%  | **72.08%** | **73.5%**  | 72.38% |\n\nOur findings are:\n- POS significantly improves model prediction debugging accuracy compared to other Table QA baselines. \n- On the TabFact dataset, POS achieves a debugging accuracy of 76.74% with GPT-4o-mini, 72.85% with GPT-4o, and 72.08% with GPT-4, outperforming other methods by margins of up to 21%. \n- On WikiTQ, POS reaches the highest accuracy of 74.45 with GPT4-o. \n\nThese results again confirm that POS offers a distinctive interpretability advantage and usefulness compared to existing Table QA models.\n\nReferences:\n- [c] The effectiveness of feature attribution methods and its correlation with automatic evaluation scores, NeurIPS\n\n- [d] HIVE: Evaluating the Human Interpretability of Visual Explanations, ECCV.\n\n- [e] What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods, NeurIPS.\n\n- [f] Debugging Tests for Model Explanations, NeurIPS.\n\n- [g] Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation, ICLR."
            }
        },
        {
            "title": {
                "value": "Regarding unprocessable rate of POS"
            },
            "comment": {
                "value": "> 1. The authors report that 9.8% of samples on TabFact and 27.8% on WikiTQ could not be fully processed using POS and thus defaulted to end-to-end QA methods. Even on relatively simple Table QA datasets like TabFact and WikiTQ, these rates are notably high, raising concerns about POS\u2019s scalability to more complex datasets, such as TabMWP. If the unprocessable rate rises significantly with more complex datasets, it raises the concern that POS may only improve interpretability with a sacrifice of precision provided in pure program-based methods. The authors do not provide enough analysis regarding this matter, such as a comparison of unprocessable rates between POS and other program-based models and the unprocessable rates on more complex table-QA datasets.\n\n\nThank you for the great feedback! \nAs Reviewer `j2Vr` noted, the high rate of unprocessable samples in POS impacts scalability, particularly with complex datasets.\n\nEncouraged by this feedback, we delved into all cases where samples were unprocessable by POS (9.8% on TabFact and 27.8% on WikiTQ) and identified that **one-time planning** (generating a complete plan of steps at once in the beginning) is the bottleneck.\nHere is an example where one-time planning made the sample unprocessable:\n\n**Query**: True or False? No games were played in **december**\n\nTable caption: 2011 final games in mlb\n\n| game_id |   game_date  | score | attendance | team_name |\n|:-------:|:------------:|:-----:|:----------:|:---------:|\n|    1    |  5 nov 2011  |   3   |    2000    |  falcons  |\n|    2    |  12 nov 2011 |   4   |    1500    |   hawks   |\n|    3    |  21 oct 2111 |   2   |    1800    |   eagles  |\n|    4    | 30 sept 2011 |   1   |    2200    |   bears   |\n|    5    |  20 nov 2011 |   3   |    2100    |  wildcats |\n\nPlan:\n\n1. Extract the month from the **date** column then add a new column.\n2. Select rows where **month = 12**.\n3. Use a **CASE** statement to return **TRUE** if the number of rows is **0**, otherwise **FALSE**.\n\nIn this example, the NL Planner generated Step 2 without seeing the output table of Step 1, which added a column with month names as text (e.g., \"dec\") instead of numeric values (e.g., \"12\"). \nAdditionally, there is no guarantee that the new column from Step 1 is named \"month,\" potentially causing Step 2 to reference a non-existent column.\nWe found `198` such cases in TabFact and `1207` in WikiTQ, where the one-time planning approach makes Table QA queries unprocessable.\n\nIn one-time planning (generating a complete plan of steps at once in the beginning), the NL Planner assumes that previous transformations have succeeded and produced the expected output. If any step fails (e.g., due to missing columns or data formatting issues), the process halts, leading to an unprocessable sample.\n\nWe solved this issue by `step-by-step planning`\u2014prompting the NL Planner to generate one step at a time (instead of generating a complete plan at once), with the input of NL Planner being the previous steps and the current intermediate table. \nAs shown in the tables below, step-by-step planning led to substantial improvements in processing accuracy and unprocessable rates for both TabFact and WikiTQ.\n\nTable: Table QA accuracy (&uarr;) on TabFact and WikiTQ datasets using step-by-step planning\n\n| Method                    | TabFact         | WikiTQ          |\n|---------------------------|-----------------|-----------------|\n| End-to-end QA             | 71.17%          | 49.24%          |\n| POS one-time planning     | 77.22%          | 48.90%          |\n| POS step-by-step planning | 83.45% (+6.23%) | 54.95% (+6.05%) |\n\n\nTable: Unprocessable rates (&darr;) on TabFact and WikiTQ datasets using step-by-step planning. \n\n| Method                    | TabFact       | WikiTQ         |\n|---------------------------|---------------|----------------|\n| POS one-time planning     | 10.56%        | 22.63%         |\n| POS step-by-step planning | 3.16% (-7.4%) | 13.58 (-9.05%) |\n\nUsing GPT4o-mini as the LLM backbone, we found that POS is able to process the vast majority \nof examples using SQLs only, reaching up to about `97%` on TabFact and `86%` on WikiTQ!\nIt is important to note that WikiTQ contains considerable data noise [a], thus achieving this processable rate using SQLs alone is non-trivial. \n\nAgain, we appreciate the Reviewer\u2019s insights, which guided us toward a more robust and scalable solution for handling the problem.\n\nRegarding the scalability of POS to more complex datasets like TabMWP [b]. After looking at the dataset, we acknowledge that TabMWP indeed presents additional challenges due to its complexity and the diversity of reasoning requirements. \nWe will ensure that our paper includes a discussion on the potential and limitations of scaling POS to handle more complex problems.\n\n- [a] On the potential of lexico-logical alignments for semantic parsing to sql queries\n\n- [b] Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"
            }
        },
        {
            "title": {
                "value": "Thank You for Your Thoughtful Review and Feedback!"
            },
            "comment": {
                "value": "We sincerely appreciate Reviewer `j2Vr`'s thoughtful feedback of our paper! Your comments, particularly on the `unprocessable rate` and the `generalizability` of in-context planning examples, provided valuable insights. We detail how your insights help us answer to your concerns below!"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Plan-of-SQLs (POS), an approach for enhancing interpretability in Table Question Answering. POS decomposes complex queries into atomic sub-queries, each translated into SQL commands executed sequentially. This design enables users to trace the decision-making process step-by-step. The authors claim that POS not only outperforms comparable Table QA methods in interpretability but also maintains competitive accuracy, with promising results on benchmarks such as TabFact and WikiTQ."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- POS is well-designed for interpretability by breaking down complex queries into simple, understandable natural language planning. This sequential process makes it easier for users to follow and verify each stage of the answer generation. \n- The authors employ both subjective and objective evaluations, involving human and LLM judges, to assess POS\u2019s interpretability, coherence, and helpfulness, providing strong evidence for POS's improvement on interpretability. \n- POS performs reasonably well on key benchmarks, achieving high interpretability without compromising accuracy."
            },
            "weaknesses": {
                "value": "- The authors report that 9.8% of samples on TabFact and 27.8% on WikiTQ could not be fully processed using POS and thus defaulted to end-to-end QA methods. Even on relatively simple Table QA datasets like TabFact and WikiTQ, these rates are notably high, raising concerns about POS\u2019s scalability to more complex datasets, such as TabMWP. If the unprocessable rate rises significantly with more complex datasets, it raises the concern that POS may only improve interpretability with a sacrifice of precision provided in pure program-based methods.  The authors do not provide enough analysis regarding this matter, such as a comparison of unprocessable rates between POS and other program-based models and the unprocessable rates on more complex table-QA datasets. \n\n- The technical contribution of POS is limited, since the whole framework primarily involves prompt engineering. Essentially, compared to traditional program-based methods, POS simply adds an additional layer of prompts to generate natural language \u201cplans\u201d that function as pseudo-comments for SQL statements. This does not introduce substantial technical contribution beyond traditional program-based methods. \n\n- The NL Atomic Planner in POS depends on in-context planning examples that are specifically tailored to each dataset. This raises questions about its adaptability to a variety of reasoning types and Table QA problems. It remains unclear whether the current prompting schema can generalize effectively across different Table QA tasks. Further experimentation across a broader range of datasets (ideally 5\u20136) would be needed to demonstrate POS\u2019s generalizability and robustness."
            },
            "questions": {
                "value": "Please refer to the weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents Plan-of-SQLs (POS), an interpretable Table Question Answering (Table QA) method that enhances model transparency by decomposing complex queries into atomic natural-language sub-queries. Each sub-query is sequentially converted into SQL commands, allowing the input table to be transformed step-by-step until the final answer is obtained. This decomposition approach ensures that every transformation is clear and traceable, providing a fully interpretable reasoning process. POS achieves competitive performance on standard Table QA benchmarks (TabFact and WikiTQ) while significantly improving clarity, coherence, and helpfulness compared to existing models such as CoTable and DATER."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-structured and written, with a logical flow that makes the methodology and experiments easy to follow. The proposed POS advances interpretability in Table QA by decomposing complex queries into atomic, natural-language sub-queries sequentially converted into SQL commands. The authors support their interpretability claims with comprehensive experiments, including both human and LLM evaluations, showcasing the clarity, coherence, and helpfulness of POS explanations."
            },
            "weaknesses": {
                "value": "While POS enhances interpretability, its innovation appears incremental, with its main contribution being the decomposition of queries into SQL-translatable atomic steps. Furthermore, POS shows limited accuracy improvements over existing models, which could diminish its attractiveness in applications where interpretability is less prioritized. Another concern is POS\u2019s reliance on sequential processing via the NL Atomic Planner, where each plan requires the previous step's results, including intermediate table states. This dependence on continuous LLM calls may lead to inefficiencies, particularly for complex or large tables, as each step adds computational overhead. Detailed efficiency comparisons with non-sequential methods or potential optimizations would strengthen the paper and address this limitation."
            },
            "questions": {
                "value": "1.\tGiven POS\u2019s sequential nature and reliance on intermediate tables as inputs for each step, what are the efficiency implications?\n2.\tIn Appendix C, the ablation study discusses changes in interpretability after removing different modules, but no quantitative metrics are provided to measure these changes. Could the authors include specific interpretability metrics to quantify the impact of each module?\n3.\tThere appear to be issues with Figure 6. The function parameters in Step 1 are incorrect, and there is an unintended split in the image for Step 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new in-context learning method for solving table-based QA, Plan-of-SQLs (PoS). The method first generates a sequence of atomic actions as the \"plan\" and translates the actions in the plan to simple SQL queries step by step. The final answer would be the result of the sequence of the translated SQL queries.\n\nThe author emphasizes the interpretability of the method by arguing all actions are taken through SQL queries and the output is the result of the SQL queries.\n\nIn terms of performance, the proposed method achieves comparable performance compared with baselines while is much better in interpretability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed method PoS learned many good merits from existing methods, including the plan generation, and step-by-step execution. It also proposed a novel point about the interpretability of Table QA. The explanation generation method is also novel where it highlights the related col/row/cells in the queries. This method is uniquely designed for Table QA."
            },
            "weaknesses": {
                "value": "1. Since this is an in-context learning method, the performance is greatly influenced by the choice of LLM backbones. The paper only uses gpt-3.5-turbo-16k-0613 which has already been deprecated by OpenAI. I recommend the authors choose more variety of models and use newer models. This would make the results more reliable and reproducible.\n\n2. The XAI comparison is not very convincing. The highlight method is preferable for SQL-based methods such as PoS or Text-to-SQL. However, it is not explicit for other baselines without SQL, such as Dater and CoTable. \n\n3. The authors argue that it is better for interpretability to merely depend on SQL to process the table and answer the question. However, the necessity of this constraint is still questionable for the table qa in practical scenarios. The tables are of various formats, and the answer usually cannot be produced by a SQL query. Such limitation may explain the lower performance of PoS compared with many other baselines.\n\n4. The novelty of the method is limited. Since the step-by-step execution has been already proposed in CoTable, this work seems a simple extension of CoTable with SQL queries and plan generation. And the final performance is even reduced with the added components.\n\n5. The paper's presentation requires improvement, as some tables extend beyond the page margins."
            },
            "questions": {
                "value": "1. Are there any cases where the answer cannot be generated by SQL queries? \n\n2. What is the motivation to add highlights in the XAI experiments? Did you try to remove them in the experiments and see the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Plan-of-SQLs (POS), a method for interpretable Table Question Answering that decomposes queries into simple, sequential SQL steps, ensuring full transparency in the reasoning process. POS enables users to trace how answers are derived, significantly improving interpretability over existing models. Through human and LLM evaluations, POS demonstrates strong clarity, coherence, and competitive accuracy on benchmarks like TabFact and WikiTQ, making it suitable for high-stakes applications demanding clear model explanations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-motivated, addressing the critical need for interpretability in Table QA, especially for high-stakes fields where transparency is essential.\n2. It includes a comprehensive evaluation, using both human and LLM judges to assess interpretability and predictive accuracy, showcasing POS\u2019s advantages in clarity, coherence, and overall effectiveness compared to existing methods."
            },
            "weaknesses": {
                "value": "1. **Limited Novelty**: While the paper addresses an important problem in Table QA, the core ideas, such as breaking down queries into atomic steps and using SQL transformations for interpretability, are established in other domains. This reduces the method's originality, as it largely adapts existing decomposition and programmatic interpretability approaches rather than introducing fundamentally new techniques specific to Table QA.\n\n2. **Limited Effectiveness and Scalability**: The performance improvements with POS are incremental, indicating only a modest boost in interpretability and accuracy over baseline methods. Additionally, the approach relies on SQL-based transformations, which may restrict its applicability in more dynamic or complex data environments where SQL alone may be insufficient. As a result, the method lacks clear potential for further application in other QA scenarios or for handling more complex reasoning tasks, limiting its scalability and broader impact."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}