{
    "id": "0PxLpVURTl",
    "title": "MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Masked Image Modeling Representations",
    "abstract": "We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. MIM-Refiner is motivated by the insight that strong representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple instance discrimination (ID) heads that are connected to different intermediate layers. In each head, a nearest neighbor ID objective constructs clusters that capture semantic information which improves performance on downstream tasks, including off-the-shelf and fine-tuning settings.\n\nThe refinement process is short and simple -  yet highly effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, sets a new state-of-the-art in linear probing (84.7\\%) and low-shot classification among models that are pre-trained on ImageNet-1K. MIM-Refiner efficiently combines the advantages of MIM and ID objectives, enabling scaling ID objectives to billion parameter models using relatively little compute. MIM-Refiner compares favorably against previous state-of-the-art SSL models on various benchmarks such as low-shot classification, long-tailed classification and semantic segmentation.",
    "keywords": [
        "self-supervised learning",
        "masked image modeling",
        "instance discrimination",
        "computer vision",
        "contrastive learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We introduce MIM-Refiner, a refinement process of MIM(masked image modeling)-models using an ensemble of instance discrimination heads attached at intermediate layers to leverage the best pre-trained MIM representations.",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=0PxLpVURTl",
    "pdf_link": "https://openreview.net/pdf?id=0PxLpVURTl",
    "comments": [
        {
            "comment": {
                "value": "**Pixel-level representation vs DINOv2**\n\nDINOv2 is an excellent vision foundation model that has demonstrated outstanding performances across various tasks. However, at its core, DINOv2 is a scaled up version of iBOT which uses a dataset of 142M curated dataset of high-quality images. Notable, the dataset is created by retrieving images that are similar to those used for, e.g., semantic segmentation (ADE20K, Cityscapes, Pascal VOC) from an extremely large web-crawled image collection. This dataset curation procedure, together with the patch-level objective of iBOT allows DINOv2 to learn strong pixel-level representations. However, it can not be understated that an essential contributing to this performance is the private web-scale dataset that is highly curated which makes a fair comparison against DINOv2 impossible, as a model pre-trained on 100x more data will obviously outperform a model trained on ImageNet-1K in most cases. The closest comparison to DINOv2 is to compare against the models published by iBOT as their underlying training methodology is identical. We compare against iBOT on all benchmarks where we outperform it by large margins in all settings, including ADE20K segmentation with a frozen encoder (Table 4) and full fine-tuning on ADE20K segmentation (Table 5) using an UperNet semantic segmentation head.\n\nDue to these insights, together with the demonstrated scalability of MIM for billion-scale datasets and model sizes [4], we hypothesize that MIM-Refiner can scale way beyond ImageNet-1K, potentially even outperforming DINOv2 due to highly efficient pre-training. To put it into perspective, [4] trained MAE models up to 6.5B parameters whereas the largest DINOv2 model is 1.1B parameters. MIM-Refiner could leverage the efficient pre-training of MAE to train a 6.5B parameter model followed by a short refinement process on, e.g., the curated DINOv2 dataset which would require a fraction of the compute it would take to train a 6.5B DINOv2 model.\n\n\n[4] Singh 2023, \"The effectiveness of MAE pre-pretraining for billion-scale pretraining\" https://arxiv.org/abs/2303.13496\n\n**Pixel-level representation preservation under frozen backbone**\n\nWe evaluate the performance of MIM models and their refined versions on the pixel-level task of ADE20K semantic segmentation with a frozen encoder in Table 4. The refined models show significant gains in mIoU over their unrefined counterparts across all model sizes. Table 15 in the appendix confirms these results on many more models."
            }
        },
        {
            "comment": {
                "value": "We appreciate your helpful review, particularly the extensive look into related work and are happy to see that our paper was well received and easy to follow. We address your concerns below.\n\n**Conceptual differences to CAE**\n\nThe CAE approach is well motivated by their intuition to decouple representation learning from learning the pre-training task where they demonstrate the validity of this intuition via experimental evaluation of their proposed pre-training approach. However, they do not conduct an extensive analysis of pre-trained features or leverage any pre-trained representations, which are two key contributions of our paper. Additionally, we show in Figure 7f of the appendix that CAE also faces degrading feature representation in later blocks, suggesting that their approach alleviates but not fully solves the representation degradation. Contrary, Figure 10b of the appendix shows that refined models achieve peak representation quality in the last block without any feature degradation. We would therefore consider CAE and MIM-Refiner to be orthogonal where CAE presents an improved pre-training method that could also be refined for even better representation. However, as the largest CAE model is a ViT-L/16 and our focus is on large-scale models, we focus on MIM methods that published even larger models.\n\n**Full-finetuning and dense prediction**\n\nOur work builds on the insights of sequential MIM -> ID pre-training methods [1, 2] (as discussed in the related work section) to refine MIM models. These related works do not show improvements or even degradation of full fine-tuning and dense prediction tasks. However, our proposed improvements over previous approaches greatly improve representation quality and, consequently, we do not find our models to suffer from these issues. We show this in Table 5, where our models are on-par or slightly better in full fine-tuning settings with large amounts of data. While we agree that refinement with an ID objective is unlikely to bring major performance gains in full fine-tuning with large amounts of labels, we want to stress that this is the strong suit of MIM models where MIM models show state-of-the-art results. MIM-Refiner therefore heavily improves MIM models in a plethora of benchmarks while preserving (or even slightly improving) upon their state-of-the-art full fine-tuning performances. A visual depiction thereof is provided in Figure 2 left, where MIM-Refiner effectively unifies the advantages of MIM and ID.\n\nAdditionally, it has been demonstrated that MAE is exceptionally good at COCO object detection and instance segmentation, where training a pre-trained MAE further via weakly-supervised training on a web-scale dataset of 3 billion images even degraded performance vs a plain ImageNet-1K pre-trained MAE by 0.7 AP [3]. If not even 3B additional images can boost object detection performance, we do not expect a significant performance gain from our extremely short refinement process that does not add additional data.\n\nNevertheless, we agree that it is important to investigate whether or not the refinement degrades performance on COCO object detection and instance segmentation. We therefore conduct experiments in the setting suggested by reviewer HaAb where we train MAE and MAE-Refined with a Mask R-CNN head using the ViTDet framework on COCO. The results suggest that the refinement process preserves the representation quality also for object detection and instance segmentation downstream tasks. We show results below and added them to the paper (Appendix B.10), together with the above discussion.\n\n\n\n| Model | AP$^\\text{box}$ | AP$^\\text{mask}$  |\n|---|---|---|\n| MAE | **53.5** | **47.8**  |\n| MAE-Refined | **53.5** | 47.7  |\n\n\n[1] Lehner 2023, \"Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget\" https://arxiv.org/abs/2304.10520\n\n[2] Jiang 2023, \"Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations\" https://openreview.net/forum?id=jwdqNwyREyh\n\n[3] Singh 2023, \"The effectiveness of MAE pre-pretraining for billion-scale pretraining\" https://arxiv.org/abs/2303.13496"
            }
        },
        {
            "comment": {
                "value": "Thank you for your valuable review and suggestions to further strengthen the practicality of our approach. We are happy that the storyline of our paper was well understood and appreciated. We respond to your questions below.\n\n**Overhead of queue**\n\nWe use a queue size of 65K where, notably, the queue operates within the bottleneck dimension (256 for all model sizes) of the contrastive head.\nThe topk NN is found by calculating the cosine similarity for a given sample with all 65K queue entries, and then randomly selecting one of the top k entries in the similarity matrix. Additionally, no gradients flow through the NN-swap so it does not add overhead to the backward pass. These considerations, together with the fact that we train large models lead to a minor overhead from the NN queue. We compare runtimes without a queue, with a top1 NN-swap and with a top20 NN-swap below, which we also included into the paper (Appendix B.17).\n\n| Queue size | topk | L/16 | H/14 | 2B/14  |\n|---|---|---|---|---|\n| 0 | -  | 12.8s | 30.1s | 76.7s  |\n| 65K | 1  | 12.9s | 30.2s | 76.9s  |\n| 65K | 20 | 13.0s | 30.5s | 77.5s  |\n\n\n**Evaluation on dense prediction tasks**\n\nDense prediction tasks are an important area of computer vision where MIM models have demonstrated exceptional performance. We chose ADE20K semantic segmentation as benchmark for dense downstream tasks as it has established protocols for evaluation via a linear probe (Table 4) and full fine-tuning with feature pyramids and a segmentation head (Table 5) where MIM-Refiner shows strong improvements in linear probing and slight improvements in full fine-tuning. Additionally, it has been demonstrated that MAE is exceptionally good at COCO object detection and instance segmentation, where training a pre-trained MAE further via weakly-supervised training on a web-scale dataset of 3 billion images even degraded performance vs a plain ImageNet-1K pre-trained MAE by 0.7 AP [1]. If not even 3B additional images can boost object detection performance, we do not expect a significant performance gain from our extremely short refinement process that does not introduce additional data.\n\nNevertheless, we agree that it is important to investigate whether or not the refinement degrades performance on COCO object detection and instance segmentation. We therefore conduct experiments in the suggested setting where we train MAE and MAE-Refined with a Mask R-CNN head using the ViTDet framework on COCO. The results suggest that the refinement process preserves the representation quality also for object detection and instance segmentation downstream tasks. We show results below and added them to the paper (Appendix B.10), together with the above discussion.\n\n\n\n| Model | AP$^\\text{box}$ | AP$^\\text{mask}$  |\n|---|---|---|\n| MAE | **53.5** | **47.8**  |\n| MAE-Refined | **53.5** | 47.7  |\n\n\n[1] Singh 2023, \"The effectiveness of MAE pre-pretraining for billion-scale pretraining\" https://arxiv.org/abs/2303.13496\n\n\n**Representation degradation in other modalities**\n\nWe find it an interesting avenue to explore and will follow-up shortly, as we do not have a setup for the other D2V2 modalities ready-to-go and focused on getting timely object detection result to facilitate discussion.\n\nHowever, we did have a pipeline for VideoMAE [2] and AudioMAE [3] ready-to-go, which shows a similar trend, interestingly enough, also for smaller models. We include these preliminary result Appendix E.\n\n[2] Tong 2022, \"VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training\" https://arxiv.org/abs/2203.12602\n\n[3] Huang 2022, \"Masked Autoencoders that Listen\" https://arxiv.org/abs/2207.06405"
            }
        },
        {
            "comment": {
                "value": "**Clarification of training diagram in Figure 4**\n\nWe aim to depict the training pipeline of MIM-Refiner in Figure 4 by showing the differences between ID, MIM and MIM-Refiner with the goal that the \"copy\" lines should indicate that MIM-Refiner starts from a pre-trained MIM model where multiple ID heads are attached at later blocks instead of only a single ID head at the last block.\n\nAdditionally, due to reviewer Auz1's feedback, we added more details to the introductory paragraph of Section 4, which hopefully also clarifies the overall training methodology if it was not clear from Figure 4.\n\nWe regret that this was not found to illustrate our method clearly. We hope the additional information clarifies our experimental setup and otherwise would be keen to hear more details about what information is missing or was not clearly presented in the figure."
            }
        },
        {
            "comment": {
                "value": "Thank you for your review and helpful comments to help us improve the paper. We address your points individually.\n\n**End-to-end training of ID and MIM**\n\nMIM and ID objectives are quite conflicting where, roughly speaking, MIM considers every pixel/patch to be equally important, as the whole image needs to be reconstructed, while ID only cares about distinguishing positive and negatives in a batch of samples, which implicitly weights pixels/patches by their information content. This conflict also results in vastly different hyperparameter choices that are required for optimal performance. For example, MIM models use little image augmentations (only cropping and resizing but no color augmentations) but high masking ratios (75\\%). Contrary, ID models use sophisticated image augmentation pipelines with different augmentation strengths per view, color augmentations and multi-crop augmentation but only small masking ratios (e.g, 25\\% for iBOT/DINOv2). These findings were highlighted by two related works [1, 2] (as cited in Section 5.2) which independently came to the conclusion that a sequential training can effectively alleviate this conflict.\n\nAlso in terms of scalability, MIM models scale extremely well to large model sizes, whereas ID models require much more compute and data. As larger models typically perform better, it is desirable to develop scalable approaches. Attempts to develop end-to-end combinations of MIM and ID introduce large compute overheads due to, e.g., target networks or lower masking ratios, which heavily limits scalability. This is evident by the fact that these methods have not been trained on model scales beyond ViT-L, where the largest model of most methods is a ViT-B. Prominent examples are [3, 4, 5]. In contrast, our sequential approach can effortlessly scale up to a 2B parameter model, on a relatively small compute budget, by leveraging the compute efficiency of MIM models.\n\nWe also show that end-to-end MIM and ID combinations do not fully leverage the potential of the ID objective in Appendix B.15, where refining a CMAE-B model with our proposed methodology also improves performance despite the fact that an ID objective was already used in combination with the MIM objective in pre-training.\n\nAdditionally, MIM-Refiner models have strong off-the-shelf performances, so while MIM-Refiner requires a multi-stage pre-training, it makes downstream training much easier where simple models like a $k$-NN classifier or a linear probe reach somewhat comparable results to fully fine-tuning a model. For example, D2V2-Refined-H achieves 84.7\\% with a simple linear probe on ImageNet-1K classification that can be easily trained on a single GPU. Fine-tuning the same model boosts this performance by 2.1 \\% while requiring much more compute, necessitating multi-GPU or even multi-node training setups.\n\nTo summarize, the fundamental differences of MIM and ID require trade-offs in an end-to-end setting and we therefore opt for a sequential approach. While this increases the complexity of the training pipeline, multi-stage pre-training pipelines are somewhat common, and, since pre-training has to be done only once before a model can be fine-tuned (or simply evaluated) on a broad range of downstream tasks, multi-stage pre-training pipelines do not drastically decrease practicality. For example, in language modeling (e.g. [6]), it is very common to \"refine\" models after pre-training to integrate alignment with human preferences, reasoning, long context understanding, tool usage or safety guards. These multi-stage pre-training pipelines in language models are common because their benefit outweighs the added complexity. We would argue that this also holds true for MIM-Refiner, where the benefits of broader adaptability to many more use cases outweighs the additional pre-training pipeline complexity.\n\n\n[1] Lehner 2023, \"Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget\" https://arxiv.org/abs/2304.10520\n\n[2] Jiang 2023, \"Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations\" https://openreview.net/forum?id=jwdqNwyREyh\n\n[3] Huang 2022, \"Contrastive Masked Autoencoders are Stronger Vision Learners\" https://arxiv.org/abs/2207.13532\n\n[4] Zhou 2021, \"iBOT: Image BERT Pre-Training with Online Tokenizer\" https://arxiv.org/abs/2111.07832\n\n[5] Assran 2022, \"Masked Siamese Networks for Label-Efficient Learning\" https://arxiv.org/abs/2204.07141\n\n[6] Dubey 2024, \"The Llama 3 Herd of Models\" https://arxiv.org/abs/2407.21783"
            }
        },
        {
            "comment": {
                "value": "Thank you for your profound review and suggestions that helped us to improve our paper. We are glad that the analysis part was found to be a nice read. We corrected the typos and address your points below.\n\n**Clarifications on experimental setup**\n\nWe agree that the most important hyperparameters should be presented in the main text. Therefore, we restructured the beginning of Section 4 to include an overview of the experimental setting. Your comment also made us aware that we never referred to the full set of implementation details, as included in Appendix C, including how many blocks are finetuned (C.5 Table 22), how many epochs are needed for different models (C.5 Table 22) or the ID head structure (C.4).\nWe fixed this oversight by appropriately referring to it in Section 4.\n\n\n**Comparison against prolonged MIM training**\n\nWe provide a comparison similar to the proposed one already within our paper, where CrossMAE-L is pre-trained for only 800 epochs (due to computational resource restrictions of the authors), while MAE-L is pre-trained for 1600 epochs. One can clearly see in, e.g., Table 10 or Table 14 of the appendix that CrossMAE-Refined outperforms MAE, where CrossMAE-Refined is pre-trained for 800 epochs followed by 30 epochs of refinement while MAE is pre-trained for 1600 epochs.\n\nWhile we see that this is not a perfect comparison due to the differences in the decoder between MAE and CrossMAE, we believe that it provides sufficient evidence to underline the effectiveness of our method. Unfortunately, a direct comparison by prolonging the training of a pre-trained MIM model has multiple issues as we will outline below.\n\nFirst, MIM models are commonly pre-trained using a cosine annealing learning rate schedule, which means that for a proper comparison, one would need to train the whole model from scratch with a higher epoch count, which is extremely expensive (note that we simply downloaded pre-trained MIM checkpoints and never needed to train one from scratch). During the later epochs of the cosine annealing schedule, the model is updated with tiny learning rates. If one would use the pre-trained model and start training it for some more epochs, using a standard warmup into cosine annealing schedule and the same pre-training objective, it would essentially destroy the updates of the last few epochs due to increasing learning rate, before conducting the \"same\" updates again once the learning rate decreases again.\n\nSecond, often times, only the ViT encoder is published (without the decoder), so prolonging the training would require some sort of mechanism to initialize the decoder as starting the training with a randomly initialized decoder would most likely degrade the encoder features until the decoder has learned to produce sensible reconstructions.\n\nThird, MIM models are often trained for many epochs. For example, MAE models are trained for 1600 epochs. It is highly unlikely that prolonging this training by, e.g., another 50 epochs, will significantly change the performance of the model as performance gains tend to saturate.\n\nLastly, the number of epochs for MIM pre-training is often optimized as a hyperparameters. For example, in data2vec 2.0 the amount of epochs is decreased with model size, which suggests that larger models converge faster. Therefore, adding additional epochs could also degrade performance due to overfitting. While compute costs could be another explanation why the number of epochs was decreased with model size, we find the overfitting explanation more realistic as D2V2 is extremely compute efficient and D2V2-B or D2V2-L could have easily been trained longer as their compute budget also included training of a D2V2-H model. Therefore, prolonged training of smaller models would not have significantly impacted the total compute budget.\n\n\n**Clarification on \"relative\" in Figure 3d**\n\nYour conclusion is correct, we calculate the relative improvement by subtracting the performance of block i + 1 from the performance of block i. Additionally, we divide by the maximum relative improvement to put both performance metrics (k-NN accuracy and reconstruction loss) into the same value range with 1 as upper bound. We included the methodology to calculate the relative improvement into the caption of Figure 3 and expanded the description in Appendix D.2."
            }
        },
        {
            "title": {
                "value": "General Response"
            },
            "comment": {
                "value": "We thank all reviewers for their positive feedback, constructive comments and suggestions.\nWe are pleased to see that the reviewers highlighted the clarity of our paper and appreciated our analysis as well as our extensive experiments. Several reviewers recognized the clear motivation of our approach from the thorough analysis.\n\nWe updated the paper to incorporate the feedback of the reviewers. To summarize, we made the following changes:\n\n- Added more details to the experimental setup in the main paper (first paragraph in Section 4) such as training duration and included a reference to the extensive implementation details and hyperparameters in the appendix (reviewer Auz1).\n- Included methodology to calculate the relative improvement in Figure 3d also in the caption instead of only in the appendix (reviewer Auz1).\n- Added results for COCO object detection and instance segmentation in Appendix B.10 (reviewer HaAb and JDMo)\n- Added preliminary analysis of feature degradation of masked pre-training in other modalities in Appendix E (reviewer HaAb).\n- corrected various typos\n\nAdditionally, we respond to each review individually, addressing the raised questions and concerns."
            }
        },
        {
            "summary": {
                "value": "The paper focuses on bridging the gap between large MIM pre-trained models and SOTA methods. The paper first discovers that  MIM models have different types of blocks: those that mainly improve the pre-training objective and others that are responsible for abstraction. Then, the paper proposes a method MIM-Refiner, which adds Instance Discriminator heads on pre-trained MIM models for refinement. The ID heads exploit the intermediate representations to consistently improve the performance of MIM pretrained models. While the performance gains on large dataset full-finetuning are small, the proposed methods show remarkable gains on few-shot settings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper first points out the influence of the lightweight decoder on the feature learning of the encoder in MIM methods.\n2. The analyzing part is well-written."
            },
            "weaknesses": {
                "value": "1. The description of the method and experimental setup needs to be clarified. (a) Which blocks need to be fine-tuned during refinement, or do all blocks need to be fine-tuned? (b) How many epochs are needed to refine different models? (c) What is the structure of the ID head? Answers to all these questions should be contained in the manuscript.\n2. Unfair comparison. The paper misses an important baseline - train the original model with 0 heads with the same epochs to demonstrate the importance of refinement (instead of just training more epochs).\n3. Some typos. L267-269, see Table 1 instead of Figure 3b."
            },
            "questions": {
                "value": "Please refer to the weakness. I believe a clear description of the method and experimental setup is one of the most important things when writing a paper (weakness 1).\n\nAdditional question: what does the \u201crelative\u201d in Figure 3(d) mean? Does the value calculated by the performance of ( the i+1 th layer - the i-th layer)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a contrastive learning boosting method called MIM-Refiner to refine the features of pre-trained MIM models. MIM-Refiner leverages multiple instance discrimination heads (ID) which are connected to different immediate layers. Each ID head contains a contrastive loss that captures semantic information to improve the quality of learned representations. By training a few epochs, the features of MIM-Refiner surpass the current MIM models on multiple experiments: on ViT-H with data2vec 2.0 on ImageNet-1K, the accuracy of the proposed method reaches state-of-the-art performance on linear probing and low-shot classification."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper proposes a detailed analysis of the blocks of MIM models in which different blocks extract features with a specific focus and the most efficient features learned by MIM are from the middle blocks.\n\n2. A contrastive learning-based method called MIM-Refiner is proposed to refine the representation of current MIM models by attaching the middle layers with ID objective. \n\n3. Experimental results show the effectiveness and generalization ability of MIM-Refiner on downstream tasks."
            },
            "weaknesses": {
                "value": "1. As the discussion of end-to-end training, the proposed method MIM-Refiner seems to be a two-step training method, with first step training MIM models and fine-tuning the updated models by incorporating ID heads to middle layers. Practically, this might increase the complexity of the training paradigm and deployment. Is it possible to improve the proposed method with end-to-end training on MIM and ID? If not, what are the potential bottlenecks to circumvent this goal?\n\n2. There is no overview diagram that shows the detailed architecture of MIM-Refiner or how the training diagram goes. The diagram in Figure 4 provides partial information but does not clearly illustrate these points."
            },
            "questions": {
                "value": "Please refer to weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "---\n\n## **Summary**\n\nThe paper identifies the representation degradation issue in Masked Image Modeling (MIM)-pretrained large foundation models. To address this, the authors propose a simple yet effective method to prevent degradation and further improve the representation quality of MIM methods by adding auxiliary contrastive losses to the last layers of Vision Transformers (ViTs) on top of the MIM objective. The paper provides improved performances with large margins over current state-of-the-art (SOTA) methods through extensive experiments and rigorous analysis, demonstrating the success of the proposed approach.\n\n---"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "---\n\n## **Strengths**\n\n- The paper is well-written, with clear observations, a well-developed motivation, a straightforward idea, a clearly-stated method, extensive experiments, and comprehensive analysis.\n  \n- It effectively identifies the representational degradation phenomenon in large visual foundation models pre-trained with MIM self-supervised learning (SSL), providing evidence through multiple experiments.\n  \n- The proposed method offers a simple and effective solution to prevent this issue and improve the representation quality of MIM SSL.\n\n- Rigorous experiments and analysis are conducted to show the success of the proposed method, with large improvements over current SOTA.\n\n---"
            },
            "weaknesses": {
                "value": "---\n### **Limitations**\n\n1. To prevent representation quality degradation in the last layers of ViTs, the authors experiment with contrastive loss, which requires constructing a queue/pool for positive and negative samples. I noticed the proposed method uses a top 20-NN approach to retrieve positive samples in the queue, which could contribute significantly to the increased training time per step. What's the queue size used? how much does it contribute to the increased training time per step?\n\n2. Since the paper emphasizes preserving the richness of representations, evaluation on dense prediction tasks such as object detection and instance segmentation (OD/IS) would be valuable, in addition to the provided segmentation probing on ADE20K.\n\n   - It would be meaningful to compare the performance of MIM-refiner-pretrained ViT-L on COCO object detection against MAE-pretrained ViT-L following the ViTDet framework [1].\n\n   - [1] Li, Y., Mao, H., Girshick, R., & He, K. (2022, October). *Exploring plain vision transformer backbones for object detection.* In European Conference on Computer Vision (pp. 280-296). Cham: Springer Nature Switzerland.\n\n---\n\n### **Recommendation**\n\nConsidering the strengths and weaknesses discussed above, my recommendation for this paper is **ACCEPT**. This is a strong paper with a clear contribution."
            },
            "questions": {
                "value": "---\nSince D2V2 is used as a baseline, does the representation degradation issue also appear in the audio and language domains?\n\n---"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces MIM-Refiner, which leverages contrastive learning to boost MIM models. The proposed method is simple and has demonstrated effectiveness in few-shot image classification tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1: This paper is well-written and easy to follow.\n\nS2: This paper is not the first to point out that the encoder in MIM methods partially performs image encoding and representation learning. A similar conclusion is also discussed in [A], highlighting that MIM methods using a single ViT structure tend to face this issue. The reviewer previously conducted experiments on MAE-B, showing that introducing an additional decoder can effectively alleviate this problem. This paper demonstrates that, for methods like MAE that use an asymmetric encoder-decoder architecture\uff0c especially in larger models\uff0c a small decoder cannot fully decouple encoding and decoding, providing academic insights.\n\nS3: This paper proposes a simple and effective MIM-Refiner method, refining the later blocks of MIM models to enhance MIM representations effectively.\n\n[A] Context Autoencoder for Self-Supervised Representation Learning."
            },
            "weaknesses": {
                "value": "W1: Existing work [A] has shown that fine-tuning MIM models can enhance their representation capability (for image classification), but the improvement under full fine-tuning is minimal. Additionally, MAE has demonstrated significant transfer performance on dense prediction tasks [B] (object detection/instance segmentation). Fine-tuning MIM models with contrastive learning methods is unlikely to bring substantial improvement and may even negatively impact performance.\n\nW2: Current vision foundation models, such as DINOv2, exhibit strong patch-level representation learning capabilities and combine MIM and CL. Their learned representations have shown effectiveness in tasks like image classification, pixel classification, and depth estimation. Although this paper discusses the relationship between MIM-Refiner and these models, suggesting that MIM-Refiner can build on them for further improvement, I am concerned that MIM-Refiner may degrade pixel-level representation performance for tasks like semantic segmentation or depth estimation (especially when the backbone is fixed).\n\n[A] Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label Efficient Representations.\n\n[B] Exploring Plain Vision Transformer Backbones for Object Detection."
            },
            "questions": {
                "value": "see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}