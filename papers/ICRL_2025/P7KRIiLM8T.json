{
    "id": "P7KRIiLM8T",
    "title": "u-$\\mu$P: The Unit-Scaled Maximal Update Parametrization",
    "abstract": "The Maximal Update Parametrization ($\\mu$P) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u-$\\mu$P, which improves upon $\\mu$P by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: $\\mu$P ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u-$\\mu$P models reaching a lower loss than comparable $\\mu$P models and working out-of-the-box in FP8.",
    "keywords": [
        "maximal update parametrization",
        "learning dynamics",
        "hyperparameter transfer",
        "efficiency",
        "training",
        "stability",
        "scaling",
        "numerics",
        "fp8",
        "low precision"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We improve \u00b5P by combining it with Unit Scaling, leading to a simpler scheme with better default hyperparameters, lower loss, more efficient sweeping and simple FP8 training.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=P7KRIiLM8T",
    "pdf_link": "https://openreview.net/pdf?id=P7KRIiLM8T",
    "comments": [
        {
            "title": {
                "value": "re"
            },
            "comment": {
                "value": "it is more the transfer discrepancy; \nembedding lr is one key part; \nin addition, the og mup paper seems to have cleaner transfer for depth (fig 4 in your paper vs fig 4 in mup paper), batch size (fig 4 your paper vs fig 19 mup paper) than what is here. ( # training steps drifts a little);"
            }
        },
        {
            "comment": {
                "value": "Thanks a lot for your reply and asking a clarifying question! Yes, that is an illustration of what I meant. Essentially, for a large LR, the 10x final LR would also be relatively large, and therefore the final loss worse than if it were decayed more; this could change the optimal LR (e.g. for the transfer). I think it's a pretty intuitive argument and I've mostly seen it in practice and not many papers. (Though there is actually another active submission that looks at exactly this: https://openreview.net/forum?id=hrOlBgHsMI -- I'm neither an author or a reviewer there, just happened to see it right before you posted your reply :D)"
            }
        },
        {
            "comment": {
                "value": "Thanks for providing us with feedback, we appreciate the care taken when reading the paper and writing your response. We wanted to clarify one point to make sure we address the intent of your question:\n\nRegarding your comment \"the results of the paper seem to contradict some results of the original mup paper\" - did you have something particular in mind? There are several points where our message differs a little from that of the original mup paper: the changed embedding LR scaling rule, our argument for the need for independent weight decay \\& non-parametric norms, our results showing worse transfer results for regular mup than suggested by the original paper.\n\nWas your suggestion about us summarizing all of these points, or just e.g. the transfer results discrepancy?"
            }
        },
        {
            "comment": {
                "value": "Thanks very much for your feedback, we really appreciate the detail you're provided to make the paper better. We have a small clarifying question we wanted to ask before we write our response:\n\nYou mention that a fixed LR decay percentage can skew results - this wasn't something we were aware of, but in exploring this question we came across Figures 21 (right) & 22 in [H\u00e4gele et al. (2024) ](https://arxiv.org/abs/2405.18392) which support your suggestion. We were wondering if these were the results you had in mind, or if there are others? We'll attempt to evaluate our method in light of these results if time permits."
            }
        },
        {
            "summary": {
                "value": "The authors present a combination of the maximal update parametrization (muP) and unit scaling, coined u-muP. It brings together the two main ideas of 1) hyperparameter transfer and 2) the ideal principle of unit variance of activations, weights, and gradients. The authors implement this idea with decoder language models, showing both the transfer of parameters, the performance, as well as scaling to 7B models and FP8."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "I am very positive about this paper -- I think it is both a valuable contribution and an important direction for future work, as it is both practically and theoretically motivated and I agree with the concept of unit scale. I also appreciate the demonstration of failed HP transfer of muP for typical Llama-like models, which I have experienced myself. The experiments are very broad and consider not only HP transfer, but dependence between parameters, numerical properties during training, FP8 and 7B scale with downstream evaluations. Certainly, the 7B experiments are most convincing. \n\nSome side note: As the authors note themselves (so I do not see it as a weakness, but future work), unit scaling does not give guarantees for the behavior during training, so I would be particularly interested to see this method combined with models like the outlier protected block (He et al., NeurIPS 2024 https://arxiv.org/pdf/2405.19279), investigating the outliers during training, for which this model might enable even better/easier FP8 training."
            },
            "weaknesses": {
                "value": "While I am an advocate for the paper, I want to raise some points/irregularities that came to my mind while reading and I think would need to be addressed, both to improve the work, its insights or my score. They concern, in particular, the experimental setups:\n\n- Why not use a larger dataset for the HP transfer experiments? I understand it is to compare to the setup of Yang et al., but I am asking because it is really small compared to modern training settings. For instance, in Fig. 4, 34k steps imply ~4 epochs over the dataset? Similarly, a warmup of 2000 steps is relatively long compared to the overall steps? In comparison, the large scale training only used 500 warmup steps.\n- When sweeping the LR, is the final LR always changed to 10% of the chosen rate? The final LR should either be swept independently or kept fixed to a low enough value for a proper LR cooldown, otherwise this can skew results.\n- Comparison to SP, in particular 7B: If understand correctly, you use the exact same model for SP and u-muP. Does this mean you also use the tweaks (non-trainable RMSNorm, independent WD) for the SP model? Since the LR setup for Llama was chosen without those changes enabled, I think a fair comparison would be to use the original model, in particular with coupled weight decay.\n\nI particularly think the point on the comparison to SP is very important \u2014 the main focus of the paper is on a comparison to muP, but there is the simple reason of being more convincing to adopt the method because of its performance and not just its elegance (e.g. for practitioners, many of which haven\u2019t adopted muP yet either).\n\nRelation to prior work: I think it would be important to add references and discussions to the field of signal propagation in neural networks, e.g. Noci et al. https://arxiv.org/pdf/2206.03126 and the many references within their related work."
            },
            "questions": {
                "value": "I have raised questions/weaknesses in the section above, which I would be happy to discuss and eventually raise my score. Beyond, I am curious what the authors think about HP transfer to larger scale and longer training. This connects to the first point above. For instance, there seems to be a slight shift of the optimal LR to the right when growing batch sizes (Fig. 4 middle). This could be problematic for planning large scale runs (where batch sizes have become enormous). Similarly, do you have concerns about transferring to much longer training lengths (e.g. more than 1M steps for Llama 3, unfeasible for LR sweeps). To be clear: I do not expect the authors to have a solution for this, I am just curious about their thoughts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper combines two approaches\u2014(1) mup and (2) unit-scale\u2014to enable hyperparameter transfer for low-precision training (FP8). The paper is overall well-written and contains many details and ablations.\n\nI do like this direction of research, and low-precision training is important for speeding up training, lowering training costs, and enabling more research.\n\nHowever, I have several concerns regarding the paper listed below. I am happy to raise the score if these are addressed properly."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I think this is important research direction that has many practical values. The paper contains a lot of useful details (many of them are in  the appendix). I really appreciate that. \n\nThe embedding scaling rule seems interesting and novel but also controversal."
            },
            "weaknesses": {
                "value": "- Transfer across batch, depth, training steps is not convincing (Fig. 4).\n - (important) The learning rate in the embedding seems unnatural and contradicts the original mup paper. In the infinite-width setting, the update will go to zero, and the input layer is frozen; this doesn't seem right to me.\n - (Important) Everett also studies hyperparameter transfer thoroughly and is highly related to this paper. Please make a more comprehensive comparison. In particular, the mean-field parameterization is very close to the unit-scale proposed here. Please clarify what's new and what the differences are.\n- Citations to previous mean-field papers are needed and should have been discussed.\n- The results of the paper seem to contradict some results of the original mup paper. I would love to see a table/section summarizing them + an explanation of why the original mup setup is not right.\n- I  also want to see a wall clock time comparison between fp-8 vs. bf-16 runs at different scales, which is why we want to use fp-8. In addition, it may be good to know how much memory can be saved.\n\nAgain, I think this can be a good paper and I would like it to get improved."
            },
            "questions": {
                "value": "Independent weight decay vs coupled weight decay? need more clarification. I am not sure which one to use. Lingle proposes to use wd=0.1 * LR, while og mup, wortsman and here propose scale-independent weight decay.  \n\nFig 2. Are the legends regarding C_embed correct? It says C_embed = 1 is better than C_embed  = 1/ root(fan_out). \n\nThe uses of non-trainable rmsnorm \"scales\" seem non-standard to me, have you done ablations by yourself?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposed u-$\\mu P$, which combines $\\mu P$ with unit scaling to facilitate hyper-parameters (HPs) search. u-$\\mu P$ does not require a base shape and the HPs have less interdependency. Moreover,  u-$\\mu P$ empirically maintains unit scaling for the activations, weights and gradients, which enables FP8 low-precision training."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is well-written and the idea is clearly delivered. The authors have conducted extensive experiments to compare the performance of the proposed methods with the original $\\mu P$."
            },
            "weaknesses": {
                "value": "* As the authors discussed, this work lacks a comparison with other proposed methods (e.g., Large et al., 2024).\n* There is no theoretical justification of why choosing a different scaling for the embedding learning rate."
            },
            "questions": {
                "value": "* Maybe the authors already mentioned this, but I wonder how is the performance of u-$\\mu P$ with embedding LR =1 compared with $\\mu P$?\n\n* I am curious about how whether u-$\\mu P$ can be applied to non-transformer models and how to select the HPs (as in line 304-316) in this case? I am not asking for additional experiments, any useful insights or discussion would be appreciated.\n\n* Typo in Figure2 right panel: the solid and dotted lines are swapped?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}