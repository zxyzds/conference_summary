{
    "id": "wLmJIs1uqG",
    "title": "LancBiO: Dynamic Lanczos-aided Bilevel Optimization via Krylov Subspace",
    "abstract": "Bilevel optimization, with broad applications in machine learning, has an intricate hierarchical structure. Gradient-based methods have emerged as a common approach to large-scale bilevel problems. However, the computation of the hyper-gradient, which involves a Hessian inverse vector product, confines the efficiency and is regarded as a bottleneck. To circumvent the inverse, we construct a sequence of low-dimensional approximate Krylov subspaces with the aid of the Lanczos process. As a result, the constructed subspace is able to dynamically and incrementally approximate the Hessian inverse vector product with less effort and thus leads to a favorable estimate of the hyper-gradient. Moreover, we propose a provable subspace-based framework for bilevel problems where one central step is to solve a small-size tridiagonal linear system. To the best of our knowledge, this is the first time that subspace techniques are incorporated into bilevel optimization. This successful trial not only enjoys $\\mathcal{O}(\\epsilon^{-1})$ convergence rate but also demonstrates efficiency in a synthetic problem and two deep learning tasks.",
    "keywords": [
        "Bilevel Optimization",
        "Lanczos Process",
        "Krylov Subspace"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wLmJIs1uqG",
    "pdf_link": "https://openreview.net/pdf?id=wLmJIs1uqG",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates a gradient based method for solving bilevel optimization problems with a strongly convex lower-level objective. To enhance the efficiency of hypergradient computation, the paper employs the Krylov subspace method alongside the Lanczos process to accelerate the solution of the linear systems involved. The paper presents non-asymptotic convergence results for the proposed method. Numerical experiments on data hyper-cleaning, synthetic problems, and logistic regression demonstrate its effectiveness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This work leverages the Krylov subspace method and the Lanczos process to solve the linear system involved in hypergradient computation, providing a new approach for efficiently approximating the hypergradient.\n\n2. A non-asymptotic convergence rate of $O(\\epsilon^{-1})$ is established."
            },
            "weaknesses": {
                "value": "1. The description of Algorithm 2 is somewhat unclear, particularly in relation to the theoretical convergence result in Theorem 3.12. Theorem 3.12 requires the step size for\u00a0$x$ to be zero during the initial few steps in each epoch, implying that before updating\u00a0$x$, the algorithm must obtain a\u00a0$y_k$\u00a0sufficiently close to\u00a0$y^*(x_k)$, as also reflected in the theoretical analysis. This indicates that the epoch $h$ functions primarily as an outer loop index, where at each iteration $h$, the algorithm first solves the lower level problem to obtain a\u00a0$y_k$\u00a0that is sufficiently close to the lower level solution before proceeding to update\u00a0$x$. However, in other gradient based algorithms, such as SOBA, this requirement for\u00a0$y_k$\u00a0to be close to the lower-level solution is not imposed.\n\n2. In the convergence analysis, the proof of Theorem 3.12 , the step size for\u00a0$x$\u00a0must be very small to ensure stability of the Krylov subspace method and the dynamic Lanczos process in LancBiO as\u00a0$x_k$ is updated. It appears that the inclusion of the dynamic Lanczos process constrains the step size for\u00a0$x$, potentially making it smaller than in algorithms without this process, such as SOBA. However, the paper lacks a discussion of this issue.\n\n3. The parameter\u00a0$m$, which defines the dimension of the Krylov subspace in LancBiO, should play a crucial role in the efficiency of the Krylov subspace method and hence the performance of LancBiO. However, the convergence analysis does not show how the choice of\u00a0$m$ impacts the performance of LancBiO.\n\n4. The convergence analysis requires an unusual assumption, Assumption 3.10, which is not present in other works, such as SOBA. Why is Assumption 3.10 necessary? Is it because\u00a0$y_k$\u00a0generated by LancBiO cannot be guaranteed to converge to\u00a0$y^*_k$ ?"
            },
            "questions": {
                "value": "Is the proposed algorithm applicable in a stochastic setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses the complexities of bilevel optimization, a framework widely used in machine learning applications. It introduces a novel approach that utilizes the Lanczos process to construct low-dimensional Krylov subspaces, aiming to alleviate the computational challenges associated with hypergradient calculations. By avoiding the direct computation of the Hessian inverse, the proposed method demonstrates improved efficiency and achieves a convergence rate of $ O(\\epsilon^{-1}) $. The authors present a theoretical foundation for their approach and validate it through experiments on synthetic problems and deep learning tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- **Innovative Approach**: The incorporation of subspace techniques into bilevel optimization is novel and contributes significantly to the field, potentially opening new avenues for research.\n- **Theoretical Rigor**: The paper offers a solid theoretical framework that is well-justified, providing confidence in the proposed method's validity and effectiveness.\n- **Empirical Validation**: The experimental results show promising performance improvements over existing methods, suggesting practical applicability in real-world scenarios.\n- **Clarity of Presentation**: The paper is well-organized, making complex concepts accessible to readers, which enhances its impact."
            },
            "weaknesses": {
                "value": "- **Limited Scope of Experiments**: While the experimental results are promising, they are conducted on a limited set of problems. Broader validation across diverse benchmarks would strengthen the paper's claims.\n- **Assumptions in Theory**: The theoretical results rely on certain assumptions that may not hold in all contexts, potentially limiting the generalizability of the findings.\n- **Lack of Comparison with More Methods**: The paper could benefit from comparisons with additional state-of-the-art bilevel optimization methods to contextualize the improvements more clearly."
            },
            "questions": {
                "value": "1. How does the proposed method perform in scenarios where the lower-level problem is not strongly convex? \n2. Can the authors elaborate on how the method scales with increasing dimensions in the bilevel optimization problems?\n3. Are there any limitations observed when applying the proposed method to non-standard bilevel optimization problems, such as those with noisy or sparse data?\n4. Could the authors provide more detailed insights into the computational complexity of solving the small-size tridiagonal linear system mentioned?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents an approach for bilevel optimization using Krylov subspace methods and the Lanczos process to approximate inverse-Hessian vector products. The method constructs low-dimensional Krylov subspaces and solves tridiagonal linear systems, achieving convergence to an $\\epsilon$-stationary point with $\\mathcal{O}(\\epsilon^{-1})$ complexity. Experimental evaluations are conducted to illustrate its performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper introduces a novel modification to the Lanczos process by re-linearising the objective functions within the iteration, specifically for bilevel optimization. This reduces large-scale subproblems to smaller tridiagonal linear systems, and the explanation provides a potential framework for applying subspace techniques in this context."
            },
            "weaknesses": {
                "value": "I identify two weaknesses:\n\n1.To my knowledge, many recent advances in bilevel optimization have focused on addressing problems where the function is not necessarily strongly convex, or even non-convex. The strong convexity assumption in this paper may be overly stringent, potentially limiting the method's applicability to a broader range of optimization problems.\n\n2.The proposed method is technically constrained to deterministic settings. While LancBiO can be extended to stochastic scenarios, its performance in these settings has been inconsistent. It remains uncertain whether LancBiO can be effectively adapted for stochastic environments, which is crucial for many practical applications. For example, despite including experiments in stochastic settings, SOBA seems to perform better, indicating challenges in extending LancBIO effectively."
            },
            "questions": {
                "value": "Could the authors provide further details on extending LancBiO to stochastic scenarios? Additionally, could they elaborate on how the Lanczos process ensures convergence in stochastic settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes two novel algorithms, SubBiO and LancBiO, for solving bilevel optimization problems in which the lower-level problem is strongly convex. The main contribution lies in incorporating the Krylov subspace and the Lanczos process into bilevel optimization, achieving a more accurate hypergradient estimate by efficiently and dynamically solving the associated linear system. Under certain conditions, the authors establish non-asymptotic convergence for LancBiO and conduct an empirical study to validate the efficiency of both SubBiO and LancBiO."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "$\\textbf{S1:}$ The introduction of subspace techniques into bilevel optimization is well-motivated. The explanations of how the proposed SubBiO and LancBiO algorithms relate to existing methods are helpful. \n\n$\\textbf{S2:}$ A convergence rate guarantee is provided for LancBiO under certain conditions. \n\n$\\textbf{S3:}$ To empirically justify the improvement achieved by accurately solving the associated linear system, the experiments report the residual norm of the linear system."
            },
            "weaknesses": {
                "value": "For the theory, the main concerns are as follows:\n\n$\\textbf{W1: Assumption 3.10}.$ Based on (10), Assumption 3.10 depends on the step size, $\\lambda$. However, in Lemma 3.11 and Theorem 3.12, $\\lambda$ is specified to satisfy $\\lambda\\sim \\mathcal{O}(\\frac{1}{m^4})$. Under this setting, it seems that Assumption 3.10 may not hold when $m$ is sufficiently large. Therefore, in general, how can Assumption 3.10 be checked\u2014either theoretically or empirically? A bit more discussion on these issues would be helpful. \n\n$\\textbf{W2: Theoretical results lack discussion}.$ In reading Theorem 3.12, it is unclear how the results compare to other bilevel algorithms, as some existing algorithms (e.g., deterministic SOBA) also reach an \\epsilon-stationary point within $\\mathcal{O}(\\epsilon^{-1})$ outer iterations.  \nAdditionally, there is no complexity analysis. For example, what is the number of oracle calls required to reach an $\\epsilon$-stationary point? Typically, bilevel optimization literature provides the number of gradient, Hessian-vector, and Jacobian-vector products required to reach a stationary point with precision $\\epsilon$. \n\n\n$\\textbf{W3: Lack of convergence analysis for SubBiO}.$ Given that SubBiO has a simpler structure than LancBiO, it would be beneficial to include a theoretical analysis or, if not feasible, to discuss the reasons for its absence.\n\nFor the experiments, the main concern is as follows:\n\n$\\textbf{W4: Experiments could be expanded}.$ The experiments could be more comprehensive. For example, it would be beneficial to include additional competing bilevel methods, especially Hessian-free algorithms like F2SA (Kwon et al., ICML 2023). Additionally, using more datasets in the data hyper-cleaning and logistic regression tasks would help validate the efficiency of both SubBiO and LancBiO."
            },
            "questions": {
                "value": "Apart from the questions raised in the Weaknesses section, some additional questions are as follows:\n\n$\\textbf{Q1:}$ In the implementation of SubBiO, how is the two-dimensional subproblem in line 4 solved? A bit more discussion on these choices would be helpful. \n\n$\\textbf{Q2:}$ In the convergence analysis of LancBiO, the hyperparameter $m_0$ plays an important role, but it is not included in the experiments. Could the authors clarify why? \n\n$\\textbf{Q3:}$ Can the authors provide more detail on why the Lanczos process in Algorithm 2 does not affect the final convergence guarantee? A brief proof sketch of Theorem 3.12 would be helpful.\n\nSuggestions for improvement that did not affect the score:\n\n$\\textbf{About $m_0$ in Lemma 3.11 and Theorem 3.12:}$ First, $m_0$ in Lemma 3.11 should satisfy $m_0 = \\Omega(1)$, meaning $m_0$ must be greater than some positive constant, as implied at the end of the proof of Lemma G.4. Second, $m_0$ in Theorem 3.12 should be set to $m_0 = \\Omega(\\log m)$, following from lines 1643\u20131647 and equation (63) in the proof of Lemma H.2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}