{
    "id": "PPDheO2z5v",
    "title": "Actra: Optimized Transformer Architecture for Vision-Language-Action Models in Robot Learning",
    "abstract": "Vision-language-action models have gained significant attention for their ability to model trajectories in robot learning. However, most existing models rely on Transformer models with vanilla causal attention, which we find suboptimal for processing segmented multi-modal sequences. Additionally, the autoregressive generation approach falls short in generating multi-dimensional actions. In this paper, we introduce Actra, an optimized Transformer architecture featuring trajectory attention and learnable action queries, designed to efficiently process segmented multi-modal trajectories in language-conditioned robot imitation learning. Furthermore, we propose a contrastive dynamics learning objective to enhance its understanding of environment dynamics and multi-modal alignment, complementing the primary behavior cloning objective. Through extensive experiments on three large-scale robot manipulation benchmarks, Actra exhibits substantial performance improvements over state-of-the-art models.",
    "keywords": [
        "Robotics",
        "Embodied AI",
        "Transformer",
        "Multimodality",
        "Contrastive Learning",
        "Dynamics Learning",
        "Multimodal Alignment"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "An optimized Transformer architecture for vision-language-action models, designed to process multimodal trajectories in robot learning, complemented by a contrastive dynamics learning approach.",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PPDheO2z5v",
    "pdf_link": "https://openreview.net/pdf?id=PPDheO2z5v",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes optimized Transformer Architectures for Policy Learning. The authors propose 3 contributions: a custom self-attention mask for policy learning, learnable action queries for each action dimension and an contrastive loss function to improve policy learning by having the model differentiate between plausible dynamics and unrealistic ones. The proposed architecture is tested on three common IL benchmarks including CALVIN and VIMA with several ablations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- novel dynamics self-supervised loss to improve policy learning performance, which does not require pretraining and improves performance\n\n- proposed architecture changes improve upon a wide range of transformer baselines in different benchmarks\n\n- the paper is overall well written and easy to follow with several illustrative figures"
            },
            "weaknesses": {
                "value": "- The terminology \"VLA\" is misleading since the model lacks vision-language pretraining. This is fundamentally a standard Transformer-based policy without language generation or image understanding capabilities.\n\n- Performance issues:\n\n    - CALVIN results are significantly below SOTA (1.x vs 3-4 average rollout length from methods like GR-1 or 3DDA), with existing methods like GR-1 already implementing similar trajectory attention mechanisms and learnable action queries\n    - VIMA improvements are marginal, with lower performance compared to recent approaches like [4] that also utilize per-dimension action tokens\n    - Low Diffusion baseline performance on Relay Kitchen and Push-T compared to originally reported metrics - what explains that discrepancy?\n- Limited novelty in technical contributions:\n    - Trajectory attention is an incremental modification seen in multiple prior works (Octo [1], GR-1 [2])\n    - Learnable action queries are also incremental and were previously explored in GR-1, ACT [3] or [4] and similar research\n    - No substantial architectural innovations except the self-supervised loss\n\n- Methodological limitations:\n    - Absence of scaling analysis across model sizes and environments\n    - No large-scale experimental validation\n    - Training duration of 5-10 epochs is insufficient to demonstrate convergence\n    - Lacks comprehensive ablation studies on larger-scale settings\n    - Design changes of generalist policies need evaluation on large-scale datasets to see if the proposed changes scale as intended\n\n\nSources:\n\n[1]: Team, Octo Model, et al. \"Octo: An open-source generalist robot policy.\"\u00a0RSS 2024\n\n[2]: Wu, Hongtao, et al. \"Unleashing large-scale video generative pre-training for visual robot manipulation.\"\u00a0ICLR 2024\n\n[3]: Zhao, Tony Z., et al. \"Learning fine-grained bimanual manipulation with low-cost hardware.\"\u00a0RSS 2023\n\n[4]: Li, Jiachen, et al. \"Mastering robot manipulation with multimodal prompts through pretraining and multi-task fine-tuning.\"\u00a0_arXiv preprint arXiv:2310.09676_\u00a0(2023)."
            },
            "questions": {
                "value": "- Could you elaborate on the low performance of Actra on CALVIN compared to recent sota methods? What factors do you think contributes to these differences and how would you bridge them in future work? \n\n- The training duration of 5-10 epochs shows promising initial results. Have you explored longer training runs to understand the full convergence behavior and potential performance ceiling of the approach? \n\n- How are the contributions of trajectory attention and learnable action queries different to prior work that employed the same already?\n \n- Could you consider including additional scaling experiments to demonstrate how this approach performs across different model capacities and dataset sizes? This could help better understand its broader applicability as a VLM policy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the problem of learning VLA models for robot manipulation tasks. The proposed method optimizes the architecture of classical transformer models by introducing trajectory attention. The authors also propose to use learnable action queries to make the action decoding more focused on relevant information. The method is validated on several simulation benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The method achieves good performance from the evaluation perspective; \n\nThe paper is well structured and easy to follow."
            },
            "weaknesses": {
                "value": "The design is not sufficiently evaluated through experiments.  \n\nThe comparison with baselines is insufficient, more prior works need to be included and compared, e.g., Behavior Transformers[1], CoTPC[2]. \n\n[1] Behavior Transformers: Cloning k modes with one stone, NeurIPS\u201922. \n\n[2] Chain-of-Thought Predictive Control, ICML\u201924."
            },
            "questions": {
                "value": "How would the proposed attention mechanism affect the inference speed? \n\nWhat are the failure modes of the method? Providing additional analysis would help readers better understand the work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Vision language action models typically use causal attention, which may be suboptimal for processing multimodal sequences. The paper makes two contributions: 1) analysis of how the attention pattern of VLAs can be modified to improve performance, and 2) sequence-level contrastive learning helps. Simulation experiments on VIMABench, Maniskill2, and CALVIN show that the method outperforms AR models and shows the effectiveness of contrastive learning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper explores an interesting direction: whether the autoregressive mask pattern makes sense for VLAs. While it is less well supported by efficient inference methods (i.e., flash-attention, ring-attention, etc.), the reviewer believes that exploring such direction is beneficial for the community. \n2. Thorough ablation studies. It helps the reader understand how the change in attention pattern affects the model's performance."
            },
            "weaknesses": {
                "value": "1. Certain claims in the abstract or introduction may not be correct and need to be elaborated.\n\n\u201cAdditionally, the autoregressive generation approach falls short in generating multi-dimensional actions\u201d (Line 15-16)\nMany existing VLAs (such as Octo [1] and OpenVLA [2]) are AR models. Many behavior cloning models [4] are also AR models. They all generate multi-dimensional actions and perform real robot tasks.  \n \n\u201cVLAs predominantly build upon the pioneering foundations laid by Decision Transformer and Trajectory Transformer \u2026 This paradigm has become a cornerstone across recent VLAs\u201d (line 46-49). \nThese two formulations are drastically different structures than RT-2, OpenVLA, etc, where there\u2019s no interleaving of vision, action, and proprioception data and do not model long sequences. Instead, many recent approaches have gone towards VQA models such as LLaVA / BLIP / Transfusion. \n\n2. The paper can benefit from comparisons with the current state-of-the-art VLAs (Octo [1] and OpenVLA [2]). Additionally, many recent works such as Implicit behavior cloning [5] and diffusion policy [6] can work with <50 demonstrations. Multi-task diffusion-based VLA (Octo, scaling up and distilling down [7]) also shows promise in few-shot learning. It is not obvious that the proposed method is more sample-efficient then these methods.\n\n3. Lack of ablations and experiments on a real robotics setup. 4. The paper does not report confidence intervals (standard deviation or standard error) on the results. It is hard to conclude from the reported results to claim that contrastive learning helps Actra. \n5. Scalability: many of the baselines (i.e. RT-1, Gato, VIMA) can benefit significantly from pre-training (in fact most of the evaluations for these models are done post large-scale pre-training). It is not obvious if this method is as scalable (both in terms of model sizes and dataset sizes) compared to the baselines. \n6. Past works have explored sequence-level contrastive learning (i.e. ConDT [3]). The paper can benefit from including more ablation studies over different types of sequence-level contrastive learning frameworks. \n\n[1] Team, Octo Model, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari et al. \"Octo: An open-source generalist robot policy.\" arXiv preprint arXiv:2405.12213 (2024).\n\n[2] Kim, Moo Jin, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov et al. \"OpenVLA: An Open-Source Vision-Language-Action Model.\" arXiv preprint arXiv:2406.09246 (2024).\n\n[3] Konan, Sachin G., Esmaeil Seraj, and Matthew Gombolay. \"Contrastive decision transformers.\" In Conference on Robot Learning, pp. 2159-2169. PMLR, 2023.\n\n[4] Radosavovic, Ilija, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, and Jitendra Malik. \"Humanoid locomotion as next token prediction.\" arXiv preprint arXiv:2402.19469 (2024).\n\n[5] Florence, Pete, Corey Lynch, Andy Zeng, Oscar A. Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. \"Implicit behavioral cloning.\" In Conference on Robot Learning, pp. 158-168. PMLR, 2022.\n\n[6] Chi, Cheng, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. \"Diffusion policy: Visuomotor policy learning via action diffusion.\" The International Journal of Robotics Research (2023): 02783649241273668.\n\n[7] Ha, Huy, Pete Florence, and Shuran Song. \"Scaling up and distilling down: Language-guided robot skill acquisition.\" In Conference on Robot Learning, pp. 3766-3777. PMLR, 2023."
            },
            "questions": {
                "value": "1. Claim in abstract: \u201cAdditionally, the autoregressive generation approach falls short in generating multi-dimensional actions.\u201d Can you elaborate why it AR has fall short in action generation? \n2. How many tokens are there to represent action at each timestep?\n3. How does the sample complexity of the model compare to diffusion policy [1] (for a single task setup, such as that in table 5)? \n4. What\u2019s the sequence length of the model? After adding CDL, how much slowdown is present for training? \n\n[1] Chi, Cheng, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. \"Diffusion policy: Visuomotor policy learning via action diffusion.\" The International Journal of Robotics Research (2023): 02783649241273668."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}