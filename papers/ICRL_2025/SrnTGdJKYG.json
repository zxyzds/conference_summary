{
    "id": "SrnTGdJKYG",
    "title": "Neural Deconstruction Search for Vehicle Routing Problems",
    "abstract": "Autoregressive construction approaches generate solutions to vehicle routing problems in a step-by-step fashion, leading to high-quality solutions that are nearing the performance achieved by handcrafted, operations research techniques.\nIn this work, we challenge the conventional paradigm of sequential solution construction and introduce an iterative search framework where solutions are instead deconstructed by a neural policy. Throughout the search, the neural policy collaborates with a simple greedy insertion algorithm to rebuild the deconstructed solutions. Our approach surpasses the performance of state-of-the-art operations research methods across three challenging vehicle routing problems of various problem sizes.",
    "keywords": [
        "neural combinatorial optimization; vehicle routing problem; reinforcement learning; neural deconstruction"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We introduce an iterative search framework in which solutions are deconstructed by a neural policy.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=SrnTGdJKYG",
    "pdf_link": "https://openreview.net/pdf?id=SrnTGdJKYG",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes Neural Deconstruction Search (NDS), a novel method for vehicle routing problems that iteratively improves solutions by selectively removing and re-inserting nodes. A reinforcement learning-based policy network guides the removal process, while a greedy insertion strategy reconstructs efficient routes. NDS achieves competitive performance and improves solution quality compared to traditional approaches."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Originality: The paper proposes a unique deconstruction-reconstruction framework for VRPs, leveraging a reinforcement learning policy to optimize solution quality in a novel way.\n- Quality: The experiments are thorough, showing significant performance improvements across multiple VRP benchmarks and validating the approach\u2019s robustness.\n- Clarity: The paper is well-organized, with clear explanations and visuals that make the deconstruction and reconstruction process easy to understand."
            },
            "weaknesses": {
                "value": "- The paper introduces a deconstruction and re-insertion heuristic for VRP improvement, but it lacks a clear comparison with well-known heuristics, such as the 2-opt method, which also iteratively refines solutions. Providing an explanation of how the proposed approach differs from 2-opt would clarify the advantages of using a learning-based method for the deconstruction-recreation process.\n\n- While the paper presents NDS as a novel learning-based improvement heuristic (Costa, 2020), it does not include comparisons with other learning-based approaches in the same category. A comparative analysis with similar methods would offer a more complete view of NDS\u2019s performance and highlight its specific strengths and weaknesses within the context of learning-based VRP solvers.\n\n- The current experiments are restricted to a specific subset of VRP problems, but VRP encompasses a wide variety of problem settings (see Berto, 2024). Testing the proposed approach on additional VRP problems, or explaining any limitations that prevent its application to other settings, would strengthen the generalizability of the method and provide clarity on its applicability across diverse VRP scenarios.\n\n- The paper lacks details on how LEHD, BQ, and other learning-based solver baselines were trained and configured for this study. This is especially crucial because the reported performance for these methods differs from their original papers. Specifically, LEHD was originally trained on problems with up to 100 nodes; further clarification on how it was trained in this paper\u2019s setting is necessary to interpret the experimental results accurately. Including these details would make the experimental setup more transparent and allow for better reproducibility and understanding of the baseline comparisons.\n\nd O Costa, P. R., Rhuggenaath, J., Zhang, Y., & Akcay, A. (2020, September). Learning 2-opt heuristics for the traveling salesman problem via deep reinforcement learning. In Asian conference on machine learning (pp. 465-480). PMLR.\n\nBerto, F., Hua, C., Zepeda, N. G., Hottung, A., Wouda, N., Lan, L., ... & Park, J. (2024). Routefinder: Towards foundation models for vehicle routing problems. arXiv preprint arXiv:2406.15007."
            },
            "questions": {
                "value": "Is the proposed deconstruction-reconstruction framework specifically designed to be effective only for VRP-type problems, or could it be adapted to other combinatorial optimization problems with different structural properties, such as Maximum Independent Set (MIS)? Have the authors considered or explored the potential adaptability of this approach to CO problems beyond VRP? Understanding any insights or limitations regarding its generalizability would clarify whether this framework could be broadly applicable across different types of optimization challenges."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper falls within the realm of using learning-based approaches or elements to solve vehicle routing problems. Specifically, it proposes to deviate from the existing step-by-step approaches that usually focus on autoregressive construction techniques. Instead, it proposes an iterative search framework that uses a neural policy for deconstruction and a relatively simple greedy insertion algorithm to repair the respective solutions.\nThe paper shows promising results on various vehicle routing problem variants.\n\nOverall, I think that this paper makes a valuable contribution to the field but is currently borderline in its exposition as the authors clearly oversell the contribution. Yet, this aspect can easily be healed and if the authors are willing to do so, I believe that this paper will be of interest to the ICLR community and could be accepted."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1) The paper is technically well-written and easy to follow\n\n2) The presented experiments are of sufficient breadth\n\n3) The proposed methodology opens an interesting avenue for algorithm design that deviates from existing approaches that usually focus on using neural policies to construct solutions in a step-by-step fashion.\n\n4) The results show promising performance compared to existing methods."
            },
            "weaknesses": {
                "value": "As indicated in my summary, the authors are currently (significantly) overselling the contribution of the proposed method.\n\nThis relates to the fact that numerical experiments are - to some extent - comparing apples with oranges due to limiting the computation times of the studied algorithms instead of using a proper performance-based stopping criterion. This experimental design choice clearly favors the search technique proposed by the authors, as the neural deconstruction works instantaneously.\n\nIn practice, one would not use such a time-based criterion as the problems studied are static problems usually solved in a day-ahead fashion, where solution times are not limited to seconds. In such cases, one would usually finetune an algorithm based on a performance-based stopping criterion, i.e., the number of consecutive iterations without improvements. \n\nBeyond this rather unconventional experimental design decision, the authors also do not provide details if and if so how the benchmark algorithms have been tuned.\n\nLooking at the fact that hyperparameter tuning seems to be missing for the benchmark algorithms and that the experimental design in general favors the proposed algorithm, I think that statements like \"Our approach surpasses the performance of state-of-the-art operations research methods\" are not sufficiently substantiated and should be toned down, not only in the abstract but in the manuscript.\n\nI think that the authors make an interesting contribution to the field that is obvious without such overselling statements. The paper as well as the understanding of the reader will benefit from a more nuanced discussion and analyses of performance and computational complexity. I think that the authors can easily address this aspect, e.g., by\n1) toning down the respective claims, particularly in the papers abstract, contribution section, and results discussion.\n2) adding information on hyperparameter tuning for all algorithms in an appendix\n3) adding results where the benchmark algorithms have a more suitable stopping criterion. -even if the proposed algorithm then does not surpass the benchmarks, one can see the full picture and discuss the results for both stopping criteria more nuanced\n4) commenting on the training effort of the proposed method compared to the benchmarks not requiring such a training phase"
            },
            "questions": {
                "value": "In Section 4.4 you discuss generalization against distribution shifts, did you also investigate this for varying instance sizes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper adopts a destroy-reconstruct iterative search framework to improve the quality of VRP solutions. The destruction step is performed by the proposed attention-based model, which removes customer nodes step by step from the solution; then, the removal of customer nodes are inserted into the destroyed solution node by node using a simple greedy insertion method. In addition, this paper also adopts a simulated annealing mechanism to allow worsening solutions to be accepted during the iteration process. Experimental results show that the proposed method achieves state-of-the-art performance on CVRP, CVRPTW, and PCVRP."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper writing is fluent.\n2. Experiments show outstanding performances on three VRPs."
            },
            "weaknesses": {
                "value": "1. From my point of view, the framework of this paper is mainly based on SISRs. This paper replaces the heuristic destruction process of SISRs with a learning-based destruction strategy. The greedy insertion with simulated annealing in this paper is essentially the same as the heuristic reconstruction process \"greedy insertion with blinks\" in SISRs, both of which have the probability of accepting differential solutions to jump out of the local optimum. Based on the above observations, I think the framework of this paper remains at the engineering level. It is a migration of the framework of SISRs, which is not novel enough.\n2. Since the performance of SISRs is promising, it is foreseeable that it will be better after adding the machine learning strategy. In Table 1, the performance gap between the proposed method and SISRs is not significant especially when N is large. Therefore, I believe that the main performance source of the proposed method is the framework of SISRs.\n3. The proposed method is similar to the SISRs framework, so I suggest the authors discuss its relation to SISRs in the related work section. Many details are not explained clearly, such as how to train the model on VRPTW and PCVRP, what changes are made, and how to meet the constraints.\n4. There needs more baselines to compare. It should introduce more comparison algorithms for VRPTW and PCVRP, and CVRP should also introduce newly emerged baselines such as ELG [1] and UDC [2].\n\n[1] Gao,  Chengrui, et al. \"Towards generalizable neural solvers for vehicle routing problems via ensemble with transferrable local policy.\" arXiv preprint arXiv:2308.14104 (2023).\n\n[2] Zheng,  Zhi, et al. \"Udc: A unified neural divide-and-conquer framework for large-scale combinatorial optimization problems.\" arXiv preprint arXiv:2407.00312 (2024)."
            },
            "questions": {
                "value": "1. Why is greedy insertion used to reconstruct the solution, rather than other methods such as regret insertion?\n2. Can the proposed method solve the TSP?\n3. There is no ablation study on the random seed v. And could you describe the reason for choosing the hyper-parameters in this paper, such as threshold factor and temperature?\nMinor question:\n4. Figure 3 should be corrected to a vector diagram."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Tins paper uses deep neural network (DNN) to trun the operation of removing nodes into a sequence generation process with the aim of achieving a fast serach process without sacrificing the high quality search guidance of the DNN. The solution is then finally reconstructed using greedy methods. While the work has some merit, the main contribution does not meet the novelty expectations of ICLR."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Although the method is simple, it achieves a certain effect."
            },
            "weaknesses": {
                "value": "1.\tJust replacing a heuristic process of removing nodes with a DNN is too simple. The contribution here does not significantly advance the field. I think the authors need to explain what are the differences and advantages of the proposed method for removing nodes over the previous methods?\n2.\tA similar approach of using DNNs to remove and reconstruct solutions has been used in papers [1]. However the proposed method only uses DNNs for node removal and greedy methods for reconstruction. There are three other papers[2-4] on learning similar local search operations in VRP, and those methods can also be used directly to learn removal and reconstruction.\n3.\tIMPROVEMENTSTEP is used first in Algorithm 1, with no specific explanation as to why.\n4.\tThere are no details about the way you use DNNs in the decoding process, leading the reader to have little understanding of the mechanism.\n5.\tThis part of the formulation needs further discussion: \u201cThis contrasts with construction-based methods, where each decision is independent of prior selections.\u201d. However, the current construction-based methods also retain the customer information selected in the previous step or part of the path information during the decoding process.\n\n[1]Efficient Neural Neighborhood Search for Pickup and Delivery Problems. IJCAI 2022.\n\n[2]Learning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer. NeurIPS 2021.\n\n[3] Learning Improvement Heuristics for Solving Routing Problems. TNNLS 2021.\n\n[4] Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt. NeurIPS 2023."
            },
            "questions": {
                "value": "What are the advantages of this paper over previous papers that have implemented reconstructive solving using DNN, please explain in detail."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}