{
    "id": "ijbA5swmoK",
    "title": "Second-Order Min-Max Optimization with Lazy Hessians",
    "abstract": "This paper studies second-order methods for convex-concave minimax optimization.  \nMonteiro & Svaiter (2012)  proposed a method to solve the problem with an optimal iteration complexity of \n$\\mathcal{O}(\\epsilon^{-3/2})$ to find an $\\epsilon$-saddle point.  However, it is unclear whether the\ncomputational complexity, $\\mathcal{O}((N+ d^2) d \\epsilon^{-2/3})$, can be improved. In the above, we follow  Doikov et al. (2023) and assume the complexity of obtaining a first-order oracle as $N$ and the complexity of obtaining a second-order oracle as $dN$. \nIn this paper, we show that the computation cost can be reduced by reusing Hessian across iterations. Our methods take the overall computational complexity of $\\tilde{\\mathcal{O}}( (N+d^2)(d+ d^{2/3}\\epsilon^{-2/3}))$, which improves those of previous methods by a factor of $d^{1/3}$. \nFurthermore, we generalize our method to strongly-convex-strongly-concave minimax problems and establish the complexity of $\\tilde{\\mathcal{O}}((N+d^2) (d + d^{2/3} \\kappa^{2/3}) )$ when the condition number of the problem is $\\kappa$, enjoying a similar speedup upon the state-of-the-art method. \nNumerical experiments on both real and synthetic datasets also verify the efficiency of our method.",
    "keywords": [
        "min-max optimization; second-order methods; computational complexity"
    ],
    "primary_area": "optimization",
    "TLDR": "We propose novel second-order methods for min-max optimization that are provably better than existing optimal methods",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ijbA5swmoK",
    "pdf_link": "https://openreview.net/pdf?id=ijbA5swmoK",
    "comments": [
        {
            "summary": {
                "value": "This work proposed second-order algorithms LEN and LEN-restart for C-C and SC-SC min-max problems, these algorithms incorporate lazy Hessian update and Extragradient update, the computational cost analysis and numerical experiment results showed the outperformance of the proposed algorithms versus existing algorithms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. A new algorithm in min-max optimization with better computational complexity versus existing results.\n2. The paper is well organized, the flow is easy to follow."
            },
            "weaknesses": {
                "value": "1. The main component seems to be a combination of Doikov et al. (2023) on lazy Hessian and Adil et al., (2022) on extragradient, which may restrict the novelty a bit.\n2. The experiment can be further enhanced. \n   - First, the $O(d^{1/3})$ improvement suggests the outperformance is valid in high-dimensional cases (while not in low-dimensional cases), now the experiment cannot exhibit such a pattern, how does the algorithm perform in low-dimensional case?\n   - It is not clear how the choice of $m$ affects the performance, maybe you can follow Doikov et al. (2023) to add more experiments for clarification.\n   - The theory part uses the gap function as the measurement, while the experiment uses the gradient norm or point distance, maybe you can add more clarification to rationalize your choice."
            },
            "questions": {
                "value": "/"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Authors generalize the results of Nikita Doikov, El Mahdi Chayti, and Martin Jaggi. Second-order optimization with lazy hessians. In ICML, 2023. related with lazy Hessian for Cubic regularized Newton method for Saddle-point problems (SPP). The developed result was cleary presented and sufficiently technical. Moreover, requires principally new ideas. However, the the motivation to consider second-order methods are quite limited and is mainly theoretical from my point of view. Also in the paper authors doesn't observe the accuracy required to solve auxiliary problem."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I guess the paper is good from mathematical point of view. The results is strong, original, well presented. I agree with authors that they developed significantly new tricks to work with this class of problems. I guess the paper is good!"
            },
            "weaknesses": {
                "value": "For me the main drawback is motivation. I do not understand why we should use second-order method with expensive iteration rather than the first-order one. I understand the motivation for convex optimization where the number of iteration significantly reduces by using optimal second-order scheme, but I do not understand it for SPP where the difference is minor."
            },
            "questions": {
                "value": "1) Is it possible to generalize the result in case strongly convex-strongly concave case with different constant of strong convexity/concavity?\n2) Could you estimate the required accuracy for auxiliary problem that guarantee the desired precision for target problem? \n I can make rating higher if you can positively answer for this questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper authors utilize the lazy Hessians technique inside second-order extragradient method for min-max optimization. The main idea of lazy Hessians technique is to update Hessian in second-order method only every $m$ iterations. Authors consider convex-concave and strongly-convex-strongly-concave setups. Authors claim, that if $m=\\Theta(d)$ iterations, where $d$ is a dimension of the problem, their method achieves state-of-the-art computational cost."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Authors propose a method with the same oracle complexity, as state-of-the-art methods but with less computational complexity. This means, that their algorithm can achieve the same estimation error with the same number of iterations, but overall spending less computational resources and taking less time. The experimental results only support this point. This makes method more attractive from the practical point of view. The paper is written in a clear way, and it is easy to understand."
            },
            "weaknesses": {
                "value": "Overall, the paper feels like a very incremental result. Authors employ a known technique to reduce number of Hessian computations to existing second-order method to solve convex-concave min-max problem. To adapt proposed method to strongly-convex-strongly-concave problem, authors use a universal restarts framework, that works like a \"wrap\" around any method for convex(-concave) problems and gives better theoretical convergence for strongly-convex(-strongly-concave). Despite the fact that this paper proposes a new method with better computational complexity compared to other analogs, it lacks any novel ideas or solution of any complex problems.\n\n## Typos\n1. Lines 785-786: it should be $\\frac{m^2 + 2m}{2}$\n1. Lines 862-863: upper index in the second sum should be $\\pi(t) + s  - 1$\n2. Definition 3.5: in the end it should be $||\\hat z - z^*||\\le \\epsilon$"
            },
            "questions": {
                "value": "1. Could you explain please, how did you get second inequality in eq. (17)?\n1. Lines 155-160. Sentence \"Their methods only take a lazy CRN update (2) at each iteration...\" seems to me very weird. I needed to read it couple of times to understand. Please, rewrite in in more clear way."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a second-order algorithm designed for convex-concave minimax optimization problems. The algorithm brings the Lazy Hessian (or Lazy *Jacobian* for this paper, I guess) approach by Doikov et al. (2023) to second-order minimax optimization algorithms as in Adil et al. (2022), Lin et al. (2022). The new algorithm can achieve a computational complexity smaller by an order of $d^{1/3}$ compared to previous results both for the convex-concave case. The paper also shows superlinear convergence in the strongly-convex-strongly-concave case (by adding a restarting scheme) where we also have an improvement of $d^{1/3}$."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- It is interesting to see that the idea of Doikov et al. (2023) can also work for minimax optimization problems.\n- The paper seems to be well-written and clear, and there are empirical results supporting the theoretical arguments."
            },
            "weaknesses": {
                "value": "- The results only cover convex-concave minimax optimization for now, and there are no clear ways suggested to extend to broader problem classes like nonconvex-nonconcave functions or variational inequalities.\n- As the main idea comes from combining two lines of previous work, it\u2019s a bit hard to say that the ideas are highly original (except for the proof techniques of bounding the errors from lazy updates for the minimax case, particularly when there are EG steps in between)."
            },
            "questions": {
                "value": "- In the proof of Theorem 4.1, what we essentially do is find an upper bound of\n    $$ \\begin{align*}\n    \\frac{1}{\\sum_{i=0}^{t-1} \\eta_i} \\sum_{i=0}^{t-1} \\eta_i \\langle F(z_i),  z^{\\star} - z_i\\rangle\n    \\end{align*} $$\n    which itself is an upper bound of the Gap function by Lemma A.1.\n    \n    My question is, can\u2019t we just try to come up with an upper bound of the *MVI error* $\\langle F(\\hat{z}), z^{\\star} - \\hat{z} \\rangle$ instead? If this works, it might be possible to extend the results to general MVIs. (I think one problem could be that we can\u2019t use things like Jensen\u2019s inequality if we step out of the convex-concave assumption.)\n    \n- Is the $\\beta$ in Definition 3.4 just there for technical reasons (to ensure something like bounded iterates for the unconstrained case)?\n\n- This might be a slightly irrelevant question, but any ideas of whether the proposed idea (or maybe the Lazy Hessian idea for minimization problems) could work for higher-order cases with $p \\ge 3$?\n\n- There seem to be complicated lower bound results in Adil et al. (2022) in terms of the *iteration* complexity. Can this naturally lead to some type of a lower bound in the *computational* complexity as well with which we can compare the new/previous upper bounds?\n\n- Minor typo. In the middle of the inequality of Definition 3.3, I think it should be $f(x^{\\star}, y^{\\star})$.\n\n**References.** \\\nAdil et al., 2022. Optimal methods for higher-order smooth monotone variational inequalities. \\\nDoikov et al., 2023. Second-order optimization with lazy Hessians. \\\nLin et al., 2022. Explicit second-order min-max optimization methods with optimal convergence guarantee."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}