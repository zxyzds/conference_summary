{
    "id": "OhcWlo1M8q",
    "title": "DensBO: Dynamic Ensembling of Surrogate Models for Hyperparameter Optimisation",
    "abstract": "Hyperparameter optimisation (HPO) of machine learning models is crucial for achieving optimal performance for different tasks. Surrogate-based optimisation techniques, such as Bayesian optimisation (BO), have been successfully applied to tackle this problem. BO is subject to different design choices of its components. In particular, depending on the nature and the size of the search space, the choice of the surrogate model has a substantial impact on the overall performance of BO. Surrogate models in BO approximate the function to optimise and guide the search towards promising regions by predicting the function value for different solution candidates. Combining different machine learning (ML) models is known to lead to performance gains, e.g., in different prediction tasks. To this end, we propose a novel dynamic approach to ensemble surrogate models in the BO pipeline, leveraging the complementary powers of different surrogate models at different stages of the optimisation process. We empirically evaluate our method on numerous benchmarks and demonstrate its advantage compared to state-of-the-art single-surrogate BO baselines. We highlight the usefulness of our approach in finding good hyperparameter configurations in mixed (numerical and categorical) search spaces for a wide range of problems.",
    "keywords": [
        "hyperparameter optimisation",
        "Bayesian optimisation",
        "surrogate models"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=OhcWlo1M8q",
    "pdf_link": "https://openreview.net/pdf?id=OhcWlo1M8q",
    "comments": [
        {
            "summary": {
                "value": "DensBO offers a novel and dynamic approach to ensemble surrogate models in Bayesian optimization. While some transfer knowledge between tasks have been used in the past, this method uses the dynamic ensemble on the same task. This allows the use of different surrogate models at different stages of the optimization. This might also be useful to deal with numerical and categorical search spaces.\nThe method trains all surrogate models and creates a weighted ensemble, where the weights are updated using an exponential moving average between the previous and the new weights. At each time step, the surrogate model that has the lowest MSE error on the new sampled points, gets the weight of 1 and zero for the rest.\nThe paper evaluates the method on YAHPO Gym and JAHS-Bench-201 making use of a total of 859 available instances. The results show that the proposed method outperforms using individual models in Bayesian optimization. While the method comes at the cost of an additional hyperparameter, the paper convincingly demonstrates that the method is very robust as long as the hyperparameter is set to a high value.\nOverall, the paper proposes an interesting addition to the Bayesian optimization methodology, that is of general interest to the ICLR, but has one major weakness (see below) due to which I have to rate this paper borderline reject."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is easy to read and understand.\n* The paper explains the choices of the surrogate model by DensBO at the different stages of the optimization. Although gradient boosting (GB) is not competitive as a single surrogate model, the authors show that their model can pick random forest (RF)in the beginning of the optimization (few observations) and then dynamically chooses GB when they have a higher number of evaluations.\n*  They also show results on different budgets, which is very interesting to point out when the DensBO is not working well. In a small budget Gaussian processes outperform DensBO. In fact, ensembling requires training and querying more than one surrogate model, which means that it increases the overhead of BO, and should thus not be used in very low budget settings.\n* Finally, the explanation of the fraction budget used as a hyper parameter. The authors test and explain its effect on the behavior of DensBO."
            },
            "weaknesses": {
                "value": "* The experimental setup is uncommon for Bayesian optimization and might introduce unwanted bias into the evaluation. Concretely, the models are only updated every eight iterations (see line 819), and references the HEBO documentation and SMAC. However, I cannot find a clear recommendation to do so in the HEBO documentation, and also no trace of this in the SMAC paper [1]\n* Related work does not discuss meta-learning in Bayesian optimization, see for example the work by Martin Wistuba and Nicolas Schilling between 2015 and 2018. These are probably not helpful for solving the problem at hand, but at least should be mentioned.\n* Additional solutions to the 2021 NeurIPS BBO challenge besides HEBO that ensemble different BO methods are not discussed. In particular, the 2nd place, and the winner of the warm start leaderboard, used extensive ensembling of different BO methods (2nd place) and surrogates (winner of the warm start leaderboard, see also [2]).\n\nReferences:\n1. https://jmlr.csail.mit.edu/beta/papers/v23/21-0888.html\n2. https://arxiv.org/abs/2012.08180"
            },
            "questions": {
                "value": "* The fact that GPs do not work well on dimensions above 20 has been disputed recently [1, 2]. Also, the provided references about BO with GPs not working for a higher number of dimensions are 7 years old or older. Can you still make this claim using up-to-date-literature?\n* Scikit-learn\u2019s implementation of random forests and extremely randomized trees does not handle categorical features - how do you deal with this in practice? (The same might hold for gradient boosting, depending on the scikit-learn class used)\n* How is the uncertainty for gradient boosting computed?\n\nReferences:\n1. https://arxiv.org/abs/2402.02229\n2. https://arxiv.org/abs/2402.02746"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a new surrogate model for Bayesian optimization. They use a weighted ensemble of 4 different surrogates which are weighted best on their prediction performance in the past. This new surrogate is compared to each individual surrogate on several benchmarks with respect to rank."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is well-written and the idea clearly communicated. The paper uses sufficiently many benchmarks. The idea to dynamically choose the most suitable surrogate model during optimization is interesting."
            },
            "weaknesses": {
                "value": "The evaluation metric is not a suitable choice for this setup for two main reasons:\n\n1. We have clusters of surrogates: there are 5 variants of ENS, 3 variants of tree-based methods and 1 GP method. Why is this a problem? Let's assume we want to compare GP to ENS on 2 benchmarks. If ENS is better than the GP on one benchmark and the GP better on the other, both would get a rank of 1.5. If we had 5 variants of ENS (where these variants also have a clear ranking among each other), then ENS-best would get a rank of 1 on the problem where ENS does better than GP and a rank of 2 where the GP does better. However, the GP gets a rank of 6 where the ENS variants do better. Thus, the GP overall gets a rank of 3.5 and ENS-best 1.5. Only adding more variants of ENS make the GP look much worse than it is. In my opinion, we should compare 1 ENS variant with a GP and RF (no other tree-based methods).\n2. We don't see absolute differences: we see the GP doing initially very well for \"cheap\" benchmarks. The authors claim to eventually do better. However, differences towards the end of the optimization process might be completely meaningless (small absolute gains). \n\nThe approach to set the ensemble weights is not well motivated and not ablated (see \"Questions\" for ideas what to ablate). I do not understand why this depends on the answer to \"which model would have been best in evaluating the current point?\" if we continue choosing points somewhere else in the space.\n\nAdding the state-of-the-art for the respective benchmarks would be useful to give a useful baseline for the benchmarks.\n\nMissing baseline: use GP if the problem instance contains only continuous hyperparameters, otherwise use RF"
            },
            "questions": {
                "value": "How important is the initialization of w? What happens if we init all with equal weights?\n\nHow well does ENS with oracle w? This means that we choose only the surrogate among the available candidates that did best on the respective benchmark.\n\nWhat is the motivation for using 3 tree-based models? How would ENS change if there was only RF (best tree-based model) and GP?\n\nHow important is it to choose weights based on fit to last observe point rather than general fit?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors present DensBO, a method which dynamically uses a weighted ensemble of different surrogate models when optimizing the acquisition function."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-written and the method is clearly presented. \n- The ablation with the smoothing factor $\\alpha$ is interesting and provides insight into how much the prior performance should impact the ensemble. Furthermore, the promising performance of $\\alpha=1.0$ alleviates some of the overhead cost of using an ensemble surrogate model. \n- The authors conduct an extensive empirical evaluation on over 800 optimization problems and demonstrate that their ensemble method consistently outperforms the single models as well as the static ensemble. Furthermore, the ensemble is more effective than individual models when controlling for the total budget as well as controlling for the number of evaluations.\n- The experiments are described in detail and the results seem fully replicable."
            },
            "weaknesses": {
                "value": "- This paper does not provide enough evidence that ensembles, and the additional costs associated with fitting multiple models, outperform careful initial model selection across the many different objectives. It would be interesting to see another baseline for \u201cperformance of best single-surrogate model for problem\u201d, which would allow us to compare the impact of ideal model selection compared to ensembling. For a more realistic setting, we could also include another baseline of \u201cperformance of best single-surrogate model as determined by best MSE on initial points.\u201d If this baseline performs well, this indicates that the overhead cost of ensembles are unnecessary. \n- Furthermore, there is no comparison between DensBO vs other static ensembling methods which carefully select the ensemble model weights (rather than using a naive equal weighting). It is unclear how much of the improved performance originates from the dynamic ensembling proposed from the paper or from ensembling in general.\n- While I believe DensBO is the first to use a dynamic ensemble of GP and trees fit to the same BO optimization trajectory, there has been previous work which utilizes dynamic ensembling for BO which are weighted based on model fit [1, 2, 3]. If the claim is that the specific MSE weighting scheme is desirable, it would be helpful to see comparisons with other weighting schemes.\n\n1. Feurer et al, Scalable Meta-Learning for Bayesian Optimization using Ranking-Weighted Gaussian Process Ensembles, 2018.\n2. Lu et al, Ensemble Gaussian Processes with Spectral Features for Online Interactive Learning with Scalability, 2020.\n3. Nagy et al, Ensemble Gaussian Processes for Adaptive Autonomous Driving on Multi-friction Surfaces, 2023"
            },
            "questions": {
                "value": "- Have you tried running experiments with a different number of starting points? Due to the varying scaling properties of the individual surrogate models, the number of initial points may impact the performance of the methods given a fixed budget. Furthermore, considering your hypothesis about why GB works poorly in line 462, it may be helpful to understand the performance gap."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "&nbsp;\n\nThe authors introduce a dynamic ensembling approach for Bayesian optimization considering both continuous and mixed continuous/discrete parameter spaces. A key finding of the authors' work appears to be that model selection (the limiting case of the authors' approach when the exponential moving average weighting parameter alpha is set to 1) outperforms weighted ensembling across the majority of experiments. The empirical findings of the paper are informative and rigorous and as such, I am leaning towards accepting the paper. I do, however, have some major concerns with the baselining of prior work in model selection for Bayesian optimization, namely the approach taken in [6] that I believe should be incorporated. Additionally, the work would be further strengthened if a thorough ablation on the components of HEBO could be performed. Most notably if the model ensembling inherent to HEBO could be decoupled from the ensembling approach the authors' introduce. I will upgrade my score if these concerns can be addressed during the rebuttal phase, albeit I understand it will require extensive work on the part of the authors.\n\n&nbsp;"
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "&nbsp;\n\n1. The empirical evaluation of the authors' method is extensive and offers conclusive evidence that dynamic model selection outperforms a single surrogate model and a static ensemble across a suite of continuous and mixed continuous/discrete hyperparameter optimization tasks.\n\n2. The authors codebase is well-documented and I am led to believe that the results are fully reproducible.\n\n3. The paper is written in a clear and objective fashion.\n\n4. The finding that model selection outperforms a weighted ensemble in the majority of experiments is a very interesting finding for the Bayesian optimization community.\n\n&nbsp;"
            },
            "weaknesses": {
                "value": "&nbsp;\n\n**MAJOR POINTS**\n\n&nbsp;\n\n1. In Equation 6, the authors take the average of the individual model standard deviations to compute the ensemble standard deviation. In Equation 9 of [10], the authors provide an expression for the predictive uncertainty of the ensemble that decomposes uncertainty into aleatoric and epistemic uncertainty. As far as I can tell, Equation 6 is only computing the aleatoric uncertainty and is missing the term for epistemic uncertainty (disagreement between models). What was the authors' justification for omitting the epistemic uncertainty component of the predictive uncertainty?\n\n2. It would be worth adding the work from [6] as an additional baseline method since the parameter setting of alpha=1 i.e. model selection appears to perform best in experiments. This method, which uses Bayesian optimization in an inner loop to select the best model at each iteration of BO would be applicable to experiments over continuous hyperparameter spaces. It would be informative to see how model selection using the MSE compares against model selection using the marginal likelihood as in [6].\n\n3. For the empirical results in the appendix it may be worth plotting the log regret in place of the regret of each method. This may provide a clearer picture of the performance deltas to be expected between the methods.\n\n4. In Section 6, the baseline single GP implementation is presumably implemented using HEBO? If so, it makes some sense that the single GP variant of HEBO is performant in the low sample regime as early in the optimization trace, the treatment of the GP hyperparameters is fully Bayesian and hence there is an inherent notion of model ensembling. It would be interesting to run an ablation with the robust acquisition formulation (fully Bayesian treatment of hyperparameters) turned off. This could be achieved by using only the input/output warping and MACE components of HEBO. This would also decouple the inherent model ensembling within HEBO from the ensembling introduced by the authors.\n\n5. It would be worth emphasizing in the introduction as well as the overall narrative of the paper, the result that alpha=1 i.e. model selection outperforms ensembling in the experiments. This seems to be a key finding of the paper.\n\n&nbsp;\n\n**MINOR POINTS**\n\n&nbsp;\n\n1. There are some missing capitalizations in the references e.g. \"Bayesian\" in place of \"bayesian\".\n\n2. When citing Bayesian optimization on line 47, it may be worth referencing some of the originating papers [2, 3] as discussed in [4].\n\n3. On line 84, the statement that Gaussian processes are performant in settings where the dimension is lower than 20 should potentially be moderated in light of recent work such as [5] which shows that Bayesian optimization can be performant in problems with 100s of dimensions if the lengthscale prior is scaled with the dimensionality. \n\n4. In light of the major points above, the sentence, \"We present, for the first time, a dynamic ensembling approach for surrogate models in the context of HPO\" may need to be moderated in light of [6] which introduces an ensemble of GP models (cf. Equation 8 in [6]) for Bayesian optimization and reports experimental results for hyperparameter tuning (neural networks and SVMs) of models fit on UCI datasets.\n\n5. In the codebase it may be beneficial to rename the HEBO directory since I initially assumed this was directly copied from the HEBO codebase whereas in fact it contains code relevant for the method introduced in this paper.\n\n6. On line 134, why is the loss function \"estimated\"?\n\n7.  Line 139, see point 2 above.\n\n8. On line 142, a set notation with curly braces may be more appropriate than parentheses.\n\n9. On line. 154, is there a typo with the bolding of lambda in the correlation vector?\n\n10. When citing the expected improvement acquisition function [9] should be cited as discussed in [4].\n\n11. On line 217, in terms of the claim that this work introduces dynamic model ensembling for hyperparameter optimization see again the major points above, namely the reference to [6].\n\n12. Line 230, extraneous colon.\n\n13. Missing full stop at the end of Equation 16.\n\n14. Figure 9, extraneous bracket around panel d. It may be worth additionally explaining the black dotted line in the figure as the distance to the closest variance bound.\n\n15. In line 3 of Algorithm 1 it would be worth specifying how the model weights are initialized.\n\n16. The notation in line 5 of Algorithm 1 is a little confusing. For example mu and sigma should be vector quantities and the notation suggests the models are fit on lambda alone and not the associated costs.\n\n17. On line 7, it may be better if lambda_t is defined as a vector. Similarly on line 8.\n\n18. On line 307, \"heteroscedasticity and non-stationarity respectively\" may be a better phrasing given that the Box-Cox and Yeo-Johnson transforms in HEBO address heteroscedasticity specifically whereas the Kumaraswamy transform addresses non-stationarity specifically. The authors indeed emphasize this in Appendix C.\n\n19. In Section 4, in the description of the aspects of HEBO that yield performance gains, there are two additional components worthy of mention. The first, is the fact that HEBO uses the Multi-objective ACquisition Ensemble (MACE) method of [11] which ensembles the EI, PI, and UCB acquisition functions. The second, is that HEBO uses a robust acquisition function formulation which is equivalent to a fully Bayesian approach to the GP hyperparameters early in the optimization trace. In the HEBO paper, all 4 components were shown to improve performance independently of each other in an ablation study.\n\n20. It would be worth citing the MACE paper [11] as the source work for the approach taken in HEBO. Out of interest, does the reference by Forrester also include such an acquisition function ensemble?\n\n21. In the caption of Figure 2, it would be worth adding the number of random trials the errorbars are computed over.\n\n22. Missing full stop at the end of Equation 9.\n\n23. On line 784, I believe noise may still be heteroscedastic, yet adhere to a Gaussian noise model, save for the fact that the parameters of the Gaussian distribution will vary depending on the position in the input space? Willing to discuss this point further!\n\n24. Equation 10 may be more clearly defined as a piecewise function?\n\n25. It would be worth citing the Adam optimizer [12] given that it is used.\n\n26. In line 845, the Q-function (state-action value function) holds the predicted discounted future return (discounted cumulative reward) rather than the expected simple reward.\n\n27. For the significance test in Section I of the Appendix it would be good to formally state the null hypothesis and report the p-value of the permutation test.\n\n&nbsp;\n\n**REFERENCES**\n\n&nbsp;\n\n[1] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. and Agarwal, S., 2020, December. [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf?ref=hackernoon.com). In Proceedings of the 34th International Conference on Neural Information Processing Systems (pp. 1877-1901).\n\n[2] Kushner, HJ., [A Versatile Stochastic Model of a Function of Unknown and Time\nVarying Form](https://www.sciencedirect.com/science/article/pii/0022247X62900112). Journal of Mathematical Analysis and Applications 5(1):150\u2013167. 1962.\n\n[3] Kushner HJ., [A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise](https://asmedigitalcollection.asme.org/fluidsengineering/article-abstract/86/1/97/392213/A-New-Method-of-Locating-the-Maximum-Point-of-an?redirectedFrom=fulltext). Journal of Basic Engineering 86(1):97\u2013106. 1964.\n\n[4] Garnett, R., [Bayesian optimization](https://bayesoptbook.com/). Cambridge University Press. 2023.\n\n[5] Hvarfner, C., Hellsten, E.O. and Nardi, L. [Vanilla Bayesian Optimization Performs Great in High Dimensions](https://proceedings.mlr.press/v235/hvarfner24a.html). Proceedings of the 41st International Conference on Machine Learning, in Proceedings of Machine Learning Research 235:20793-20817. 2024.\n\n[6] Malkomes, G. and Garnett, R., [Automating Bayesian optimization with Bayesian optimization](https://proceedings.neurips.cc/paper_files/paper/2018/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf). Advances in Neural Information Processing Systems, 31. 2018.\n\n[7] Malkomes, G., Schaff, C. and Garnett, R., [Bayesian optimization for automated model selection](https://proceedings.neurips.cc/paper/2016/hash/3bbfdde8842a5c44a0323518eec97cbe-Abstract.html). Advances in Neural Information Processing Systems, 29. 2016.\n\n[8] Gardner, J., Guo, C., Weinberger, K., Garnett, R. and Grosse, R.,[Discovering and exploiting additive structure for Bayesian optimization](https://proceedings.mlr.press/v54/gardner17a.html). In Artificial Intelligence and Statistics (pp. 1311-1319). PMLR. 2017.\n\n[9] Saltines, VR., One Method of Multiextremum Optimization. Avtomatika i Vychislitel\u2019naya Tekhnika (Automatic Control and Computer Sciences) 5(3):33\u201338. 1971.\n\n[10] Kendall, A. and Gal, Y., [What uncertainties do we need in bayesian deep learning for computer vision?](https://proceedings.neurips.cc/paper/2017/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html). Advances in Neural Information Processing Systems, 30. 2017.\n\n[11] Lyu, W., Yang, F., Yan, C., Zhou, D. and Zeng, X., [Batch Bayesian optimization via multi-objective acquisition ensemble for automated analog circuit design](https://proceedings.mlr.press/v80/lyu18a/lyu18a.pdf). In International Conference on Machine Learning (pp. 3306-3314). PMLR. 2018.\n\n[12] Kingma and Ba, [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980), ICLR 2015.\n\n&nbsp;"
            },
            "questions": {
                "value": "&nbsp;\n\n1. In the introduction could the authors explain why the citation to \"Language Models are few-shot learners\" is an appropriate citation for the point that evaluating a single hyperparameter configuration of a model can be very expensive?\n\n2. What is the motivation for assessing MSE only on the newly sampled points and not on the entire dataset collected so far in the trace? Do the authors have any intuition for how behavior would change if the MSE was defined on the full dataset? The motivation is outlined partially in Section C.2 of the Appendix.\n\n&nbsp;"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "&nbsp;\n\nNo ethical concerns.\n\n&nbsp;"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}