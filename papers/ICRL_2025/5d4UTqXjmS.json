{
    "id": "5d4UTqXjmS",
    "title": "Visual Large Language Models Exhibit Human-Level Cognitive Flexibility",
    "abstract": "Cognitive flexibility has been extensively studied in human cognition but remains relatively unexplored in the context of Visual Large Language Models (VLLMs). This study assesses the cognitive flexibility of state-of-the-art VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card Sorting Test (WCST), a classic measure of set-shifting ability. Our results reveal that VLLMs achieve or surpass human-level set-shifting capabilities under chain-of-thought prompting with text-based inputs. However, their abilities are highly influenced by both input modality and prompting strategy. In addition, we find that through role-playing, VLLMs can simulate various functional deficits aligned with  patients having impairments in cognitive flexibility, suggesting that VLLMs may possess a cognitive architecture, at least regarding the ability of set-shifting, similar to the brain. This study reveals the fact that VLLMs have already approached the human level on a key component underlying our higher cognition, and highlights the potential to use them to emulate complex brain processes.",
    "keywords": [
        "Cognitive Flexibility",
        "Visual Large Language Models",
        "Wisconsin Card Sorting Test"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "This study evaluates the cognitive flexibility of Visual Large Language Models (VLLMs) using the Wisconsin Card Sorting Test, revealing their human-like set-shifting capabilities influenced by input modality and prompting strategies.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5d4UTqXjmS",
    "pdf_link": "https://openreview.net/pdf?id=5d4UTqXjmS",
    "comments": [
        {
            "summary": {
                "value": "This article studies the cognitive flexibility of three multimodal large language models\u2014Gemini, ChatGPT, and Claude\u2014that support both text and image input using the WCST test. Cognitive flexibility here refers to the models' ability to adjust their understanding of task rules and complete tasks correctly based solely on feedback indicating correctness or incorrectness. The experiment includes SaT-VI, SaT-TI, CoT-VI, and CoT-TI conditions, where SaT means no chain-of-thought guidance and the model outputs answers directly, while CoT involves chain-of-thought guidance. The results show that CoT significantly outperforms SaT, achieving or surpassing human-level performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.This study uses the WCST to examine the cognitive flexibility of VLLMs. The WCST is widely applied in cognitive science and is known for its strong reliability.2.The authors explored the potential of VLLMs to simulate specific patterns of cognitive impairment through role-playing."
            },
            "weaknesses": {
                "value": "1.Although the article mentions that the simulated patterns of the models align with real cases, the authors did not conduct cognitive experiments or correlate data with real subjects to demonstrate that VLLMs' simulation of cognitive impairment is reasonable.\n2.The article only evaluates the models on a specific cognitive test (WCST). While the WCST is a classic test in cognitive science, it lacks real-world simulation, and performance on this test cannot fully represent performance in real-world scenarios.\n3.The authors should consider incorporating more visualizations."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work aims to evaluate the cognitive flexibility of vision language models (VLMs), using a classic task from the neuropsychological literature (the Wisconsin Card Sort Task). The authors conclude that, under certain conditions (depending on input modality and prompting technique), VLMs can display human-level flexibility. Experiments are also reported in which prompting is used to simulate neuropsychological impairment."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- This work employs a well validated task from the neuropsychological literature, potentially enabling a rich comparison with human cognition.\n- The experiments investigate several state-of-the-art VLMs, increasing the robustness of the findings."
            },
            "weaknesses": {
                "value": "- Most importantly, the results are not diagnostic regarding the relative cognitive flexibility of VLMs/LLMs and humans. This is because the human participants are effectively at ceiling. In order to have a meaningful comparison, a version of the task (or a different task) would need to be identified where human performance was not at ceiling.\n- No theoretical motivation is provided for investigating cognitive flexibility in LLMs / VLMs. It is noted that this is a well studied task in the neuropsychology literature, which is true, but this does not automatically yield theoretically important questions about LLMs / VLMs. It is also suggested that 'This investigation not only advances our understanding of VLLMs but also offers insights into the nature of cognitive flexibility itself,' but it is not clear what insights this work offers about cognitive flexibility.\n- There is also no explicit motivation for studying VLMs in particular, as opposed to LLMs. Is there any particular reason why it is important to study these processes in the visual domain?\n- The paper only includes experiments with a single task. Many more tasks and conditions would be needed to support the claims that are advanced in this paper.\n- For tests of large-scale pretrained models such as LLMs and VLMs, it is also important to try and ensure that the tasks used for evaluation are not present in the model's training data. This is a concern here given the popularity of this task in the cognitive literature. One possible approach might be to also test an equivalent version of the task that uses different surface features, to ensure that performance does not depend on memorization (or pseudo-memorization).\n- There are no statistical tests provided throughout the entire paper, although there are many statements about the differences between certain conditions. It is important to perform statistical tests to determine which of these differences are reliable.\n- It is unclear what's learned from the experiments simulating neuropsychological impairment. There are some assertions about similarities to the pattern of behavior in certain patient populations, but very few references, and no direct comparison with human behavior. It would be ideal to have a direct comparison with behavior to support such claims."
            },
            "questions": {
                "value": "### Questions\n- What is the theoretical motivation for studying cognitive flexibility in VLMs / LLMs?\n- What is the theoretical motivation for studying cognitive flexibility in VLMs in particular? What does the visual domain add to such an evaluation?\n\n### Suggestions\n- The task should be modified so as to identify conditions where human performance is not at ceiling.\n- More tasks should be investigated. \n- Statistical tests should be included to support comparisons.\n- A direct comparison with human behavior should be included for the experiments simulating neuropsychological impairment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper \"Visual Large Language Models Exhibit Human-Level Cognitive Flexibility\" evaluates the cognitive flexibility of state-of-the-art VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card Sorting Test (WCST). It finds that VLLMs can match/surpass human performance in adapting to changing rules, especially with chain-of-thought reasoning and text-based inputs.\n\nKey contributions:\n1. VLLMs demonstrate human-level cognitive flexibility, particularly with CoT prompting.\n2. Performance significantly changes based on input  modality (text vs. visual) and prompting strategy.\n4. VLLMs can simulate cognitive impairments, offering potential for modeling brain function.\n\nThe study suggests that VLLMs has some cognitive abilities and points to potential in advanced applications in AI and neuroscience."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Experiment methodology: The paper is methodically thorough, using a well-established cognitive flexibility test WCST and evaluating SOTA VLLMs.. The experimental design includes 4 different setups and 6 scoring functions. This enables a detailed comparison under varied conditions, providing some level of robustness to the findings. Including human participants as a comparative baseline grounds the findings in a relatable context.\n\nClarity: The paper is easy to follow and well-organized, with a clear explanation of the WCST, input modalities, and experimental conditions. The results are presented in detailed tables and figures, aiding in the understanding of model performance comparisons\n\nOriginality: The paper explores an area by applying well known congnitive test to assess performance  in VLLMs. It introduces a unique approach by examining how these models simulate cognitive impairments, adding some level of depth and innovation to the study."
            },
            "weaknesses": {
                "value": "Overstatement of Cognitive Flexibility Claims: Although the paper demonstrates that VLLMs can achieve human-level performance in the WCST under specific conditions, the claim that they exhibit human-like cognitive flexibility seems overstated. Cognitive flexibility in humans involves a broader spectrum of real-world applications and examined using multiple tests, and the findings are limited to a highly structured test. A more cautious interpretation of the results would strengthen the paper's scientific rigor.\n\nRole-playing Cognitive Impairments Needs Validation: While simulating cognitive impairments through role-playing prompts is innovative, this method remains speculative without validation against clinical populations. The paper could improve by discussing potential methods for validating these simulated impairments against real-world data, making the findings more actionable and grounded in reality.\n\nInsufficient Validation of Prompt Designs: While the paper employs CoT and STA prompting strategies, it does not fully explore the impact of different prompting setups or attempt to validate the prompts across varied task conditions and models. For example, the reliance on CoT prompting for achieving high performance raises questions about how much of the cognitive flexibility observed in VLLMs is genuinely attributable to their internal architecture versus the external aid provided by sophisticated prompts (which for some of the models are not advertised as the suggested approach)."
            },
            "questions": {
                "value": "1. While the paper demonstrates that VLLMs can achieve human-level performance in the WCST, the broader claim of human-like cognitive flexibility seems to require more context. Could the authors clarify how they see the findings generalizing to real-world applications? Specifically, how do the authors view the limitations of the WCST in capturing the full spectrum of cognitive flexibility in humans, and do they plan to evaluate VLLMs using additional tests that capture a wider range of flexibility?\n\n2. The paper heavily relies on a specific cot prompting approach to achieve high performance. Could the authors provide more details about how different prompting strategies and setups affect the models' performance across tasks? Specifically, does prompt wording changes the performance significantly? \n\n3. The paper evaluates three SOTA models. How do the authors envision their findings generalizing to other models (includingn on VLLMs), especially those with different architectures or less advanced capabilities? Are there plans to extend the study to a broader range of models or to compare different architectural approaches to cognitive flexibility? \n\n4. Cognitive flexibility in real-world settings often involves adapting to highly dynamic environments where rules are unclear and change rapidly. Do the authors have plans to test the models in more dynamic, less structured tasks where adaptability is required in real time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}