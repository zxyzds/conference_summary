{
    "id": "gJk4N7zscD",
    "title": "Stochastic Monkeys at Play: Random Augmentations Cheaply Break LLM Safety Alignment",
    "abstract": "Safety alignment of Large Language Models (LLMs) has recently become a critical objective of model developers. In response, a growing body of work has been investigating how safety alignment can be bypassed through various jailbreaking methods, such as adversarial attacks. However, these jailbreak methods can be rather costly or involve a non-trivial amount of creativity and effort, introducing the assumption that malicious users are high-resource or sophisticated. In this paper, we study how simple random augmentations to the input prompt affect safety alignment effectiveness in state-of-the-art LLMs, such as Llama 3 and Qwen 2. We perform an in-depth evaluation of 17 different models and investigate the intersection of safety under random augmentations with multiple dimensions: augmentation type, model size, quantization, fine-tuning-based defenses, and decoding strategies (e.g., sampling temperature). We show that low-resource and unsophisticated attackers, i.e. $\\textit{stochastic monkeys}$, can significantly improve their chances of bypassing alignment with just 25 random augmentations per prompt.",
    "keywords": [
        "LLM",
        "Large Language Model",
        "safety alignment",
        "augmentations",
        "randomness",
        "jailbreaks",
        "attacks"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We show that the safety alignment of state-of-the-art LLMs can be cheaply bypassed with simple random augmentations.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=gJk4N7zscD",
    "pdf_link": "https://openreview.net/pdf?id=gJk4N7zscD",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates a simple but effective jailbreak attack using random perturbations. The experimental results demonstrate that this attack can significantly improve the chances of bypassing the safety alignment of 17 different models. Additionally, the paper examines how argumentation type, model size, and decoding strategy impact safety measures."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is well-written with clear problem formulation\n2. This paper demonstrate how easy the safety alignment can be bypassed\n3. The ablation analysis of the proposed argumentation attack is detailed"
            },
            "weaknesses": {
                "value": "The positioning and technical contribution of this paper seems to be unclear.\n1. As a jailbreak attack paper, it lacks comparison with state-of-the-art blackbox jailbreak attacks (such as PAIR and DeepInception) in terms of attack success rate and computational cost. To strengthen the paper's contribution, the authors may need to include comprehensive comparisons with these baseline methods under the same evaluation settings.\n2. As an analysis paper, despite being comprehensive, the conclusions are not new to the community. The techniques used (i.e., perturbation) are already widely employed to evaluate LLMs' safety and robustness (e.g., smoothllm). Furthermore, the impacts of model size, quantization, fine-tuning, and generation configurations on LLM safety have been well-studied previously."
            },
            "questions": {
                "value": "1. How frequently do random augmented prompts affect the semantic meaning of the original malicious prompt? Since the output is binary, could this lead to false safe/unsafe classifications by the evaluator, potentially impacting the experimental results?\n2. Have you considered combining random augmentations with existing jailbreak attacks to see if it can lead to better performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examines the vulnerability of state-of-the-art large language models (LLMs) to random input augmentation attacks. It explores how different augmentation strategies, model configurations, and generation parameters affect attack success rates. The authors focus on two main augmentation types: Character-Level and String Insertion. Experimental results show that Character-Level augmentation yields higher attack success. Larger models demonstrate better resistance in general, though size alone doesn't ensure robustness. Furthermore, aggressive quantization tends to increase susceptibility to attacks, with variable effects across models. The paper also tests a fine-tuning-based defense, finding that it enhances resistance against longer attacks but remains vulnerable to shorter ones, with resistance varying based on attack lengths simulated during fine-tuning. A side effect noted is an increased rejection rate for benign prompts. Through extensive experimentation, the authors conclude that random input augmentation is a low-cost yet effective attack against LLM safety alignment, providing insights for future work in enhancing model robustness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-organized and clearly written, with a coherent structure and logical progression that supports the arguments effectively.\n2. The paper examines the impact of various aspects such as LLM parameter size, quantization strategies, sampling parameters, and fine-tuning on resisting random input augmentation attacks"
            },
            "weaknesses": {
                "value": "1. This paper focused on open-source chat models but lacks research on system prompts in the attack and defense settings\n2. This paper did not discuss the impact of different tokenizer vocabulary sizes on this attack"
            },
            "questions": {
                "value": "1. From the attack perspective, the authors did not discuss how the system prompt was configured for the open-source models in the experimental setup. It\u2019s possible they used the default system prompt for each model or left it empty. In fact, setting the system prompt with a defensive focus (e.g., instructing the model to only output safe content) could impact the results. From a defense perspective, the authors should also examine whether using a system prompt as a defense mechanism is effective.\n\n2. The authors should discuss the vocabulary sizes of the different LLMs tested. Larger vocabulary models are more likely to contain \"glitch tokens,\" which may not have been fully trained and could potentially be more vulnerable to the attack methods proposed. Would these models be more susceptible to such attacks due to this factor?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates how simple random input augmentations can bypass safety alignment in state-of-the-art large language models. The authors demonstrate that \u201cstochastic monkeys\u201d (low-resource, unsophisticated attackers) can evade safety mechanisms in LLMs by applying 25 random augmentations to each prompt. Besides, they evaluate these augmentations across several dimensions, such as augmentation type, model size, quantization, fine-tuning-based defenses, and decoding strategies, highlighting that random augmentations enable bypassing of safety features with minimal resources. Notably, character-level augmentations consistently outperform string insertion in breaking alignment. Finally, the paper proposes a novel evaluation metric to evaluate safety and includes a human study to balance false positive and negative rates, furthering the robustness of their findings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper applies simple random augmentations as a threat model, contributing a fresh perspective to the understanding of LLM safety alignment vulnerability.\n\n2. The study is thorough, evaluating the effectiveness of random augmentations across multiple dimensions, and the use of a human-validated metric strengthens the reliability of the findings.\n\n3. By demonstrating that low-cost, unsophisticated methods can bypass LLM safety features, the paper reveals critical vulnerabilities that may shape future research in secure LLM deployment, particularly for sensitive or public-facing applications."
            },
            "weaknesses": {
                "value": "1. While the authors discuss their method\u2019s simplicity relative to adversarial attacks, a more detailed comparison against other established jailbreak and adversarial attack methods would better contextualize the effectiveness of random augmentations.\n\n2. The study could benefit from an expanded discussion on the practical implications of these findings, especially in terms of what types of applications or LLM deployment scenarios are most at risk and possible mitigations.\n\n3. The quantization experiment results in Figure 4 show variability across different models and sizes, but the authors provides vague explanations for these differences. Further exploring why certain models (e.g., Qwen 2) exhibit unique behavior under augmentations would enhance the study.\n\n4. The writing of the conclusion is inconsistent with the abstract and conclusion, making it difficult to understand the main takeaway of this paper."
            },
            "questions": {
                "value": "1. Can the authors provide a comparison of the success rate of random augmentations against the success rate of more sophisticated adversarial attacks in the same LLM models?\n\n2. Among all these various dimensions( augmentation type, model size, quantization, fine-tuning-based defenses, and decoding strategies), can authors provide a ranking of these factors based on their significance towards the attack performance? In other words, the reader would like to know which factors should be the top concerns in their LLM applications. \n\n3. Have the authors considered examining the temporal robustness of this attack, i.e., does the attack success vary over repeated attempts or prolonged interaction with the same model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates jailbreaking open-source LLMs via small adjustments to the prompt. The paper investigates two types of augmentations: a \"string\" level insertion of a new word, and a \"character\" level augmentation which edits, deletes, or inserts a character.\nThe paper evaluates using the SORRY-bench dataset, across a variety of small language models, showing that augmentation can improve ASR under a variety of sampling temepratures. The paper also evaluates the attack under different defenses and under model quantization."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The method is simple and shows promise as a new form of jailbreak attack. The writing is clear and easy to understand, and the results are well-presented."
            },
            "weaknesses": {
                "value": "I was not entirely convinced by the evaluation results; Although they looked promising, it would be much stronger if the paper also included more diverse results.\n\nFor example, evaluation with/without system prompts would help clarify limitations of the approach. Additional experiments with larger models, or with instruction-tuned models would also further convince me that the approach is robust and general (or more importantly, reveal something interesting about the nature of LLM robustness to perturbed prompts). \n\nI would also like to see more comparison to prior work; The paper evaluates comparing to (Huang et al 2023), which I felt was interesting as it demonstrates an attack surface that isn't covered by prior work (this point should be emphasized more).\n\nTo make this more complete, it would be interesting if the paper compared against the other cited work: Andriushchenko & Flammarion, 2024, Vega et al. 2023, and Zou et al. 2023."
            },
            "questions": {
                "value": "Restating a few points from Weaknesses:\n\n1) How does your method compare to prior work that adjusts prompts? While there is some evaluation comparing to Huang et al., it would be extremely interesting to build a more complete picture of the attack surface\n\n2) How does your attack perform on instruction-tuned models and when including (or excluding) system prompts? (it's also unclear if system prompts were used during evaluation)\n\n3) How susceptible are closed-source models to this attack? \n\n4) Have you tried string-level augmentations that insert words from a dictionary instead of random text? Would this meaningfully change anything?\n\n5) I could not find any data with varying choices of p. While the paper mentions p=0.05 works well, it would be interesting to see how p changes the strength of the attack."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Facing the demand of high-resource and sophisticated design of prompts in jailbreaking attacks on LLM safety alignment, this paper studies how simple random augmentations to the input prompt bypass safety alignment. The authors perform an evaluation of 17 models and investigate random augmentations with multiple dimensions, such as augmentation type, model size, etc."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The method proposed by the author is simple yet effective. Random augmentation is a simple modification to input prompts, yet such method can increase the success rate of harmful requests by up to 20~26%.\n2. The evaluation across multiple dimensions is thorough. The authors investigate random augmentation in augmentation type, model size, quantization, fine-tuning-based defenses, and decoding strategies. And the authors reveal several observations, such as character-level augmentations tend to be more effective than string insertion augmentations."
            },
            "weaknesses": {
                "value": "1. The authors claim their method to be \"cheap\", but no experiments support this point. The authors can add computation cost compared to other jailbreaking methods.\n2. There is no comparison to other jailbreaking methods. The authors' statement that \"adversarial attacks typically assume white-box...random augmentations are black-box...we do not compare...\" is not reasonable. There also exist some jailbreaking methods against black-box models. The authors should compare with them. (https://arxiv.org/abs/2310.08419 ; https://arxiv.org/abs/2312.02119)\n3. No widely-used commercial LLMs are evaluated. The authors should evaluate the proposed methods against widely-used commercial LLMs (e.g. ChatGPT), where bypassing the safety alignment is a more significant threat.\n4. There is no discussion of mitigation methods against the proposed methods."
            },
            "questions": {
                "value": "1. How safety judge work? Is it done by LLMs, human experts or word matching?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}