{
    "id": "TbRFc2HPoN",
    "title": "Distributionally Robust Policy Learning under Concept Drifts",
    "abstract": "Distributionally robust policy learning aims to find a policy that performs well \nunder the worst-case distributional shift, and yet most existing methods for \nrobust policy learning consider the worst-case joint distribution of \nthe covariate and the outcome. The joint-modeling strategy can be unnecessarily conservative\nwhen we have more information on the source of distributional shifts. This paper studies\na more nuanced problem --- robust policy learning under the concept drift, \nwhen only the conditional relationship between the outcome and the covariate changes. \nTo this end, we first provide a doubly-robust estimator for evaluating\nthe worst-case average reward of a given policy under a set of perturbed conditional distributions. \nWe show that the policy value estimator enjoys asymptotic normality even if the nuisance parameters \nare estimated with a slower-than-root-$n$ rate. We then propose a learning algorithm that outputs the policy maximizing the \nestimated policy value within a given policy class $\\Pi$, and show\nthat the sub-optimality gap of the proposed algorithm is of the order \n$\\kappa(\\Pi)n^{-1/2}$, with $\\kappa(\\Pi)$ is the entropy integral of $\\Pi$ under the Hamming distance\nand $n$ is the sample size.\nThe proposed methods are implemented and evaluated in numerical studies, \ndemonstrating substantial improvement compared with existing benchmarks.",
    "keywords": [
        "distributionally robust optimization",
        "offline policy learning",
        "concept drift",
        "bandit learning",
        "reinforcement learning."
    ],
    "primary_area": "causal reasoning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TbRFc2HPoN",
    "pdf_link": "https://openreview.net/pdf?id=TbRFc2HPoN",
    "comments": [
        {
            "summary": {
                "value": "This paper studies robust policy learning under the concept drift, where the distributional shift occurs only in the conditional reward distribution. The authors first develop a doubly-robust estimator for the worst-case policy value under concept drift, which is proved to be asymptotic normal, and then design a robust policy learning algorithm which achieves $n^{-1/2}$ regret."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Separating concept shift from covariate shift offers a fresh and insightful perspective in this field. The methods developed in this paper come with strong theoretical guarantees: the policy value estimator exhibits asymptotic normality, and the policy learning algorithm has $n^{-1/2}$ sub-optimality gap, improving previous results. Additionally, experiments conducted in a synthetic environment demonstrate that the proposed algorithm consistently outperforms baseline methods, further supporting the effectiveness of the approach."
            },
            "weaknesses": {
                "value": "While the paper is motivated by the idea that existing methods may be suboptimal under concept shift alone, it does not clearly demonstrate how its proposed rates improve upon existing ones. The improvement noted in Remark 4.3 seems more a result of advanced theorem-proving techniques (e.g., chaining) rather than a genuine improvement in performance metrics. Additionally, while the introduction raises the question of \"optimal worst-case average performance\" (lines 75-77), the paper lacks an optimality analysis of the derived bounds. Consequently, the theoretical contributions do not align strongly with the paper's motivation and feel somewhat insufficient in addressing the posed questions.\n\nFurthermore, the experiments are conducted solely in a synthetic environment, with no practical examples illustrating the benefits of the proposed algorithm."
            },
            "questions": {
                "value": "How are the rates here compared with existing rates?\n\nAre there lower bounds of value estimation and regret?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the problem of distributionally robust contextual bandit, where the uncertainty only lies in the conditional reward distribution P(Y|X).\n\nSome major comments are listed below:\n1. First, Lemma 2.3 cannot be claimed as the contribution of this paper, as it is a standard result in the literature of KL constrained DRO. The authors shall provide credit to existing literature properly. \nKullback-Leibler Divergence Constrained Distributionally Robust Optimization, Hu and Hong\nMoreover, min over \\eta in (3) has a closed form solution. Please check theorem 1 in the above paper. Therefore, there is no need for an iterative method to obtain \\eta^*.\n\n2. The technical presentation is hard to follow. Notations are not defined or not clearly introduced before use. For example g_\\pi is not defined. What does propensity score function \\pi_0 represent is not clear. Does the propensity score function mean the same thing as the behavior policy? \n\n3. Assumption 3.3. holds if $\\theta^*_{\\pi}(x)$ is continuous in x. Can the authors provide examples for this to hold?\n\n4. The problem in this paper is in an offline setting, however, assumption 2.1 guarantees that any action can be visited in the training dataset with high probability, and therefore, the most challenging part of offline RL, e..g, partial coverage, is not addressed in this paper. How the performance depends on the concentrability coefficient is not clear from this paper. This limits the significance of this work. \n\n5. This paper investigates only distributional shift in the conditional reward, however, this does not seems to require development of new techniques comparing to distribution shift in the entire P(x,y)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "see the summary"
            },
            "weaknesses": {
                "value": "see the summary"
            },
            "questions": {
                "value": "see the summary"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses distributionally robust policy learning under concept drift, focusing on cases where only the conditional relationship between covariates and outcomes changes. Using a KL-divergence-based framework, the authors define an uncertainty set that constrains shifts in the conditional distribution and develop a computational approach supported by duality to evaluate and optimize policies efficiently. They provide theoretical guarantees, including asymptotic normality and convergence rates, adding robustness to the methodology. Initial numerical results suggest the approach is less conservative than traditional joint distribution shift methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper discusses distributionally robust policy learning under concept shift. It relies on standard assumptions and considers a KL-divergence-based evaluation framework. It presents a duality result and provides an efficient estimator based on this result, with guarantees for asymptotic estimation rates and asymptotic normality.\n\nThe result is interesting and proposed concept shift makes sense because in practice, considering the worst-case joint distribution of the covariate and the outcome can be too conservative."
            },
            "weaknesses": {
                "value": "- The duality and theoretical results appear fairly standard, with proofs similar to existing literature. \n- The numerical results are very preliminary and lack in-depth discussion.  For example, $\ud835\udc65$ is assumed to follow a Gaussian distribution. \n- Additionally, the authors should expand on the results in Table 1. While they claim their method is better, the table mainly shows it is less conservative (which is expected, given the smaller uncertainty set they consider compared to the joint distribution worst-case uncertainty set). \n- They should provide more justification for why this choice makes sense in practice and discuss how to empirically determine the appropriate uncertainty type for policy learning (joint distribution shift vs. concept shift), which is not covered in the experiments."
            },
            "questions": {
                "value": "The authors could also expand on how to choose the uncertainty set parameter, as the results depend heavily on this choice. Additionally, they could discuss why they opted for KL-divergence over alternatives like Wasserstein distance, which might offer different insights or benefits in the context of distributional robustness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}