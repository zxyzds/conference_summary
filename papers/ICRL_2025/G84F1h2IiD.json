{
    "id": "G84F1h2IiD",
    "title": "Spike No More: Stabilizing the Pre-training of Large Language Models",
    "abstract": "Loss spikes often occur during pre-training of large language models.\nThe spikes degrade the performance of large language models and sometimes ruin the pre-training.\nSince the pre-training needs a vast computational budget, we should avoid such spikes.\nBased on the assumption that the loss spike is caused by the sudden growth of the gradient norm, we explore factors to keep the gradient norm small through an analysis of the spectral norms of the Jacobian matrices for the sub-layers.\nOur findings suggest that stabilizing the pre-training process requires two conditions: small sub-layers and large shortcut.\nWe conduct various experiments to empirically verify our theoretical analyses.\nExperimental results demonstrate that methods satisfying the conditions effectively prevent loss spikes during pre-training.",
    "keywords": [
        "neural language model",
        "transformer",
        "llm"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=G84F1h2IiD",
    "pdf_link": "https://openreview.net/pdf?id=G84F1h2IiD",
    "comments": [
        {
            "title": {
                "value": "Thank note"
            },
            "comment": {
                "value": "I implemented this method in [my fork of Fish-Speech](https://github.com/bfs18/poorman-fish-speech), and it has proven to be highly effective. The previously observed spikes in gradient norm and loss have vanished. My model, a 0.5B AR transformer-based text-to-speech system, has greatly benefited from this method."
            }
        },
        {
            "summary": {
                "value": "This paper presents a strategy to avoid spikes in loss during the training of LMs by keeping the gradient norm small. To manage the upper limit of gradient norms effectively, the method involves (i) using small initial values for sub-layers, and (ii) maintaining the standard deviation of embeddings around 1."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The findings (i) and (ii) from the analysis are well presented, although they have been previously utilized in past studies.\n* This work examines various learning hyperparameters.\n* This work also presents the results for the 13B model in Table 6 of the Appendix.\n* The paper is well-written and easy to understand."
            },
            "weaknesses": {
                "value": "* Although the theoretical analysis is intriguing, I question the practical value of this work, as most practices described in Section 4 are already in use. Utilizing small values for initialization to ensure stable training is well-known, and both Scaled Embed and Embed LM have been introduced in prior literature. If this work could offer a novel, advanced method for embedding normalization, it might receive more interest from the community.\n* The activation function F was assumed to be either an identity function or ReLU, as stated on line 152 of page 3. What would be the results if widely used activation functions in recent LLMs, such as SiLU and SwiGLU, were applied?\n* I am curious about how loss spikes impact the performance of downstream tasks on LLM leaderboards, beyond just affecting perplexity. Are these spikes also harmful to the accuracy of downstream tasks?\n* I believe it would be beneficial to conduct a theoretical analysis of the relationship between learning rate, loss spikes, and model sizes. This suggestion stems from the observation that the learning rates causing loss spikes differ according to model size."
            },
            "questions": {
                "value": "Please see the above weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors provide theoretical support to reduce the likelihood of divergence based on stabilization via small sub-layer initialization and large shortcut values. They then propose two techniques to effectively support this assumption and evaluate accordingly."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper conducts mathematical analysis to demonstrate the requisite terms they later leverage.\nPaper is clear and provides actionable results."
            },
            "weaknesses": {
                "value": "The authors only tested on smaller models, it is well established that most instability problems happen with larger models (>100B parameters). It would be beneficial to evaluate the loss curves on larger models or more diverse datasets \n\nAlthough the focus of this paper was to stabilize training, they underperform on loss-curves compared to vanilla approaches to disprove Le Scao et al's findings. This is hypothesized to be related to learning-rates - which is demonstrated by looking at a absolute min score. However,  by only exploring LR adjustments on smaller models, it isn't immediately clear that the proposed method is consistently better as sub-optimal lr's are more stable, a lr scheduler could account for differences in the long-term."
            },
            "questions": {
                "value": "Where approaches evaluated on benchmarks post-training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper analyzes gradient norms to explain and reduce the spikes of training loss. However, this paper does not address the differences between LLM and small models, differences between deep and shallow layer, and the relationship between spikes and gradient norm. Most importantly, the risk of training LLM is simplified by evaluating the performance on benchmark datasets while the risk of spending computing resources is not well addressed unfortunately requires lots of improvements."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper effectively addresses the critical issue of loss spikes during training, providing a detailed analysis of their relationship with gradient norms and embedding means. It evaluates existing approaches like \"Scaled Embed\" and \"Embed LN,\" discussing their effectiveness in mitigating spikes. Additionally, the paper offers valuable insights into the impact of learning rate adjustments on model stability and compares the behavior of spikes in large language models (LLMs) with smaller models, providing a broader context for its findings."
            },
            "weaknesses": {
                "value": "The paper suffers from an unclear relationship between spikes and poor performance, with insufficient explanations of key terms and assumptions. The evaluation section is not well-explained, and there are inconsistencies in terminology. Additionally, it lacks necessary plots and data to support its assumptions, and some figures are difficult to interpret due to overlapping lines. Reproducibility is a concern as common public architectures are not used, and some discussions are considered irrelevant to the main topic."
            },
            "questions": {
                "value": "The relationship between spikes and poor performance is not clearly established.\n\nSome loss spikes can be recovered during training, while others cannot.\n\nAre you assuming that spikes are (1) always bad, (2) risks that result in divergence in some cases, or (3) sometimes acceptable and not harmful?\n\nL40: What is catastrophic divergence? Does it mean the spike never goes down again?\n\nWhat model is used in Figure 1?\n\nWhat is the difference between spikes in LLMs and other smaller models?\n\nL47: The assumption between spikes and gradient norm is unclear. What did you observe in L121?\n\nL53: What is the standard deviation of embedding means? Does this equate to a large shortcut in the later sections?\n\nL63: Evaluation is not explained.\n\nL83: Conventionally, does shortcut mean residual?\n\nCan you comment on the parallel FF+Attn setting where there is no intermediate vector? It is broadly used in many models, e.g., Pythia, Mesh-Transformers, and PaLM.\n\nclear writing in section 2.1, 2.2\n\nL130: What is the difference between W_* and W?\n\nL134: You should mention the single-head attention and F as the identity function (linear settings) in the abstract and introduction.\n\nL134: Please plot the distribution of x, x', and W, and note the standard deviation and mean of each layer to support your assumption. In Appendix F, there are only plots for initialized models. I could not find plots for W or during training and pretrained models.\n\nL160: What are d and d_ffn? Did you mention them somewhere?\n\nL167: This is an estimation as shown in L159. Please be consistent.\n\nL174: It's unclear what this well-known formula is.\n\nEq 13: Why is variance the degree of freedom?\n\nCondition 2 of large shortcut comes from assumption 1.\n\nL219, Eq 19: Where does the 2 come from?\n\nL234: The writing in Section 4 could follow the format in Section 5.2 to improve readability.\n\nL250: Both \"Scaled Embed\" and \"Embed LN\" are existing approaches, not introduced by this paper.\n\nL291: Why should it be close to 1 and not as large as possible? If so, why is it called 'large shortcut' in the previous sections? Similarly, should the sublayer be as small as possible or close to some value? Why do we scale the embed by some value but not a larger one? What is the optimal scale from your theory?\n\nL309: C4 is too small to be the pretraining data for LLM.\n\nL317: Why not use a common public architecture (e.g., LLama, Mixtral, Gemma) for reproducibility? Is there any common pretraining baseline in the literature (e.g., Dettmers et al. (2022) and Le Scao et al. (2022))?\n\nFigure 3 (a): Lines are overlapped and cannot see what happened. Is there any other metric like the number of spikes other than visualization?\n\nL367: Only in the embed layer?\n\nThe training risk mentioned in the abstract is not addressed in the experiment. We can only see some marginal improvement over perplexity. In the abstract, it indicates that 'if we don't take spikes seriously, the whole effort will be in vain due to catastrophic divergence.' However, both vanilla and embed detach do not suffer from catastrophic divergence in the experiment results.\n\nSection 6.1: How does the learning rate affect the gradient norm? How does the learning rate affect the four baseline methods? Did you indicate that stabilization can be achieved by a smaller learning rate? If so, there is no actual need for \"Scaled Embed\" or \"Embed LN\"? In Figure 5, we can see that vanilla performs better than Scaled Embed. To achieve better final performance as depicted in Table 3, can we use a small learning rate to travel through the risky early training stage and increase the learning rate later to avoid spiking?\n\nSection 6.2: Good point! I expect a similar discussion in Section 6.1 to explain why a smaller learning rate can reduce spikes. Please remind me if I missed something. Also, this indicates the settings of 'short seq in the early stage and long seq later.' Can we say that 'small lr in the early stage and large one later' is also possible?\n\nL490: Can you use your theory to explain why preLN is more stable than post-LN?\n\nL509: The efficiency and learning rate discussion is a bit too far and not quite relevant."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}