{
    "id": "cP00UB2654",
    "title": "Quantifying Prediction Consistency Under Model Multiplicity in Tabular LLMs",
    "abstract": "Fine-tuning large language models (LLMs) on tabular data for classification can lead to the phenomenon of \\emph{fine-tuning multiplicity}, where equally well-performing models make conflicting predictions on the same input. Fine-tuning multiplicity can arise due to variations in the training process, e.g., seed, random weight initialization, retraining on a few additional or deleted data points. This raises critical concerns about the robustness and reliability of Tabular LLMs, particularly when deployed for high-stakes decision-making, such as finance, hiring, education, healthcare, etc. This work formalizes the unique challenge of fine-tuning multiplicity in Tabular LLMs and proposes a novel measure to quantify the robustness of individual predictions without expensive model retraining. Our measure quantifies a prediction's robustness by analyzing (sampling) the model's local behavior around the input in the embedding space. Interestingly, we show that sampling in the local neighborhood can be leveraged to provide probabilistic robustness guarantees against a broad class of equally-well-performing fine-tuned models.  By leveraging Bernstein's Inequality, we show that predictions with sufficiently high robustness (as defined by our measure) will remain consistent with high probability. We also provide empirical evaluation on real-world datasets to support our theoretical results. Our work highlights the importance of addressing fine-tuning instabilities to enable trustworthy deployment of Tabular LLMs in high-stakes and safety-critical applications.",
    "keywords": [
        "trustworthy large language models",
        "Reliable machine learning",
        "tabular data",
        "model multiplicity",
        "high-stakes application"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cP00UB2654",
    "pdf_link": "https://openreview.net/pdf?id=cP00UB2654",
    "comments": [
        {
            "summary": {
                "value": "The paper studies the fine-tuning multiplicity in tabular LLMs, where different settings, such as varying random seeds, lead to different models and correspondingly different predictions for a given sample or dataset. The authors propose a measure called consistency, which is based on the model\u2019s local behavior around the input. Asymptotic guarantees for consistency are provided, and an experimental comparison with other model multiplicity metrics (e.g., arbitrariness, discrepancy, pairwise disagreement), as well as predictive probabilities and dropout methods, is conducted."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is clearly written and easy to follow.\n\nThe authors provide an extensive literature review.\n\nThe authors consider the interesting problem of uncertainty quantification for tabular LLMs and propose using a relatively inexpensive measure to assess it.\n\nA comparison is made with various methods of computing model multiplicity."
            },
            "weaknesses": {
                "value": "1. Could you please provide more background and intuition behind the consistency measure? Essentially, it is not clear why the choice of the $f(x_i) - |f(x)-f(x_i)|$ metric was made and what it means locally for a given sample, as well as globally when averaged for a given model. It is also unclear why the discrepancy in the metric $f(x)$ vs. $2f(x_i)-f(x)$ occurs, given the sign on the absolute value.\n\n2. Consistency, as designed, is a measure of robustness or stability, and more comparison or motivation on how consistency differs from existing robustness methods should be provided (there exist different measures, just as an example please see (1) and (2)). Further, while there is a high rank correlation with model multiplicity metrics, more discussion is needed on why a local robustness metric is a good approximation of model multiplicity.\n\n3. Theorem 1 provides interesting asymptotic guarantees. However, as the authors mentioned, it is challenging to use in practice. Specifically, it is not clear whether the current consistency value is high or low for the theorem to apply. To provide more information on the usefulness of the theorem or $k$, is it possible to estimate $\\beta$ for some simpler hypothesis spaces, such as linear models?\n\n4. Tables 1 and 2 in the main paper, as well as other tables in the Appendix, do not contain confidence intervals (either computed or estimated). Therefore, it is difficult to assess the robustness of the presented values and compare them across datasets and metrics.\n\n5. Methods that enumerate all models in the Rashomon set for different hypothesis spaces, such as RashomonGB, TreeFarms, and CorelsEnum, can also be added to the literature review for completeness.\n\n(1) J. Djolonga, F. Hubis, M. Minderer, Z. Nado, J. Nixon, R. Romijnders, D. Tran, and M. Lucic. Robustness Metrics, 2020.\n\n(2) Tessa Han, Suraj Srinivas, Himabindu Lakkaraju. Characterizing Data Point Vulnerability as Average-Case Robustness, 2024."
            },
            "questions": {
                "value": "1. A lot of the model multiplicity metrics used in the paper are label-based. Does the choice of the threshold of 0.5 influence the rank correlation between consistency and these metrics?\n\n2. Do you have any intuition as to why the rank correlation is low between consistency and arbitrariness/pairwise disagreement for the German dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The work studies the problem of model multiplicity in the application of LLMs to tabular data. Model multiplicity refers to the phenomenon that multiple models of similar accuracy assign confliciting predictions to individual instances. The authors propose a measure of model multiplicity called *consistency*, provide a theoretical observation about it, and evaluate it in four standard tabular datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Model multiplicity is an interesting problem. It's certainly worth studying it in the context of tabular datasets, and with regard to models resulting from multiple finetuning runs on the same data."
            },
            "weaknesses": {
                "value": "(1) \n\nThe proposed measure of consistency has a few different issues that prevent it from being an informative measure in my opinion.\n\n(a) The biggest issue with it is that it is apparently unable to identify constant predictors adequately. Constant predictors, by definition, cannot exhibit model multiplicity. Unfortunately, the measure is sensitive to the value of the constant: A constant 0 predictor would have consistency 0, whereas a constant 1 predictor would have consistency 1. Note that 0 is the smallest possible value consistency can take on, while 1 is the largest possible value. In other words, two equivalent predictors can have maximal or minimal consistency.\n\nThis shows why we cannot read anything into a particular value of consistency. The authors write: \"We desire that the measure S(x, f) should be high if the input x is consistent across this broad class models.\" Unfortunately, even if S(x, f) = 0, it could be that the model output is perfectly \"consistent\" as the example of the constant 0 predictor shows.\n\n(b) The measure S(x, f) is a random variable that depends on the number of sample points and the radius of the sphere. It's not clear how sensitive the measure is to these design choices. This raises the quesiton how conflicting values of sensitivity should be reconciled.\n\n(c) There is no reason that the model family should respect the Euclidean geometry implicit in the definition of the consistency measure. This gives additional reason to believe that the measure cannot be useful across all model families.\n\nIn light of these issues, I prefer the definitions from prior work. For example, the measure of pairwise disagreement by Black et al. (2022) already gets around the above issues. \n\n(2)\n\nAs a result of the definitional problems with consistency, I find the suggested main \"theorem\" to be less than helpful. Let's even take aside the fact that it's a direct application of the assumptions and Bernstein's inequality. It's not clear what we can read into. Again, the constant 0 predictor gives the same output to any input, but the theorem would only say that Pr(F(x) > -\\epsilon) is large. This is, of course, a vacuous statement.\n\n(3)\n\nThe selection of datasets is rather disappointing. Although standard, these four datasets are very small, outdated, and problematic for different reasons. This is especially unfortunate, given the wide availability of many better tabular datasets. For example, the tableshift project has more than a dozen useful tabular datasets:\n\nhttps://github.com/mlfoundations/tableshift\n\nLikewise, the folktables package gives easy access to a plugin alternative and extension to the Adult dataset:\n\nhttps://github.com/socialfoundations/folktables\n\nThere is really no good reason for the empirical evaluation to be so limited."
            },
            "questions": {
                "value": "Could you please comment on the issues with the definition?\n\nWhat are the advantages of consistency compared with existing measures?\n\nDo you have results on other/larger datasets? If not, why not?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper aims to develop a consistency measure for evaluating prediction robustness under model multiplicity in fine-tuned Tabular Large Language Models (LLMs). The authors claim that this measure provides probabilistic guarantees that predictions will remain stable over a range of equally well-performing fine-tuned models, without needing extensive retraining. However, the main result is incorrect, and the result (except for experimental part) is irrelevant to \"tabular LLM\"."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper brings forward the important problem of efficiently measuring prediction robustness in tabular LLM finetuning."
            },
            "weaknesses": {
                "value": "- The introduced \u201cconsistency\u201d term lacks meaningful interpretation. Although it is intended to indicate prediction robustness across models, Theorem 1 does not support this interpretation. The result (even if correct) says if a data point has a sufficiently high consistency score with respect to a random model, the prediction of another random model from the same broad class of fine-tuned models will be at least the level of consistency (meaning that it is approaching class 1) with high probability. But what if the underlying class label should be 0?\n\n- The paper\u2019s theoretical formulation has significant issues. It lacks a clear definition of \u201crandom functions\u201d and assumes that \u201cfunctions are independent and identically distributed (i.i.d.)\u201d without providing a rigorous explanation. These assumptions are unconventional and do not align with standard definitions typically found in probability theory.\n\n-  The consistency measure itself is random, as it depends on sampled points, and its properties would only stabilize as the sample size k approaches infinity. This suggests that further study should focus on the properties of the deterministic limit of the measure rather than its finite-sample properties.\n\n- Assumption 1 is trivial. If F, F' are bounded functions, certainly the variance of Z is bounded.  \n \n- The paper claims \"while model multiplicity in machine learning has been studied in various contexts, the unique challenges of fine-tuning multiplicity in Tabular LLMs remain relatively unexplored.\"  While the paper claims to address fine-tuning multiplicity specifically for tabular LLMs, the main results are general and do not leverage any unique characteristics of tabular data or LLMs. The main result is irrelevant to the paper scope."
            },
            "questions": {
                "value": "Pls see my comments and suggestions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of multiplicity in fine-tuning LLMs (i.e., fine-tuning under different conditions), particularly when applied to tabular predictions. While these models may achieve comparable average performance, they can generate conflicting predictions for the same inputs. To address this, the paper introduces a consistency measure that evaluates model behavior in the embedding space, focusing on the local region around the input. Additionally, the paper provides theoretical guarantees for the metric and supports it with empirical validation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality: The paper introduces the concept of fine-tuning multiplicity in LLMs applied to tabular data. The problem is interesting and possibly unexplored.  A key originality is the proposed consistency measure which offers a novel way to assess prediction robustness without requiring the retraining of multiple models.\n\nQuality: The paper is well written and motivated with the proposed metric supported by both theoretical analysis and empirical evidence.\n\nClarity: The paper is clearly written, effectively contextualizing the problem within the scope of tabular data.\n\nSignificance: The issue addressed is highly relevant when using LLMs for tabular data, especially in terms of evaluating prediction robustness. The proposed metric tackles a critical challenge in ensuring the reliability of LLM-based classification systems for tabular data.\n\nThe significant ablations to understand the different dimensions is a particular strength"
            },
            "weaknesses": {
                "value": "- The datasets assessed are more conventional tabular datasets (<50 features). It would be useful to understand if the findings translate to higher dimensional tabular datasets often found in genomics and physics. Examples suggested are like the Higgs (physics) and TCGA (genomics) datasets. This is especially relevant of how this dimensionality interplays with the embedding space.\n\n- The authors are encouraged to contrast a bit more explicitly to other related approaches in which multiplicity and consistency can be assessed --- e.g. ensemble based methods"
            },
            "questions": {
                "value": "- How would results translate on high dimensionality data? -Examples suggested are like the Higgs (physics) and TCGA (genomics) datasets.\n\n- The authors mention an easy extension of the theory to multi-class problems. Do the empirical results hold beyond binary on multi-class tabular datasets or are there any nuances? It would be interesting to understand in terms of the methods generality."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}