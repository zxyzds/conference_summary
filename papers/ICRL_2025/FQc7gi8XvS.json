{
    "id": "FQc7gi8XvS",
    "title": "On the Convergence of FedProx with Extrapolation and Inexact Prox",
    "abstract": "Enhancing the FedProx federated learning algorithm (Li et al., 2020) with server-side extrapolation, Li et al. (2024a) recently introduced the FedExProx method. Their theoretical analysis, however, relies on the assumption that each client computes a certain proximal operator exactly, which is impractical since this is virtually never possible to do in real settings. In this paper, we investigate the behavior of FedExProx without this exactness assumption in the smooth and globally strongly convex setting. We establish a general convergence result, showing that inexactness leads to convergence to a neighborhood of the solution. Additionally, we demonstrate that, with careful control, the adverse effects of this inexactness can be mitigated. By linking inexactness to biased compression (Beznosikov et al., 2023), we refine our analysis, highlighting robustness of extrapolation to inexact proximal updates. We also examine the local iteration complexity required by each client to achieved the required level of inexactness using various local optimizers. Our theoretical insights are validated through comprehensive numerical experiments.",
    "keywords": [
        "Federated Learning",
        "Optimization"
    ],
    "primary_area": "optimization",
    "TLDR": "We analyzed FedProx with extrapolation and provide convergence guarantees when proximal updates are inexact.",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=FQc7gi8XvS",
    "pdf_link": "https://openreview.net/pdf?id=FQc7gi8XvS",
    "comments": [
        {
            "comment": {
                "value": "Thank you very much for your response. You have answered all of my questions and concerns.    \nI will keep my current score."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer yKw5"
            },
            "comment": {
                "value": "> We thank you for taking time to review our paper.\n\n---\n#### Weakness 1:\n\n>The reviewer may have misinterpreted our assumptions. We assume each function $f_i$ is convex (could have multiple minimizers). and operate under the interpolation regime, where the intersection of the set of minimizers of $f_i$ is nonempty. Additionally, we assume the global objective $f$ is strongly convex (Assumption 5), which suggests that the intersection is a singleton.\n\n> The strong convexity assumption simplifies our presentation. Without it, the same reformulation applies to biased SGD in the general convex case, which we can analyze similarly using biased SGD theories.\n\n> Without the interpolation, the algorithm converges to a neighborhood of the true solution. As discussed in Appendix E, the interpolation assumption aligns with FedExProx when viewed through the lens of parallel projection methods.\n\n---\n### Weakness 2:\n\n> The referenced work is relevant but was posted after ours, so it wasn\u2019t initially included. Their work provides improved guarantees on PL objectives compared to the original FedExProx algorithm introduced in [2]. Our objective is somewhat 'orthogonal' to theirs, as we focus on removing the impractical assumption of exact proximal operator evaluations. We agree that this reference adds clarity, and it's included in the latest version of our paper.\n\n\n> [2] The Power of Extrapolation in Federated Learning, H. Li, K. Acharya, P. Richt\u00e1rik \n\n---\n#### Weakness 3:\n\n> Yes, indeed. For exact proximal operators, FedExProx yields an unbiased SGD, where adaptive step sizes are well-understood. However, for inexact FedExProx, literature on adaptive step sizes for biased SGD is lacking, and we are investigating this gap. Preliminary results (Figures 6 and 7) show that gradient diversity accelerates the algorithm, while the stochastic Polyak step size is less effective, highlighting the need for tailored adaptive step size strategies for biased SGD.\n\n> For the case of client samping, the algorithm performs suboptimally due to the added stochasticity\u2014an expected outcome, as client sub-sampling does not inherently benefit biased compression, as noted in [1]. To address this, one could apply the well-known Error Feedback-21 strategy [1], [2] for biased compression; however, implementing this requires modifying the original FedExProx algorithm, which falls outside the scope of our current focus.\n\n> [1] EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback. P. Richt\u00e1rik, I. Sokolov, I. Fatkhullin\n> [2] EF21-P and Friends: Improved Theoretical Communication Complexity for Distributed Optimization with Bidirectional Compression. K. Gruntkowska, A. Tyurin, P. Richtarik.\n\n---\n#### Weakness 4  & Question 2: \n\n> (1): We have added in the lastest version of the paper that the local convergence rates are derived based on existing theories.\n\n> (2): We have changed accordingly in the lastest version of the paper that the local convergence rates are derived based on existing theories.\n\n> (3): Unfortunately, it is not possible to achieve convergence to the exact solution even if $\\varepsilon_1$ is sufficiently small; there will always be a neighborhood in this case, determined by the value of $\\varepsilon_1$. With relative approximation, however, this neighborhood vanishes as the bias term in (12) diminishes near the optimum. Thus, these two approximation approaches are not directly comparable.\n> Smaller values of $\\varepsilon_1$ or $\\varepsilon_2$ generally require more local client steps. With absolute approximation, $\\varepsilon_1$ determines the neighborhood size but does not affect total communication rounds directly (Theorem 1). In contrast, a smaller $\\varepsilon_2$ in relative approximation increases local computation but reduces total communication rounds, akin to the difference between standard SGD and variance-reduced SGD.\n\n> (4): Thank you for the suggestion.  We have now added a note in Table 1 to highlight this.\n\n> (5) & Question $2$: Thanks the suggestion. We include Theorem 2 to illustrate that directly applying results from the biased SGD perspective yields a suboptimal convergence bound and a much restrictive condition on accuracy of the approximation. In contrast, Theorem 3, with a reformulated approach, offers a tighter bound and a relaxed condition, supporting the effectiveness of extrapolation in the inexact case. We include Theorem 2 only to highlight the improvement achieved with Theorem 3.\n\n> (6) Thank you for pointing this out; this is indeed a relevant paper. We have now added this reference to our paper to enhance readability.\n\n> Typos: We have corrected this typo.\n\n---\n#### Question 1: \n\n> There may be some confusion here. As shown in Fig. 1(a), FedExProx outperforms exact FedProx (without extrapolation) even with inexact proximal updates, though its convergence rate is slower than exact FedExProx, consistent with our theoretical predictions."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer QD48"
            },
            "comment": {
                "value": ">  We thank you for taking time to review our paper.\n\n---\n#### Weakness $1$: \n> We agree with the reviewer that detailing the behavior of the algorithm in the non-interpolated setting is essential. To address this, we have added the following discussion to the appendix of our paper in Appendix E:\n\n> In the absence of the interpolation regime assumption, the algorithm converges to a neighborhood of the true minimizer $x_\\star$ of $f$. This occurs because $f$ and $M^{\\gamma}$ have the same minimizer only under the interpolation regime assumption, as established by Fact 7 and [2]. Since inexact FedExProx can be formulated as biased SGD on the objective $M^{\\gamma}$, it converges to the minimizer $x_{\\star}^{\\prime}$, provided that inexactness is properly bounded. As a result, the algorithm converges to $x_\\star^{\\prime}$, located within a $\\|\\|x_\\star - x_\\star^{\\prime}\\|\\|$-neighborhood of $x_\\star$ whose size depends on $\\gamma$. Notably, the effects of inexactness and interpolation are, in some sense, 'orthogonal', meaning they do not interfere with each other.\n \n> In addition, FedProx does not require the interpolation regime assumption. However, like FedExProx and its inexact variant, it converges to a neighborhood of the solution. The interpolation assumption was initially introduced based on the motivation behind FedExProx. It is known that the parallel projection method for solving convex feasibility problems is accelerated by extrapolation. Given the similarity between projection operators and proximal operators (which are, in fact, projections onto certain level set of the function), FedExProx was proposed. The interpolation assumption here corresponds to the assumption that the intersection of these convex sets is non-empty in the convex feasibility problem. Although seemingly arbitrary for FedProx, the interpolation assumption aligns naturally with FedExProx when viewed through the lens of parallel projection methods.\n\n---\n#### Weakness $2$: \n> Theorem 4 and 5 provide both local computational complexities to achiveve absolute approximation (definition 3) and relative approximation (definition 4) using local gradient descent and accelerated gradient descent respectively. It\u2019s possible the reviewer may have missed the second part regarding relative approximation, as both are currently presented in one line due to space constraints. If the paper is accepted and additional space is available, we will separate these results to prevent any misunderstanding.\n\n---\n#### Weakness $3$: \n> Thank you for your feedback. We have removed some redundancies and streamlined the statement of Theorem 1 for greater clarity.\n\n---\n#### Weakness $4$:\n> Thanks for pointing this out. We have corrected the typo.\n\n---\n#### Question $1$:\n> Good question. Unfortunately, we believe it's not possible to achieve an exact solution under conditions of arbitrary inexactness.  For the reformulation of inexact FedExProx, the gradient estimator comprises a gradient term and a bias term. Unlike absolute approximation, relative approximation bounds the bias so it decreases near the solution. Allowing arbitrary inexactness means the bias could vary widely, resulting in convergence only within a neighborhood, similar to absolute approximation.\n> \n> From the perspective of biased SGD, this would be equivalent to assuming that we could use a random vector as a gradient estimator and still converge to the exact minimizer, which is unlikely to hold true.\n\n---\n#### Question $2$: \n> For relative approximation in Theorem 3, $\\varepsilon_2$ must be sufficiently small, specifically $\\varepsilon_2 < \\frac{\\mu}{4L_{\\max}}$. If $\\mu$ and $L_{\\max}$ are known, we select $\\varepsilon_2$ accordingly; otherwise, we estimate these values or choose a very small $\\varepsilon_2$. A smaller $\\varepsilon_2$ increases local computations per communication round but also accelerates progress per round, similar to local training methods.\n\n> In practice, the server can broadcast an accuracy level $\\varepsilon_2$ to each client, directing them to perform local SGD, AGD, or other methods during each communication round. The required local iterations will depend on $\\varepsilon_2$ and the local objective's characteristics.\n\n---\n#### Question $3$: \n> In general, we believe that extrapolation would be beneficial across a broader range of conditions. As explained in our response to Weakness 1, if we do not assume the interpolation regime, the FedExProx algorithm still converges to a neighborhood around $x_\\star$ with radius $\\|\\|x_\\star - x_\\star^{\\prime}\\|\\|$, where $x_\\star^{\\prime}$ is the minimizer of $M^{\\gamma}$. This occurs because, without the interpolation assumption, the minimizers of $f$ and $M^{\\gamma}$ do not necessarily coincide. In this setting, the size of the neighborhood depends on both $\\gamma$ and $\\alpha$, imposing additional constraints if we aim to reach a specific level of accuracy."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer htCV"
            },
            "comment": {
                "value": "> We thank you for taking time to review our paper.\n\n---\n#### Weakness $1$ & Question $1$: \"While the question is natural and important, the solution is quite straightforward, and does not introduce any novel tools or analysis. Excluding the approximation 2 which is nice.\" & \"What is the main challenge and main novelty in your paper?\"\n\n>We respectfully disagree with the reviewer\u2019s assessment that the straightforward analysis in our paper is a limitation. While our analysis does not introduce novel tools, this choice is intentional. \n>Our approach reformulates the inexact FedExProx algorithm in terms of established algorithms, such as SGD (stochastic gradient descent) with a biased gradient estimator and CGD (compressed gradient descent) with biased compression. This reformulation enables us to leverage existing analytical tools effectively. Without those connections, the relatively straightforward analysis presented would not be feasible. In summary, to address this natural and important problem, we opted for a reformulation that allows us to apply existing methodologies, rather than developing entirely new tools.\n\n> The primary challenge in our paper lies in formulating the problem appropriately and establishing optimal complexity bounds for the algorithm. For instance, to reach the conclusion that 'extrapolation aids the convergence of FedExProx, even with inexact proximal operators, provided that inexactness is bounded in a certain manner,' we first needed to recognize an intrinsic connection between FedExProx and biased compression in CGD. This insight allows us to apply existing theoretical frameworks to demonstrate the algorithm\u2019s effectiveness. Without identifying this relationship, reaching such a conclusion would not have been feasible. In addition, it was essential to identify an appropriate way to bound the inexactness, allowing us to eliminate the neighborhood effect. This step was crucial in ensuring that our analysis remains rigorous and aligns with optimal complexity bounds.\n\n> The main novelty of our paper lies in providing an analysis of FedExProx when proximal operators are evaluated inexactly, a scenario that has not been previously studied. Our analysis leads to the new insight that extrapolation remains effective even with inexact proximal operators\u2014a conclusion not previously established. Additionally, as the reviewer noted, we introduce a relative approximation approach that eliminates the neighborhood effect, thereby making the algorithm more practical and applicable in real-world settings.\n\n---\n#### Weakness $2$ & Question $2$: \"The paper does not consider stochastic gradients which is the more relevant case in practice, it is important to understand how will the results change in light of this?\" & \"Why do you not take stochastic gradients into account? How will the results change assuming this?\"\n\n> We do consider stochastic gradients in our analysis. Specifically, in Appendix F, we provide a convergence guarantee for inexact FedExProx with $\\tau$-nice sampling of clients (which results in stochastic gradients) and $\\varepsilon_2$ relative approximation, as detailed in Theorem 8 based on biased SGD theory. In the specific case of client sub-sampling, the algorithm performs suboptimally due to the added stochasticity\u2014an expected outcome, as client sub-sampling does not inherently benefit biased compression, as noted in [1]. To address this, one could apply the well-known Error Feedback-21 strategy [1], [2] for biased compression; however, implementing this would require modifications to the original FedExProx algorithm, which falls outside the scope of our current discussion. For inexact FedExProx with $\\varepsilon_1$ direct approximation, a convergence guarantee can be similarly derived, as outlined in Appendix F.2.\n\n> [1] EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback. P. Richt\u00e1rik, I. Sokolov, I. Fatkhullin\n> [2] EF21-P and Friends: Improved Theoretical Communication Complexity for Distributed Optimization with Bidirectional Compression. K. Gruntkowska, A. Tyurin, P. Richtarik."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer VGz2"
            },
            "comment": {
                "value": "> We thank you for taking time to review our paper.\n\n---\n#### Weakness $1$: \"The theoretical analysis is restricted to globally strongly convex problems, ... Extending this analysis to non-convex cases would significantly increase the paper\u2019s impact.\"\n\n> We agree with the reviewer that extending the analysis to non-convex cases could significantly enhance the paper's impact. However, it remains unclear whether extrapolation would indeed accelerate the convergence of the algorithm in this case, even when proximal operators are assumed to be solved exactly. This uncertainty stems from the foundational motivation behind the FedExProx algorithm, which is based on the parallel projection method for solving convex feasibility problems. Initially, it was observed that extrapolation can accelerate the parallel projection method (in this convex interpolated setting). Given the similarity between projection operators and proximal operators (the latter can be viewed as a projection to a level set of the function), the FedExProx algorithm was developed. In this context, extrapolation is considered in conjunction with convexity; whether it remains beneficial in non-convex settings is still unclear. This rationale led us to focus on the convex case first.\n\n> We have added the above discussion in Appendix E to the latest version of the paper as a clarification.\n\n\n---\n#### Weakness $2$: \"The assumption of smoothness might not always hold in federated learning, particularly when clients have heterogeneous data distributions. A discussion on how the proposed approach might generalize or be adapted for non-smooth settings would strengthen the paper.\"\n\n> The smoothness assumption is pretty common in convex optimization, and we adopt it here for simplicity of discussion and presentation. In fact, even if we do not assume each local objective function $f_i$ to be $L_i$-smooth, the corresponding Moreau envelope $M^{\\gamma}_{f_i}$ is still $\\frac{1}{\\gamma}$ -smooth as illustrated in [1]. Consequently, the inexact FedExProx still yields a form of SGD with a biased gradient estimator on the convex smooth objective $M^{\\gamma}$. This allows us to leverage the relevant theoretical framework to analyze the convergence result in this scenario. Although some technical nuances arise, they do not impact the validity of our conclusion.\n\n> We have added the above discussion in Appendix E to the latest version of the paper as a clarification.\n\n> [1] The Power of Extrapolation in Federated Learning, H. Li, K. Acharya, P. Richt\u00e1rik\n\n---\n\n#### Weakness $3$: \"The experimental part is the weakest part of this work.\"\n\n> Thank you for your feedback. In this work, our primary focus has been on the theoretical aspects. Could you please indicate which specific parts of the experimental section you feel could be strengthened, or suggest any additional experiments you would find valuable?"
            }
        },
        {
            "title": {
                "value": "Response to all reviewers"
            },
            "comment": {
                "value": "We sincerely thank all the reviewers for taking the time to review our paper.\n\nThe reviewers highlighted key strengths, noting that the paper addresses a significant theoretical gap, enhancing its real-world applicability. They found the problem natural and important, the approximation methods well-designed, the writing clear and accessible, and the connection to biased compression interesting.\n\nThe reviewers also raised some concerns, for which we have prepared a comprehensive, case-by-case response to each reviewer. We hope this detailed clarification addresses their questions and provides additional insight into our work. We have incorporated these changes into the latest version of the paper."
            }
        },
        {
            "summary": {
                "value": "This paper investigates the convergence behavior of FedExProx, a recent extension of the FedProx federated learning algorithm, which includes server-side extrapolation to improve performance in federated settings. A key issue with existing analyses of FedExProx is the assumption that each client can compute the proximal operator exactly, which is unrealistic in practical applications. This paper relaxes this assumption, examining the algorithm\u2019s behavior in cases where the proximal operator is only computed approximately. The authors establish convergence results in smooth, globally strongly convex settings, demonstrating that the algorithm still converges, albeit to a neighborhood around the solution. They also show that careful control can reduce the negative impact of inexact proximal updates and draw connections to biased compression methods. Additionally, they provide an analysis of the local iteration complexity needed for clients to achieve a specific level of inexactness, with empirical validation of their findings through numerical experiments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper addresses a significant gap in existing work on FedExProx by relaxing the exact proximal computation assumption. This makes the analysis more applicable to real-world federated learning systems, where inexact computations are the norm due to resource constraints."
            },
            "weaknesses": {
                "value": "- The theoretical analysis is restricted to globally strongly convex problems, which may limit its applicability to a broader range of federated learning applications that involve non-convex objectives. Extending this analysis to non-convex cases would significantly increase the paper\u2019s impact.\n\n- The assumption of smoothness might not always hold in federated learning, particularly when clients have heterogeneous data distributions. A discussion on how the proposed approach might generalize or be adapted for non-smooth settings would strengthen the paper.\n\n- The experimental part is the weakest part of this work..."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work sets to explore a recent algorithm in FL called FEDPROX. This algorithm leans on exact computation of what is called proximal operator.\nThe paper asks the following natural question: what if we do not fully solve the operator, but rather solve approximately?\nIn the smooth+strongly-convex case, this paper explore this questions assuming two kinds of approximations $\\epsilon_1$ and $\\epsilon_2$."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The question is indeed natural and relevant to concurrent FL problems\n\n- The authors cleverly define two kind of approximations and show that one is better then the other, allowing us to converge to the true optimum\n\n- The writing is very clear and easy to follow\n\n- Experiments are illustrative, and in a sense validate the theory"
            },
            "weaknesses": {
                "value": "- While the question is natural and important, the solution is quite straightforward, and does not introduce any novel tools or analysis. Excluding the  $\\epsilon_2$ approximation which is nice.\n\n- The paper does not consider stochastic gradients which is the more relevant case in practice, it is important to understand how will the results change in light of this?"
            },
            "questions": {
                "value": "- What is the main challenge and main novelty in your paper?\n\n- Why do you not take stochastic gradients into account? How will the results change assuming this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper focuses on a federated learning algorithm, called FedExProx, that requires each client to compute exactly a proximal operator. The authors analyze the FedExProx method when the proximal operators of each client is not computed exactly. Theoretical guarantees are provided in the strongly convex and smooth setting and the convergence rate of the algorithm is established. Moreover, the authors highlight a connection with the bias compression methods, that allows them to obtain more refined convergence guarantees. The iteration complexity of the local updates for gradient descent and accelerated gradient descent are also provided. Experimental results validate the theoretical results and showcase the effect of the different notions of inexactness in the computation of the proximal operator in the convergence of the method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The convergence is established under different notions of inexactness of the proximal operator.\n- The paper recovers as a special case the results of the original paper on FedExProx, when the proximal operators are evaluated exactly.\n- The connection with the biased compression is interesting."
            },
            "weaknesses": {
                "value": "- Theorems 1, 2, 3 require the notion of interpolation. Even though an explanation of regimes that satisfy this condition is provided, considering that there are previous works [1], [2] that extend beyond that setting, this assumption seems to be an avenue for future work in this field. More specifically, the initial FedProx algorithm [1] is analyzed in the general non-interpolated setting. In addition, the follow-up work regarding the FedExProx algorithm [2] considers in the main paper the interpolated regime. However, the authors provide additionally an illustration of the algorithm's behaviour in the non-interpolated setting (see Appendix F.3 in [2]). In that sense, it would be useful to provide some additional details on the behaviour of the algorithm in the non-interpolated setting or to comment on the main challenges in extending the current proof technique beyond the interpolation framework, offering in that way a more complete picture and direction for future research. \n- Theorems 4, 5 seem to evaluate the inexactness achieved in each client. However, the inexactness is only with respect to the notion of the absolute approximation, for which we know that Theorem 1 is not optimal (since for the same amount of inexactness Theorem 3 gives convergence to the exact solution). Thus, it seems that a characterization of the inexactness in terms of the relative approximation would be also useful. Hence, providing similar theorems for the relative approximation case seems to be a nice addition to the current results. \n- Minor: The statement of Theorem 1 can be made shorter in order to increase the readability of the paper.\n- Minor typo: In Figure 1, it is mentioned \u201cFigure (c) demonstrates how varying values of $\\epsilon_1$ affect FedExProx with relative approximation.\u201d but as shown the varying values correspond to $\\epsilon_2$.\n\nReferences:  \n[1] Federated Optimization in Heterogeneous Networks, T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, V. Smith    \n[2] The Power of Extrapolation in Federated Learning, H. Li, K. Acharya, P. Richt\u00e1rik"
            },
            "questions": {
                "value": "- Theorem 1 seems to provide convergence guarantees under the natural assumption of absolute approximation. However, the guarantee provided, as mentioned, includes a neighbourhood of convergence which is not optimal. On the other hand, the connection with biased compression provides a refined theorem (Theorem 3), establishing convergence to the exact solution. The amount of inexactness, though, in Theorem 3 is bounded. Do you think that one can achieve the best of both worlds, namely convergence to the exact solution but for arbitrary inexactness.\n- How one can compute the relative inexactness $\\epsilon_2$ in practice? Are there inherent computational tradeoffs or challenges in the computation of the relative inexactness $\\epsilon_2$ in comparison to estimating the constant $\\epsilon_1$? It would be nice also if you could comment on ways to approximate $\\epsilon_2$ in practical federated learning problems.  \n- Is it possible to raise the assumption on interpolation in the strongly convex setting by using a more refined proof technique or do you think that extrapolation might be beneficial only on that regime?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There are no ethics concerns."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper considers a finite-sum $\\mu$-strongly convex problems for which the interpolation conditions holds, and where each client objective is convex and $L$-smooth.\nThe work then considers the FedExProx method, which combines proximal client updates with an extrapolated server step, and extends this work to handle inexactness of the client prox computations. Specifically:\n\n- with fixed absolute inexactness they show that the method converges to a neighbourhood of the solution (using a factor $1/4$ smaller extrapolation step).\n- with a type of relative inexactness (smaller than order $\\mu^2/L^2$) they show exact convergence but for a restrictive extrapolation server stepsize $\\alpha$.\n- for relative inexactness with a more stringent condition (smaller than order $\\mu/L$) they show that the same (large) extrapolation stepsize can be used as in the exact case.\n- They provide convergence rate for the local strongly convex and smooth objective with gradient descent and Nesterov acceleration."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The writing is very clear and transparent. They state how results are obtained (relative inexactness by using analysis from biased SGD and from compression) and discuss limitations.\n- Considering relative inexactness for federated learning seems interesting"
            },
            "weaknesses": {
                "value": "- the work requires very strong assumptions: the solution needs to be unique (strong convexity) and shared amongst all clients (interpolation condition)\n- There is a large body of work on relative inexactness for proximal methods starting with [1], where it is used to essentially inherent the nice properties of an exact proximal computation. Considering the strong assumptions (strong convexity and interpolation condition) it does not seem very surprising that one can extend to a multi-client setting. It would be good to cite this work and put it into context. \n- The work does not treat adaptive stepsizes and partial participation as in (exact) FedExProx (they do discuss the difficulty of client sampling in the appendix).\n\nMinor:\n\n- The local convergence rates are not new. It would be good to explicitly state this.\n- After Theorem 2 when discussing the slowdown due to small $\\alpha$, it would be informative to plug in $\\varepsilon_2=c\\mu^2/L_{max}^2$ for some $c<1$ and simplify the expression.\n- It it possible to to get convergence not only to a neighborhood even for absolute inexactness. It might be worth choosing the $\\varepsilon_1$ sufficiently small, to make the comparison with relative inexactness more direct (how does the choice effect the client steps and the communication rounds?).\n- For absolute inexactness the server stepsize $\\alpha$ is a factor \u00bc  smaller. Maybe stress that this affects the rate explicitly in Table 1.\n- It is maybe worth stating how many iterations (e.g. with Nesterov) are needed to make $\\varepsilon_2 =\\mu/L$ vs $\\varepsilon_2 =\\mu^2/L^2$ to make the comparison/tradeoff more explicit between the two relative inexactness results.\n- It seems like some concurrent work is treating absolute inexactness which might be worth mentioning [2]\n\nTypos:\n\n- Eq. 4 both f and $\\phi$ are present\n\n[1] https://arxiv.org/pdf/2410.15368v1\n\n[2] https://www.emis.de/journals/JCA/vol.6_no.1/j149.pdf"
            },
            "questions": {
                "value": "- Fig. 1(a) indicates that inexactness can help whereas the theory predict otherwise. For inexact proximal gradient inexactness have shown to help for certain regimes (see e.g. page 5 of [3]). Is it possible that something analogue can be said in your setting?\n- It is not very clear how much having a more stringent requirement on the relative inexactness ($\\mu/L_{max}$ as compared with $\\mu^2/L_{max}^2$) buys in terms of the global rate. Is it possible to explicitly compare $S(\\varepsilon_2)$ with $(1-4\\varepsilon_2 L_{max})$?\n\n[3] https://proceedings.neurips.cc/paper_files/paper/2011/file/8f7d807e1f53eff5f9efbe5cb81090fb-Paper.pdf"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}