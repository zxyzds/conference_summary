{
    "id": "UAA2nWUtVl",
    "title": "Cascade Reward Sampling for Efficient Decoding-Time Alignment",
    "abstract": "Aligning large language models (LLMs) with human preferences is critical for their deployment. Recently, decoding-time alignment has emerged as an effective plug-and-play technique that requires no fine-tuning of model parameters. However, generating text that achieves both high reward and high likelihood remains a significant challenge. Existing methods often fail to generate high-reward text or incur substantial computational costs. In this paper, we propose Cascade Reward Sampling (CARDS) to address both issues, guaranteeing the generation of high-reward and high-likelihood text with significantly low costs. Based on our analysis of reward models (RMs) on incomplete text and our observation that high-reward prefixes induce high-reward complete text, we use rejection sampling to iteratively generate small semantic segments to form such prefixes. The segment length is dynamically determined by the predictive uncertainty of LLMs. This strategy guarantees desirable prefixes for subsequent generations and significantly reduces wasteful token re-generations and the number of reward model scoring. Our experiments demonstrate substantial gains in both generation efficiency and alignment ratings compared to the baselines, achieving five times faster text generation and 99% win-ties in GPT-4/Claude-3 helpfulness evaluation.",
    "keywords": [
        "Language Model Alignment",
        "Large Language Models"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We significantly reduce the inference cost for decoding-time alignment methods while persevering high alignment ratings.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=UAA2nWUtVl",
    "pdf_link": "https://openreview.net/pdf?id=UAA2nWUtVl",
    "comments": [
        {
            "summary": {
                "value": "- The paper focuses on enhancing decoding-time alignment methods, particularly addressing the high computational costs associated with existing approaches.\n- It introduces **Cascade Reward Sampling (CARDS)**, a novel method that samples segments, leveraging the insight that reward models (RMs) favor high-reward prefixes. CARDS incrementally generates high-reward semantic segments, aiming to achieve optimal reward in the final output by prioritizing these segments.\n- Experimental results demonstrate significant improvements in both performance and speed over baseline methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The shift to segment-level sampling, along with the use of uncertainty as a termination signal for segments, presents a unique and novel approach.\n- The results show significant gains on performance and speed, renders this approach very practical."
            },
            "weaknesses": {
                "value": "- The reliance on target score poses a notable limitation. How can one determine this? Different RMs provide values in different scales. \n    - Exploring alternative sampling strategies, like sampling multiple segments per step and selecting the highest-scoring option before proceeding greedily, could be beneficial (atleast as a baseline comparison)\n- The experiments are conducted solely on the HH-RLHF dataset, limiting the generalizability of the findings. Especially, HH-RLHF is a very simple dataset as many inference time methods are already better than trained methods such as PPO and DPO. Testing on few more datasets would solidify the findings."
            },
            "questions": {
                "value": "Suggestions:\n- One could include more baselines such as In-Context learning for alignment (Rethinking alignment via in-context learning), Best of N based on full sequence rewards"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a novel segment-based sampling method for efficient decoding-time alignment, leveraging rejection sampling to iteratively generate small semantic segments of high reward."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThis paper conducts a rigorous analysis of reward models and demonstrates the RMs can serve as value functions on semantically complete segments.\n2.\tThe generation method is segment-based and the length of segment is dynamic.\n3.\tThe experiment is adequate and reasonable and the paper is well written."
            },
            "weaknesses": {
                "value": "1.\tThe experiments are conducted on 7B models. The method could be verified on more larger models.\n2.\tThe parallelization scheme of dynamic segmentation whether has a slow inference time when the batch size is larger."
            },
            "questions": {
                "value": "The same as above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a decoding-time alignment method for large language models (LLMs) that ensures high-reward and high-likelihood text generation with reduced computational costs. CARDS uses rejection sampling to create text by dynamically determining segment lengths based on the predictive uncertainty of LLMs, significantly enhancing efficiency. This method also maintains fluency and aligns closely with human preferences, demonstrating substantial improvements over existing methods in both speed and alignment accuracy during text generation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The article raises an excellent question on how to enhance the efficiency of alignment during the reasoning phase.\n2. Many designs in the article's methodology are interesting, such as \"Our method leverages the comprehension ability of pre-trained LLMs for segmentation.\"\n3. The experimental results of CARDS are outstanding."
            },
            "weaknesses": {
                "value": "1. The presentation of the article is somewhat difficult to follow. For example, Figure 1, which explicates the contributions mentioned in the introduction, requires the integration of content from many sections later in the text to be understood. Moreover, Section 4.1.1 repeatedly refers to Figure 2c without clearly explaining how Figure 2c is produced and its detailed meaning. Section 4.2.1, however, does not mention Figure 2c at all.\n2. Many intermediate conclusions in the methodology lack theoretical support and are merely based on simple experiments and conjectures. For instance, \"a full-response reward will be high given a high-reward prefix\" and \"This is because initiating a new segment is more unpredictable than continuing an existing one.\"\n3. I think previous works [1] also assume Lemma 1 is valid; however, this paper still does not provide convincing proof, thus, the contribution in this part seems incremental.\n4. CARDS appears to be a method for enhancing the efficiency of decoding-time alignment, but I have no idea why CARDS could lead to significant performance improvements in the experiment.\n\n[1] Args: Alignment as reward-guided search."
            },
            "questions": {
                "value": "1. What do you mean by \"However, while some of the existing decoding-time alignment methods still struggle with the trade-off between alignment and fluency, they all encounter significant efficiency challenges due to auxiliary steps added to their decoding process.\"?\n2. In line 239, \"Therefore, the above observation also suggests that RMs can be used as value functions on semantically complete preffxes.\" Can you explain why?\n3. In Lemma 1, I still cannot understand why can we assume \"Assuming the reward models are equivalent to value functions when evaluating semantically complete prefixes\". Is there any previous literature proof of this?\n4. What if the prefixes is not semantically complete?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on reward-guided decoding-time alignment. They propose Cascade Reward Sampling (CARDS), which is mainly based on two claims:\n\n- They claim to have analyzed the properties of reward models (RMs) on incomplete text, and hypothesize that RMs can serve as approximations for value functions.\n- They show that values of predictive uncertainty can be a segmentation signal, and thus can divide the generation of an entire response into multiple steps (segments). \n\nCombining two claims, they establish the target distribution for sampling a new semantic segment. Therefore, they finally sample one segment each step, get the reward, and use rejection sampling to align the distribution to what they have established.\n\nExperiments are conducted on HH-RLHF dataset. They have compared the performance of CARDS with RAD/ARGS and naive rejection sampling, showing general advantages."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "## Originiality\n- The segmentation trick with uncertainty is novel and interesting, and it deserves to be shared with a broad audience.\n- The assumption on reward distribution is original, though may not be true.\n\n## Clarity\n- This paper is well-written and well-organized.\n- Most figures and tables are friendly to the readers.\n- The intuitions are clearly expressed.\n\n## Significance\n- The weak assumption on the expressibility of reward model is more reasonable than prior works.\n- This work has improved the performance of reward-guided decoding-time alignment, compared with baselines."
            },
            "weaknesses": {
                "value": "## Major\n- The reviewer cannot buy the idea that reward model is a value function. Only the last signal of reward model is trained, and the intermediate prediction is more like black box. There is a well-known paper [5] showing that DPO model acts like Q-function, but the result only holds for credit assignment setting.\n- There is a new paper showing that \"partial reward\" may not be correlated to \"full reward\". Please see Appendix C.3 of [6].\n- There is no non-trivial mathematical proof. So it would be better to remove the claim that \"We first **rigorously** analyze the properties of reward models\". The authors have conducted some experimets to support their intuitions or assumptions, that are great, but still far from being called \"rigorous\", since there is no theoretical guarantee.\n- Lack of baselines. CARDS is only compared with two baselines, RAD/ARDS (these two are the same), and naive rejection sampling. Many reward-guided decoding-time alignment approaches have emerged in these two years. For example, controlled-decoding [1] can be a baseline, which samples next token from $\\pi(y)\\exp(r(y)/\\beta)$. ([1] is different from RAD/ARDS, since it doesn't need a top-k sampling at first.) And the reviewer guesses that [1] should be faster than CARDS.\n## Minor\n- Equation 11(b) only holds for soft-RL, which should be made clear to the readers.\n- Lack of datasets. The experimental results on HH-RLHF are acceptable but limitted. More datasets are worth exploring. For example, Ultrafeeback[2], HelpSteer[3], Beavertails[4] are worth trying.\n\n*The empirical approach is generally fine to the reviewer, but unfortunately, the claims have many problems. The reviewer is willing to raise the rating if the issues can be solved.*\n\n[1] Controlled Decoding from Language Models, https://arxiv.org/abs/2310.17022\n\n[2] UltraFeedback: Boosting Language Models with Scaled AI Feedback, https://arxiv.org/abs/2310.01377\n\n[3] HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM, https://arxiv.org/abs/2311.09528\n\n[4] BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset, https://arxiv.org/abs/2307.04657\n\n[5] From r to Q\u2217: Your Language Model is Secretly a Q-Function, https://arxiv.org/abs/2404.12358\n\n[6] TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling, https://arxiv.org/abs/2410.16033"
            },
            "questions": {
                "value": "- The equation (4) is not well defined. The expectation is taken based on which distribution?\n- Which reward model did you use for Figure 2b,3,4? Did you only examine one reward model to support claims shown in these figures?\n- Can this approach be applied to some smaller and weaker reward models, like GPT2 models [1][2]? This would be important for some groups with restricted computation resources.\n- Can this approach be possibly applied to multi-objective alignment, like what many reward-guided decoding-time algorithms [3][4] can do? (Anyway, they are concurrent works, so there is no need to compete with them.)\n\n[1] https://huggingface.co/Tristan/gpt2_reward_summarization\n\n[2] https://huggingface.co/Ray2333/gpt2-large-helpful-reward_model\n\n[3] PAD: Personalized Alignment at Decoding-Time, https://arxiv.org/abs/2410.04070\n\n[4] GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment, https://arxiv.org/abs/2410.08193"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns, thanks."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}