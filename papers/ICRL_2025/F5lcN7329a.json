{
    "id": "F5lcN7329a",
    "title": "Consistent Neural Embeddings through Flow Matching on Attractor-like Neural Manifolds",
    "abstract": "The primary objective of brain-computer interfaces (BCIs) is to establish a direct connection between neural activity and behavioral actions through neural decoders. \nConsistent neural representation is crucial for achieving high-performance behavioral decoding over time.\nDue to the stochastic variability in neural recordings, existing neural representation techniques yield dynamical instability, leading to the failure of behavioral decoders in few-trial scenarios.\nIn this work, we propose a novel Flow-Based Dynamical Alignment (FDA) framework that leverages attractor-like ensemble dynamics on stable neural manifolds, which facilitate a new source-free alignment through likelihood maximization.\nThe consistency of latent embeddings obtained through FDA was theoretically verified based on dynamical stability, allowing for rapid adaptation with few trials.\nFurther experiments on multiple motor cortex datasets validate the superior performance of FDA.\nThe FDA method establishes a novel framework for consistent neural latent embeddings with few trials. \nOur work offers insights into neural dynamical stability, potentially enhancing the chronic reliability of real-world BCIs.",
    "keywords": [
        "Brain-Computer Interface",
        "Neural Decoding",
        "Flow Matching",
        "Dynamical Stability"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=F5lcN7329a",
    "pdf_link": "https://openreview.net/pdf?id=F5lcN7329a",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes an consistent neural embeddings using flow matching, and leverages attractor-like ensemble dynamics. The numerical experiments showed consistent alignment results and better results than existing algorithms. This paper also theoretically showed the stability on the alignment using the algorithm."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper uses the source-free alignment via likelihood maximization (FDA-MLA) and uses pre-training and fine-tuning to achieve consistent neural embeddings from non-stationary neural signals. The set up of latent extraction is via conditional feature extraction based on neural dynamics."
            },
            "weaknesses": {
                "value": "1. The dynamical stability verification is not clearly explained for the theoretical support.\n\n2. Some results are puzzling (see Questions below)."
            },
            "questions": {
                "value": "1. In part 3.2.2, can you explain the reason of using maximum mean discrepancy, why it is better than other matrix, can you show it ?\n\n2. For table 1, can you explain why the results of ERDiff is extremely different and worse than the others, that is a relatively new paper published in NeurIPS 2023.\n\n3. For table 1, why the R2(%) decreases to 23.79(FDA-MLA) and 45.23(FDA-MMD) at Day 8, but increases to 50.15 and 55.9? In the Cycle--GAN paper, the R2 decreases continuously. My understanding is that the alignment should be worser with longer time-gap relative to Day 0.\n\n4. For table 1, why the R2(%) of CEBRA is much better in RT-M dataset than CO-M dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes FDA - an alignment method to maintain performance of BCI decoder across recording sessions. The method is based on flow matching to achieve consistent neural embeddings which is theoretically shown to be dynamically stable and facilitates alignment in few-trial scenarios. Alignment performance is validated on multi-day datasets of monkeys performing motor tasks, showing competitive results against other baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The method based on flow matching is original, tackling an important and long standing issue in BCI field.\n* Problem formulation makes sense. Quantitative results are quite thorough with comparison using several baselines and datasets.\n* The paper is well written for the most part. Figures are clear and well presented."
            },
            "weaknesses": {
                "value": "* The paper cited a recent related work [1] but did not compare with it. Incorporating this additional baseline would provide a more comprehensive evaluation of the proposed method.\n* Some texts are unclear and need more elaboration (details in Questions).\n\n[1] Ayesha Vermani, Il Memming Park, and Josue Nassar. Leveraging generative models for unsupervised alignment of neural time series data. In The Twelfth International Conference on Learning Representations, 2024."
            },
            "questions": {
                "value": "* What aspect in the proposed method uses \"attractor-like ensemble dynamics\u201d? Can the authors provide more clarification on what \u201cattractor-like ensemble dynamics\u201d mean and how it is relevant to certain part of FDA? The term is used a lot in the Introduction but was never referred to again in the Methodology or Experiments & Results. \n* Line 124: how was the signal window sampled? how many windows are sampled per trial? Are the windows overlap with each other? \n* Line 125: why behavior label $y_i$ is only taken at the w-th timestep of $x_i$? Does this mean the method decodes the downsampled behavior time series rather than the original one? The other baselines didn\u2019t seem to have the behavior target downsampled. Decoding the downsampled behavior might make it an easier task than decoding the original behavior.\n* Figure 1: in the first block, shouldn\u2019t $c^S$ be at the top and $c^T$ be at the bottom?\n* Also figure 1: according to description in Methodology section, $x^T$ and $z^T$ should not be used during Pre-training phase (left and middle blocks)?\n* Line 185: does the transformer utilize positional embeddings? If so, what kind of positional embeddings was used?\n* Line 206: how was $\\eta$ pre-defined? Is $\\eta$ kept the same across days? \n* Figure 2c: Is each point on the plot the average of all test sessions or average of different choices of samples with the same ratio $r$? Providing this clarification and also adding errorbars for each point would make it more informative.\n* Also figure 2c: will performance of other baselines improve and reach the same performance of FDA if $r$ increases? If so, at how many trials will they become comparable to FDA? This is important to gauge the helpfulness of FDA in cases where scarcity of target samples is not a problem. \n* Figure 2d: why there are 9 days in Table 1 but 11 rows/columns in the shown matrices?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a novel Flow-Based Dynamical Alignment framework to obtain consistent neural representation. The FDA approach uses flow matching techniques to extract dynamics on stable manifolds. The FDA work addressing the challenge of dynamical instability offers insights into neural dynamical stability. The latent extracted from FDA has better decoding performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe proposed FDA method does perform better than baselines on decoding cursor velocity, and the learned latent space has more stability than baselines.\n2.\tThe proposed method has relatively better computational efficiency although may need more parameters.\n3.\tThe authors did multiple ablation studies on main components."
            },
            "weaknesses": {
                "value": "One weakness (as is already notified by the authors) is the lack of cross-subject validation, for example, between Monkey C and M."
            },
            "questions": {
                "value": "1.\tThe results on Monkey C seem very different from Monkey M, could you explain the reason?\n2.\tHow many trials are in the source domain?\n3.\tThe authors computed average MLE, what is the average across? Then what are all MLEs instead of the average one?\n4.\tIs fine tuning the reason that FDA performs better on few-trials scenarios?\n5.\tHow do you choose the hyper parameters? Especially the dimensionality of your embedded latent space. Also, when you compare all the different models, do they have the same latent dimension?\n6.\tI am just curious, in Table 1, for each method, the worst r2 is in a different day, e.g., in CO-M, LSTM has the worst r2 in day29, but in FDA-MLA, it is just day8. Could you explain the reason?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors proposed a flow-based framework to align neural embeddings across days. They benchmarked their model against five existing models, three of which focus on BCI alignment: ERDiff (based on diffusion), NoMAD (based on LFADS), and Cycle-GAN (based on Cycle-GAN). Their proposed model consistently outperformed the others across various benchmarks."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The alignment of raw neural signals or latent embeddings is crucial and necessary for real-world BCI applications.\n\n2. To the best of my knowledge, this is the first study to apply flow-matching to BCI signal alignment.\n\n3. Extensive benchmarking was conducted against state-of-the-art models.\n\n4. The writing is clear and easy to follow."
            },
            "weaknesses": {
                "value": "1. I am concerned about the evaluation of NoMAD and Cycle-GAN. In a highly relevant submission (https://openreview.net/forum?id=LNp7KW33Cg), which almost certainly comes from the same group, the R\u00b2 of NoMAD and Cycle-GAN is nearly six times better than what is reported in this paper.\n\n2. This paper uses the exact same dataset as the Cycle-GAN paper (https://elifesciences.org/articles/84296). In the Cycle-GAN paper, the average R\u00b2 is above 50% (Figure 3A), consistent with the companion submission I mentioned earlier, but in this paper, the evaluation shows an average R\u00b2 below 10% (Table 1). Why is there such a significant discrepancy?\n\n3. It is inappropriate to cherry-pick results in the presentation. Main Table 1 and Supplementary Table 8 are nearly identical. However, the authors included nine sessions per monkey in Table 1 but omitted two sessions that had the worst performances (Day 22 and 25 in CO-M; Day 40 and 77 in RT-M). Highlighting the best two or three sessions is one thing, but removing the worst two or three sessions is something entirely different."
            },
            "questions": {
                "value": "1. I am still confused about the term \"attractor-like.\" The authors mentioned it in the context of \"*utilizing attractor-like ensemble dynamics (Gonzalez et al., 2019), a representation mechanism for encoding stimuli in the brain.*\" I reviewed the referenced paper, but found that \"attractor\" is only mentioned once: \"*Second, attractor-like mechanisms ensure the persistence of representations over short periods of time (days), even if the animals are not exposed to the task or if the circuit is perturbed by lesions.*\" I believe additional explanation of what you mean by \"attractor-like\" in this context would be helpful.\n\n2. Azabou et al., 2023a and Azabou et al., 2023b are the same paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}