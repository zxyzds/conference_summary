{
    "id": "Ju7zj6tUm6",
    "title": "How vulnerable is my learned policy? Adversarial attacks on modern behavioral cloning policies",
    "abstract": "Learning from Demonstration (LfD) algorithms have shown promising results in robotic manipulation tasks, but their vulnerability to adversarial attacks remains underexplored. This paper presents a comprehensive study of adversarial attacks on both classic and recently proposed algorithms, including Behavior Cloning (BC), LSTM-GMM, Implicit Behavior Cloning (IBC), Diffusion Policy (DP), and VQ-Behavior Transformer (VQ-BET). We study the vulnerability of these methods to untargeted, targeted and universal adversarial perturbations. While explicit policies, such as BC, LSTM-GMM and VQ-BET can be attacked in the same manner as standard computer vision models, we find that attacks for implicit and denoising policy models are nuaced and require developing novel attack methods. Our experiments on several simulated robotic manipulation tasks reveal that most of the current methods are highly vulnerable to adversarial perturbations. We also investigate the transferability of attacks across algorithms and architectures, providing insights into the generalizability of adversarial perturbations in LfD. We find that, the success rate of the transfer attacks is highly dependent on the task, raising necessity for more fine-grained metrics that capture intricate details of adversarial weakness of the state distribution. In summary, our findings highlight the vulnerabilities of modern BC algorithms, paving way for future work in addressing such limitations.",
    "keywords": [
        "Adversarial Attacks",
        "Learning from Demonstrations",
        "Behavior Cloning"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "Adversarial attacks on modern behavioral cloning policies",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Ju7zj6tUm6",
    "pdf_link": "https://openreview.net/pdf?id=Ju7zj6tUm6",
    "comments": [
        {
            "summary": {
                "value": "The paper analyses the vulnerability of imitation learning policies to adversarial attacks in the visual input space. The paper also introduces a novel adversarial attack for diffusion-based policies, as it finds that these are generally harder to attack with sota attacks.\nThe paper evaluates on the Robomimic environments and finds that attacks dont generalise even when similar vision backbones are used, and that diffusion policies are generally more robust."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- relevant topic and well grounded selection of attacks and imitation learning algorithm\n- solid evaluation in the given environment\n- novel attack for hardest-to-attack diffusion policies"
            },
            "weaknesses": {
                "value": "- motivation for diffusion policies attack is unclear: when is the setting of having access to the denoising process of the policy realistic?\n- the empirical evaluation is limited to just one environment (Robomicic). To draw conclusions, at least 1-2 additional environments should be considered.\n- A contrasting and comparing to prior works is largely missing\n- e.g. the finding that denoising results in high adversarial robustness has been made before (see \"(Certified!!) Adversarial Robustness for Free!\")"
            },
            "questions": {
                "value": "- Could the authors specify the threat model more clearly and motivate the setting for having access to diffusion steps"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the vulnerability of modern behavior cloning policies against popular adversarial attack methods. The behavior cloning methods includes original BC, LSTM-GMM, Implicit BC, Diffusion Policy and VQ behavior transformer. Adversarial attack methods includes projected gradient descent (PGD) and universal adversarial perturbation (UAP). Authors adapted these attack methods for some of the BC methods. The investigation is conducted on four manipulation tasks: Lift, Can, Square, and Push-T. The work explored a few interesting questions in addition: is perturbation learned transferable to policies learned using different BC methods? What is the impact of feature extraction backbone on adversarial attack and is the perturbation transferrable across different backbones. How does the action prediction horizon of diffusion policy affect its vulnerability?"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The work provide some insights on adversarial robustness of general BC methods (DP is much more robust than other methods)\n2. The work provide adaptation of classic adversarial attack methods to newer BC methods like DP, IBC, and demonstrated attack success using the adapted methods.\n3. The work conducted some transferrable analysis on perturbation learned, across BC methods or across visual backbones."
            },
            "weaknesses": {
                "value": "1. The writing needs improvement in general. (see questions)\n2. Some arguments are not well explained or supported by the result provided. (see questions)"
            },
            "questions": {
                "value": "1. line 221: the first \"maximize\" should be \"minimize\"? \n\n2. line 221-223: this explanation is confusing to me, why probability of selecting the targeted action low would leads to no clear loss function?\n\n3. line 222: what does clean mean here?\n\n4. line 229: do you mean decrease? (same for the later \"increase\" used)\n\n5. Algorithm 1: the initialization of S seems redundant?\n\n6. line 342: Q2 is not answered.\n\n7. line 352- 356: please consider provide concise task description if possible. (even not in main paper)\n\n8. line 363: again, this will be more clear if some concise task/benchmark description is provided somewhere.\n\n9. line 374: what does the perturbed observation looks like, could you provide a few examples?\n\n10. Figure 2: why report task success rate rather than attack success rate? Also, I think normal success rate refers to task success rate? \n\n11. line 411-412: could you provide some task complexity related information? How is complexity measured?\n\n12. line 426-427: can you explain this in more detail? A lot of factors should be considered here, for example, for more complex tasks, their task success rate are likely lower, so you might want to use some relative attack success rate to account for this.\n\n13. line 426-427: Table 1 is success rate?, Table 2 is Mean IoU? how do you compare this two and reach the observation that \"we noticed an increased propensity for adversarial perturbations to transfer between algorithms\"?\n\n14. Table 1: more explanation on what the columns and rows are? are attacks obtained in column methods and applied on row methods? also, what is \"random\", this is not explained in the paper I think?\n\n15: line 460: \"we observed high transferability for some algorithms (e.g., Diffusion Policy-C and VQ-BET),\" are you referring to LSTM-GMM?\n\n16: line 752- 754: The hyperparameter setting is not clear to me. First, please check the sentence. Second, what is perturbation if epsilon is set to 0.625 (which is pretty big compared to usual adversarial attack work.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the vulnerability of commonly used behavioral cloning algorithms. It considers well-known PGD and UVA attacks developed for supervised learning and directly applies them to compromise Behavior Cloning (BC), LSTM-GMM, and VQ-Behavior Transformer (VQ-BET) algorithms. It also shows that the PGD attack can be adapted to compromise Implicit Behavior Cloning (IBC) and Diffusion Policy (DP)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper highlights that commonly used behavioral cloning algorithms are vulnerable to PGD attacks."
            },
            "weaknesses": {
                "value": "The threat model is undefined. Are we considering data poisoning attacks? What can the attacker modify, states, actions, or both? What is the attacker's goal? What are the constraints to the attacker? None of them are defined in the paper. From 3.2.1, it seems that the paper considers both targeted and untargeted attacks. But the accurate definitions are missing, and it is unclear what tilde{p}_theta is. \n\nWhile undefined in the paper, a reasonable attack objective is to induce the agent to learn a bad policy. In this case, the attacker should modify multiple data points collectively. However, the simple attacks considered in the paper, such as PGD and UAP, were developed for the one-shot supervised learning setting and are ineffective for the sequential setting. The new attacks for IBC and Diffusion Policy are straightforward adaptions of PGD and also myopic.  \n\nThe reason why the simple myopic attacks can still work, as shown in the paper, is because the paper completely ignores defenses. However, there are well-known defenses for supervised learning, such as adversarial training and randomized smoothing, that can be easily adapted to the sequential setting when attacks are myopic, as considered in the paper. Simply showing that unprotected behavioral cloning algorithms are vulnerable to PGD attacks is not very interesting."
            },
            "questions": {
                "value": "Please see the discussion on weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a thorough study about the adversarial vulnerability of several representative imitation learning methods (e.g., Vanilla BC, LSTM-GMM, IBC, Diffusion Policy and VQ-BeT) under two white-box attacks (PGD and UAP). The evaluation also covers the transferbility of perturbations across different policies and architectures."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The transferbility of perturbation between policies and architectures is also investigated.\n2. The paper is well organized and easy to follow."
            },
            "weaknesses": {
                "value": "1. I concur with the authors that naive success rates may not accurately reflect the impact of adversarial attacks on robotic policies. It is essential to develop additional metrics, particularly those addressing safety concerns associated with real-robot deployments.\n2. The current version seems that we have to develop tailored attacks given specific task, with privilege knowledge about the model parameters (PGD) and training samples (UAP). Thus the practical implications are limited for real-world deployments.\n\n**[Minor:]**\n1. Subscripts are not used correctly in line 752-753"
            },
            "questions": {
                "value": "1. It is noted that the perturbations are applied uniformly throughout the trajectory during inference concerning UAP attack. Then what about PGD? Are perturbations computed for every single step during inference with updated observations?\n2. How to determine the 'target action' when performing targetted attack? Does it also need to be updated during the inference process?\n3. What is the precise number of iterations or steps utilized in the PGD attack? It is my belief that an extensive optimization process may diminish the practicality of the attack, as it could become less 'stealthy' for robots engaged in real-time operations.\n4. As indicated in Tables 5 and 6, the IBC policy appears to be more vulnerable in the <Square> and <Can> tasks (than <Lift> and <Push-T>), while the LSTM-GMM demonstrates exceptional robustness in the <Can> task. Do these results imply that it is challenging to draw definitive conclusions regarding the relative robustness of the different policies (models), given that their performance varies across tasks?  Could the authors discuss potential factors that might contribute to the varying performance?\n5. Have the authors studied about multi-task learning setting? Do you expect perturbations could be transferred across tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}