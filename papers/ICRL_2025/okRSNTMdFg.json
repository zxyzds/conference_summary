{
    "id": "okRSNTMdFg",
    "title": "Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts",
    "abstract": "With the rapid progress of diffusion-based content generation, significant efforts are being made to unlearn harmful or copyrighted concepts from pretrained diffusion models (DMs) to prevent potential model misuse. However, it is observed that even when DMs are properly unlearned before release, malicious finetuning can compromise this process, causing DMs to *relearn the unlearned concepts*. This occurs partly because certain benign concepts (e.g., \"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"), facilitating their relearning via finetuning. To address this, we propose **meta-unlearning** on DMs. Intuitively, a meta-unlearned DM should behave like an unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes malicious finetuning on unlearned concepts, the related benign concepts retained within it will be triggered to *self-destruct*, hindering the relearning of unlearned concepts. Our meta-unlearning framework is compatible with most existing unlearning methods, requiring only the addition of an easy-to-implement meta objective. We validate our approach through empirical experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4 and SDXL), supported by extensive ablation studies.",
    "keywords": [
        "Meta-Unlearning",
        "Diffusion Models"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We develop the meta-unlearning framework for diffusion models to prevent malicious finetuning to relearn unlearned concepts.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=okRSNTMdFg",
    "pdf_link": "https://openreview.net/pdf?id=okRSNTMdFg",
    "comments": [
        {
            "withdrawal_confirmation": {
                "value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."
            }
        },
        {
            "comment": {
                "value": "Dear Reviewers,\n\nWe appreciate your time and insightful feedback on our work.\n\nA publicly available unlearned DM could be maliciously finetuned to relearn the unlearned concepts. In our meta-unlearning framework, we have two design goals: (1) a meta-unlearned DM should behave like an unlearned DM when used as is; and (2) if someone maliciously finetunes the meta-unlearned DM, the model should **automatically self-destruct to avoid misuse**.\n\nOur meta-unlearning framework is compatible with almost all existing unlearning methods, requiring only the addition of an easy-to-implement meta objective. When combining adversarially robust unlearning methods, we can defend against both adversarial attacks and malicious finetuning.\n\nAfter careful consideration, we have decided to withdraw our paper from ICLR. Thank you once again for your thorough review and thoughtful comments.\n\nBest,\\\nThe Authors"
            }
        },
        {
            "summary": {
                "value": "This paper describes a novel approach to unlearning of concepts in (image) diffusion models that aims towards robustness against re-learning those same concepts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper addresses a relatively novel, interesting problem. The proposed solution appears to work well in the evaluated scenarios (-> see weaknesses) and is applicable to many models, concepts and unlearning approaches.\n\nGiven my background, I cannot judge the completeness of related work. However, it is described in detail and cites also recent advances."
            },
            "weaknesses": {
                "value": "My main concern with the paper is that during the unlearning the knowledge of the finetuning dataset $D_{FT}$ is assumed. For a practical scenario, I would rather expect that the re-learning could take place with any set of images with the respective concepts. As a consequence such images cannot be directly \"meta-unlearned\" against. It would be crucial to see how the relearning performs on such images.\n\nThe evaluation and results is not described well: On which images have the scores of Table 1 been computed? For the table, please add also standard deviation. In this scenario it seems plausible (though not probable) that there might be outliers appearing for meta-unlearning. How is the similarity of the generated images to the finetune dataset? Also, the evaluation is also done on only one setting. (nudity). Additionally, how close are the generated images to the finetuning datasets? From my understanding, it would be impossible to avoid having 1-1 copies of the fine-tuning dataset in the image generation.\n\nThe presentation of the paper should be improved considerably. Some (partially subjective) suggestions:\n- Figure 1. did confuse me more that it helped. I would also discourage the forward reference here.\n- The description of related work contains a lot of citations but they are not explained well. I would focus on a smaller area of interest but invest more space into covering this more in-depth. In particular, consider removing Section 2.2 and expland Section 2.3,"
            },
            "questions": {
                "value": "How does the approach perform if the finetuning set is unknown?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper discuss the phenomenon of \"malicious fine-tuning\" in diffusion models and introduces a meta-unlearning method to mitigate it. The proposed approach prevents the model from relearning malicious concepts by \"self-destructing\" related benign concepts that could support the undesired relearning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Introducing meta-learning into the unlearning domain is an interesting approach.\n2. The extensive experimental results demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The problem formulation is unclear to me. What exactly is meant by \"malicious fine-tuning\"? Does this refer to fine-tuning on a malicious dataset D_{FT} \\in D_{forget}? Is \"fine-tuning\" here simply standard fine-tuning, or fine-tune based unlearning (e.g., gradient ascent)? If malicious fine-tuning simply involves standard fine-tuning on a subset of D_{forget}, then it\u2019s somewhat expected that the model would relearn knowledge intended for unlearning, which makes the problem formulation seem a bit trivial.\n\n2. I\u2019m also skeptical of the approach that destructs related benign concepts. In my opinion, the goal should be to unlearn harmful concepts (e.g., \"nudity\") while retaining benign knowledge (e.g., \"skin\"), rather than destruct it. Effective unlearning of a harmful concept should not come at the cost of unlearning related benign concepts, unless absolutely necessary\u2014which I think it is not.\n\n3. For the \"Robustness to adversarial attacks\" analysis, while it's reasonable to compare with RECE given that your approach builds on it, it would strengthen the work to include comparisons with other baselines."
            },
            "questions": {
                "value": "1. In \u2018Concept relationship during meta-unlearning\u2019 analysis, the orthogonal term appears to show a slight downward trend. If I interpret this correctly, it suggests that the target unlearned concept (e.g., \"nudity\") and the related benign concept are becoming decoupled. Is this result expected, or is it a side effect of the meta-unlearning approach? Since the goal is to unlearn the malicious concept and destruct the knowledge of the related concept, why are they being decoupled?\n\n2. Could you provide more detail for the \u2018robustness to adversarial attacks\u2019 analysis? Specifically, could you describe the type of adversarial attack used and share implementation details? I didn\u2019t find this information in either the main paper or the appendix, but please let me know if I\u2019ve overlooked something."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposed a meta-unlearning framework to prevent the diffusion models from relearning the unlearned concepts.  Unlearning diffusion models have been studied and promising solutions have been proposed. Yet, unlearned diffusion models can relearn the unlearned concepts through fine-turning. A key innovation of this paper is that the forward perspective of how the diffusion models will be fine-tuned is considered. More specifically, the authors 'simulated' the fine-tuning process of the diffusion models and then used the 'simulated' fine-tuning process to guide the unlearning process such that (i) the learning stagnates along the direction of the unlearned concepts, and (ii) self-destruct the retained information in the diffusion models when fine-tuning. These forward perspective is novel and well motivated."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper provides a forward perspective of unlearning diffusion models. This approach is well motivated and sounds novel."
            },
            "weaknesses": {
                "value": "**Major arguments**:\n- *A1*: The value of the orthogonal term fluctuates substantially. This can be many reasons, e.g., high dimensionality of the parameter space, lagged loss landscape, and competing objectives of L(D_{retain}) and L(D_FT). It indicates that the \"self-destruct\" mechanism isn't reliably working consistently. The improvement might be coming from the minimization of the gradient norm, not the \"self-destruction\" mechanism that the author claim. An ablation study isolating the impact of the orthogonal term would identify the key factors for the effectiveness of the proposed framework.\n- *A2*: The proposed method assumes that how the diffusion model will be fine-tuned is known. In practice, however, this is rarely the case. This is a key limitation of the proposed method. The paper could be improved by showing the results for the diffusion models unlearned based on an expected fine-turning process but actually fine-tuned on a different process.\n- *A3*: While the authors claim that the existing unlearning methods can relearn the unlearned concepts in their experiments, they did not show the results for all the models they claimed for (For example, Wu, Yongliang, et al. \"Unlearning Concepts in Diffusion Model via Concept Domain Correction and Concept Preserving Gradient.\" arXiv preprint arXiv:2405.15304 (2024)). It is not clear whether the claims are valid without the results.\n\n**Some technical comments**:\n- In lines 6 and 10 in the Algorithm 1, I think we should substract the gradient instead of adding it in order to minimize the objective."
            },
            "questions": {
                "value": "1. To what extent the orthogonal term improves the unlearning performance?\n2. When the diffusion models are fine-tuned differently from what anticipated at unlearning process, how effective is the proposed method?\n3. Provide evidence for the claim as described in A3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}