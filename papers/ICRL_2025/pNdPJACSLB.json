{
    "id": "pNdPJACSLB",
    "title": "Learning K-U-Net in Constant Complexity with Application to Time Series Forecasting",
    "abstract": "Training deep models for time series forecasting is a critical task with an inherent challenge of time complexity. While current methods generally ensure linear time complexity, our observations on temporal redundancy show that high-level features are learned 99.5\\% slower than low-level features. To address this issue, we introduce a new exponentially weighted stochastic gradient descent algorithm designed to achieve constant time complexity in deep learning models. We prove that the theoretical complexity of this learning method is constant. Evaluation of this method on Kernel U-Net (K-U-Net) on synthetic datasets shows a significant reduction in complexity while improving the accuracy of the test set.",
    "keywords": [
        "machine learning",
        "time series",
        "complexity reduction"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=pNdPJACSLB",
    "pdf_link": "https://openreview.net/pdf?id=pNdPJACSLB",
    "comments": [
        {
            "summary": {
                "value": "The authors propose a constant complexity method for time series forecasting. The motivation for forecasting is not so clear though."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Clear time series problem definition in the preliminaries\n- Interesting method to use modfiy the gradient updates and study the optimization method in U-Nets"
            },
            "weaknesses": {
                "value": "- The abstract is very short and vague. Not all time series methods have linear complexity.\n- Need spaces between words and references\n- U-Net architecture is typically not used in time series forecasting\n- Some mathematical variables are not italicized\n- State-of-the-art time series baselines are missing from Table 1, e.g., DeepAR, MQ-CNN, MQTransformer, ETS and foundation models, e..g, Chronos (Ansari et. al, \"Chronos: Learning the language of time series\", 2024)\n- There are references missing in Section 2.2\n- The complexities of current methods in Section 2.2 can be move to an appendix\n- This paper seems very preliminary and in its current state is not ready for publication."
            },
            "questions": {
                "value": "1. How tied is the method to a U-Net architecture?\n2. What motivates the authors to use a U-Net?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel exponentially weighted stochastic gradient descent algorithm - proposed to improve the training speeds of Kernel U-Nets. The method is tested on a synthetic dataset for multivariate/multihorizon forecasting, showing faster training convergence and OOS performance improvements."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The introduction of transformer based models has significantly increased the sizes of SOTA models for time series forecasting. While inference speed gets more focus, improvements in training efficiency can help to reduce the computational requirements of large models - democratising research and reducing the environmental impact of their training."
            },
            "weaknesses": {
                "value": "However, there are several key limitations in the proposed method, that raise some questions:\n\n1. *Experimental limitations*: While applications of Kernel U-Nets have been studied, it is not immediately clear that they are the state-of-the-art for multivariate/multihorizon forecasting - with many other more competitive models having been proposed in Table 1 as well. As such, detailed performance comparisons against other SOTA and baseline models for multivariate/multihorizon forecasting are required, using non-synthetic real-world datasets. Actual wall clock time of the training methods would also be beneficial to see how much faster the model trains on the same hardware, assuming performance is similar to baselines.\n\n2. *Generalisation of findings* - many of the motivations the paper are stated without evidence/references, and it is not immediately clear how applicable they are to the wide universe of time series forecasting problems, datasets, and architectures. E.g. that slower training convergence is a result of gradient imbalances across layers, and that increasing gradient updates in deeper layers can lead to training improvements in general."
            },
            "questions": {
                "value": "1. How well do kernel U-Nets perform with respect to other SOTA models for multivariate multihorizon forecasting, and how much quicker do they take to train?\n2. Assuming that removing overlapping patches is desirable for performance, how does the method compare to simply downsampling the data and constructing minibatches with no overlaps in batched sequences? \n4. How well does the proposed approach perform on real world time series datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an algorithm called Exponentially Weighted Stochastic Gradient Descent with Momentum (EW-SGDM), aimed at reducing the training complexity of deep learning models on time series forecasting tasks. The approach is tested on the Kernel U-Net, a U-Net variant for time series forecasting. The core contribution lies in the identification of significant redundancy in the low-level feature learning within U-Net models. By strategically ignoring these redundancies, EW-SGDM optimizes computational resources. The paper presents experiments on multiple synthetic time series datasets."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Originality: the paper observes significant redundancy in patch-based training of time series models, which can lead to faster training behavior. Unfortunately, the rest of the paper fails to sufficiently build on this insight to present a contribution of wide interest to the ICLR community."
            },
            "weaknesses": {
                "value": "It is not clear what this paper tries to contribute:\n- If an optimization paper, it should carefully compare the proposed optimization method against standard and SOTA optimization methods, on a variety of settings. In particular, it should include a comparison against the Layer-wise Adaptive Rate Scaling (LARS, cf. https://arxiv.org/abs/1708.03888 )\n- If a time series paper, it should compare the Kernel U-Net against other time series forecasting methods.\n- In both cases, the experimental validation is severely lacking, and the paper only provides results on synthetic datasets. There should be a comprehensive experimental validation, including real-world datasets.\n- There should comprehensive analysis of the learning dynamics, with and without the proposed method, to assess whether the proposed training algorithm indeed reaches its goals."
            },
            "questions": {
                "value": "- Line 097: as written, the lookback window sees into the future from $t$, shouldn't it be defined as $(x_{t-L+1}, \\ldots, x_t)$ ?\n- Line 124: a `\\ref` is defined without a target.\n- Line 159: it's not clear what is intended by \"the patch length\" (S); please define explicitly.\n- Line 160: should reference Figure 2, not Figure 1. Besides, the real Figure 1 is not introduced or explained in the text.\n- Line 184-188: the claims here are provided without citation or analysis.\n- Line 446: the reference should be fixed (authors separated by commas in BibTeX)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper considers time-series forecasting problem that forecasts $x_{L+1},\\dots,x_{L+T}$ using $x_{1},\\dots x_{L}$, where $x\\in\\mathbb{R}^M$ is a feature vector.\nThey introduce a technique they call exponentially weighted SGD with moment (EW-SGDM) to make the \"learning complexity\" of a time-series forecasting model, Kernel U-Net, constant to the output sequence length $T$.\nThis method heuristically sets learning rates for each layer to encourage learning of deeper layers.\nThey conducted experiments on simple synthetic datasets."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed method seems to be simple and usable for various tasks."
            },
            "weaknesses": {
                "value": "- The mathematical notations are not appropriately used and thus hard to follow. For example, although $W^{(l)}$ is a weight, and thus, expected to be a matrix and $S$ is a hyperparameter, and thus, expected to be a scalar, Eq (2) says $W^{(l)}=S^{l-1}$.\n- The reason that the proposed method reduces complexity is not explained. The description, which should be around L268, is missing.\n- Experiments are limited to simple synthetic experiments using sine curves."
            },
            "questions": {
                "value": "It would be appreciated if the legends of Figures 7-15 were described in a more human-friendly manner."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed.",
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}