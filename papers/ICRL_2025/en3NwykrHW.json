{
    "id": "en3NwykrHW",
    "title": "Minimax Optimal Regret Bound for Reinforcement Learning with Trajectory Feedback",
    "abstract": "We study the reinforcement learning (RL) problem with trajectory feedback. The trajectory feedback based reinforcement learning problem, where the learner can only observe the accumulative noised reward along the trajectory, is particularly suitable for the practical scenarios where the agent suffers extensively from querying the reward in each single step. For a finite-horizon Markov Decision Process (MDP) with $S$ states, $A$ actions and a horizon length of $H$, we develop an algorithm that enjoys an optimal regret of $\\tilde{O}\\left(\\sqrt{SAH^3K}\\right)$ in $K$ episodes for sufficiently large $K$. To achieve this, our technical contributions are two-fold: (1) we incorporate reinforcement learning with linear bandits problem to construct a tighter confidence region for the reward function; (2) we construct a reference transition model to better guide the exploration process.",
    "keywords": [
        "Reinforcement learning theory",
        "regret analysis",
        "trajectory feedback"
    ],
    "primary_area": "learning theory",
    "TLDR": "We prove a nearly optimal regret bound for reinforcement learning with trajectory feedback.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=en3NwykrHW",
    "pdf_link": "https://openreview.net/pdf?id=en3NwykrHW",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses reinforcement learning with trajectory feedback, where the reward in a Markov Decision Process is not observed throughout an episode but only as a cumulative reward at the episode\u2019s end. The authors propose an algorithm that achieves a regret bound with statistical behavior comparable to the state of the art in standard reinforcement learning literature."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper proposes an algorithm that achieves a better asymptotic regret bound than those previously established for reinforcement learning with trajectory feedback. The authors employ a novel method in their proofs, which appears robust."
            },
            "weaknesses": {
                "value": "First, I must admit that I am not well-versed in the literature on regret bounds in reinforcement learning with bandits.\n\nTo me, the framework of this paper appears quite limited. Moreover, the asymptotic regret bounds seem similar to those achieved in more classical frameworks, giving the impression that the work involves only minor modifications to standard proofs. However, I may be mistaken, as I am not deeply familiar with this literature."
            },
            "questions": {
                "value": "Here are few questions :\n1. Could the authors elaborate on the statement in lines 261\u2013262: \"Therefore, by running [...] for some constant $c>0$\"? A more detailed explanation of this step would be helpful.\n2. The definition of the set in (10) seems unclear to me. First, how do the authors compute $\\max W^{\\pi'}(\\hat{R},P)$? Additionally, how is the quantity on the right-hand side of the selection criterion, i.e., the practical value of $\\tilde{O}$?\n3. Can the method employed in this paper be adapted to consider cases where $K$ is not that large compared to $S$, $A$ and $H$, similar to the approach in [Zhang et al.]?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies a RL problem with delayed rewards, which are sampled after letting a trajectory roll out for a certain time horizon. This is a practical scenario that appears in several applications and that, to date, doesn't have a detailed and comprehensive solution. The paper shows how optimal regrets can be obtained that are similar to those of traditional RL algorithms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper studies a novel and interesting variant of the standard RL problem, where rewards are generated with a delay rather than at each time instants. The provided bounds are technically sound and improve upon the existing literature."
            },
            "weaknesses": {
                "value": "1. Rewards are assumed to be a linear function of the trajectory (sequence of states and actions). This may not be practical in real-life. This is not practical even in the case discussed in the introduction - medical care. \n\n2. Learning the optimal policy, with the assumption of linear rewards, is now a (linear) regression problem. This is the main part from where the similarity between regret bounds for traditional RL and the proposed method arises. Can you comment on more complicated or realistic bandit models? \n\n3. Using a parameterized policy that can be systematically updated after the end of each trajectory, instead of policy elimination, might result in tighter regret bounds. Is this the case?"
            },
            "questions": {
                "value": "Do the methods/ideas extend to cases where rewards are nonlinear functions of the trajectories?\n\nThe connections to linear bandits is interesting, and it would be beneficial to expand upon this connection (formulation and examples). The literature in this area is also rather extensive, and a more comprehensive review/comparison would be helpful.\n\nSome numerical/illustrative examples could help the reader appreciate the theoretical results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper considers the episodic reinforcement learning with trajectory feedback. In this paper, the authors proposed an online learning algorithm that achieves optimal regret which scales as sqrt(SAH^3K). Unlike existing methods directly applied linear bandit, the proposed algorithm does not suffer from the regret with higher order on S."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper provides rigorous theoretically analysis for the proposed approach. Instead of building confidence region as linear bandit, the proposed algorithm maintain a policy set within a constant range, which can be done through data sampling in batches. The theoretical motivation behind this idea is very clearly presented in the paper with rigorous proofs. \nComparing to existing work, the proposed algorithm achieve optimal regret bound for RL with trajectory feedback only. Given the trajectory feedback is more general for real application, the proposed work has a potential to be applied to real problem with better performance guarantees."
            },
            "weaknesses": {
                "value": "The paper focuses on proposing algorithm that has tighter regret bound. While the rigorous proofs are appreciated, a more comprehensive comparison would help with understanding of the importance of the difference. It will become clearer with some toy experiment showing how the linear bandit performance in the episodic MDP environment when state/action is huge and time horizon is long. \nAnd since the proposed algorithm still require exponential time cost, it is also needed to compare the complexity in the paper."
            },
            "questions": {
                "value": "Beside what I wrote in the previous section, I would also like to understand more about Theorem 1 and 7. The statement of 'a RL problem with trajectory feedback has the same asymptotically optimal regret as standard RL' is a very intriguing result. I would like to read more explanation on the interpretation of Theorem 7 in the papaer."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors present a regret bound for RL with trajectory feedback. The lower bounds improves upon existing results reducing the dependency in the state space and are minimax optimal. To achieve the bounds, the authors use better confidence bounds than previous work and a more refined exploration scheme."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "A strong theoretical result improving upon SOTA for an important problem\n\nRefined techniques for a well studied problem\n\nI appreciate the authors' attempt to provide an informal explanation of the proofs. It is not trivial."
            },
            "weaknesses": {
                "value": "I am curious to have a formal understanding of the term \"for sufficiently large K\" when presenting THM1. This is important because it can render the result.\n\nThe presentation of the algorithm is convoluted. Especially. it is not clear what is Design in Algorithm 5. In general, I think this can be grossly simplified. \n\nThe last term in THM 7 is kind of disappointing as it renders the result less exciting. \n\nI believe the examples in the introduction were discussed by others before. A proper reference is needed. \n\nThere are some typos in the text that should be fixed: e.g., \"ofte\" (l46) and \"fro\" (l250)"
            },
            "questions": {
                "value": "Q1: Please elucidate the initial K issue. \n\nQ2: The paper seems incremental wrt Efroni et al (2021). What is the main contribution wrt Efroni et al?\nIs it a mix and match of a model and know techniques? If not, are the new techniques applicable elsewhere?\n\nQ3: Can you say something about the constant c>0 in l262? Does it depend on the problem itself?\n\nQ4: L282-285: as far as I understand the statement \"it holds that \\pi^* \\in \\Pi_{\\ell+1}\" is a high probability even rather than an almost sure statement. So, if I don't understand what is going on. More specifically, for any finite T or K_\\ell all elements in Eq (9) and after are random variables, but they seem to be treated as fixed because everything is taken in the limit. But it cannot be taken as such (at least not without a proof). I am probably missing something here, but this seems like a loose usage of \\tilde{O} notations. Please explain.\n\nQ5: How is K_0 set in Algorithm 1?\n\nQ6: It seems to me that the computation needed for Algorithm 5, line 9 would be quite difficult. Is this correct? Can you say something about the complexity of the different algorithms?\n\nQ7: What do you suspect is the real dependence of the low order terms in THM7? \n\nI would be happy to raise my score if the questions above are answered properly, especially Q4."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an algorithm that achieves a minimax optimal regret bound for RL with trajectory feedback. Specifically, the proposed algorithm achieves a regret bound of $\\tilde{O}(\\sqrt{SAH^3K})$ for an episodic MDP."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper tackles a well-motivated problem, proposing an asymptotically optimal regret bound algorithm for scenarios with trajectory-level feedback.\n\n- Up to Section 4, the authors clearly present their motivations, objectives and proof sketch, making it easier for readers to grasp the core concepts of the paper."
            },
            "weaknesses": {
                "value": "The paper has numerous typos and complex, undefined notations, giving the impression of a hastily composed and unpolished draft. The structure and expressions appear to be inspired by [1], yet there are instances of notations that are either not used or are undefined in the context of this paper. A thorough revision to polish the paper would improve its clarity.\n\nIn particular, Section 5 lacks logical flow, as many concepts are listed without a thorough theoretical explanation, making it difficult for readers to follow, especially given the insufficient description of each pseudocode. Considering page limitations, substituting theoretical proofs with a more accessible explanation of the pseudocode in the main paper could enhance readability.\n\nThe proposed algorithm also appears to have impractical elements. See Questions.\n\nTypos\n\n- Line 46: \"ofte\" should be \"often\"\n\n- Algorithm 2: $\\mathcal{D}_2$ is not defined\n\n- Algorithm 3, Line 3: The font of \u201cplan\u201d needs correction\n\n- Line 388: \"we\" should be capitalized to \"We\"\n\n- Line 483: $S^1 1$ should be corrected to $S^{11}$\n\n- Algorithm 5: $\\lambda_\\pi$ is not defined.\n\n[1] Zhang, Zihan, et al. \"Near-optimal regret bounds for multi-batch reinforcement learning.\" Advances in Neural Information Processing Systems 35 (2022): 24586-24596."
            },
            "questions": {
                "value": "- On line 310, optimization is mentioned for the double-state reward function $r(s,s',h,h')$. Could the authors provide more detail on this?\n\n- In Algorithm 4, Line 7, does $\\bar{\\pi}$ refer to what is stated in Eq (6)? Is it possible to obtain $\\bar{\\pi}$ in a tractable manner?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies reinforcement learning (RL) with trajectory feedback. The learner can only observe the accumulative noisy reward once a trajectory is terminated. The state transition model is unknown to the learner. The paper develops an online learning algorithm and shows that its regret bound is optimal."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. RL with trajectory feedback is an interesting problem.\n3. This is a solid theory paper. The analysis is rigorous and sound."
            },
            "weaknesses": {
                "value": "1. There is no experiment.\n2. In the upper bound of Theorem 7, the last three terms dominate. In contrast, the abstract and the introduction claim that the upper bound is determined by the first term. The authors should clarify under what conditions, if any, the first term dominates. If the first term is not asymptotically dominant, the authors should explain why they only focus on the first term in the abstract and the introduction.\n3. The introduction claims that the developed algorithm for RL with trajectory feedback achieves the same asymptotically optimal regret bound as the standard RL. The authors should explain why trajectory feedback does not lead to a worse regret bound and what properties of their algorithm allow them to overcome the information disadvantage of only receiving trajectory feedback.\n4. Section 3 should clarify that how the expected trajectory reward is a linear function of the state-action visitation frequencies.\n5. Some mathematical derivations are not intuitive. The authors can add explanations about what the mathematical properties mean and how they are derived. Here are some examples.\n5.1 The second key observation on page 5.\n5.2 Inequality (3).\n5.3 The equation in (8).\n6. There are a few typos: The third term of the upper bound in Theorem 7. P1 in Line 4 of Algorithm 2. D2 in Line 6 of Algorithm 2."
            },
            "questions": {
                "value": "1. In the upper bound of Theorem 7, the last three terms dominate. In contrast, the abstract and the introduction claim that the upper bound is determined by the first term. The authors should clarify under what conditions, if any, the first term dominates. If the first term is not asymptotically dominant, the authors should explain why they only focus on the first term in the abstract and the introduction.\n2. The introduction claims that the developed algorithm for RL with trajectory feedback achieves the same asymptotically optimal regret bound as the standard RL. The authors should explain why trajectory feedback does not lead to a worse regret bound and what properties of their algorithm allow them to overcome the information disadvantage of only receiving trajectory feedback."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}