{
    "id": "636M0nNbPs",
    "title": "Painting with Words: Elevating Detailed Image Captioning with Benchmark and Alignment Learning",
    "abstract": "Image captioning has long been a pivotal task in visual understanding, with recent advancements in vision-language models (VLMs) significantly enhancing the ability to generate detailed image captions. However, the evaluation of detailed image captioning remains underexplored due to outdated evaluation metrics and coarse annotations. In this paper, we introduce DeCapBench along with a novel metric, DCScore, specifically designed for detailed captioning tasks. DCScore evaluates hallucinations and fine-grained comprehensiveness by deconstructing responses into the smallest self-sufficient units, termed primitive information units, and assessing them individually. Our evaluation shows that DCScore aligns more closely with human judgment than other rule-based or model-based metrics. Concurrently, DeCapBench exhibits a high correlation with VLM arena results on descriptive tasks, surpassing existing benchmarks for vision-language models. Additionally, we present an automatic fine-grained feedback collection method, FeedQuill, for preference optimization based on our advanced metric, demonstrating robust generalization capabilities across auto-generated preference data. Extensive experiments on multiple VLMs demonstrate that our method not only significantly reduces hallucinations but also enhances performance across various benchmarks, achieving superior detail captioning performance while surpassing GPT-4o.",
    "keywords": [
        "Vision-Language Model",
        "Detailed Image Captioning",
        "Caption Metric",
        "Alignment",
        "Preference Optimization",
        "Large Language Models"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We introduce DeCapBench and a novel evaluation metric DCScore for detailed image captioning evaluation. While with a fine-grained feedback collection method enabling the model to achieve superior performance across various tasks.",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=636M0nNbPs",
    "pdf_link": "https://openreview.net/pdf?id=636M0nNbPs",
    "comments": [
        {
            "summary": {
                "value": "Image captioning is an important task that has recently been explored using VLMs to generate detailed captions. Traditional metrics or coarse annotations may not be ideal to evaluate the performance of detailed image captioning. \n\nThis paper proposes evaluation metric for detailed captioning tasks, considering both hallucination and comprehensiveness. A public benchmarks has been proposed. The paper also introduces FEEDQUILL, a scalable method for fine-grained feedback collection by decomposing and verifying responses. Experimental results seem reasonable."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "To address the problem of evaluating the performance of detailed image captioning, this paper proposes a new evaluation metric to take both hallucination and comprehensiveness into consideration. It also constructed an evaluation benchmark using the proposed evaluation metric to the ImageInWords images and their corresponding hyper-detailed image captions. The experimental results seem reasonable."
            },
            "weaknesses": {
                "value": "To generate benchmark for detailed image captioning, 400 images from ImageInWords dataset are used to generate benchmark with the proposed evaluation metric. Only 400 images seems a very small subset. The uniqueness of the proposed benchmarks needs to be further clarified. \n\nThe performance of the proposed method does not always achieve the best results. More explanations and justifications are expected.\n\nThe organisation of the paper can be further improved. It would be good to have a self-contained version rather than leave some important content in appendix. \n\nSome notations are not clearly defined. For example, in 4.1, the definition of the fraction of correct units seems not easy to be understand."
            },
            "questions": {
                "value": "To generate benchmark, why only 400 images are selected? How these images are selected? 400 images seems very small subset. \n\nWhy LLaVA models are used as base model? It will be good if more popularly used models can be investigated to demonstrate the effectiveness of the proposed fine-grained feedback collection.\n\nThe decomposition in section 4.1 seems similar as that in 3.1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a DeCapBench together with a DCScore and FeedQuill preference optimization method to evaluate and improve the ability of detailed image captioning. More specifically, DCScore is implemented in an F1 score style to asses the hallucination and comprehensiveness of the output captions. The introduced detailed captioning benchmark DeCapBench is further conducted to evaluate the captioning capability of VLMs. In addition, this paper also proposes a fine-grained feedback collection method to formulate the reward function for model alignment. The experimental results demonstrate extensive comparisons and results against multiple closed- and open-sourced approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Evaluating detailed image captioning remains a challenging task since current mainstream captioning datasets (e.g., COCO) only contain relatively coarse-grained and short captions. Since most of the metrics for image captioning rely on ground-truth captions, designing a metric to properly assess the quality and hallucination degree of the output captions is crucial, especially in the multimodal LLM era.\n2. The proposed fine-grained feedback as the reward function and the PPO-based alignment framework is reasonable and technically sound.\n3. The quantitative comparisons and experimental analysis are comprehensive, and the performance of the proposed method is promising.\n4. The overall paper is well-structured and easy to follow."
            },
            "weaknesses": {
                "value": "1. As we know, the multimodal LLMs themself has inevitable hallucination issues. Does integrating VLMs into the verification step guarantee that the evaluation results are trustworthy? Or the evaluation results may still be affected by potential hallucinations or input prompts.\n2. While this paper provides results like Table 1 to show the proposed DCScore better aligns with human judgments, it remains unclear whether this behavior can be generalized to other datasets or tasks."
            },
            "questions": {
                "value": "Please refer to the Weaknesses. The following is a minor question.\n\n1. This paper mainly considers LLaVA to be the VLM. Are other commonly used multimodal LLMs, such as VILA or InternVL-2, also applicable to this proposed RL alignment framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new metric DCScore and a new benchmark DECAPBENCH to evaluate the detailed image captioning capabilities of  VLMs. DCScore is designed to measure both the hallucination and comprehensiveness of generated captions. To calculate DCScore, ground-truth and generated captions are first broken down into primitive information units. Then, the primitive information units from the generated captions are compared with those from the ground-truth captions. In addition, GPT-4o is utilized to judge whether each primitive information unit from the generated captions corresponds to the image. Based on these results, a precision score and a recall score are derived, representing non-hallucination and comprehensiveness, respectively. Empirical study shows that DCScore is more aligned with human judgments than previous image captioning evaluation metrics. By combining the proposed DScore and 400 high-quality and detailed image-caption pairs, the benchmark DECAPBENCH is established.\nThe paper also proposes FEEDQUILL, an automatic fine-grained feedback collection method to collect preference data for model training. This method breaks down each response into primitive information units, ensembles multiple VLMs to score these units, and constructs preference data from these scores. Experiments demonstrate that models trained using FEEDQUILL outperform those trained with other preference data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. A novel metric for evaluating detailed image captions and an automatic feedback collection method are proposed. The proposed metric aligns more closely with human judgments and can measure hallucination and comprehensiveness. The proposed feedback collection method is able to construct better preference data without human annotators than previous preference data collection methods.\n2. The experiments show that the proposed FEEDQUILL generalizes much better than other preference data when LLaVA models are used. In addition, the model trained with FEEDQUILL has better performance on various downstream benchmarks, demonstrating its effectiveness in enhancing models' image captioning capabilities.\n3. The paper is well-written and organized."
            },
            "weaknesses": {
                "value": "1. The explanation of the DCScore evaluation process is not entirely clear. Please see the questions below.\n2. When evaluating the effectiveness of FEEDQUILL with various VLMs, only LLaVA-family models are utilized. Why aren't any non-LLaVA-family models (e.g. InternVL-2-8B) used in Table 6?"
            },
            "questions": {
                "value": "About DCScore  \nStep 1 of the evaluation process is unclear to me.\n1. Who are the \u201chuman experts\u201d? What\u2019s the definition of \u201cexperts\u201d in this paper?\n2. Why are the decomposers for generated captions and ground-truth captions different (LLM vs. human experts)? Can LLM be used for both?  \n\nStep 3  \n3. Is the goal of this step to compensate for the missing details in the ground-truth captions?\nWhat\u2019s the difference between $P_{true}$ and $Q$?\n\nAbout DECAPBENCH  \n4. How are the 400 high-quality, human-curated public detailed captions chosen? Is there any criterion for this selection?\n\nAbout FEEDQUILL  \n5. In the related work section, the paper mentioned that using GPT-4v to collect preference data could pose risks of bias and unreliability as the preference judgment of GPT-4v is not manually verified. As FEEDQUILL also leverages multiple VLMs to collect preference pairs, aren't the collected data also likely to be influenced by these models' bias and unreliability?\n\nAbout experiments  \n6. Do other non-LLaVA VLMs, e.g. InternVL-2-8B, trained with the FEEDQUILL-collected preference data also show superior results on downstream tasks?  \n7. How many FEEDQUILL preference data are used for training in the last row of Table 2?\n\nMinor comments  \n8. Line 319: \"..., responses with fewer hallucinations are often inherently less helpful.\" Is this sentence correct?  \n9. Typo in line 334: \"In To fully exploit the characteristics ...\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose:\n1. DCScore: a metric to evaluate both hallucination and comprehensiveness\n\n\n2. DeCapBench: an image captioning benchmark (contains only testing dataset) for hallucination evaluation\n\n\n3. FeedQuill: a method to mitigate hallucination in vision-language models (VLMs), which consists of the following steps:\n  - (1) Collect the responses from VLMs. \n  - (2) Employ LLM to decompose the responses into primitive information units. \n  - (3) Use an off-the-shelf VLMs to verify the correctness of each information units. \n  - (4) Label data as positive or negative based on the verification scores.\n  - (5) Train a reward model with preference dataset constructed in (4).\n  - (6) Fine-tune the target VLM to generate less hallucinated and more enriched captions through PPO.\n\nFinally, several vl benchmarks are achieved as SOTA performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Mitigate the hallucination issues in VLM is crucial, especially in the detailed image captioning task.\n- The proposed metric DCScore sounds reasonable, is provided with a comprehensive comparison with previous metrics (e.g., Faithscore and RLAIF-V), and is demonstrated high consistency with human evaluation.\n- The conducted experiments and related ablation studies are extensive."
            },
            "weaknesses": {
                "value": "[Metric - DCScore]\n1. Why are non-descriptive captions included as a part of this metric? \n2. This metric appears to rely on paid API (i.e., GPT-4o) for its evaluation process. It would be advantageous if the metric could also be adapted to work with open-source VLMs as alternatives to GPT-4o.\n\n[Benchmark - DeCapBench]\n1. The testing dataset in DeCapBench consists of only 400 samples. How does this compare to other visual data hallucination quality sets, such as HallusionBench [1]\n\n[Method - FeedQuill]\n1. In table3, there is a lacked experiment to compare the FeedQuill with the simplest cross-entropy loss (i.e., image caption loss) using the same PPO-finetuned dataset. A comparison of FeedQuill with cross-entropy loss on hallucination-measured datasets, such as mmHal-V and DeCapBench, would be valuable.\n2. In addition to MSCOCO, OpenImages, and ShareGPT4V, what other datasets are included in fine-tuning the VLM with PPO?\n3. Is the preference score $c_r$ a scalar value? If so, why is it necessary to train an additional reward model $R_{\\phi_r}$ to generate the $r_{r_t}$ in Algo 1? Could the $c_r$ be directly used as a part of reward?\n\n\n[1] HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models (CVPR'24)"
            },
            "questions": {
                "value": "Please answer my questions in \u201cweaknesses\u201d section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces a specialized metric (**DCScore**) and a benchmark (**DeCapBench**) for detailed image description evaluation. The core idea is to break down the reference and generated caption into the \"smallest self-sufficient units\", and then quantify the precision and recall of information units conveyed by the generated caption. The authors demonstrate that the new metric and benchmark achieve the best consistency with human evaluations. In addition, based on a similar concept, the authors propose a method (**FeedQuill**) for automatically constructing preference data for RLHF. Extensive experiments validate that the collected preference data can train a strong image captioning model."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper introduces a new metric and benchmark for evaluating the quality of detailed image captions. Correlation analysis with human evaluations indicates that these new assessments are effective and superior.\n2. This paper presents an efficient method for collecting preference data and demonstrates that such data can be used to build more effective image captioning models.\n3. The experiments are comprehensive. The authors\u2019 claims are well-supported by substantial experimental evidence, and they have conducted detailed ablation studies, providing valuable best practices for the community.\n4. The paper is well-written with clear figures and tables, effectively conveying information."
            },
            "weaknesses": {
                "value": "1. Section 3.2 does not introduce the instructions provided to human annotators for scoring image captions. Disclosing the task instruction for annotators is crucial; if the basis for scoring largely aligns with the design of an automated metric, the metric will likely benefit in correlation assessments.\n2. DCScore relies on the hyper-detailed human captions in the ImageInWords dataset. However, as \"a picture is worth a thousand words,\" reference descriptions might not fully reflect all the semantics of an image, while the model may describe image details that are correct but not mentioned in the reference caption.\n3. The proposed metric conceptually resembles prior works like FaithScore and RLAIF-V (I am delighted to see this discussed in Appendix). The divide-and-conquer approach and evaluation using LLMs is not novel. Collecting preference data for model optimization is also a consensus in the research community. While I see no other obvious flaws, **I am not fully convinced of the overall contribution**. I look forward to being further convinced by the authors and other reviewers."
            },
            "questions": {
                "value": "1. The main experiments employ LLaVA-Onevision-7B. Why was this setting not maintained consistently in other experiments? For instance, the ablation study \u201cPreference Data for Reward Model\u201d used LLaVA-1.5-7B, and the \u201cSource of Response\u201d experiments used LLaVA-1.5-13B.\n2. In Appendix A.1.1, the authors claim that DCScore accounts for non-descriptive elements, unlike other metrics. Could the authors further explain why it is important to consider **non-descriptive** elements in **image captioning** tasks, which aim to generate descriptions for images?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}