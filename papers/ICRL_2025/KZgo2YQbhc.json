{
    "id": "KZgo2YQbhc",
    "title": "PaRa: Personalizing Text-to-Image Diffusion via Parameter Rank Reduction",
    "abstract": "Personalizing a large-scale pretrained Text-to-Image (T2I) diffusion model is chal-\nlenging as it typically struggles to make an appropriate trade-off between its training\ndata distribution and the target distribution, i.e., learning a novel concept with only a\nfew target images to achieve personalization (aligning with the personalized target)\nwhile preserving text editability (aligning with diverse text prompts). In this paper,\nwe propose PaRa, an effective and efficient Parameter Rank Reduction approach\nfor T2I model personalization by explicitly controlling the rank of the diffusion\nmodel parameters to restrict its initial diverse generation space into a small and\nwell-balanced target space. Our design is motivated by the fact that taming a T2I\nmodel toward a novel concept such as a specific art style implies a small generation\nspace. To this end, by reducing the rank of model parameters during finetuning, we\ncan effectively constrain the space of the denoising sampling trajectories towards\nthe target. With comprehensive experiments, we show that PaRa achieves great\nadvantages over existing finetuning approaches on single/multi-subject generation\nas well as single-image editing. Notably, compared to the prevailing fine-tuning\ntechnique LoRA, PaRa achieves better parameter efficiency (2\u00d7 fewer learnable\nparameters) and much better target image alignment.",
    "keywords": [
        "Text-to-Image diffusion model",
        "Diffusion model fine-tuning"
    ],
    "primary_area": "generative models",
    "TLDR": "Achieve lightweight and reliable personalized models through the subspace of the pre-trained Stable Diffusion model.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=KZgo2YQbhc",
    "pdf_link": "https://openreview.net/pdf?id=KZgo2YQbhc",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a framework for T2I model personalization called PaRa (Parameter Rank Reduction). It introduces an innovative approach to control the rank of diffusion model parameters, thereby constraining the initially diverse generation space to a smaller, more balanced target space. This enables the generation of personalized concepts and single-image editing. Furthermore, multiple individually fine-tuned PaRa modules can be combined to achieve the fusion of multiple personalized concepts. The framework also demonstrates higher parameter efficiency and better alignment with target images, with experimental results validating its effectiveness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. This paper is well-structured and easy to understand.\n2. The idea of achieving T2I model personalization by controlling the rank of diffusion model parameters is highly innovative. Additionally, detailed explanations are provided for the introduced learnable low-rank parameters.\n3. The experimental section is well-designed with sufficient data, demonstrating the effectiveness of PaRa in single/multi-subject generation and single-image editing, as well as its compatibility with other modules."
            },
            "weaknesses": {
                "value": "1. The methodology section includes extensive explanations of the mathematical principles behind PaRa but lacks an organized overview of the model\u2019s framework. From the subsequent experimental section, it is evident that the approach also utilizes text embeddings, among other elements. While these are not the main focus of the methodology, they should be appropriately explained. Additionally, adding some visualizations in the methodology section would make the concepts more intuitive.\n2. In the last part of the introduction (lines 110\u2013111), it states that \"PaRa achieves state-of-the-art performance in personalized single/multi-subject generation.\" However, the experiments do not provide sufficient comparisons to support this claim. Firstly, there is no comparison with encoder-based personalization methods mentioned in the introduction. Secondly, the fine-tuning-based personalization methods used in the experiments are not the latest approaches in this field, which weakens the persuasiveness of the results.\n3. In section 3.1, it is mentioned that \u201cB is initialized to zero and fine-tuned with a few text-image pairs.\u201d The experimental section should clarify the scale of the data used, the fine-tuning time, and other relevant information. Additionally, to support the claim of higher parameter efficiency, a computational efficiency analysis should be provided.\n4. From the image editing results, it appears that introducing PaRa may result in some loss of texture information. The analysis and interpretation of the experimental results are insufficient."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "A diffusion based image personalization method is presented via a parameter rank reduction technique. Particularly, the proposed method controls the rank of the diffusion model parameters to restrict the generation space into a small and well-balanced target space, achieving trade-off between the training data distribution and the target distribution. The motivation of the method lies in two main aspects. Firstly, this paper assumes that taming a T2I model toward a novel concept implies a small generation space. Secondly, the rank of matrix is important for parameter efficient fine-tuning. In this case, by reducing the rank of model parameters during finetuning, the proposed method is proven to achieve advantages over existing finetuning approaches on single/multi-subject generation and single-image editing."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Solving personalisation from matrix decomposition is a reasonable and good idea.\n2. Combing rank with edibility, especially diversity of the generated image is proven effective in this paper.\n3. The experiments are convincing in explaining the superiority of the proposed solution."
            },
            "weaknesses": {
                "value": "1. Matrix decomposition is proven to be effective in parameter efficient fine-tuning tasks, e.g. image editing. Although the difference between the proposed solution to Lora is clear. As both methods are based on matrix decomposition, it's not clear what are the foundational differences, and how the authors come up with the current solution.\n2. SSIM is used in this paper for image diversity evaluation. I'm not quite sure whether SSIM is the best one, as SSIM focuses on pixel-level difference instead of semantic level difference. e.g. position difference of the same instance may contribute more to SSIM than than an extra accessory. \n3. Writing should be improved to highlight the contributions, especially how the solution is formed, and how it is different from the existing techniques."
            },
            "questions": {
                "value": "1. One important motivation of the paper is that taming a T2I model toward a novel concept implies a small generation space, making it possible to perform one-shot training for the low-rank parameter B. I'm curious about the relationship between edibility and the optimal rank of B. Is it always possible to learn a good low-rank B to perform reasonable editing in one-shot manner? It's just an open question. Please share your experiences.\n3. As a one-shot training techniques for image editing, how the model perform with different one-shot pairs? Please explain robustness of the model.\n4. Both numbers and visualisation are good in explaining the superiority of Para compared with existing techniques. Then, what are the limitations of Para? Please provide a rough failure case analysis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work aims to balance personalization and text editability by reducing the rank of model parameters, effectively narrowing the generation space to better align with target concepts. This method is more parameter-efficient and achieves better target image alignment than existing techniques like LoRA. It also supports combining multiple personalized models and facilitates stable single-image editing without additional noise inversion processes. The paper shows PaRa\u2019s effectiveness through comprehensive experiments on single and multi-subject generation tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed Parameter Rank Reduction (PaRa) method is a creative solution to the challenge of T2I model personalization, offering a new perspective on controlling the generation space.\n2. PaRa demonstrates significant parameter efficiency, requiring 2\u00d7 fewer learnable parameters compared to existing methods like LoRA.\n3. The paper provides extensive experimental results showing PaRa\u2019s advantages in single/multi-subject generation and single-image editing and shows better results than LoRA.\n4. The paper is very well written and presented."
            },
            "weaknesses": {
                "value": "1. The authors have not compared their method with SOTA such as LyCoris, DiffuseKronA, etc. Including these results would provide a better comparison of the method."
            },
            "questions": {
                "value": "The only concern is insufficient comparisons to SOTA, adding which would allow a better comparison."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new method to personalizing diffusion model with few samples, which is to reduce the rank of the original weights in diffusion models. This method allows the finetuned model to have smaller diversity and better alignment with the training data. This paper conducts extensive experiments on various generation task including single subject, multi-subject generation and editing. The paper is well written and easy to understand."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1) This paper provides a very interesting ideas to personalize diffusion model by reducing the rank of the weights. \n2) The proposed finetuning method is simple, effective and show good flexibility."
            },
            "weaknesses": {
                "value": "1) Different hyper-parameters may affect the alignment of the previous methods, such as the Rank of LoRA and fine-tuning steps. Authors may need to provide more clarification on how these hyper-parameters are chosen for the compared methods to make sure they achieve good enough alignment in the experiments."
            },
            "questions": {
                "value": "Please refer to the concerns in the Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}