{
    "id": "zcTLpIfj9u",
    "title": "Future-Guided Pretraining via Time-to-Event Supervision for 3D Medical Imaging",
    "abstract": "In medicine, making effective treatment decisions requires detecting early warning signs of disease. With the rise of 3D medical foundation models, there is promise in large-scale pretraining to capture new and more informative imaging biomarkers associated with future disease risk. Current self-supervised learning (SSL) techniques for 3D medical imaging largely capture structural properties via reconstruction and contrastive losses \u2013 local features that provide only indirect signal on disease progression. Electronic health records (EHRs) present an underutilized resource for future information, offering an easily paired and scalable amount of weak supervision representative of patient outcomes. To this end, we propose future-guided pretraining to explore the benefits of training 3D image encoders on future medical events. By combining classic techniques from timeto- event modeling and recent pretraining techniques using longitudinal event data from EHRs, we show that future-guided pretraining enhances the ability to predict future patient outcomes (average AUROC increase of 25.3% and time-dependent c-statistics increase of 23% compared to baseline models) without degrading the ability to perform standard binary classification tasks (e.g. image labeling for diagnostic tasks). This study lays the groundwork for innovative ways to combine EHR and imaging modalities for clinical risk prediction.",
    "keywords": [
        "Multimodal learning",
        "medical imaging",
        "Electronic Health Records"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zcTLpIfj9u",
    "pdf_link": "https://openreview.net/pdf?id=zcTLpIfj9u",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a future-guided pretraining approach using time-to-event supervision to enhance the prognostic capabilities of 3D medical imaging models. By incorporating longitudinal EHR data into the pretraining process and predicting time-until-event, the model outperforms traditional methods across multiple standard tasks, as demonstrated by thorough experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Innovative Approach: The method creatively leverages EHR data following a medical scan to assist model pretraining, demonstrating better performance compared to imaging-only pretraining.\n2. Comprehensive Evaluation: Extensive comparisons across multiple tasks validate the robustness and efficiency of the TTE-based approach across different architectures."
            },
            "weaknesses": {
                "value": "1. Dependence on Large EHR Datasets: This approach relies on extensive, high-quality EHR data, which many medical datasets do not include.\n2. Limited Modality Scope: Tested only on CT images; broader modality testing could validate versatility across imaging types.\n3. Interpretability: The TTE pretraining\u2019s impact on specific pixel-level biomarkers is less clear; additional analysis on feature attribution could help."
            },
            "questions": {
                "value": "1. Why start from 3D image scans instead of 2D medical images? Is this due to the dataset choice, or has similar work already been done on 2D data?\n2. How does the choice of time segmentation for EHR data affect model results during pretraining? Specifically, my understanding is that the model predicts the probability of a patient experiencing a certain event at intervals like 1, 2, or 3 years post-scan. How does the granularity of these time segments impact the performance of the pretrained encoder?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors proposed to utilize the time-to-event information in EHR that paired with the imaging data as a form of supervision for the pre-training purpose. A public dataset with both 3D images and EHR notes is employed for the pre-training and downstream applications. Another dataset without the time events is also used for the evaluation of model adaptation. The manuscript is easy to follow. However, it also suffers from several critical flaws, which are detailed below."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Propose utilizing the time events as pre-training tasks specially designed for prognosis tasks in downstream applications. \n- The manuscript is overall easy to follow"
            },
            "weaknesses": {
                "value": "- The proposed method is limited in generalization since it will require longitudinal time-to-event EHR data as the supervision for the pre-training. In comparison to the common self-supervised pre-training, the proposed methods are harder to scale up.\n\n- There is no comparison evaluation between the proposed method and prior methods in model pre-training. Only the results of the proposed method with different model architectures are reported. It will be difficult to appreciate the benefits of the proposed method.\n\n- The selected model architecture also raises questions since there are many popular model networks in medical imaging, e.g., 3D-UNet, ViT, etc. It will be helpful to see their performance compared to the vanilla ResNet. \n\n- Baselines without the pre-training process should also be reported.\n\n- The current setting utilizes public data for both pre-training and downstream applications. Having a separate evaluation dataset of a prognosis task will be helpful. \n\n- The proposed method is limited in technical innovation, though utilizing the time-to-event data as a form of supervision is relatively new in the pre-training. Mostly existing techniques are adopted for the pre-training."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a self-supervised learning (SSL) method for 3D medical imaging data that leverages electronic health records (EHR) to provide extra sources of supervision via time-to-event modeling. The proposed method, future-guided pretraining, performs time-to-event (TTE) survival modeling of various medical events in the longitudinal EHR associated with each 3D scan. The authors show that future-guided pretraining consistently improves downstream TTE modeling and prognostic classification tasks \u2013 also improving data efficiency \u2013 without degrading standard diagnostic classification performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The presentation quality is very high. Care has been taken to logically organize the paper, clearly articulate key points, and straightforwardly present results with concise figures and tables.\n- The core idea is creative, making use of the wealth of longitudinal EHR data associated with each 3D volume for pretraining.\n- Discussion or related work and background is particularly strong.\n- Experiments are sufficiently thorough and easy to interpret \u2013 results are convincing."
            },
            "weaknesses": {
                "value": "- The actual description of the TTE pretraining approach is brief (lines 184-191) and somewhat unclear. I would advise the authors to flesh out this section. See specific questions below.\n- A description or list of the 8,192 EHR pretraining tasks is never provided. I\u2019m aware there may not be a convenient place to list this many items, but a general description of categories of events or a few illustrative examples would be helpful. Without this information, it\u2019s impossible to assess whether, e.g., one the TTE pretraining tasks is *also* used as a downstream TTE modeling task. In this case, there may be concerns of \u201clabel leakage\u201d.\n\nI\u2019m happy to increase my score once these issues are addressed \u2013 this is an otherwise strong submission."
            },
            "questions": {
                "value": "- What exactly does it mean that Steinberg et al.\u2019s method was used to \u201c[sample tasks to maximize entropy given the frequency distribution of medical codes populating the DAG\u201d? I feel that a basic plain-language description of the motivation for this procedure is needed first: why is this method being applied at all? Are there way more than 8k events and the goal is to settle on a subset of 8k \u201cmeaningful\u201d/common ones for pretraining? I don\u2019t understand the motivation.\n- Unless I am misunderstanding, this is the only description of the TTE pretraining procedure and labels used: \u201cWe define our TTE task labels by predicting the time until the next occurrence of a medical code.\u201d The previous Section 3 described deep survival modeling in the abstract, so I expected Section 4 to more concretely describe how TTE pretraining works. Is this a \u201ccompeting risks\u201d approach, where multiple events are being modeled simultaneously (in \u201cmulti-label\u201d fashion)?\n- What are the 8,192 EHR tasks/events? I\u2019m aware it would be cumbersome or impossible to list and define them all, but any reasonable attempt to convey information about them would be useful. What kinds of \u201cevents\u201d are they? What are some examples?\n- Related to the above point, are the downstream labels *also* present in the set of TTE pretraining tasks? If so, isn\u2019t there concern of \u201clabel leakage\u201d, where the model has been pretrained on label information present in the downstream training dataset? Please clarify this.\n\n**Minor comments/questions:**\n- Line 13: Maybe \u201cbuild\u201d instead of \u201ccapture\u201d since you use this word in the next sentence.\n- In-text citation style seems off \u2013 should be parenthetical (\\pcite{}) in most cases when used at end of sentence/clause: \u201cSox et al. (2024)\u201d -> \u201c(Sox et al., 2024)\u201d\n- Change \u201ce.g.\u201d -> \u201ce.g.,\u201d throughout\n- Would include more recent references [1,2] when discussing deep prognosis models on longitudinal medical imaging (first paragraph of Section 2)\n- \u201ci.e. 8192\u201d -> \u201ci.e., 8.192\u201d\n- \u201cOur approach improves training data efficiency, increasing training labels by an average of 3x over labels assigned to patients based on their current EHR visit.\u201d This is a bit unusual to highlight as a main contribution \u2013 I don\u2019t think readers will understand what \u201cincreasing training labels\u201d means without having read the entire paper (nor why this impact data efficiency). Perhaps clarify language here to indicate that your approach provides 3x as many sources of supervision during SSL + that this is what provides data efficiency benefits.\n- \u201cPretraining task labels as assigned per-CT scan and vary in density based on pretraining approach, see Figure 2.\u201d Perhaps \u201cas assigned\u201d is meant to be \u201care assigned\u201d? Also change \u201c, see Figure 2\u201d -> \u201c(Figure 2)\u201d.\n- Be consistent with \u201cc-statistic\u201d vs. \u201cC-statistic\u201d\n\n**References**\n[1] Holste, Gregory, et al. \"Harnessing the power of longitudinal medical imaging for eye disease prognosis using Transformer-based sequence modeling.\" NPJ Digital Medicine 7.1 (2024): 216.\n[2] Sriram, Anuroop, et al. \"Covid-19 prognosis via self-supervised representation learning and multi-image prediction.\" arXiv preprint arXiv:2101.04909 (2021)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}