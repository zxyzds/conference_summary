{
    "id": "OW332Wh9S5",
    "title": "DC-Spin: A Speaker-invariant Speech Tokenizer For Spoken Language Models",
    "abstract": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
    "keywords": [
        "speech tokenizer",
        "self-supervised learning",
        "spoken language model",
        "speech language model",
        "speech resynthesis",
        "audio codec"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "DC-Spin extracts speaker-invariant tokens to improve spoken language models and speech resynthesis, and analyses offer insights for designing better speech tokenizers.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=OW332Wh9S5",
    "pdf_link": "https://openreview.net/pdf?id=OW332Wh9S5",
    "comments": [
        {
            "title": {
                "value": "Response to Reviewer wRzi"
            },
            "comment": {
                "value": "We sincerely appreciate the reviewer's detailed feedback and the opportunity to address the concerns raised.\n\n**Response to Weakness 1**  \nIn Table 2, the performance gains of our method are notable relative to prior work. The speech tokenizers proposed in NAST and Gat et al. are more complex, often requiring extensive hyperparameter tuning, while we found Spin and DC-Spin to be more stable, with minimal tuning required.\n\nIn Table 3, our approach demonstrates comparable or better results than high-resource speech LMs, even with significantly less data and compute, suggesting that our tokenizer effectively lowers the training requirements.\n\n**Response to Weakness 2**  \nWe acknowledge the limitation of not including SLM-based TTS evaluations. While time constraints prevent us from conducting this experiment, we reference another paper on self-supervised speech encoder-based tokenizers [1], where consistent results between speech resynthesis and TTS suggest that our speech tokenizer would likely show similar performance in TTS.\n\n**Response to Weakness 3**  \nWhile we understand UTMOS is only a proxy for MOS, policies at some authors' institutions make human MOS evaluations difficult within the rebuttal period.\n\n**Response to Weakness 4**  \nWe conducted some initial scaling experiments, shown in the table below, which indicate slight improvements with larger models. (tokenizer: HuBERT Base + DC-Spin$_{200,4096}$)\n\n| SLM Size | tSC   | sWUGGY | sBLIMP |\n| -------- | ----- | ------ | ------ |\n| 150M     | 69.21 | 80.59  | 62.13  |\n| 300M     | 69.32 | 81.91  | 62.33  |\n\nHowever, larger models tend to overfit, particularly given our smaller dataset compared to prior works, which typically use 10X more data. We would like to conduct further scaling experiments, though these may not be feasible within the rebuttal period.\n\n\n**Response to Weakness 5**  \nOur DC-Spin training objective aligns with Spin, whose speaker invariance has been demonstrated in [2,3]. We rely on this property in our approach to achieving a speaker-invariant tokenizer.\n\n\n**Response to Question 1**  \nFollowing Expresso [4], we use an ASR based on wav2vec 2.0 Large pre-trained on Libri-Light 60k hours dataset and fine-tuned with LibriSpeech 960 hours corpus (Section 4.3). The high ASR-WER is due to out-of-domain speaking styles in the Expresso dataset, such as laughter and whispering.\n\n**Response to Question 2**  \nThe proxy tasks in Section 4.7 (bitrate, n-gram predictability, ABX, PNMI, and CNMI) do not need additional SLM training, mitigating the need for 12 hours of training on 8 GPUs. These proxies can be computed within an hour, allowing for faster development and evaluation.\n\n**Response to Question 3**  \nAs explained in Section 3.3 and Table 1, HuBERT offers a relatively better initialization for DC-Spin and has stable training compared with EMA-based data2vec. We suspect that the HuBERT training objective is more similar to Spin and DC-Spin, contributing to the success of these fine-tuning methods, i.e., predicting discrete tokens. The only difference between the Spin objective and HuBERT is that Spin's prediction targets are learned during training, while HuBERT has fixed targets.\n\n**Response to Question 4**  \nIn [3], the authors have shown that fine-tuning more layers leads to model collapse. We also observed a similar pattern in our experiments. This phenomenon is caused by a trivial solution to the Spin objective that can be easily achieved. The trivial solution is to ignore all input speech information and possibly encode only the relative position of each frame. Hence, freezing some layers of a pre-trained model enforces the encoder to preserve the original information of speech.\n\n**Response to Question 5**  \nThe nature of each downstream task has led us to choose the final DC-Spin codebook size carefully. Here are some properties in the tokenizer required for each task.\n1. **tSC** and **sBLIMP** benefit from smaller codebooks for coarse-grained semantic and syntactic information.\n2. **sWUGGY** and **Resynthesis** require finer phonetic and acoustic details, favoring a larger codebook.\n\nAccording to Figure 1, we chose 500 to obtain the best overall performance across tasks. Note that this choice is based on the downstream tasks and the tokenizer's operating framerate (50Hz), a different setup requires a different codebook size.\n\n---\n\n[1] Shi, Jiatong, et al. \"MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model.\" arXiv (2024).  \n[2] Chang, Heng-Jui, Alexander H. Liu, and James Glass. \"Self-supervised fine-tuning for improved content representations by speaker-invariant clustering.\" arXiv (2023).  \n[3] Chang, Heng-Jui, and James Glass. \"R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces.\" arXiv (2023).  \n[4] Nguyen, Tu Anh, et al. \"Expresso: A benchmark and analysis of discrete expressive speech resynthesis.\" arXiv (2023)."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer SENG"
            },
            "comment": {
                "value": "We sincerely appreciate the reviewer's detailed feedback and the opportunity to address the concerns raised. We believe, however, that certain assumptions may not fully capture the recent advances and diversity in spoken language model (SLM) research, as a range of approaches for integrating speech into LMs are actively being explored.\n\n---\n\n**(I) Response to the Reviewer's Comments on Speech Tokenization**  \nThe reviewer suggests that connecting a speech encoder to an LM should involve continuous speech representation, not discrete tokens. While continuous representations are indeed common, recent work has shown that discrete token-based methods are both valuable and effective. For example:\n1. *Spirit LM* [1]: Uses K-means clustered units in training a speech and text interleaved LLM.\n2. *Moshi* [2]: Proposes Mimi, a neural audio codec that transforms speech into discrete tokens for their full-duplex spoken dialogue framework's input and output units.\n3. *hertz-dev* [3]: Similar to Moshi, also uses a neural audio codec.\n\nMoreover, a recent study comparing discrete and continuous representations for SLM-based ASR found both approaches to offer comparable performance [4]. Additionally, one of the reviewer's own references (https://arxiv.org/abs/2309.00169) supports the use of discrete tokens in ASR, reinforcing our approach. We hope this clarifies the value of discrete tokens in SLM.\n\n---\n\n**(II) Response to the Reviewer's Comments on Speech Generation**  \nThe reviewer notes that modern TTS focuses on high naturalness and voice cloning, expressing concern that removing speaker information could reduce effectiveness. However, the referenced papers also use discrete tokens as TTS output units, which seems to contradict the concern raised. Furthermore, in Tables 4 and 19 of our paper, we show that our speech tokens outperform neural audio codecs under similar bitrates.\n\nThe ASR-WER in our work is higher due to two main factors:\n1. We used expressive speech data, such as laughing and whispering, which is not standard in ASR training and naturally yields higher WER.\n2. The vocoder we used was not carefully optimized, as our focus was on demonstrating the tokenizer's effectiveness. Future research could improve resynthesis quality using enhanced vocoder architectures (e.g., a diffusion transformer).\n\nAdditionally, regarding voice cloning, the goal of removing speaker information is to enhance content representation; speaker details can be reintroduced in the vocoder stage, as outlined in Section 4.1.\n\n---\n\n**Response to Question 1**  \n> How do you handle speaking style, which is also a kind of speaker-variant?\n\nOur approach removes speaker identity from speech tokens to encourage the tokenizer to encode better content information. We did not consider perturbing speaking styles like loudness and emotional speech. Hence, we leave this part for future studies because these kinds of perturbations require a more complex data augmentation / generation process. E.g., controllable voice conversion or TTS.\n\n\n**Response to Question 2**  \n> In the most right hand side of Figure 2, it is not clear to me how the codebook is formed and how the discrete speech tokens are generated? Is K-means clustering still be used?\n\nAs described in Section 3.2, we first explained that we directly use the tokens from a Spin codebook without K-means. Next, we described that DC-Spin has two Spin codebooks and share the same encoder. We can add more detailed illustration in the appendix to clarify how DC-Spin works.\n\n---\n\n**We believe the weaknesses mentioned do not constitute grounds for rejecting the paper. While we respect the practical perspective of the reviewer, we encourage the reviewer and Area Chair to consider the academic contribution and future impact of our work.**\n\n---\n\n[1] Nguyen, Tu Anh, et al. \"Spirit-lm: Interleaved spoken and written language model.\" arXiv preprint arXiv:2402.05755 (2024).  \n[2] D\u00e9fossez, Alexandre, et al. \"Moshi: a speech-text foundation model for real-time dialogue.\" arXiv preprint arXiv:2410.00037 (2024).  \n[3] https://si.inc/hertz-dev/  \n[4] Xu, Yaoxun, et al. \"Comparing Discrete and Continuous Space LLMs for Speech Recognition.\" arXiv preprint arXiv:2409.00800 (2024).  \n[5] Shi, Jiatong, et al. \"MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model.\" arXiv preprint arXiv:2406.09869 (2024)."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer eraa"
            },
            "comment": {
                "value": "We sincerely appreciate the reviewer's thorough feedback and are grateful for the opportunity to clarify and address these points.\n\n\n**Response to Question 1**  \nWe observed the auxiliary codebook in DC-Spin behaves similarly to the single large codebook used in the original Spin model, with comparable quality in SLM and codebook metrics. However, the primary codebook's performance has improved in DC-Spin (Table 14). Additionally, we have conducted an ablation study with varying auxiliary codebook sizes, as detailed in the revised paper's Appendix B.1 (Figure 4). These results underscore the auxiliary codebook's effectiveness.\n\n\n**Response to Question 2**  \nWe follow the robustness evaluation pipeline in [1] (code at https://github.com/ShovalMessica/NAST), and we followed their official setup as described in Section 3 of [1]:\n> (i) time-stretch using a Phase Vocoder method to stretch or shrink the time domain signal in the range $[0.8, 1.2]$ without changing the pitch; (ii) pitch-shifting the speech signal by four semitones using the resampling method over the time-stretched signal; (iii) reverberation following similar setting as simulated via the pyroomacoustics audio room simulations package; and (iv) noise injection using a randomly sampled Signal-to-Noise Ratio (SNR) in the range of $[5, 15]$. Background noises are sampled from the Deep Noise Suppression (DNS) challenge which includes a diverse set of noise types from AudioSet, Freesound, and Demand.\n\n\n**Response to Question 3 and the Weakness**  \nTo explore DC-Spin's potential at higher bitrates, we experimented with residual vector quantization (RVQ) to expand bandwidth and incorporated Mel spectrogram reconstruction training (Appendix B.3). We applied the Spin loss to the primary codebook to enhance speaker-invariant content, while additional codebooks were designated for capturing fine-grained acoustic details. Despite these modifications, zero-shot SLM performance did not improve significantly, though we believe further studies could yield higher resynthesis quality. A possible direction for future work is outlined in [2].\n\n---\n\n[1] Messica, Shoval, and Yossi Adi. \"NAST: Noise Aware Speech Tokenization for Speech Language Models.\" arXiv preprint arXiv:2406.11037 (2024).  \n[2] Shi, Jiatong, et al. \"MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model.\" arXiv preprint arXiv:2406.09869 (2024)."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer q3wD"
            },
            "comment": {
                "value": "We sincerely appreciate the reviewer's detailed feedback and valuable insights, which have helped us clarify key aspects of our work.\n\n\n**Response to Weakness 1**  \nIn Table 2, the improvements of DC-Spin over Spin become more pronounced with smaller primary codebook sizes (e.g., units = 50 or 100). While the improvements are less evident at 500 units, they are still present. Additionally, the chunk-wise streaming approach illustrates the ease with which our tokenizer adapts to streaming applications. For this reason, we provided the detailed explanations in the appendix. We will revise the contribution section in the introduction to emphasize this more clearly.\n\n\n**Response to Weakness 2**  \nAlthough the baseline methods we used, EnCodec and SpeechTokenizer, are relatively recent (2022 and 2023) and remain widely used, we acknowledge that there are newer alternatives. We can possibly try to experiment with DAC, but due to the rebuttal time frame, we might not be able to report the results. Additionally, WavTokenizer is considered concurrent work since it was first posted to ArXiv in late August this year (https://iclr.cc/Conferences/2025/FAQ).\n\n\n**Response to Weakness 3**  \nAs noted in Section 3.2, larger Spin codebooks have been shown by Chang et al. (2023) to better capture phonetic representations. Since both the primary and auxiliary codebooks share the same encoder, we expect the auxiliary codebook to indirectly assist the primary codebook in encoding high-quality phonetic units. We have clarified this mechanism in the paper. For example: \"Moreover, Chang et al. (2023) found larger Spin codebooks capture better phonetic representations *in the encoder*.\" Although we found that DC-Spin helps capturing phonetic units, there is no evidence about ignoring acoustic details. Besides, in Tables 4 and 19, we show that DC-Spin consistently outperform K-means in speech resynthesis.\n\n\n**Response to Question 1**  \nThe auxiliary codebook is used only during training and is not involved in token extraction. When tokenizing speech with DC-Spin, we rely solely on the primary codebook, resulting in a bitrate comparable to that of K-means$_{500}$."
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel approach to speech tokenization for spoken language models (SLMs), focusing on speaker invariance and phonetic richness through the Double-Codebook Speaker-invariant Clustering (DC-Spin) technique. DC-Spin utilizes a dual codebook design to separate phonetic and speaker-invariant features, enhancing SLM performance in tasks like automatic speech recognition (ASR) and speech synthesis. Additionally, the paper proposes SpinHuBERT, a pre-trained model that leverages DC-Spin for robust and accurate tokenization. The contributions of DC-Spin are evaluated against existing tokenizers, demonstrating improved robustness, phonetic alignment, and real-time processing efficiency, setting a new standard in resource-constrained and scalable speech tokenization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Introduced the Double-Codebook Spin to capture the fine-grained phonetic units better, and gave the detail process about how to select the codebook size.\n2. The experimental setup is thorough, covering both zero-shot and supervised tasks, with a clear evaluation on robustness and inference efficiency. \n3. This work analyzed multiple proxy tasks on speech tokenizers to reveal their relationship with the performance of the spoken language model."
            },
            "weaknesses": {
                "value": "1. Although this work states three contributions, they were incremental somehow. For example, the DC-Spin doesn't show convincing advantages against Spin method in Table 2, especially considering the much larger codebook size of DC-Spin. As for the chunk-wise streaming simulation, it has been used in many similar works such as ASR and TTS. And the authors claimed it in the contribution part but they just mentioned it in a short paragraph in the experiment section.\n2. The compared neural audio codec methods were a little bit dated. Encodec and SpeechTokenizer methods were proposed in 2022 and 2023. Many new and powerful works have been proposed since then. For example, descript audio codec (DAC) and hifi-codec should be included in the comparison. Besides, WavTokenizer is another work focusing on low bitrate, making it suitable for comparison.\n3. The motivation of designing the DC-Spin is not clear. The authors claimed the auxiliary codebook can capture fine-grained phonetic units but didn't provide explanations. Moreover, it both primary and auxiliary codebooks pay attention to the phonetic units, the acoustic detail would be ignored in DC-Spin to some extent, which is harmful for the generative task."
            },
            "questions": {
                "value": "1. I am a little bit confused about the bitrate of Kmeans and DC-Spin reported in Table 4 and Table 19. For example, DC-Spin_{500,4096} has the primary codebook with 500 size and the auxiliary codebook with 4096 size. But the final bitrate is the same as the Kmeans_{500} which only has a 500 size codebook. Could you explain it in detail?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents the development of robust and efficient speech tokenizers for spoken language modeling and speech resynthesis. The authors introduce two innovative methods, SpinHuBERT and DC-Spin, designed to meet four essential criteria for ideal tokenizers: capturing phonetic information, preserving acoustic details, robustness to perturbations, and enabling efficient inference. Experimental results demonstrate that a 150M parameter Transformer decoder, trained on downstream tasks, performs optimally with DC-Spin speech tokens. Furthermore, DC-Spin delivers superior reconstruction quality at extremely low bitrates compared to popular codecs. The authors also identify proxy metrics \u2014 shown to correlate highly with downstream task performance \u2014 and advocate for these metrics as a basis for evaluating future speech tokenizers."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* **Originality:** The paper\u2019s contribution of SpinHuBERT and DC-Spin tokenizers demonstrates originality.\n* **Quality:** The evaluation is comprehensive, evaluating the proposed tokenizers across multiple downstream tasks and metrics to show their effectiveness.\n* **Clarity:** The presentation is clear.\n* **Significance:**  Disentangling factors within discrete speech tokens, as explored in DC-Spin, addresses a growing area of interest (e.g., FACodec). DC-Spin seems to stand out as one of the first speech tokenizers to separate speaker information from codebooks."
            },
            "weaknesses": {
                "value": "**Comparison in Speech Resynthesis:** While the results show that DC-Spin performs well at low bitrates (<1.5 kbps), it is worth noting that current state-of-the-art speech synthesis systems (e.g., NaturalSpeech2, VoiceCraft, MaskGCT) use codecs with higher bitrates to achieve high-quality speech reconstruction. A comparison of DC-Spin against existing tokenizers at these higher bitrates would provide valuable insight into its performance for speech resynthesis under more typical conditions."
            },
            "questions": {
                "value": "1. **Claim:** \u201cThe first codebook (primary) extracts discrete units for downstream applications. The second codebook (auxiliary) is a large codebook that enhances the encoder\u2019s capability to capture fine-grained phonetic units.\u201d Might there be an ablation study on the independent use of each codebook to support this finding? (i) Evaluating downstream task performance using only the primary codebook vs. only the auxiliary codebook, (ii) analyzing the phonetic content captured by each codebook independently, and (iii) examining how varying the size of each codebook impacts the overall system performance are useful experiments to help validate the claim on the role of codebooks.\n2. **Robustness Testing:** Could the authors provide further details on the types and degrees of perturbations applied in testing the robustness of DC-Spin? Specifically, can you provide quantitative details on the noise levels, time stretch factors, reverberation parameters, and pitch shift ranges, and comment on how well they compare to real-world conditions?\n3. Can DC-Spin can be adapted to operate at higher bitrates comparable to systems like NaturalSpeech2 or VoiceCraft? If so, how does its performance compares in terms of speech quality metrics (e.g. MUSHRA scores) at those higher bitrates?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Double-Codebook Speaker-invariant Clustering (DC-Spin) as a method to enhance speech tokenization by making the speech tokens to be speaker-invariant. \nBy decreasing the proportion of the speaker information in the speech tokens, DC-Spin aims to improve zero-shot SLM tasks and speech resynthesis. Through comparisons of tokenization techniques and model scalability, the study highlights the effectiveness of tokens easily modeled by n-gram language models or aligned with phonemes, offering valuable guidelines for designing speech tokenizers for SLMs."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This paper proposes a method called DC-Spin to improve speech tokenization.\nThe speech tokens generated by the proposed method is speaker-invariant and can preserve more content information compared to open-source tokenizers."
            },
            "weaknesses": {
                "value": "The motivation of this paper does not sound convincing.\nA common way of connecting a speech encoder to a LM is directly through speech representation, which is of continuous values. \nThe discretization from representation to discrete tokens will only degrade the quality of the speech input. Check out the papers below:\nhttps://arxiv.org/pdf/2402.08846\nhttps://arxiv.org/abs/2309.00169\nhttps://www.isca-archive.org/interspeech_2024/yang24f_interspeech.pdf\nThe discretized tokens will eventually be converted to continuous vectors through the embedding table of the LM. \nTherefore, there is no need to discretized the continuous embedding for the input of LM.\n\nFrom the LM output perspective, yes, there is a need for speech tokenization if we want the LM to generate speech.\nHowever, the TTS community has already gone far beyond word accuracy. We are talking about voice clone, high naturalness.\nThus, removing the speaker information may even hinder these speech tokens to be used.\nEven if we neglect the voice clone issue, the ASR-WER of the speech resynthesis is far below standard (18.x in the paper). That means the system can't be used. Checkout the numbers in the following papers:\nhttps://arxiv.org/abs/2309.00169\nhttps://arxiv.org/abs/2406.02430\nhttps://arxiv.org/abs/2301.02111"
            },
            "questions": {
                "value": "The Spin mentioned in the paper perturb the speech utterance by randomly scaling the F0 and formant frequencies.\nAs I know, speaker variation is more than different F0 and formants. How do you handle speaking style, which is also a kind of speaker-variant?\n\nIn the most right hand side of Figure 2, it is not clear to me how the codebook is formed and how the discrete speech tokens are generated? Is K-means clustering still be used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a new method for speech tokenization named SpinHuBERT, this tokenization is based on pre-training a HuBert speech encoder with the spin objective, then with an ASR target, and finally fine-tuning with a DC-Spin objective. The evaluation shows the high performance of the method across a wide range of tasks, such as spoken language modeling, speech resynthesis, robustness under distortions and inference efficiency. Furthermore, the authors evaluate different proxy tasks for spoken language modeling which identifies decent candidates."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed method is novel, and since speech tokanization is widely used improvement are significant. \n2. The paper is clearly written which enables to easily re-implement and use the proposed method. \n3. The proposed method shows a strong performance over the other baseline methods."
            },
            "weaknesses": {
                "value": "1. Proposed method out-performs the baseline on most tasks, but the margins are small while the proposed method is significantly more complex.\n2. Evaluation on directly synthesizing speech is not performed. It is unclear if in a real setting the proposed method is able to generate coherent and intelligible speech. This evaluation should be added to show the methods performance in the task SpeechLMs are used for.\n3. Throughout the paper, UTMOS is used instead of real human raters. While UTMOS is a good proxy, it does not fully replace human raters. At least some of the evaluations should be done using MOS with human raters. \n4. Table 3 shows evaluation of other method with much larger models. While a 150m parameter model is decent, SpeechLMs are regularly scaled to larger sizes. The paper is lacking an evaluation of a full-sized SpeechLM model. Such a model would give the reader a better option to appreciate the performance of the proposed method.\n5. While the tokens are claimed to be speaker invariant, this is not evaluated in the experimental setup. I would expect the authors to include an experiment which verifies this claim. \n6."
            },
            "questions": {
                "value": "1. In table 8 and 4 ASR-WER is very high. What ASR model was used? What is the reason for this?\n2.  Why is using the proxy tasks better than directly using TSC/ sWUGGY / sBLIMP. They don't seem easier to compute and TSC/ sWUGGY / sBLIMP are already proxies for the ability to generate intelligible audio. \n3. Why use HuBert as the base encoder and not some other encoder? what are the properties of HuBert that make it best for this task?\n4. Why fine-tune and not train from scratch with the SPIN objective?\n5. In figure 1, we see that different tasks have the best performance with different code-book sizes. For example the 1000 size codebook has the best performance on TSC (but is not used later in the paper). How should the final model be picked?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}