{
    "id": "ttq44QjKda",
    "title": "Bounds on $L_p$ Errors in Density Ratio Estimation via $f$-Divergence Loss Functions",
    "abstract": "Density ratio estimation (DRE) is a fundamental machine learning technique for identifying relationships between two probability distributions. $f$-divergence loss functions, derived from variational representations of $f$-divergence, are commonly employed in DRE to achieve state-of-the-art results. This study presents a novel perspective on DRE using $f$-divergence loss functions by deriving the upper and lower bounds on $L_p$ errors. These bounds apply to any estimator within a class of Lipschitz continuous estimators, irrespective of the specific $f$-divergence loss functions utilized.\nThe bounds are formulated as a product of terms that include the data dimension and the expected value of the density ratio raised to the power of $p$.\nNotably, the lower bound incorporates an exponential term dependent on the Kullback--Leibler divergence, indicating that the $L_p$ error significantly increases with the Kullback--Leibler divergence for $p > 1$, and this increase becomes more pronounced as $p$ increases.\nFurthermore, these theoretical findings are substantiated through numerical experiments.",
    "keywords": [
        "density ratio estimation",
        "variational divergence optimization",
        "Kullback\u2013Leibler divergence",
        "$f$-divergence",
        "$L_p$ error",
        "the curse of dimensionality",
        "and GAN."
    ],
    "primary_area": "learning theory",
    "TLDR": "We provide both lower  and upper  bounds on $L_p$ errors in DRE, which hold for any member of a group of Lipschitz continuous estimators regardless of the specific $f$-divergence loss functions used.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ttq44QjKda",
    "pdf_link": "https://openreview.net/pdf?id=ttq44QjKda",
    "comments": [
        {
            "summary": {
                "value": "This paper establishes L_p\u200b error bounds of the density ratio in Density Ratio Estimation under f-divergence loss. The bounds scale with $N^{-1/d}$ and certain p-Renyi divergence, but is independent from the specific choice of fff-divergence."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The problem of uncertainty quantification in density ratio estimation is an important fundamental question. This work contributes a new understanding of its theoretical limits by deriving L_p\u200b error bounds of the density ratio. The nearly matching upper and lower bounds are a strong point, and the independence on the specific f-divergence within the loss function is also intriguing."
            },
            "weaknesses": {
                "value": "The bound\u2019s dependence on $N^{\u22121/d}$ means that in high-dimensional spaces (where d is large), the error bound becomes practically useless. There is also a lack of downstream application or implication of the theoretical results. As a whole, this raises questions on the meaningfulness of the bounds. \n\nThe experiments presented exhibit high variance, which limits the interpretability and robustness of the empirical validation."
            },
            "questions": {
                "value": "How do these bounds compare to other known bounds in DRE or related density estimation methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "* The paper provides new upper and lower bounds on the Lp errors of a specific density ratio estimator. (Equations 4,5, and 6).\nThis estimator is constructed by minimizing an f-divergence based loss function. \n\nThese bounds provide new insights about how the dimensionality of the data, and the KL divergence between the distributions affect the error of the estimator."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* As far as I know, the derived lower and upper bounds on the density ratio estimator are new.\n* The paper is well-written, and easy to follow.\n* The presentation is clear."
            },
            "weaknesses": {
                "value": "The theoretical results are interesting. \nIn my opinion, what is missing from the paper is to show that these theoretical results can make a difference in some important applications. The paper contains a nice list of motivations for density ratio estimation, but the experimental section only contains some simple toy problems, and therefore the impact of the theory in real applications is not perfectly clear."
            },
            "questions": {
                "value": "* Equation 5 (the upper bound) contains the expectation operator on the left-hand side. This expectation is missing from the lower side (Eq 4). Is it a typo?\n* In Equations 4 and 5, is it possible to create bounds for dimensions 1, and 2 as well?\n* The upper and lower bounds are only derived for a specific density ratio estimator. Is it known how the convergence rate of this estimator compares to other density ratio estimators?\n* How can we use these theoretical results in applications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an approach to density ratio estimation based on $f$-divergences. This idea originated in a paper by Nguyen, Wainwright, and Jordan, who introduced this methodology, which also forms the foundation of generative adversarial networks (GANs). The dual representation of the $f$-divergence involves optimization over a function space, where the optimizer\u2014obtained, for instance, using a neural network\u2014provides an estimator for the likelihood ratio when evaluated on samples. The consistency of this estimator was established in Nguyen et al., and various convergence results have been proved over recent years. The main contribution of this paper is to provide upper and lower bounds on the $L^p$-error, assuming both measures have compact support and that the log-likelihood function is bi-Lipschitz. These bounds are the first of their kind."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The paper is a novel and original contribution to the field of density estimation and highly relevant to recent advances in generative modeling (for example through generative adversarial networks). To my knowledge this are the first $L^p$-error bounds in this context. \nThe proof of the result is very clever and relies on a representation of the $f$-divergence (called the conceptual loss function) which provides access the density ratio.  The estimates are then obtained using a \"nearest neighbor\" approach and by showing that the conceptual loss function and the real loss function are close to each other.  I believe that the techniques of proofs will find other applications where $f$-divergences are used (e.g. in generative modeling).  I found the presentation of the results to be excellent  both, conceptually, in the main text and, technically in the supplementary material.  Also notable is that this result is practical as the implementation of the estimator is totally straightforward using a neural network architecture."
            },
            "weaknesses": {
                "value": "+ There is a very unfortunate typo in the informal theorem 3.5 where in the lower bound  $(1-p)$ should be $(p-1)$.   \n\n+ An explanation of why the result does not depend on the $f$-divergence should be provided.  I believe this is because of the Lipschitz assumption on the log-likelihood and the compactness of the support. In that way the behavior of $f$ at $0$ and at $\\infty$ are irrelevant. \n\n+ Some head-to-head experimental comparison with other density ratio estimators would have been helpful."
            },
            "questions": {
                "value": "+ Can you explain why the choice of $f$ in the divergence does not matter?  Does it only change the constant? \n\n+ What are the implications of your results on the analysis of GANs?\n\n+ What are the implications of your results neural estimation of $f$-divergence (see e.g. the recent https://jmlr.org/beta/papers/v23/21-1212.html)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This article presents an analysis of error rates for density ratio estimation (DRE) in the context of $L^p$-losses applied to $f$-divergence minimization. The study derives upper and lower error bounds, noting that these rates increase with \\(p\\) and exhibit the curse of dimensionality. Theoretical findings are supported by numerical illustrations, which help validate the proposed rates."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Problem is interesting. The proofs seem to be correct. Numerical illustrations support the theoretical findings."
            },
            "weaknesses": {
                "value": "While the work appears mathematically sound and introduces some interesting proof techniques, I am not inclined to recommend acceptance at this stage, primarily due to the following points:\n\n- **Lack of Comparison with Recent DRE Literature**\n    The paper would benefit from a more thorough comparison with recent work in DRE estimation, particularly research involving divergence-based losses. It is, at the current stage, not possible for me to set the results into the context of the large field of DRE (see questions).\n- **Context for Techniques in Sections 4.1 and 4.2**\n    The conceptual reformulation presented in Section 4.1 and the nearest-neighbor-based approach in Section 4.2 would benefit from contextualization. Comparing these methods with existing approaches would clarify their originality and contribution to the field.\n- **Practical Applicability of Assumptions**\n    The main results rely heavily on the Lipschitz continuity of both the function class used and the energy function of the distributions. However, ensuring these assumptions in practical settings may be challenging. This reliance makes the interpretability of the bounds somewhat limited and may constrain practical applicability.\n- **Interpretability of Practical Implications**\n    It remains unclear what practical conclusions can be drawn from the theoretical results. For example, are there implications for algorithmic design that might mitigate the curse of dimensionality or handle large sample sizes effectively? The implications of error rate growth with respect to $p$ should also be discussed.\n- **Minor Issues and Clarifications**\n    The parameter $N$ in Theorem 3.5 is undefined. It should be clarified in what sense equation (4) holds---is this with high probability?"
            },
            "questions": {
                "value": "+ Are conceptual reformulation presented in Section 4.1 and the nearest-neighbor-based approach in Section 4.2 widely used or newly adapted in the paper?\n+ How does Assumption 3.3 relate to the pseudo-self-concordance of losses as discussed in [1]?\n+ Could the proof techniques introduced here be compared to those used in [1, 2]? Is the presented approach more general than these prior works?\n + Can the derived rates with respect to sample size be directly compared to those established in the literature?\n\n[1] Zellinger, W., Kindermann, S., and Pereverzyev, S. V. \"Adaptive Learning of Density Ratios in RKHS.\" Journal of Machine Learning Research, 24:1\u201328, 2023.\n\n[2] Menon, A. and Ong, C. S. \"Linking Losses for Density Ratio and Class-Probability Estimation.\" International Conference on Machine Learning, pp. 304\u2013313, PMLR, 2016."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}