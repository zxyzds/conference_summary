{
    "id": "iQtz3UJGRz",
    "title": "A Bi-metric Framework for Efficient Nearest Neighbor Search",
    "abstract": "We propose a new bi-metric framework for designing nearest neighbor data structures. Our framework assumes two dissimilarity functions: a ground-truth metric that is accurate but expensive to compute, and a proxy metric that is cheaper but less accurate. In both theory and practice, we show how to construct data structures using only the proxy metric such that the query procedure achieves the accuracy of the expensive metric, while only using a limited number of calls to both metrics.  Our theoretical results instantiate this framework for two popular nearest neighbor search algorithms: DiskANN and Cover Tree. In both cases we show that, as long as the proxy metric used to construct the data structure approximates the ground-truth metric up to a bounded factor, our data structure achieves arbitrarily good approximation guarantees with respect to the ground-truth metric. On the empirical side, we apply the framework to the text retrieval problem with two dissimilarity functions evaluated by ML models with vastly different computational costs. We observe that for almost all data sets in the MTEB benchmark, our approach achieves a considerably better accuracy-efficiency tradeoff than the alternatives, such as re-ranking.",
    "keywords": [
        "nearest neighbor search",
        "information retrieval"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=iQtz3UJGRz",
    "pdf_link": "https://openreview.net/pdf?id=iQtz3UJGRz",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a bi-metric framework for nearest neighbor search. It well abstracts a widely used technique for nearest neighbor search, but with several assumptions. The experimental results demonstrate its effectiveness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1. This paper studies an important problem. \n\nS2. The method proposed seems sound. \n\nS3. The experimental results are good to support the effectiveness."
            },
            "weaknesses": {
                "value": "W1. The main contribution (Tm 3.4) is built on top of existing works, which makes the novelty insufficient. \n\nW2. The method is applicable to the case where two embedding models are generated for the same data source. However, there still exist another case where a simple embedding model is generated for another expensive embedding for the sake of nearest neighbor search. For example, product quantization for high-dimensional vectors in DiskANN. \n\nW3. The method assumes that d(x, y) \u2264 D(x, y) \u2264 C \u00b7 d(x, y), which may not be practical."
            },
            "questions": {
                "value": "Q1. Could it be possible to extend the results on PQ vs high-dimensional vectors ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to build a nearest neighbors index over a cheap-to-compute metric $d$, which approximates nearest neighbor search over an expensive, but more accurate, metric $D$. In practice, this means the paper aims to make retrieval over embeddings produced by a small, cheap model emulate retrieval over embeddings produced by a much more expensive model.\n\nThe paper proposes using the cheap $d$ to create the index, and then starting each query's search with $d$ before transitioning to using $D$ in distance computations (albeit still leveraging the same index and graph connectivity, which was created on $d$)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Extensive suite of datasets and models are benchmarked.\nTheory on using a graph built on one metric to navigate a related metric is novel."
            },
            "weaknesses": {
                "value": "1. Contrived and unrealistic setup: critical to the paper's thesis is the belief that the embedding for $x\\in X$ is computed every time the ANN algorithm accesses $x$. It therefore implies that using a cheaper embedding model makes queries cheaper, because these embeddings are re-computed for every query. This is unrealistic because in practice, retrieval systems generally embed all $x\\in X$ offline during the indexing phase, so model size has no impact on query latency or throughput. Embedding the entire dataset offline is far more practical because it makes embedding a fixed cost, rather than one linear in the number of queries. For example, if each query involved 1000 embedding calls, then just 1000 queries would lead to the same embedding cost as embedding 1M elements offline. Furthermore, offline indexing can often achieve >10x throughput gains from batching, which makes the computation much more amenable to ML accelerator hardware. This tilts the decision even further in favor of embedding during indexing.\n1. Main result from Theorem 3.4 result has a poorly described connection to your proposed metric in the experiments, and the choice of $Q/2$ seems quite arbitrary and isn't explored until the end of the paper (but this isn't described in Section 4.1).\n1. Figures are poorly described: \"Single metric (limit)\" is not described anywhere. I'm assuming the x-axis is $Q$, but $Q$ isn't in the x-axis label despite being extensively referenced in the body text, and I have to infer.\n1. Poorly structured: for example, Algorithm 1 is mentioned casually several times in Section 3, but only appears in the appendix, with no indication it could be found there. Theorem 3.4 is the main theorem in the section but is not formatted to emphasize its importance."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a general bi-metric framework for efficient nearest neighbor search by combining an expensive, accurate ground-truth metric $D$ and a cheap, approximate proxy metric $d$. The framework constructs data structures based on the proxy metric $d$ for efficiency, while using limited calls to metric $D$ in query and retaining the accuracy of the ground-truth metric $D$. The authors demonstrate this framework with DiskANN and Cover Tree algorithms and apply it to text retrieval tasks, indicating improvements in the accuracy-efficiency trade-off compared to existing re-ranking or bi-metric methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Theoretical guarantees of the approximation quality (accuracy) are provided.\n\n2. Although methods that combine a cheap metric $d$ with an expensive metric $D$ already exist, the proposed method addresses the limitations of existing methods: (1) The proposed method achieves the accuracy of the expensive metric $D$; (2) The proposed method requires only a sub-linear number of evaluations of $d$ and $D$ during the query stage. These two points represent significant improvements in the nearest neighbor search problem.\n\n3. Extensive experiments across 15 MTEB retrieval datasets were conducted and showed consistent performance gains."
            },
            "weaknesses": {
                "value": "1. The proposed framework is applicable only to algorithms that use the concept of an $r$-net (where, given a parameter $r$, an $r$-net is a small subset of the dataset that ensures every data point is within distance $r$ to the subset in the net). \nFor nearest neighbor algorithms that do not rely on $r$-net, the proposed framework is likely inapplicable.\n\n2. The proposed framework lacks a clear explanation, making it difficult to assess the novelty of the algorithm.\nThe paper does not provide pseudocode in the main text or use toy examples to help readers understand the algorithm. \nThe proposed algorithm is only partially described in Section 4, while Section 3 mainly focuses on theoretical analysis, leaving readers somewhat lost, as they do not fully grasp the proposed method at this point. \n\n3. The experiment results show only the number of calls to metric $D$, while the number of calls to metric $d$ is not shown. \nSince a key claimed improvement over existing bi-metric methods is the reduction (or even sub-linear scaling) in calls to metric $d$, it would be valuable to report the number of calls to $d$ as well.\n\n4. It would be valuable to evaluate across datasets with varying intrinsic dimensionalities. \nSince the bi-metric framework depends on the proxy metric $d$ closely approximating the ground-truth metric $D$, datasets with higher intrinsic dimensionality may introduce greater approximation error, making it more challenging to achieve accurate results. \nAdditionally, higher intrinsic dimensionality could increase query complexity and reduce the efficacy of the $r$-net structure used in the method, potentially impacting both accuracy and efficiency. \nTesting the framework across datasets with different intrinsic dimensionalities would clarify its robustness and scalability under diverse data conditions.\n\n5. In several places, such as line 98 (Equation 1) and line 279 (Algorithm 1), the main text references equations or algorithms that appear much later or only in the appendix. \nThis disrupts the continuity and can cause confusion for readers. \nIt would be helpful if the authors could either move these elements earlier in the text or provide clearer guidance to improve readability and flow."
            },
            "questions": {
                "value": "1. Could you please discuss how your proposed framework applied to other commonly used data structures like Locality-Sensitive Hashing, Proximity Graph, K-d Tree, and Product Quantization? (W1)\n\n2. Could you consider reorganizing the content (including pseudocode in the main text) to improve clarity and provide a more cohesive flow? (W2)\n\n3. Moreover, could you provide a step-by-step explanation of the proposed algorithm, possibly with a toy example, to enhance clarity and help readers fully understand the method before delving into the theoretical analysis? (W2)\n\n4. The paper states that only a sub-linear number of evaluations of both $d$ and $D$ are required during querying. However, it seems that the sub-linear evaluations of $d$ are neither illustrated nor proven, and this is not immediately clear. How does the proposed algorithm achieve a sub-linear number of distance computations? (W3)\n\n5. How does the proposed framework handle datasets with different intrinsic dimensionalities, particularly regarding the accuracy of the proxy metric and the effectiveness of the $r$-net structure under these conditions? (W4)\n\n6. Could the authors consider restructuring the placement of key equations and algorithms, or provide additional guidance to improve readability and continuity throughout the paper? (W5)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}