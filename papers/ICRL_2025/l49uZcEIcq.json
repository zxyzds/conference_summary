{
    "id": "l49uZcEIcq",
    "title": "Verbosity $\\neq$ Veracity: Demystify Verbosity Compensation Behavior of Large Language Models",
    "abstract": "When unsure about an answer, humans often respond with more words than necessary, hoping that part of the response will be correct. We observe a similar behavior in large language models (LLMs), which we term ``Verbosity Compensation\" (VC). VC is harmful because it confuses the user understanding, leading to low efficiency, and influences the LLM services by increasing the latency and cost of generating useless tokens. In this paper, we present the first work that defines and analyzes Verbosity Compensation (VC), explores its causes, and proposes a simple mitigating approach. We define Verbosity Compensation (VC) as the behavior of generating responses that can be compressed without information loss when prompted to write concisely. Our experiments, conducted on five datasets of knowledge and reasoning-based QA tasks with 14 newly developed LLMs, reveal three conclusions. 1) We reveal a pervasive presence of verbosity compensation (VC) across all models and all datasets. Notably, GPT-4 exhibits a VC frequency of 50.40\\%. 2) We reveal the large performance gap between verbose and concise responses, with a notable difference of 27.61\\% on the Qasper dataset. We also show this difference cannot be naturally mitigated with the capability of LLM increases Both 1) and 2) highlight the urgent need to mitigate the frequency of VC behavior and disentangle verbosity with veracity. We propose a simple yet effective cascade algorithm that replaces the verbose responses with the other model-generated responses. The results show that our approach effectively alleviates the VC of the Mistral model from 63.81\\% to 16.16\\% on the Qasper dataset. 3) We also find that verbose responses exhibit higher uncertainty across all five datasets, suggesting a strong connection between verbosity and model uncertainty. Our dataset and code will be released.",
    "keywords": [
        "Large Language Models",
        "Verbosity of LLM",
        "LLM Uncertainty",
        "LLM Routing"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We define and categorize the verbose compensation behavior of large language models (LLMs), finding a significant performance gap associated with verbose responses, which we attribute to model uncertainty.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=l49uZcEIcq",
    "pdf_link": "https://openreview.net/pdf?id=l49uZcEIcq",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a concept termed \u201cVerbosity Compensation\u201d (VC), where LLMs respond with overly verbose answers to compensate for uncertainty. VC is shown to decrease response efficiency, increase costs, and obscure clarity, highlighting an urgent need to mitigate this behavior. To address VC, the paper introduces a cascade algorithm that effectively reduces verbosity by replacing verbose responses with alternative model-generated responses, achieving a significant reduction in VC, particularly demonstrated with a 63.81% to 16.16% decrease for the Mistral model on the Qasper dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Interesting research problem."
            },
            "weaknesses": {
                "value": "1. In the verbosity compensation and uncertainty section, how to quantity the uncertainty for each split is not clear. How to get the perplexity evaluation metric and eigenvalues is not clear.\n2. In my understanding, In performance difference calculation, the recall is calculated by exact matching, checking if the generation contains the exact same string of the ground truth answer. There might be some correct but not exactly matched answer in LLM generation, especially in the lengthy (verbose) response. So would be better to design a partial credit to further validate the findings of this work.\n3. The proposed cascade model selection method can leak the ground truth answer. In the selection process, the detector would take ground truth answer as input for answer selection.\n4. In section 4.2, what does the |r|>3 mean? \n5. In line 365 and figure 3, what does the llama-3-80b mean? I believe llama-3 doesn\u2019t have the 80B version."
            },
            "questions": {
                "value": "please see the weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studied the verbosity issue of LLMs. The authors tested the frequency that LLMs generate verbose answers (a behavior defined as **verbosity compensation** by the authors), and showed the following findings:\n\n(1) All mainstream LLMs generate verbose answers.\n(2) Models have different performance when generating concise and verbose answers. Most models perform worse when their response are verbose.\n(3) There is a positive correlation between answer verbosity and model uncertainty. This means that models are usually more uncertain about the answers when they generate excessively long answers.\n(4) Using verbosity as an indicator to route models (if the smaller model generates a verbose answer, then we give this example to the larger model) is a promising approach to balance model performance and inference cost."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The authors are the first to formally define the verbosity compensation (VC) behavior of LLMs which refers to generating an answer that is more verbose than required.\n2. The authors collected a benchmark from existing NLP task data to analyze the frequency of VC in LLMs. They further designed metrics to calculate the impact of VC on model performance.\n3. The authors did extensive experiments to test the correlation between VC and (1) model performance (2) model uncertainty. These experiments led to the findings I listed in paper summary.\n4. The authors proposed a simple but effective model routing technique based on their findings on VC: routing the instance to stronger models if the weaker model exhibits verbosity in answering. They further tested its effectiveness in reducing verbosity and balacing performance-cost trade-off.\n5. The paper is well written and the experimental results are clearly presented."
            },
            "weaknesses": {
                "value": "1. The fairness of evaluation metrics is questionable. This includes two aspects:\n\n(1) The authors wrote in section 3.1: *We define a response $r$ to be verbose if and only if it can be further compressed with fewer tokens\nwhile keeping the same meaning.* However, in actual experiments, they directly used \"the number of tokens in the response\" as the verbosity detector. **They did not measure \"whether the answer can be compressed while keeping the same meaning\"**. This may contain bias, for example, if the gold answer is 3 tokens but the model generates a wrong answer that is 5 tokens, although the answer is wrong, but I don't think the wrong answer is verbose if it cannot be further simplified. However, such a case will be identified as verbose by the authors as their criteria of conciseness is \"less than 4 words\".\n\n(2) In section 3.2, the authors define $\\Delta$ (performance difference) as the model score on cases where its answer is verbose minus the score on cases where its answer is concise. This means, **the two scores are not calculated on the same test instances**. First, I suspect this is not 100% fair, because there may be only a few \"verbose instances\" or \"concise instances\" for some LLMs, which will lead to inaccurate estimation. Second, I suspect this metric can be affected by spurious correlations. The authors showed in section 5.3 that generating verbose answers indicate a model's uncertainty. And it is possible that model-uncertain cases are those that are more difficult. Therefore, it is reasonable that models perform worse on such instances.\n\n2. The evaluation set is limited to scenarios where the gold answer is less than 4 words. However, I think such short-form QA is just one of the possible situations where verbosity may occur, and it might not even be the most critical one. Verbosity may be more harmful in open-ended generation cases that are not evaluated in this paper. For example, *repeating the question when answering a factual question* may be less annoying than *saying a lot of useless words when planning a trip*.\n\n3. When evaluating the proposed cascading approach in model routing, the authors only compared it with randomly selecting models. However, they did not compare with more related baselines such as \"model routing based on response uncertainty\". For example, if the weaker model is uncertain about the response, then give this question to the stronger model."
            },
            "questions": {
                "value": "1. The equation in section 3.2 uses the score of cases where $V(x,y,r)=1$ as the minuend, and the score of cases where $V(x,y,r)=0$ as the subtrahend. However, the authors defined the formula as \"the average score of the concise responses minus the average score of the verbose responses\". Should the minuend and subtrahend be reversed?\n2. I have concerns about the definition of verbosity. From Figure 1 and the definition of verbosity detector, it seems like answers like *Paris is the capital of France* will be judged as verbose for the question *Which city is the capital of France?*. However, I think such a response seems pretty natural and human-like, and it does not bring any confusion or inefficiency to the communication. Therefore, I wonder whether the definition of \"if the gold answer is less than 4 words, then any answer longer than 4 words is verbose\" is too strict and whether it will be widely accepted."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel concept of verbosity compensation in LLMs. They define this term, explore it's presence in different models using combination of different datasets, also they try to come up an algorithm to tackle this issue of VC. The paper defines verbosity compensation as a tendency of LLMs to generate overly verbose responses when the LLMs are unsure about the answer. Coming to the evaluation part there are different sections of it with combination of automated evaluation and manual human evaluations. The paper does have some interesting things but there are some issues with the actual implementation and also writing, it is not very clear the exact research question the paper is trying to solve."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The model has a new concept of verbosity compensation\n\nThey explore 14 different LLM for comparison and performance evaluation\n\nThe idea of different type of VCs\n\nThe work also introduces algorithm to mitigate the issue that has been defined in the paper"
            },
            "weaknesses": {
                "value": "- > The problem doesn't seem to very well defined, I couldn't understand the exact research question the paper is trying to solve.\nIf the problem is about the length of output, I don't see it as a major issue.\n\n -> The dataset used for this experiment is creates using existing datasets from previous works, most of these are from previous years and there is high chance that these were in the training data that was used for training different LLMs used in the paper and the details of these data-leakage are not explored or mentioned.\n\n-> The prompt used for the experiment is not clear, more details on why 3 words was choose and why is it mentioned multiple times single phrase if possible, but then you mention single phrase and also 3 words. Given that the work depends a lot on the prompt, more prompts should have been experimented and more details of those results are needed.\n\n-> Though there is mention of verbosity detector the actual definition and formula on how you calculate it is missing.\n\n-> There is introduction of algorithm, the paper mentions that VC might causes latency, there can be latency due to algorithm as well.\n\n-> Writing can be improved\n\n-> Most of the examples mentioned in the paper have some numbers in the answers, out of 3 correctly given answers 2 doesn't have any numbers, given that there is existing research that LLMs are not good at the numbers, it would be better to compare the behavior on those lines as well, if the output is wrong because of the numbers and not other factors."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}