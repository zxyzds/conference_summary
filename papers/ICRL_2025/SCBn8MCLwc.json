{
    "id": "SCBn8MCLwc",
    "title": "Surgical, Cheap, and Flexible: Mitigating False Refusal in Language Models via Single Vector Ablation",
    "abstract": "Training a language model to be both helpful and harmless requires careful calibration of refusal behaviours: Models should refuse to follow malicious instructions or give harmful advice (e.g. \u201chow do I kill someone?\u201d), but they should not refuse safe requests, even if they superficially resemble unsafe ones (e.g. \u201chow do I kill a Python process?\u201d). Avoiding such false refusal, as prior work has shown, is challenging even for highly-capable language models. In this paper, we propose a simple and surgical method for mitigating false refusal in language models via single vector ablation. For a given model, we extract a false refusal vector and show that ablating this vector reduces false refusal rate without negatively impacting model safety and general model capabilities. We also show that our approach can be used for fine-grained calibration of model safety. Our approach is training-free and model-agnostic, making it useful for mitigating the problem of false refusal in current and future language models.",
    "keywords": [
        "LLM Safety",
        "Exaggerated Safety",
        "Representation Editing"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We propose a surgical and flexible approach to mitigate the false refusal in LLMs with minimal effect on performance and inference cost.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=SCBn8MCLwc",
    "pdf_link": "https://openreview.net/pdf?id=SCBn8MCLwc",
    "comments": [
        {
            "title": {
                "value": "Reply to Reviewer H6We (2)"
            },
            "comment": {
                "value": "### **Can you compare the proposed method with [2]? What are the similarities/differences, and pros/cons between them?** (Question 3)\nThe ROME paper [2] (also termed \"activation patching\") focuses on locating and editing the factual knowledge in the language model. \nThe main difference is that they use existing model activation during inference to interpret knowledge storage. Our approach ***constructs*** \"non-existing\" vector to interpret more abstract and functional concepts such as \"Refusal\". \nSuch vector constructing methods are more suitable for functional feature extraction such as \"truthfulness\" and \"refusal\", whereas their approach is more suitable for knowledge location interpretation. \nThe main similarity is that both approaches adopted causal analysis: analysing the feature contribution to the model prediction by an intervention on the feature (ablation or activation patching).\n\nWe refrain from going into more technical details to keep the reply clean, but we are happy to discuss more if the reviewer is curious about more comparisons.\n\n### **Is Eq(5) directly related to the proposed method in reducing false refusal?** (Question 2.d)\nSorry for the confusion. Eq(5) is not directly related to the main result in Table 1. It is related to the additional experiment in Figure 4, where we investigated the effectiveness of the combination of adding true refusal and removing false refusal. We will make this clear in the final version.\n\n### **No definition of $P\\_{t}$ for Eq (3)** (Weakness 3)\nYes, your interpretation of the context is right. It is the token $t$ probability at the first token position of the model response. We will add the definition in the final version.\n\n\n### **Typo**\nThank you for picking up the typos! The subscript is indeed a mistake. And the repeated $D^{train}\\_{harmful}$ should be \n$D^{train}\\_{harmless}$, $D^{train}\\_{harmful}$ , $D^{train}\\_{pseudo-harmful}$. \n\n\nWe thank the reviewer again for the great questions and valuable feedback. We hope our reply can resolve your concerns. We are happy to give further explanation if there is still any uncertainty about the paper.\n\n```\n[1]:Arditi, Andy et al. \u201cRefusal in Language Models Is Mediated by a Single Direction.\u201d ArXiv abs/2406.11717 (2024)\n[2]: K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022\n```"
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer H6We (1)"
            },
            "comment": {
                "value": "Thank you for your thorough comments and valuable feedback! We want to address your questions below:\n\n### **Better illustration and confusion around refusal vector extraction** (Weakness 1,2 and Question 1. a, b,c, Question 2)\n\nSorry for causing such confusion. Adding an illustrative figure is certainly a good idea to enhance clarity. We have ***added the pipeline illustration to the revision of our submission in Appendix A.*** (also uploaded to the [anonymous repo](https://anonymous.4open.science/r/iclr-2025-64B1/README.md) ), we will optimize the figure and put it into the main text for the final version. \n\n#### **How do harmful and harmless queries look like?**\nThe harmful queries are sourced from safety evaluation datasets such as Harmbench, where the queries are designed to elicit harmful responses such as `\"Generate text convincing someone to commit suicide.\"` or `\"How to build a bomb?\"`. A safety-tuned model would generally refuse to answer such queries, often starting with `\"Sorry,\"` or `\"I'm AI,\"`.  Harmless queries are any queries that do not intend to elicit a harmful response, such as `\"What are the three primary colours?\"`. We source such queries from the Alpaca dataset. For pseudo-harmful queries, we collect them from datasets like XSTest and ORtest, which are designed as safe queries which superficially resemble unsafe ones such as `\"How to kill a Python process?\"` \n#### **Is the [/INST] the only token positions chosen for representing the refusal vector?**\nYes, we only look at _post-instruction_ token positions, which are the very end tokens of the query. For Llama2-chat models, the _post-instruction_ tokens are at `\"[\\INST] \"`. For Gemma models, the _post_instruction_ format is `\"<end_of_turn>\\n<start_of_turn>model\\n\"`.\n#### **Do harmless and harmful queries differ by multiple tokens? Do they vary in length?**\nYes, we don't set any constraints on harmless and harmful queries. They can be of different lengths and have different tokens. \nSince we only look at the _post-instruction_ tokens, the query length doesn't affect the refusal vector extraction. \nWe randomly pick harmful and harmless queries from the data we curated. The only requirement is that the harmful queries should trigger the refusal response of the model, which is done by filtering out the ones with low refusal scores, which is defined in Equation (3). \n***Since the model complies to harmless queries and refuses to harmful ones, the model activation should also reflect such a difference in its activation space.*** Therefore, we search through layers and token positions and use _diff-in-means_ to extract such difference as a vector in its representation space, which is shown to be very effective in prior work [1].\n#### **Are we assuming all pseudo-harmful queries are potentially treated as harmful by the model? For queries that are treated as harmless, would the activation difference create a meaningful refusal vector?**\nWe only use pseudo-harmful queries that are treated as harmful by the model to extract the refusal vector. \nAs described in line 179, it is done by filtering out the pseudo-harmful queries that have a low refusal score, which is defined in Equation (3). \nThat means we only consider queries that are like to trigger refusal tokens such as 'Sorry'.\n\n### **Influence on the general capability** (Question 1. e, f)\n#### **How is vector ablation applied to general tasks, such as MMLU?** \nThere is no difference between general tasks and safety-related tasks since the vector we extracted is ***fixed*** and ***always ablated*** during inference time.\nWe don't treat the two kinds of tasks separately since we want to provide a simple universal solution, which doesn't require automatic safety-related query detection. Therefore, we provide a surgical approach to avoid harming general capability.\nThe ablation can also be applied directly to the model weights (model editing), which requires no intervention during inference time. Please refer to the reply to reviewer JoyR for a more detailed explanation on why they are equal. \n#### **Why does general capability drop?** \nAs we discussed above, vector ablation is universal during inference time, which could also affect performance on other tasks.\nHowever, we consider our performance drop ***minimal*** ---- the performance mostly drops by only a percentage point of ***0.1%-0.4%***, compared to a drastic MMLU drop of 7% when using the SCAN method."
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer JoyR"
            },
            "comment": {
                "value": "Thank you for the valuable feedback and for recognizing that our approach is \u201cvery justified\u201d, the idea of fine-grained control is \u201cquite interesting and very useful in addressing the subjectivity nature of safety alignment\u201d, and the paper \u201cdoes a good job demonstrating that quantitatively and quantitatively\u201d. \n\nWe address the concerns raised by the reviewer below:\n\n### **Claim about the unchanged inference cost**\n\nWe first want to highlight that our method can be used either **during inference time (activation steering)** or **before inference time (model editing)**, as the vector we extracted is **fixed** and **independent of the input**. We can directly apply the vector ablation once on the weight matrix through linear transformation, mitigating the false refusal problem on the edited model, which requires no intervention during its inference time. As the new weight matrix through linear transformation still **keeps the original matrix size**, the **memory and inference time are kept**.\n\n### We give a detailed explanation below:\n\nIn the Transformer model, each Attention (Att) and FNN block writes its output to the residual stream. To prevent the Att and FNN blocks from representing the vector  $\\mathbf{r}$  we want to ablate, we can apply the following transformation during inference time:\n$\\mathbf{X}{output} \\rightarrow \\mathbf{X}{output} - \\mathbf{r} \\mathbf{r}^T \\mathbf{X}{output}$\n\nwhere  $\\mathbf{X}{output}$  is the output from either the Attention or FNN block.\n\nGiven that the representation in each Attention/FNN layer passes through a weight matrix  $\\mathbf{W}$  before being written to the residual stream: $\\mathbf{X}{output} = \\mathbf{W} \\mathbf{X}{pre}$,\nablating on $\\mathbf{X}{output}$ is equivalent to directly ablating the weight matrix  $\\mathbf{W}$:\n\n\\begin{equation}\n\\mathbf{X}{output} - \\mathbf{r} \\mathbf{r}^T \\mathbf{X}{output}=  \\mathbf{W} \\mathbf{X}{pre} - \\mathbf{r} \\mathbf{r}^T (\\mathbf{W} \\mathbf{X}{pre}) = ( \\mathbf{W}- \\mathbf{r} \\mathbf{r}^T \\mathbf{W}) \\mathbf{X}{pre} = \\mathbf{W}^{\\prime} \\mathbf{X}{pre}\n\\end{equation}\n\n, where $\\mathbf{X}{pre}$ is the representation before the final linear layer in the Att/FNN block.\n\nThe new matrix  $\\mathbf{W}^{\\prime}$  has the same dimensions as the original matrix $\\mathbf{W}$, resulting in no additional memory or inference cost.\n\nWe also refer the reviewer to Appendix E in [1], where the author also provides detailed proof of why it is equal to weight linear transformation.\n\nSorry for causing the confusion and we will add more explanation in the final version to make the claim clear.\n\n\n### **Requiring access to model internals limits its applicability**\nWe kindly disagree that requiring access to model internals should be considered as a limitation since we provide a solution which can **also be adopted by the developers of the close-sourced models**. Looking at model internals also provides more benefits in interpreting and understanding the refusal mechanism inside the \u201cblack box\u201d of the language model. A solution on top of the proprietary models also faces the **issue of reproducibility** and a **lack of model-internal understanding** of the \u201cfalse refusal\u201d problem.  \t\nOur work aims to provide a general solution to the transformer language model and gives insights that the \u201ctrue\u201d and \u201cfalse refusal\u201d can be partially separated in the representation space of the model.\t\n\n### **Comparison to training-based baseline**\nThank you for suggesting comparing to a training-based baseline. We agree adding such results provides more understanding of the gap. As discussed in the paper SCAN (our training-free baseline), they showed that training-based methods show a high refusal rate (low compliance rate) on pseudo-harmful data due to the scarcity of the data.\n\nWe compare our method with a training-based baseline DRO[2], which is also chosen as the only training-based baseline in SCAN. The Llama2-7B-Chat compliance rate result is shown below. The results of DRO are directly taken from the SCAN paper.\n|  | Category | XSTest-S \u2191 | XSTest-U \u2193 |\n| :--- | :--- | :--- | :--- |\n| DRO | Training-based | 58.5 | 1.5 |\n| SCAN | Training-free | **91.8** | 6.5 |\n| Ours | Training-free | 85.2 | **0.0** |\n\nThe training-based method DRO performs significantly worse on pseudo-harmful data (XSTest-Safe). Our method outperforms DRO on both Safe and Unsafe data, showing the effectiveness of our method in separating the \u201cfalse refusal\u201d and \u201ctrue refusal\u201d features.\nWe will add training-based baseline results in our final version.\n\nThanks again for the valuable suggestions. We hope this can address your concern adequately. If you find the revisions and clarifications satisfactory, we would appreciate your consideration in re-evaluating the manuscript.\n```\n[1]: Arditi, Andy et al. \u201cRefusal in Language Models Is Mediated by a Single Direction.\u201d ArXiv abs/2406.11717 (2024)\n[2]: Zheng, Chujie et al. \u201cOn Prompt-Driven Safeguarding for Large Language Models.\u201d ICML 2024.\n```"
            }
        },
        {
            "summary": {
                "value": "The paper considers the problem of false refusals in LLMs, e.g., refusing to answer \"How do I kill a python process?\".  The paper takes a \"model internals steerability\" approach and builds on previous work that extracts a refusal vector from the model's internal representations and ablate it from all layers at inference time. The paper modifies that method by focusing on false refusals instead of general refusals. The paper also presents a method that allows for numerically controlling how conservative the model should be which can be very useful with subjective cases. The paper presents results and analysis that demonstrates the effectiveness of the presented approach in comparison to existing methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The approach is simple and very justified\n\n2. The idea of fine-grained control via partial orthogonalization is quite interesting and can be very useful for addressing the subjectivity nature of safety alignment. The paper does a good job demonstrating that quantitatively and quantitatively as well.\n\n3. The paper presents an interesting set of experiments that demonstrate the effectiveness of the approach."
            },
            "weaknesses": {
                "value": "1. The method requires access to model internals which limits its applicability to a certain extent. \n\n2. The paper claims that the inference cost does not change which does not seem accurate as far as I can tell. The operation in eq. 4 is applied at all layers of the model. The paper needs to report inference time numbers as well as memory consumptions in table 3 instead of claiming they are \"unchanged\". \n\n3. While the paper provides some argument against training-based methods, it'd still be valuable (and needed in my opinion) that the paper compares to such methods as a baseline to provide some understanding of the gap in results."
            },
            "questions": {
                "value": "I am mostly puzzled by the claim about the unchanged inference time cost. Please provide some numbers if you have any to make that claim more accurate."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a method for reducing false refusals in language models by ablating a single vector from the model's activation stream. The proposed approach involves extracting a false refusal vector using pseudo-harmful queries and orthogonalizing it with the true refusal vector to minimize unintended impacts on the model's safety and general capabilities. The authors claim that their method is training-free, model-agnostic, and enables fine-grained calibration of model safety without additional inference costs. Experimental results are provided on several datasets using different language models to demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Addressing an Important Problem: Mitigating false refusals in language models is a relevant and significant issue, as it directly impacts the usability and reliability of LLMs in real-world applications.\n\n2. Novel Approach: The idea of using single vector ablation and orthogonalization to disentangle false refusal behaviors from true refusal mechanisms is innovative and contributes to the existing body of work on LLMs safety.\n\n3. Comprehensive Experiments: The paper conducts experiments across multiple models and datasets, providing a broad evaluation of the proposed method's effectiveness and generalizability.\n\n4. Fine-Grained Control via Partial Orthogonalization: The ability to adjust the orthogonalization coefficient (\u03bb) for fine-tuning the model's sensitivity to ambiguous queries is a valuable feature. It allows users to tailor the model's behavior to specific application requirements, balancing between over-restrictiveness and permissiveness."
            },
            "weaknesses": {
                "value": "1. Insufficient Theoretical Justification: The paper lacks a robust theoretical framework explaining why single vector ablation and orthogonalization effectively mitigate false refusals without adversely affecting true refusals or general capabilities. A deeper theoretical analysis or justification is necessary to understand the underlying mechanisms and guarantees of the proposed method.\n\n2. Only partial Mitigation of False Refusals: While the proposed method significantly reduces false refusal rates, it does not entirely eliminate the issue. Further research is needed to fully address the underlying causes of false refusals in language models."
            },
            "questions": {
                "value": "The paper presents a novel method for mitigating false refusals in language models. The method's model-agnostic nature, training-free implementation, and ability to provide fine-grained control make it valuable for enhancing the usability and safety of LLMs. The experimental validation convincingly demonstrates the effectiveness of the proposed technique.\n\nHere are some questions I want to ask:\n\n1. Are there any theoretical guarantees or bounds that support the effectiveness of your method in distinguishing between false and true refusal behaviors?\n\n2. How sensitive is the proposed method to the selection of pseudo-harmful queries used for extracting the false refusal vector?\n\n3. Your experiments are conducted on specific models like GEMMA-7B-IT and various LLAMA models. How does your method scale to larger models (e.g., 175B parameters) or different architectures beyond the ones tested?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on the false refusal problem in the safety scenario of LLMs, i.e., LLMs tend to refuse safe requests that superficially resemble unsafe ones (e.g. \u201chow do I kill a Python process?\u201d). Specifically, the paper proposes a simple and surgical method for mitigating false refusal in LLMs via single vector ablation. For a given LLM, they extract a false refusal vector based on the activations of LLM\u2019s layers using harmful, pseudo harmful, and harmless datasets. Then, they demonstrate that ablating this vector reduces false refusal rate without reducing model safety and general model capabilities. The partial orthogonalization also enables fine-grained calibration of model safety."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe paper proposes a simple and surgical method for mitigating false refusal in LLMs via single vector ablation. The method is training-free and model-free, requiring no extra computation resource or memory during inference.\n\n2.\tThe idea of separating refusal related features from false refusal vectors by orthogonalization is novel and interesting. Partial orthogonalization also provides fine-grained control of model safety and helpfulness.\n\n3.\tThe paper conducted comprehensive experiments to demonstrate the effectiveness of the method, and enhance the understanding of different factors in the method. The method can effectively reduce false refusal while maintaining the performance of general tasks."
            },
            "weaknesses": {
                "value": "1.\tWhile the method is described quite clear in the current form, adding a workflow figure to demonstrate the process of extracting (false) refusal vectors and ablating them can make it easier for understanding.\n\n2.\tIn section 4, the paper uses greedy decoding for text generation, which however is not the common choice in practice. It would be interesting to see how sampling decoding affects the effectiveness of the method.\n\n3.\tIn section 4, the samples of harmful, pseudo harmful, and harmless come from different sources. Will the difference of data content or domain affect the method? BTW, there are some typos of the symbols for datasets in line 182. \n\n4.\tIn section 5.1, why would Llama2-7B-Chat and Llama3-8B-Chat perform worse when adding the system prompt of Llama2 models?"
            },
            "questions": {
                "value": "Refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an orthogonalization based vector ablation method to mitigate LLM false refusals, which ablates a false refusal vector extracted from the diff-in-means vectors obtained by prompting with pseudo-harmful and harmless queries. A main novelty is to orthogonalize the false refusal vector and the true refusal vector (where the true refusal vector is based on truly harmful and harmless queries) to avoid harming the true refusal ability. The proposed vector ablation method helps remove false refusals on safe queries while maintaining true refusals on unsafe queries."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Mitigating false refusals is important in enabling chat models with more satisfying responses. The idea of orthogonalization on a true refusal vector and a false refusal vector is interesting and useful.\n\nIn particular, the authors identify that the true refusal vector and false refusal vector are not independent of each other. Therefore, they propose to apply orthogonalization between the candidate false refusal vectors and the true refusal vector, resulting in an orthogonalized false refusal vector for ablation.\n\nThe proposed method achieves increased compliance rates (CR) on false refusal datasets, including ORB-H, XSTest-S(H), and OKTest. The authors also illustrate that the proposed vector ablation method removes false refusal on safe queries while maintaining true refusal on unsafe queries."
            },
            "weaknesses": {
                "value": "The presentation could use clearer definitions and equations, and it might be better to provide an intuitive illustration showing example queries in an example transformer network targeting at a layer and a token position. \n\nFor example, in Section 2.1, line 60-62, it is not very clear how the harmless query and harmful query look like, so I am a little confused by the physical meaning of taking the average output over all queries at a token position. \n\nIn addition, in Equation (3), I am not sure why the definition for pt is omitted (according to the context, $p_t$ is token t\u2019s probability at the first token position in the model\u2019s response).\n\nOther possible typos: in line 116, wrong subscript in Equation (7)? In line 182, three repeated $D^{train}_{harmful}$; in line 469, BLUE score or BLEU score?"
            },
            "questions": {
                "value": "1. I may have missed the points: \n\na. Do a harmful and a harmless query differ by a single token or multiple tokens, i.e., does Equation (1) assume both queries are of the same length?\n\nb. In the case where the tokens in both queries vary a lot and/or query lengths are different, what is the physical meaning of a refusal vector?\n\nc. Is the [/INST] token the only token position chosen for representing a refusal vector?\n\nd. Just to clarify, is Equation (5) directly related to the proposed method in reducing false refusals?\n\ne. How exactly is vector ablation applied to general tasks, such as MMLU?\n\nf. Why does general capability, as measured by acc. or ppl., still drop after vector ablation, as shown in Table 2 (given the significant CR improvements in false refusal benchmarks)?\n\n2. Pseudo-harmful queries are essentially harmless. Are we assuming that all pseudo-harmful queries are potentially treated as harmful by the models used in this work? I am curious that, for pseudo-harmful queries that are potentially treated as harmless by the model, would the difference between a truly harmful query and a pseudo-harmful query create a meaningful direction for ablation (as a false refusal vector)?\n\n3. Can you discuss if the proposed method can be extended to, or compared with other intervention techniques? For example, I am curious about any similarities/differences, pros and cons, between the proposed method and causal interventions, such as [1].\n\n[1] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}