{
    "id": "OeHSkJ58TG",
    "title": "Incidental Polysemanticity: A New Obstacle for Mechanistic Interpretability",
    "abstract": "Polysemantic neurons \u2014 neurons that activate for a set of unrelated features \u2014 have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more \"features\" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand networks' internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, a phenomenon we term incidental polysemanticity. Using a combination of theory and experiments, we show that incidental polysemanticity can arise due to multiple reasons including regularization and neural noise; this incidental polysemanticity occurs because random initialization can, by chance alone, initially assign multiple features to the same neuron, and the training dynamics then strengthen such overlap. Our paper concludes by calling for further research quantifying the performance-polysemanticity tradeoff in task-optimized deep neural networks to better understand to what extent polysemanticity is avoidable.",
    "keywords": [
        "polysemanticity",
        "mechanistic interpretability",
        "AI safety",
        "deep learning",
        "science of deep learning",
        "neural computation",
        "interpretability"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "Polysemanticity in deep neural networks might be attributable to causes other than optimizing for the task, i.e., it may be incidental.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=OeHSkJ58TG",
    "pdf_link": "https://openreview.net/pdf?id=OeHSkJ58TG",
    "comments": [
        {
            "summary": {
                "value": "The paper investigates the emergence of polysemanticity in neural networks, demonstrating that it arises not solely from bottleneck dimensions (extending Elhage et al.'s work). The authors propose that incidental polysemanticity occurs during random initialization and early training, providing mathematical derivations to support this claim."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper focuses on origins for polysemanticity, a central and important problem in mechanistic interpretability\n- The paper provides thorough walkthroughs of mathematical derivations and explains individual resulting terms (feature benefit, interference, regularization)"
            },
            "weaknesses": {
                "value": "In general, I am unsure about the weight of novel contributions in this paper relative to the bar of an ICLR acceptance. I will defer to the AC in this regard.\n\nCritiques on Section 1, L1 Regularization\nI am not surprised that random initialization determines whether true features are encoded monosemantic or binary. The authors clearly show how random initialization arguments are reflected in experiments on training dynamics and scaling the number of hidden neurons. However, I am unsure how these findings translate into language models, which are not explicitly trained with an L1 regularization.\n\nCritiques on Section 3, Neural Noise\n- Without context from Elhage et al. it is unclear, why kurtosis is the main metric tracking sparsity in this context. An accompanying background section motivating kurtosis would be useful\n- Section 3.2 quickly summarizes the mathematical analysis of this section. A motivation of why highlighting bipolar noise would be useful. (Gaussian noise is a natural choice)"
            },
            "questions": {
                "value": "--"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Polysemanticity describes the quality of an internal feature/neuron in which the feature will have a high activation for multiple features, indicating that the feature is associated with more than one feature. The prevailing theory on polysemanticity is that it is necessary: when the feature space is smaller than the number of features, then neurons must represent multiple features. This paper explores the possibility that polysemanticity may not occur necessarily, but as a feature of training. Particularly the paper looks at sparsity enforcement in the hidden layer, and observes unnecessary and random, or incidental polysemanticity."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Toy model is illustrative and easy to apprehend.\nIn-depth and accessible discussion in section 2.\nFavorable results matching theory in section 2.\nWell written and easy to follow."
            },
            "weaknesses": {
                "value": "The examples for incidental polysemanticity are explored with L1 regularization of internal representation, or noise injections, and when the feature dimension is larger than necessary. The main question is why polysemanticity occurs in typical settings, and it is unclear how these results apply to the more typical scenarios. This lessens the paper's contribution significance. This paper could be improved by elucidating more how the content applies to more typical scenarios or providing content more in alignment with those scenarios.\n\nSection 3/4 discuses how noise is tied to sparsity, but does not verify that noise is tied to polysemanticity. The paper could be improved by providing experiments showing how noise causes incidental polysemanticity."
            },
            "questions": {
                "value": "I don't see any baseline polysemantic neurons count for your model, with no regularization or noise. Is this because there is no way to define them, as there is no winner-takes-all enforcement? It would be beneficial to establish some sort of baseline.\n\nSection 4, on noise injections, explains the relationship between noise injections and sparsity, but I fail to see a discussion on what this has to do with polysemantic neurons. Are there experiments on the number of incidentally polysemantic neurons with noise injections? This would be beneficial.\n\nIs there an example of incidental polysemantic neurons when m < n, or the feature space is smaller than the required number of features? This would help move the scenario of the ecperiemnt closer to what is typical.\n\nCan you explain how incidental polysemanticity with L1 regularization relates to the base case? Is it that the scenario where the feature space dimension is much smaller than the number of features is similar to a scenario with feature sparsity? Making this more clear would help readers understand your contribution.\n\nCan you explain how noise injections relate to explaining the cause of polysemanticity in the typical case? Perhaps I be unfamiliar with how common these types of noise injections are.\n\nAn interesting experiment (if they have not been done) is to train an autoenoder in a more normal scenario: without regularization, noise, and with m < n. Then check if the number of polysemantic neurons is greater than n-m. If so, then this could be evidence of incidental polysemnticity in a typical scenario.\n\nMinor Issues:\n051) In that case\n135) By $W_i = f_i$ , do you mean $W_i=e_i$?\nFigure 3, graph not displaying range bars for last two points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Polysemantic neurons that are activated by different natural concepts have attracted the attention of researchers recently due to their influence on feature interpretability. In this paper, the authors introduce a new scenario where polysemanticity might be caused by non-task factors in the training process. In the toy models, they consider two conditions. i.e., the $l_1$ training regularization and hidden layer noise. With the theoretical and empirical evidence, they reproduce the polysemanticity in the toy models and analyze the learning dynamics in the training process."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The theoretical analysis of the training dynamics in toy models is solid and cooperates well with the empirical evidence.\n2. The paper is well-organized and easy to follow. The main insight is clear and the two conditions in the toy models are explained well.\n3. Analyzing the mechanism of polysemantyicity is important and this paper proposes an insightful perspective."
            },
            "weaknesses": {
                "value": "1. This paper focuses on the polysemanticity that is not related to the tasks. It would be better to add more discussions on the differences between incidental polysematicity and the original task-related polysemanticity. For example, as polysemanticity is not related to the tasks, can we get rid of the performance-interpretability trade-off? Besides, in the neural networks, how can we distinguish the incidental polysemanticity?\n2. The analysis in this paper is conducted on the toy models. I understand it is for the ease of theoretical analysis. However, it would be better to provide some insights into the polysmenaticity in the neural networks trained on real-world data and add some additional experiments or discussions.\n3. The explanation and motivation of Figure 6 is a little confusing and it is perhaps a digression from the main topic of the paper. It would be better to provide more discussions of the new insights.\n4. In this paper, the authors mainly focus on how to obtain polysemantic neurons. However, in real-world data, the main challenge lies in obtaining the monosemantic neurons. Consequently, is it possible to provide some insights about how to attain monosemanticity based on the analysis in this paper?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}