{
    "id": "QtKYYatG3Z",
    "title": "Evaluating the World Models Used by Pretrained Learners",
    "abstract": "A common approach for assessing whether large pretrained models develop world models is by studying the behavior of fixed models. However, many of the benefits of having a world model arise when transferring a model to new tasks (e.g. few- shot learning). In this paper, we ask: what does it mean to test if a _learner_ has a world model embodied in it? We consider a simple definition of a true world model: a mapping from inputs to states. We introduce a procedure that assesses a learner\u2019s world model by measuring its inductive bias when transferring to new tasks. This inductive bias can be measured in two distinct dimensions: does a learner extrapolate to new data by building functions of state, and to what degree do these functions capture the full state? We use this procedure to study the degree to which pretrained models extrapolate to new tasks based on state. We find that models that perform very well on next-token prediction can extrapolate to new tasks with very little inductive bias toward state. We conclude by assessing the possibility that these models learn bundles of heuristics that enable them to perform well on next-token prediction despite preserving little of state.",
    "keywords": [
        "large language models",
        "world models",
        "transfer learning",
        "evaluation"
    ],
    "primary_area": "generative models",
    "TLDR": "We propose and implement a framework for answering the question: what does it mean to test if a learner has a world model embodied in it?",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=QtKYYatG3Z",
    "pdf_link": "https://openreview.net/pdf?id=QtKYYatG3Z",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a framework for evaluating whether learning algorithms develop world models using two measurements: i) inductive bias toward \u201cstate\u201d and ii) state recovery. These metrics are used across various model architectures and a handful of domains to highlight that high performance on next-token prediction does not imply inductive bias toward state."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. **Originality**. I think the overall question is interesting.\n\n2. **Quality**. I like that the authors choose simple testbeds to investigate potentially interesting phenomenon. \n\n3. **Clarity**. I think the structural choice of first introducing the theoretical framework then moving to the exact implementation makes sense.\n\n4. **Significance**. The paper provides framework for understanding why models might perform well on pretraining tasks without developing true world modelsI. I like that the paper includes evaluations of multiple modern architectures, and this investigation appears to reveal interesting patterns. For example, Mamba models perform better with state supervision but worse with next-token pretraining vs. transformers (Table 1)."
            },
            "weaknesses": {
                "value": "1. **Lack of clear, grounded definitions.** The paper defines a world model as a mapping from inputs to \u201cstate\u201d (page 1); however, this mapping is clearly insufficient. Without a precise, formal definition of what a world model is and ought to achieve, the two metrics for measuring world model quality are not well justified. For example, a lookup table mapping inputs to states would satisfy this definition but isn't what we typically mean by a \"world model\". As another example, a useful world model might abstract away irrelevant state details while capturing key dynamics. I suggest i) a more precise definition of a good world model and its properties and ii) a discussion of how these properties relate to the extensive world model literature. This lack of definition is confusing given existing definitions in the literature [1-3]. \n\n2. **Confusing writing.** Continuing with W1, the paper uses terms like \"state\" without grounding in established frameworks (e.g., MDPs). This lack of clarity made the paper very hard to follow. I also found it difficult to understand the motivation and premise behind the work.\nLack of engagement with relevant literature. Given the usage of reinforcement learning terminology, I am surprised by the lack of engagement with the relevant literature. For example, this work does not engage with the extensive literature on state abstractions, which seeks to address similar research questions. The state abstraction literature in reinforcement learning [4-7] has extensively studied questions like: i) What makes a good state representation?, ii) When can we compress states while preserving important properties?, and iii) How do different types of state abstractions affect learning and transfer? The \"partial reconstruction of state\" metric, defined in Definition 2.2, seems related to concepts like bisimulation metrics (how can we cluster states that behave similarly?) and MDP homomorphisms (what state mappings preserve the essential dynamics?). \n\n3. **Unnecessary \u201cmathiness\u201d**. Proposition 2.3 is tautological. \n\n4. **Unclear methodological decisions**. I don\u2019t understand the synthetic dataset construction part in Section 2.2. If one constructs a dataset that is generated from some behavior policy, there is no reason that assigning random outputs would map to any meaningful task.\n\n\n5. **Results presentation**. Results lack significance testing or even any error bars. \n\n6. **Wrong format**. The paper is missing the line numbers that are standard in the ICLR template.\n\n[1] Ha, David, and J\u00fcrgen Schmidhuber. \"World models.\" arXiv preprint arXiv:1803.10122 (2018).\n\n[2] Chen, Chang, et al. \"Transdreamer: Reinforcement learning with transformer world models.\" arXiv preprint arXiv:2202.09481 (2022).\n\n[3] LeCun, Yann. \"A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27.\" Open Review 62.1 (2022): 1-62.\n\n[4] Li, Lihong, Thomas J. Walsh, and Michael L. Littman. \"Towards a unified theory of state abstraction for MDPs.\" AI&M1.2 (2006): 3.\n\n[5] Ravindran, Balaraman, and Andrew G. Barto. \"Approximate homomorphisms: A framework for non-exact minimization in Markov decision processes.\" (2004).\n\n[6] Van der Pol, Elise, et al. \"Mdp homomorphic networks: Group symmetries in reinforcement learning.\" Advances in Neural Information Processing Systems 33 (2020): 4199-4210.\n\n[7] Abel, David, David Hershkowitz, and Michael Littman. \"Near optimal behavior via approximate state abstraction.\" International Conference on Machine Learning. PMLR, 2016."
            },
            "questions": {
                "value": "1. Could the authors please clarify and motivate this instantiation of a world model?\n\n2. Could the authors please justify why these choices of metrics are connected to desirable properties of a world model?\n\n3. I don\u2019t think the finding that incorrect board position still yields correct predictions of legal moves is that surprising from the state abstraction perspective. What seems to be happening is that the internal learned representation is lossy; model-based RL typically works with imperfect models that learn sufficient dynamics for good policy learning. In essence, one doesn't need a perfect world model for decision making. Could the authors clarify why this is undesirable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper aims to test if a learner has a world model embodied in it.\nTo assess a learner\u2019s world model, the paper measures its inductive bias when transferring to new tasks and proposes two metrics to measure the inductive bias: inductive bias (IB) and state recovery (SR).\n\nExperiments on five pretrained models (RNN, LSTM, Transformer, Mamba, Mamba-2) in areas where the true world model is known (orbital mechanics, lattice problems and Othello games) show that these models learn bundles of heuristics that enable them to perform well on next-token prediction despite preserving little of state and having poor transfer properties for new problems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper proposes a new procedure to test if a learner has a world model embodied in it by measuring its inductive bias when transferring to new tasks, instead of studying the behavior of fixed models.\nThis procedure provides a novel perspective to investigate whether pretrained models develop world\nmodels.\n2. The test models and scenarios in the experiments are representative, and results are inspiring."
            },
            "weaknesses": {
                "value": "1. Experiment results may not generalize to large pretrained models, such as LLMs, which are pretrained in a vast amount of data and have billions of parameters. I wonder if authors have plans to scale up the experiments to larger models, or if they could discuss potential limitations in generalizing their findings to LLMs.\n2. The title for Table 1 and Table 2 should be more detailed and include result analysis, including key trends that are highlighted in the tables and why the trend exits."
            },
            "questions": {
                "value": "1. Please provide a more explicit explanation of the question \"what does it mean to test if a learner has\na world model embodied in it?\", and how experiment findings relate to this question. And please state the motivation behind this question.\n2. What are the advantages of the proposed measuring procedure over other studies to assess whether pretrained models develop world models (e.g., studying the behavior of fixed models)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new framework to evaluate whether the learning algorithms utilizes world models to make predictions. Basically, it develops two metrics that captures the model's world modeling capabilities, one is inductive bias towards world state when it transfers into new tasks; the other is the degree to which a learner recovers state, such as whether the new prediction function utilizes information of all the state, or some aspects of them. Experiments are done under controlled setup, the results are kind of interesting, it shows that although most of the models perform well on next-token predictions, they have poor inductive bias towards world state, hence transfer properties, and the authors hypothesized that the models might rely on bundles of heuristics."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow. The evaluation framework is described in a clear manner, from the definition of inductive bias and state recovery, towards on how to measure them via the lens of transfer learning, etc.\n- This paper does show some valuable insights on whether or not the models are utilizing world models to do the prediction, I do find the empirical observations about the model's limited inductive bias and poor transfer properties interesting. For example, the case listed in Bundles of Heuristics about the model does not need to understand the board correctly in order to make the legal moves is kind of surprising. \n- The controlled synthetic experiments is well-done and it shows very clear evaluation and understanding on the world-modeling capabilities of current models that good at next-token prediction."
            },
            "weaknesses": {
                "value": "- I\u2019m curious whether the measurement of inductive bias is highly dependent on the distribution of the training data. For instance, if the data distribution is more uniform and not covers only a small subspace, would this significantly alter the conclusions? In the case of orbital mechanics, would a model trained on a more uniform distribution still capture universal laws?\n- In the cases of Lattice and Othello, do the ground-truth states represent the minimal information needed for next-token prediction? I\u2019m particularly considering the results shown in Figure 2.\n- I appreciate the 'empirical' evaluation framework, but it seems to depend on various factors, like the IB loss function, reconstruction loss, transfer setup, data distribution, and so on. How sensitive are the conclusions in the paper to these parameters, and how well do they generalize?\n- Could you comment more on how these world models connect with the memorization and reasoning concepts in the LLM community if possible?\n\n(Note: I\u2019m not an expert in world models; I\u2019m reading this from a general perspective.)"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents new methods to study whether a trained model\u2019s performance is made possible by an internal \u201cworld model\u201d, meaning representations of the underlying \u201cstate\u201d which generates the data. For example, in their gravitational trajectories example, the data itself is the observed position of the planets at different time-steps, while the underlying state includes their velocities and masses (which determine future positions). To that end, the authors present and motivate two metrics (IB and SR) meant to capture the extent to which a learned model\u2019s behavior can be understood as depending only on the (unseen) states, and as depending on all such states (instead of a coarser classification). Experiments are run on several different sequential datasets with a known underlying state, testing how different architectures score on these metrics against random baselines, as well as a baseline directly trained on state observations. In some cases, other methods are used to interpret model behavior, like regression. The authors indeed find that different architecture and task pairs score differently on these metrics, and discuss their interpretation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The topic is pertinent and of broad scientific interest and the work is ambitious. They propose a set of concepts and formal metrics that (to my knowledge) are novel and are a potentially valuable contribution. These metrics are connected to empirical experiments in a potentially fruitful way. The baselines seem reasonable. \n\nThe discussion of related work seems adequate (although it would be hard for me to tell whether it\u2019s missing something important). The orbital mechanics domain is elegant and seems like a good toy domain for future work. The other domains have appeared in previous related work, and so seem like a reasonable choice. The empirical observation in Section 4.3 that the learned model fine-tuned on state prediction is correct about legal moves but not the whole board is interesting."
            },
            "weaknesses": {
                "value": "Unfortunately, I believe there are several important weaknesses with the paper\u2019s main contributions (which might or might not be easy to address), as well as its presentation.\n\nThe main contribution of the paper are the metrics defined in Section 2.1. While their motivation is understandable, it is not immediately clear, nor in my opinion sufficiently justified, why these metrics are tracking what the authors propose they are tracking (i.e. the extent to which a model\u2019s internal mechanisms correspond to the world-model generating the data). While the binary definitions (2.1 and 2.2) make intuitive sense (modulo some presentation problems which I discuss below), there\u2019s no explanation for why their quantitative extension is appropriate for the author\u2019s purposes. For example, even when dividing by the random baseline, it is hard to interpret what different scales correspond to. For example, whether a 0.6 IB represents, intuitively speaking, a model clearly using a pretty accurate state-model (albeit still short of a perfect one), or instead a 0.6 IB can be attained \u201cquite easily\u201d even when the model is just using a bunch of heuristics, and very far away from having a coherent world model. Of course, I understand the results in Tables 1 and 2 are interpreted as evidence that these metrics are tracking the desired concepts. But I do think deeper discussion is required on why this intuition is warranted, and what these metrics in particular add to the picture. For example, on the gravitational orbits experiments, the authors also turn to symbolic regression to elucidate whether the model can or cannot be understood as comprehensively applying the generating laws, and this seems like a much more straightforwardly interpretable result.\n\nOn a similar note, my understanding is that the calculated metrics only provide potentially interesting information when the states used to compute them are the real states used to generate the data. This of course reduces the direct applicability of this technique in cases of particular importance, like the LLM-related examples which the authors present as motivation. The technique might still be useful in several contexts or more indirect ways, but I\u2019d like to see more explicit discussion of them -- otherwise the reader might be led to believe they don\u2019t exist.\n\nThe settings the authors choose for their experiments make sense, but my understanding is that they are not necessarily an important contribution: we don\u2019t really care that much whether a specific architecture, run on a setting with a specific encoding, does or doesn\u2019t recover a particular functional form. Rather, they serve mostly as exemplifications and explorations of the use of the proposed metrics, which are indeed the main contribution (and are ultimately aimed at applications to practical models of the kind referred to in the opening paragraph). \n\nThe observation in Section 4.3 that models reconstruct the board well enough to match legal moves, but not perfectly, is very interesting, and hints towards some frugal, partial construction of a world-model shaped by the immediate training incentives. Especially since this is so interesting, it would have been useful to include in the main text a more detailed discussion on what it could mean, as well as possibly point to empirical follow-ups in settings other than Othello. (Such an addition is not possible at this stage). \nFinally, I also have some major reservations about mathematical presentation and writing, which I present below."
            },
            "questions": {
                "value": "I start with some other major conceptual considerations:\n\nThe mathematical presentation in Section 2 seems convoluted and unclear.\nIf my understanding is correct, it can be easily proven that Definition 2.1 is equivalent to a very simple mathematical property: m decomposing through /phi (in the support of P). That is, the existence of a function f such that m = f o /phi. Stating this explicitly as the definition would be easier for the reader, as opposed to going through s*. On a similar note, the name \u201cinductive bias towards state\u201d seems like a strange one for this property, especially because it sounds quantitative (L could have more or less inductive bias) when the definition is in fact binary. It would seem better to call the definition \u201cL learns functions of state\u201d (or something like that), and reserve discussions of inductive bias for the latter quantitative versions.\n\nIf my understanding is correct, it can be easily proven that Definition 2.2 is equivalent to another very simple mathematical property: the fact that the function f from the previous definition is not injective. This again is easier to understand. Here again the name doesn\u2019t seem as representative as it could be. To me, it would seem more intuitive to name the positive property (f being injective) as \u201cL fully uses state\u201d (or \u201cfully separates states\u201d, or something like that), and then refer to the negation as \u201cL doesn\u2019t fully use state\u201d.\nIn fact, these names seem not only like suboptimal choices, but also possibly misleading. Consider, for example, a learning algorithm L that, for any dataset D, creates a model m which maps every input to a single constant output. This would satisfy Definition 2.1. Yet it seems misguided to say that this useless L has \u201cinductive bias towards state\u201d. I think similarly counterintuitive edge cases exist for Definition 2.2.\n\nI understand you might want to introduce other terms, like s*, to make your quantitative definitions in Section 2.1 possible. But in any event this should come in Section 2.1, after the simple binary definitions are transparently expressed. Although I also suspect there could be cleaner ways to phrase them that become obvious once you rewrite Definitions 2.1 and 2.2 as above, or even better quantitative definitions altogether.\n\nOn a different important note, in several places the authors mention that this paper studies whether a \u201clearner\u201d has a world-model, which sets the work apart from more extensive existing work on whether a fixed model has learned a world-model. Nowhere in the text is it particularly clear what they mean by this, but my interpretation is that their metrics are defined for learning algorithms L (plus distributions over data Q, etc.), as opposed to a single learned model m. If this is indeed the motive for their words, then I\u2019m not sure this really grants a substantive distinction, or sets the work apart from other work in an important way. The reason is that the interesting parts of their definitions all pertain to a single model m, trained on a single dataset D. And then, they simply average in a certain way over datasets D to come up with a metric for the whole learning algorithm L. While this is a natural step, it doesn\u2019t seem like a substantive contribution, and I would have appreciated more thorough discussion of how this new level can open new possibilities for the study of world-model construction.\n\nOn another note, Section 3 mentions that (for the models trained on the general dataset as opposed to the narrow slices) the law recovered through regression is not the generating one, and this implies that \u201cthe model extrapolates based on piecemeal heuristics; it constructs different laws for different sequences\u201d. This implication doesn\u2019t seem justified enough in the main text. Most notably, how do the authors know that the reason regression finds the wrong law is that \u201cthe model constructs different laws for different sequences\u201d, as opposed to something like, \u201cthe model constructs the same law for all sequences, but it is simply the wrong law\u201d? The fact that training on narrow slices does recover the correct law doesn\u2019t seem to imply the former over the latter. I can imagine that through experimentation, or also through observing the next-token performance of these models, the authors have developed robust intuitions about which of the two is happening. But if so they are not sufficiently transmitted in the main text.\n\nNow a less important conceptual consideration:\nGiven your observation that models trained on the general dataset for Orbital Mechanics don\u2019t recover the correct law, it would be positive to include some discussion of the extent to which such architectures can computationally reproduce such laws. That is, the extent to which this failure is due to inductive training bias, or to fundamental limitations of the computational architecture. I understand state-trained models provide some (possibly conclusive) evidence in the direction of \u201cthe model architecture can perfectly compute the correct law in a forward pass if the learned model is the correct one\u201d. Or maybe it is obvious that the architecture can implement this computation, or some existing literature has studied this in-depth. But I would have benefitted from making this explicit.\n\nFinally, some stylistic points:\nIn a few places (mostly the introduction), the authors mention they study world-modelling capabilities along two distinct dimensions: \u201cinductive bias towards state\u201d and \u201crecovering of state\u201d. Of course, these terms refer to Definitions 2.1 and 2.2, but their intuitive meaning was not at all clear to me from reading just their short descriptions in the introduction (and not even immediately upon reading the Definitions, as I\u2019ve explained above), and again I believe they could be made immediately precise by just using the right words: whether the learned model decomposes through states, and whether it makes use of all state distinctions.\n\nIn a few places the authors mention new functions (for example, \u201chow much do new functions depend solely on state versus non-state functions of input?\u201d), but to my understanding these \u201cnew functions\u201d are simply \u201cthe learned model\u201d, and should better be called that.\nIt seems unnecessary to introduce the variable L, since m\\hat ( \u00b7 ; \u00b7 ) can already be seen as the learning algorithm, which provides a learned model m\\hat( \u00b7 ; D) for every dataset D, which provides an output m\\hat(x ; D) for every input x.\n(If for some reason the authors don\u2019t want to incorporate this change, then at least they should introduce the variable L earlier, when the term \u201clearning algorithm\u201d is introduced.)\n\nThe variable you introduce for the state space is /Phi, but you later use \\mathcal{Q}, for example in equation 1 and Definition 2.2.\nDefinitions 2.1 and 2.2 depend on /phi, and this should be made explicit in their names (for example, \u201cinductive bias towards state /phi\u201d).\nThe quantitative metrics should be definitions, since they are more central to the paper (or non-trivial) than the binary definitions 2.1 and 2.2.\n\nA large value of IB(Q), that is, a high \u201cinductive bias towards state\u201d, indicates that the learned model cannot be well-approximated as decomposing through state. This is intuitively backwards: a high \u201cinductive bias towards state\u201d intuitively corresponds to the model being likely to behave in terms of those states, and not the other way around.\n\nI understand that the function /phi is supposed to be the \u201creal\u201d state function throughout, meaning the one actually used to generate the data. This is especially obvious in the experimental sections, but should somewhere be mentioned explicitly, especially because otherwise the definitions in Section 2 don\u2019t necessarily have the same intuitive meaning or motivation that you discuss.\nIn a couple places, you introduce the Orbital mechanics setting with the sentence \u201cThe true world model is the world\u201d, meaning, of course, that the data is generated from Newton\u2019s law, and this is a physical law that approximately describes the real world. But I find this sentence doesn\u2019t add anything, adds unnecessary pomp, and in fact slightly misleads: my first reaction to such a sentence (especially given your motivating examples with GPT-2) was that you would have found a way to extract world models from \u201cmessy, complex, real-world data\u201d (like GPT-2\u2019s training corpus). But that is not the case, and your use of Orbital mechanics as an experimental setting is not necessarily different or more interesting than other latent-function-approximation tasks.\n\nProbably the footnote of Figure 1 should quickly mention how these laws are extracted from model behavior, which is, of course, through symbolic regression.\nFinally, there are numerous infelicities or mistakes in the text, and I present here just a few representative ones:\n- First sentence in the introduction is awkwardly phrased\n- End of page 1: \u201cstudied by studying\u201d.\n- The introduction seems too short to make some of the ideas clear (see especially my comment above about the descriptions of \u201cinductive bias towards state\u201d and \u201crecovering state\u201d).\n- When defining what it means for a dataset D to be consistent, you say \u201cwith_then_\u201d, which should instead be either \u201cif_then_\u201d or \u201cwith_we have_\u201d.\n- Introducing /epsilon is probably not necessary.\n- Right before Section 2.1, you say \u201cs* has an inductive bias towards state\u201d, which should be \u201cL\u201d instead.\n- Before equation 3: \u201cfor any chosen distribution over Q over dataset D\u201d.\n- \u201cBuild multi-task learner to model extrapolations\u201d -- typo in first sentence of this paragraph\n- \u201cA symbolic regression is a method\u2026\u201d typo in this sentence\u201d\n- \u201cpiece-meal\u201d -- inconsistent use of hyphen\n- In the reproducibility statement: \u201cAll other datasets used in the paper already publicly available.\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}