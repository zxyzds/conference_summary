{
    "id": "xI71dsS3o4",
    "title": "(Mis)Fitting Scaling Laws: A Survey of Scaling Law Fitting Techniques in Deep Learning",
    "abstract": "Modern foundation models rely heavily on using scaling laws to guide crucial training decisions. Researchers often extrapolate the optimal architecture and hyper parameters settings from smaller training runs by describing the relationship between, loss, or task performance, and scale. All components of this process vary, from the specific equation being fit, to the training setup, to the optimization method. Each of these factors may affect the fitted law, and therefore, the conclusions of a given study. We discuss discrepancies in the conclusions that several prior works reach, on questions such as the optimal token to parameter ratio. We augment this discussion with our own analysis of the critical impact that changes in specific details may effect in a scaling study, and the resulting altered conclusions. Additionally, we survey over 50 papers that study scaling trends: while 45 of these papers quantify these trends using a power law, most under-report crucial details needed to reproduce their findings. To mitigate this, we we propose a checklist for authors to consider while contributing to scaling law research.",
    "keywords": [
        "survey",
        "scaling laws",
        "large language models",
        "foundation models"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We survey over 50 papers on scaling laws and discuss how systematic underreporting of details can change the conclusions of a paper",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=xI71dsS3o4",
    "pdf_link": "https://openreview.net/pdf?id=xI71dsS3o4",
    "comments": [
        {
            "summary": {
                "value": "This paper surveyed/ more than 50 papers about the scaling law of language models. Authors discussed different aspects of scaling law including fitting forms, model training, data extraction, and fitting optimization. Based on that, authors provided a checklist, which helps to transparent settings for reproducible results in future research. Experiments also were conducted to verify their replication and analyses."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper has several strengths:\n- This paper considers a timely topic, the scaling law of language models. Understanding this topic will help to effectively train LLMs, avoiding resource overuse. \n- Authors discussed the discrepancies in experiment settings of different papers and empirically verified it. Results are aligned with previous works.\n- Authors open-sourced their code to reproduce results which benefits the community since the source code is usually absent from previous papers."
            },
            "weaknesses": {
                "value": "Despite these strengths, this paper has several weaknesses:\n- The scale of the model in experiments is not big enough. Authors consider only models with less than 400M params, ignoring the existence of larger models with billions of params. \n- The writing in some parts of the main paper causes confusion. E.g., Section 5 is about data extraction after training, I was confused by which kind of data could be extracted."
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors survey a large corpus of papers that involve scaling laws, and find that many papers underreport necessary details for reproducibility, which they demonstrate with experiments that demonstrate a large variability depending on those exact choices of details. They propose a checklist for authors to consider when publishing scaling laws."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper gives a good overview of many papers on scaling laws, and nicely categorizes the important steps: functional form, training setup, data(points) extraction, curve fitting. The checklist provides a clear way of reproducibility and quality assessment of scaling experiments. I think the topic of scaling law studies is important and relevant, and the writing is clear."
            },
            "weaknesses": {
                "value": "The main concern for me is the following: what is the main goal the authors are trying to convey? To me, there are two obvious takeaways, which is 1) changes to the scaling law setup can change the results drastically, and 2) previous papers very much underreport crucial details. However, both of these things are rather clear already to the community and also illustrated by published papers: for example, point 1) is shown by Porian et al., and point 2) is a broader critique of reproducibility problems, which (unfortunately) is a generic problem. I do not see a clear and actionable interpretation beyond that. For instance, how do the different choices of fitting actually affect the scaling laws? (The assessment is mostly just \u201cthe results vary dramatically\u201d \u2014 but how?) What should I as a researcher now do for my future scaling studies, having read your paper, beyond using the checklist? Are there clearly \u2018wrong\u2019 or \u2018right\u2019 choices? Was there a most predictive scaling law (e.g. when you leave out some experiments as a validation set)?\n\nTo be clear, I very much believe there is merit in a survey or pointing out these problems; as it stands, however, the paper is foremost \u201cjust\u201d a survey, and I am not convinced this merits publishing at the conference.\n\nSome additional comments: \n * The paper template says ICLR 2024\n * The Figures are unfortunately of low quality (very pixelated), especially considering the fact that it\u2019s natural to zoom in to compare the many lines and details. I suggest the authors include the pdf forms for proper rendering."
            },
            "questions": {
                "value": "I have already listed direct questions in the section above, and I would be open to discuss this in the rebuttal. I hope the authors see the comments to be constructive, and can clarify or improve the distinctive value of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work revisits scaling laws and factors influencing the results reported in recent papers. The paper takes a \"review approach\", outlining some of the debates in recent literature as well as common strategies. To draw conclusions and emphasize their points, the last pages are dedicated to an in-depth analysis by the authors on small to moderate size transformers."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "I really like this paper and think it is of great value for the community to \"recap\" scaling law results and provide a critical discussion, complemented by experiments showing which factors matter when choosing a scaling law. It was really a pleasant read, quality of writing is good and the motivation clear."
            },
            "weaknesses": {
                "value": "One could maybe claim the paper is not too constructive, as it shows that choices (optimizer, fitting method, lr annealing, data) matter when fitting a scaling law: there is no correct answer. However, this conclusion demystifies the topic, which I like very much: there is no magic, just common choices and \"usual\" results. This said, there are a couple of very minor points.\n\n1) Proposing a checklist is helpful, but, as the authors themselves seem to hint, the number of factors to account for is potentially infinite. What about Adam beta2? What about weight decay? What about hybrid algorithms? What about qk norm and new tricks? The reality this paper points out is that, indeed, such choices matter, and I do not think any checklist can be conclusive.\n\n2) section 7.1: why did you decide to set alpha=beta?\n\n3) The paper lacks a bit of conclusions: what should researchers do? should we trust scaling laws? what are the things that hold true despite changing the setting? Is there some practical rule for scaling that holds approximately in your experiments? (would have been interesting alpha and beta)\n\ntypo spot: \"was was\" in the abstract, repetition."
            },
            "questions": {
                "value": "-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a survey on the fitting of scaling laws, and argue that current practices are lacking in scientific rigor. Apart from an extensive survey, the authors presents a reproducibility checklist, and compare 51 papers to this checklist. They generally find that important details are underreported, e.g. the method to calculate model parameters might not be given. They also provide a replication study of Hoffman, using data extracted from the paper PDF and data they\u2019ve collected themselves. Here they find that subtle choices in the curve fitting can result in significantly different conclusions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Scaling laws is an important topic, and scientific rigor here can benefit the research community.\n- The authors illustrate how subtle choices in the curve fitting can cause significant results\n- Section 7 is great."
            },
            "weaknesses": {
                "value": "- Significant parts of the papers are dedicated to a survey. I\u2019m not sure survey papers are the right fit for ICLR main track.\n- There are not so many empirical results."
            },
            "questions": {
                "value": "- could you provide explicit recommendation regarding how to perform the curve fitting? I think this is different from a checklist which allows reproduction. \n- Could you expand section 7?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}