{
    "id": "vqbd2OQnGp",
    "title": "Knowledge And Capability Transfer Through Large Language Models' Parameters Fusing",
    "abstract": "The post-training phase of large language models (LLMs) plays a pivotal role in refining models to follow instructions and align with human preferences. However, this phase is fraught with challenges, particularly in sourcing high-quality post-training data. This paper introduces a novel approach, termed Parameters Fusing, that simplifies the post-training process by amalgamating model parameters delta from existing instruct-tuned checkpoints with a new base model tailored to specific domain data obtained by continual pre-training. Utilizing open-weight models such as Meta's Llama, our method replicates the effects of the traditional post-training phase while significantly reducing both time and resource costs. Moreover, it facilitates the customization of model attributes (e.g., tool usage, instruction-following, coding proficiency, and tonal qualities) by adjusting parameter deltas from multiple checkpoints. This approach not only minimizes the challenges of post-training data acquisition but also provides a flexible and efficient framework for enhancing LLMs with domain-specific knowledge or capabilities.",
    "keywords": [
        "large language model",
        "post-training",
        "transfer learning",
        "model merging",
        "weights averaging",
        "artificial intelligence"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "An innovative methodology that effectively replicates the entire post-training process by integrating model parameters delta from existing checkpoints",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vqbd2OQnGp",
    "pdf_link": "https://openreview.net/pdf?id=vqbd2OQnGp",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces an innovative approach for post-training large language models (LLMs) through \"Parameters Fusing,\" a method that fuses model parameters from instruct-tuned checkpoints into a newly pre-trained model. The goal is to replicate post-training effects without the extensive time and resource costs typically required. By leveraging parameter deltas, the authors enable the efficient transfer of domain-specific knowledge and model capabilities, showcasing the model's ability to maintain or enhance performance across multiple benchmarks. Experiments validate that fusing models can rival or even exceed the effectiveness of traditional post-trained models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper clearly explains the challenges of post-training and the need for efficient knowledge transfer, establishing a strong foundation for the introduction of Parameters Fusing.\n- The \"Parameters Fusing\" approach is a creative and resource-efficient alternative to conventional post-training, presenting a valuable technique for the efficient transfer of knowledge in LLMs.\n- The paper includes rigorous experiments across multiple benchmarks, which provide clear empirical support for the proposed method's performance and efficiency.\n- By using open-weight models like Llama, the authors demonstrate an adaptable approach that can be widely applied across different models and domains.\n- The paper offers a well-structured theoretical grounding, discussing the relationships among model parameters, training steps, and knowledge acquisition."
            },
            "weaknesses": {
                "value": "- The study could benefit from comparisons with other parameter-efficient methods in addition to traditional post-training, such as adapter-based or LoRA methods, to contextualize its performance and efficiency.\n-  It is unclear if Parameters Fusing will perform as effectively on larger models. Expanding the analysis to address scalability and potential limitations in diverse applications would strengthen the paper.\n- While the paper focuses on Llama models, it does not fully address whether the approach is model-agnostic or if any adjustments would be necessary for different architectures.\n- The approach may introduce a risk of overfitting in highly specialized domains. Including an analysis of model generalizability when exposed to new or unseen tasks would improve the robustness of the findings.\n- Although Parameters Fusing is efficient, there is limited discussion about interpretability and potential risks (e.g., model degradation) when applying delta parameters from various sources."
            },
            "questions": {
                "value": "- There is minimal discussion on the risks of model degradation when fusing parameters from multiple sources, especially when domain mismatches or conflicting knowledge bases are involved. Investigating and reporting any observed performance declines, conflicts in fused knowledge, or mitigation strategies would strengthen the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel post-training approach termed \"Parameters Fusing\" designed to simplify the transfer of knowledge and capabilities in large language models (LLMs) during the post-training phase. Traditional post-training requires extensive high-quality data and significant resource consumption. This research innovatively achieves the effects of the post-training phase by merging parameter deltas from existing instruct-tuned models with a newly pre-trained base model, thereby enhancing instruction-following capabilities and domain-specific knowledge without conventional post-training."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tInnovation: The \"Parameters Fusing\" approach leverages parameter deltas to achieve post-training effects, representing an innovative advancement over traditional methods which requires high-quality training data.\n2.\tCost effectiveness: This method significantly reduces post-training costs, making model customization more economical and efficient.\n3.\tFlexibility: Parameter delta operations allow freedom within homologous models, enabling fine-tuning across characteristics like coding ability and tool usage.\n4.\tExperiments: Experimental results show that fused models perform excellently across benchmarks, approaching or even exceeding traditional post-trained models, validating the method's effectiveness."
            },
            "weaknesses": {
                "value": "1.\tPotential Performance Limitations: In some benchmarks, fused models slightly underperform compared to traditional post-trained models, indicating potential limitations in transfer efficiency.\n2.\tExperimental Transparency: Certain experimental details, particularly criteria for choosing different parameter delta combinations and the implementation process, are insufficiently detailed, potentially affecting reproducibility.\n3.\tLack of Adaptive Delta Selection: The method relies on manual tuning of delta combinations, which increases costs and limits flexibility. An adaptive mechanism for delta selection would enhance efficiency and usability."
            },
            "questions": {
                "value": "Please refer to the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel approach, parameter fusing, which simplifies knowledge and capability transfer in large language models (LLMs) by integrating parameter deltas\u2014the differences between instruct-tuned and base model checkpoints\u2014into a new base model. This technique allows LLMs to incorporate specialized skills or domain-specific knowledge without the need for resource-intensive post-training phases. Parameter Fusing is grounded in the observation that performance improvements correlate with a concave relationship to changes in parameters, suggesting diminishing returns as models approach an optimal performance plateau. This relationship was validated through comprehensive experiments, showing that parameter fusion not only matches but can sometimes enhance the effects of traditional post-training. By leveraging open models, such as Meta\u2019s Llama, this method enables efficient and flexible customization of LLMs, significantly reducing costs and time associated with conventional fine-tuning while ensuring adaptability for diverse applications."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "This work builds on prior research in parameter aggregation but offers fresh insights and significant contributions. Notably, it presents an intriguing hypothesis that links performance gains to parameter changes\u2014a relationship convincingly supported by experimental results. Beyond its theoretical contributions, the paper demonstrates a practical application for its proposed Parameter Fusing approach: when LLMs require continual pretraining to acquire specialized skills or domain-specific knowledge, Parameter Fusing offers a resource-efficient alternative to traditional post-training. The experimental outcomes are promising, validating the method's effectiveness. Overall, this paper introduces a novel perspective on post-pretraining, with potential for wide-reaching applications in future research. It is poised to make a meaningful impact on the LLM research community."
            },
            "weaknesses": {
                "value": "My major concern is that there lacks a quantitative evaluation to evaluate if the new knowledge in a continual pretrained model will be preserved in the fused model. In the current experiments, this validation is achieved by showing merely one example in Table 4. More concrete results should be provided in the main experiment section."
            },
            "questions": {
                "value": "When fusing parameters from different checkpoints, is there any criteria that can be used to select the most effective parameter deltas?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to use the change of model parameters for representing the knowledge learned by LLMs. The core idea is to perform a weight averaging operation for pre-trained and post-trained model parameters. It is discovered that such weight averaging leads to comparable results."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The idea itself is interesting;\n\n- The method is straightforward, easy to follow.\n\n- The results provide some insights on how to transfer pre-trained LLMs to a new task/domain: if we have a pretrained LLM $f$, and two other checkpoints, one ($g_1$) is pretrained, the other ($g_2$) is post-trained on the new domain, then we can adapt f to this new domain by $f + (g_2 - g_1)$."
            },
            "weaknesses": {
                "value": "- The method itself is simple, but the presentation needs substantial improvements. Now the presentation makes the paper seem complicated. The core ideas are clear and easy to follow, but the writing is confusing with so many long subscriptions in equations. For example, $\\theta_{model_i-pretrain}, \\theta_{post-train-llama3.1-8b}$ are redundant expressions, making readers more confusing.\n  \n- Moreover, the figures are so small. The x and y labels are hard to see. It is highly recommended that the authors improve the representation of equations, and provide a straightforward illustration of their method by figures. This is also an effect of too long subscriptions.\n\n- The empirical improvements are marginal (Figs 1, 2, Tabs 1, 2). The current results fail to provide useful insights or surprising ovservations. It is recommended that the authors show some scenarios where existing post-trained models cannot achieve very good results, yet the proposed method easily outperform them with simple parameter fusing."
            },
            "questions": {
                "value": "- To my understanding, the technical novelty is limited. Parameter fusing is performed via valinna operations. Simplicity is a strength, but the technical novelty is lacking, given that the obtained results are not very promising (the improvements are marginal). Although I'm not an expert on LLMs, it could be easily found that the method requires naive addition/subtractions on the whole model parameters. Therefore, I cannot accurately assess the value of the proposed parameter fusing approach. It is recommended that the authors elaborate on how their approach differs from or improves upon existing parameter fusion techniques in the context of LLMs.\n\n- What if $f$ and $g_1$ are pre-trained on different domains (the notations is from the strength part)? Does the method assume that both of them have already be pre-trained on a variety of data, and share some common knowledge?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}