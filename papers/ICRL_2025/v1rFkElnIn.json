{
    "id": "v1rFkElnIn",
    "title": "Decoupled Subgraph Federated Learning",
    "abstract": "We address the challenge of federated learning on graph-structured data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where inter-connections between different clients play a critical role. We present a novel framework for this scenario, named FedStruct, that harnesses deep structural dependencies. To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients. Instead, it leverages explicit global graph structure information to capture inter-node dependencies. We validate the effectiveness of FedStruct through experimental results conducted on six datasets for semi-supervised node classification, showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of clients.",
    "keywords": [
        "Federated Learning",
        "Subgraph Federated Learning",
        "Inter-Connected Graphs",
        "GNN",
        "Decoupled GCN"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=v1rFkElnIn",
    "pdf_link": "https://openreview.net/pdf?id=v1rFkElnIn",
    "comments": [
        {
            "title": {
                "value": "Weaknesses 2 and 3"
            },
            "comment": {
                "value": "**Weakness 2:**\n\nAs discussed in our paper, any subgraph federated learning (SFL) method should be evaluated by balancing the communication-privacy-accuracy trilemma.  At one extreme,  disregarding communication and privacy concerns entirely would mean transmitting all node features and connections to a central server, where a centralized GNN could achieve high accuracy. Conversely, focusing solely on privacy and communication cost would result in each client  training a local model in isolation, yielding  poor performance. Thus, an effective SFL framework must strike a careful balance to achieve good performance while minimizing privacy leakage and reducing communication cost.\n\nFedGCN is not a private framework, as it involves sharing aggregated node features across clients (aggregating node features does not provide privacy, as recently shown in [1]). \nThis privacy leakage is exacerbated by the structured nature of node feature vectors, where each entry has a meaningful value. For instance, in the Cora dataset, each node (a paper) is represented by a 1,433-dimensional binary vector, where each entry indicates the presence or absence of a specific word in the paper (node). As a result, even receiving the summation of several vectors can enable a client to infer individual entries in other clients\u2019 node features.\n\nIn the 2-hop FedGCN, each node is also required to send its list of neighbors to external neighbors (neighbors located on other clients) to compute the necessary summations. This process is detailed in Equation 3 on page 5 of the FedGCN paper (we elaborate more on this in the answer to your Weakness 3).\n\nWhile Fedsage+ provides more privacy than FedGCN, it still involves sharing node feature embeddings with other clients, which makes it less private than FedStruct. \nFedsage+ slightly outperforms FedStruct only on the Amazon Photo dataset, with a difference of approximately $1\\\\%$. \nOn the other 5 datasets, however, FedStruct significantly outperforms Fedsage+.  For example, for Cora and Chameleon, the improvement is $14\\\\%$ and $13\\\\%$, respectively.\n\nAlso, as shown in Table 2, methods like FedSGD, which do not leverage interconnections between clients, already perform close to a centralized approach on Amazon Photo, leaving little room for improvement. Consequently, for this dataset, the advantage of utilizing external nodes is less pronounced compared to other datasets.\n\n**Weakness 3:**\n\nThank you for your comment. We recognize the need to clarify this aspect further. \n\nIn the FedGCN scheme, as detailed in Equation 3 on page 5 of the FedGCN paper, to calculate $\\boldsymbol{\\hat{y}}_i$, the following information must be sent to the server by Client $z$:\n\n\\begin{align}\n\\begin{cases}\n    \\boldsymbol{h}\\_{zi} = &\\sum\\_{j \\in \\mathcal{N}\\_i}{\\mathbb{I}\\_z(c(j)) \\boldsymbol{A}\\_{ij} \\boldsymbol{x}\\_j}\\\\\\\\\n     \\boldsymbol{h}\\_{zj} = &\\sum\\_{m \\in \\mathcal{N}\\_j}{\\mathbb{I}\\_z(c(m)) \\boldsymbol{A}\\_{jm} \\boldsymbol{x}\\_m} \\quad \\forall j \\in \\mathcal{N}\\_i \\setminus i.\n\\end{cases}\n\\end{align}\n\nOnce  the server obtains $\\boldsymbol{h}\\_{zj}$ for all clients $z\\in [K]$, it calculates the aggregate $\\boldsymbol{h}\\_{j} = \\sum\\_{z\\in [K]} \\boldsymbol{h}\\_{zj}$ and forwards this result to the client where node $i$ resides. \nHowever, as $\\boldsymbol{h}\\_{j}$ is a quantity pertaining to node $j$, the server must know that $i$ and $j$ are connected. \nSimilar arguments hold for all other nodes. Hence, the server needs access to the global adjacency matrix for this scheme to work with 2 hops.\n\nTo clarify this point in response to your comment, we have provided this explanation in Appendix C.4 of the paper and referenced it in the main text (Line 460).\n\n[1] Ngo, K.H., \u00d6stman, J., Durisi, G. and Graell i Amat, A., 2024, August. Secure Aggregation Is Not Private Against Membership Inference Attacks. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 180-198). Cham: Springer Nature Switzerland."
            }
        },
        {
            "title": {
                "value": "Response To Weakness 1 (Part 2)"
            },
            "comment": {
                "value": "**Weakness 1 (Part 2):**\n\nFollowing Equation 32 in the paper, we define the intermediate matrix $\\boldsymbol{B}_{ijk} = \\boldsymbol{\\tilde{A}}^{[i]}_k \\boldsymbol{\\hat{A}}^{[k]}_j$. \nThe following steps outline the remaining computations:\n\n1. **Calculating $\\boldsymbol{B}\\_{ijk}$**\n    \nClient $1$ calculates $\\boldsymbol{B}\\_{211},\\, \\boldsymbol{B}\\_{221},\\,\\boldsymbol{B}\\_{231}$ as follows\n\\begin{align}\n\\boldsymbol{B}\\_{211} = \\begin{bmatrix}\n0 & 0 & 1 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n\\end{bmatrix}\\quad\n\\boldsymbol{B}\\_{221} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n\\end{bmatrix}\\quad\n\\boldsymbol{B}\\_{231} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n\\end{bmatrix}\n\\end{align}\n\nClient 1 sends these matrices to Client $2$. Notice that Client $2$, by receiving $\\boldsymbol{B}\\_{211},\\, \\boldsymbol{B}\\_{221},\\,\\boldsymbol{B}\\_{231}$, cannot reconstruct $\\tilde{A}_1^{[1]},\\, \\tilde{A}_3^{[1]}$ and $\\tilde{A}_1^{[3]}$ due to the low-rank nature of the adjacency matrix. \nMoreover, Client 1 also prunes $\\boldsymbol{B}\\_{ijk}$ to remove any concern. \n\nSimilarly Client $3$ calculates $\\boldsymbol{B}\\_{213},\\, \\boldsymbol{B}\\_{223},\\,\\boldsymbol{B}\\_{233}$ as\n\\begin{align}\n\\boldsymbol{B}\\_{213} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n\\end{bmatrix}\\quad\n\\boldsymbol{B}\\_{223} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n\\end{bmatrix}\\quad\n\\boldsymbol{B}\\_{233} = \\begin{bmatrix}\n0 & 0 & 1 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n\\end{bmatrix}\n\\end{align}\nwhich are sent to the Client $2$ after pruning.\n\nWe follow a similar procedure for the other combinations of $\\boldsymbol{B}\\_{ijk}$, as outlined in Algorithm 3.\n\n2. **Calculating $[\\boldsymbol{\\hat{A}}^{[i]}_j]^{2}$**\n\nUpon receiving $\\boldsymbol{B}\\_{211},\\, \\boldsymbol{B}\\_{221},\\,\\boldsymbol{B}\\_{231}$ and $\\boldsymbol{B}\\_{213},\\, \\boldsymbol{B}\\_{223},\\,\\boldsymbol{B}\\_{233}$, Client $2$ can compute  $[\\boldsymbol{\\hat{A}}^{[2]}_1]^{2},\\,[\\boldsymbol{\\hat{A}}^{[2]}_2]^{2},\\,[\\boldsymbol{\\hat{A}}^{[2]}_3]^{2}$ as\n\\begin{align}\n    [\\boldsymbol{\\hat{A}}^{[2]}\\_1]^{2} = (\\boldsymbol{\\tilde{D}}^{[2]})^{-1} \n    \\left( \n    \\boldsymbol{B}\\_{211} + \\boldsymbol{B}\\_{212} + \\boldsymbol{B}\\_{213}\n    \\right)\\\\\\\\\n    [\\boldsymbol{\\hat{A}}^{[2]}\\_2]^{2} = (\\boldsymbol{\\tilde{D}}^{[2]})^{-1} \n    \\left( \n    \\boldsymbol{B}\\_{221} + \\boldsymbol{B}\\_{222} + \\boldsymbol{B}\\_{223}\n    \\right)\\\\\\\\\n    [\\boldsymbol{\\hat{A}}^{[2]}\\_3]^{2} = (\\boldsymbol{\\tilde{D}}^{[2]})^{-1} \n    \\left( \n    \\boldsymbol{B}\\_{231} + \\boldsymbol{B}\\_{232} + \\boldsymbol{B}\\_{233}\n    \\right)\n\\end{align}\n\nNote that $\\boldsymbol{B}\\_{212},\\, \\boldsymbol{B}\\_{222},\\,\\boldsymbol{B}\\_{232}$ can be computed locally in Client $2$. \nUsing the above equations, Client  $2$  can update its local 2-hop adjacency matrix without access to the global adjacency matrix.\nClients $1$ and $3$ follow the same procedure to compute $[\\boldsymbol{\\hat{A}}^{[1]}_j]^{2}$ and $[\\boldsymbol{\\hat{A}}^{[3]}\\_j]^{2}$ for each $j \\in \\{1,2,3\\}$, respectively.\n\nThis procedure can be extended to any number of hops  $\\ell$. \nAt hop  $\\ell$ , Client  $i$  only accesses $[\\boldsymbol{\\hat{A}}^{[i]}\\_j]^{\\ell}$ for all  $j \\in [K]$  and does not learn the full global matrix $[\\boldsymbol{\\hat{A}}]^{\\ell}$. \nMoreover, if additional security is required, the summation over $\\boldsymbol{B}\\_{ijk}$ could be performed on a secure server using homomorphic encryption.\n\nIn response to your comment, we have now added a more detailed explanation in Appendix D to address this point thoroughly. This clarification emphasizes that clients do not have access to the complete $\\ell$-hop adjacency matrix."
            }
        },
        {
            "title": {
                "value": "Response To Weakness 1 (Part 1)"
            },
            "comment": {
                "value": "We appreciate the reviewer\u2019s thoughtful comments and address them in detail below. If our responses satisfactorily address the reviewer\u2019s concerns, we would be grateful if they would consider adjusting their score accordingly.\n\n**Weakness 1:**\n\nThe scheme for obtaining the local partitions of the $L$-hop combined adjacency matrix $\\boldsymbol{\\bar{A}}^{[i\n]}$ is designed specifically to ensure that no client has access to the entire $\\ell$-hop ($\\ell\\in[1,\\ldots, L]$)  adjacency matrix $\\boldsymbol{\\hat{A}}^\\ell$. Also, due to the low-rank nature of the adjacency matrix, reconstructing the unknown entries is not possible.\n\nWe realize that our explanation of this mechanism may not have been sufficiently clear in the paper, and we appreciate your comment, which will allow us to improve the clarity of the paper.\n\nBelow, we provide a more detailed explanation and outline the improvements we plan to incorporate into the paper to address this point effectively, following your comment.\n\nTo illustrate our algorithm and its privacy mechanism, we provide an example to compute the 2-hop combined adjacency matrix, denoted as $\\boldsymbol{\\bar{A}}$.\n\nConsider a graph with 9 nodes, partitioned across 3 clients, given by the following adjacency matrix:\n\\begin{align*}\n\\mathbf{A} = \\begin{bmatrix}\n0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\\n1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\\n1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\\\\\\\\n0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 \\\\\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 \\\\\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 \\\\\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \\\\\\\\\n\\end{bmatrix}\n\\end{align*}\nwhere each entry $A_{ij} = 1$ indicates a connection between nodes $i$ and $j$, and $A_{ij} = 0$ indicates no connection.\n\nClients are assigned subsets of nodes: Client 1 has nodes 1, 2, and 3; Client 2 nodes 4, 5, and 6;  and Client 3 has nodes 7, 8, and 9.\n\nEach client has information about its internal nodes and connections. Furthermore, it knows the incoming and outgoing connections between its internal nodes and nodes located in other clients. Hence,  client $i$ knows a subset of the global adjacency matrix $\\mathbf{A}$, called $\\boldsymbol{\\tilde{A}}^{[i]}$, corresponding to the connections between its own nodes and to some nodes in other clients.\nIn particular, Client 1 knows\n\n$\n\\text{internal connections (including self loops): }\n\\boldsymbol{\\tilde{A}}\\_1^{[1]} = \\begin{bmatrix}\n1 & 1 & 1 \\\\\\\\\n1 & 1 & 1 \\\\\\\\\n1 & 1 & 1 \\\\\\\\\n\\end{bmatrix}\n$\n\n$\n\\text{outgoing connections: }\n\\boldsymbol{\\tilde{A}}\\_2^{[1]} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n\\end{bmatrix}\\quad\n\\boldsymbol{\\tilde{A}}\\_3^{[1]} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n\\end{bmatrix}\n$\n\n$\n\\text{incoming connections: }\n\\boldsymbol{\\tilde{A}}\\_1^{[2]} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n\\end{bmatrix}\\quad\n\\boldsymbol{\\tilde{A}}\\_1^{[3]} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n\\end{bmatrix}.\n$\n\nSimilarly, Client 2 knows\n\n$\n\\text{internal connections (including self loops): }\n\\boldsymbol{\\tilde{A}}\\_2^{[2]} = \\begin{bmatrix}\n1 & 1 & 1 \\\\\\\\\n1 & 1 & 1 \\\\\\\\\n1 & 1 & 1 \\\\\\\\\n\\end{bmatrix}\n$\n\n$\n\\text{outgoing connections: }\n\\boldsymbol{\\tilde{A}}\\_1^{[2]} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n\\end{bmatrix}\\quad\n\\boldsymbol{\\tilde{A}}_3^{[2]} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n\\end{bmatrix}\n$\n\n$\n\\text{incoming connections: }\n\\boldsymbol{\\tilde{A}}_2^{[1]} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n\\end{bmatrix}\\quad\n\\boldsymbol{\\tilde{A}}_2^{[3]} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n\\end{bmatrix}\n$\n\nand Client 3 knows\n\n$\n\\text{internal connections (including self loops): }\n\\boldsymbol{\\tilde{A}}\\_3^{[3]} = \\begin{bmatrix}\n1 & 1 & 1 \\\\\\\\\n1 & 1 & 1 \\\\\\\\\n1 & 1 & 1 \\\\\\\\\n\\end{bmatrix}\n$\n\n$\n\\text{outgoing connections: }\n\\boldsymbol{\\tilde{A}}\\_1^{[3]} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n\\end{bmatrix}\\quad\n\\boldsymbol{\\tilde{A}}\\_2^{[3]} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n\\end{bmatrix}\n$\n\n$\n\\text{incoming connections: }\n\\boldsymbol{\\tilde{A}}\\_3^{[1]} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n\\end{bmatrix}\\quad\n\\boldsymbol{\\tilde{A}}\\_3^{[2]} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 0 \\\\\\\\\n0 & 0 & 1 \\\\\\\\\n\\end{bmatrix}\n$\n\nUsing $\\boldsymbol{\\tilde{A}}^{[i]}\\_j, j =1,2,3$, Client  $i$  can compute both the degree matrix $\\boldsymbol{\\tilde{D}}^{[i]}$ and the normalized adjacency matrix $\\boldsymbol{\\hat{A}}^{[i]}_j$ for all  $j \\in [K]$. For simplicity, we assume that $\\boldsymbol{\\tilde{A}}^{[i]}\\_j = \\boldsymbol{\\hat{A}}^{[i]}\\_j\\quad \\forall i,j \\in [K]$ in this example, as they differ only by a normalization constant."
            }
        },
        {
            "title": {
                "value": "Response to Weakness 2"
            },
            "comment": {
                "value": "**Weakness 2:**\nThank you for these comments. We address each of your points below:\n\n1. We agree with the reviewer that a deeper analysis of $\\beta$ is valuable for understanding its impact across various graph settings, especially in heterophilic and homophilic contexts. Unfortunately, these simulations require time and, given the time constraints of the rebuttal period, we are unable to include them in this response. \nWe will conduct additional experiments with varying $\\beta$ values and we will do our best to report these results within the rebuttal period. Regardless, if the paper is accepted, we will expand the final version to include a comprehensive analysis of $\\beta$'s influence on the performance.  \nIn the paper, we pragmatically set $\\beta_{\\ell}$ to be nonzero (and equal to one) only for $\\ell=L_s$. \nThis choice allows all powers of $\\boldsymbol{\\tilde{A}}$ to contribute to the predictions.\nTo see this, from Equation 4 in the paper we obtain $\\boldsymbol{\\bar{A}}=\\boldsymbol{\\hat{A}}^{L_s} = (\\boldsymbol{\\tilde{D}}^{-1} (\\boldsymbol{A}+\\boldsymbol{I}))^{L_s}$.\nWriting the power out reveals that all powers $\\ell\\in[L_s]$ are effectively included in $\\boldsymbol{\\bar{A}}$.\nGiven the strong performance of FedStruct even for this pragmatic choice of $\\beta_{\\ell}$, we did not pursue a thorough study of its impact. In this sense, we emphasize that optimizing $\\\\{\\beta_{\\ell}\\\\}_{\\ell=1}^{L_s}$ would only strengthen the performance of FedStruct. As mentioned above, following your comment, we will conduct this investigation  and include results in the revised manuscript.  \nClosely related, we did consider the effect of the number of layers ($L\\_s$) in Figure 4(b), which indirectly addresses the impact of $\\beta\\_{\\ell}$. In Figure 4(b), at each propagation layer $L\\_{s}$ the parameter $\\beta_l$ is set to  $\\begin{cases}\n    1 & \\ell = L\\_s\\\\\\\\0 & \\text{otherwise}\n\\end{cases}$. \n\n2. For the Chameleon graph in Table 5, we set $L_s$ and $L$ to 1 due to its high density. Chameleon has an average node degree of 15.85, and incorporating multi-hop connections increases this further, approximating a fully connected structure. In a two-hop setting, each node accesses around 250 nodes on average, or roughly 10\\% of the total number of nodes. This increased connectivity leads to over-smoothing, which is problematic in dense graphs. Limiting $L_s$ and $L$ to 1 helps mitigate this over-smoothing effect and optimizes performance."
            }
        },
        {
            "title": {
                "value": "Response to Weakness 1"
            },
            "comment": {
                "value": "We appreciate the reviewer\u2019s thoughtful comments and address them in detail below. If our responses satisfactorily address the reviewer\u2019s concerns, we would be grateful if they would consider adjusting their score accordingly.\n\n**Weakness 1:**\nWe sincerely thank the reviewer for their thorough analysis and for highlighting the potential confusion surrounding Equations (30), (31), and (32) in the paper. We confirm that the symbol $K$ should indeed be replaced by $k$, which represents the client index in these equations. Additionally, the dimensions of $\\boldsymbol{\\tilde{A}}^{[i]}_j$ and $\\boldsymbol{\\hat{A}}^{[i]}_j$ are $\\vert\\mathcal{V}_i\\vert \\times \\vert\\mathcal{V}_j\\vert$, as noted. We have incorporated these corrections in the revised paper.\n\nTo clarify how our privacy-preserving scheme works in practice\n(i.e., to show how each client obtains its local partition of L-hop combined adjacency matrix without sharing the L-hop global adjacency matrix), we start by outlining the information accessible to each client  $i \\in [K]$  at the outset:\n\nEach client  $i$  has access to:\n\n- $\\\\boldsymbol{\\\\tilde{A}}^{[i]}_j \\\\in \\\\mathbb{R}^{\\\\vert\\\\mathcal{V}_i\\\\vert \\\\times \\\\vert\\\\mathcal{V}_j\\\\vert}$ for all  $j \\\\in [K]$ : \nrepresenting the outgoing edges from client  $i$  to client  $j$ .\n\n- $\\\\boldsymbol{\\\\tilde{A}}^{[j]}_i \\\\in \\\\mathbb{R}^{\\\\vert\\\\mathcal{V}_j\\\\vert \\\\times \\\\vert\\\\mathcal{V}_i\\\\vert}$ for all  $j \\\\in [K]$ : representing the incoming edges from client  $j$  to client  $i$ .\n\nUsing $\\boldsymbol{\\tilde{A}}^{[i]}_j$, Client  $i$  can compute both the degree matrix $\\boldsymbol{\\tilde{D}}^{[i]}$ and the normalized adjacency matrix $\\boldsymbol{\\hat{A}}^{[i]}_j$ for all  $j \\in [K]$ .\n\nBy induction, we can demonstrate that if each Client  $i$  has access to $[\\boldsymbol{\\hat{A}}^{[i]}_j]^{\\ell - 1}$ for all  $j \\in [K]$ , they can  collaborate with each other to compute $[\\boldsymbol{\\hat{A}}^{[i]}_j]^{\\ell}$ without accessing the global matrix. Here\u2019s a step-by-step breakdown:\n\n1. Base case: For  $\\ell = 1$ , we have $[\\boldsymbol{\\hat{A}}^{[i]}_j]^{1} = \\boldsymbol{\\hat{A}}^{[i]}_j$, which is available to Client $i$ from the start for all  $j \\in [K]$ .\n\n2. Inductive step: \nSuppose each Client $i$  knows $[\\boldsymbol{\\hat{A}}^{[i]}\\_j]^{\\ell - 1}$ for all  $j \\in [K]$.\nThen, Client  $k$  can compute the intermediate term \n$\\boldsymbol{B}\\_{ijk} = \\boldsymbol{\\tilde{A}}^{[i]}\\_k [\\boldsymbol{\\hat{A}}^{[k]}\\_j]^{\\ell-1}$ \nfor all  $j \\in [K]$ \n(since it knows $[\\boldsymbol{\\hat{A}}^{[k]}\\_j]^{\\ell - 1}$  and  $\\boldsymbol{\\tilde{A}}^{[i]}\\_k$ for all  $i,j \\in [K]$)  and share it with Client  $i$.  \nNotice that Client $i$, by receiving  $\\boldsymbol{B}_{ijk}$,cannot reconstruct $[\\boldsymbol{\\hat{A}}\\_j^{[k]}]^{\\ell-1}$ due to the low rank nature of $\\boldsymbol{\\tilde{A}}\\_k^{[i]}$. \nMoreover, Client $k$ also prunes $\\boldsymbol{B}\\_{ijk}$ to remove any concerns. \nClient  $i$  can then compute \n$[\\boldsymbol{\\hat{A}}^{[i]}\\_j]^{\\ell}$\n by \n\\begin{align}\n[\\boldsymbol{A}\\_j^{[i]}]^l = (\\boldsymbol{\\tilde{D}}^{[i]})^{-1} \\sum\\_{k\\in [K]}{\\boldsymbol{B}\\_{ijk}}.\n\\end{align}\n\nThis summation allows Client  $i$  to update its local adjacency matrix without requiring the entire global adjacency matrix.\n\nAt each step  $\\ell$, Client  $i$  only accesses $[\\boldsymbol{\\hat{A}}^{[i]}\\_j]^{\\ell}$ forall $j \\in [K]$ \nwithout learning the entire global matrix\n $[\\boldsymbol{\\hat{A}}]^{\\ell}$. \nFurthermore, if additional security is required, the summation over $\\boldsymbol{B}_{ijk}$ can be performed on a secure server using homomorphic encryption.\n\nWe hope this explanation clarifies the feasibility and privacy mechanisms of FedStruct. In response to your comment, we have also provided a more detailed explanation in Appendix D of the paper to address this point thoroughly. We appreciate your detailed feedback, which has helped strengthen our manuscript."
            }
        },
        {
            "comment": {
                "value": "We appreciate the reviewer\u2019s thoughtful comments and address them in detail below. If our responses satisfactorily address the reviewer\u2019s concerns, we would be grateful if they would consider adjusting their score accordingly.\n\n**Weakness 1:**\n\nThank you for your comment. In response, we will revise the abstract accordingly in the final version of the paper, if accepted, to better address your feedback. However, due to the page limit constraints in this submission (which are relaxed for accepted papers), we are unable to modify the abstract within this response.\n\nHere is the revised abstract that we plan to include in the final version of the paper if accepted:\n\n\nMany real-world data are inherently graph-structured. In practice, such graph data is often distributed across multiple clients, each holding private subgraphs, as in transaction networks. Direct data sharing is typically restricted due to privacy concerns, regulatory constraints, and proprietary limitations. Federated learning (FL) provides a promising solution by enabling collaborative model training without exposing raw data. However, in subgraph federated learning (SFL), where clients possess non-overlapping subgraphs and there are interconnections between subgraphs, such as in transaction networks, effective model training is challenging due to privacy constraints.\n\nThis paper introduces FedStruct, a novel SFL framework designed to tackle this challenge.  To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients. Instead, it leverages explicit global graph structure information to capture inter-node dependencies.\nWe validate the effectiveness of FedStruct through extensive experimental results conducted on six\ndatasets for semi-supervised node classification, showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of clients.\n\n**Weakness 2:**\n\nWe thank the reviewer for the comment. We actually\n cited Figure 1 in the introduction section, Page 2, line 89 as \"Fig. 1\". We have changed the wording to \"Figure 1\" to remove the confusion. \n\nWe also modified Figure 2 in the paper to make it clearer. \n\n**Weakness 3:**\n\nTo clarify, note that\nFedStar is not a subgraph FL method but rather a graph classification method that utilizes structural information to improve the performance of graph classification. Prior research has shown that explicitly incorporating structural information can improve GNN performance.\n\nA key novelty of our paper is the use of explicit structural information within an SFL framework, specifically to improve subgraph federated learning accuracy and enhance privacy.  We also introduce a new structure embedding method, Hop2vec,  designed to capture these structural dependencies effectively.\n\nIn this paper, we use \"structural knowledge\" and \"structural information\" interchangeably, with no intended difference in meaning. To avoid any ambiguity, we have rephrased the last sentence of the related work section to consistently use ``structural information'' and avoid the term \"structural knowledge.\"\n\n**Weakness 4:**\n\nWe thank the reviewer for bringing this to our attention. We have added a comma and checked all other equations so that they are consistent with this style.\n\n**Weakness 5:**\n\nWe appreciate your feedback. Our scheme provides enhanced privacy compared to existing approaches, as it significantly reduces the amount of shared information among clients.\n\nWhile it is straightforward to identify schemes that lack privacy entirely (for instance, previous solutions that share node features or embeddings, as well as FedGCN, where clients access aggregated node features of other clients (thus breaching privacy), and the server requires access to the global adjacency matrix), precisely quantifying the privacy level of a scheme like FedStruct remains very challenging. Developing a mathematical measure or bounds for privacy in subgraph federated learning, federated learning more broadly, and even general machine learning is an open problem that is still being researched. Although we are actively working on establishing privacy bounds for FedStruct, these results are part of our ongoing research and therefore outside the scope of the current paper.\n\nIn Appendix G.1, we discuss the privacy properties of FedStruct, following an approach similar to [1] (FedCog), to provide insights into the privacy considerations of our scheme.\n\n[1] Lei, R., Wang, P., Zhao, J., Lan, L., Tao, J., Deng, C., Feng, J., Wang, X. and Guan, X., 2023. Federated learning over coupled graphs. IEEE Transactions on Parallel and Distributed Systems, 34(4), pp.1159-1172."
            }
        },
        {
            "summary": {
                "value": "The paper works on subgraph FL for node classification, where inter-connections between different clients is important. \n\nIt first computes a global L-hop neighborhood matrix before training. During training, it uses GNN for node feature embedding and use L-hop matrix multiplying a trainable matrix to calculate node structure embedding. Both embeddings are concatenated to get the final prediction result. Experiments show the performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Subgraph FL with inter-connections is an important topic.\n2. Completing the missing L-hop features by learning a L-hop node structure embedding is an interesting idea.\n3. Experiments show the performance."
            },
            "weaknesses": {
                "value": "1. Privacy leakage. Before training, clients communicate to calculate the L-hop neighborhood matrix $\\hat{A}$. In the 2-hop case, since the client knows 1-hop neighbors and the information during the communication, it is still able to reconstruct the 2-hop graph. Pruning cannot guarantee the privacy.\n2. FedSage+ and FedGCN can outperform FedStruct.\n3. In FedGCN, the server does not require a global adjacency matrix for homomorphic encryption. It only needs to know the node ids for encrypted aggregation and identify which nodes belong to each client for sending the aggregation result back."
            },
            "questions": {
                "value": "As in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel framework, FEDSTRUCT, to tackle the challenge of federated learning on graph-structured data distributed across multiple clients, particularly in scenarios involving interconnected subgraphs, it utilizes explicit global graph structure information to capture inter-node dependencies. The effectiveness of FEDSTRUCT is validated through extensive experiments on six datasets for semi-supervised node classification, demonstrating performance that approaches that of centralized methods across various scenarios, including different data partitioning strategies, levels of label availability, and numbers of clients."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1)\tThis paper studies a significant and interesting problem, and the method can be used in a wide range of real-world applications. \n2)\tThe paper is overall well motivated. The proposed model is reasonable and sound. Theoretical analysis is performed."
            },
            "weaknesses": {
                "value": "1) The abstract lacks a description of the background. I recommend briefly outlining the context of the issues addressed in this paper before elaborating on the key problems that are solved.\n\n2) Figure 1 has not been cited and its placement is too early in the text; please adjust this detail. Additionally, Figure 2 is unclear; I recommend adjusting the proportions or border thickness of each subfigure.\n\n3) In the Related Work section, you mention that FED-STAR shares structural knowledge, yet in the conclusion, you state, \"No work has leveraged explicit structural information in SFL.\" Are \"structural knowledge\" and \"structural information\" the same concept? Please provide more clarification in the conclusion.\n\n4) The formula following (1) is missing a comma; please check for similar issues throughout the paper.\n\n5) Privacy is one of the directions addressed in this paper, yet most references are to other works. I suggest including some original proofs or experiments related to privacy to enhance the completeness of the article."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a novel SFL method called FEDSTRUCT, which leverages the augmented explicit structure $\\bar{A}$ to promote the SFL model performance. Moreover, they propose HOP2VEC to learn local structure embedding. FEDSTRUCT precalculates the $\\bar{A}$ with privacy protection and prunes the $\\hat{A}$ matrix to decrease the calculation complexity and communication costs, thus balancing the communication-privacy-accuracy trilemma."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed method is novel, utilizing augmented explicit structure which can be regarded as global knowledge to promote the performance of the SFL model. \n2. Utilizing pruning skills decrease the calculation complexity and communication costs.\n3. Well written and well formulated problem."
            },
            "weaknesses": {
                "value": "1. **Focus on Privacy**. How to  obstain the local L-hop combined adjacency matrix while not share the L-hop global Adjacency Matrix maybe play the core role in FEDSTRUCT. In APP D, the equations [30] [31], [32], what does the $\\hat{A}^{[K]}_j$ mean? Should be $\\hat{A}^{[k]}_j$? If so , the next question, for client $i$, how does it know all $\\hat{A}^{[k]}_j$ for $ k \\in [K]$ without sharing the global adjacency matrix in all clients.  So another question when computing, the $\\tilde{A}^{[i]}_j \\in \\mathbb{R}^{|\\tilde{V}_i| \\times |{V}_j|}$, the same to $\\hat{A}^{[i]}_j$? So $\\hat{A}^{[i]}_k \\times \\hat{A}^{[k]}_j$ should be $\\mathbb{R}^{|\\tilde{V}_k| \\times |{V}_k|}  \\times \\mathbb{R}^{|\\tilde{V}_k| \\times |{V}_j|}$\uff0c but according the definition before, the $|\\tilde{V}_k| \\neq |{V}_k| $, how does the computation continue? Maybe I miss something? I really hope you can explain it for me to understand the feasibility of FEDSTRUCT. That's my main concern about this paper.\n\n2. **About the hyperparameters.** **1)** The analysis of $\\beta$ is not enough, an essential parameter in FEDSTRUCT for various homophilic and heterophilic graphs, which directly dominate the performance and affect the  judgment of FEDSTRUCT's contributions. **2)**  It is so strange that the parameter $L_s$ and $L$ is set to1 for heterophilic graph chameleon in table 5. As the author states in lines 297-299, a heterophilic graph should own multi-hop nodes and a high-frequency filter to augment the local graph representation."
            },
            "questions": {
                "value": "see weaknesses.\n\nIf the first questions can be well explained, the rating should be higher."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}