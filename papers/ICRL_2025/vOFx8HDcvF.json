{
    "id": "vOFx8HDcvF",
    "title": "Stochastic Bandits Robust to Adversarial Attacks",
    "abstract": "This paper investigates stochastic multi-armed bandit algorithms that are robust to adversarial attacks, where an attacker can first observe the learner's action and *then* alter their reward observation.\nWe study two cases of this model, with or without the knowledge of an attack budget $C$, defined as an upper bound of the summation of the difference between the actual and altered rewards. For both cases, we devise two types of algorithms with regret bounds having additive or multiplicative $C$ dependence terms.\nFor the known attack budget case, we prove our algorithms achieve the regret bound of ${O}((K/\\Delta)\\log T + KC)$ and $\\tilde{O}(\\sqrt{KTC})$ for the additive and multiplicative $C$ terms, respectively, where $K$ is the number of arms, $T$ is the time horizon, $\\Delta$ is the gap between the expected rewards of the optimal arm and the second-best arm, and $\\tilde{O}$ hides the logarithmic factors.\nFor the unknown case, we prove our algorithms achieve the regret bound of $\\tilde{O}(\\sqrt{KT} + KC^2)$ and $\\tilde{O}(KC\\sqrt{T})$ for the additive and multiplicative $C$ terms, respectively.\nIn addition to these upper bound results, we provide several lower bounds showing the tightness of our bounds and the optimality of our algorithms.\nThese results delineate an intrinsic separation between the bandits with attacks and corruption models.",
    "keywords": [
        "Robust Algorithms",
        "Multi-armed Bandits",
        "Adversarial Attacks"
    ],
    "primary_area": "learning theory",
    "TLDR": "A comprehensive study of bandit algorithms robust to adversarial attacks.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=vOFx8HDcvF",
    "pdf_link": "https://openreview.net/pdf?id=vOFx8HDcvF",
    "comments": [
        {
            "title": {
                "value": "On Writing"
            },
            "comment": {
                "value": "> **Q:** In general, the writing of the paper is very focused on presenting as many results are possible and is very dense in terms of results. The paper could have been formatted better with more discussions around interpreting the results rather than having so many results in the main paper.\n\n**A:** We thank the reviewer for suggesting a better exposition. We agree that this paper would benefit from more discussions around interpreting the results. For the final version, we will clarify the results with additional interpretations as given in the previous answers, e.g., [the response to Reviewer C1Fc](https://openreview.net/forum?id=vOFx8HDcvF&noteId=cfHY7THs7T).\n\n> **Q:** There are some typos and inconsistencies in the theorem statements and proofs. For eg in proof of Lemma 14: i) the constants are changed from 36 to 64 in $N_k$. ii) Line 798, it should be 'triggered' instead of 'trigger' iii) Similarly lines 804 to 807 on page 15 in the same proof have $\\delta$ with subscripts.\n\n**A:** We thank the reviewer for highlighting these typos and inconsistencies. We corrected the mentioned typos and made multiple careful passes in revising this paper during the rebuttal.\nSpecifically for the examples, i) we will consistently use the constant $64$. ii) We corrected the typo in Line 798. iii) We added the subscripts from $\\Delta_k$ in Lines 804-807."
            }
        },
        {
            "title": {
                "value": "On Experiment Results"
            },
            "comment": {
                "value": "> **Q:** 3. Experimental results don't have confidence bars, and in the case of no corruptions with known budgets, the STOP algorithms perform worse than other methods. Some discussion on the performance in the absence of corruption is warranted.\n\n> **Q:** Can you please explain why the algorithms potentially perform worse in low corruption settings in the experiments?\n\n**A:** We thank the reviewer for giving us the opportunity to explain the performance of the STOP algorithms when $C=0$.\n              Developing these two STOP algorithms for the known budget case is mainly for (1) later algorithm design for the unknown budget case\n              and\n              (2) the theoretical interest of deriving gap-independent and multiplicative $C$ bounds.\n\nThe worst performance of the two STOP algorithms when $C=0$ is expected.\n              Because both STOP conditions are triggered when there are still some suboptimal arms in the candidate arm set.\n              Then, after the STOP conditions, the algorithms uniformly pull these remaining arms, resulting in a linear increase in regret.\n              Although the regret increases linearly in the later stage of the plot, it is still logarithmic and theoretically bounded.\n              Because the remaining suboptimal arms have high reward means close to the optimal arm, which only incurs a slow linear increase. On the other hand, note that the y-axis in Figure 3(a) is in $10^3$ scale while the scales of Figures 3(b)(c)(d) are in $10^4$.\n              That is, the two STOP algorithms --- although with the worst performance among plots in Figure 3(a) with $C=0$ --- still outperform other scenarios when $C=3000, 6000, 9000$.\n\nIn this revision, we added this discussion to Appendix F and revised all the figures in the paper with a confidence-bar plot (e.g., see Figures 3 and 4 of Section 6)."
            }
        },
        {
            "title": {
                "value": "On Lower Bound Comparison Between Corruption and Attack Cases"
            },
            "comment": {
                "value": "> **Q:** 2. Since this paper focuses on making the distinction between attacks and corruption, it seems the main difference is in the inability to use randomization to reduce the scale of the attack, resulting in the need for deterministic algorithms where potentially any arm suffers from all of the corruptions. Thus in the known corruption level case, the results for the setting are directly implied by earlier. (Although this work does a tighter analysis in terms of gaps). A thorough analysis of the lower bounds comparing the setting rather than just comparing the dependence on $K$ could benefit the reader.\n\n> **Q:** Can you please mention how the lower bounds change or are implied from the corruptions setting to the attack setting in case of unknown horizons?\n\n**A:** We thank the reviewer for this instructive suggestion. We will add the following discussion to the revised manuscript:\n\n\"For the unknown corruption/attack budget case, the lower bound for corruption is $\\Omega(C + \\text{polylog}(T))$, which is achieved by Liu et al. (2021) in the corruption scenario. However, it is not achievable in the attack scenario (Theorem 3). For instance, in terms of $T$, the *polynomial* achievable additive and multiplicative regret bounds (Propositions 4 and 5) for the attack scenario are worse than the *logarithmic* achievable lower bound $\\Omega(C + \\text{polylog}(T))$ for the corruption scenario. This highlights the difficulty of designing algorithms for the attack case.\n              Additionally, in terms of $C$, the $\\Omega(KC)$ lower bound for the attack scenario refutes the possibility of achieving the $\\Omega(C + \\text{polylog}(T))$ bound.\"\n\n- Reference: Junyan Liu, Shuai Li, and Dapeng Li. Cooperative stochastic multi-agent multi-armed bandits robust\nto adversarial corruptions. arXiv preprint arXiv:2106.04207, 2021"
            }
        },
        {
            "title": {
                "value": "On Gap-Dependent Results for the Unknown Corruption Budget Case"
            },
            "comment": {
                "value": "> **Q:** 1. The results or the discussion do not clarify whether gap-dependent results can be obtained for the unknown horizon setting.\n\n> **Q:** Can you please explain if the gap-dependent results in the unknown corruption case can be obtained for the algorithms under consideration?\n\n**A:** No, non-trivial gap-dependent results in the unknown corruption case cannot be obtained for the algorithms considered in this paper.\nThis is because in bandit literature, the gap-dependent bounds usually appear in $O\\left(\\sum_k \\frac{\\log T}{\\Delta_k} \\right)$-like logarithmic formulas.\nWhen it comes to polynomial (in $T$) regret bounds (e.g., the $O(\\sqrt{KT})$ term of our \\pewr and \\mssewr algorithms for the unknown budget case), this gap-dependent logarithmic term would be dominated by the polynomial terms---even when $\\Delta_k$ is very small (by the gap-independent regret conversion)---and would not appear in the regret bounds.\n\nHowever, the current lower bounds (esp., Theorem 3) do not rule out the possibility of devising other algorithms with gap-dependent results in the unknown corruption case. One possible gap-dependent regret bound that future research may consider is $O\\left(\\exp(C)\\cdot\\sum_k \\frac{\\log T}{\\Delta_k} \\right)$."
            }
        },
        {
            "title": {
                "value": "On Clarity of Comparison"
            },
            "comment": {
                "value": "> **Q:** Clarity of Comparison (Lines 417-422): The comparison in this section is unclear, and I would appreciate a clearer exposition/steps, especially since the reference provided here is incorrect.\n\n**A:** We thank the reviewer for providing the chance to clarify the comparison in Lines 417-422. In this revision, we further elaborated on the comparison of the three key terms of Theorem 2 of Bogunovic et al. (2021), as follows,\n\n\"The most related to ours is the phase elimination algorithm proposed in Bogunovic et al. (2021) for linear bandits. However, directly reducing their results (Bogunovic et al., 2021, Theorem 2)  to our stochastic bandit case would yield a $\\tilde{O}(\\sqrt{KT} + KC\\sqrt{K}\\log T+ C^2)$ upper bound for $C\\le {\\sqrt{T}}/{(K\\log(\\log K)\\log T)}$ only, and, for general $C$, their results would become $\\tilde{O}(\\sqrt{KT} + KC\\sqrt{K}\\log T + K^2C^2)$. Our result in Theorem 8, simplified as $\\tilde{O}(\\sqrt{KT} + KC\\log T + KC^2)$, is tighter than theirs by a factor of $\\sqrt K$ on the second $\\log T$ term, and, for the general $C$ case, our result is tighter by a factor of $K$ on the third $C^2$ term.\""
            }
        },
        {
            "title": {
                "value": "On Venue Suitability"
            },
            "comment": {
                "value": "> **Q:** Venue Suitability: I\u2019m not entirely sure this paper is a strong fit for ICLR, as I\u2019m not aware of similar works published at this venue previously. This is a consideration for the authors, as they might find broader reach at an alternative venue.\n\n**A:** We thank the reviewer for this suggestion. Although there are other learning venues that this paper can find broader attention, we believe that ICLR is well-suited to this work as some prior works on attack/robust algorithms for bandits were published in ICLR, e.g.,\n- Ma, Yuzhe, and Zhijin Zhou. \u201dAdversarial Attacks on Adversarial Bandits.\u201d ICLR, 2023.\n- Buening, Thomas Kleine, et al. \u201dBandits Meet Mechanism Design to Combat Click-bait in Online Recommendation.\u201d ICLR, 2024. *(Spotlight Poster)*"
            }
        },
        {
            "title": {
                "value": "On Lower Bound Claim"
            },
            "comment": {
                "value": "> **Q:** Lower Bound Claim: The paper claims that the lower bound result of $\\Omega(KC)$ is new; however, this result is already established in Stochastic Linear Bandits Robust to Adversarial Attacks (see Appendix C.3). The proof and exposition provided here are quite similar to those in the mentioned paper.\n\n**A:** We thank the reviewer for pointing out the similarity between our lower bound and the one in the Stochastic Linear Bandits Robust to Adversarial Attacks paper. After carefully reviewing the two proofs, we agree that the lower bound result of $\\Omega(KC)$ is very close to the one in the mentioned paper. We revised the manuscript to clarify this: \n\n\"... originally derived in the linear bandit scenario by Bogunovic et al. (2021, Theorem 3).\"\n\nWe note that as the lower bound (Theorem 1) is not the main contribution of our paper, the novelty of our work remains intact."
            }
        },
        {
            "title": {
                "value": "On Literature Review"
            },
            "comment": {
                "value": "> **Q:** Literature Review: The literature review in this paper can be improved. The reference section is also too brief and lacks organization. For example, Bogunovic et al. (2020), as cited, do not address the linear setting; this is covered in other relevant papers that are not cited, such as Stochastic Linear Bandits Robust to Adversarial Attacks and A Robust Phased Elimination Algorithm for Corruption-Tolerant Gaussian Process Bandits.\n\n**A:** We appreciate the reviewer\u2019s information on this point. We did notice the other two papers\nmentioned by the reviewer during the preparation of our manuscript but missed them due to\ntheir similar bibliography key strings. We added these references in the revised manuscript\naccordingly as follows,\n\n\u201cApart from the MAB model, there are works studying robust algorithms on structured ban-\ndits under attacks, e.g., linear bandits (Bogunovic et al., 2021; He et al., 2022), Gaussian\nbandits (Bogunovic et al., 2020; 2022), and Lipchitz bandits (Kang et al., 2024).\u201d"
            }
        },
        {
            "title": {
                "value": "On Terminology"
            },
            "comment": {
                "value": "> **Q:** Terminology: I like the terminology of \u201cattacks\u201d to distinguish it from the classical \u201ccorrupted\u201d setting. However, if the authors intend to introduce this terminology shift, they should properly credit the original paper that first explored this setting and provided robust algorithms: \u201cCorruption-Tolerant Gaussian Process Bandit Optimization.\u201d To my knowledge, this was the first work to present robust algorithms for scenarios in which the attacker can observe the learner's decisions.\n\n**A:** We thank the reviewer for suggesting a more precise credit to the original robust work. We added the update to the revised manuscript accordingly:\n\n\"Bogunovic et al. (2020) are the first to devise robust algorithms for bandits under attacks (referred to as corruptions).\""
            }
        },
        {
            "title": {
                "value": "On Algorithm Design"
            },
            "comment": {
                "value": "> **Q:** Algorithm Design: I didn\u2019t notice any novel or original elements in terms of algorithm design. The PE algorithm has been applied in this context in prior work (cited below), and the idea of using CORRAL has already been explored in similar settings, such as in Misspecified Gaussian Process Bandit Optimization. However, I only find this to be a minor weakness of the paper.\n\n**A:** We thank the reviewer for this feedback. While our algorithms draw inspiration from existing techniques, we have made several crucial modifications to adapt them to the adversarial attack model. These adjustments have resulted in tighter bounds than previous work in weaker corruption models or structured bandits. Key improvements include:\n\n- **Phase Elimination with Tighter Concentration Radius ($\\texttt{PE-WR}$); An improvement of $K$ Factor:** We improved the additive regret upper bound for the unknown $C$ case (Theorem 9) by reducing a $K$ factor compared to the results derived from adapting a known linear bandit algorithm to MAB (Bogunovic et al., 2021). This was made possible by a new algorithm design inspired by multi-phase elimination and tighter concentration analysis.\n- **New Stopping Condition for $\\texttt{SE-WR}$ in Model Selection (e.g., CORRAL); Improved Multiplicative Regret Bounds:** For the unknown $C$ case, we achieved improved multiplicative regret upper bounds of $\\tilde{O}(KC\\sqrt{T})$ and $\\tilde{O}(\\sqrt{KC}T^{2/3})$ (Theorem 10), which offer much better $T$ dependence than prior results from Lipschitz bandit algorithms (Kang et al., 2024). This improvement stemmed from our more precise multiplicative regret analysis for the known $C$ case, which involved careful threshold selection in the $\\texttt{SE-WR}$ stopping condition.\n\nAdditionally, we also present several new lower bounds in the adversarial attack model (Propositions 2, 4, 5), each matching the corresponding improved upper bounds presented in our paper, highlighting the optimality of our algorithm design."
            }
        },
        {
            "title": {
                "value": "On Novelty and Contributions"
            },
            "comment": {
                "value": "We sincerely thank the reviewer for their valuable feedback on our work. We want to take this opportunity to address the concerns raised regarding the novelty and contributions of our research.\n\n## 1. Novelty and Contribution\n\nWe want to emphasize that our work is the first to rigorously study the canonical problem of defending against adversarial attacks in the stochastic multi-armed bandit (MAB) setting. Existing literature has primarily focused on related but distinct problems:\n\n- **Attack Perspective:** Prior research, such as Jun et al. (2018), has explored the problem from the attacker's perspective, designing strategies to mislead bandit algorithms.\n- **Defense in Structured Bandits:** Other studies, including Bogunovic et al. (2021), have investigated defenses within structured bandit settings, such as linear bandits.\n- **Weaker Adversary Models:** Work like Lykouris et al. (2018) addresses defense against corruption but under a weaker adversary model (refer to Lines 68--75 in our manuscript for a detailed comparison).\n\nNone of these works have directly addressed the core challenge of defending against adversarial attacks in stochastic MABs, which we believe is a significant gap that our research fills.\n\n## 2. Comprehensive Study of Robust Algorithms\n\nOur work thoroughly investigates robust algorithms for stochastic MABs under an adversarial attack model. We explore known and unknown attack budget scenarios (or \"adaptive budget case\" as mentioned by Reviewer fgr9). Specifically, we analyze four distinct cases across Sections 4.1, 4.2, 5.1, and 5.2, covering:\n\n- **Additive Regret Bounds** with respect to the known attack budget $C$.\n- **Multiplicative Regret Bounds** with respect to known $C$.\n- **Additive Regret Bounds** with respect to the unknown attack budget $C$.\n- **Multiplicative Regret Bounds** with respect to unknown $C$.\n\nFor each case, we develop algorithms that achieve tight regret upper bounds, matching the corresponding lower bounds detailed in Section 3. The significance of these results is outlined in Table 1 and further discussed in our response to Reviewer MtDK.\n\n## 3. Algorithmic Innovations and Improvements\n\nWhile our algorithms draw inspiration from existing techniques, we have made several crucial modifications to adapt them to the adversarial attack model. These adjustments have resulted in tighter bounds than previous work in weaker corruption models or structured bandits. Key improvements include:\n\n- **Removal of $1/\\Delta_k^2$ Dependence:** We eliminated the $1/\\Delta_k^2$ dependence in the attack term $C$ from the original SE-WR upper bound analysis under the weaker corruption setting Lykouris et al. (2018). This was achieved through a refined concentration inequality analysis.\n- **Improvement of $K$ Factor:** We improved the additive regret upper bound for the unknown $C$ case (Theorem 9) by reducing a $K$ factor compared to the results derived from adapting a known linear bandit algorithm to MAB (Bogunovic et al., 2021). This was made possible by a new algorithm design inspired by multi-phase elimination and tighter concentration analysis.\n- **Enhanced Multiplicative Regret Bounds:** For the unknown $C$ case, we achieved improved multiplicative regret upper bounds of $\\tilde{O}(KC\\sqrt{T})$ and $\\tilde{O}(\\sqrt{KC}T^{2/3})$ (Theorem 10), which offer much better $T$ dependence than prior results from Lipschitz bandit algorithms (Kang et al., 2024). This improvement stemmed from our more precise multiplicative regret analysis for the known $C$ case, which involved careful threshold selection in the SE-WR stopping condition.\n\nAdditionally, we also present several new lower bounds in the adversarial attack model (Propositions 2, 4, 5), each matching the corresponding improved upper bounds presented in our paper."
            }
        },
        {
            "summary": {
                "value": "The paper investigates the classical MAP problem in the adversarial attack setting. The authors provide several tight results covering the case when the attack budget is known/unknown, multiplicative and additive bounds, as well as lower bounds."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper addresses a gap in the literature, recognizing that adversarial attacks have not been thoroughly explored within the classical multi-armed bandit (MAB) framework and effectively filling this gap.\n- The authors examine both additive and multiplicative bounds, providing a clear comparison that shows which approach performs better based on the attack budget C.\n- Figures 1 and, especially, Figure 2 nicely illustrate the results of attack-based multiplicative and additive bounds, offering a well-structured presentation that I haven't seen in comparable works with this level of detail.\n- I also like seeing the clear separation between corruption and attack results/settings in one place.\n- The paper presents novel findings and situates them within the existing literature, demonstrating that the derived upper bounds are tight (known C case)."
            },
            "weaknesses": {
                "value": "1. Algorithm Design: I didn\u2019t notice any novel or original elements in terms of algorithm design. The PE algorithm has been applied in this context in prior work (cited below), and the idea of using CORRAL has already been explored in similar settings, such as in Misspecified Gaussian Process Bandit Optimization. However, I only find this to be a minor weakness of the paper. \n\n2. Terminology: I like the terminology of \u201cattacks\u201d to distinguish it from the classical \u201ccorrupted\u201d setting. However, if the authors intend to introduce this terminology shift, they should properly credit the original paper that first explored this setting and provided robust algorithms: \u201cCorruption-Tolerant Gaussian Process Bandit Optimization.\u201d To my knowledge, this was the first work to present robust algorithms for scenarios in which the attacker can observe the learner's decisions.\n\n3. Literature Review: The literature review in this paper can be improved. The reference section is also too brief and lacks organization. For example, Bogunovic et al. (2020), as cited, do not address the linear setting; this is covered in other relevant papers that are not cited, such as Stochastic Linear Bandits Robust to Adversarial Attacks and A Robust Phased Elimination Algorithm for Corruption-Tolerant Gaussian Process Bandits.\n\n4. Lower Bound Claim: The paper claims that the lower bound result of \u03a9(KC) is new; however, this result is already established in Stochastic Linear Bandits Robust to Adversarial Attacks (see Appendix C.3). The proof and exposition provided here are quite similar to those in the mentioned paper.\n\n5. Venue Suitability: I\u2019m not entirely sure this paper is a strong fit for ICLR, as I\u2019m not aware of similar works published at this venue previously. This is a consideration for the authors, as they might find broader reach at an alternative venue.\n\n6. Clarity of Comparison (Lines 417-422): The comparison in this section is unclear, and I would appreciate a clearer exposition/steps, especially since the reference provided here is incorrect."
            },
            "questions": {
                "value": "See 4 and 6 in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the design of stochastic bandits algorithms robust to adversarial attacks. In particular, the paper considers an easier setting in which the learner is aware of the attacker budget, and a harder setting in which the learner is not aware of the attacker budget. These results are complemented by lower bounds. Finally, the authors provide an experimental analysis that shows the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper advances the state of the art on algorithms robust to adversarial attacks. The paper is well-written and the relationship/improvement relative to previous work is well described."
            },
            "weaknesses": {
                "value": "The technical contribution is quite weak. For instance, the algorithmic approaches follow previous work and the analysis is not very involved."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies stochastic bandit algorithms which are robust to adversarial attacks under a strong adversary that can see the observed arm before attacking.   \nThe paper considers settings with unknown budget cost or known budget cost $C$.\nIn the known budget case, they provide a gap dependent $((\\frac{K}{\\delta}) \\log T  + KC))$ upper bound that matches the lower bound. They also give gap-independent extensions with upper bounds of $\\tilde{O}(\\sqrt{KTC})$ or $\\tilde{O}(\\sqrt{KT} + KC)$ bound.\n\nFor the unknown case, they show two stopping criteria-based algorithms, one with an additive dependence in C: $O(\\sqrt{KT} + KC^2)$. They show an algorithm that gets $O(T^\\alpha)$ regret without corruptions, must have at least $O(T^\\alpha + C^\\beta)$ regret for $\\beta \\geq \\frac{1}{\\alpha}$, thus this upper bound matches this lower bounds in exponents of $C$ (given that it has $\\sqrt{T}$ dependence without $C$). Similarly, they give algorithms with multiplicative dependence on $C$ for the regret, that is  $\\tilde{O}(\\sqrt{KC}T^{\\frac{2}{3}})$ or $\\tilde{O}(KC\\sqrt{T})$.\n\nThe paper also provides experimental evidence showing the effectiveness of their algorithms against the attack strategies developed by Jun et al. 2018 comparing it will other corruption-robust MAB algorithms studies in the literature."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper differentiates between attack and corruption models of manipulating multi-armed bandits. It provides insights into the difference between corruption and attacks in terms of the required corruption/attack budget and thus the increased difficulty in preventing attacks compared to corruption.\n\n2. For the successive elimination algorithm SE-WR with increased confidence, also used in Lykouris et al. (2018), the paper shows a tighter regret bound, by better analysis of the concentration results which leads to $O(KC)$ term instead of a gap dependent term in  Lykouris et al. (2018).\n\n3. The authors also give a gap-independent bound for SE-WR and extend the SE-WR algorithm to work in the unknown attack budget settings. They also provide an analysis of the resulting algorithms. \n\n4. The paper also provides experimental evidence showing the effectiveness of their algorithms against the attack strategies developed by Jun et al. 2018 in comparison with multiple MAP defense strategies proposed in the literature."
            },
            "weaknesses": {
                "value": "1. The results or the discussion do not clarify whether gap-dependent results can be obtained for the unknown horizon setting. \n\n\n2. Since this paper focuses on making the distinction between attacks and corruption, it seems the main difference is in the inability to use randomization to reduce the scale of the attack, resulting in the need for deterministic algorithms where potentially any arm suffers from all of the corruptions. Thus in the known corruption level case, the results for the setting are directly implied by earlier. (Although this work does a tighter analysis in terms of gaps). A thorough analysis of the lower bounds comparing the setting rather than just comparing the dependence on $K$  could benefit the reader. \n\n3. Experimental results don't have confidence bars, and in the case of no corruptions with known budgets, the STOP algorithms perform worse than other methods. Some discussion on the performance in the absence of corruption is warranted.\n\nNit:\n1. In general, the writing of the paper is very focused on presenting as many results are possible and is very dense in terms of results. The paper could have been formatted better with more discussions around interpreting the results rather than having so many results in the main paper.\n\n 2. There are some typos and inconsistencies in the theorem statements and proofs. For eg in proof of Lemma 14:\n  i) the constants are changed from 36 to 64 in $N_k$.\n  ii) Line 798, it should be 'triggered' instead of 'trigger'\n   iii) Similarly lines 804 to 807 on page 15 in the same proof have $\\delta$ with subscripts."
            },
            "questions": {
                "value": "1. Can you please mention how the lower bounds change or are implied from the corruptions setting to the attack setting in case of unknown horizons?\n\n2. Can you please explain if the gap-dependent results in the unknown corruption case can be obtained for the algorithms under consideration? \n\n3. Can you please explain why the algorithms potentially perform worse in low corruption settings in the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}