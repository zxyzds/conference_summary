{
    "id": "JDa5RiTIC7",
    "title": "Simulate Before Act: Model-Based Planning for Web Agents",
    "abstract": "Language agents have shown promising performance in automating web-based tasks, but the complexity and vast search spaces of real-world websites challenge reactive agents in identifying optimal solutions. While tree search agents offer enhanced exploration by interacting with actual websites, they often incur high costs, potential risks, and are challenging to implement for real-world websites. This paper explores a novel paradigm leveraging large language models' (LLMs) internal world models for planning in complex environments, presenting a middle ground between reactive agents and tree search agents. Results on two representative benchmarks, VisualWebArena and Mind2Web-live, demonstrate that our approach largely closes the gap between reactive agents and tree search agents, while maintaining efficiency and safety advantages. Notably, tree search can be considered as approaching an upper bound for our method, as it explores actual websites rather than simulations. This work opens new avenues for research into more effective and secure strategies for autonomous agents in complex, dynamic environments. It represents a step forward in improving upon reactive agents while approaching the performance of tree search methods, without incurring their implementation challenges and costs.",
    "keywords": [
        "Web Agents; World Model; Planning; MPC"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "This paper presents a preliminary study on leveraging the internal world model of complex environments within LLMs to help web agents strike a balance between accuracy and efficiency.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=JDa5RiTIC7",
    "pdf_link": "https://openreview.net/pdf?id=JDa5RiTIC7",
    "comments": [
        {
            "withdrawal_confirmation": {
                "value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."
            },
            "comment": {
                "value": "We sincerely appreciate the thoughtful feedback from all reviewers. We acknowledge that the submitted version may need further refinement, so we have decided to improve it further and submit a polished version at a later date.\n\nHere we briefly address some concerns from the reviewers:\n> 1. Planning horizon is limited to one step\n\nFirst, we would like to point out that all different state representations consistently outperform the reactive baseline across all horizon settings (see Figure 3). \nSpecifically, there is a slight improvement from planning horizon 1 to 2 for the state change description and accessibility tree.\nWe chose horizon 1 for our main experiments because it performs comparably to horizon 2 while being significantly less costly.\nAlso, we want to highlight that achieving better performance with longer horizons is not a major goal of this work.\nInstead, our aim is to demonstrate the viability and potential of using LLMs for model-based planning in complex environments, a goal we have successfully achieved in this paper.\n\n> 2. Inaccurate claim that tree search is an \"upper-bound\" of our method\n\nWe thank the reviewers for identifying this issue. We agree that the claim lacks rigor. The core problem is that tree search is not always feasible on real-world websites due to irreversible actions, despite its slightly better performance on VWA. Our model-based planning offers greater flexibility and, importantly, can complement tree search methods, for example, by using simulation to enhance the value function in tree search.\n\n>3. The superiority of state change description over HTML/accessibility tree\n\nWe agree that we should make no claims about the strict superiority of state change description over HTML or accessibility tree. Also, this is orthogonal to our main message. As a pioneering study on LLM-simulated world models, our goal is to show that LLM's general knowledge about the websites can indeed help the performance of web agents, regardless of the representation (see Fig 3).\n\nUnfortunately, we did not make these points clear in our submitted version. Therefore, we have decided to withdraw the submission and further refine the writing."
            }
        },
        {},
        {
            "summary": {
                "value": "This paper proposes a method for controlling a web agent. The main novelty is to use an LLM to simulate the execution agent\u2019s actions outcome, and use these simulations to choose more effectively the agent\u2019s next actions. This is done by simulating the execution of the actions the agent can choose, observing the \u201cobtained\u201d reward, and choose the action that maximizes this reward. The authors explored different planning horizons, i.e., different lengths of simulated trajectories, but the results show that one-step lookahead is best.  They call this approach MPC (due to its similarity to MPC in control theory). MPC also include an action selection and refinement step, where the latter asks the LLM to filter out some actions. Experimental results on two domains show that a one-step lookahead, which evaluate an action based on the reward obtained in the simulated next state, is better than a reactive agent."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe web agents domain is interesting, relevant, and challenging. \n2.\tExploring the use of LLMs to simulate executions for planning is an exciting direction. \n3.\tThe experimental results show the proposed MPC algorithm is better than the Reactive agent and faster than the Tree Search agent.\n4.\tI like the action refinement stage. Also, the ablation study shows much of the benefit is attributed to it. \n5.\tA nice discussion on different ways to represent the states in this domain. \n6.\tThe ablation study is very helpful."
            },
            "weaknesses": {
                "value": "1.\tI am not sure the proposed project is mature enough. Since the objective is to use LLMs to plan for web agents, I would expect the planning component to be richer. Currently, it\u2019s not really planning but more a one-step lookahead. \n2.\tI find the depiction of the results somewhat misleading with respect to the comparison of the tree search and MPC results. As I see it, the Tree Search is most of the time better, and the advantage of MPC over it is only runtime and number of simulations. This is not said clearly enough. In fact, in Table 1 the authors write that there is a 33% increase over the baselines but this is not true when considering Tree Search.\n3.\tIn the proposed algorithm, only one simulation is done per action. This makes sense when the planning horizon is 1, but when the planning horizon this is not very reasonable. \n4.\tI like a lot section 3.1, which clarified to me finally the exact setting you were dealing with. However, I am not sure that viewing the environment as deterministic (see first paragraph of 3.1) is a valid assumption in the web agents domain. \n\nMinor issues:\n-\t\u201cAs an initial exploration in this domain, we prioritize establishing the viability and potential of this paradigm over performance optimization\u201d \u2013 I could not fully understand this sentence.\n-\t\u201c... storing the action sequence .... not feasible for real-world websites.\u201d \u2013 why? Storing sequences of actions isn\u2019t easy to do? \n-\t\u201c\u2026 LLMs employed as world models primarily focus on facilitating decision-making in planning rather than training.\u201d \u2013 I am not sure what you mean in this sentence by \u201cplanning\u201d and \u201ctraining\u201d (I\u2019ve worked on both planning and training, but I still can\u2019t make sense of this sentence). \nSuggestions:\n-\tMore information on what Web Agents are supposed to do with some examples would help section 2.1."
            },
            "questions": {
                "value": "1.\tThe authors write that the Tree Search results can serve as an upperbound. I do not see why this is true. I hope the authors can elaborate on this. \n2.\tI wonder if there aren\u2019t any stronger baselines than the reactive agent for this type of web agents?\n3.\tCan you provide more details on the baseline Reactive agent? Did it also use the same set of actions? how did it evaluate these actions?\n4.\tHow is this work novel compared to others that have used LLMs to simulate executions for planning? Or is this the first work to do so? \n5.\tPage 6. \u201cThis stage identifies promisting actions to improve the coverage.\u201d  - what is \u201ccoverage\u201d in this context?\n6.\tWhy there are no completion rate results in VWA?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The presented work explores the usage of pre-trained LLMs to plan in complex web environments, aiming to balance between reactive agents and tree search agents. However, several issues need attention."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The work showcases a novel use case of pre-trained large language models for planning in complex web environments."
            },
            "weaknesses": {
                "value": "1. The planning process lacks adequate explanation, particularly regarding self-refinement and how the methodology differs from reactive methods if it doesn't involve backtracking.\n2. The effectiveness of the results is questionable due to a planning horizon limited to one step."
            },
            "questions": {
                "value": "1. What do you mean by internal world models of LLMs? This is vague and has no strong meaning, do you mean a pre-trained language model?\n2. Figure 2 has a mistake. Office products and Electronics categories in the search tree do not match with the arrows. Please update and change the score accordingly. The score does not match the example narrative.\n3. The authors need to better explain and articulate the planning process involved (Section 3.1).\n4. What is self-refinement in Figure 2?\n5. Does the suggested MPC methodology perform back-tracking like tree methods? If not how is this different from reactive methods?\n6. In Section 3.2, while discussing the ideal representation of observation space representation, the authors mentioned that HTML is computationally intensive, however in Section 4.1, for Mind2Web-live benchmark, authors have used HTML for O. This is contradictory. \n7. The discussion in Section 3.2 initially suggests that the authors are introducing innovative steps to process states into natural language. However, it becomes apparent that the approach mainly involves utilizing trajectories from benchmarks as the basis for state representation, rather than employing a new processing approach.\n8. It is unclear how the results demonstrate effectiveness, given that the authors specified a planning horizon of only 1 (Section 4.1 setup). This needs discussion in the paper. A \"planning horizon of 1\" in the planning context means that the model or agent only looks one step ahead when making decisions.\n\n**Minor Comments:**\n1. SoM is directly used in Table 1 without defining earlier. This should be defined in section 3.2.\n2. Authors need to correct typos and grammatical mistakes. eg., Page 1, ln 50, \u2018tree\u2019 is spelled wrong."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a model-based planning approach for web agents, which combines elements of reactive agents and tree search methods to enable autonomous agents to navigate complex web environments efficiently. The authors propose using large language models (LLMs) to simulate actions in a virtual environment, minimizing real-world interaction and associated risks. They employ MPC to implement this framework, with the agent simulating action sequences before executing only the first action of the most promising sequence. Testing on benchmarks like VisualWebArena and Mind2Web-live, the method shows performance gains over-reactive agents, although it falls slightly short of the upper bounds achieved by tree search."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The approach is innovative in using LLMs as internal world models for web navigation, which addresses the limitations of both purely reactive agents and full tree search methods. This middle-ground approach leverages the predictive capabilities of LLMs for complex tasks, presenting a new application of LLMs in a web automation context.\n2. The paper presents thorough empirical validation across two benchmarks, demonstrating the effectiveness of the proposed model against standard baselines. The inclusion of ablation studies and sensitivity analyses further enhances the robustness of the findings.\n3. The paper is well-structured, with clear explanations of each step in the MPC framework. Figures and tables effectively illustrate experimental setups, results, and comparisons with baselines.\n4. The proposed approach is highly significant for the development of web agents, as it reduces the computational and risk constraints associated with online planning. The method could lead to safer, more efficient web automation tools and potentially influence future models for other complex environments."
            },
            "weaknesses": {
                "value": "1. While the model shows promising results, its reliance on LLM-generated simulations raises questions about the fidelity of these simulations to actual web environments. As noted in the paper, inaccuracies in the LLM-based simulation may lead to suboptimal action choices, especially for tasks requiring long-horizon planning.\n2. The authors acknowledge that the method incurs high computational costs, particularly when using state-of-the-art models like GPT-4o. This limits practical applicability, and future optimizations for cost reduction could improve usability.\n3. The approach struggles with tasks that require multi-step planning, as performance deteriorates with longer horizons. This limitation is apparent in high-difficulty tasks on the VisualWebArena benchmark, highlighting the need for more advanced multi-step planning within the simulation framework. Do you have any modifications on how to improve this?\n4. The results on Mind2Web-live are less compelling than on VisualWebArena, potentially due to the complexity and variability of real-world websites. This indicates that the approach may be more suitable for controlled environments and could face challenges in broader real-world applications. (I am slightly unfamiliar with the literature/benchmarks used for web-based agents)"
            },
            "questions": {
                "value": "1. Could alternative state representations or intermediate reward structures improve the accuracy of the simulated state transitions in longer-horizon planning tasks?\n2. Given the high computational costs, have the authors considered fine-tuning or using smaller, task-specific LLMs as a substitute for GPT-4o in the model-based planning framework?\n3. The paper mentions that tree search serves as an upper bound for the proposed method. Could further integration of tree search techniques within the MPC framework enhance performance without compromising efficiency?\n4. Since multi-step simulations present challenges, would it be feasible to integrate a mechanism for error correction in simulated trajectories? This might help mitigate the performance degradation observed in longer planning horizons.\n\nMinor typos:\n\nLine 50: \u201cTee Search\u201d instead of \u201cTree Search\u201d\n\nLine 143:  \"search necessitate backtracking to previous states.\" \"search necessitate\" should be \"search necessitates.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents an algorithm for improving web agents by incorporating planning into their execution. Instead of relying on full tree search or backtracking, the authors propose using the LLM's ability to predict the outcome of an action through a textual description, referred to as the \"internal world model\". Experiments show that, with a planning horizon of H=1, the agent evaluated in this way outperforms the reactive agent baseline."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea is simple and effective, and the paper is clear and easy to understand. The improvement demonstrated in the experiments is significant. In terms of novelty, the main idea is a straightforward yet novel combination of existing tools, which is fine. While it is possible that similar approaches have been proposed previously, I am not familiar enough with this area to give any specific example."
            },
            "weaknesses": {
                "value": "I like the simplicity of the main idea. However, I was a bit disappointed to learn that you're fixing the horizon to H=1. If I understand correctly, this means you essentially use the model to simulate the outcome of each relevant action and commit to the best one. While this is technically planning, it is borderline planning (just a bit more than scoring state-action pairs). You mention that \"our planning resembles MPC, which effectively manages error accumulation...,\" which is generally true, but such claims are trivialized when the horizon is a single step. In this case, phrases like \"pioneering study\" also seem a bit farfetched.\n\nWhat is the main issue that makes longer horizons unhelpful? Is it related to the hallucination issue you describe? I think the paper would benefit strongly from further development of the representations to make them reliable enough to increase the effective planning horizon. Otherwise, the approach may struggle to scale.\n\nSection 3.2 describes the representations and lists their advantages, but it would also be a good place to discuss their limitations. To clarify, I'm not asking for a critique, as any choice of representations is a tradeoff. However, discussing this tradeoff here would be both interesting and valuable for future development.\n\nThe text, while mostly clear, needs improvements. Some sections are overly wordy (e.g. introduction), which makes it harder to focus on the important parts. Some paragraphs are simply unnecessary (most clearly lines 522-525). Additionally, some paragraphs sound generated by an LLM, which isn't an issue if they are concise (though that\u2019s not always the case here). I recommend a careful proofreading.\n\nIt also seems that parts of Figure 2 are misplaced (e.g., \"Electronics\" is swapped with \"Office Products\")."
            },
            "questions": {
                "value": "- Is the tree search agent (upper bound) equivalent to your MPC, only using the perfect model instead of the LLM-based simulation? It's not clear, but I think it's not. Such an agent sounds like a true upper bound. Please discuss that.\n\n- Please specify which parts (i.e. components, algorithms, prompts, etc.) are shared by the main methods you compare (reactive, MPC, tree search).\n\n- In Section 4.2 you show that MPC exceeds the performance of tree search. How can it be the case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}