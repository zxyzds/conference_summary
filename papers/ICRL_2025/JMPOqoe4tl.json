{
    "id": "JMPOqoe4tl",
    "title": "$\\sigma$-zero: Gradient-based Optimization of $\\ell_0$-norm Adversarial Examples",
    "abstract": "Evaluating the adversarial robustness of deep networks to gradient-based attacks is challenging.\nWhile most attacks consider $\\ell_2$- and $\\ell_\\infty$-norm constraints to craft input perturbations, only a few investigate sparse $\\ell_1$- and $\\ell_0$-norm attacks.\nIn particular, $\\ell_0$-norm attacks remain the least studied due to the inherent complexity of optimizing over a non-convex and non-differentiable constraint.\nHowever, evaluating adversarial robustness under these attacks could reveal weaknesses otherwise left untested with more conventional $\\ell_2$- and $\\ell_\\infty$-norm attacks.\nIn this work, we propose a novel $\\ell_0$-norm attack, called $\\sigma$-zero, which leverages a differentiable approximation of the $\\ell_0$ norm to facilitate gradient-based optimization, and an adaptive projection operator to dynamically adjust the trade-off between loss minimization and perturbation sparsity.\nExtensive evaluations using MNIST, CIFAR10, and ImageNet datasets, involving robust and non-robust models, show that $\\sigma$-zero finds minimum $\\ell_0$-norm adversarial examples without requiring any time-consuming hyperparameter tuning, and that it outperforms all competing sparse attacks in terms of success rate, perturbation size, and efficiency.",
    "keywords": [
        "adversarial examples",
        "sparse attacks",
        "gradient-based attack",
        "machine learning security"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "The $\\sigma$-zero attack uses a differentiable approximation of the $\\ell_0$ norm for gradient-based optimization, with an adaptive projection operator that balances loss minimization and perturbation sparsity.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=JMPOqoe4tl",
    "pdf_link": "https://openreview.net/pdf?id=JMPOqoe4tl",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a novel $\\ell_0$-norm attack by leveraging a differentiable approximation of the $\\ell_0$-norm constraint. The approach can be applied to finding both minimum $\\ell_0$-norm and fixed budget adversarial examples. The authors conduct extensive evaluations on diverse datasets (e.g. CIFAR-10, ImageNet) and models. The results show that the proposed attack is effective and efficient in reducing the number of queries and memory usage."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The problem is well-motivated as $\\ell_0$-norm attacks are less studied due to the non-convexity nature.\n- The paper is easy to follow and well-organized\n- The evaluations are comprehensive and indeed show that the proposed algorithm is more effective and efficient."
            },
            "weaknesses": {
                "value": "- I appreciate the authors' efforts in evaluating the method. However, I was a bit confused by the inconsistency in some parts of the evaluations. For example, the memory usage is shown for minimum-norm attacks while it's missing in the bounded-norm attack results; The mean runtime for bounded attacks is shown in Tables 2 and 3 while it's missing in Tables 10 - 12; In Tables 10 - 12, the budget level that the number of queries corresponds to is also missing."
            },
            "questions": {
                "value": "- To enable a clearer comparison between different attacks, could the authors provide ASR under varying iterations and perturbation budgets for fixed-budget attacks, similar to Figure 2? Showing results on one model would suffice and help better understand the performance under different conditions.\n- The transferability of the proposed attack across different model and architectures is unclear. Would it be possible to evaluate the method on a different model/architecture than the one used for generating adversarial examples? Such an evaluation would strengthen the applicability of the attack across varied settings.\n- In the minimum-norm attacks, ASR of the three datasets are reported on the same budgets: $k = 24, 50,\\infty$. However, in the fixed-budget comparisons, the results of ImageNet are reported at budgets $k = 100, 150$ while the other two datasets are evaluated at $24, 50, 100$. Could the authors clarify the reasoning behind this difference?\n- Minor writing issues\n  - The bounded-norm attacks are not cited at the first appearance in Section 3.1\n  - In Table 2\u2019s caption, $q_{100}$ and $s_{100}$ are mentioned, but it seems these should be $q_{24}$ and $s_{24}$, based on the table\u2019s content."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces \u03c3-zero, a novel attack aimed at creating sparse adversarial examples under the \u21130 norm constraint. By using a differentiable \u21130 norm approximation and adaptive projection operator, the attack achieves high success rates across multiple benchmarks (MNIST, CIFAR-10, and ImageNet) and models. It outperforms existing sparse attacks in both efficiency and effectiveness."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **Innovative Approach**: \u03c3-zero combines a unique differentiable approximation of the \u21130 norm with adaptive projections, addressing challenges in sparse adversarial attack optimization with a gradient-based method.\n2. **Robust Evaluation**: The experimental setup is thorough, involving extensive model types and datasets, and a range of comparison attacks. Evaluation metrics like success rate, median perturbation size, runtime, and query count provide a clear assessment of \u03c3-zero's performance relative to existing methods.\n3. **Efficiency and Robustness**: \u03c3-zero demonstrates effective performance without the need for adversarial initialization or hyperparameter tuning, potentially broadening its applicability."
            },
            "weaknesses": {
                "value": "1. **Limited Innovation**  \n   The approach presented shows minimal originality and bears a strong resemblance to existing methods, with only slight modifications. These adjustments lack ingenuity, as they do not introduce new insights or significant advancements over prior work. The lack of a novel perspective limits the contribution of this study to the field.\n\n2. **Lack of Theoretical Understanding and Analysis**  \n   A major shortcoming of this work is the absence of a robust theoretical foundation. The paper lacks in-depth analysis to demonstrate the underlying principles or broader significance of the method. Without a theoretical understanding, the impact of the work remains superficial, leaving readers without a clear sense of the method\u2019s potential applicability or contribution to the advancement of adversarial robustness.\n\n3. **Limited Application Scope**  \n   All experiments focus exclusively on white-box attacks, which limits the generalizability of the findings. While white-box evaluations are useful for testing model robustness, the absence of analysis in other contexts raises concerns about the method\u2019s broader applicability. Expanding to include black-box or transfer-based attacks, for instance, could improve the study\u2019s relevance and highlight potential real-world applications.\n\n4. **Simplistic Model Choices in Experiments**  \n   Although the experiments involve several models, these are relatively simple in structure. The paper does not consider more complex architectures, such as ResNet-101 or models of similar or higher complexity, which are frequently used in evaluating ImageNet-related tasks. Given that larger and more sophisticated models are common in real-world applications, the study\u2019s reliance on simpler architectures limits the robustness and scalability of its findings.\n\n\n**Miner Comments:** \n1. There is an indexing error in Table 2 that needs to be addressed.\n2. Footnote 1 states, 'when the source point $x$ is already misclassified by $f $, the solution is simply $\\delta^* = 0$. This is quite puzzling, as attacks are typically conducted on samples that can be correctly classified. If the attack is performed on samples that are already misclassified, I question the persuasiveness of the higher ASR observed in the experimental results."
            },
            "questions": {
                "value": "**Q1**: In the introduction, you mention that \u201cevaluating adversarial robustness under these attacks could reveal weaknesses otherwise left untested with more conventional \u21132- and \u2113\u221e-norm attacks.\u201d Could you specify what types of model weaknesses can be exposed by \u21130 attacks that may remain undetected under conventional \u21132 and \u2113\u221e attacks? Examples or specific scenarios would be helpful for understanding.\n\n**Q2:** Have you considered any step size decay methods other than cosine annealing in your algorithm? Exploring more direct approaches might further reduce computational complexity.\n\n\n**Q3:** Does your method enhance the improvement or understanding of attacks using other norms? Based on your assertion that '\u21130-norm attacks, which perturb only a minimal fraction of input values, can identify the most sensitive features affecting the model's decision-making,' could your method be integrated into attacks with other norms to avoid targeting these sensitive features, thereby reducing the visual damage of adversarial examples?\n\n\n**Q4:** I appreciate your clarification that 'the goal of \u21130-norm attacks is not to be indistinguishable to the human eye\u2014a common misconception regarding adversarial examples\u2014but rather to demonstrate whether and to what extent models can be deceived by altering only a few input values.' However, in the visualizations of adversarial examples provided in your appendix, the perturbations lack semantic information comprehensible to humans. How can we understand and apply the adversarial perturbations generated by your method in this context?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a $\\ell_0$-norm adversarial attack that efficiently breaks various models trained on MNIST, CIFAR10, and ImageNet with 100% success rate. The proposed method is significantly faster than prior attacks at a high success rate. The proposed attack is composed of three main components:\n1) A differentiable relaxation of the $\\ell_0$ loss (Eq. 7).\n2) An adaptive projection to project near-zero components to zero. The threshold for projection, $\\tau$, is adapted dynamically such it is increased when the sample is adversarial and decreased otherwise.\n3) Cosine annealing of the learning rate."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- The proposed method is highly effective against a diverse set of models on multiple datasets and all attack budgets (Table 1-3, Figure 2). Particularly, on ImageNet $\\sigma$-zero improves the attack success rate at 100 by nearly 10% across all models (Table 3).\n- Compared with BBadv that often achieves similar ASRk values, the proposed method is 10-200x faster."
            },
            "weaknesses": {
                "value": "- I recommend moving the ablations on the components of the proposed method (Table 5) to the main body of the paper. For completeness, consider adding the missing row with normalization/adaptive $\\tau$ but without projection and dropping the column approximation as all rows rely on it."
            },
            "questions": {
                "value": "- Eq. 4 defines the regularized objective with a fixed regularization coefficient of $1/d$. How important is this normalization? Do you have any theories or ablations?\n- BBadv is said to be slow, particularly it relies on adversarial initialization. Would $\\sigma$-zero benefit in any way if it is also initialized the same way if we ignore the extra runtime?\n\nMinor:\n- Line 321: The sentence starts with \u201cDespite BBadv does not suffer \u2026\u201d needs grammar correction, e.g., substituting \u201cDespite\u201d with \u201cAlthough\u201d.\n- Consider annotating Figure 2 with subfigure captions.\n- How is table 4 ordered? It seems to be random."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a $\\ell_0$-norm attack, called sigma-zero, which leverages a differentiable approximation of the $\\ell_0$ norm to facilitate gradient-based optimization. The attack can find minimum $\\ell_0$-norm adversarial examples. The experiments show that sigma-zero exhibits good performance in different settings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is easy-to-follow\n\n2. The method is simple but effective\n\n3. The experiments are relatively comprehensive"
            },
            "weaknesses": {
                "value": "1. Despite the effectiveness of sigma-zero on different models, the evaluation on $\\ell_0$-robust models is missing. Please consider including the results of sAT / sTRADES [1], e.g., those trained on CIFAR-10, $k_{train}=6\\times20$ in pixel space.\n\n2. Due to the early-stopping mechanism widely adopted in various attacks, when the batch size is large, white-box attacks could run even faster than black-box attacks with the same iteration budget. Thus, I am still curious about the results of sigma-zero, sPGD and Sparse-RS with the same budget, e.g., N=10000. You can report the same metrics on a subset of representative models as in Table 13.\n\nI will adjust my score if the authors can address my concerns.\n\n[1]  Xuyang Zhong, Yixiao Huang, and Chen Liu. Towards efficient training and evaluation of robust models against l0 bounded adversarial perturbations."
            },
            "questions": {
                "value": "1. Typo on the title of Table 2: \"Columns $q_{100}$ and $s_{100}$ ... at $k=100$\", but $k$ is 24 in the below."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}