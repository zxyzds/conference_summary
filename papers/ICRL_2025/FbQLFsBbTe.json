{
    "id": "FbQLFsBbTe",
    "title": "FastCLIP: A Suite of Optimization Techniques to Accelerate CLIP Training with Limited Resources",
    "abstract": "Existing studies of training state-of-the-art Contrastive Language-Image Pretraining (CLIP) models on large-scale data involve hundreds of or even thousands of GPUs due to the requirement of a large batch size. However, such a large amount of resources is not accessible to most people. While advanced compositional optimization techniques for optimizing global contrastive losses have been demonstrated effective for removing the requirement of a large batch size, their performance on large-scale data remains underexplored and not optimized. To bridge the gap, this paper explores several aspects of CLIP training with \\textit{limited resources} (e.g., up to tens of GPUs). First, we introduce FastCLIP, a general CLIP training framework built on advanced compositional optimization techniques while designed and optimized for the {\\bf distributed setting}. Our framework is equipped with an efficient gradient reduction strategy to reduce communication overhead. Second, to further boost training efficiency, we investigate three components of the framework from an optimization perspective: the schedule of the inner learning rate, the update rules of the temperature parameter and the model parameters, respectively. Experiments on different strategies for each component shed light on how to conduct CLIP training more efficiently. Finally, we evaluate the performance of FastCLIP and the state-of-the-art training baseline (OpenCLIP) on different compute scales up to 32 GPUs on 8 nodes, and three data scales ranging from 2.7 million, 9.1 million to 315 million image-text pairs to demonstrate the significant improvement of FastCLIP in the resource-limited setting.",
    "keywords": [
        "Multimodal Learning",
        "Contrastive Learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=FbQLFsBbTe",
    "pdf_link": "https://openreview.net/pdf?id=FbQLFsBbTe",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces FastCLIP, a distributed training framework designed to optimize CLIP model training using compositional optimization techniques, removing the dependency on large batch sizes for effective model performance. The framework incorporates an efficient gradient reduction strategy to minimize communication overhead and conducts comprehensive ablation studies on various components, including learning rate schedules (constant vs. cosine), temperature parameter update rules, and different optimizers (AdamW, LAMB, Lion, and SGDM)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper studies an important problem in CLIP training, \"how to efficiently and effectively training CLIP models with limited resources\". \n\n2. This paper designs an efficient distributed training framework based on advanced compositional optimization techniques. It conducts ablation studies on the several key components during training, such as the update rule of learning rate, temperature parameter and model papers, providing valuable insights for future work in optimizing large-scale model training.\n\n3. Experimental results across various compute and data scales demonstrate that FastCLIP significantly outperforms existing methods, such as OpenCLIP, enhancing training efficiency on setups with up to 32 GPUs."
            },
            "weaknesses": {
                "value": "1. This paper is not designed for general CLIP training but is instead built on the assumptions and techniques from prior work[1]. As a result, its applicability may be limited to scenarios that align with these specific assumptions, restricting its generalizability to broader CLIP training tasks.\n\n2. The paper primarily builds on existing techniques, such as compositional optimization and gradient reduction strategies, without introducing fundamentally new concepts. While it refines and optimizes these methods for CLIP training, the core ideas themselves are not particularly novel.\n\n3. The paper's writing lacks clarity, making it difficult to follow at times. The structure of the paper would benefit from significant revision to improve its readability and logical consistency.\n\n4. The experiment compares FastCLIP with OpenCLIP, which is designed for large-scale clusters and requires large batch sizes. In contrast, FastCLIP is tested on a relatively smaller cluster (up to 32 GPUs), violating the original conditions under which OpenCLIP was evaluated. Additionally, the results in experiments 4 and 5 are inconsistent, with different methods showing unstable performance. This lack of stability weakens the conclusions and limits the insights that can be drawn for future work.\n\n[1] Yuan Z, Wu Y, Qiu Z H, et al. Provable stochastic optimization for global contrastive learning: Small batch does not harm performance. ICML 2022."
            },
            "questions": {
                "value": "See the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a distributed framework designed to improve the efficiency of CLIP by optimizing resource use and reducing dependency on large batch sizes and numerous GPUs. They use several optimization techniques, including a gradient reduction strategy and a flexible LR schedule, to facilitate efficient CLIP training with limited resources. The study compares FastCLIP to OpenCLIP and demonstrates consistent performance improvements on different scales of data and compute resources."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Efficient resource utilization for settings with limited computational resources, making CLIP training more accessible.\n\n- Systematic Optimizations: The paper provides a structured approach to optimizing multiple aspects of CLIP training, including LR schedules, temperature parameter updates, and gradient communication strategies.\n\n- Experimental Validation: Comprehensive testing on various datasets and compute scales effectively shows the benefits of FastCLIP compared to OpenCLIP."
            },
            "weaknesses": {
                "value": "- Limited Novelty in Optimization Techniques: The optimization techniques applied, such as learning rate decay and gradient reduction, are well-established and may not contribute novel methodological insights.\n\n- Resource Comparison Limitations: The paper does not conduct an extensive ablation study for larger datasets due to resource constraints, which may limit the generalizability of results on extremely large data scales.\n\n- Assumption of Availability of Multiple GPUs: Although designed for resource-limited environments, FastCLIP still requires access to multiple GPUs, which may limit applicability for highly resource-constrained users."
            },
            "questions": {
                "value": "How does the performance of FastCLIP vary across different types of GPUs or compute environments?\n\nCould other advanced optimization techniques (e.g., adaptive optimizers beyond AdamW) further enhance the framework's efficiency?\n\nWould additional experiments on extremely large datasets (>1 billion samples) align with the current findings for smaller datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduced FastCLIP, which uses data-parallel for CLIP training. It claimed to (1) present an efficient gradient reduction strategy to reduce communication overhead, (2) compare inner LR schedule between constant and cosine schedule, (3) compare temperature param update methods and (4) compare optimizers."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper compared CLIP training performances with different temperature parameter updating methods and different optimizers.\n2. The appendix provides detailed experiment results and parameter settings.\n3. An introduced cosine schedule could provide better training accuracy."
            },
            "weaknesses": {
                "value": "1. Figure 3 is not so clear about the efficiency improvement. The difference between OpenCLIP and FastCLIP is not significant.\n2. Figure 4 (a) does not show convergence on the accuracy curve which could be problematic.\n3. Figure 4 (b) (c) do not show a significant difference in scalability.\n4. The writing style needs improvement, and some part of this paper needs to be summarized. Also, the experiment part needs more details, the authors use many numbers in Table 3, 4, 5 but there is no information on what do they represent, accuracy, or some other metrics.\n5. Lack of novelty. This paper compared multiple combinations of existing CLIP training techniques and proposed using data parallelization to scale up the training process. From this summary, existing techniques do not contribute much to novelty except cosine LR schedule, also, data parallelism mentioned in this paper does not show promising results compared to OpenCLIP."
            },
            "questions": {
                "value": "1. See weaknesses.\n2. Is there any other significant difference between OpenCLIP and FastCLIP? Seems like they are also using data-parallel.\n3. What do the number differences represent in Table 3, 4, 5, and Figure 2? The current comparison is not very straightforward.\n4. Should the training accuracy converge?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}