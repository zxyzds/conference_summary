{
    "id": "kaqrwQ96xW",
    "title": "Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning",
    "abstract": "As text-to-image diffusion models become advanced enough for commercial applications, there is also increasing concern about their potential for malicious and harmful use. Model unlearning has been proposed to mitigate the concerns by removing undesired and potentially harmful information from the pre-trained model. So far, the success of unlearning is mainly measured by whether the unlearned model can generate a target concept while maintaining image quality. However, unlearning is typically tested under limited scenarios, and the side effects of unlearning have barely been studied in the current literature. In this work, we thoroughly analyze unlearning under various scenarios with five key aspects. Our investigation reveals that every method has side effects or limitations, especially in more complex and realistic situations. By releasing our comprehensive evaluation framework with the source codes and artifacts, we hope to inspire further research in this area, leading to more reliable and effective unlearning methods.",
    "keywords": [
        "unlearning",
        "diffusion model",
        "text-to-image model",
        "unlearning evaluation"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kaqrwQ96xW",
    "pdf_link": "https://openreview.net/pdf?id=kaqrwQ96xW",
    "comments": [
        {
            "summary": {
                "value": "The Holistic Unlearning Benchmark provides a comprehensive evaluation framework for text-to-image diffusion model unlearning. The benchmark assesses unlearning methods across five key aspects, revealing effectiveness on target concepts, faithfulness of images, compliance with prompts, robustness on side effects, and consistency in downstream applications. The authors aim to inspire further research for more reliable and effective unlearning methods by releasing the evaluation framework."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper introduces a novel and comprehensive framework specifically designed for benchmarking unlearning methods in text-to-image diffusion models, which is crucial for ensuring ethical and responsible AI usage. Specifically, the paper evaluates six methods from five different aspects. The experimental settings are clearly presented and the takeaway notes are interesting. Overall, it raises an awareness of evaluating unlearning methods for the generalization ability."
            },
            "weaknesses": {
                "value": "While the paper proposes an interesting evaluation framework for unlearning methods, it does not present an in-depth discussion in each experiment that technically analyzes how certain method affects the performance. Therefore, it lacks depth in providing a comprehensive technical analysis for current unlearning methods.\n\nUnmatched Content to Purpose: For instance\uff0cin Sec.5.2, the mentioned purpose is to find out how does unlearning change the underlying estimated distribution, while the experiment and the discussion only answer which methods change the distribution. This makes the content somewhat unmatched to its subtitle."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a holistic unlearning benchmark for text-to-image diffusion models, aimed at evaluating methods for removing undesired or potentially harmful content from generative models. The authors apply their benchmark to six existing unlearning methods, testing on four specific target concepts. The experimental results reveal that all tested methods have certain limitations or side effects when applied in practice."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-structured and clearly written, making it accessible and easy to follow.\n- By proposing a new benchmark for unlearning, the paper addresses an emerging direction in generative model research. Given the increasing power and prevalence of generative models, it is timely and important to examine ways to prevent harmful content generation."
            },
            "weaknesses": {
                "value": "### Limited Scope and Specificity of the Proposed Benchmark\n\n- **Narrow Methodology**: The benchmark focuses on only six existing unlearning methods, limiting its generalizability. It is tailored specifically to these methods rather than serving as a more widely applicable benchmark.\n- **Restricted Target Concepts**: The benchmark is tested on just four target concepts, which may not sufficiently represent real-world applications. Additionally, unlearning is often applied to remove harmful or inappropriate content, yet the chosen concepts do not necessarily reflect this priority. Given this specificity, the generalization claims made in the limitations section may be overstated.\n- **Dependency on GPT4o**: The evaluation heavily relies on GPT4o, introducing uncertainty into the results. As the GPT series rapidly evolves, the outcomes of the benchmark may vary across versions. While GPT4o capabilities may justify its use, further quantitative evidence is necessary to substantiate its role in the benchmark.\n\n### Lack of Experimental and Theoretical Support for Key Takeaway Messages\n\nThe paper presents several takeaway messages, but many lack sufficient experimental evidence or theoretical grounding, limiting their generalizability. Key examples include:\n\n- **Page 6**: The authors emphasize \"diverse and complex prompts,\" yet the prompts used in the experiments are limited to a few specific concepts. Furthermore, the discussion on balancing faithfulness and prompt compliance does not introduce significant new insights, as similar evaluations exist in works like ImageReward and PickScore.\n- **Page 7**: The concept of over-erasing is discussed, but it is primarily explored through specific, hand-crafted related concepts. For example, Appendix E categorizes a gas pump as a \"Machine\" and connects it to a coffee machine, while an English Springer is linked to other dog breeds. The considerable gap between these examples suggests that defining over-erasing and similar relationships requires a more comprehensive analysis.\n- **Page 9**: Distributional differences are studied using MNIST, a dataset with limited relevance to more complex image datasets. Testing on larger, more diverse datasets, such as ImageNet, would strengthen this analysis.\n- **Page 10**: The choice of downstream tasks and experimental setup may not align well with the study's primary focus on unlearning text-based concepts. The downstream tasks are primarily image-focused, creating a possible mismatch with the unlearning of textual content.\n\n### Conclusion\n\nThe limited scope of methods and target concepts, coupled with the reliance on GPT4o, raises concerns about the generalizability and long-term relevance of this benchmark in a rapidly evolving field. The takeaway messages presented lack substantial new insights, with many findings aligning with existing knowledge or lacking sufficient experimental support. Furthermore, the paper does not adequately demonstrate how this benchmark could be applied to effectively prevent harmful content generation in real-world settings. For these reasons, the contribution of this work may not yet meet the level of significance required for publication in this venue."
            },
            "questions": {
                "value": "please see weakness points above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Various unlearning techniques have emerged to prevent generating images related to copyright or NSFW content. Although previous methods also has been evaluated, their evaluations were limited to specific scenarios and lacked comprehensive analysis of various side effects. This paper conducts extensive research to uncover the limitations of these methods."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The observation that existing unlearning methods perform differently depending on the complexity of prompts used to test concept removal is insightful.\n- The paper suggests several ways to measure model performance more precisely, which contribute valuable analyses.\n  - The measurement of influence on related concepts in Section 5.1 appears to be a useful approach for thoroughly testing the over-erasing issue in unlearning.\n  - Section 5.2\u2019s sampling from an unconditional model provides a way to verify if the model\u2019s overall distribution has changed.\n-  I believe that the task this paper aims to address is important, and it provides valuable insights through some new tests."
            },
            "weaknesses": {
                "value": "- Although this is a paper focused on benchmarking and analysis, the number of concepts used was limited, and there were no experiments related to violence, nudity, or copyright issues - topics of particular interest in unlearning. Wouldn\u2019t it be more beneficial to increase the number of concepts rather than reduce the number of prompts for each? \n  \n- The value of some additional performance analyses is unclear for me.\n  - In Figure 2, using simple and diverse prompts does not seem to add significant distinction. Doesn\u2019t MS-COCO already contain a sufficient variety of simple and diverse prompts unlike the simple prompts for testing target concept?\n  - It\u2019s unclear why the task of selectively erasing target concepts in Section 4.3 is necessary. I believe a well-functioning unlearning algorithm may erase any prompt containing the target concept in real world scenario.\n  - I think making the model to prevent generation even when the user provides highly specific information in the downstream task(e.g. HED inputs) can harm the model's performance on downstream tasks. The task is contradictory. (whether to follow user instruction or ignore to erase the concept)\n\n- The paper would benefit from testing if the techniques remain robust against recently popular red-teaming methods, such as ring-a-bell and UnlearnDiffAtk."
            },
            "questions": {
                "value": "The questions are somewhat related to the disadvantages I wrote. \n- Is it possible to add NSFW or copyrighted character for benchmark concept? Actually, the small number of the concept is the primary concern of this review as a benchmark paper.\n- Could you explain the advantages of some of the additional analyses mentioned under weaknesses in more detail?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}