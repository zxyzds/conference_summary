{
    "id": "cselR6Jne3",
    "title": "Teaching LLMs to Decode Activations Into Natural Language",
    "abstract": "Interpretability methods seek to understand language model representations, yet the outputs of most such methods---circuits, vectors, scalars---are uninterpretable, requiring further effort to interpret. In contrast, we propose to study LatentQA, the task of answering open-ended questions about model activations in natural language. Towards solving LatentQA, we propose Latent Interpretation Tuning (LIT), which finetunes a decoder LLM on a dataset of activations and associated question-answer pairs, similar to how visual instruction tuning trains on question-answer pairs associated with images. We use the decoder for diverse reading applications, such as extracting relational knowledge from representations or uncovering system prompts governing model behavior. Our decoder also specifies a differentiable loss that we use to control models, such as debiasing models on stereotyped sentences and controlling the sentiment of generations. Finally, we extend LatentQA to reveal harmful model capabilities, such as generating recipes for bioweapons and code for hacking, from safety-tuned models, even when given benign prompts.",
    "keywords": [
        "llm safety",
        "llm interpretability",
        "ml safety",
        "activation steering"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We train an LLM to answer open-ended questions about model latents and use it for interpretability and control",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cselR6Jne3",
    "pdf_link": "https://openreview.net/pdf?id=cselR6Jne3",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a new task LatentQA. It aims at improving interpretability and control in language models (LLMs) by transforming latent activations into language responses. To facilitate this, the authors propose Latent Interpretation Tuning (LIT), which fine-tunes a decoder model to answer open-ended questions about model activations. By doing so, the decoder can provide insights into latent model tendencies, biases, and behavioral control. Authors provide comprehensive and various tasks for the experimental setting and showcase the advanced model performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper propose a new dataset, LatentQA, aiming to improve interpretability and control in language models (LLMs) by transforming activations into language responses.\n2. This paper includes comprehensive experiments and anlysis\n3. This paper provides some insights from explaining activations in LLM."
            },
            "weaknesses": {
                "value": "1. This paper is hard to follow at the first glance. The term 'activation' should be explained clearer at the beginning.\n2. When comparing with Patchscope, it would be fair and convincing to use the same LLaMA3.1-8b in your method for Patchscope.\n3. In the controllable generation, control refers to persona, I would like to see other types of control in the experiment, e.g., topics\n4. Methods like DExpert and Patchscope are training-free methods, while your LIT needs finetuning LLMs. The comparison seems to be unfair. At least you should compare the training/inference latency between different strategies.\n5. There's only one LLM employed for the task, I would expect to see more LLMs to be tested.\n\nI would be happy to raise my score once my concerns are addressed."
            },
            "questions": {
                "value": "1. What is LoRRA finetuning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this work, the authors explore an interesting problem of LLMs to interpret their own latent representations (activations). They curated a dataset called LATENTQA with over 1M QA pairs and used it to train a decoder specifically designed for this purpose. The authors demonstrate a range of valuable applications for their proposed model, including reading model latent representations and controlling model behavior. Compared to existing methods, the trained decoder achieves a significant performance improvement."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The studied problem, directly decoding activations into natural language,  is interesting and unique. \n\n2. They contribute a unique large-scale dataset with 1.2 million data points specifically designed for this task, which is a valuable resource for the community.\n\n3. The authors investigate a diverse range of applications for their approach, demonstrating its effectiveness across several tasks. These include extracting relational information from latent representations, decoding personas embedded in system prompts, controllable sentiment generation, debiasing, and eliciting harmful capabilities. \n\n4. The proposed method shows promising results across these applications."
            },
            "weaknesses": {
                "value": "1. For the experiments, there\u2019s a concern about fairness in comparison. Since the proposed model is trained on a large-scale dataset, many of the baselines used are training-free methods. Comparing these directly may not be entirely fair, and it may lead to overclaiming the observed performance improvements.\n\n2. Another concern is the generalization capability of the model. Specifically, it would be insightful to test whether it\u2019s possible to achieve cross-model understanding, such as using activations from one model (e.g., LLaMA) to interpret the behavior of a different model (e.g., Mistral). \n\n3. The authors frequently mention \"interpretability,\" but the connection between this approach and interpretability is not entirely clear. It\u2019s challenging to see how the studied applications, such as decoding latent information or controlling behaviors, directly contribute to a deeper understanding of the model\u2019s internal workings."
            },
            "questions": {
                "value": "Here is a list of questions for the authors:\n\n1. **Practical Use of Predicting Future Model Behavior**: What are the practical applications of predicting future model behavior based on latent representations, and why is this an important problem to investigate?\n\n2. **Contextualization Using WikiText-103**: \n   - (a) Since WikiText-103 is used to extract phrases containing the subjects, these phrases may also include objects. Could this lead to potential data leakage?\n   - (b) Do other baselines also use WikiText-103 for contextualization? If not, could this create an unfair comparison?\n\n3. **Determining the Appropriate Layer for Activations**: In the footnote, it mentions \"during execution replace the activations of ??? with [ACT] at the appropriate layer.\" How is the \u201cappropriate layer\u201d determined for replacing activations? Section 4 notes that the activations of the 15th layer in the target model are used as input, based on early experiments. Could you provide more detail on how these early experiments guided the choice of layer? \n\n4. **Choice of Layer for Replacement**: Why do you consistently replace activations at layer 0, rather than other layers? Is this choice shown to be optimal, or were other layers considered as well? Has any analysis been conducted to evaluate whether this layer is indeed the most effective for achieving the desired outcomes?\n\nThese clarifications would help in understanding the setup and methodology more thoroughly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "On line 462, the authors suggest that the proposed method has the potential to generate harmful content across various domains, including the creation of recipes for bioweapons and cyberweapons. Although the authors argue that their use of publicly available models (e.g., Llama 3) is unlikely to significantly impact the public\u2019s capacity to develop harmful models, an additional ethics review could still be beneficial."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose the task and dataset for LatentQA -- a set of ([ACT], question, answer), where [ACT] is the model activation when prompted with the question and answer. This question and answer is attributed with stimuli -- something like a persona, or style to answer the question. For example, given an original question \"What should I have for dinner?\" the authors add a stimuli like \"Answer as if you were a billionaire living on a private island\", and get the model's answer by: original_question + stimuli. Next, the authors ask GPT-4 to generate QA pairs to describe the properties of the model's answer.\n\nThis LatentQA dataset is then used to train a decoder to produce the answer given the question, and patching [ACT] to the decoder.\n\nNext, this decoder can be used to read/control LLM behavior. The experimental result is based on this read and control task"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- An important and novel problem: how to interpret LLM activation in natural language\n- Clever way to materialize this motivation into an idea\n- I like that the resulting decoder can be used to both interpret activation and control LLM behavior"
            },
            "weaknesses": {
                "value": "Execution of a great idea that falls short. For several reasons:\n1. I find patching activation from layer 15 to layer 0 weird. Wouldn't the model just completely fail? When you get layer 15's activation, it is already accumulating outputs from layers 0-14, and thus numerically will be very different from the original distribution of the model's layer 0 activation. I also fail to understand the reasoning behind doing this.\n2. The method is model specific -- so each time a person wants to do this for a different model, they need to collect 1.2 data points from that model? they can probably still use the same QA provided by the original LatentQA dataset, but it is still a heavy price.\n3. Some crucial writing needs to be made more clear (check Questions)\n4. On 5.1 results: I do not find this evaluation fair (comparing training-free method with training-based method); are there other training-based methods you can compare to?\n5. On 5.1: There is a debiasing specific activation steering method: https://arxiv.org/pdf/2406.03631 have you tried to compare debiasing results with this method?\n6. On 5.2: LIT seems to perform the worst on 'Generate Positive'."
            },
            "questions": {
                "value": "1. Is dialog = control + control response? (lines 214-215) \n2. What do you mean by repeatedly calling STEER(ACT, \"control\")? (lines 258-259)\n3. Is STEER(ACT, \"control\") the gradient computed when doing LoRA fine-tuning on the \"control\" question-answer string, and patching the activation with [ACT]?\n4. Why do you need to sample activations from prompts from he Databricks Dolly dataset on top of the stuffs you already have on LatentQA?\n5. In my understanding, RepE is a fully training free, representation engineering method. Why did you use LoRRA fine-tuning with RepE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces **LatentQA**, a novel interpretability task designed to translate large language model (LLM) activations into natural language, making model internals more accessible and understandable for humans. Unlike traditional interpretability techniques that yield abstract outputs (e.g., vectors or circuits), LatentQA enables direct, human-readable interpretations of model behavior. To achieve this, the authors constructed a comprehensive LatentQA dataset, created with the aid of GPT-4 to generate question-answer pairs that describe qualitative attributes of model activations. They further developed a method called **Latent Interpretation Tuning (LIT)** to fine-tune an LLM to perform LatentQA, effectively enabling it to \"caption\" model activations with descriptive language. This approach not only advances interpretability but also offers a means to control model behaviors, such as reducing biases and steering sentiment, through its differentiable, language-based output."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed LatentQA task introduces a groundbreaking method for interpreting LLM activations by translating them directly into natural language. This approach is more user-friendly and accessible compared to existing interpretability techniques, which often rely on complex, abstract outputs.\n\n2. LIT uniquely combines interpretability with control, enabling it not only to decode and explain LLM activations but also to steer model outputs in desired directions, such as adjusting biases or modifying response tones.\n\n3. LIT offers a powerful approach for identifying and revealing potentially harmful model behaviors, making it a valuable tool for safety auditing and alignment."
            },
            "weaknesses": {
                "value": "1. The paper would benefit from an improved pipeline diagram to clarify the process flow.\n\n2. LatentQA heavily relies on extensive data and faces challenges in scaling efficiently. The model\u2019s interpretative ability is limited to the specific personas and activations it was trained on, which restricts its generalizability. For example, it can only interpret personas included in its training dataset. It would be valuable to assess whether LatentQA can perform well in a zero-shot setting, such as on tasks similar to those in [1].\n\n3. The paper states that LatentQA selects the 15th layer's activations as input for the decoder, supported by early experimental results. However, intuitively, activations from later layers in large language models may be better learned and thus more effective. An ablation study analyzing the impact of different layers\u2019 activations as input would enhance the robustness of the findings.\n\n4. The dataset used is synthetically generated, which could introduce biases or noise. A more detailed description of the dataset construction process, along with an analysis of its diversity, would strengthen the paper's validity.\n\n[1] Hendel, Roee, Mor Geva, and Amir Globerson. \"In-context learning creates task vectors.\" arXiv preprint arXiv:2310.15916 (2023)."
            },
            "questions": {
                "value": "Will the data used in this study be publicly accessible?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}