{
    "id": "Usa4pF1e5I",
    "title": "SLiM: One-shot Quantized Sparse Plus Low-rank Approximation of LLMs",
    "abstract": "Large Language Models (LLMs) have revolutionized natural language understanding and generation tasks but suffer from high memory consumption and slow inference times due to their large parameter sizes. Traditional model compression techniques, such as quantization and pruning, mitigate these issues but often require retraining to maintain accuracy, which is computationally expensive. This paper introduces SLiM, a novel approach for compressing LLMs using a one-shot Quantized Sparse Plus Low-rank Approximation. SLiM eliminates the need for costly retraining by combining a symmetric quantization method (SLiM-Quant) with a saliency-based low-rank approximation. Our method reduces quantization error while leveraging sparse representations compatible with accelerated hardware architectures. Additionally, we propose a parameter-efficient fine-tuning recipe that significantly reduces overhead compared to conventional quantization-aware training. SLiM achieves up to a 5.4\\% improvement in model accuracy for sparsity patterns like 2:4, and the fine-tuning step further enhances accuracy by up to 5.6\\%, demonstrating state-of-the-art performance. This work provides a pathway for efficiently deploying large models in memory-constrained environments without compromising accuracy.",
    "keywords": [
        "sparsity",
        "2:4 sparsity",
        "quantization",
        "low-rank",
        "lora"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We introduce SLiM, a one-shot quantized sparse plus low-rank method for compressing LLMs without retraining, reducing memory and inference costs while boosting accuracy by up to 5.6%, offering efficient deployment in memory-constrained environments.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Usa4pF1e5I",
    "pdf_link": "https://openreview.net/pdf?id=Usa4pF1e5I",
    "comments": [
        {
            "summary": {
                "value": "The manuscript proposes an LLM compression method that combines quantization, sparsity, and low-rank approximation. This approach applies low-rank approximation guided by a significance-driven optimization function, followed by fine-tuning of LoRA blocks, ultimately achieving advanced improvements in compression accuracy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Previous algorithms aimed at compression during inference generally do not retain LoRA. This manuscript innovatively retains it, and demonstrates its feasibility in terms of time efficiency through inference efficiency analysis.\n\n2. The idea of using saliency to guide the optimization of LoRA is highly novel."
            },
            "weaknesses": {
                "value": "1. The motivation for introducing Equation 8 is unclear. For instance, the authors claim that deriving LR from E_Q and E_S is intended to \u201ccancel the compression errors\u201d, yet introduce a significance function F that does not seem directly related to these compression errors.\n\n2. Prior to Equation 10, the significance calculation method from Wanda is introduced without clear motivation. To my knowledge, there are alternative methods for measuring significance, such as AWQ[1] and SparseGPT[2], yet the authors appear not to consider them.\n\n3. In Section 3.2, the manuscript claims that previous error minimization methods did not consider \"the importance of different weights\", though methods like OBS[3] clearly address the importance of weights (see its explanation of L_q).\n\n4. The manuscript does not explain the pruning approach used and lacks relevant formula descriptions for E_S.\n\n5. There is a lack of comparison with advanced joint strategies for sparse quantization, such as JSQ[4].\n\n6. The manuscript lacks statistics on model size, which may lead to unfair comparisons. Since SLIM introduces additional LoRA layers with a substantial rank (r=0.1), describing the extra parameter overhead is necessary. Similarly, theoretical computations for the number of OPs are also not provided.\n\n[1] Lin, et al. AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration[J].\n\n[2] Frantar, et al. Sparsegpt: Massive language models can be accurately pruned in one-shot.\n\n[3] Hassibi, et al. Optimal brain surgeon and general network pruning.\n\n[4] Guo, et al. Compressing large language models by joint sparsification and quantization."
            },
            "questions": {
                "value": "1. Significance is usually used to guide pruning decisions, so why is the significance calculation method proposed in Wanda used as the optimization target for quantization and sparsification? Why does optimizing the significance metric result in better low-rank adapters?\n\n2. Why does not pruning weights in LQ-LORA lead to a significantly higher error rate?\n\n3. Why does the discussion on Inference Speedup not consider the impact of quantization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose to combine quantization and pruning to compress LLMs. A new quantization method called SLiM-Quant is proposed to reduce the quantization error. To develop the new method, the mean squared error used in the literature was converted to a probabilistic formulation, which is solved by numerical integration over the weight histogram. Then a pruning method (structured or unstructured pruning) is applied. To compensate for the accuracy loss due to pruning and quantization, a new low-rank adapter is proposed, which is based on the assumption of an additive invertible saliency function. Finally, the low-rank adapters are quantized in a manner of 16x16 tiles, which is finally merged into the quantized sparse weight."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Experimental results show that the proposed method leads to improved model accuracy under the same pruning and quantization scheme.\n\n2. A novel low-rank adapter is proposed in this paper."
            },
            "weaknesses": {
                "value": "1. But why is the proposed low-rank adapter not compared with the previous low-rank adapter? Without such a comparison, it's hard to tell the advantage of the proposed method.\n\n2. The combination of quantization and pruning seems to be a straightforward method. In this paper, the two methods seems to be loosely coupled. As mentioned in Questions, the joint optimization has some unanswered questions.\n\n3. Actually acceleration performance is not reported in this paper."
            },
            "questions": {
                "value": "1. When merging the quantized sparse weight and low-rank adapters, there might be overflow due to addition. The non-zero values from the low-rank adapters might also be added to originally the pruned weights, which disturbs the pruning mechanism (in particular the 2:4 structured pruning). How should those potential problems during weight merging be solved?\n\n2. What is the evaluation metric used in Table 1-3?\n\n3. Does the quantization method proposed in this paper lead to zero values?\n\n4. The paper did not propose a new pruning method. But in Table, SLiM works better than other methods under a pruning-only scheme.\n\n5. The additive invertible saliency function comes from nowhere. The property is not explained in the supplementary either."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces SLIM, a method for compressing Large Language Models (LLMs) using a one-shot approach that combines quantization, sparsity, and low-rank approximation. It reduces memory usage and inference time without retraining, while maintaining high accuracy. SLIM also includes an efficient fine-tuning process, achieving up to 5.8% accuracy improvement over existing compression techniques, making it ideal for deploying large models on resource-constrained hardware."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The proposed symmetric quantization minimizes quantization error without altering the weights, resulting in reduced computational and memory overhead.\n2. SLIM introduces a saliency-based method to reconstruct weights based on their importance, targeting weights with the highest impact on model output, which helps retain model accuracy. \n3. SLIM improves model accuracy by up to 5.4% over state-of-the-art pruning and quantization methods for 2:4 sparsity patterns. With fine-tuning, the accuracy gain can increase to 5.8%.\n4. The fine-tuning recipe for sparse, quantized models significantly reduces the time and resources needed for fine-tuning. For example, a 13B parameter model that typically takes 36 days to fine-tune is reduced to just 14 hours on an H100 GPU."
            },
            "weaknesses": {
                "value": "1. The specific method for determining the quantization scale in L199-L202 should be described in more detail to clarify the contribution. Some background information (e.g., symmetric quantization, description of quantization errors, etc.) could be shortened or moved to the supplementary material to reduce the length.\n2. Some more advanced quantization methods [1-5] significantly outperform GPTQ. Would combining these methods further enhance compression performance?\n3. The combination of these different compression methods (sparsity, quantization) should result in a higher compression rate. However, related experiments on the memory footprint of the compressed model and the analysis of inference efficiency are not included in the paper.\n\n\n[1] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. ICML 2023.\n[2] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. MLSys 2024.\n[3] OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. ICLR 2024.\n[2] AffineQuant: Affine Transformation Quantization for Large Language Models. ICLR 2024.\n[3] QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs. Arxiv 2024."
            },
            "questions": {
                "value": "1. The issues mentioned in the weaknesses should be addressed.\n2. In L205-206, why can the impact of weight matrix quantization and pruning be modeled as additive noise? Is there any experimental or theoretical support for this claim? Why wouldn\u2019t the combination of the two errors introduce larger errors? Intuitively, if $W^Q= W+ E_Q$ and $W^S =W+ E_S $, why would $W^Q+W^S=W_C$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}