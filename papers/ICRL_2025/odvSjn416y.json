{
    "id": "odvSjn416y",
    "title": "Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models",
    "abstract": "Instruction-tuned language models (LM) are able to respond to imperative commands, providing a more natural user interface compared to their base counterparts. In this work, we present Promptriever, the first retrieval model able to be prompted like an LM. To train Promptriever, we curate and release a new instance-level instruction training set from MS MARCO, spanning nearly 500k instances. Promptriever not only achieves strong performance on standard retrieval tasks, but also follows instructions. We observe: (1) large gains (reaching SoTA) on following detailed relevance instructions (+14.3 p-MRR / +3.1 nDCG on FollowIR), (2) significantly increased robustness to lexical choices/phrasing in the query+instruction (+12.9 Robustness@10 on InstructIR), and (3) the ability to perform hyper-parameter search via prompting to reliably improve retrieval performance (+1.4 average increase on BEIR). Promptriever demonstrates that retrieval models can be controlled with prompts on a per-query basis, setting the stage for future work aligning LM prompting techniques with information retrieval.",
    "keywords": [
        "retrieval",
        "instructions",
        "search",
        "prompts"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We build a dense retrieval model that can take free forms prompts like a language model",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=odvSjn416y",
    "pdf_link": "https://openreview.net/pdf?id=odvSjn416y",
    "comments": [
        {
            "summary": {
                "value": "Currently, retrievers are only able to retrieve texts similar to input queries, mostly with text similarity. In this paper, the authors present Promptriever, the first retrieval model able to be prompted with textual instructions. \n\nFor example, the users can pass in complex instructions to filter the passages to be relevant to a specific topic or exclude certain categories of passages.\n\nTo do so, the authors create and release a synthetic dataset of query-passage relevance pairs augmented with instructions. They use the MS MACRO dataset and generate instructions using Llama-3-70B, which includes diverse length formats and styles. Then, they use GPT-4o to generate the instruction negative passages. The Promptriever is then trained on the augmented data. \n\nAs a result, the Promptriever maintains strong retrieval scores in standard settings. Compared to the original RepLlaMA, it also follows instructions better. The authors also perform multiple ablations such as the instruction-negatives."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The motivation is strong and the story is convincing. The authors identify the retrievers' current lack of instruction-following abilities to motivate the method. They help to bridge the gap by introducing Promptriever.\n2. The experiment performance is promising and shows improvements in various datasets.\n3. The ablation studies are sufficient and necessitate the needs of the different training components."
            },
            "weaknesses": {
                "value": "See questions."
            },
            "questions": {
                "value": "1. Can you list out some use cases where the promptriever is used in scenarios such as RAG? How would it perform?\n2. It would be nice to see some qualitative examples of Promptriever compared to traditional retrievers, besides the example in the intro.\n3. You mentioned in L240 that cross-encoders perform best due to their significant compute advantage. Could you list out the compute resources required by different baselines and Promptriever?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors introduce an instruction-aware retriever named Promptriever, which can be prompted like an LM. They build a dataset for training such a retriever based on MS MARCO. They conduct experiments based on the dataset, and reveal several interesting conclusions.\n\n\nPros:\n- The problem discussed in the paper, i.e., building instruction-following IR models, is interesting. \n- The problem definition is clear. The proposed method is easy to follow.\n- Experimental results verify the effectiveness of the proposed method.\n\n\nCons:\n - The dataset is generated solely based on MS MARCO. It would be great if more datasets could be considered, especially for the test. BEIR should also be considered for the zero-shot scenario.\n- It would be great if the definition of instructions in IR could be clearly defined. For example, given the example \"Which type of volcano eruption has not been seen?\", the authors claim that the volcano types and formation can be added as additional instructions. I wonder why these are treated as additional instructions but not part of the original information need (i.e., the original queries).\n- More details about the experimental setup should be provided. For example, are the baseline models trained using similar assessment data (with instructions in the queries)?\n- Better baseline models should be added. For example, we can generate query rewrites via LLMs and then retrieve documents using the rewritten query (converting the NLP-like instruction into keyword-like queries)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Pros:\n- The problem discussed in the paper, i.e., building instruction-following IR models, is interesting. \n- The problem definition is clear. The proposed method is easy to follow.\n- Experimental results verify the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "Cons:\n - The dataset is generated solely based on MS MARCO. It would be great if more datasets could be considered, especially for the test. BEIR should also be considered for the zero-shot scenario.\n- It would be great if the definition of instructions in IR could be clearly defined. For example, given the example \"Which type of volcano eruption has not been seen?\", the authors claim that the volcano types and formations can be added as additional instructions. I wonder why these are treated as additional instructions but not part of the original information need (i.e., the original queries).\n- More details about the experimental setup should be provided. For example, are the baseline models trained using similar assessment data (with instructions in the queries)?\n- Better baseline models should be added. For example, we can generate query rewrites via LLMs and then retrieve results using the rewritten query (converting the NLP-like instruction into keyword-like queries)."
            },
            "questions": {
                "value": "- How are the baseline models trained? Are they using similar assessment data (with instructions in the queries)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "na"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Promptriever, a bi-encoder model that enriches the per-instance query context within prompts. To train this model, per-query instructions and instruction negatives are synthetically generated. The authors evaluate Promptriever across various scenarios, including instruction-following datasets, in-domain retrieval, and out-of-domain retrieval. The model not only surpasses previous bi-encoders but also achieves performance comparable to state-of-the-art cross-encoder retrievers, showcasing its effectiveness in handling free-form language prompts."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper introduces a novel idea by challenging the assumption that the same instruction can be applied uniformly across queries.\n- The authors curate a training dataset that serves as a solid resource for providing detailed, per-instance instructions.\n- I particularly liked the analysis in Section 5, which addressed several questions I had while reading the methodology and experimental results sections."
            },
            "weaknesses": {
                "value": "- The generated instructions are only used during model training. It would have been insightful to hold out a dev or test split to evaluate how the model performs on this dataset.\n- For BeIR, the use of generic instructions feels too broad. Task-specific instructions, such as those in TART, might have been more effective and better aligned with the goals of the paper."
            },
            "questions": {
                "value": "- For out-of-domain retrieval, what is the rationale behind selecting the best score out of 10 prompts, rather than fixing a prompt or reporting an average score?\n- Could the authors provide some qualitative examples illustrating how Promptriever performs across different categories in Table 6?\n- In Table 11, simpler prompts like \u201cThink carefully about relevance\u201d seem to yield strong performance. Why might these simpler prompts outperform other more detailed ones that closely match the training data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents Promptriever, an innovative retrieval model that incorporates instruction-following capabilities akin to LLMs. By curating a new instruction training set from MS MARCO, the authors developed a bi-encoder retriever that adjusts its responses based on natural language instructions, demonstrating substantial improvements in retrieval tasks. Specifically, Promptriever achieves state-of-the-art results on instruction-following retrieval tasks, showing significant robustness to query phrasing and the ability to perform hyperparameter search via prompting. This model sets a new precedent for future work in aligning LLM prompting techniques with IR systems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The focus on enabling retrievers to understand human instructions aligns well with the current demand for more intuitive and adaptable search technologies, marking a significant advancement in retrieval systems.\n- The methodological approach of not requiring human annotations is notable, offering a scalable and cost-effective solution for training retrieval systems.\n- The clarity of the paper and its comprehensive details support replicability and transparency, which are crucial for advancing research in this area."
            },
            "weaknesses": {
                "value": "My main concern is about the **experiments**.\n- The t-test is not conducted on the experimental results. In IR, it is very important to validate whether the improvement is significant.\n- The use of varying amounts of training data for different models (Promptriever vs. RepLLaMA) raises concerns about the fairness and validity of the comparative performance analysis.\n- The performance improvement on in-domain retrieval is very marginal. I guess the instruction-tuning process may affect the original retrieval performance.\n- In Table 5, the selected baselines are not strong on BEIR benchmark. For example, E5-mistral has achieved 56.9 on BEIR. I think the authors should compare their methods with more advanced baselines.\n- A minor suggestion: please use `` and \u2018\u2019 (two single quotes) to generate double quotes in LaTeX."
            },
            "questions": {
                "value": "Please consider to address my concerns in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}