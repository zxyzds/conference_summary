{
    "id": "lgnAEBE1Xq",
    "title": "Contrastive Unlearning: A Contrastive Approach to Machine Unlearning",
    "abstract": "Machine unlearning aims to eliminate the influence of a subset of training samples (i.e., unlearning samples) from a trained model. Effectively and efficiently removing the unlearning samples without negatively impacting the overall model performance is challenging. Existing works mainly exploit input and output space and classification loss, which can result in ineffective unlearning or performance loss. In addition, they utilize on unlearning or remaining samples ineffectively, sacrificing either unlearning efficacy or efficiency. Our main insight is that direct optimization on the representation space utilizing both unlearning and remaining samples can effectively remove influence of unlearning samples while maintaining representations learned from remaining samples. We propose a contrastive unlearning framework, leveraging the concept of representation learning for more effective unlearning. It removes the influence of unlearning samples by contrasting their embeddings against the remaining samples' embeddings so that their embeddings are closer to the embeddings of unseen samples. Experiments on a variety of datasets and models on both class unlearning and sample unlearning showed that contrastive unlearning achieves the best unlearning effects and efficiency with the lowest performance loss compared with the state-of-the-art algorithms.",
    "keywords": [
        "Machine unlearning",
        "Privacy",
        "Contrastive learning"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We propose a novel machine unlearning approach based on contrastive learning.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=lgnAEBE1Xq",
    "pdf_link": "https://openreview.net/pdf?id=lgnAEBE1Xq",
    "comments": [
        {
            "summary": {
                "value": "In this paper, the authors propose a new machine unlearning method, inspired by contrastive learning. \n\nThe proposed contrastive unlearning pushes away the positive pairs (from the same class) and pulls the negative pairs close to each other. \n\nThe idea is validated on several backbones and datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- It seems that the contrastive paradigm is new for the field of unlearning. \n- The experimental results, from both efficiency and efficacy, seem to show the advantages of the proposed contrastive unlearning (while I'm quite confused about *unlearning acc*, which will be elaborated in Weaknesses and Questions)."
            },
            "weaknesses": {
                "value": "I'm not familiar with machine unlearning. I read some papers from recent ML conferences, such as ICML, NeurIPS, and CVPR, and try my best to provide a fair review. Some comments may be naive and I'd like to update my score after reading the response from the authors and reviews from other reviewers. Here are my concerns:\n\n- I'm quite confused about the setting of single class unlearning. If we expect a model to forget a class, why don't we just add some rules? For a simple classification task, the rule could be very simple: assign random labels if the model outputs the class to unlearn. The rule may be useless for more powerful models, such as zero-shot classifiers (e.g., CLIP) and LLMs. However, the authors fail to conduct experiments on these models. \n\n- Some visualization may be helpful for the \"contrastive\" unlearning. It is quite common to show some t-SNE visualization in contrastive learning. With the visualization, we can know whether the unlearning samples are pushed to the decision boundary. \n\n- In Eq. (5) and Eq. (6), $z_i$ is not normalized, which is a standard step in contrastive learning. In CL, $z_i$ is usually processed by $z_i = \\frac{z_i}{\\|\\|z_i\\|\\|}$. Is it a special setting to ensure the performance?\n\n- The theoretical analysis is lacking. Although it is not necessary, some theoretical analysis may help to improve the quality of this paper. \n\n- There are some typos, such as  $ \\mathcal D_{ts} \\cup \\mathcal{D}_{tr} = \\emptyset $ in Line-198."
            },
            "questions": {
                "value": "- Why do the authors test the single class unlearning only on simple classification models? \n- How will the embeddings of unlearning samples change? Are they really pushed to the decision boundary? \n- If we normalize the representation $z_i$ in Eq. (5) and (6), how will the performance change?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel framework for machine unlearning, which is the process of removing the influence of specific training samples from a trained model. The authors propose a method called contrastive unlearning that leverages representation learning to effectively eliminate the impact of unlearning samples while maintaining the model's performance on the remaining data. The approach contrasts the embeddings of unlearning samples with those of the remaining samples, adjusting the model's representation space to make the unlearning samples' embeddings similar to those of unseen test samples. The paper concludes that contrastive unlearning is a promising technique for machine unlearning, with potential applications in complying with privacy regulations. Through extensive experiments on various datasets and models, the paper demonstrates that contrastive unlearning outperforms existing methods in terms of unlearning efficacy, model performance preservation, and computational efficiency."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper introduces contrastive learning to the machine unlearning, effectively removing the influence of unlearning samples without significant loss in model performance. Contrastive unlearning is computationally efficient, requiring fewer iterations to achieve the desired unlearning effect compared to other methods.\n\n2. The experiments are comprehensive, including accuracy gap and membership inference attack for unlearning performance. They convinced me of the SOTA performance of the proposed contrastive paradigm."
            },
            "weaknesses": {
                "value": "1. Weak scalability compared to non-contrastive methods. The proposed contrastive learning method constructs the positive pair and negative pair in a batch, hence, it might need a large batch size in the practical application which has a large class number. So I suggest using a MoCo-like, more advanced contrasive learning method to further enhance the scalability of the proposed method. \n\n2. The assumption for test samples is a little bit strong, e.g., \" If the embeddings of the unlearning samples become indistinguishable from the embeddings of the test samples, we can claim that the model is no longer influenced by the unlearning samples\". We usually assume the training and test examples follow I.I.D. However, it seems that this paper did not have this assumption."
            },
            "questions": {
                "value": "1, Could you provide some discussion for the limits regarding the number of unlearning samples. More unlarning samples in the contrastive framework might damage the representation of ramaining samples.\n2. Following the weakness 2, do the existing methods have the same assumption as line 62-65?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The author introduces a novel form of contrastive loss tailored to the machine unlearning paradigm. The main insight is to selectively remove the most relevant features of the unlearning samples while preserving the quality of embeddings for the remaining samples. Experimental results demonstrate that the proposed contrastive loss achieves superior performance on single-class and sample-unlearning tasks, surpassing state-of-the-art methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed supervised contrastive learning framework is well-structured and easy to understand. By constructing positive and negative pairs, the encoder effectively deactivates the embedding of unlearning samples.\n2. The framework performs well across three benchmarks, showing improved unlearning efficiency and effectiveness over existing methods."
            },
            "weaknesses": {
                "value": "1. The approach of using supervised contrastive learning for unlearning is somewhat simplistic, as it overlooks the potential generalization decay of the model, as noted in Question 1. Merely introducing a framework that achieves improved performance is not sufficiently insightful. ICLR standards are high, and this work could benefit from a deeper exploration of these issues for the community.\n2. The presentation quality could be enhanced, especially regarding punctuation usage. For example, there is a missing comma after Eq. (5) and an extra period before Eq. (6)."
            },
            "questions": {
                "value": "1. Is the metric in Eq. (1) for machine unlearning reasonable? In the case of a well-performing pre-trained model, strong generalization should allow it to handle unseen samples effectively without degradation.\n2. In Tables 2 and 5, the efficacy metric is the retraining time. However, retraining time can vary due to multiple factors, such as I/O rates and the number of GPU cores, which may affect the reliability of this metric.\n3. Given that cross-entropy loss is included in the retraining, why is the processing time less than other baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors design a contrastive unlearning method. This method could remove the influence of unlearning samples by constrastive their embeddings against the remaining samples' embeddings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is well-organized.\n\n2. The motivation is clearly described.\n\n3. The appendix is comprehensive."
            },
            "weaknesses": {
                "value": "1. Table 1 is confusing. The results for \u201cremain test\u201d on RN18 seem to suggest that higher is better, so why is 85.79 bolded?\n\n2. A similar issue appears in Table 3.\n\n3. The datasets used are only CIFAR-10 and SVHN. It is recommended to include additional datasets.\n\n4. The authors should release the code to ensure reproducibility."
            },
            "questions": {
                "value": "Please see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}