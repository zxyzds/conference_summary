{
    "id": "TOUrnb1EaG",
    "title": "DNA Language Models for RNA Analyses",
    "abstract": "Genomic Language Models (gLMs), encompassing DNA models, RNA models, and multimodal models, are becoming widely used for the analysis of biological sequences. Typically, models trained on RNA are used for RNA-related tasks, and models trained on DNA sequences are used for DNA tasks. However, this requires the development and maintenance of several classes of models to match the modality of the sequence. These models take significant resources and data to create, and maintaining separate models for DNA and RNA tasks is a computational burden. \n\nTo reduce this burden, we introduce novel Adaptive Mixture of Codon Reformative Experts (CodonMoE) that can be incorporated into DNA gLMs in order to adapt them for mRNA-based predictive tasks. We show that, by using this plug-and-play operator, DNA-based gLMs can achieve performance similar to that of RNA-trained models on mRNA tasks. We further show that recent, efficient sub-quadratic DNA-based state space model (SSM) architectures can be used with the CodonMoE to achieve parameter- and computationally-efficient predictions for mRNA tasks. Specifically, experimental results demonstrate that CodonMoE improves diverse DNA-based backbones by a big margin, with some models achieving comparable or superior performance to current state-of-the-art RNA-specific models across several downstream tasks, while reducing both time complexity and model parameters.\n\nOur results provide a path for focusing development efforts of gLMs on DNA models, which can then be adapted to mRNA tasks. Because DNA data is more prevalent than assembled mRNA data, and modeling efforts can focus on a single class of model, this is likely to foster improved DNA models for mRNA tasks at lower computational cost, and is a significant step towards unifying genomic language modeling.",
    "keywords": [
        "Genomic Language Models",
        "RNA Sequence Analysis",
        "Parameter-Efficient Fine-Tuning",
        "Mixture of Experts",
        "Computational Efficiency"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "We propose CodonMoE, a versatile module that adapts DNA language models of diverse architectures for RNA analyses, reducing the burden of maintaining separate DNA and RNA models while significantly improving DNA model performance.",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TOUrnb1EaG",
    "pdf_link": "https://openreview.net/pdf?id=TOUrnb1EaG",
    "comments": [
        {
            "summary": {
                "value": "The authors suggest a strategy to adapt models trained on DNA data for mRNA downstream tasks. The proposed strategy uses the pre-trained language model as a backbone model to create nucleotide-level representations of the sequence. From the nucleotide-level representations codon-level representations are created and fed into the MoE module, and eventually the output representation is fed into prediction layers for the downstream tasks. The authors show for three different backbone models (HYenaDNA, Caduceus, and GPN-MSA) that the performance on the RNA downstream tasks can be improved by using their MoE adapter."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- **Novelty**: Adapting DNA models for RNA downstream tasks with an MoE approach is novel and could be interesting.\n- **Relevance**: \n  * The authors show that their adaption strategy improves backbone model performances on codon-level RNA downstream tasks (compared to the not fine-tuned backbone models). The presented performance values are comparable with selected baseline, i.e models trained on the RNA modality.\n  * The ablation study shows that the MoE module outperforms a simple mean baseline.\n- **Clarity**: The scope of this work and the author's main contribution, i.e. applying an MoE module to codon-level inputs retrieved from a DNA backbone model, is described clearly and well. \n- **Reproducibility**: The code wrt the MoE module is given. (Code for training and full reproduction is assumed to be given once acceptance.)"
            },
            "weaknesses": {
                "value": "- **Novelty**: The fact that Mixture of Expert models initialized from some backbone language model typically perform well is known in the general language space [1-2] and applying this principle to the DNA/RNA domain seems straight forward.\n- **Clarity/Relevance**:\n  * The authors missed to provide information about how many parameters were added by the MoE module? Could performance gains just arose because of the increased number of parameters? The authors should set up an experiment in which the MoE module is compared with a fine-tuning strategy for which the parameters of the prediction head is increased such that the number of parameters of both approaches match.\n  * For comparison, the authors should report performance values of the backbone models trained on the RNA domain.\n  * The design choice of using codon-level representations (and also the author's choice for codon-level based downstream tasks) is not explained well. How well would an naive MoE approach without codon-level representation (on nucleotide-level based downstream tasks) work? \n\n[1] OLMoE: Open Mixture-of-Experts Language Models\n\n[2] Upcycling Large Language Models into Mixture of Experts"
            },
            "questions": {
                "value": "- What are the performance values of the backbone models trained on the RNA domain?\n- How many parameters where added to the method by using the MoE module?\n- Why do models trained on the DNA domain so poor? What is the intuition behind this? DNA and RNA domain seem highly correlated."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a MoE-based head for finetuning DNA LMs for RNA specific tasks.\nThey apply their CodonMoE head on 2 DNA LMs (Evo and Caduceus) and demonstrate improved Spearman correlation on 2 benchmarking datasets (mRFP expression  and SARS-cov-2 vaccine degradation)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors introduce a method for improving DNA LMs on RNA tasks in a parameter efficient way by utilizing codon structure in their model architecture. The benchmark improvements seem to be significant."
            },
            "weaknesses": {
                "value": "The authors show that CodonMoE improves performance on RNA tasks, however it is not clear that the MoE is at all necessary, and may just be adding unnecessary complexity for a 20M param head. CodonMean is a good baseline for ablation, however this baseline does not have additional training parameters unlike CodonMoE (20M parameters). The authors should include an additional baseline to specifically ablate the mixture-of-experts architecture vs. standard dense transformer layers. This baseline should have the same number of trainable parameters as CodonMoE and use exactly the same training hyper-parameters."
            },
            "questions": {
                "value": "- How is train/test split produced for each tasks. Did the authors apply sequence similarity based split, or how is train/test leakage avoided?\n\n-Table 1 Model parameters are misleading. It does not contain the parameter count for the base models (Caduceus, etc), but does for CodonBert? This column should provide the number of params for the base mode, and the additional params for CodonMoE head. Additionally, the authors should give the exact number of parameters in M, not >80M and <20M."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents Adaptive Mixture of Codon Reformative Experts (CodonMoE), which can be integrated into DNA gLMs to tailor them for mRNA-based predictive tasks. The proposed method is assessed using both Transformer and SSM-based architectures across two mRNA-related tasks, showing performance that is on par with that of RNA-specific models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Leveraging LMs cross bio-modalities is a meaningful direction, given the amount of data available in DNA and protein modalities. The proposed CondonMoE is flexible and can be plug-and-play into different backbones. The performance is encouraging for a limited number of demonstrated use case. In addition, theoretical proof demonstrates that CodonMoE is a universal approximator of RNA properties at the codon level."
            },
            "weaknesses": {
                "value": "The primary limitation of the paper is that it evaluates only two mRNA downstream tasks, with the proposed method showing better performance on just one of these compared to RNA-specific models. This raises questions about the effectiveness of the proposed method and under what circumstances mRNA-specific models should be preferred. A broader evaluation of additional mRNA downstream tasks, similar to those presented in the CodonBERT paper, is necessary.\n\nThe experimental design can also be improved, especially in terms of ablation study of the respective influence of the backbone FMs for feature embedding and the head used for regression and prediction. For example:\n\n\u2022\tIt would be interesting to see what performance can be achieved if the proposed CondonMoE is plugged into the mRNA-specific models, given that CodonMoE just reads mRNA sequences with clearly-identified codons. \n\n\u2022\tIn addition, The RNA baselines (RNABERT, RNA-FM and CodonBERT) have been trained with TextCNN head whereas their DNA counterparts has been trained with MLP and XGBoost heads. This makes it hard to do a fair comparison against the baselines.\n\nIt would also be interesting to see what performance can be achieved if protein-based LMs are used as another baseline, by converting the known codons to protein sequences as the input, given that protein-based LMs have been trained on even larger datasets."
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}