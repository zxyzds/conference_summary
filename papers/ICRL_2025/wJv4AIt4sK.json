{
    "id": "wJv4AIt4sK",
    "title": "Effective Interplay between Sparsity and Quantization: From Theory to Practice",
    "abstract": "The increasing size of deep neural networks (DNNs) necessitates effective model compression to reduce their computational and memory footprints. Sparsity and quantization are two prominent compression methods that have been shown to reduce DNNs' computational and memory footprints significantly while preserving model accuracy. However, how these two methods interact when combined together remains a key question for developers, as many tacitly assume that they are orthogonal, meaning that their combined use does not introduce additional errors beyond those introduced by each method independently. In this paper, we provide the first mathematical proof that sparsity and quantization are non-orthogonal. We corroborate these results with experiments spanning a range of large language models, including the OPT and LLaMA model families (with 125M to 8B parameters), and vision models like ViT and ResNet. We show that the order in which we apply these methods matters because applying quantization before sparsity may disrupt the relative importance of tensor elements, which may inadvertently remove significant elements from a tensor. More importantly, we show that even if applied in the correct order, the compounded errors from sparsity and quantization can significantly harm accuracy. Our findings extend to the efficient deployment of large models in resource-constrained compute platforms to reduce serving cost, offering insights into best practices for applying these compression methods to maximize hardware resource efficiency without compromising accuracy.",
    "keywords": [
        "theory of compression",
        "model compression",
        "quantization",
        "max-scaled numerical encoding",
        "sparsity",
        "unstructured sparsity",
        "structured sparsity",
        "N:M sparsity",
        "large language models",
        "magnitude pruning",
        "post-training quantization",
        "efficient inference"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We mathematically analyze the interplay of sparsity and quantization, proving they are not orthogonal operations. This means their combined error is greater than the sum of their parts, especially due to quantization.",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wJv4AIt4sK",
    "pdf_link": "https://openreview.net/pdf?id=wJv4AIt4sK",
    "comments": [
        {
            "summary": {
                "value": "The paper explores the relationship between two widely used compression techniques: sparsity and quantization. Specifically, it demonstrates that these techniques are not independent of one another; the order in which they are applied can significantly impact the results. Additionally, their combination can lead to error propagation, with accumulated errors affecting consecutive layers. The study draws from both theoretical analysis and experimental results conducted on large, modern neural networks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper covers an interesting and timely topic. Given the increasing size of parameters in pre-trained models, there is growing interest in techniques such as quantization and sparsity. Providing both analytical and empirical insights into the relationship between these techniques is valuable, especially as they are often studied separately.The findings in this paper, such as the optimal order for applying sparsity and quantization and the established upper bounds, can offer practical guidance for researchers in this area.  \n- The paper effectively demonstrates the non-orthogonality of sparsity and quantization, determining the optimal sequence for applying these transformations through theoretical analysis, supported by empirical studies on large, modern networks.  \n- The work is well-written, easy to follow, and enjoyable to read."
            },
            "weaknesses": {
                "value": "- In the experiments section, the results appear promising and generally align with the theoretical findings. However, it is unclear whether the reported results represent averages of multiple runs or single-run outcomes. If they are averages, what are the standard deviations?  \n- Additionally, I believe the related work section should remain in the main body of the paper, particularly since there is available space before reaching the 10-page limit. Moving it to the appendix could diminish its visibility and importance."
            },
            "questions": {
                "value": "- In Table 2, what do the bold-out results represent? This should be explained in the caption.   \n- In Table 2: perhaps it would be beneficial to show the delta to the sparsity 0% instead/additionally  (e.g. in the appendix)?   \n\nOverall, I find the topic of this paper both interesting and potentially valuable to researchers focusing on sparsity and quantization. The claims and theorems are clearly articulated (I briefly reviewed the details of Theorems 3.5, 3.6, 3.7, and 3.9 in the appendix), and the empirical evaluation, while primarily centered on magnitude-based sparsity, is compelling and conducted across various models and tasks. I believe the strengths of the paper outweigh weaknesses (in fact, I do not have significant concerns regarding weaknesses). Therefore, I am inclined to recommend acceptance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides a comprehensive theoretical and empirical investigation into the interplay between sparsity and quantization, two widely used model compression techniques. The authors mathematically prove that sparsity and quantization are non-orthogonal operations, meaning their combined use introduces compounded errors beyond those incurred by each method independently. They further derive the optimal order of applying sparsity before quantization (S\u2192Q) to minimize additional errors. These theoretical findings are validated through extensive experiments on large language models (OPT, LLaMA), vision transformers (ViT), and convolutional neural networks (ResNet). The paper also introduces the novel \"orthogonality bound\" metric to efficiently estimate the performance of sparse-quantized models without expensive retraining."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper makes significant theoretical contributions by proving the non-orthogonality of sparsity and quantization and deriving the optimal $S\\to Q$ order. These insights challenge conventional assumptions and provide valuable guidance for model compression. \n- The mathematical analysis is rigorous and comprehensive, covering tensor-level and dot product-level errors. \n- The experimental results are extensive, spanning diverse models (OPT, LLama, ResNet, ViT) and settings.\n- The orthogonality bound metric seems like a useful tool for practitioners. \n- Overall the paper is well-structured, with clear definitions, detailed appendices, and informative tables."
            },
            "weaknesses": {
                "value": "- While the experiments cover a range of models and settings, the datasets used (WikiText2, ImageNet1k) are relatively small and few. Evaluating on larger, more challenging datasets would further strengthen the findings. \n- The paper does not explore the impact of different sparsity patterns (e.g., block-wise sparsity) or more advanced quantization schemes."
            },
            "questions": {
                "value": "- Evaluating the findings on larger, more diverse dataset would be nice. \n- It would be interesting to see how the optimal $S\\to Q$ order and orthogonality bound extend to other sparsity patterns and quantization schemes. Can the authors comment on the generality of their findings in this regard?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the interaction between weight sparsity, weight quantization, and activation quantization in small-to-moderate sized LLMs, ViTs, and CNNs. The authors prove and demonstrate empirically that these methods cannot be considered as purely orthogonal compression modalities under the orthogonality definitions proposed in the paper. Specifically, the authors show that the composition of these strategies is order-dependent and the combined error incurred generally exceeds the sum of the errors produced by applying each method independently."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* A timely and important topic as sparsity and quantization are promising compression strategies for the large model scales popular today.\n* The paper includes a comprehensive summary of relevant literature.\n* The proofs are relatively easy to follow and explained in an intuitive manner by the authors in the main text.\n* Empirical results generally appear to support the theoretical findings.\n* While many works have studied the combination of sparsity and quantization, this is the first that I am aware of to rigorously consider the interplay between these methods in detail. \n* Empirical experiments include both LLMs and vision models. \n* Extensive supplementary info includes an analysis of several leading SOTA methods from LLM pruning and quantization literature."
            },
            "weaknesses": {
                "value": "Overall I am leaning towards accept; however, some concerns regarding the empirical experimental design causes me to doubt the applicability of the results to more general settings:\n\n* The primary metrics considered in the empirical results are perplexity or cross-entropy loss. While these are certainly reasonable proxies for downstream task performance, they are not perfectly correlated. While some accuracy metric for CV models was included in the appendices, it would be beneficial to extend this to downstream tasks for LLMs such as the OpenLLM v1 leaderboard evaluation tasks. It has been shown previously that PPL and CE can be particularly misleading metrics for quantized and sparse models [1]. \n* The experimental design for Section 4.1 is potentially concerning. If I understand the described process correctly, in the Q->S case the pretrained models are pruned and quantized before each forward pass (i.e., instantaneous masking and quantizing). Are the parameters themselves stored as dense fp32 tensors during this process and quantization is simulated similar to QAT approaches? Are the optimizer states left in fp32? The authors note issues with training dynamics in the Q->S case in Appendix A and my concern is that this could be related to numerical precision issues during fine-tuning rather than providing a reliable comparison on the order of compression. Adding a more detailed summary of the fine-tuning approaches in the appendix would potentially clear up any misunderstandings on this point. \n* In the Q->S case quantized activations are used but in the S->Q case it appears the full precision activations are used. It's unclear to me if the dramatic difference in performance is caused by the quantized activations during fine-tuning rather than the specific order of compression for the weights. \n\n\n[1] A. Jaiswal, Z. Gan, X. Du, B. Zhang, Z. Wang, and Y. Yang, \u201cCompressing LLMs: The Truth is Rarely Pure and Never Simple,\u201d Oct. 02, 2023, arXiv: arXiv:2310.01382. doi: 10.48550/arXiv.2310.01382.\n\n\n### Suggestions / Typos:\n* Defining \u201ctensor and dot-product levels\u201d earlier in the text would improve the reader's understanding. Specifically it may be worthwhile to relate these terms to \u201cweights\u201d and \u201cactivations\u201d respectively.  I note that activations / dot-products are also represented as tensors. \n* On L68, the authors refer to the challenge of quantizing LLMs due to outliers in \u201ctensor distributions\u201d and reference the smoothquant paper. This should be corrected to \u201cdot-product outliers\u201d as the challenge typically arises from outliers in the activations, not the weights (which instead follow a more gaussian-like distribution typically). \n* I suggest separating references for fine-grained (N:M and similar) and structured (neuron-level or larger NN components) sparsity in the related work discussion on L115. In particular, it would be beneficial to introduce N:M sparsity before it appears in section 3. \n* L469: state-of-the-arts -> state-of-the-art"
            },
            "questions": {
                "value": "* In the Q->S case, the authors make the argument that this ordering may lead to additional errors when two otherwise unequal weights in the non-quantized precision are set to the same value once quantized. This is an intuitive conclusion but it would be interesting to ground this discussion in empirical evidence of the proportion of weights that this affects, on average, in a pre-trained model. \n* Are the pretrained LLMs obtained from the base models or instruct-tuned variants? Making this explicit in the paper would be beneficial. \n* L312 states that all linear layers were compressed for LLMs. Can you confirm that this included the lm-head, but not the encoder which is typically implemented as an embedding?\n* Table 10 values for 1:4 are counter to typical intuition that higher sparsities generally perform worse. Could the authors confirm that this is 1:4 and not 3:4 sparsity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors present a theoretical proof showing that the order in which magnitude-based pruning and scaled block quantization are performed is of importance in the context of preserving model performance. The authors define notions such as orthogonality of two operations \u2014 which is when the composition of the two operations does not result in any additional error than applying each individual transformation. The authors show theoretically that magnitude pruning and quantization are not orthogonal operations (when going beyond tensor-level) and further showed, both theoretically and empirically, that applying pruning first and then quantization generally leads to better performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Notation, definitions, and theorems in Section 3 are generally clear and their significance is adequately articulated.\n- The authors have addressed an issue that has gone overlooked in the pruning/quantization literature through both theoretical proofs and derivations as well as empirical studies that further solidify their claims."
            },
            "weaknesses": {
                "value": "- The discussion following Theorem 3.9 is very hard to digest for a reader who hasn\u2019t spent as much time as the authors thinking about this problem. I\u2019d encourage the authors to prune the text, retaining only the essential message (which presumably is what\u2019s written in italics) and moving other information to the Appendix.\n- Overall, the theoretical claims and experiments are not astonishing as one would perhaps expect that pruning should precede quantization.\n- The theoretical contribution is quite limited as it only holds for magnitude-based pruning (without fine tuning) and block-wise quantization. Importantly, magnitude pruning has gone out of fashion in the context of LLMs because it requires costly fine-tuning to recover model performance and is outperformed by methods like SparseGPT and WANDA when fine-tuning is not performed. The authors mention in the Appendix that, empirically, the order had less of an impact for WANDA and SparseGPT. \n- The experiments seem to be quite orthogonal to the theoretical results. By employing fine-tuning for all the experiments, the authors are making their original theoretical proofs/derivations inapplicable in the context of the experiments as the derivations are based on errors calculated when no fine-tuning is applied. \n- Proof of Theorem 3.5: Only show equality is attained for L1 norm and not all norms. Is it clear that this implies that equality is also achieved for all other norms? Statement of Theorem or proof should be modified to address this. \n- Proof of Theorem 3.6 is only a counter-example for the L1 norm. Is it immediate that the theorem is true in general for norms beyond the L1 norm? Either the statement of the theorem or the proof should be modified to address this.\n- Throughout the paper, some statements are true for all norms, others are only shown for the L1 norm, and then the empirical experiments utilize the L2 norm for measuring errors. \n- The generalization of orthogonality in Definition 3.8 is not clear to me as functions are now being applied coordinate-wise. Is the composition only permitted to happen in one coordinate (similar to in Theorem 3.9). It might be worth it to explicitly write out the definition as the lack of an explicit definition of orthogonality also makes the statement of Theorem 3.9 confusing."
            },
            "questions": {
                "value": "- In Section 3, the authors mention performing quantization at the level of \u201cblocks.\" Could you clarify what you mean by a \u201cblock\u201d in this context? Does it refer to a set of weights associated with a CNN filter, or does it resemble the M:N sparsity blocks? Or is it something entirely different?\n- Consider renaming Definition 3.4 to avoid confusion, as it defines \"orthogonality\" between two functions in a way that diverges from the standard interpretation. Traditionally, orthogonality between functions is defined by the condition \\(\\int f(x) g(x) \\, dx = 0\\), so using \"orthogonality\" here might lead to misinterpretation.\n- Why is it important to consider block-wise quantization in Section 3? Since it\u2019s a theoretical derivation, why don\u2019t you simply assume quantization on the tensor level?\n- Theorem 3.5 assumes \u201cmax-scaled block-wise quantization\u201d. Is such quantization prevalent in the literature and in practice?\n- Theorems 3.5 and 3.6 imply that the optimal order is pruning followed by quantization. Theorem 3.7 analyses the error for the suboptimal order. Why is that of interest?\n- Is Equation 12 a lower bound or an upper bound? You might want to rename it accordingly to \u201cOrthogonality Lower Bound\u201d or \u201cOrthogonality Upper Bound\u201d to help the reader.\n- \u201cIf the compression methods are non-orthogonal, and the evaluation metric indicates better model performance with lower values, we expect the compressed model\u2019s evaluation metric to exceed the orthogonality bound.\u201d \u2014 I read this sentence several times and I still can\u2019t understand it. What do you mean by \u201clower values\u201d? Which values?\n- \u201cFor OPT, LLaMA, ViT, and ResNet fine-tuning, we employ sparse fine-tuning on a dense\u201d \u2014 what method exactly are you using? Please cite the paper.\n- \u201cwe apply one-shot quantization to sparse fine-tuned models\u201d \u2014 again, what method exactly are you using? Please cite the paper. is it the \u201cmax-scaled block-wise quantization\u201d?\n- \u201cwe directly fine-tune the model in a quantized and sparsified manner\u201d \u2014 how does one fine tune a quantized model? Isn\u2019t there an issue with doing that?\n- Could you summarize the \u201cExperimental setup\u201d in the form of a table. Otherwise, there are too many details in the paragraph and it\u2019s very hard to digest.\n- In Figure 1, the error accumulates across layers. This stands in contrast to Figure 1 in [1] which shows attenuation of noise injected in intermediate layers. Could it be that the authors should compute a relative error instead of an absolute error (see caption in Figure 1 of that paper)?\n- \u201cand/or reduce quantization effective bitwidth.\u201d \u2014 what do you mean by \u201ceffective bit width\u201d?\n- \u201cTOPS/mm2\u201d \u2014 what\u2019s TOPS and mm^2?\n\n[1] Stronger Generalization Bounds for Deep Nets via a Compression Approach"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}