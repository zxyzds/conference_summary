{
    "id": "pq3RANvCZC",
    "title": "Binary Hypothesis Testing for Softmax Models and Leverage Score Models",
    "abstract": "Softmax distributions are widely used in machine learning, including Large Language Models (LLMs) where the attention unit uses softmax distributions. We abstract the attention unit as the softmax model, where given a vector input, the model produces an output drawn from the softmax distribution (which depends on the vector input). We consider the fundamental problem of binary hypothesis testing in the setting of softmax models. That is, given an unknown softmax model, which is known to be one of the two given softmax models, how many queries are needed to determine which one is the truth? We show that the sample complexity is asymptotically $O(\\epsilon^{-2})$ where $\\epsilon$ is a certain distance between the parameters of the models.\n\nFurthermore, we draw analogy between the softmax model and the leverage score model, an important tool for algorithm design in linear algebra and graph theory. The leverage score model, on a high level, is a model which, given vector input, produces an output drawn from a distribution dependent on the input. We obtain similar results for the binary hypothesis testing problem for leverage score models.",
    "keywords": [
        "Binary hypothesis testing",
        "softmax distributions",
        "large language models",
        "attention"
    ],
    "primary_area": "learning theory",
    "TLDR": "We analyze the binary hypothesis testing problem for softmax and leverage score models, which are relevant to large language models and attention mechanisms.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=pq3RANvCZC",
    "pdf_link": "https://openreview.net/pdf?id=pq3RANvCZC",
    "comments": [
        {
            "summary": {
                "value": "This paper studies the sample complexity of binary hypothesis testing in the context of softmax and leverage score models. First, the paper identifies the requirement of energy constraints for both problems as otherwise the testing problems are straightforward. Then, the lower and upper sample complexities are identified for the two problems."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The hypothesis testing lens to study properties of softmax models is an interesting idea."
            },
            "weaknesses": {
                "value": "1. The write-up needs significant improvements. I find it hard to follow the introduction and the related work section on theoretical LLMs, which include many unrelated works. I am also skeptical of how the authors motivate the problem, focusing purely on LLMs.\n\n2. It seems like the main result depends on the result from Polyanskiy & Wu, and the significance of results on top of these main results is not clear. I believe the key result is Theorem 3.1, which is invoked by controlling Hellinger distance under some perturbations. The upper bound in Theorem 3.1. is directly from Polyanskiy & Wu. So, I believe the main result is the lower bound in Theorem 3.1. Studying the analysis, the proof is straightforward (the contribution is mild) and contains an implicit assumption on the number of queries, $m$, not explained.\n\n3. I believe the example in L261-268 is wrong for the leverage score model. Could you explain why $\\delta > 0$ will put all mass on $2 \\in [n]$ for $\\mathrm{Leverage}_{B}(s)$? In addition, I understand that the problem is straightforward without the energy constraints, but this needs to be justified by motivating the problem in this setting."
            },
            "questions": {
                "value": "1. Why do you say \"asymptotically\" in the abstract?\n2. Can you explain the case of $\\delta \\geq 0.1$ for lower bound in Theorem 3.1.?\n3. Why do you assume $m \\leq 0.01 \\delta^{-2}$? I don't see this assumption in the main body of the paper. Why would this assumption is meaningful in the context of binary hypothesis testing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper analyzes the sample complexity of the binary hypothesis testing problem for softmax distributions and leverage scores, with the motivation of improving theoretical understanding of LLMs. The theoretical contribution of the paper is providing upper and lower bounds on the sample complexity, both for softmax operators and leverage scores, which are obtained by estimating the Hellinger distances between the distributions and then applying them in conjunction with prior results in hypothesis testing literature."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The theoretical results seem sound. \n- The notations and exposition of the results is clear to follow."
            },
            "weaknesses": {
                "value": "- The glaring weakness is that the motivation of the paper is largely unclear and disconnected from the work done in the paper. The motivation of the works is to study LLMs through a better understanding of the softmax attention. On the other hand, the analysis, which is about the sample complexity of distinguishing two softmax distributions in a hypothesis testing problem, seems largely tangential and the results are never connected back to the original motivation or explain how it serves to improve LLM understanding, either in theory or via simulations.\n- The exposition on leverage scores seems to have been shoehorned and seems to have no connection to the motivation either, except some vague comments about their usefulnes. I understand that it might be of interest to show the similarities, but the authors need to explain why how this additional perspective connects to the question of interest. \n- Presentation Issues: The paper has some writing issues outside of the theoretical analysis, which makes the paper hard to follow. \n    - The motivation needs clarity, and the chose approach needs to be justified.\n        - Line 53: _Can we distinguish different ability parts of large language models by limited parameters sampling?_ I am not sure what the authors mean by \"ability parts\" or \"limited parameters sampling\".\n        - Line 60-63: I find it hard to follow as to how hypothesis testing and distinguishing one softmax distribution from another, can help improve the theoretical understanding of LLMs or determine which parameters are important for inference. \n   - The related work section is vague; it is not clear which references are relevant to the motivation or techniques used in this work. Similarly, in conclusions and future work, the exposition is largely about solving the hypothesis testing problem and does not substantiate on potential practical applications.\n    - Lines 47-48, 56-58 need sentence restructuring.\n- Overall, I believe that this work seems to be more hypothesis testing centric, and thus might be a better fit for a different venue both in terms of applicability of the results and audience interest."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors derive upper bounds and lower bounds on the number of queries needed to distinguish between two different softmax or leverage score model distributions. This is also known as the binary hypothesis testing problem. The proofs require manipulating inequalities between the Hellinger distance and the total variation distance. Additional energy constraints on the input query is also needed for proving the upper bounds of sample complexity."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "I\u2019m not an expert in this field, but the lower and upper bounds seem like a novel results for softmax and leverage score models for the binary hypothesis testing problem."
            },
            "weaknesses": {
                "value": "- This paper is not properly motivated, at least in how it\u2019s written. The introduction sections touch on LLMs and how attention is an important component of LLMs. In the rest of the paper, the authors go on to prove bounds on number of samples needed for binary hypothesis testing for a generic softmax and leverage score model without circling back to how it relates to LLM. If the research question is understanding the sample complexity of softmax and leverage score model under the binary hypothesis testing problem, which I think it\u2019s a worthwhile research question, the authors shouldn\u2019t motivate it from LLMs without ever relating it to LLMs.\n- Similar to the first point, I\u2019m not sure if there are any interesting applications for the provided sample complexity bounds. It\u2019s unclear to me if these bounds are tight or not or do they ever prescribe any meaning quantities in practice. Tentatively, I\u2019m suggesting one experiment the authors can do in an application domain like LLM:\n    - Suppose there are two LLMs. Given different input sequences, the next token distribution comes from a softmax over the logit vectors. Then one interesting question could be how many input sequences do we need to tell which LLMs we\u2019re using based on the next token softmax distribution. This could be interesting in the context of knowledge distillation or some sort of toxic speech detection problems. Then the authors can plot the predicted number of samples using the bounds and also the empirical number of samples needed to actually distinguish between the two distributions."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies binary hypothesis testing, which identifies an unknown distribution given two possible candidates, for the softmax and the leverage score models. \n\n*Setup and tools*: The analysis focuses on the softmax model commonly used in deep learning and on the leverage score models used in graph theory and linear algebra. The authors conducted their proofs with information-theoretic tools and norm manipulation.\n\n*Contributions*: The authors show that the sample complexity (in terms of queries) of the binary hypothesis testing is $\\mathcal{O}(\\varepsilon^2)$, where $\\varepsilon$ is a distance-based measure between parameters, for both the softmax model and the leverage score models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Studying the softmax as a sampling operation is interesting\n- The focus on the binary hypothesis testing is original and could have a potential impact on machine learning and deep learning \n- The analysis and the proofs for both models are detailed"
            },
            "weaknesses": {
                "value": "- The authors state the main problem with respect to LLMs (l053), but they do not explain the theoretical, methodological, or practical implications of their results for LLMs or neural networks. \n- Considering simple models for a proper theoretical study is valid, but in my opinion, studying the softmax operator as a standalone is an oversimplification, especially if the motivation is the use of the softmax in LLMs and neural networks. \n- In the attention mechanism, the softmax matrices depend on the input sequence of tokens ($\\mathrm{softmax}(XW_Q^TW_KX/d_{\\kappa}$). Hence, I don't think that the current analysis with a fixed matrix $\\mathbf{A}$ (see Definitions 2.6/2.7) holds. Could the authors elaborate on this? In particular, what plays the role of $A$ and $x$ in LLMs and transformers?\n- Similarly, the authors justify the scaling of the input (Definition 2.8) by the use of batch-norm. However, in most transformers, including LLMs, LayerNorm and its variations are preferred instead of batch normalization. In addition, the product $Ax$ of Definition $2.6$ is rather a $XW_Q^TW_KX$ with learnable $W_Q, W_K$. Again, what plays the role of $A$ and of $x$, and why would it be justified to consider $x$ bounded in such a case? Could the authors elaborate on that matter?\n- The writing could be improved: several sentences are unclear to me and there are many repetitions in some paragraphs (e.g., \"delve\").\n- The structure could be improved: currently, it does not highlight the strengths of the paper and makes it look more like a concatenation of results."
            },
            "questions": {
                "value": "*Questions related to the weaknesses*\n1) What are the practical, methodological, or theoretical implications of the results for LLMs, transformers, and neural networks? \n2) Could the authors think of experiments (even on synthetic data) to illustrate or showcase the importance of their theoretical results? (I feel the need to note that I have nothing against fully theoretical papers, but in its current form, the relevance of the theoretical contributions of this paper is limited and it lacks discussion to fully understand their potential implications in machine learning/deep learning).  \n3) The authors mention an analogy between softmax and leverage score models in the abstract. What is the analogy or connection between them, except that both are parameterized by a matrix and take vectors as inputs?\n4) In the attention mechanism, the softmax matrices depend on the input sequence of tokens ($\\mathrm{softmax}(XW_Q^TW_KX/d_{\\kappa}$). Hence, I am not sure the current analysis with a fixed matrix $\\mathbf{A}$ (see Definitions 2.6/2.7) holds. Could the authors elaborate on this? In particular, what plays the role of $A$ and $x$ in LLMs and transformers?\n5) Similarly, the authors justify the scaling of the input (Definition 2.8) by the use of batch-norm. However, in most transformers, including LLMs, LayerNorm and its variations are preferred instead of batch normalization. In addition, the product $Ax$ of Definition $2.6$ is rather a $XW_Q^TW_KX$ with learnable $W_Q, W_K$. Again, what plays the role of $A$ and of $x$, and why would it be justified to consider $x$ bounded in such a case? Could the authors elaborate on that matter?\n\n*I list below potential typos and sentences I did not understand .*\n\n- l034: missing reference replaced by a \"?\"\n- l043: *\"potent capabilities\"* --> What does that mean? Do the authors mean \"powerful capabilities\"?\n-l045: *\"prevailing prevalence\"* --> Is it a typo to have the adjective and noun combined? Do the authors simply mean \"prevalence\"?\n-l047-l048: *\"However, their is [...] of the whole\"* --> I do not understand the sentence, it seems unfinished.\n-l050-l051: *\"as well as sparsity [...] above\"* --> I do not understand this part of the sentence at all.\n- l053: *\"can we distinguish [...] sampling\"* --> I do not understand the question nor its connection to the paragraph above it (l047-l051). Could the authors elaborate on that?\n- l059-l081: 5 repetitions of the word \"delve\", use of complicated vocabulary (distinguishing ability, intricacies, inquiry, etc.) that hinders the meaning and understanding of the paragraph."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}