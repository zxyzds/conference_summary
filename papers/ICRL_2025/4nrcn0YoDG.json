{
    "id": "4nrcn0YoDG",
    "title": "Global Identifiability of Overcomplete Dictionary Learning via L1 and Volume Minimization",
    "abstract": "We propose a novel formulation for dictionary learning with an overcomplete dictionary, i.e., when the number of atoms is larger than the dimension of the dictionary. The proposed formulation consists of a weighted sum of $\\ell_1$ norms of the rows of the sparse coefficient matrix plus the log of the matrix volume of the dictionary matrix. The main contribution of this work is to show that this novel formulation guarantees global identifiability of the overcomplete dictionary, under a mild condition that the sparse coefficient matrix satisfies a strong scattering condition in the hypercube. Furthermore, if every column of the coefficient matrix is sparse and the dictionary guarantees $\\ell_1$ recovery, then the coefficient matrix is identifiable as well. This is a major breakthrough for not only dictionary learning but also general matrix factorization models as identifiability is guaranteed even when the latent dimension is higher than the ambient dimension. We also provide a probabilistic analysis and show that if the sparse coefficient matrix is generated from the widely adopted sparse-Gaussian model, then the $m\\times k$ overcomplete dictionary is globally identifiable if the sample size is bigger than a constant times $(k^2/m)\\log(k^2/m)$, where $k$ is the number of atoms in the dictionary, with overwhelming probability. Finally, we propose an algorithm based on alternating minimization to solve the new proposed formulation.",
    "keywords": [
        "Dictionary learning",
        "overcomplete",
        "sparse",
        "identifiability"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "Overcomplete dictionary learning is globally identifiable via the proposed L1+Volume minimization if the sparse coefficients are strongly scattered in the hypercube.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4nrcn0YoDG",
    "pdf_link": "https://openreview.net/pdf?id=4nrcn0YoDG",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a new formulation for the overcomplete dictionary learning problem. The authors show global identifiability of the dictionary and sources up to permutation and scaling provided that the atoms are sufficiently sparse."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper seems mathematically sound (be careful with the dimensions, see the detailed comments below). Its positioning with respect to the existing literature should be better documented though. Two results appear as particularly related: Hu and Huang 2023 and Agarwal et al/ Rambhatla et al. It would help to have a clear discussion on the improvement of the paper compared to those results."
            },
            "weaknesses": {
                "value": "Sparse coding or sparse dictionary learning are not new"
            },
            "questions": {
                "value": "Detailed Comments:\n\n- Maybe recall what complete and overcomplete (no orthogonality) dictionary mean\n- Formulation (2) should be better introduced. Why is \n- line 106, you say that A should be a dictionary that guarantees exact recovery of all s-sparse vectors. Do you mean that min ||x||_1 s.t y= Ax should have a unique solution for all s-sparse vectors?\n- line 107, what is the cellular hull?\n- you should clarify the notion of scattered cellular hull before introducing your results.\n- Statement of Lemma 1 is misleading. First of all, from what I understand the weights d_{*c} reach the maximum of \\sum_c d_c ||e_cS_*|| under the constraint \\|d\\|\\leq m. Secondly, if the max is attained for (3), why not just optimize the l1 norm squared?\n- Is it always possible to scale the columns of A_{\\#} and rows of S_{\\#} to satisfy (5). This is not obvious to me\n- If I understand well you want the set S to be reduced to canonical vectors p? and S could include vectors that are not in the span of Q but all vectors in span(Q) must be of the form q/||q||?\n- From your definition of B_m, the set is a subset of R^k (i.e. it is given by some linear combinations of the columns of Q). Moreover S is also a subset of R^k so how can the intersection of those subsets be a subset of R^m (i.e given by rows of Q)? Maybe you mean the columns of Q?\n- line 158-159, I would add just one sentence, to explain that for the correlation to be maximum, you need the cosine of the angle between the vectors to be maximum which implies d_c = \\alpha \\|e_c^T S_*\\| for all c\n- lines 164-165, there are alphas missing. \n- line 178 and Figure 1. If I understand well, the set B_m is an intersection of spheres of dimension m. If my understanding is correct, I think it would be worth mentioning it somewhere because it looks as if the points clouds in Fig 1 have non empty inerior (especially the 2-strongly scattered one) while my guess is there are empty. \n- lines 241-244, in your proof sketch, again if I understand well, you define your matrix Q from the left factor of the SVD of A_#. I.e. if you have A_# = U\\Sigma V^T, then you define Q as V. Then why not say it like that. I feel this is simpler and much more clear\n- On line 248, you refer to assumption 4 which does not appear anywhere (the hyperlink does not work)\n- line 251-253, shouldn\u2019t the pseudo inverse be applied on the right of S_*, i.e. from line 252, the dimensions of W seems to be n\\times n to me. Moreover, what you need to project to have the decomposition of line 251 are the rows of S_* not the columns. \n- One lines 268-269, if I\u2019m not wrong you mulyiply both sides by S_# and not S_*\n- On line 272, there is a transpose missing on the second A_#\n- On line 272, the last equality in Equation (8) is not completely clear to me. Isn\u2019t ||e_c^T S_*||_1 = ||w_c^T S_#||_1 and not ||e_c^T S_#||_1 ? why is ||w_c^T S_#||_1 = ||e_c^T S_#||_1 ? Does the relation follow from (5) and the fact that A_# = A_*D\\Pi ? It would help to have even a short additional explanation here.\n- lines 303-305, I don\u2019t understand the sentence. You say that the sparsity is implicitely implied in (5)? How come ?\n- lines 302 - 303 should be rephrased. I think what you mean is that \u201csparsity is required to have the strongly scattered condition used in the statement of Theorem 1\u201d instead of \u201csparsity is implied in Assumption 1\u201d\n- line 308 \u201cdoes not necessarily mean that the sparse coefficients S_# is identifiable\u201d \u2014> \u201care identifiable\u201d ?\n- lines 313 -320, Assumption 4 seems quite strong (or quite vague) on the dictionary. Is it easy to find such dictionaries? (I.e. you don\u2019t provide any numerical illustration). It would be perhaps good to have a short comment such as the one at the beginning of section 2.3\n- lines 339-340, \u201cthe most crucial condition is assumption 3 that cell ..\u201d \u2014> \u201cthe most crucial condition is assumption 3, or the fact that cell(S_#) should be generated \u2026\u201d?\n- lines 341-342: \u201cand show that when it satisfied assumption 3\u201d \u2014> do you mean \u201cand show that it satisfies assumption 3\u201d?\n- Section 2.3., lines 337-346, I don\u2019t really understand why, if you can make it work in the sparse Gaussian model, you can\u2019t make it work in the Bernoulli Gaussian model. If the probability in the Bernoulli distribution is set to s/n, can\u2019t you get a result similar to what you have with sufficient probability? Even if you can\u2019t be at least s-sparse, isn\u2019t \u201cat least s-sparse\u201d with sufficient probability enough?\n- line 348 - 349 \u201cif for every column of S\u201d \u2014> \u201cif every column of S\u201d\n- lines 362-363 : \u201cis equal to\u201d or \u201cequals\u201d but not \u201cequals to\u201d \n- line 380, I would remove the line \u201cwhich is a good sign that the bound is tight \u201d\n- line 383 \u201ceven if identifiability of S_# is not required\u201d, what do you mean \u201cis not required\u201d? Aren\u2019t all your result focusing on the identifiability of S_# ? i would remove the paragraph starting from \u201cOn the other hand\u201d because it makes everything unclear.\n- line 389 - 390, the sentence \u201cDue to the novel formulation (2) for overcomplete \u2026\u201d does not make sense either. Do you mean \u201cWe will now design an algorithm for formulation (2) for which uniqueness (up to permutation and scaling) of the dictionary and sources was shown above\u201d\n- line 427 \u201cwhich is not preferable as one step of an iterative algorithm \u201d just remove."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the identification problem in over-complete dictionary learning by introducing a new formulation. The authors primarily build on the analysis from [Huang & Hu, 2023], extending the concept of \"sufficiently scattered\" to the over-complete setting. By combining this extension with scaling and independence conditions for $A$ and $S$, the authors argue that \"sufficiently scattered\" serves as a sufficient condition for the identifiability of $A$ under the proposed formulation (2). Additionally, they provide a theoretical guarantee that this \"sufficiently scattered\" condition holds with high probability under the commonly used Bernoulli-Gaussian distribution."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The idea is well-motivated, and the problem is relevant to the community. While previous work typically relies on column incoherence for $A$, the authors propose a novel sufficient condition of $S$ for the global identifiability of the over-complete dictionary learning problem under their formulation. This is achieved by extending the \"sufficiently scattered\" condition from non-negative matrix factorization (NMF) to the context of dictionary learning."
            },
            "weaknesses": {
                "value": "1) The connection between the proposed \"sufficiently scattered\" condition and the conditions outlined in [3] remains unclear. Could the authors clarify this relationship?\n\n2) The paper appears to be incomplete. For instance, the figure for the experimental section is missing, and in line 187, it seems that $\\mathcal{S} \\subseteq \\mathbb{R}^k$ should be used."
            },
            "questions": {
                "value": "Given that the \"sufficiently scattered\" condition has been previously introduced in NMF and topic modeling, and that similar identifiability conditions appear in [1,2], could the authors discuss the specific technical challenges posed by applying this condition in the dictionary learning (DL) setting compared to the NMF/topic modeling context? \n\n[1] Kejun Huang, Nicholas D Sidiropoulos, and Ananthram Swami. Non-negative matrix factorizationrevisited: Uniqueness and algorithm for symmetric decomposition. IEEE Transactions on Signal Processing, 62(1):211\u2013224, 2013.\n\n[2] Kejun Huang, Xiao Fu, and Nikolaos D Sidiropoulos. Anchor-free correlated topic modeling: Identifiability and algorithm. Advances in Neural Information Processing Systems, 29, 2016.\n\n[3] P. Georgiev, F. Theis and A. Cichocki, \"Sparse component analysis and blind source separation of underdetermined mixtures,\" in IEEE Transactions on Neural Networks, vol. 16, no. 4, pp. 992-996, July 2005"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an approach for dictionary learning that uses a loss that mixes a modified, weighted version of the ell-1 norm of the mixture matrix coefficients (with different weights for different rows) with the volume of the dictionary matrix. It identifies a condition for successful identification of the mixing matrix called strong scattering. Similar to existing results, the likelihood of strong scattering for random mixing coefficient matrices such as sparse Gaussian, finding a scaling low for the number of vectors used in learning to scale like $\\mathcal{O}\\left(\\frac{k^2}{m} \\log \\frac{k^2}{m}\\right)$, where $k$ is the number of dictionary elements and $m$ is the data dimension. An alternating minimization algorithm for the proposed optimization is included as well."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The formulation appears novel and the analytical results are comprehensive.\nA sound identifiability condition is presented."
            },
            "weaknesses": {
                "value": "As with other conditions for sparse learning and recovery, it appears that the required strong scattering condition cannot be efficiently checked.\n\nIt is difficult to assess how much stronger the sufficient scattering condition is versus \"that of complete dictionary learning\".\n\nSome specific arguments are not clear (see questions).\n\nA figure in the experimental section (cf. Line 466) is missing."
            },
            "questions": {
                "value": "Line 165: is a square power missing outermost in the second term? Why does this line imply $\\alpha = 1$?\n\nLine 171: Why is Assumption 1 reasonable? Is this equality always possible? If so, can that be shown as a lemma?\n\nLine 188: if $\\mathcal{B}_m \\subseteq \\mathcal{S}$, then isn't $\\mathcal{B}_m \\cap \\mathcal{S} = \\mathcal{B}$?\n\nLine 246: Assumption 4 has not yet been introduced - can you move the definition earlier?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel  formulation for dictionary learning with the dictionary matrix being overcomplete.  Under certain conditions, the authors demonstrate that the novel formulation guarantees global identifiability on the overcomplete dictionary. Finally, the authors design  an alternating optimization algorithm to solve the proposed formulation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "It is impressive that the proposed formulation can guarantee  global identifiability over dictionary learning with an overcomplete dictionary matrix under some conditions."
            },
            "weaknesses": {
                "value": "1. It is not easy to verify whether $A$ and $S$ satisfy the Assumptions 3-4. Hence, it is difficult to evaluate the practical applicability of the theoretical results. \n2. The paper provides only a simple simulation experiment, and the results are somewhat unconvincing.\n2. The theoretical results are related to the optimal solution to equation 2. However, the proposed optimization algorithm for solving equation 2 cannot  guarantee convergence to a global optimum."
            },
            "questions": {
                "value": "1. In Lemma 1, it seems that $\\Phi=I$ only when the optimal solution to equation 2 is unique. Hence, if there are multiple optimal solutions, does Lemma 1 still hold? If not, how to demonstrate that the optimal solution to equation 2 is unique? \n2. How to prove that $A$ in Assumption 4 must exist? In addition, note that $A$ needs to satisfy Assumption 1 as well.\n3.  In line 363, the authors state that they aim to check whether the optimal value of equation 12 equals to 1. However, Theorem 2 only gives the probability that the maximum value is greater than 1. What's the relationship between them?\n4. Are optimization problems 14 and 2 equivalent? How to determine $\\lambda$?\n5. For the synthetic experiment, using the estimation error to evaluate the algorithm's performance is somewhat unconvincing. It is more reasonable to show that there exist a permutation matrix and a diagonal matrix that can convert the learned dictionary into the real one. In addition, multiple experiments should be conducted to record the corresponding success probability.   \n6. Why didn't the authors compare the proposed algorithm with other dictionary learning algorithms in the experiment? Currently, only a simple experiment is available.\n7. Where is the Figure mentioned in line 466?\n8. Many sentences in Introduction overlap with Hu and Huang (2023a)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}