{
    "id": "rpouyo09V0",
    "title": "ConvCodeWorld: Benchmarking Conversational Code Generation in Reproducible Feedback Environments",
    "abstract": "Large language models (LLMs) have proven invaluable for code generation, particularly in interactive settings. However, existing code generation benchmarks fail to capture the diverse feedback encountered in multi-turn interactions, limiting our ability to evaluate LLMs in these contexts. To address this gap, we present a set of novel benchmarks that explicitly model the quality of feedback provided to code generation LLMs. Our contributions are threefold: **First**, we introduce CONVCODEWORLD, a novel and reproducible environment for benchmarking interactive code generation. CONVCODEWORLD simulates 9 distinct interactive code generation scenarios while systematically combining three types of feedback: (a) compilation feedback; (b) execution feedback with varying test coverage; (c) verbal feedback generated by GPT-4o with different levels of expertise. **Second**, we introduce CONVCODEBENCH, a fast, static version of benchmark that uses pre-generated feedback logs, eliminating the need for costly dynamic verbal feedback generation while maintaining\nstrong Spearman\u2019s rank correlations (0.82 to 0.99) with CONVCODEWORLD. **Third**, extensive evaluations of both closed-source and open-source LLMs on CONVCODEWORLD reveal key insights: (a) LLM performance varies significantly based on the feedback provided; (b) Weaker LLMs, with sufficient feedback, can outperform single-turn results of state-of-the-art LLMs without feedback; (c) Training on a specific feedback combination can limit an LLM\u2019s ability to utilize unseen combinations; (d) LLMs solve problems in fewer turns (high MRR) may not solve as many problems overall (high Recall), and vice versa. All implementations and benchmarks will be made publicly available at https://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld",
    "keywords": [
        "Large language models",
        "Multi-turn code generation",
        "Benchmark"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "A novel multi-turn-based code generation benchmark with diverse feedback combinations",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=rpouyo09V0",
    "pdf_link": "https://openreview.net/pdf?id=rpouyo09V0",
    "comments": [
        {
            "summary": {
                "value": "The authors introduce ConvCodeWorld, a novel framework designed to benchmark the code generation performance of large language models (LLMs) in a conversational context. This setting allows LLMs to receive various types of feedback, including compiler messages, execution outputs, and human-like verbal feedback. To support reproducibility, the authors developed a benchmark, ConvCodeBench, that uses ConvCodeWorld with pre-generated feedback logs. They conducted a comprehensive evaluation of state-of-the-art (SOTA) LLMs and a detailed ablation study, providing valuable insights to inform future research on conversational code generation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The primary strength of this paper lies in its comprehensive approach:\n\n- The authors cover a broad spectrum of models, including proprietary and open-weight LLMs.\n- They thoroughly explore feedback settings across multiple types\u2014compilation, execution, and verbal.\n- Extensive ablation studies and analyses reveal findings that highlight key factors for advancing conversational coding tasks."
            },
            "weaknesses": {
                "value": "- **Novelty:** While this paper offers valuable insights into evaluating conversational code generation, the topic itself is not new. Prior works like InterCode and MINT have explored compilation, execution, and verbal feedback mechanisms (see Table 6).\n- **Clarity of Writing:** The paper\u2019s clarity could be improved, particularly in the introduction. Expanding the background on conversational code generation tasks and typical settings would help readers appreciate the unique contributions of this work. Additionally, certain terminology, like \"partial coverage\" in Table 1, could be more explicitly explained.\n- **Experiment Details:** Some important experimental details are missing or unclear:\n    1. **Temperature settings** for experiments are not specified. This detail is relevant as multi-turn interactions could benefit from more varied output.\n    2. The **construction of the proposed dataset** is insufficiently explained, particularly given the reported 29% success rate on BigCodeBench. This contrasts with single-turn performance on ConvCodeWorld, which appears higher.\n    3. **Prompting methods** lack clear definition. Both Section 4.1 and Appendix B are vague on this, with no explicit description of the prompt setup for experiments. \n    4. The construction of $f_v$ and $f_v^*$ is not well explained. Examples in Section 2.1.1 and Appendix F could be enhanced with a more formal, detailed breakdown of these prompt components.\n     5. In Section A.3, could the authors clarify the meaning of \"a model referenced ground truth code in $f_v^*$\"? Why would the model reference ground truth, and could examples illustrate this behavior?"
            },
            "questions": {
                "value": "- While the authors evaluate verbal feedback with proprietary GPT models, it would be beneficial to include SOTA open-weight models as well. Given that GPT models evolve frequently, this reliance may risk data leakage. Testing open-weight models, even if currently less performant, could strengthen the reproducibility and discussion of findings.\n- The claim in Appendix A.2 that the approach achieves \"1.5% of the cost of human annotation\" seems optimistic. Beyond the token generation cost, quality and accuracy of the model-generated content should also be factored in."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces ConvCodeWorld, a benchmark to evaluate LLMs in solving programming problems via multi-turn interactions with diverse feedback combinations. The benchmark encompasses 9 distinct feedback scenarios that combine compilation feedback, execution feedback with varying test coverage, and real-time verbal feedback simulated at different expertise levels using GPT-4o.  Additionally, a static version called ConvCodeBench is proposed, which uses pre-generated feedback logs to reduce computational costs while maintaining a high correlation with the live benchmark. In experiments, a comprehensive set of open/closed-source LLMs are evaluated on ConvCodeWorld and ConvCodeBench."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Evaluating LLMs in interactive environments is a significant and emerging area of study.\n2. The diverse combination of different feedback is reasonable.\n3. The introduction of static alternative ConvCodeBench reduces the overhead of real-time feedback generation making it scalable and practical for large-scale experiments.\n4. The authors conduct comprehensive experiments covering 17 open- and closed-source LLMs.\n5. The paper is easy-to-follow."
            },
            "weaknesses": {
                "value": "1. In the MINT paper (Wang et al., 2024), the multi-turn interaction (Figure 1, page 3) already includes execution results and human feedback. The human feedback in MINT covered both novice feedback (referred to as \"lazy user\" feedback in MINT) and expert feedback (referred to as \"natural language\" feedback in MINT). Given this, the novelty of this paper appears limited. Could the author include more comparisons between ConvCodeWorld and MINT to enhance clarity and novelty?\n    - Reference: Wang, X., Wang, Z., Liu, J., Chen, Y., Yuan, L., Peng, H., & Ji, H. MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. In The Twelfth International Conference on Learning Representations.\n\n2. Discrepancy in feedback simulation compared to actual human feedback\n    - Figure 13 (appendix) shows one example of the simulated expert-level verbal feedback, which is overly detailed and well-structured. I am concerned that it might not accurately reflect how real experts provide feedback when using LLMs in coding. An analysis comparing simulated feedback to actual human feedback would be both helpful and necessary.\n\n    - A similar concern was found in the simulated novice-level verbal feedback (Figure 12, appendix). The simulated novice-level feedback includes information that seems unrealistic for novice programmers (e.g., \"Also, I think there exists a simpler way to sort a list in Python.\"). How could a novice programmer provide such feedback based on the compilation and execution results? As suggested above, an analysis of how the simulated feedback relates to actual human feedback is necessary.\n\n3. Lack of Clarity in Evaluation Setup: The process of using partial execution feedback was not clearly explained. Did the author randomly omit test cases from the list of tests with full coverage? If so, what criteria were applied to ensure that the selection of test cases was reasonable? Providing a detailed experimental setup would enhance clarity.\n\n4. The claim that \"Weaker Models with Feedback Surpassing Single-Turn SOTA Models\" seems an unfair comparison.  While I understand that the aim is to highlight the value of multi-turn interactions, weaker models with multi-turn feedback inherently benefit from additional input context compared to single-turn, stronger models. It is therefore unsurprising that the weaker model with multi-turn interactions outperforms single-turn, stronger models. A fair comparison would evaluate the same LLMs in both single-turn and multi-turn scenarios (I thought this could also be concluded from the results section). The statement about using the same LLM would more effectively emphasize the role of multi-turn interactions in enhancing performance.\n\n5. Bais in Static ConvCodeBench\n    - The use of pre-generated feedback logs from a reference model seems biased. The errors made by the reference model might not reflect the actual errors the target model would encounter on the same problem. Thus, successful error correction might not be attributed to the feedback (or multi-turn interaction) on the code generated by the reference model, as the target model may already be capable of solving the problem on its own.\n    - The correlation calculated between ConvCodeWorld and ConvCodeBench is unreasonable based on the MRR and Recall since the results may be affected by the bias mentioned above. A better way to show this correlation is to show the correlation of error patterns existing in the generated code between the reference model and the target model."
            },
            "questions": {
                "value": "1. Could the author add more comparisons between ConvCodeWorld and MINT to enhance the clarity?\n2. Could the author address the discrepancy in feedback simulation compared to actual human feedback?\n3. Could the author explain the evaluation setup with partial execution feedback included? A follow-up question would be what if the problem does not have full coverage tests? How many of such cases exist in the dataset?\n4. Could the author respond to the potential bias in the static benchmark?  A better way to show the correlation between ConvCodeWorld and ConvCodeBench might involve presenting the error patterns between the reference model and the target model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose ConvCodeWorld, an environment for evaluating multi-turn code generation across 9 diverse feedback combinations. This entails compilation feedback, execution feedback (partial and full test coverage), and LLM-simulated verbal feedback (novice-level and expert-level). For this, they transform BigCodeBench-Instruct, a single-turn Python code generation benchmark. Additionally, they also propose a static benchmark to serve more practical purposes, ConvCodeBench, with pre-computed logs based on a weak model (CodeLlama-7B-Instruct). They find that performance on ConvCodeBench strongly correlates with performance on ConvCodeWorld. Using ConvCodeWorld, they evaluate 17 (3 closed-source and 14 open-source) LLMs of varying sizes. Through this, they highlight many key findings, including: (1)  Performance varies across feedback settings, often affecting the rankings, highlighting the need to select models based on the feedback available, (2) Weaker models with feedback can surpass single-turn SOTA models, (3) Models trained on a specific type of feedback struggle to generalize to unseen combinations of feedback, and (4) There is a tradeoff between an LLM's ability to solve problems in fewer turns (higher MRR) and solve many problems in total (high recall)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- As the authors have highlighted, evaluating multi-turn code generation is very difficult. By providing an environment with access to many diverse combinations of feedback, ConvCodeWorld could potentially be very useful to the research community, to hill-climb on multi-turn coding capabilities of LLMs.\n- The authors have been very thorough about exploring different types of scenarios, with partial/full execution feedback, and novice/expert-level verbal feedback. This allows more fine-grained evaluation.\n- There is an extensive analysis of performance of many different LLMs, and the key findings will likely be very interesting to the research community."
            },
            "weaknesses": {
                "value": "- While I agree that having a static benchmark like ConvCodeBench is useful, I am not convinced that it can be interpreted as a multi-turn evaluation set. Namely, my understanding is that it does not evaluate whether a model can iteratively improve its own prediction. The output from the previous turn is pre-defined, and so this seems to simply be trying to repair a program (which the model under test did not generate). If it fails to do it at a given turn, the output of that turn is ignored, and in the next turn, a fixed program is provided, which could be very different from the program the model under test generated in the previous turn. This setting seems to be a bit unnatural to me.\n- The environment is constrained to Python only. \n\n\nSuggestion:\n- It would have been interesting to have an ablation without the compiler feedback.\n- Other relevant paper to cite: https://arxiv.org/pdf/2306.09896"
            },
            "questions": {
                "value": "- Could you clarify that the number of samples for the single-turn baseline (i.e., first column in Table 3 and Table 4) is the same as the number of maximum turns?\n- Currently, it seems that for a given example, the same combination of feedback is given at each turn. Have you considered varying the type of feedback at different turns (e.g., compiler only in turn 1, compiler and execution in turn 2, compiler and novice feedback in turn 3, compiler and expert feedback in turn 4). Perhaps certain types of feedback or more useful (and more practical) at different steps?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents ConvCodeWorld, a benchmark for evaluating how well language models use feedback to address a code generation problem across multiple turns. While multi-turn interactive code generation work exists, the impact of the type of feedback on model performance is not as thoroughly examined. To carry out this study, the authors formulate three types of feedback (compilation, execution, verbal) and construct a pipeline (ConvCodeBench) that bootstraps existing code generation benchmarks to construct a static benchmark that follows the ConvCodeWorld formulation. In the results, the authors find that generally, closed source models perform best. Through ablations of different feedback techniques and open source models, the authors also discuss trends around the topics of generalization, the impact of turns, and the correlation between ConvCodeBench and ConvCodeWorld."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Feedback in code generation feels like a fairly understudied area. The premise of the paper is quite interesting in my opinion. The results are also neat, particularly how different models perform well under different feedback scenarios.\n- The fact that ConvCodeBench bootstraps existing code generation benchmarks is quite compelling. While the authors apply this towards BigCodeBench, it sounds like it would work for a wide variety of similar benchmarks.\n- The evaluation is quite comprehensive, and the experimental setup is easy to understand + sound.\n- Section 4.2.3 - Given the knowledge about how ReflectionCoder was trained, it is quite interesting to see that the model\u2019s performance varies with the type of feedback provided.\n- The authors\u2019 experiments reveal a number of interesting insights. I particularly liked how ConvCodeBench performance is shown to be a good proxy for ConvCodeWorld performance, and the relationship between MRR and recall."
            },
            "weaknesses": {
                "value": "- Section 2.1.1 is written in a way that a lot of references are made to the appendix, which requires a lot of jumping back and forth. As Appendix A.3 is referenced multiple times, it might be worth considering just putting the information justifying the claims discussed here directly in the main paper.\n- The mathematical notation in the paper is formalized correctly. From a readability standpoint, I would have preferred to just read what feedback types correspond to what results, as opposed to having to map symbols back to their corresponding feedback. Section 4.2.1 was a bit difficult to parse because of this. Of course, this is just my opinion.\n- For Takeaway 2 (Impact of Expert-level\u2026) in Section 4.2.1 - It was helpful to be told these observations as opposed to reading them from the table, but an explanation for why such trends were observed feels a bit lacking (\u201cstruggle to utilize complex feedback\u201d feels a bit coarse-grained as a justification for a lot of observations)\n- I\u2019m not sure the takeaway \u2013 that feedback helps weaker models perform stronger than stronger models in a zero-shot setting \u2013 is that surprising given prior work (e.g. Mint, InterCode discuss this to some capacity)."
            },
            "questions": {
                "value": "- How are you detecting whether ground truth code is leaked by the GPT 4o based expert? From reading A.3, I couldn\u2019t quite understand this. Is the `ground_truth_code` function doing an exact match lookup in GPT 4o\u2019s feedback? Or is there another implementation? Table 7 seems to be justifying that the verbal feedback doesn\u2019t leak ground truth code because of downstream task performance differences, instead of a direct examination of the verbal feedback\u2019s content.\n- What does Table 9 mean? When it is first referenced in Section 2.1.1, the phi character does not appear to be defined at that point in the paper. Also, GPT 4/4o models are tested exclusively. Any reason why Claude / Llama / other models were not used?\n- I agree with the justification that human annotators are more costly than GPT 4o. However, is it possible that novice/expert-level feedback looks very different between humans and language models? I think a justification for why LMs are a good proxy for human feedback for ConvCodeWorld would be good assurance.\n- Line 247: Which two settings are being referenced for this comparison?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}