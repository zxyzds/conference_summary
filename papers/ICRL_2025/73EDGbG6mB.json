{
    "id": "73EDGbG6mB",
    "title": "Parrot: Seamless Spoken Dialogue Interaction with Double-Channel Large Language Models",
    "abstract": "Recent advancements in large language models (LLMs) have demonstrated significant potential in enhancing real-time spoken interactions. Presently, open-source methodologies predominantly depend on intermediate generative text-based translations to manage real-time spoken dialogues. However, these techniques often struggle with providing seamless interactions that involve real-time streaming audio inputs. In this research, we unveil an innovative spoken dialogue language model, Parrot, distinguished by its unique pre-training and supervised fine-tuning (SFT) pipeline. This pipeline deviates from conventional methodologies by utilizing both single-channel audio data and double-channel spoken dialogue data to train the textless speech language model. During pre-training, we transmute single-channel audio input into a sequence of discrete tokens, thereby instructing the LLM to identify audio tokens via next-token predictions. In the SFT phase, we pioneer a novel approach to double-channel generative spoken dialogue language modeling with a unique ``next-token-pair prediction\" objective, facilitating the LLM's comprehension of natural human conversations. Our inventive pipeline equips the LLM to produce spoken interactions that are more natural and fluid than those generated by previous text-based approaches, as substantiated by thorough evaluations.",
    "keywords": [
        "Speech Language Models",
        "Generative Spoken Dialogue Language Modeling"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose a novel spoken dialogue language model that uses an innovative pre-training and SFT pipeline with dual-channel audio data and next-token-pair prediction paradigm, achieving real-time streaming interaction.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=73EDGbG6mB",
    "pdf_link": "https://openreview.net/pdf?id=73EDGbG6mB",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces Parrot, an audio LLMs models designed for modeling two-channel dialog audio. Parrot is built by fine-tuning an off-the-shelf text LLM on tokenized audio. At first, a single channel audio is used, as given in multiple standard datasets (eg LibriLight). Next, it is fine-tuned on a dialog dataset, Fisher. Here, the model is trained to simultaneously predict two tokens, one for each channel. According to the evaluation, this leads to a more natural dialog flow than in baseline model, dGSLM."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper proposes a simplification of a two-tower approach described by dGSLM, by using a single Transformer predicting two tokens."
            },
            "weaknesses": {
                "value": "1. In a few places, the paper misrepresents related work. Examples:\n  * \"the academic community primarily utilizes open-sourced models (Zhang et al., 2023a; Xie & Wu, 2024; Rubenstein et al., 2023; Huang et al., 2024; Wang et al., 2023a; Nachmani et al., 2024; Wang et al., 2023b) following a cascading approach.\" For instance, [Rubenstein et al., 2023] and [Nachmani et al., 2024] are not cascaded models. They are also not open-sourced.\n * \"Much of the prior research has utilized the encoder-decoder architecture to enhance pre-training (Borsos et al., 2023; Lakhotia et al., 2021; Kharitonov et al., 2022; Polyak et al., 2021; Chen et al., 2023; 2022; Hsu et al., 2021; Zeghidour et al., 2022; Defossez et al., 2023; Agostinelli et al., 2023; Ao et al., \u00b4 2022; Tang et al., 2022; Wu et al., 2023).\". ** The first three models are decoder-only LMs, and perhaps many others.\n\n2. The evaluation of the proposed model is limited and is insufficiently described. \n * S4.5.1 evaluates some pause-prediction accuracy metrics, but the metrics are not really introduced in the text. These metrics are evaluated on some synthetic data --- is there a reason a hold-out subset of Fisher is not used?\n * AudioQA evaluation and comparison to AudioQWEN-2 is only mentioned in a single sentence without any discussion or description. At the same time, it does require some discussion. What are the metrics used? The AudioQA figure indicates some reasonable performance on CoVost2 and FLEURS, I assume better than AudioQWEN-2. However, the training data does not include speech-to-speech translation examples nor contains non-tier1 languages. Should we assume the model somehow picked it up from purely text base model?\n* Audio-QWEN2 reports 90+% accuracy on VocalSound. In Figure 8c it is reported to be below 80%. What is the reason for the difference?\n\n3. The paper should at least mention Moshi https://arxiv.org/abs/2410.00037"
            },
            "questions": {
                "value": "* I would appreciate it if some of the weaknesses related to describing the evaluation study are resolved.\n\n * \"encodes each second of audio into 30-50 discrete tokens from a codebook of size 2048.\" Why is the token rate variable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "**Paper Summary**:\n- The authors present a textless spoken dialogue language model and corresponding training pipelines. The work involves two training stages: during pre-training, the LLM model is used to instruct the prediction of the next audio token, and in SFT, a double-channel mechanism is applied to predict the next token pair. Ablation studies demonstrate that this method yields more natural and fluid dialogue generation compared to baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Summary Of Strengths**:\n- Comprehensive Presentation: The paper includes all necessary sections, diagrams, and tables to demonstrate their contribution, the usefulness of the work, while also acknowledging its limitations.\n- Research Direction: The application of LLMs for audio token prediction and the double-channel mechanism are interesting and challenging directions in spoken language modeling."
            },
            "weaknesses": {
                "value": "**Summary Of Weaknesses**:\n- Novelty and Clarity: In the contribution summary, the authors list (1) the Parrot model and its innovative pre-training and SFT pipeline, (2) the paradigm of double-channel spoken language modeling, and (3) the evaluations. In my opinion, textless spoken language models are not particularly rare, especially in machine translation tasks (e.g., [Seamless: Multilingual Expressive and Streaming Speech Translation](https://arxiv.org/abs/2312.05187), [UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units](https://arxiv.org/abs/2212.08055)). Additionally, the pipeline and the double-channel modeling mechanism appear to be two perspectives on the same concept. It is difficult to consider the potential readiness for future exploration and evaluation as innovative contributions.\n- Limitations: Besides the limitation regarding the inability to integrate audio tokens, further analysis of the audio tokenizer should be elaborated on (e.g., how it impacts computational efficiency or downstream inference latency). More case studies should also be included to demonstrate the effectiveness of this work, as A.5 seems unfinished."
            },
            "questions": {
                "value": "As listed in the limitation section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a spoken dialogue model, Parrot, which leverages large-scale single-channel audio data for pre-training and moderate-scale dual-channel dialogue data for supervised fine-tuning. The authors also propose a \u201cnext token-pair prediction\u201d approach for spoken dialogue language modeling. The study claims that Parrot facilitates more natural and fluid conversations compared to Dialog GSLM and traditional cascaded spoken dialogue systems."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors state that they will open-source their training and inference framework, which would be a valuable contribution to the community, especially since existing end-to-end spoken dialogue models either do not disclose their training methodologies (e.g., Moshi) or lack sufficient documentation (e.g., Mini-OMNI).\n2. The authors also evaluate their approach on turn-taking properties, such as recognizing when the user has paused and determining appropriate moments to interrupt the user."
            },
            "weaknesses": {
                "value": "1. The paper lacks specific details on the cascaded system used for evaluation. DialogGSLM used a relatively weak cascaded system in their work, so it would strengthen this study if the authors evaluated a cascaded system with state-of-the-art ASR, LLM, and TTS models, such as Hugging Face's Speech-to-Speech (https://github.com/huggingface/speech-to-speech), for a fairer comparison. Additionally, latency in cascaded systems can be minimized by parallel threading each module, as demonstrated in the Hugging Face repository.\n\n2. Given the primary objective of this work is to enable engaging and naturally fluid conversations, a human study would be valuable for a thorough evaluation\u2014similar to assessments in text-based dialogue systems. When motivating their approach, the authors point out limitations in cascaded methods; more analysis is needed to clarify if these limitations genuinely impact user experience in human-AI conversations. User study could focus on user satisfaction compared to baseline systems as well as other auxiliary metrics such as naturalness of turn-taking and semantic coherence of response.\n\n3. Although the code is public, it lacks sufficient documentation, and I was unable to run their demo. For example, an inference.py file appears to be missing. \n\n4. The paper would benefit from further discussion on design choices:\n\na. I was curious about the choice to use a single codebook. Table 4\u2019s synthesized audio quality results lack clarity on the dataset used. Multiple codebooks often improve results\u2014did the authors experiment with this option?\n\nb. The motivation for using next-token pair prediction, rather than multi-stream prediction as seen in Moshi, is also unclear. While streaming audio generation is effective, the paper doesn\u2019t address the length limits for audio modeling. Given the potential length of audio sequences, does the model implement techniques to reduce sequence length?\n\nc. The authors use only human-human conversation data for supervised fine-tuning. Adding human-AI conversation data, even synthetic, could be beneficial as there are nuance differences in how humans communicate with AI versus other humans.\n\n5. Clarity\n\na. Section 4.5.1 is difficult to follow. My understanding is that the ground truth is generated using GPT-4. Did the authors verify this ground truth with human judgments? I would recommend the authors to clarify their methodology for generating and validating the ground truth data, and include this information in the paper.\n\nb. Section 4.5.3 presents an interesting observation. Do the authors have any intuition as to why this occurs?\n\n6. In addition to evaluating turn-taking properties, the paper could also benefit from evaluating \"speaking-while-listening\" capabilities (https://arxiv.org/pdf/2408.02622).\n\n7. Did the authors assess whether the LLM undergoes catastrophic forgetting when fine-tuned on audio data? For example, is Parrot still capable of instruction-following or answering factual questions at a level comparable to LLAMA 3.1?\n\n8. In section A.1, the paper makes claims about prior works that lack experimental or discussion-based support. For instance, the statement that RQ transformers reduce model efficiency is not substantiated. Unsubstantiated claims should be avoided, as they could lead to incorrect community conclusions."
            },
            "questions": {
                "value": "Check weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The Parrot framework aims to address double-channel spoken dialogue modeling by implementing a pipeline that includes pre-training on single-channel audio and fine-tuning on double-channel audio data. The framework introduces a \"next-token-pair prediction\" approach within a decoder-only model architecture. However, the proposed solution lacks substantial originality and leaves questionable details for evaluation, which ultimately weakens the paper."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The framework presents a relevant approach to double-channel spoken dialogue, aiming to improve latency by avoiding text generation stages, which could contribute to real-time applications."
            },
            "weaknesses": {
                "value": "* Minimal Novelty: The framework essentially serves as a decoder-only adaptation of dGSLM combined with a textually pre-trained model (TWIST), offering limited technical innovation. This incremental change does not justify the need for a separate model or paper.\n\n* No Human Evaluation: The absence of human assessment significantly limits the validity of the framework's claims about improving conversational fluidity and natural interaction, which are central to spoken dialogue applications.\n\n* Lack of Established Benchmark Comparisons: Despite the existence of standard benchmarks like ZeroSpeech [1] and StoryCloze [2] for textless spoken language models, the paper does not include comparisons with these datasets. This omission raises concerns about the thoroughness of the experimental validation.\n\n* Poorly Defined Evaluation Methodology: The evaluation details, especially for the reflective pause and interruption response accuracy (Section 4.5.1), are incomplete. Key information, such as the evaluation metric definitions like interaction accuracy, is missing, making it hard to verify the claimed improvements.\n\n* Insufficient Explanation of Key Evaluation Components: See the comments below.\n\n1. Zerospeech 2021 benchmark, https://arxiv.org/abs/2011.11588\n2. StoryCloze, https://arxiv.org/abs/2305.13009"
            },
            "questions": {
                "value": "* L473-476: The discussion on \"layer-wise\" and \"consistent\" channel embeddings is unclear. These terms appear only once and lack explanation, leaving their meanings and relevance ambiguous to the reader.\n* L1162-1187: The purpose of the GPT score is not clear. It is mentioned in the appendix, but its usage and significance are not explained in the main text, making it difficult to understand its role in the evaluation.\n8 483-505: Since AIR-bench questions are in text format, it is unclear how the proposed model, which is audio-based, handles text input. Without further clarification, it is difficult to interpret the evaluation results accurately.\n* Section A.1: The essential distinctions between the proposed method and closely related prior work should be concisely summarized in the main related works section. The current related works section lacks sufficient comparison, especially with the most relevant prior methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents an approach to spoken dialogue interaction through the development of a large language model (LLM) named Parrot. The authors propose a two-stage training pipeline that leverages single-channel audio data for pre-training and double-channel audio data for supervised fine-tuning (SFT). The key innovation lies in the \"next-token-pair prediction\" paradigm, which aims to enhance the model's ability to comprehend and generate natural human conversations in real-time."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. This work explores an interesting topic, namely dual-channel speech input modeling. Previous work has focused on user-assistant turn-based interactions, but in real interactions, immediate processing is required. Therefore, there is a need for a listening channel to continuously process user's speech input.\n\n2. The author made reasonable reviews and citations to related work."
            },
            "weaknesses": {
                "value": "1. The writing of this paper is very poor, with some tables and figures not being referenced in the main text, and inconsistent statements in the context, making the entire article very difficult to understand. (see Questions for details)\n\n2. The structure design of the model and \"next-token-pair prediction\" paradigm are not well-motivated, as there is a significant gap between training and inference stages.\n\n    -  The author inputs listening tokens and speaking tokens as pairs into the LLM, which doubles the context length. LSLM [1] has demonstrated the effectiveness of modeling double-channel through embedding fusion and is more efficient in context. Therefore, the author needs to prove the effectiveness of their modeling approach\n    -  In the training process, the prediction of the next token depends on the previous predicted tokens. But in the inference process, according to section 3.3, listening tokens are sent to LLM in chunks. At this point, predicting the next speaking tokens no longer depends on the previous predicted speaking tokens. This inference method will disrupt the casual modeling during training.\n\n3. The lack of evaluation details.\n\n      -  In section 4.5.1, LLAMA-Omini and SpeechGPT do not have the ability to interrupt and pause. How does Reflective Pause and Interruption caculated?\n\n      -  In section 4.5.2, all these evaluation metrics cannot reflect the linguistic quality of the model. For an open-ended instruction-following task, there is no evaluation of response quality.\n\n      -  In section 4.6, the evaluation of speech-to-text tasks, such as ASR and ST, is discussed. However, this model is a textless speech-to-speech model and does not have text generation capability, so it is unclear how these tasks are evaluated.\n\n4. Some results are very strange, which it\u2018s hard for me to believe that it's real.\n\n      -  According to table 1, the model is only trained on English-Only speech data, but in figure 8(c), the model can perform Chinese ASR task on aishell, SER task on MELD, and sound-related task on AIRBench-Sound. This is very strange.\n      -  In appendix A.4.2, the larger latency, the worse performance. There is no explanation for this strange results."
            },
            "questions": {
                "value": "1. In section 4.1, the authors claim to have used 14,000 hours of single-channel for pretraining and 2,200 hours of double-channel for sft. However, in table 1, there are over 70,000 hours of single-channel data and 2,000 hours of double-channel data. At the same time, InstructS2S-200K is a single-channel data, how do you use it in stage 2?\n\n2. In section 4.5.3, the author claims that the embedding of two channels is in different spaces, but in Figure 7, there are two completely different feature spaces, with the right side appearing consistent with the author's statement, so what is the left side of the figure?\n\n3. In Appendix A.6.3, the author lists \"prompt for gpt score,\" but there is no mention in the main text of the need for gpt evaluation.\n\n4. How much data was used to train the speech tokenizer? And what's the output rate of the speech tokenizer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}