{
    "id": "AcR5Mngp1p",
    "title": "Knowledge-localized Unlearning for Faithful Forgetting in Language Models",
    "abstract": "Large language models are exposed to privacy risks since they are trained on large text corpus, which may include sensitive or private information. Therefore, existing studies have attempted to unlearn undesirable knowledge exposed without permission from a language model. However, they are limited in that they have overlooked the complex and interconnected nature of knowledge, where related knowledge must be carefully examined. Specifically, they have failed to evaluate whether an unlearning method faithfully erases interconnected knowledge that should be removed, retaining knowledge that appears relevant but exists in a completely different context. To resolve this problem, we first define a new concept called superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. Based on the definition, we introduce a new benchmark, FaithUnBench, to analyze and evaluate the faithfulness of unlearning in real-world knowledge QA settings. Furthermore, we propose a novel unlearning method, KLUE, which identifies and updates only knowledge-related neurons to achieve faithful unlearning. KLUE categorizes knowledge neurons using an explainability method and updates only those neurons using selected unforgotten samples.  Experimental results demonstrate that widely-used unlearning methods fail to ensure faithful unlearning, while our method shows significant effectiveness in real-world QA settings.",
    "keywords": [
        "Unlearning",
        "Knowledge-localization",
        "Faithful Unlearning",
        "Superficial Unlearning"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "Knowledge-localized Unlearning to Ensure Faithful Forgetting for Language Models",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=AcR5Mngp1p",
    "pdf_link": "https://openreview.net/pdf?id=AcR5Mngp1p",
    "comments": [
        {
            "summary": {
                "value": "This paper focuses on the issue of \"unfaithful\" unlearning knowledge from the LLMs, including failing to erase the knowledge it should remove and unintentionally erasing irrelevant knowledge. A new benchmark, FaithUnBench, is proposed for analyzing and evaluating the faithfulness of unlearning in the knowledge QA settings, consisting of Paraphrased QA, Multi-hop QA, and Same-answer QA datasets. The authors also present an approach to mitigate the issue. In particular, it identifies and updates only the knowledge-related neurons based on selected unforgotten samples."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Present a dataset for faithful unlearning knowledge from LLMs, targeting at different categories of unfaithful issues\n* Experimental results demonstrate the effectiveness of the approach, with further detailed analysis"
            },
            "weaknesses": {
                "value": "* The approach relies on the world knowledge graph, which is restricted to the triple-based QA settings"
            },
            "questions": {
                "value": "* Have you tried larger p value for the experiments in section 5.6? It seems that the tendency is still going up.\n* Have you compared different prompting templates for MCQA? Furthermore, apart from the convenience for evaluation, have you compared it with other ways of prompting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors define superficial unlearning and construct a new benchmark, FaithUnBench, to analyze and achieve faithful unlearning.\nFurthermore, the authors propose a novel knowledge-localized unlearning method, KLUE, to mitigate superficial unlearning and\nreveal that our method outperforms other unlearning methods, dramatically mitigating superficial\nunlearning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This article contributes a dataset and an effective method to the community."
            },
            "weaknesses": {
                "value": "1. There is no comparison of the proposed dataset with previous datasets, such as MUSE, WMDP, KnowUnDo, TOFU.\n2. In the Faithful Unlearning setting, some knowledge related to current entities should not be forgotten. Has this consideration been taken into account in the constructed dataset? For example, in Figure 1, changing Tom Cruise's nationality should not affect the answer regarding his notable works. Intuitively, the unlearning process is more likely to damage content related to Tom Cruise rather than affect another person's nationality."
            },
            "questions": {
                "value": "The goal of unlearning is to completely forget specific knowledge. Has the author considered the following scenario: testing whether the knowledge has truly been forgotten by asking about it in a different language?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "To study the impact of machine unlearning on other related knowledge, the authors define a new concept called superficial unlearning. Based on the definition, they propose FaithUnBench to reveal that existing unlearning methods do not ensure faithful unlearning. To achieve faithful unlearning, the authors propose KLUE to update only knowledge-related neurons via the gradient ascent method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. It is interesting to evaluate the faithfulness of unlearning using Multi-hop QA and Same-answer QA.\n\n2. It is reasonable to precisely update by localizing certain parameters to reduce the side effects of unlearning."
            },
            "weaknesses": {
                "value": "1.There is a lack of detailed comparison with existing datasets. For example, RWKU [1] also adopts 200 popular real world entities as the unlearning target knowledge and also considers the impact on related knowledge.\n\n2.The proposed localization method is overly simplistic. How can neurons that express unrelated knowledge be avoided during localization? Additionally, neuron localization is not the only method for localizing key parameters. How does it compare with other localization methods?\n\n3.There is a lack of more in-depth analysis. For example, in the analysis of the distribution of localized neurons, whether there is a certain distribution pattern for the neurons corresponding to different knowledge. \n\n[1] RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models"
            },
            "questions": {
                "value": "1.There is a lack of quality assessment for the constructed benchmark. How can the noise introduced by GPT-4o be avoided?\n\n2.Why can the problem of superficial forgetting be solved by neuron localization? Some case studies can be done on Multi-hop QA to check if MAf is really reduced rather than MAt being increased due to fewer updated model parameters."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on the issue of superficial unlearning in language models, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. To investigate the phenomenon of superficial unlearning, this paper introduces a new benchmark, FaithUnBench, to evaluate unlearning methods in real-world knowledge QA settings. Then, it proposes an unlearning method, which identifies and updates only knowledge-related neurons to achieve faithful unlearning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.This paper explores a promising direction: the timely issue of removing sensitive or private information from language models.\n\n2.This paper defines the problem of superficial unlearning and constructs a benchmark for a more in-depth analysis and evaluation of unlearning methods."
            },
            "weaknesses": {
                "value": "1.The choice of evaluation metrics is unreasonable. Why is the UA of all baselines equal to 0.33 (only GA on Gemma2 is 30.30)? Does this suggest that the unlearning dataset is simple enough and more rigorous testing methods need to be designed? For instance, WMDP [1] also uses multiple-choice QA to evaluate the unlearning effect, yet existing unlearning methods struggle to reach the level of random guessing.\n\n2.The implemented baselines are relatively few, and more models (LLaMA3) and unlearning methods (such RMU [1] and NPO [2]) need to be compared.\n\n[1] The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning.\n\n[2] Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning."
            },
            "questions": {
                "value": "1.What is the form of the data to be removed? Is the Base QA used as the training data? This is easier compared to unlearning the Harry Potter books."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}