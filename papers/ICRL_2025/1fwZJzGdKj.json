{
    "id": "1fwZJzGdKj",
    "title": "Multi-Agent Collaborative Data Selection for Efficient Language Model Pretraining",
    "abstract": "Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LLM pretraining. To tackle this problem, we propose a novel multi-agent collaborative data selection mechanism. In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the LLM training process. We conduct extensive empirical studies to evaluate our multi-agent framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LLM training, and achieves an average performance gain of 10.5% across multiple language model benchmarks compared to the state-of-the-art methods.",
    "keywords": [
        "Language Model Pretraining; Data-efficient Training; Data Selection"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1fwZJzGdKj",
    "pdf_link": "https://openreview.net/pdf?id=1fwZJzGdKj",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes to a mechanism to select data into the training process based on 3 main measure (quality, domain and topic) of the data. The 3 measures are adjusted dynamically and aggregating together to determine whether data point can be selected during the training process. A RL paradigm is employed to realize the proposal. Good performance is demonstrated in the provided experiments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "\u2022 A good analysis of how different aspects influence the performance of LLM training.\n\u2022 The experiment seems to demonstrate this kind of scoring of data points can help to improve the data deficiency and performance."
            },
            "weaknesses": {
                "value": "\u2022 The connection to agent or multi-agent paradigm seems weak to me. It might not be necessary to formulate the problem and the solution via \"agent\" concept. A direct stochastic optimization formulation might provide  more direct description and help audience better mastering what is the proposal."
            },
            "questions": {
                "value": "1. Is it possible to not employ the agent or multi-agent metaphor to formulate the proposal? What is the truth power of the proposal? Is it a compossible and multi-step paradigm of stochastic optimization  based on 3 hand-crafted dimensions? \n2. Ine line 161, what is the loss function l? Also please explain more regarding the definition of the reward function. How is it different o related to the loss function of LLM auto-regression loss or other kinds of losses (if any other).\n3. In Algorithm 1, please clarify whether the sampling distribution of the data are different from iteration to iteration (line 3). \n4. In line 275, 373M model is used in ablation study. It is pretty small a model. The conclusions drawn from it might not be transferrable to LLMs of billions of parameters. Please justify the study."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel multi-agent collaborative data selection mechanism aimed at enhancing data efficiency during the pretraining of large language models (LLMs). Recognizing that existing data selection methods often operate independently and may conflict with one another, the authors propose a framework where each data selection method functions as an independent agent. An agent console dynamically integrates the information from all agents throughout the training process. The agents adjust their weights based on reward signals derived from the model's performance on reference tasks. The framework is designed to flexibly and robustly combine various data selection strategies, such as data quality scoring, topic diversity, and domain information. Extensive experiments demonstrate that this multi-agent approach significantly accelerates convergence in LLM training and achieves an average performance gain of up to 10.5% across multiple benchmarks compared to state-of-the-art methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. Introducing a multi-agent framework to collaboratively select pretraining data is a novel idea that addresses inherent conflicts among existing methods.\n2. The empirical evaluation is extensive, comparing the proposed method against a wide range of baselines and demonstrating significant improvements.\n3. The paper clearly articulates the motivation, methodology, and findings, making it accessible to readers.\n4. Improving data efficiency in LLM pretraining is a critical challenge, and the proposed method offers a practical solution with demonstrable benefits."
            },
            "weaknesses": {
                "value": "1. While the experiments show promising results on models up to 1.3 billion parameters, it is unclear how the approach scales to larger models commonly used in practice.\n2. The choice of agents (quality, domain, topic) seems somewhat ad-hoc. A discussion on how to generalize the selection of agents or include other data selection criteria would strengthen the paper.\n3. While ablation studies are included, more detailed analysis on how each agent contributes to different types of tasks could provide deeper insights."
            },
            "questions": {
                "value": "1. How does the proposed framework perform when scaling up to larger models (e.g., 10B+ parameters) and datasets (e.g., trillions of tokens)?\n2. Can you provide a more detailed analysis of the computational overhead introduced by the multi-agent system compared to baseline methods? \n3. What guidelines can be provided for selecting or designing agents for other data selection criteria? Is the framework flexible enough to incorporate new agents easily? How sensitive is the method to the choice of number and types of agents?\n4. How sensitive is the performance to the choice of reference tasks used for calculating rewards in the influence functions?\n5. Can you elaborate on how the dynamic adjustment of agent weights impacts the learning process over time? Are there scenarios where this adjustment could lead to suboptimal data selection?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a multi-agent approach to strategically select data for pretraining LLMs. The paper motivates the need for a multi-agent design by sharing a case study on the SlimPajama pretraining dataset. They illustrate that the most common data set considerations (and their corresponding metrics), including data quality, topic diversity, data impact, and data domain are not straightforward to jointly optimize. Therefore, they propose a multi-agent system, where each data selection method/metric is represented as an agent. Through this multi-agent approach, these methods can be mixed via multi-agent collaboration, forming a highly adaptive approach in data selection. They show that their multi-agent approach is effective: the data curated by their method leads to faster convergence in training and improved benchmark performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "-Mixing data quality and data selection techniques is a challenging problem: they show in their case study that typical data curation techniques can conflict and naively combining them is not sufficient.\n\n-Unlike many off-the-shelf multi-agent systems, they are proposing optimization of each agent\u2019s weights (stored in memory) based on reward signals from the model undergoing pretraining.\n\n-Results show that their multi-agent data selection produces the best performance. Ablations show the three agents in collaboration outperform all other permutations of the agents (with and without collaboration) and strongly outperform the setting with no agents at all. \n\nOriginality:\nI am not aware of another multi-agent approach for data selection in pretraining - so this appears to be a novel application of multi-agent systems.\n\nQuality:\nAll key components covered - clear literature review, motivation, experiment design, results. It would have been better if they made their contribution differentiations clear in the literature review - for ex, confirming if they are indeed the first multi-agent approach for data selection. And if not, how are they different.\n\nClarity: \nThe paper was generally well written and easy to read.\n\nSignificance:\nPretraining is the most critical and expensive operation for LLMs. To make the best use of your pretraining, optimizing the data is key."
            },
            "weaknesses": {
                "value": "-Figures can be improved, see my comments below.\n\n-Experiments only on 1B model. It would be interesting to see the impacts across more model sizes (smaller and larger) and model architectures to see which range of models really benefit from this. \n\n-This shows a lot of potential already, but the point would be very strongly proven if they could show comparison to other 1B model performances (DeepSeek, TinyLlama, etc.), showing that their approach yields superior models in general."
            },
            "questions": {
                "value": "-Figure 2 could be more clear to show that each agent has its own memory. Consider focusing only on the Domain Agent\u2019s flow of work to make it easier to follow.\n\n-In Table 1, it would be best to bold the top results so it\u2019s easier to see that your approach is indeed among the best.\n\n-Should this really be considered a multi-agent system or is this really a multi-step process? I don\u2019t see any use of reasoning or decision making here. It seems at each step, each agent is systemically called/updated and each data point is labeled with a combination of scores from the \u201cagents\u201d. What is \u201cagentic\u201d about this? The approach is still valuable, just questioning whether it falls under \u201cagents\u201d.\n\n-Does your approach add significant additional latency to the pretraining stage? \n\n-Is your approach particularly valuable for small models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors argue that training data selection is an important component in LLM training. The various techniques that had been proposed might be conflicting in their recommendations. The authors are proposing a technique in which the different data selection algorithms are considered independent agents, with an \"agent console\" integrating the recommendations. The approach enable the dynamic adjustment of contributions of the agents during the training process of the LLM. The SlimPajama dataset is used as the working example."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* A case study validates the initial claim of the paper, that the quality, diversity and influence scores of data are not strongly correlated with each other.\n* Relatively extensive experiments were conducted with the training of a 1.3B LLAMA LLM."
            },
            "weaknesses": {
                "value": "* The fact that the diversity of topics, quality of material, and influence on the trained LLMs are different metrics is not a surprising observation - these are obviously very different things. A strong correlation between them would be the surprise. \n* The term \"agent\" has a relatively clear definition in the AI literature, as an autonomous entity, that takes actions in an environment, in the pursuit of a goal. The fact that the authors have to introduce a definition for the term \"agent\" and \"agent console\" and define them in terms of \"data selection method\" makes the paper difficult to follow. It doesn't seem that these \"agents\" are taking any actions, or have any autonomy. \n* In practice, the proposed approach appears to be a way to make a decision about what training data to include based on weighting three pre-existing metrics about the data sources. This decision process could have been easily written without introducing agent language. In fact, practitioners very likely are already making such judgements."
            },
            "questions": {
                "value": "* How does the definition of the agent used in this paper relates to the term agent used in other fields of AI? Such as multi-agent RL, or the type of work usually published in AAMAS? \n* The term \"console\" is usually applied to a component that is used by a human operator. Is the term agent console in the paper related to it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}