{
    "id": "WEQL5ksDnB",
    "title": "Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation",
    "abstract": "Audio-visual speech recognition (AVSR) incorporates auditory and visual modalities to improve recognition accuracy, particularly in noisy environments where audio-only speech systems are insufficient. While previous research has largely addressed audio disruptions, few studies have dealt with visual corruptions, e.g., lip occlusions or blurred videos, which are also detrimental. To address this real-world challenge, we propose CAV2vec, a novel self-supervised speech representation learning framework particularly designed to handle audio-visual joint corruption. CAV2vec employs a self-distillation approach with a corrupted prediction task, where the student model learns to predict corrupted frames while clean targets are generated by the teacher model. Specifically, we suggest a unimodal multi-task learning, which distills cross-modal knowledge and aligns the corrupted modalities, by predicting clean audio targets with corrupted videos, and clean video targets with corrupted audios. This strategy mitigates the dispersion in the representation space caused by corrupted modalities, leading to more reliable and robust audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that the corrupted representation learning method significantly enhances recognition accuracy across generalized environments involving various types of corruption.",
    "keywords": [
        "robust audio-visual speech recognition",
        "audio-visual corruption",
        "multimodal representation learning"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=WEQL5ksDnB",
    "pdf_link": "https://openreview.net/pdf?id=WEQL5ksDnB",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces CAV2vec, a self-supervised audio-visual speech representation learning framework focused on robustness to joint audio-visual corruption in noisy environments.\nCAV2vec uses a self-distillation framework with corrupted prediction tasks. It leverages a teacher-student model where the student learns to predict corrupted frames while clean targets are generated by the teacher model. They proposed an unimodal multi-task learning strategy that mitigates the dispersion in the representation space caused by corrupted modalities, thereby enhancing reliable and robust audio-visual fusion."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Clear Problem Definition: The paper clearly identifies limitations in existing methods and proposes a well-reasoned solution.\nNovelty: The approach of using unimodal multi-task learning for cross-modal corrupted prediction is innovative and addresses a real-world challenge in AVSR.\nTechnical Depth: The framework uses a combination of advanced techniques, including self-distillation, multimodal Transformer encoders, and self-supervised learning without architectural changes.\nEmpirical Validation: The experiments on multiple benchmarks (LRS2, LRS3) and various corruption scenarios demonstrate improved robustness.\nClarity: The paper is well-structured, and technical sections are detailed with visual aids, providing clarity on the approach and results."
            },
            "weaknesses": {
                "value": "The evaluation on the LRS2 benchmark could be more comprehensive. The authors should consider including additional experimental results, similar to those presented for the LRS3 benchmark. This would better illustrate the model's robustness under real-world conditions."
            },
            "questions": {
                "value": "There is an error in the sentences on lines 421 to 422. Specifically, the description of CAV2vec\u2019s performance on unseen types of corruption (i.e., hand occlusion and pixelated face) is incorrect. According to Table 1, the correct values should be 5.2 and 5.1, respectively."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a technique called CAV2vec which is a model trained with a so-called corrupted prediction task: the student's encoded output on corrupted input is trained to be similar to the teacher's output on clean (uncorrupted) input in an audio-visual speech recognition task consisting of audio (speech) and video (lip tracks) inputs. They specifically using cross-modal targeting gives additional benfit. The demonstrate success against a variety of strong models, yielding equivalent state-of-the-art for AVSR on uncorrupted input (vs AV data2vec) and better results than state-of-the-art on artificially corrupted data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Paper shows good results against a variety of relevant, strong baselines. In table 1 they show that on both visual and audio corruption CAV2vec shows the best robustness. In some ways, this could be considered an extension of data2vec with the corrupted prediction task yielding better results. In this light, it is notable that the task does not degrade on the Clean condition (e.g. 1.5% is still the best number in Table 1). However on the artificially corrupted noisy conditions, Cav2vec is peforming more than 10% relative on average than the next best systems. The paper also provides ablations to tease out what part of the corruption is helping most, e.g. ACP + VCP."
            },
            "weaknesses": {
                "value": "The clearest wins are on the artificially created corrupted noise sets---this is often the case when the experiments simulate harder test conditions and corresponding training conditions to address the harder test. Other systems, do not have the benefit of training to match the altered test conditions.\n\nThe various acronyms AVCP, mACP, mCP, ACP, VCP are somewhat hard to follow. While Figure 5 helps, perhaps a table indicating exactly what they mean with columns indicating differing dimensions could help (it may not...)."
            },
            "questions": {
                "value": "Are there harder, non-simualted noisier datasets that could be used to evaluated the benefit of CAV2vec over say data2vec?\n\nIs the data2vec system trained on, is it possible to train on, the same distributions and corruptions in training as the CAV2vec system?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a self-supervised framework for learning audio-visual speech representations which are robust to noise. This is achieved by including audio and visual corruptions during the self-supervised learning (SSL) phase. As a consequence, the model is more robust to audio and visual noise during evaluation. Results are shown on the LRS2 and LRS3 datasets and the proposed approach outperforms prior SSL approaches on noisy conditions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is well written.\n\nThe paper investigates the use of audio and visual corruptions during the SSL phase which has not been thoroughly investigated in previous works."
            },
            "weaknesses": {
                "value": "The paper has a lot of similarities with AV-data2vec, AV-Hubert, Raven and Braven, especially regarding the masked prediction and cross-modal prediction tasks. It would be very useful if some discussion is added in the related work section explaining the differences with each of these approaches.\n\nSince the proposed model is pre-trained with corrupted data it's reasonable that it performs better than the other SSL methods which have not been trained with such data. The authors should make clear what the contribution is. If it's just the addition of corruptions during pre-training and then it should be clearly stated that the other approaches might also benefit if they are pre-trained with such corruptions.\n\nThe proposed model is initialised with AV-HuBERT weights. This is done for efficiency but it\u2019s still different from other methods which are randomly initialised. Can the authors comment on the impact of this initialisation? Are there any results where the model is trained from scratch?\n\nThe number of parameters in Table 1 is not 100% accurate. Raven and Braven have 673M during pre-training and during inference, however the other approach have 325M only during the pre-training phase. During inference the number of parameters is double, since models are separately fine-tuned for each modality.\n\nL. 351-353 state that publicly available pre-trained models  are used and then fine-tune them based on their hyperparameter settings. This leads to significantly worse results for RAVEn / BRAVEn than the ones published. For example, on clean LRS3 audio, results of 2.3 / 1.8 % WER for RAVEn / BRAVEn are presented whereas the published results are 1.4 / 1.1 % WER. Can the authors comment on this discrepancy? And maybe explain why this happens?\n\nL.18: Does the model predict \u201ccorrupted frames\u201d as stated? I think it predicts clean frames."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a self-distillation framework for audio-visual speech representation learning. It enhances representation robustness through multimodal and unimodal multi-task learning. Experimental results demonstrate performance improvements over existing methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The target is clear. It aims to handle audio-visual joint corruption in AVSR, which is quite common in real-world.\n\n+ The method is simple and efficient, requiring no additional modules to handle audio-visual corruptions, which makes it practical for implementation."
            },
            "weaknesses": {
                "value": "- The work is similar to several existing works in the view of methodology, like AV-Hubert, raven, dinoSR, ES^3. One difference is the cross-modal prediction of both A\u2192V and V\u2192A, instead of just V\u2192A, which is mainly due to the introduction of corruptions in both A and V modalities in this work. Although the starting point of tackling corruptions in both modalities is meaningful, this specific learning manner is not so insightful or so original. Compared with those existing works, there are little new things in this work. There are already many works learning audio-visual speech representations with EMA framework at present. I am not clear about the novelty of the work in the view of methodology compared with existing works.\n\n- The validation in this paper is not thorough enough, as the authors only demonstrated improvements on the AVSR task, although they explicitly stated their focus on AVSR. In real-world scenarios, complete corruption of one modality is possible, and missing modalities are common. If the audio-visual representations learned by this method are truly robust, then performance on single-modality tasks (ASR and VSR) would help substantiate this claim.\n\n- The reliance on AV-HuBERT pre-trained weights raises questions about the framework's standalone capabilities. It remains unclear whether comparable performance could be achieved training from scratch, or if robust audio-visual representations are inherently dependent on AV-HuBERT. \n\n- The manuscript suffers from clarity issues and leaves several questions (detailed below)."
            },
            "questions": {
                "value": "1. Figure 3's notation lacks precision. While the bidirectional arrows suggest symmetric interactions, the prediction task implies directed objectives. \n\n2. What is the definition of \"visual corruption events\"? Is it having a single corrupted frame in the whole visual sequence or something else? In Appendix A.2, the description \"Object occlusion applied to the speaker's lips occurs once, followed by Gaussian noise or blurring, each with a probability of 0.3\", does this mean that occlusion only appears in a single frame within the sequence?\n\n3. The rationale for applying MP (Masked Prediction) exclusively in multimodal settings, while CP (Corruption Prediction) spans both multimodal and unimodal scenarios. It would be better to clarify the reasons.\n\n4. The parameter k in MP's objective (targeting the average of teacher model's top-k blocks output) is not specified, and the motivation behind this design is not explained.\n\n5. The absence of L_AVCP in the overall multi-task loss function requires explanation.\n\n6. What is the motivation for MLM and what is the setting of it? How to acquire the cluster index? Need explanation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}