{
    "id": "xLPakPOKDX",
    "title": "Causally Motivated Diffusion Sampling Frameworks for Harnessing Contextual Bias",
    "abstract": "Diffusion models have shown remarkable performance in text-guided image generation when trained on large-scale datasets, usually collected from the Internet. These large-scale datasets have contextual biases (e.g., co-occurrence of objects) which will naturally cascade into the diffusion model. For example, given a text prompt of ``a photo of the living room'', diffusion models frequently generate a couch, a rug, and a lamp together while rarely generating objects that do not commonly occur in a living room. Intuitively, contextual bias can be helpful because it naturally draws the scene even without detailed information (i.e., visual autofill). On the other hand, contextual bias can limit the diversity of generated images (e.g., diverse object combinations) to focus on common image compositions. To have the best of both worlds, we argue that contextual bias needs to be strengthened or weakened depending on the situation. Previous causally-motivated studies have tried to deal with such issues by analyzing confounders (i.e., contextual bias) and augmenting training data or designing their models to directly learn the interventional distribution. However, due to the large-scale nature of these models, obtaining and analyzing the data or training the huge model from scratch is beyond reach in practice. To tackle this problem, we propose two novel frameworks for strengthening or weakening the contextual bias of pretrained diffusion models without training any parameters or accessing training data. Briefly, we first propose causal graphs to explicitly model contextual bias in the generation process. We then sample the hidden confounder due to contextual bias by sampling from a chain of pretrained large-scale models. Finally, we use samples from the confounder to strengthen or weaken the contextual bias based on methods from causal inference. Experiment results show that our proposed methods are effective in generating more realistic and diverse images than the regular sampling method.",
    "keywords": [
        "Causal Inference",
        "Diffusion Models",
        "Contextual bias",
        "Spurious Correlations",
        "Object Cooccurrence",
        "StableDiffusion"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=xLPakPOKDX",
    "pdf_link": "https://openreview.net/pdf?id=xLPakPOKDX",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a causally motivated approach to enhance image diversity and fidelity in large-scale diffusion models by addressing contextual bias, without the need for retraining or extensive data access. The proposed methods involve causality-inspired techniques to modulate the influence of contextual information during the diffusion process, thus balancing realistic image generation with diverse outputs. Through experiments on datasets like Visual Genome and COCO, the approach demonstrates significant improvements in metrics such as FID and LPIPS compared to standard diffusion models. This work contributes a novel framework for controlled image synthesis, enabling broader applicability of diffusion models in creative and diverse image generation tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper introduces a novel, causally motivated approach to address contextual bias in diffusion models, which effectively enhances image diversity and fidelity without requiring retraining or extensive data.\n\n2. The proposed methods are validated on multiple large-scale datasets, such as Visual Genome and COCO, demonstrating consistent performance improvements in key metrics like FID and LPIPS.\n\n3. The framework is adaptable and efficiently addresses contextual bias within the diffusion process, broadening the application scope of diffusion models for diverse and controlled image generation."
            },
            "weaknesses": {
                "value": "1. There is a lack of robustness when the sampled confounder $\ud835\udc36\u2032$ is semantically distant from the prompt $\ud835\udc4c$, leading to generated images that may ignore the confounder altogether\u200b. Besides, the framework\u2019s dependence on predefined confounders may limit its flexibility when generating images outside of commonly biased contexts, reducing adaptability in less standardized environments.\n2. The approach depends on complex causal graphs and sampling chains, which may lead to higher computational demands and slower generation times, limiting its scalability\u200b.\n3. Some generated images may exhibit unnatural object combinations, particularly when weakening contextual bias, which might detract from the realism of the results\u200b. \n4. While the framework introduces techniques to adjust contextual bias, it does not provide a quantitative evaluation of how well these adjustments meet specific user-defined objectives or bias levels."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose two causally-motivated sampling frameworks for Latent Diffusion Models, which either increase or decrease their contextual biases. \n\n1. CB+ increases the bias by using an LLM to describe confounders (objects) in a scene, and conditioning LDM sampling on these confounders\n2. CB- decreases the bias by \"retrieving\" confounders, marginalized over the distribution of unconditionally generated images. This attempts to retrieve confounders (objects) which are not explicitly co-occuring with the original scene, thereby increasing scene diversity.\n\nThe authors present results on Visual Genome and use COCO to sample confounders."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This is a relevant and important problem for the community, and I found it well motivated. I appreciate the nuance in stating contextual bias is \u201cnot inherently bad\u201d (L57-58) and providing two frameworks to tweak the bias in both directions\n2. There are many experiments examining specific aspects of the framework, e.g. its impact on realism and diversity of generated images (Tab.1, 4), adherence to the original prompt (Tab 3), qualitative results (Fig 5), and its complementary nature with other frameworks (Fig 6)\n3. The CB- framework is a very interesting contribution - if one is able to learn a \"good\" confounder distribution, it may help adding more diverse contextual biases to generative models"
            },
            "weaknesses": {
                "value": "1.\tWriting flow needs improvement. I found myself having to skip ahead to find where things were introduced or explained in the writing, and in some areas I was left with no clear answer (see below)\n2.\tWhat LDM is used for experiments (Fig 2, 4-6) ? Visually, it appears to be something similar to older versions of Stable Diffusion (e.g. 1.4, 2.1) From *L509*, it doesn\u2019t seem to be SDXL throughout. This is very unclear and greatly detracts from being able to contextualize the results (some details in Weakness 3). Please explicitly add these details for all experiments.\n3.\tIt appears that CB+ is replacing the contextual bias of the LDM with the contextual bias of the LLM (Gemini), and CB- is doing the same with a VLM (LlaVa). Assuming that the LDM is a slightly weaker model (see Weakness 2) detracts slightly from the FID comparisons Tab.1 \u2013 Gemini and LlaVA have: 1. much higher capacity (# params) 2. Much larger pretrain datasets than older Stable Diffusions, and thus their contextual biases may be of much higher quality. This makes it harder to make a fairer comparison. If all these results are with SDXL, a much stronger LDM, this is less of a concern (but this is unclear and should be specified)  \n4.\tHow the retrieved confounder $c\u2019$ is used practically in CB+ and CB- is very unclear (I understand the math from Eq. 3 and Eq. 7). Are you adding the nouns from $c\u2019$ directly to the prompt $y$ to generate a new image $x$? From *L387*, it sounds like you do not do this. My understanding of CB- (mostly from *L386-395*) is that you generate 10K images from COCO test+val captions and extract a set of nouns from all these images. You then randomly add these nouns to the prompt *y* (since you are not conditioning *y | c\u2019*). In my opinion, this is the primary weakness of this work. Please provide a step-by-step description of how $c'$ is incorporated in the image generation process in practice for both CB+ and CB-.\n5. The captions of figures need to be more self-contained. It is quite hard to understand them without referring back and forth from the text."
            },
            "questions": {
                "value": "1. I have a simpler baseline to suggest instead of the multi-step sampling chain in Eq. 6: use a VLM on the original $(x, y)$ as: \"*given this image, describe a list of common nouns that do not occur in this scene, but could be reasonably expected to co-occur and increase the diversity*\" \u2013 this will give you $c\u2019$. You can also check for errors by asking the VLM to extract nouns from the scene (which you already do, *L270*) and the CB- technique (*L237-241*) and removing them if they are extracted from the above prompt. It is unclear to me if the sampling chain would outperform this baseline, especially because while marginalizing over truly unconditional samples as in Eq 6 *will* increase scene diversity, but can give you objects that are very out of place (e.g. Fig 4, a tree in a bedroom, motorcycles in a kitchen, etc.) This is merely an alternate suggestion, but I would like to hear the authors' thoughts about why this may or may not work compared to CB-.\n2. I am a little confused by Eq 6. The starting point is marginalizing the likelihood of prompt $y$ given image $x\u2019$ over all unconditionally generated images $x\u2019$, which you do with a VLM. To compute this, you need to marginalize over *all* possible unconditionally generated images, which is intractable. A reasonable empirical approximation is to get a large collection of (hopefully diverse) unconditional images and marginalize over them, which is what I believe you are doing? **(a)** How many images $x\u2019$ do you marginalize over? Is it $10000$ as you write in Fig. 1 and is this from COCO test+val (*L387*)? Please provide these details explicitly in Sec 3.3 **(b)** Are these diverse enough to get the non-co-occurring conditionings you need to reduce contextual bias? I suggest a small comment or discussion towards this question.\n3. I would like more information on using CB+ and CB- with other conditionings (*L472-L483*), as I find this quite interesting and practically relevant for the community \u2013 a more explicit description of how alternate conditionings (e.g. ControlNet content or DEADiff style) can be used complementary to CB+ and CB- would be helpful"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the issue of contextual bias in image generation within the framework of causal inference. By leveraging this formulation, the authors propose two methods to either strengthen or weaken the influence of contextual biases during the image generation process. These methods rely on utilizing LLMs or VLMs to modify text prompts. The authors suggest that these adjustments will lead to more diverse and realistic generated results."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper provides an interesting formulation for the problem of contextual bias in image generations in the context of causal inference. This provides a novel perspective for thinking about how contextual bias influences generated images."
            },
            "weaknesses": {
                "value": "1.\tWhile this paper formulates the problem of contextual biases in image generation using causal graphs and confounders, this formulation is overcomplicated and unnecessary for addressing the problem at hand. Although the theoretical framing is interesting, the proposed method largely boils down to a refined form of prompt engineering.\n2.\tThe major concern for this paper is the novelty of the proposed method. Retrieving co-occurring objects using LLMs and identifying objects appearing in images using VLMs is trivial and has been commonly practiced in the task of text-to-image generation. The integration of LLMs and VLMs for prompt engineering is widely known and not innovative.\n3.\tDealing with contextual biases in image generation is not a particularly challenging task for modern diffusion models like Stable Diffusion, DALLE, and Flux. These models are highly capable of generating diverse and complex images based on input prompts. They can easily generate unconventional combinations like \u201castronaut riding a horse on Mars\u201d with prompt engineering along, without the need for special techniques to bypass contextual biases.\n4.\tThe experiment should include more challenging cases that truly require causal modeling to demonstrate the significance of the approach. Without such cases, the relevance of the method remains limited."
            },
            "questions": {
                "value": "Please refer to the concerns raised in the weaknesses section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examines the phenomenon where diffusion models tend to generate images with certain preferences due to biases inherent in the training dataset. By applying causal learning framework, the influence of confounding factors is either enhanced or mitigated, thus making the generated images lean more toward \"commonsense\" or \"counter-commonsense\" representations. Notably, the authors leverage another form of bias, that present in LLMs/VLMs, to sample and estimate the distribution of confounding factors. And my understanding is that the core of this paper is not the diffusion model itself but rather the use of LLMs and VLMs to distinguish between commonsense and counter-commonsense content."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This work insightfully recognizes that contextual bias is not inherently negative. Contextual bias can be a crucial component in generating natural scenes, and enhancing control over it is essential. Through causal graph modeling, combined with the robust reasoning capabilities of LLMs/VLMs, they automatically (a key point) adjust the \"amount of embedded commonsense\" in the generated image results without any explicit training.\n\nThe highlight of this work is its approach to embedding causal intervention mechanisms in generative models, enabling automated confounder estimation to implement do-operations. While the use of causal graphs in the CV field has been widely discussed, the authors' combination of these methods with generative modeling is novel and yields promising results."
            },
            "weaknesses": {
                "value": "1. Hallucination of Large Model (Especially in Vision Language Model)\n\nThe limitations section highlights the constraints of VLMs. It is important to note that VLMs often experience hallucination issues. For example, when both a horse and a donkey appear in an image, the VLM may incorrectly label both as \u201cdonkey.\u201d This is not a \u201cbeneficial\u201d bias (as the authors point out in the paper) but rather a \u201charmful\u201d hallucination issue, leading to inaccurate probability estimation when applying the Do operator. \n\nFurthermore, how can we ensure that the commonsense knowledge (referred to as bias in the paper) embedded in LLMs/VLMs aligns with that in the Diffusion Model? After all, these models are trained on different datasets and strategy. If there is inconsistency in commonsense between them, then using LLMs/VLMs to estimate P(c) may not be an ideal approach.  For instance, if we aim to generate an image of a \"bird,\" LLMs/VLMs might associate \"bird\" with \"tree,\" while the Diffusion model may associate \"bird\" with \"sky.\" In such cases, inconsistencies in bias arise. I believe expanding the discussion on this point could be beneficial to your work.\n\n2.While the intervention mechanism is detailed, control over diffusion remains overly rough.\n\nThe core contribution of this paper is not in Diffusion itself but rather in using LLM/VLM combined with causal inference to extract a text segment \"c\" from the prompt. This prompt + \"c\" serves as a new conditional input for generation, with different \"c\" segments assigned distinct weights, ultimately leading to weighted summation on the latent space or score space. \n\nHowever, this approach relies heavily on text prompt conditioning, limiting its effectiveness in handling complex cases. For instance, even with highly detailed prompts, the SD model may occasionally ignore specified objects in the prompt [1], which constrains the capability of the proposed algorithm.  A insightful work in this area can be found in [2], which delves deeper into the mechanisms of diffusion latent space, enabling the generation of counterintuitive or \"counter-commonsense\" images.\n\n[1] Chefer, Hila, et al. \u201cAttend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models.\u201d ACM Transactions on Graphics, vol. 42, no. 4, July 2023, pp. 1\u201310. Crossref, https://doi.org/10.1145/3592116.\n\n[2] Um, Soobin, and Jong Chul Ye. \"Don't Play Favorites: Minority Guidance for Diffusion Models.\" arXiv preprint arXiv:2301.12334 (2023)."
            },
            "questions": {
                "value": "1.To estimate the distribution of confounding factors, this method requires multiple sampling rounds from LLMs and VLMs; for example, generating a single image may involve at least 10 VLM calls, which is time-consuming.  In line 282, the author mentions pre-sampling methods.  Could you clarify the time required for this preprocessing, the approximate space complexity, and provide a detailed breakdown of the steps taken to reduce computation time? This part appears somewhat unclear.\n\n2.Would it be beneficial to include a deeper discussion on the connection between diffusion bias and LLM/VLM bias, as this is a central focus of your research? Exploring whether a gap exists, if it can be quantified, and supporting this with further literature could enhance the work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper does not have ethical concerns that are worrisome."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}