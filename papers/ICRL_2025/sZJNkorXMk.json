{
    "id": "sZJNkorXMk",
    "title": "Autocorrelation Matters: Understanding the Role of Initialization Schemes for State Space Models",
    "abstract": "Current methods for initializing state space model (SSM) parameters primarily rely on the HiPPO framework \\citep{gu2023how}, which is based on online function approximation with the SSM kernel basis. \nHowever, the HiPPO framework does not explicitly account for the effects of the temporal structures of input sequences on the optimization of SSMs.\nIn this paper, we take a further step to investigate the roles of SSM initialization schemes by considering the autocorrelation of input sequences. \nSpecifically, we: (1) rigorously characterize the dependency of the SSM timescale on sequence length based on sequence autocorrelation; (2) find that with a proper timescale, allowing a zero real part for the eigenvalues of the SSM state matrix mitigates the curse of memory while still maintaining stability at initialization; (3) show that the imaginary part of the eigenvalues of the SSM state matrix determines the conditioning of SSM optimization problems, and uncover an approximation-estimation tradeoff when training SSMs with a specific class of target functions.",
    "keywords": [
        "State Space Model; Optimization;"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=sZJNkorXMk",
    "pdf_link": "https://openreview.net/pdf?id=sZJNkorXMk",
    "comments": [
        {
            "summary": {
                "value": "The paper studies the initialization strategy used in SSM from various point of view, namely the time scale, the real and imaginary part of the state matrix. It demonstrates the dependency of the SSM timescale on sequence length based on sequence autocorrelation. Further it is shown that having a zero real part for the eigenvalues of the SSM state matrix mitigates the curse of memory. Finally, the paper demonstrates that the imaginary part of the eigenvalues of the SSM state matrix determine the conditioning of SSM optimization problems, and present a approximation-estimation tradeoff when training SSMs with a specific class of target functions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper has presented some very novel an interesting insights into the initialization scheme for SSM.\n2. The analysis is well grounded and thorough.\n3. The experiments are well though out, extensive and results are convincing of the claims made in the paper."
            },
            "weaknesses": {
                "value": "While the experimental results are convincing on the datasets considered in the paper, how are the expected to hold in larger scale experiments? Can authors comment on that.\n\nWhat do the authors think of extending these insights or draw parallel to setting(s) where the sequence length can be varied?"
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "An analysis of the effects of some parameters of the so-called SSM layer is provided. Specifically, the authors analyze 1) the relation between the timescale $\\Delta$ and the sequence length $L$, 2) the effect of the (zero) real part of the time evolution matrix, and 3) the effect of the imaginary part of the time evolution matrix, both theoretically and empirically."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The claims are clear, and most of them are supported both theoretically and empirically.\n- The topic is indeed important as the SSM-based models are increasingly used in various problems.\n- Although the analyses may not be surprising per se in terms of dynamical systems theory, their implication especially in using SSM architecture sounds new and useful."
            },
            "weaknesses": {
                "value": "The empirical evaluation is limited to synthetic data or somewhat simple benchmark data. This fact does not much diminish the value of the paper, but reporting the applicability of the proposed theory to more real-world datasets would certainly be appreciated.\n\nA thing remained a bit unclear to me is that how the discussions are necessarily relevant only to the initialization. Doesn't it make sense to constrain the parameter values (e.g., $\\Re(W)$) as guided in the given theory, not only at the initialization but also during training iterations? As the paper's title emphasizes the role of *initialization*, a bit more discussion in this regard might be helpful."
            },
            "questions": {
                "value": "No questions that may change my evaluation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides a theoretical analysis of a state-space model from three perspectives: discretisation size ($\\Delta$), the real components of the diagonal weights, and their imaginary counterparts. It discusses the scaling laws of $\\Delta$, highlights how zero real parts contribute to long memory, and introduces an approximation-estimation tradeoff influenced by the imaginary components."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**(I)** The analyses presented are rigorous, with clear explanations connecting the results to different initialization strategies, which is crucial for understanding SSMs.\n\n**(II)** The paper is well-structured, with illustrative examples that help to clarify the theoretical concepts."
            },
            "weaknesses": {
                "value": "**(I)** The paper primarily focuses on theoretical aspects, with limited empirical evaluation beyond basic synthetic examples. It would be beneficial to include practical applications that demonstrate the utility of the proposed methods.\n\n**(II)** The explanation of the three theoretical components sounds a bit separated. While they all fall under the umbrella of initializations, a stronger connection would give the paper a more cohesive narrative.\n\n**(III)** Although the theoretical results are clearly presented, they are dense. More emphasis on distilling key messages and offering practical guidelines would enhance the paper's accessibility."
            },
            "questions": {
                "value": "**(I)** The analysis focuses on the single-input single-output case. How would these results extend to the multiple-input multiple-output (S5) case? High-level insights would suffice.\n\n**(II)** The paper assumes ZOH discretisation. How would the results differ with bilinear discretisation?\n\n**(III)** Several questions regarding Theorem 4.1:\n\n   (a) The upper bound applies to the expected output. Is there a corresponding probabilistic bound?\n\n   (b) Can the upper bound be shown to be tight?\n\n   (c) How does the real part of $w_j$ affect this bound? Specifically, how would increasing or decreasing it impact the upper bound?\n\n   (d) The analysis focuses on the final output. How would the results change if the sum of outputs $\\sum_{\\ell = 1}^L y_\\ell^2$  were considered instead? This pooling is adopted by many SSMs.\n\n**(IV)** The paper explores various settings for how $\\Delta$ should scale with $L$. Given a specific task, what method would you use to determine the appropriate $\\Delta$?\n\n**(V)** When the real part of $w$ is set to zero at initialization, is it still trained in practice? If so, do you enforce non-positivity? In this case, how do you handle reparameterization, especially since you can\u2019t use the logarithm of it? Could you also show the behavior of the real part of $w$ after training? If it becomes negative, what is the benefit of zero initialization?\n\n**(VI)** On line 376, the assumption is made that $c$ is real-valued. However, many SSM implementations use a complex $c$. What necessitates this assumption?\n\n**(VII)** Given a specific task, how would you determine the appropriate balance between approximation and estimation tradeoff? Would this be driven by hyperparameter tuning or derived from theoretical insights?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}