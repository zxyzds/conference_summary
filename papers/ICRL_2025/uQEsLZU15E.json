{
    "id": "uQEsLZU15E",
    "title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate",
    "abstract": "We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs). Large-scale pre-training plays a critical role in building capable LVLMs, while evaluating its training quality without the costly supervised fine-tuning stage is under-explored. Loss, perplexity, and in-context evaluation results are commonly used pre-training metrics for Large Language Models (LLMs), while we observed that these metrics are less indicative when aligning a well-trained LLM with a new modality. Due to the lack of proper metrics, the research of LVLMs in the critical pre-training stage is hindered greatly, including the training data choice, efficient module design, etc.\n\nIn this paper, we propose evaluating the pre-training quality from the inter-modal distribution distance perspective and present MIR, the Modality Integration Rate, which is 1) Effective to represent the pre-training quality and show a positive relation with the benchmark performance after supervised fine-tuning. 2) Robust toward different training/evaluation data. 3) Generalize across training configurations and architecture choices.\n\nWe conducted a series of pre-training experiments to explore the effectiveness of MIR and observed satisfactory results that MIR is indicative about training data selection, training strategies schedule, and model architecture design to get better pre-training results. \nWe hope MIR could be a helpful metric for building capable LVLMs and inspire the following research about modality alignment in different areas.",
    "keywords": [
        "Large Vision-Language Models",
        "Cross-Modal Alignment"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We introduce MIR, a domain-informed metric for quantifying the cross-modal alignment of LVLM pre-training.",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=uQEsLZU15E",
    "pdf_link": "https://openreview.net/pdf?id=uQEsLZU15E",
    "comments": [
        {
            "title": {
                "value": "Response to Reviewer GxLA (1/2)"
            },
            "comment": {
                "value": "Thank you for your thoughtful reviews. We appreciate the opportunity to clarify key contributions in our paper: the Modality Integration Rate (**MIR**), a reliable metric for evaluating LVLM pre-training; the learnable Modality Calibration (**MoCa**), a novel lightweight module ot help LVLM narrowg the modality gap during training. \n\nHere we would like to first address some questions to clear up any misunderstandings, which may have arisen from our writing or formulation.\n\n**Q1: What is the necessity of proposing MIR?**\n\n**A1**: It\u2019s essential to clarify MIR\u2019s role in LVLM pre-training. As we highlighted in the Introduction, there has been **no** reliable metric for assessing pre-training quality specific to LVLMs. Traditional evaluation methods borrowed from LLMs, like perplexity, are **ineffective** for evaluating LVLM pre-training (See **Figure 1** in our paper, they can not accurately reflect the post-SFT model performance on downstream benchmarks) due to the different pre-training objective with LLMs. So the common way to evaluate the pre-training quality of LVLM is actually fixing the SFT data/setting to apply SFT on the pre-trained LVLM, then report performance across benchmarks. This approach, however, is **costly** and **time-consuming**. **Therefore, MIR addresses a critical need for a reliable, direct evaluation metric for LVLM pre-training that avoids the need for subsequent SFT.**\n\n**Q2: What is the motivation behind MIR\u2019s design?**\n\n**A2**: Our design in MIR stems from the motivation shown in **Figure 2** of our paper, where we demonstrate that current LVLMs exhibit noticeable modality gaps at shallow layers (including the text embedding space). During training, the shallow layers of the base LLM strives to bridge this gap. This perspective **contrasts** with the common view that the projector (e.g., LLaVA\u2019s MLP projector) can map vision representations directly into the text embedding space. Thus, MIR evaluation is on **two main aspects**: \n- Whether the pre-trained projector can precisely map the vision representations to the similar domain distribution with the language's; \n- Whether the base LLM has the capability to quickly narrow the modality gap between vision and language at the shallow layers. \n\nOur MIR design builds upon the two aspects, with core computation steps that include accumulating the modality gap across all layers, text-centric normalization, and outlier removal.\n\n**Q3: Does MIR simply use FID in its implementation?**\n\n**A3**: We should clearly clarify about the designs and insights about MIR. The method part of our paper may make some misunderstandings for you on the design of MIR. Indeed, MIR **does not simply use FID** but instead involves **three key steps**:\n\n- **Text-Centric Normalization**: MIR has a text-centric normalization for both vision and text tokens at the same LLM layer. It is motivated by the sensitivity of Frechet distance on the absolute value scale (e.g., calculating Frechet distance on (100x, 100y) will obtain the much larger result than calculating Frechet distance on (x, y)), which makes the results not accurate since the absolute values of token representations are gradually larger when the LLM layer goes deeper. This mechanism enables the reliable cross-layer comparison and the reasonable cross-layer accumulation in Step 3. \n- **Outlier Removal**: MIR uses the \u201c3-$\\sigma$\u201d principle to remove outliers for both vision and text tokens, recognizing that some outlier tokens contribute little semantically but can skew the distributions [1,2,3]. This step reduces outlier influence, which is crucial as Frechet distance is sensitive to such deviations.\n- **Layer-wise accumulation**: MIR calculates Frechet distances (without Inception encoding) between vision and text tokens at each LLM layer, accumulates these distances, and computes the logarithm of the result for the final MIR score. This cross-layer accumulation, although straightforward, is well-motivated by the observations discussed in **A2**.\n\nWe hope these clarifications provide better insight into the motivations and design of MIR, and we look forward to addressing any further questions. We will try to address the concerns raised in **Weakness 2\\&3** at the next part of our response. \n\n\n[1] Xiao et al. Efficient Streaming Language Models with Attention Sinks. ICLR, 2024. \n\n[2] Sun et al. Massive Activations in Large Language Models. COLM, 2024. \n\n[3] Gu et al. When Attention Sink Emerges in Language Models: An Empirical View. Arxiv preprint 2410.10781, 2024."
            }
        },
        {
            "summary": {
                "value": "This work proposes Modality Intergration Rate (MIR) to evaluate the pre-training quality of LVLMs, which measures the distance between vision and text modalities. The experiments show that MIR is not only robust but also highly related to the pre-training quality, showing a positive relation with the SFT performance. Inspired by the analysis of MIR, the authors further introduce a lightweight module MoCa in SFT to improve multi-modal understanding."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- MIR as a new metric is proposed to evaluate the pre-training quality of LVLMs. Sufficient experiments in the paper shows the effectiveness, robustness and generalisability of MIR.\n- Based on the MIR design, a lightweight module MoCa is proposed and used in SFT, improving multi-modal understanding.\n- The paper is well written, clear and easy to read."
            },
            "weaknesses": {
                "value": "- As shown in Equ. 3, MIR is calculated by summing the FID of each LLM layer. Therefore, my concern is whether MIR is still comparable for LVLMs with different backbone LLMs, especially those with different depths and sizes. It seems that MIR is less generic than loss and in-context evaluation metrics at this point."
            },
            "questions": {
                "value": "- In Equ. 3, how is the \u03c9 function based on the \"3\u03c3\" principle implemented? Have the authors examined the characteristics of the outlier tokens?\n- How does the MIR change at each layer of the model with MoCa, compared with the one without it? Is there a decrease in per-layer MIR as expected?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a metric called Modality Integration Rate (MIR), designed to evaluate the pre-training quality of Large Vision-Language Models (LVLMs) by measuring the distribution distance between visual and textual features. Unlike traditional metrics like loss or perplexity, MIR is shown to be a more effective and robust indicator of pre-training performance, particularly in aligning vision and language modalities. The paper demonstrates MIR\u2019s utility in assessing training strategies, dataset choices, and model configurations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The evaluation of modality alignment after pre-training LVLMs has not been explored before. The authors highlighted this gap and designed a relatively good metric to address this issue."
            },
            "weaknesses": {
                "value": "1. The authors merely adapted the FID metric to create the proposed MIR score, which limits the originality of the paper's contribution.\n2. FID has recently faced criticism as an evaluation metric. The authors should refer to [1] for more details. Besides, why did they decide to use FID rather than some other metrices, for example, KL-divergence, mutual information, maximum mean discrepancy and so on?\n3. The experiments were conducted only on LLaVa-v1.5. To demonstrate MIR's generalizability, the authors should have tested it on more LVLMs. Additionally, the datasets used in the study are too few to prove MIR's effectiveness and generalizability.\n4. In my view, the paper's motivation is somewhat lacking. Given the existing metrics for evaluating LVLM performance, introducing a new intermediate metric feels unnecessary.\n\n[1] Jayasumana, Sadeep, et al. \"Rethinking fid: Towards a better evaluation metric for image generation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."
            },
            "questions": {
                "value": "Please see Weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the Modality Integration Rate (MIR) as an evaluation metric for the pre-training quality of large vision-language models (LVLMs) and proposes a lightweight Modality Calibration (MoCa) module to enhance alignment between visual and textual modalities. The MIR metric demonstrates good robustness and generality, effectively guiding data scale, strategies, and model design during pre-training. However, the paper shows some limitations in terms of innovation and experimental rigor. For instance, the visualization in Figure 2 lacks persuasiveness, and the choice of weights for the mean and covariance in the MIR formula has not been thoroughly explored. Overall, MIR and MoCa provide a new approach for cross-modal alignment in LVLMs, but there is room for improvement in theoretical discussion and experimental validation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tIntroduction of an Innovative Evaluation Metric (MIR): The paper introduces the Modality Integration Rate (MIR), a novel metric to quantify cross-modal alignment quality during the pre-training stage of large vision-language models (LVLMs). MIR effectively reflects pre-training quality without relying on supervised fine-tuning, filling a gap in pre-training evaluation for LVLMs.\n2.\tWide Applicability and Robustness: MIR demonstrates robustness across different types and quantities of data inputs. It adapts well to various modal inputs (such as different types of images and texts) and remains stable in the face of overfitting and changes in data distribution. This makes MIR highly generalizable across different data and model configurations.\n3.\tPractical Optimization Guidance: The experiments showcase the application of MIR in optimizing data scale, data detail level, training strategies, and model architecture design, providing practical guidance for multi-modal pre-training. MIR helps researchers identify optimal data scale and training strategies during pre-training, thereby improving training efficiency.\n4.\tIntroduction of the MoCa Module for Enhanced Alignment: The paper proposes a lightweight Modality Calibration (MoCa) module that further improves cross-modal alignment by calibrating visual features. MoCa reduces MIR values and enhances multi-modal task performance, offering an efficient solution for cross-modal alignment in LVLMs.\n5.\tAdvantage over Traditional Metrics: Compared to traditional metrics such as loss function, perplexity, and other evaluation metrics, MIR shows stronger indicative power and stability, especially in multi-modal scenarios. The experiments demonstrate that MIR is more effective at capturing the fusion between visual and textual modalities during multi-modal pre-training."
            },
            "weaknesses": {
                "value": "1.\tThe paper\u2019s originality appears limited; it is recommended that the author consider either enhancing the contribution or submitting to a general CCFA venue.\n2.\tThe visualization in Figure 2 lacks persuasiveness. Using t-SNE for modality difference visualization appears somewhat redundant. t-SNE requires extensive parameter tuning, and the choice of random data can influence visualization outcomes, making it highly flexible. This flexibility allows almost any pattern to display differences through t-SNE, thus reducing rigor and persuasive impact.\n3.\tThe optimality of the ratios in the MIR formula has not been explored. The MIR formula uses a 1:1 weight ratio of mean distance and covariance to measure modality differences. However, it remains unverified whether this ratio is optimal. Investigating alternative ratios might allow for more effective assessment of pretraining quality. Further exploration of how different weightings affect MIR\u2019s effectiveness in various application contexts is recommended to enhance the indicator\u2019s adaptability and versatility."
            },
            "questions": {
                "value": "Refer to Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel metric, the Modality Integration Rate (MIR), to assess cross-modal alignment quality in large-scale Vision Language Models (LVLMs) during their pre-training phase. Through extensive experiments, the authors demonstrate the utility and effectiveness of MIR across a variety of pre-training configurations, thereby proving its practicality and efficacy in evaluating and optimizing LVLM pre-training processes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The introduction of MIR addresses the shortcomings of existing pre-training evaluation metrics, providing a fresh perspective on model assessment.\n2. The extensive experimental validation of MIR confirms its stability and predictive power, enhancing the credibility of the research.\n3. The study not only discusses theoretical aspects but also provides concrete application examples, offering valuable guidance for practical model training and optimization."
            },
            "weaknesses": {
                "value": "1. While the paper demonstrates the practicality of the MIR metric through experimental validation, the discussion on its theoretical basis may be lacking. The absence of a deep mathematical analysis of the metric's nature and influencing factors might hinder understanding of its theoretical support and conditions for applicability.\n2. The verification of MIR primarily utilizes standard, large-scale visual language datasets, such as ALLaVA and ShareGPT4V-PT. These datasets might share similar characteristics, limiting the indicator's demonstrated generalizability. Further verification experiments are necessary to establish broader applicability.\n3. The paper includes multiple experiments to verify MIR's effectiveness but does not provide a sensitivity analysis of MIR values under various parameter settings. Expanding the analysis to include variations in input data characteristics, model configurations, or training strategies could help evaluate its stability and reliability.\n4. Although the paper showcases the practicality and effectiveness of MIR, it lacks a direct comparison with other existing evaluation methods. To better highlight MIR's advantages, further quantitative comparative experiments with other modal fusion indicators are recommended."
            },
            "questions": {
                "value": "1. The paper employs the Fr\u00e9chet Inception Distance (FID) to calculate the distribution distance between modalities. Is there a specific rationale behind this choice? Are there alternative metrics that could also be suitable?\n2. The computational time and resource consumption of MIR are not detailed in the paper. What is the computational complexity of MIR when applied in actual large-scale model training?\n3. Has there been consideration for performing a parameter sensitivity analysis on MIR? For instance, how do variations in the number of model layers, the scale of modality pairing data, or different regularization strategies impact MIR values?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}