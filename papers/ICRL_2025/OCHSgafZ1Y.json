{
    "id": "OCHSgafZ1Y",
    "title": "Zero-shot Mixed Precision Quantization via Joint Optimization of Data Generation and Bit Allocation",
    "abstract": "Mixed-precision quantization (MPQ) aims to identify optimal bit-widths for layers to quantize a model.\nOn the other hand,\nzero-shot quantization (ZSQ) aims to learn a quantized model from a pre-trained full-precision model in a data-free manner, which is commonly done by generating a synthetic calibration set used for quantizing the full-precision model. While it is intuitive that there exists inherent correlation between the quality of the generated calibration dataset\nand the bit allocation to the model's layers, \nall existing frameworks treat them as separate problems. This paper proposes a novel method that jointly optimizes both the calibration set and the bit-width of each layer in the context of zero-shot quantization. Specifically, we first propose a novel data optimization approach that take into consideration the Gram-Gradient matrix constructed from the gradient vectors of calibration samples. We then propose a novel  scalable quadratic optimization-based approach to identify the model's bit-widths. These proposals will then be combined into a single framework to jointly optimize both the calibration data and the bit allocation to the model's layers.\nExperimental results on the ImageNet dataset demonstrate the proposed method's superiority compared to current state-of-the-art techniques in ZSQ.",
    "keywords": [
        "Zero-shot Quantization",
        "Post-training quantization",
        "mixed-precision quantization"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "A jointly optimization framework for post-training quantization that alternatively optimize the calibration data and model bit-widths",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=OCHSgafZ1Y",
    "pdf_link": "https://openreview.net/pdf?id=OCHSgafZ1Y",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a novel framework for zero-shot mixed-precision quantization (ZMPQ) that combines data generation and bit allocation optimization, focusing on improving quantization outcomes for deep learning models without access to original data. The approach introduces Gram-Gradient matrix-based data optimization and a scalable quadratic optimization for bit-width allocation, outperforming state-of-the-art methods on ImageNet benchmarks."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1.\tProposes a unique, joint optimization approach for data generation and bit allocation, filling a gap in zero-shot mixed-precision quantization research.\n2.\tDemonstrates superior or comparable performance to existing state-of-the-art methods under low-bit settings and varied model budgets, with results verified on multiple architectures (ResNet-18, ResNet-50, MobileNetV2).\n3.\tAs a complex system, the paper offers a well-rounded suite of ablation studies that demonstrate the robustness of the method"
            },
            "weaknesses": {
                "value": "I am not familiar with the work related to Mixed-precision quantization task. This paper is intuitive and reasonable on the whole. However there are some problems of clarity in the writing\n1. Certain proofs, such as the explanation of Equation (6) and the construction and application of the Gram-Gradient matrix, are highly technical, demanding considerable background knowledge from the reader. The explanation provided for Equation (6), in particular, feels insufficient for those not deeply versed in the topic.\n2. The paper would benefit from additional figures or qualitative results to aid in understanding complex concepts such as gradient matching, which currently lack sufficient illustration.\n3. Table 1 and 2, could provide more detail on the W/A notation (e.g., clarifying 2/2 and */2) to enhance interpretability."
            },
            "questions": {
                "value": "Some of the suggestions for clarity mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper for the first time proposes a mechanism that combines zero-shot quantization and mixed-precision quantization.On the basis, the paper proposes a joint optimization framework between bit-width allocation and synthetic data generation. However, during the entire optimization process, these two parts are carried out alternately and iteratively. There is still a lack of a deeply coupled mechanism for joint optimization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The inherent correlation between the quality of the generated calibration dataset and the bit allocation to the model's layers are considered. The paper is well-organized and clearly stated. I would suggest accepting it after the following concerns are addressed."
            },
            "weaknesses": {
                "value": "1. The references are all published until 2023. It would be best to provide some references on Mixed-precision Quantization/Zero-shot Quantization in 2024.\n2. In Section 3.1, the authors mentioned that \u201cIn the realm of zero-shot quantization, the\nvalidation set does not exist, so we only assume it here for explanation\u201d. But the evaluation of model performance is conducted on the ImageNet dataset. Please carefully explain the relation between these two sets. If the validation does not exist, will there be significant changes or simplifications in formulas 6 and 8?\n3. Please further explain the derivation process from formula 5 to formula 6.\n4. In table 1 & table 2, the bit widths of results obtained using methods from this paper are marked as (*). From quantization setting in section 4.1, weights of some layers are set to 8 bits for initialization. Please calculate the average bit width of these models and fill it in the table, which will be more conducive to comparing the results and demonstrating the superiority of the method.\n5. It would be better if the paper could compare the data synthesized in this paper with the data synthesized by existing methods and give deeper explanations or analyses."
            },
            "questions": {
                "value": "1. The references are all published until 2023. It would be best to provide some references on Mixed-precision Quantization/Zero-shot Quantization in 2024.\n2. In Section 3.1, the authors mentioned that \u201cIn the realm of zero-shot quantization, the\nvalidation set does not exist, so we only assume it here for explanation\u201d. But the evaluation of model performance is conducted on the ImageNet dataset. Please carefully explain the relation between these two sets. If the validation does not exist, will there be significant changes or simplifications in formulas 6 and 8?\n3. Please further explain the derivation process from formula 5 to formula 6.\n4. In table 1 & table 2, the bit widths of results obtained using methods from this paper are marked as (*). From quantization setting in section 4.1, weights of some layers are set to 8 bits for initialization. Please calculate the average bit width of these models and fill it in the table, which will be more conducive to comparing the results and demonstrating the superiority of the method.\n5. It would be better if the paper could compare the data synthesized in this paper with the data synthesized by existing methods and give deeper explanations or analyses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "They introduce a jointly optimization framework for zero-shot quantization that alternatively optimize the calibration data and model bit-widths"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors propose a mixed precision quantization method for ZSQ from the perspective of data quality. I support this work on the good topic."
            },
            "weaknesses": {
                "value": "My main concern is that network optimization from a gradient perspective may be passive given the poor quality of the available data, and the intuition is that bypassing the gradient information and using a gradient-free search method may be a better option."
            },
            "questions": {
                "value": "1. The quantizer setup. The GDFQ/Qimera/AdaDFQ series compared in this paper all use perchannel quantizer and naive optimization by STE. However, these details are not elaborated for the proposed method.\n2. The proposed method also works for conventional PTQ when the calibration data set is assumed not to fit the model statistics, which indicates that the method is not completely limited to ZSQ. Authors should provide more targeted explanations or provide experiments of the proposed method on PTQ for corroboration. Perhaps the proposed method can be used to build benchmarks on PTQ?\n3. The traditional benchmark on CIFAR data set has not been adopted, and the authors are requested to add explanations.\n4. line235: \"When that happens, we call the set X^(T) is k-equivalent to X^(V) . We can optimize this objective, by matching the Gram-Gradient matrix of the two sets, according to Definition 1 and Theorem 3.1\" ZSQ does not provide a validation set. I am trying to doubt the assumption in the article that when two samples are from synthetic datasets, their matching does not indicate the correctness of the feature?\n5. The generator configuration is not well aligned with the comparison method because Genie's generator is a newer version and this part needs further ablation.\n6. Lack of overhead analysis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}