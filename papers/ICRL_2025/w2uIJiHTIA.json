{
    "id": "w2uIJiHTIA",
    "title": "Multilayer Correlation Clustering",
    "abstract": "We establish Multilayer Correlation Clustering, a novel generalization of Correlation Clustering to the multilayer setting. In this model, we are given a series of inputs of Correlation Clustering (called layers) over the common set $V$ of $n$ elements. The goal is to find a clustering of $V$ that minimizes the $\\ell_p$-norm ($p\\geq 1$) of the disagreements vector, which is defined as the vector (with dimension equal to the number of layers), each element of which represents the disagreements of the clustering on the corresponding layer. For this generalization, we first design an $O(L\\log n)$-approximation algorithm, where $L$ is the number of layers. We then study an important special case of our problem, namely the problem with the so-called probability constraint. For this case, we first give an $(\\alpha+2)$-approximation algorithm, where $\\alpha$ is any possible approximation ratio for the single-layer counterpart. Furthermore, we design a $4$-approximation algorithm, which improves the above approximation ratio of $\\alpha+2=4.5$ for the general probability-constraint case. Computational experiments using real-world datasets support our theoretical findings and demonstrate the practical effectiveness of our proposed algorithms.",
    "keywords": [
        "Clustering",
        "Correlation Clustering",
        "Multilayer Networks",
        "Approximation Algorithms"
    ],
    "primary_area": "optimization",
    "TLDR": "We establish Multilayer Correlation Clustering, a novel generalization of Correlation Clustering to the multilayer setting, and design several approximation algorithms.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=w2uIJiHTIA",
    "pdf_link": "https://openreview.net/pdf?id=w2uIJiHTIA",
    "comments": [
        {
            "summary": {
                "value": "The paper extends the literature on the fundamental problem of Correlation Clustering. The plot twist here is that there are many correlation clustering instances that we have to solve on the same set of n vertices. The paper proceeds by formalizing the problem using the notion of multilayer-disagreements vector and then the authors give approximation algorithms for this. The goal is to find a common clustering\nof V that is consistent with as much as possible all layers. The main algorithm attains an Llogn-approximation where L is the number of layers. Moreover, they study the problem with probability constraints, where on each layer,  there are \u2018+\u2019 and \u2018\u2212\u2019 edge labels, with nonnegative weights in [0, 1] whose sum is equal to 1, hence the name probability constraints.\n\nNotice that the multilayer-disagreements vector the authors introduce has dimension equal to the number of layers L and every element of represents the disagreements of the clustering on the corresponding layer. The objective used is ell-p norm minimization on the said vector. For the case of probability constraints  the authors give an (\\alpha+2)-approximation  where we can use as a black box existing algorithms to get \\alpha approximation for the standard correlation clustering problem. In some cases, they slightly improve upon this generic (\\alpha+2)-approximation result."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+cute problem for correlation clustering where multiple instances are present. This is a nice twist in a famous problem and I am curious if this has been studied for more traditional clustering problems like k-means or other graph partitioning problems.\n\n+overall, the statements are clean for approximation and interesting.\n\n+well-motivated problem."
            },
            "weaknesses": {
                "value": "-one major concern I have is that there is limited novelty. Introducing a new problem is always interesting however in terms of techniques the paper heavily relies on prior works. The L layers in the input are handled in a relatively straighforward way and the analysis is a bit incremental, given the large bode of works for correlation clustering. I like the paper, but this is an important concern that I have."
            },
            "questions": {
                "value": "-The main premise, could it be applied to more problems? Are there related works that are directly related? This is a nice twist in a famous problem and I am curious if this has been studied for more traditional clustering problems like k-means or other graph partitioning problems."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In classical correlation clustering, we have a complete graph, where every edge is either labeled + or -. And there are non-negative weights on edges. The goal is to cluster the nodes into parts, so that the total disagreement is minimized. That is, total weight of + edges going between parts, and total weight of - edges going inside parts. \nGeneral problem  admit O(log n) approximation using region growing approach, and special case when weights are all 1, admits O(1) approximation ratio, which has been improved over many prior work.\n\nThis paper studies a new generalization. Imagine we have L such graphs with weights and labels, and wish to find a \"single\" clustering which works well for all L graphs. How to aggregate the scores? let (D_1, D_2, .., D_L) denote the L scores for a given clustering. Then we can consdier the l_p norm of this vector to be the quantitiy we are trying to optimize.\n\nPaper also studies one more \"probability\" version, where each edge has two weights w+ and w-, which add up to 1. \n\nFor the general problem, they show O(L log n) approximation factor.\nFor the probability version (where the weights add up to 1 for each layer), they show two results: one alpha+2 approximation and one 4-approximation, where alpha is the single-layer approximation factor."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Well written paper\nResults are interesting from a theoretical perspective\nPaper could spark nice follow-up work as it leaves many interesting challenges open\nIt is rare to find a theory paper run experiments of the kind this paper does, so much credit to the authors :)"
            },
            "weaknesses": {
                "value": "Not sure how suitable the paper is for ICLR audience, as it is more of an SODA/ALENEX type paper in my humble opinion. \n(Not taking anything away from the technical merits!)\n\nIn Section 5.1, Authors could do much more justice in explaining how they use Problem 2 to solve the general problem. In particular, what metric they use, what are x1, .., x_L and what is F? Are these the different solutions we get from the convex program? and metric space is the space of all solutions? Adding this details would make it more readable and interesting.  Also stress that Problem 3 is challenging only because the metric space could be huge. \n\n6.1 Are there no real-world datasets without any semi-synthetic aspect to sampling the weights? Especially sampling negative weights from the positive weights.\n\nWhy is pick the best not run for the larger datasets?\n\nAre there any other baselines one can think of? Perhaps some combination of adding the weights and pick a best? Perhaps using some approximation for l_p norm and then inferring a sampling strategy based on that? Like Multiplicative Weights method to weight the different layer instances?"
            },
            "questions": {
                "value": "Line 135: \"2.5 approximation, respectively\" -- means what? this is unclear. \nLine 138: Just to understand better, if the - weights satisfy triangle inequality, are the - label weights themselves positive values, or are the negative values?  This becomes clearer later, but was unclear at this point. \n\nLine 231: \"Note however that for Problem 1 of the unweighted case\" -- what does this mean?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a generalization of correlation clustering problem, termed as multilayer correlation clustering. In addition, polynomial time $O(L \\log(n) )$-accurate approximation algorithms are proposed to solve the generalized problem. The main idea is to relax the original problem to a convex problem."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The main contribution is to propose a polynomial time algorithm to output a solution of $O(L \\log(n))$ accuracy for the generalized correlation clustering problem."
            },
            "weaknesses": {
                "value": "1, The proposed multilayer correlation clustering problem lacks motivations. The aggregation of information from multiple weight functions $w_{l}^{+}$, $w_{l}^{-}, l=1,2, \\dots, L$ can be done through more convenient and efficient ways. For instance, one can aggregate information by aggregating the weight functions by considering $w^{+} = \\max_l w_{l}^{+}$,  $w^{-} = \\max_l w_{l}^{-}$ or  $w^{+} = \\sum_l w_{l}^{+}$, $w^{-} = \\sum_l w_{l}^{-}$ or $w^{+} = (\\sum_l (w_{l}^{+})^p)^{1/p}$, $w^{-} = (\\sum_l (w_{l}^{-})^p)^{1/p}$. The benefits of solving the multilayer correlation clustering problem are not be well-established in the paper.\n\n2, In section 6.1, since the case $p=\\infty$ is considered, it's fairer to compare with aggregated functions $w^{+} = \\max_l w_{l}^{+}$,  $w^{-} = \\max_l w_{l}^{-}$.\n\n3, Two baseline optimization methods for solving problem 1 are compared in the simulations. However, a method for comparing information gain and clustering accuracy for problem 1 is currently lacking."
            },
            "questions": {
                "value": "What additional information can be gained from multilayer correlation clustering compared to simply utilizing aggregated weight functions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies multilayer correlation clustering, which is a generalization of correlation clustering. Each layer represents a distinct correlation clustering instance on the same set of vertices. The goal is to find a consistent clustering for all layers while minimizing the p-norm of the multilayer-disagreements vector. \n\nThe main result is an $O(L\\log n)$-approximation algorithm for multilayer correlation clustering and improved algorithms for the special case of the problem with a probability constraint. The authors provide theoretical proofs and experimental evaluations to demonstrate the effectiveness of the algorithms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The definition of \"multilayer correlation clustering\" looks natural, and the motivation is clear.\n\n- Under the new model, the proposed algorithm achieves a good approximation ratio. In the case of probability constraints, the algorithm can achieve a constant approximation ratio.\n\n- The paper is easy to read. The explanation of the convex programming problem and the algorithm is clear.\n\n- The experimental results are good, obtaining near-optimal solutions in various real-world datasets."
            },
            "weaknesses": {
                "value": "- Lack of theoretical justification for the effectiveness of the results: There is no comparative analysis of the algorithm with related work (e.g., MCCC, Bonchi et al. (2015)), nor is there a lower bound provided.\n\n- The approach requires to solve CV or LP which are heavy for larger datasets. (In the experiments, only $p = \\infty$ was tested; I suspect that if other values of $p$ are used, the running time will be longer due to the need to solve CV.)"
            },
            "questions": {
                "value": "- Only arXiv version is cited for several papers \u2014 are they published in a conference? Please check.\n\n- The baselines (Pick-a-Best and Aggregate) are relatively trivial algorithms, but in the experiments, their results seem to also approach the optimal solution of the LP. Does this suggest that there may be some issues with the experimental setup?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}