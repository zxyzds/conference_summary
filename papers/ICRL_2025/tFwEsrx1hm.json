{
    "id": "tFwEsrx1hm",
    "title": "From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty",
    "abstract": "Large language models (LLMs) often exhibit undesirable behaviors, such as hallucinations and sequence repetitions.\nWe propose to view these behaviors as fallbacks that models exhibit under epistemic uncertainty, and investigate the connection between them.\nWe categorize fallback behaviors \u2014 sequence repetitions, degenerate text, and hallucinations \u2014 and extensively analyze them in models from the same family that differ by the amount of pretraining tokens, parameter count, or the inclusion of instruction-following training.\nOur experiments reveal a clear and consistent ordering of fallback behaviors, across all these axes: \nthe more advanced an LLM is (i.e., trained on more tokens, has more parameters, or instruction-tuned), \nits fallback behavior shifts from sequence repetitions, to degenerate text, and then to hallucinations.\nMoreover, the same ordering is observed during the generation of a single sequence, even for the best-performing models; as uncertainty increases, models shift from generating hallucinations to producing degenerate text and finally sequence repetitions. \nLastly, we demonstrate that while common decoding techniques, such as random sampling, alleviate unwanted behaviors like sequence repetitions, they increase harder-to-detect hallucinations.",
    "keywords": [
        "Hallucinations",
        "Epistemic uncertainty",
        "Degenerate text",
        "Fallback behaviors"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "When uncertain, LLMs exhibit fallback behaviors like degenerate text and hallucinations. These behaviors are linked and follow a strict order. Factors like pretraining, parameter count, and instruction-following training affect which fallback is used",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=tFwEsrx1hm",
    "pdf_link": "https://openreview.net/pdf?id=tFwEsrx1hm",
    "comments": [
        {
            "summary": {
                "value": "This paper looks at \u201cfallback\u201d behaviors in large language models in the face of epistemic uncertainty, specifically focusing on repetitive text, degenerate phrasing, and hallucinations. They set up scenarios to control the presence of epistemic uncertainty and characterise the outputs of various LLMs, evaluating how the number of parameters, amount of pretraining data, instruction-following training, and decoding impact the type of fallback behavior exhibited. They observe what they call a \u201cfallback hierarchy\u201d where as models become more uncertain (or if they are weaker to begin with), they rely more on basic degenerate behaviors. They posit that fallback behaviors are interconnected, based on the observation that models transition from one behavior to another as uncertainty decreases. Importantly, they find that larger, more sophisticated models also exhibit fallback behaviors in the face of uncertainty, even if they may be more sophisticated fallback behaviors and that some techniques claimed to mitigate degenerate behaviors have other adverse effects or only a limited ability to help"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The work offers a thorough analysis across various LLM families of different types of frequently-observed degenerate behavior\n* The experimental setup seems like a reasonable way to control for epistemic uncertainty\n* There are interesting empirical findings about the prevalence and characteristics of degenerate outputs from modern LLMs"
            },
            "weaknesses": {
                "value": "* The contributions of the work are limited: \n    * empirical observations are made but no insights about what the source of these behaviors are or how they can be fixed are given\n    * The analysis is limited to the presence of epistemic uncertainty, but there are other scenarios in which degenerate behaviors occur (e.g. when adversarial prompts are given or in the face of aleatoric uncertainty) so we\u2019re still far from a comprehensive empirical analysis of degenerate behaviors \n* The setups are rather contrived and since the work's results are only empirical, its not clear what the practical implications of them are\n* While the authors claim to explore the impact of decoding on these fallback behaviors, they seem to only look at greedy decoding and temperature sampling. They also seem to only explore one prompt as a \u201cmitigation\u201d technique\n* Some claims don\u2019t feel adequately supported. E.g., \u201cfallback behaviors\u2026 emerge\u2026unavoidably under uncertainty\u201d Even though a comprehensive set of prevention techniques weren\u2019t explored"
            },
            "questions": {
                "value": "I don\u2019t understand what is meant in the sentence \u201cMoreover, introducing randomness reduces the number of correct facts, which could be attributed to the random skipping of correct facts that have low confidence in the model.\u201d Could you clarify?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies how LLMs behave in the face of epistemic uncertainty, claiming that the LLMs \u201cfall back\u201d to various strategies, such as hallucinations, degenerate text, and more when this occurs. The study explores these fallback strategies across several model parameters, including model size, pretraining tokens, instruction tuning, and decoding algorithms.\n\nThe paper presents some interesting findings such as:\n\n1. Larger models tend to be more factual but are more likely to hallucinate as a fallback strategy rather than generate repetitive or poorly formatted text.\n2. As the number of pretraining tokens increases, the number of factual responses also increases, though the model may hallucinate more frequently.\n3. Instruction tuning makes the model's fallback strategies more complex without significantly changing the percentage of factual answers.\n4. Increasing the sampling temperature slightly reduces factuality but decreases repetitiveness and occasionally enhances creativity.\n\n\nThe authors conduct experiments on several different models, checkpoints, and datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper analyzes fallback mechanisms when the model encounters uncertainty\u2014an important and timely issue. The experimentation is extensive, with tests on multiple models across various datasets to validate its conclusions. The findings are interesting and hold significant value for other researchers working on LLM hallucinations. Some of the conclusions seem less surprising (although empirical verification of them is still useful)\n\nWhile the paper has notable strengths, there are some concerns (see weaknesses). I am open to raising my score once these issues have been addressed."
            },
            "weaknesses": {
                "value": "1. **Presentation Improvements**\n\n   - The paper\u2019s presentation could be refined. In its current form, it may take considerable time for readers to extract the main takeaways.\n   - There are multiple task settings, and results can sometimes be difficult to interpret due to the variety of datasets used for each claim. Standardizing these settings could improve clarity.\n   - In some cases, important results are presented only in the appendix, which can be frustrating for readers. Additionally, the graph captions could be more informative (for instance, specifying the dataset used for Figures 3 and 4).\n   - Terminology is sometimes inconsistent\u2014for example, the term \"stronger models\" is used, but it\u2019s unclear what this means.\n\n2. **Experimentation Concerns**\n\n   - Why are instruction-tuned models tested only on TriviaFacts? Although list-based questions may simplify evaluation, this limits conclusions to a single, small, self-created task (hence weakening the claim) \n   - Some parameters are analyzed independently, even though they often co-occur. For example, model size could be varied specifically within instruction-tuned models. This approach sometimes leads to inconsistencies. In Figure 21, llama-3-8B-inst is shown to be more factual (% factual) and hallucinates significantly more than llama-3-70B-inst\u2014contradicting the earlier finding that larger models tend to produce more factual answers while hallucinating more. This also appears inconsistent with broader findings, such as in the Llama-3 report, where llama-3-70B is generally noted as more accurate.\n   \n   These outliers and inconsistencies warrant further discussion."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper mainly analyzes the fallback behaviors of large language models (LLMs), which include sequence repetitions, degenerate text, and hallucinations. This paper conduct various experiments to explore the fallback behaviors in models from the same family that differ by  the amount of pretraining tokens, parameter count, or the inclusion of instruction-following training. It further discuss factors influencing the fallback behaviors of a LLM, such as sampling methods, simplifying the question and prompt engineering. The experiments reveal the following phenomenon:\n1. Increasing model strength through extra pretraining, more parameters, or instruction tuning shifts fallback behaviors from simple to complex, i.e., from repetitions to hallucinations.\n2. While LLMs have some internal capability to avoid hallucinations, this fallback behavior is inherent to their generation scheme and is likely unavoidable with current decoding methods. \n3. As models generate longer texts, they shift in their fallback behavior, first generating hallucinations and eventually producing degenerate text."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "\u2022\tThis paper provides a controlled analysis of LLM undesired behaviors, showing that strengthening models increases hallucinations over degenerate text and repetitions.\n\u2022\tThis paper demonstrates that during generation, LLMs undergo a phase shift from correct answers to hallucinations and then repetitions.\n\u2022\tThis paper indicates that methods to reduce text degeneration, like sampling, increase errors and hallucinations."
            },
            "weaknesses": {
                "value": "\u2022\tThis paper seems to overclaim its contribution and conclusion. According to the datasets this paper used, the LLMs need to recall multiple facts that are related to the same topic. The other QA circumstances are ignored, including the simple QA like Natural Questions, multi-hop question answering like HotpotQA, open-ended long-formed question answering but beyond biography like ELI5. Also many other knowledge intensive tasks are also not discussed, which is also related with the epistemic uncertainty.\n\u2022\tAccording to the paper, the fallback behaviors include sequence repetitions, degenerate text, and hallucinations. But degeneration is not discussed in the metric and experiment results.\n\u2022\tThe experiment setting of FQAMP seems not reasonable. If forcing uncertainty is essential, it is more natural to replace the subjects with a low popularity names. If replacing with made-up names and requiring LLMs to abstain, it is an OOD problem but not an uncertainty problem."
            },
            "questions": {
                "value": "\u2022\tIf this paper focuses on \u201cepistemic uncertainty\u201d and do not explore other types of uncertainty, why this paper is titles \u201cunder epistemic uncertainty\u201d.\n\u2022\tIs the TRIVIAFACTS dataset evaluated? The question mentioned in the paper, which is \u201cThe following 25 colors are in the Olympic rings.\u201d seems to mislead LLMs to repeat and hallucinate."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies different fallback behaviors of LLMs that occur under uncertainty,\nFor this, the authors construct settings where the model is expected to be uncertain.\nThey evaluate various models differing in both size and how long they are trained and compare greedy vs. sampling-based decoding methods.\nThe authors find that \"stronger\" models (trained longer, more parameters) exhibit more complex fallback behaviors which means that more hallucinations are produced."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The idea of \"epistemic uncertainty by construction\" is an interesting way of circumventing problems with quantification of uncertainty.\n\n- Overall, the paper seems methodologically sound.\n\n- The paper is very well-written and easy to follow.\n\n- The ideas presented in the paper can help guide further works on combating (unwanted) fallback behaviors of LLMs"
            },
            "weaknesses": {
                "value": "- Epistemic uncertainty as a whole is a quite rich concept that might warrant a larger discussion. In particular, the authors focus mainly on uncertainty about facts but mostly neglect other forms / sources of uncertainty that are also often seen as contributing to epistemic uncertainty. An example is L417-419: verbatim recollection is hypothesized to be more present in OOD settings than factual uncertainty but OOD settings are often also thought to give rise to epistemic uncertainty, because not enough training data is observed in the domain, leading to a lack of knowledge. For example, the authors could discuss the relation of epistemic uncertainty and aleatoric uncertainty [1] and the sources of epistemic uncertainty further [2] \n\n- The main dataset is comparably small with only 95 items but also highly curated. Similarly, the other datasets are also fairly small, usually around 100 examples\n\n- Some statements are a bit imprecise, for example: what is meant with internal capability to avoid hallucinations (L303) and what backs it up experimentally?"
            },
            "questions": {
                "value": "Below are some questions and comments:\n\n- Footnote 2 is a bit hard to understand without having read later parts of the paper\n\n- it's not immediately clear to me why Datasets 1-3 have different levels of epistemic uncertainty, could you clarify this?\n\nReferences\n\n[1] H\u00fcllermeier, Eyke, and Willem Waegeman. \"Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods.\" Machine learning 110.3 (2021): 457-506.\n\n[2] Baan, Joris, et al. \"Uncertainty in natural language generation: From theory to applications.\" arXiv preprint arXiv:2307.15703 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}