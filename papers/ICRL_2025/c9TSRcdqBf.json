{
    "id": "c9TSRcdqBf",
    "title": "Explainable Molecular Property Prediction: Aligning Chemical Concepts with Predictions via Language Models",
    "abstract": "Providing explainable molecular property predictions is critical for many scientific domains, such as drug discovery and material science. Though transformer-based language models have shown great potential in accurate molecular property prediction, they neither provide chemically meaningful explanations nor faithfully reveal the molecular structure-property relationships. In this work, we develop a framework for explainable molecular property prediction based on language models, dubbed as Lamole, which can provide chemical concepts-aligned explanations. We take a string-based molecular representation --- Group SELFIES --- as input tokens to pretrain and fine-tune our Lamole, as it provides chemically meaningful semantics. By disentangling the information flows of Lamole,  we propose combining self-attention weights and gradients for better quantification of each chemically meaningful substructure's impact on the model's output. To make the explanations more faithfully respect the structure-property relationship, we then carefully craft a marginal loss to explicitly optimize the explanations to be able to align with the chemists' annotations. We bridge the manifold hypothesis with the elaborated marginal loss to prove that the loss can align the explanations with the tangent space of the data manifold, leading to concept-aligned explanations. Experimental results over six mutagenicity datasets and one hepatotoxicity dataset demonstrate Lamole can achieve comparable classification accuracy and boost the explanation accuracy by up to 14.3%, being the state-of-the-art in explainable molecular property prediction.",
    "keywords": [
        "molecular property prediction",
        "explainability"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "A new explainable molecular property prediction model based on language models",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=c9TSRcdqBf",
    "pdf_link": "https://openreview.net/pdf?id=c9TSRcdqBf",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces Lamole, a framework designed for explainable molecular property prediction using transformer-based language models. Lamole utilizes Group SELFIES, a string-based molecular representation that encodes molecules at the functional group or fragment level, providing chemically meaningful tokens as input. The model integrates attention weights and gradients from the transformer architecture to compute importance scores for each substructure, aiming to generate explanations that align with chemical concepts. Additionally, a marginal loss function is introduced to align the model's explanations with chemists' annotations, enhancing the faithfulness of the explanations to the structure-property relationships. Experimental results on mutagenicity and hepatotoxicity datasets indicate that Lamole achieves comparable classification accuracy and improves explanation accuracy compared to baseline methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-organized and clearly written. It provides detailed explanations of the methodology, covering both theoretical foundations and practical implementations. \n\n- Methodology-wise, it introduces a marginal loss function that aligns the model's explanations with chemists' annotations. This approach aims to improve the faithfulness of the explanations by ensuring they are grounded in expert knowledge, which is valuable for applications requiring interpretability. \n\n- The paper conducts experiments on multiple datasets and compares the proposed method with several baselines, including both graph neural network (GNN)-based models and transformer-based models using SMILES representations. This comprehensive evaluation helps in assessing the performance of Lamole across different scenarios."
            },
            "weaknesses": {
                "value": "The technical novelty is limited, as Lamole is essentially a BERT model with Group SELFIES input and a new loss function. The use of attention weights combined with gradients for explanations lacks proper justification, especially given known issues with attention-based interpretability. The dependence on expert annotations severely limits practical applicability, while the marginal performance improvements don't justify the added complexity. The evaluation lacks comparisons with modern language models that could potentially generate explanations through prompting. The theoretical analysis of the marginal loss, while interesting, overshadows more practical concerns about model utility."
            },
            "questions": {
                "value": "1. Can you provide a summary of the major differences needed to be implemented in the BERT model to transform it into Lamole? Specifically, it appears that the main changes involve using Group SELFIES as input, adding a marginal loss function during training, and computing importance scores using attention weights and gradients. Are there any other architectural or substantial modifications required?\n\n2. Can you include large language model (LLM)-based models in the comparison with Lamole, e.g. Llama 3.1 or ChemLLM[1]? In particular, we can use prompting to generate explanations for the most important parts of the molecule for LLM-based models.\n\n3. How does the model scale with larger datasets and more complex molecules? Are there any computational bottlenecks introduced by the marginal loss or the computation of importance scores?\n\n[1] Zhang, Di, et al. \"Chemllm: A chemical large language model.\" arXiv preprint arXiv:2402.06852 (2024)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work primarily focuses on providing meaningful and faithful explanations for molecular property predictions through the use of language models. To ensure scientifically meaningful explanations, Group SELFIES is employed to decompose SMILES strings into functional groups, which serve as basic explanatory units to maintain chemical relevance. Transformer-based LMs which are pretrained and fine-tuned with Group SELFIES strings, are leveraged to combine attention weights with gradients, capturing interactions between functional groups. Additionally, a marginal loss function and a theoretical analysis are introduced to enhance the explanation results. The effectiveness of the proposed approach is demonstrated through experiments on seven datasets, accompanied by visualizations that illustrate the outcome of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The paper incorporates language models (LMs) to facilitate scientifically meaningful explanations, potentially promoting advancements in related fields.\n\n2) The paper proposes a novel method that combines attention weights with gradients to capture interactions between functional groups.\n\n3) The paper presents a brief theoretical analysis that bridges the manifold hypothesis with explainable molecular property prediction.\n\n4) The paper introduces a new model, Lamole, and demonstrates the effectiveness of the proposed methods through experiments on seven datasets. Ablation studies and visualizations further illustrate the performance of the proposed approach."
            },
            "weaknesses": {
                "value": "1) The novelty and contributions of this paper are limited. Although it aims to provide chemically meaningful explanations, understand functional group interactions, and faithfully reveal molecular structure-property relationships, it achieves little beyond addressing functional group interactions. Overall, the paper offers limited insights.\n\n2) The proposed method employs gradients and attention mechanisms to capture functional group interactions; however, it lacks in-depth or theoretical analysis, despite the experimental results demonstrating the approach\u2019s effectiveness.\n\n3) The paper includes an excessive amount of information without clearly establishing connections to the primary research problem. For instance, on line 265, it states, \u201cTo address this issue, \u2026\u201d and subsequently introduces a max-margin loss function. However, it remains unclear which specific issue is being addressed and how this function directly relates to the problem. A similar lack of clarity appears on line 323, where the authors mention \u201cprojecting the causal features and spurious features into the data manifold.\u201d It would be helpful to know if definitions of causal features and spurious features are provided in the paper. Furthermore, given that the paper focuses on identifying optimal subgraphs, it is unclear if these features are specifically discussed in the context of subgraphs.\n\n4) The work relies heavily on functional groups obtained through Group SELFIES, which may limit its generalizability and broader applicability. While the primary objective of an explanation model is to offer user-friendly interpretations, the necessity of incorporating explanation methods diminishes if only a limited set of subgraphs is available.\n\n5) Lastly, while the paper introduces LMs to address the problem, it provides little justification for their necessity or potential within this context."
            },
            "questions": {
                "value": "1) The presentation of ground truth explanations for the dataset is ambiguous. Line 826 states that \u201cthe ground truth for the mutagenic class is benzene with a chemical group, such as N=N, NO\u2082, and NH\u2082.\u201d However, the limitations section mentions that \u201cthe proposed Lamole still requires human label annotations as additional supervisory signals.\u201d This raises questions about how the ground truth is derived and the role of human annotations in this process.\n\n2) If the ground truth annotations are human-derived, are they based on chemical principles or domain-specific knowledge? Additionally, are these ground truth explanations employed in the comparative methods, and if so, how are they implemented?\n\n3) Providing a rationale for selecting baselines would strengthen the study. Given that the proposed method is an inherent explanation model, why are post-hoc methods chosen for comparison? Are there potentially more appropriate models available for this purpose?\n\n4)  Regarding the chemical explanations, the claim in line 826 lacks persuasive evidence. Additional support from relevant chemical literature, such as [1], would enhance this assertion. Could the authors also provide further evidence from chemical sources to substantiate their claims?\n\n5)  In Table 1, why does the GIN model outperform more complex models, such as ChemBERTa?\n\n6) Are all datasets in this study composed of 2D graphs? What criteria are used for dataset selection, and could the six mutagenicity datasets be considered a single dataset due to their similarities? It would be beneficial to examine experimental results from other datasets.\n\n7) Additional experimental results and conclusions from the ablation study would enhance the paper.\n\n8) Given the limited number of functional groups in molecules, what advantages does an advanced architecture for explanations offer over simpler approaches like brute-force or heuristic search methods? It would be informative to see performance results on larger and more complex molecules.\n\n9) In Section A.8 of the Appendix, the paper provides statistical data on training and inference times. What is the time complexity of the proposed method, and why are similar timing statistics not provided for all baseline methods?\n\n10) How does the performance of the pre-trained models affect the final explanation results? Furthermore, why is the Zinc dataset selected for pre-training?\n\n11) What advantages does explanation plausibility offer compared to explanation accuracy as an evaluation metric? Why is the title for Fig. 11 \u201cExplanation Accuracy\u201d?\n\n12) Given that the performance of the baselines is similar, how are they trained or fine-tuned to ensure a fair comparison? Are techniques such as grid search employed? It would be helpful to provide further details.\n\n[1]  Patlewicz, G., Rodford, R. and Walker, J.D., 2003. Quantitative structure\u2010activity relationships for predicting mutagenicity and carcinogenicity. Environmental Toxicology and Chemistry: An International Journal, 22(8), pp.1885-1893."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Lamole, an explainable molecular property prediction model based on language models. It employs GroupSELFIES, self-attention weights, and gradients to extract the impact of chemically meaningful substructures on the output of molecular property predictions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This work sheds light on the important but often overlooked topic of explainability in molecular property prediction.\n- The comprehensive experiments, including effective visualizations such as attention scores and explanation visualizations, enhance the overall comprehensibility of the paper."
            },
            "weaknesses": {
                "value": "- The baselines presented in Table 1 are outdated, with the most recent one dating back to 2020. While I understand that the baselines for explanation accuracy may be limited, there is room for improvement in the property prediction baselines.\n- Including a baseline with GROUPSELFIES (incorporating LSTM/Transformer) in Table 1 could strengthen the experimental results."
            },
            "questions": {
                "value": "- How is the threshold for the importance score determined? For example, in Figure 1(g), nitro-0 and benzene-1 do not appear to show a significant difference, as only nitro-0 and benzene-0 are presented as results.\n- Why is the variance of the EP score much larger than that of the baselines?\n- Do the results based on the experimental settings of GNNExplainer align with domain experts\u2019 perspectives? Specifically, does high explanation accuracy truly indicate that the model explains well? The binary edge classification seems somewhat unconventional and lacks validity for me.\n- Does the information flow-based explanation improve explainability in other domains? The approach appears to be general enough for broader applications.\n- How is the ground truth defined in Figure 4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Lamole, an explainable language-based model designed for predicting molecular properties. It uses Group SELFIES representation as input to a pre-trained language model and generates explanations using both gradients and attention weights. Additionally, it incorporates annotations from chemists to enhance the explanations provided."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The explainable molecular property prediction is crucial to drug discovery.\n2. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. My primary concern centers on the experimental section of the paper. The seven datasets employed are all relatively small, which may not fully demonstrate the scalability or generalizability of the proposed method. To address this, it would be beneficial to extend the testing to include medium-sized and large-sized datasets.\n2. The experimental section would benefit from having more advanced baseline comparisons. Given that the proposed method uses Group SELFIES representation, it is essential to conduct a comparison with other motif-based graph explanation methods. Additionally, since the proposed method is a language-based model, it would be highly beneficial to compare it with other language-based models for molecular property prediction.\n3. Explanation accuracy is insufficient for thoroughly evaluating an explanation method. Additional metrics, such as Fidelity +/-, should also be employed to assess the method's effectiveness. \n4. The reliance on annotations that require domain knowledge presents a limitation, as most datasets do not include this information. This restricts the applicability of the proposed method across various datasets that lack domain-specific annotations."
            },
            "questions": {
                "value": "Please refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}