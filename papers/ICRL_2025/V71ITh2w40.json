{
    "id": "V71ITh2w40",
    "title": "How Low Can You Go? Searching for the Intrinsic Dimensionality of Complex Networks using Metric Node Embeddings",
    "abstract": "Low-dimensional embeddings are essential for machine learning tasks involving graphs, such as node classification, link prediction, community detection, network visualization, and network compression. Although recent studies have identified exact low-dimensional embeddings, the limits of the required embedding dimensions remain unclear. We presently prove that lower dimensional embeddings are possible when using metric embeddings as opposed to vector-based inner product embeddings such as Logistic PCA (LPCA). We further provide an efficient logarithmic search procedure for identifying the exact embedding dimension and demonstrate how metric embeddings enable inference of the exact embedding dimensions of large-scale networks by exploiting that the metric properties can be used to provide linearithmic scaling. Empirically, we show that our approach extracts substantially lower dimensional representations of networks than previously reported for small-sized networks. For the first time, we demonstrate that even large-scale networks can be effectively embedded in very low-dimensional spaces, and provide examples of scalable, exact reconstruction for graphs with up to a million nodes. Our approach highlights that the intrinsic dimensionality of networks is substantially lower than previously reported and provides a computationally efficient assessment of the exact embedding dimension also of large-scale networks. The surprisingly low dimensional representations achieved demonstrate that networks in general can be losslessly represented using very low dimensional feature spaces, which can be used to guide existing network analysis tasks from community detection and node classification to structure revealing exact network visualizations.",
    "keywords": [
        "Exact network embedding",
        "latent distance model",
        "logistic PCA",
        "large scale network modeling"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "The paper develops a metric procedure for exact large-scale network embedding providing substantially lower dimensional exact network embeddings than previously reported",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=V71ITh2w40",
    "pdf_link": "https://openreview.net/pdf?id=V71ITh2w40",
    "comments": [
        {
            "summary": {
                "value": "The paper is discussing a relevant problem in network geometry, which is about searching for the intrinsic dimensionality of complex networks.\nThe Authors provide an efficient logarithmic search procedure for identifying the exact embedding dimension. Specifically, they propose an algorithm to efficiently search for an upper bound for the exact embedding dimension (D\u2217) of graphs. The Authors claim to demonstrate how metric embeddings enable inference of the exact embedding dimensions of large-scale networks by exploiting that the metric properties can\nbe used to provide linearithmic scaling."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea proposed is original and the question of the study is of significance."
            },
            "weaknesses": {
                "value": "The study does not refer to certain relevant literature in complex networks. \nThe study does not use as benchmark any complex network model that generates networks in a geometric space of a given dimensionality. \nThe results are of difficult to interpret is the study does not provide tests on artificial network models generated in a geometric space with a certain dimensionality."
            },
            "questions": {
                "value": "Can the authors offer results and tests using for benchmarks artificial networks generated in a geometric space of a given dimension?\nFor instance, see this article in case of hyperbolic geometry:\nGeneralised popularity-similarity optimisation model for growing hyperbolic networks beyond two dimensions\nB Kov\u00e1cs, SG Balogh, G Palla\nScientific Reports 12 (1), 968\n\nCan the author expand their method also to network with hyperbolic geometry?\nCan the authors comments why and how their method applies to many real networks that have intrinsic hyperbolic geometry considering that their method seems based on an Euclidean embedding? \n\nHow this study related to this other study that is missing in their related works part:\nDetecting the ultra low dimensionality of real networks\nP Almagro, M Bogu\u00f1\u00e1, M\u00c1 Serrano\nNature communications 13 (1), 6096\n\nWhen the Authors mention in the limitation section the to embedding using hyperbolic geometry, they refer to (Nickel & Kiela, 2017; 2018). \n\nIt is not clear to me why they do not refer to:\n+ the first model-based algorithms proposed in:\n1. Curvature and temperature of complex networks\nD Krioukov, F Papadopoulos, A Vahdat, M Bogun\u00e1\nPhysical Review E, 2009\n2. Sustaining the internet with hyperbolic mapping\nM Bogun\u00e1, F Papadopoulos, D Krioukov\nNature communications 1 (1), 62, 2010\n3. Network mapping by replaying hyperbolic growth\nF Papadopoulos, C Psomas, D Krioukov\nIEEE/ACM Transactions on Networking 23 (1), 198-211, 2015\n \n+ the first model-free and topological machine learning algorithm proposed in:\n1. Machine learning meets network science: dimensionality reduction for fast and efficient embedding of networks in the hyperbolic space\nJosephine Maria Thomas, Alessandro Muscoloni, Sara Ciucci, Ginestra Bianconi, Carlo Vittorio Cannistraci\nhttps://arxiv.org/abs/1602.06522, 2016\n2. Machine learning meets complex networks via coalescent embedding in the hyperbolic space\nA Muscoloni, JM Thomas, S Ciucci, G Bianconi, CV Cannistraci\nNature Communications 8 (1), 1, 2017"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the intrinsic dimensionality of complex networks using metric node embeddings and argues that metric embeddings are superior to vector-based inner product embeddings in this domain. The premise of this paper is dimension reduction, but it is not mentioned at what cost it is acheived. This paper has theoretical, methodological, and empirical contributions as the following.\n\na) Theoretical Contribution:\n- They proved that metric embeddings can provide lower dimensional representations compared to Logistic PCA.\n- They showed that metric embeddings can achieve dimensions up to two dimensions less than LPCA while maintaining perfect reconstruction.\n\nb) Methodological Contributions:\n- They developed an efficient search algorithm to find the minimal embedding dimension with O(N log N) complexity.\n- For large networks, they Proposed a scalable O(N log N) approach using KD-trees.\n- They Introduced a two-phase optimization approach combining the Hierarchical Block Distance Model (HBDM) with hinge-loss optimization\n\nc) Empirical Contributions:\n- Using their method, they found lower embedding dimensions than previously reported.\n- They showed successful embedding of large-scale networks (up to 1 million nodes).\n- They provided extensive experiments on various real-world networks\n\nOverall, this has strong theoretical and practical contributions to network embedding research. The experiments are generally sufficient, although it lacks some runtime reports. The paper's main strengths lie in its theoretical foundations and scalable approach for large networks, while its limitations are mainly related to optimization challenges and the scope of experimental validation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Strengths:\n- Strong theoretical foundation with proven bounds\n- Novel scalable approach for large networks\n- Comprehensive empirical validation\n- Clear practical implications for network analysis\n- Reproducible research with code provided\n- Well-written and structured presentation\n\nStrengths with experimentations:\n- Diverse dataset selection (10 different networks of varying sizes and types)\n- Multiple experimental validations (5 searches per dataset)\n- Comprehensive comparisons with baseline methods\n- Clear ablation studies showing the benefits of different components\n- Scalability demonstrated on large networks\n- Reproducibility ensured with detailed hyperparameter settings"
            },
            "weaknesses": {
                "value": "Weaknesses:\n- The approach may not find the absolute lowest possible embedding dimension\n- Optimization can be sensitive to initialization and hyperparameters\n- Limited exploration of alternative metric spaces (e.g., hyperbolic geometry)\n- The method might not be directly applicable for link prediction tasks\n- Some privacy concerns regarding potential surveillance applications\n- While suggested method has low computational complexity, the proposed implementation for preserving the A matrix in the memory is not efficient. In this work they resort to sampling methods for maintaining large A matrices in the memory. \n\nSome additional experiments could strengthen the paper:\n- Comparison with other metric embedding approaches (e.g., hyperbolic embeddings)\n- More extensive analysis of the impact of network properties on embedding dimension. Features such as connectivity, number of connected components, etc.\n- More detailed runtime analysis for different network sizes"
            },
            "questions": {
                "value": "The optimization is sensitive to initialization and hyperparameters. For a given set of hyperparameters, what is the probability of converging to (1) the global minimum answer, (2) a minimum answer, and (3) not converging? Using a Monte Carlo scheme, what would be average value for D for a given set of hyperparams?\n\nMinor comments and suggestions.\n- line 045: \"...preserving structure from* the discrete graph...\" -> \"preserving structure *of the discrete graph\"\n- line 068: 20.000 nodes -> 20,000 nodes. Use a comma instead of period\n- line 076: \"...we exploit* that the metric properties...\". Use of a better verb is recommended.\n- line 078: \"...exploit* how hierarchical...\". Use of a better verb is recommended.\n- line 140: \"wrt.\" -> \"w.r.t.\" Use periods.\n- line 263: $\\mathbb{R}^{2N\u00d7D_0}$ -> $\\mathbb{R}^{N\u00d72D_0}$. Since you are defining $\\mathbf{Z}=[\\mathbf{X} \\mathbf{Y}]^T$, it seems the dimensions should be $N\u00d72D_0$  rather than $2N\u00d7D_0$.\n- line 300: \"Everything in the graph is connected\"? Do you meant a \"fully connected graph\"?\n- line 303: \"Scalable inference:\" Are you missing a colon?\n- line 446: In Figure 3, not all Y-axis ranges are equal. Is there a reason? The Y-axis range for \"Rank 16\" is [0, 2.0], while for the others is [0, 1.0].\n- line 865: In Figure 5, not all Y-axis ranges are equal. In \"Rank 10\" the range is [0.3, 1.0], in \"Rank 13\" the range is [0.2, 1.0], while in the rest it is [0.0, 1.0]."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Built on a series of works by Chanpuriya et al. (see [1] and [2]]), this paper uses metric embedding to study the low intrinsic dimension of network embedding. First of all, the authors provide an improved upper bound of existing intrinsic dimensions and an efficient searching algorithm for finding such an upper bound. Experiments were conducted on real-world graphs to demonstrate by considering metric embeddings using the latent distance model. These findings could help to have a better understanding of network embeddings and potentially can be applicable to large-scale graphs.\n\n[1] Sudhanshu Chanpuriya, Cameron Musco, Konstantinos Sotiropoulos, and Charalampos E. Tsourakakis. Node Embeddings and Exact Low-Rank Representations of Complex Networks. Neural Information Processing Systems, 2020.\n\n[2] Sudhanshu Chanpuriya, Ryan Rossi, Anup B. Rao, Tung Mai, Nedim Lipka, Zhao Song, and Cameron Musco. Exact representation of sparse networks with symmetric nonnegative embeddings. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.),\nAdvances in Neural Information Processing Systems, volume 36, pp. 21023\u201321038. Curran Associates, Inc., 2023"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper studies interesting theoretical problems concerning the intrinsic dimension of network embeddings. The authors proposes to use a new loss function to quantify the upper bound of intrinis dimension.\n\n2. An efficient searching algorithm is proposed to quantify the provided upper bound. Experimental results also show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. I am not quite sure how significant such a new upper bound is compared with the existing work presented in Theorem 2.1. It seems that the authors use a new objective defined in Equ. (4) to obtain the upper bound $D_{L2}$ which could be $D_{LPCA}$, $D_{LPCA} -1 $, or $D_{LPCA}-2$. How significant is this result? Please make a short discussion on how significant difference between current work and [1,2].\n\n2. It is a concern that the efficient logarithmic search method presented is not effectively justified.  The authors claimed that they derived an efficient logarithmic search strategy that reliably identified an upper bound on the intrinsic dimensionality of exact network embeddings. I did not see the runtime complexity analysis and justification of correctness. How can we make sure the returned embeddings are exactly correct? How can the lower bound and upper bound parameters lb, ub be set?\n\n3. How practically useful of these quantified embeddings compared with DeepWalk and node2vec? Although this is not a very serious concern as the paper mainly focuses on the theoretical perspective of network embedding. How could usefulness be compared with popular node embedding algorithms such as DeepWalk, node2vec, and LINE etc. From my understanding, these real-world embeddings perform better in terms of node classification when the dimensionality increases. It would be helpful if authors can elaborate on this."
            },
            "questions": {
                "value": "See the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors investigate how many embedding dimensions are required to losslessly embed complex networks in metric spaces. As a motivation for their work, they mention the importance of low-dimensional embeddings for tasks \"such as node classification, link prediction, community detection, network visualization, and network compression\". Specifically, they ask whether lower-dimensional embeddings than previously achieved based on the so-called Logistic PCA model are feasible. They develop an efficient algorithm to compute the exact embedding dimension of large complex networks which is based on the hierarchical block distance model. Using kd-trees to efficiently search for nodes' neighbours during optimisation, they avoid $O(n^2)$ complexity, enabling computing lossless embeddings in time $O(n \\log n)$. Finally, they show empirically that their approach requires fewer dimensions to embed networks than previous approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The authors consider a relevant research question and improve upon the state-of-the-art, achieving network embeddings in lower-dimensional metric spaces than previous works.\n2. Their proposed $O(n \\log n)$ time algorithm for computing exact embeddings in is well-described and easy enough to follow.\n3. The empirical evaluation supports their theoretical claims."
            },
            "weaknesses": {
                "value": "1. The authors highlight their work's relevance by referring to several downstream tasks which are expected to benefit from lower-dimensional embeddings. However, they merely show that they can achieve lower-dimensional embeddings than previous work, but do not show how or whether the performance on downstream tasks is affected.\n2. The last paragraph of the abstract mentions that their work can be used to \"guide existing network analysis tasks\", however, they do not pick up this point later in the paper to explain _how_ this can be done.\n3. Some claims appear to be left somewhat vague (see my questions below)."
            },
            "questions": {
                "value": "There are a few things that remained unclear to me, specifically\n1. The present analysis is limited to unweighted networks and only considers the presence or absence of links. Would it be possible to extend the proposed approach to weighted networks or is there a fundamental limitation that prevents this?\n2. At the points when Fig. 1 is mentioned in the text (l.237), it was not clear to me how to interpret what is shown in the figure. I assume that the key is what is mentioned in l.285-286, is that right? I would recommend providing an explanation of how to read the figure earlier on.\n3. I am also wondering whether you have explored how your method behaves on \"difficult\" networks with less clear structure than those shown in Fig. 1? I am not sure what exactly \"difficult structure\" is, but would assume that, for example, Erd\u0151s\u2013R\u00e9nyi random graphs may be difficult to embed since they do not exhibit significant structure.\n4. Does the proposed Theorem 2.1 always hold in practice? Are there some assumptions that are made?\n5. Why is Algorithm 1 efficient? This is not really clear to me. And what does \"efficient\" mean here (i.e., what is the algorithm's time complexity)?\n6. L. 357 states that a \"relatively small number of dyads will remain misclassified\". Can you say more precisely what \"relatively small\" means?\n7. L. 368-269 states that \"The active set $\\mathcal{S}$ is then updated [...] until perfect reconstruction is achieved\". I assume here that Eqs. (6) and (7) are optimised in turn, is that correct? And if so, how do we know that the procedure will terminate?\n8. I was a bit surprised to see that the Roadnet-PA network requires 16 dimensions for lossless embedding. Given that road networks are essentially embedded in 2D in the real world, should it not be possible to embed it in fewer than 16 dimensions?\n9. The network shown in Fig. 2 seems like it should be embeddable in 1D, just as the network shown in Fig. 1a. However, it seems somewhat difficult for the optimisation to find this solution, given that the mean EED is higher than 1. What happens as one increases the number of blocks in the networks? Does the mean EED grow?\n\nMinor points\n- I believe there should be a comma instead of a period in l. 68 (20.000 nodes --> 20,000 nodes)\n- I believe it should be \"coarse\" instead of \"course\" in l. 142/143\n- I think it should be \"obtainable\" instead of \"obtain able\" in Theorem 2.1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper advances prior work in the field about finding exact representations of networks using low-dimensional vector embeddings in a latent space. It does so by proposing a few algorithms that scale the work to larger networks. First, the paper uses a distance-based model for network representation, which is slightly different from the bilinear model used in other works. The results show that the the former is at least as effective as the latter, and it can also accommodate scalable checking that the representation is exact. Second, the paper finds tighter upper bounds for the dimensionality needed for exact representation than prior work, proposing binary search and warm starts to speed up the procedure. Third, the paper explores using sampling and hierarchical models to facilitate scaling up the search for exact embeddings to large networks, and it claims to find exact representations for a 1M-node network."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper advances understanding of the capacity of dense vector representations of a network's nodes. Such representations are ubiquitous across graph machine learning, but this area is still not well understood, so advancements can be significant.\n- The discussion of motivation and presentation of results is generally clear.\n- The core advancement of using a distance-based model, KD-trees, and sampling and/or hierarchical models to scale up exact embedding search to large graphs seems logical and yields results on significantly larger graphs than prior work.\n- Finding tighter upper bounds for exact embedding dimensionality using binary search and warm starts is a smaller but still nice advancement."
            },
            "weaknesses": {
                "value": "- The adaptation of HBDM is the best instance of this paper's main advancement, which is scaling exact representation search to larger networks. Given this, HBDM is insufficiently described in the main paper. In particular, it is the reviewer's opinion that the discussion of Equation 6 should be significantly expanded with full description of the notation and more intuition about the terms of the loss.\n- On the other hand, there is a lot of description of relatively minor differences, e.g., the single dimension of difference between LPCA and the latent eigenmodel, which could be moved to the appendix.\n- There could be some more discussion of older but still relevant theory work, such as work on sign rank of a matrix (e.g., Razborov and Sherstov, 2010), since this line of work is essentially about the sign ranks of networks' adjacency matrices.\n---\nRazborov, Alexander A., and Alexander A. Sherstov. \"The sign-rank of AC ^0.\" SIAM Journal on Computing 39.5 (2010): 1833-1855."
            },
            "questions": {
                "value": "- Related to the main weakness listed above, how would you intuitively describe Equation 6 and the adaptation of HBDM to exact representation search here?\n- Is the KD-tree-based check that the representation is exact used only for the distance-based model (L2), or can is it also applied to LPCA and Eigenmodel? It is not clear to me how it can be applied to the latter two.\n- It seems that CC sampling would be overall stronger than RN sampling, but based on Figure 3 it seems to take longer to produce an exact representation. Why is this the case?\n\n### Typos\n- Line 136 \" yet the same embedding dimension for networks exhibiting heterophily.\"\n- Line 153 \"missclassified\"\n- Line 902 \"miss-classify\"\n- In equation 2, $\\mathcal{R}$ is a matrix, but in equation 4, it is a scalar."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}