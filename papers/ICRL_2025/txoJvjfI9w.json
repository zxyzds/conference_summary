{
    "id": "txoJvjfI9w",
    "title": "PEARL: Towards Permutation-Resilient LLMs",
    "abstract": "The in-context learning (ICL) ability of large language models (LLMs) enables them to undertake challenging tasks using provided demonstrations. However, it is prone to instability: different orderings of demonstrations can significantly influence predictions, revealing LLMs\u2019 limitations in processing combinatorial inputs. This paper shows that this vulnerability can be exploited to design a natural and completely imperceptible attack that achieves nearly 80% success rates on the SOTA open-source model, LLaMA, by simply permuting the demonstrations. In light of this, how to overcome the ordering sensitivity problem is an important issue for improving the performance of LLMs. However, current mitigation methods focus on post-processing and fail to enhance models\u2019 inherent robustness to the vast space of possible input permutations. To overcome this issue, we propose a novel Permutation-resilient learning framework (PEARL) based on distributional robust optimization (DRO), which optimizes model performance against the worst case among all possible permutations. Specifically, PEARL consists of a hard permutation mining network (P-Net) and the LLM. The P-Net identifies the most challenging permutations by formulating the task as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm. Through minimax optimization, the P-Net progressively generates harder samples to enhance the LLM\u2019s worst-case performance. Experiments with synthetic data and instruction\ntuning tasks demonstrate that the proposed PEARL framework effectively mitigates\npermutation attacks and improves overall performance.",
    "keywords": [
        "LLM",
        "Distributionally Robust Optimization",
        "OT"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=txoJvjfI9w",
    "pdf_link": "https://openreview.net/pdf?id=txoJvjfI9w",
    "comments": [
        {
            "summary": {
                "value": "The authors propose a method for improving the robustness of ICL to permutations in the demonstrations. The idea is to pose the problem as a min-max problem akin to distributionally robust optimization, where the LLM is trained to be robust over hard permutations generated by the adversarial distribution. The adversarial distribution is generated with another neural network and is learned jointly with the LLM, with alternating updates that are reminiscent of GANs. The authors present experiments that re-validate the existence of poor demonstration orders (using exhaustive search and search with their neural network), and demonstrate that their method, PEARL, outperforms two baselines (shuffling and mixup) on learning linear functions and instruction fine-tuning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The paper combines distributionally robust optimization using transport maps with the min-max problem of learning over worst-case perturbations, appears to be a unique approach. \n+ The attacker model for finding worst-case perturbation looks to have a favorable computational complexity. \n+ The robustness approach improves upon random and mixup-based baselines."
            },
            "weaknesses": {
                "value": "Most of this feedback is centered around how this work is situated against / compares to existing work in the literature. \n\n+ While the authors appear to be aware of some other works studying the fragility of ICL to demonstration order, I felt the paper did not situate their work relative to the existing studies. There is a non-trivial body of work on specifically studying demonstration ordering that has come out since the \"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity\" paper by Lu et al. 2021, and this body feels like it should be discussed as a section in related work. Also related is Chen et al. 2023 (same problem but perhaps more theoretical in nature). \n+ Similarly, the experiments do not compare how the approach compares to prior work in this space. For example, Chang & Jia (2022) quantify the influence of examples and their position and use this information to stabilize ICL.   Xiang et al. (2024) and Zhang et al. (2024) explicitly have methods that try to solve the permutation robustness problem for ICL. \n+ Section 3 is dedicated toward \"unveiling the vulnerability of LLMs to input permutations\". The phrasing makes it sound like this is new contribution, but perturbation ordering at this point is not a new phenomenon. \n\nChang, Ting-Yun, and Robin Jia. \"Data curation alone can stabilize in-context learning.\" arXiv preprint arXiv:2212.10378 (2022).\nChen, Yongqiang, et al. \"Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes.\" arXiv preprint arXiv:2311.18194 (2023).\nZhang, Kaiyi, et al. \"Batch-icl: Effective, efficient, and order-agnostic in-context learning.\" arXiv preprint arXiv:2401.06469 (2024).\nXiang, Yanzheng, et al. \"Addressing Order Sensitivity of In-Context Demonstration Examples in Causal Language Models.\" arXiv preprint arXiv:2402.15637 (2024)."
            },
            "questions": {
                "value": "+ Are the baselines in the paper supposed to correspond to methods explored in previous literature? I feel I've seen demonstration shuffling in other papers, and instance mixup suggests it is based on the mixup augmentation, but no citations or descriptions are given for either. \n+ How novel is the attack method? Currently, I feel the contribution of the attack is not as strongly posed as it could be---there is some discussion on other approaches being exponential in cost, but no explicit experimental comparison that touts the concrete benefits. The current section just shows that the attack works, but the fact that it works is not surprising given the previous work. This could be a more significant result if compared more concretely with respect to the appropriate prior work. \n+ Where do the authors see their work in this space, given the other methods for improving robustness to permutations and/or quantifying the impact of position? Are these comparable, or can they be discussed in any capacity? I would not personally require that these need to be added as additional rows in the experiment, as I fundamentally disagree with the \"moving goalposts\" that the field has become accustomed to. However, I do believe some amount of earnest and thoughtful discussion on how these approaches relate to each other would be fair to include, since the latest two were released in early 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a permutation-resilient learning framework (PEARL) aimed at improving the robustness of Large Language Models (LLMs) to the order in which examples are presented in in-context learning tasks. The framework is based on adversarial fine-tuning, where the adversary of the LLM tries to find a permutation of the inputs most detrimental to the LLM\u2019s performance. \n\nThe paper starts by empirically establishing that, for Llama 3 8B, there is indeed a significant variation of in-context learning performance depending on the ordering of the inputs. Then, they propose adopting a Distributionally Robust Optimization (DRO) framework for fine-tuning LLMs to mitigate this. \n\nIn their setting, DRO requires finding permutations where the LLM performs poorly. Since an exhaustive search would be intractable, the authors propose using a neural network (referred to as P-Net) to propose hard permutations. The network outputs are then passed through the Sinkhorn algorithm, which converts them into a doubly-stochastic matrix, and a differentiable sampling step (which uses Gumbel noise), yielding a permutation matrix. The authors then propose an adversarial optimization routine for jointly training the P-net and fine-tuning the LLM. The P-net is used only for training, and can be discarded at inference time.\n\nThe authors then evaluate PEARL in the established toy setting of in-context learning of linear functions, and in a real-world LLM fine-tuning setting. In both settings, they find that PEARL improves both average- and worst-case model performance, compared to existing approaches such as raw empirical risk minimization (ERM) or ERM combined with curriculum learning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Empirical results on LLM fine-tuning: the results indicating improved average- and worst-case performance in standard ICL benchmarks indicate that PEARL can be useful for practitioners as a part of their fine-tuning pipeline.\n\n1. Effectiveness in the few-shot setting: In addition, the gains above are already noticeable with a small number of shots (2, 3 or 4), indicating the method does not require a very large number of in-context examples to be beneficial (if this were the case, it could hinder the method\u2019s practical usefulness).\n\n1. Practical approach to working with permutations: the proposed method for sampling hard permutations is elegant, as it reduces an intractable search problem to learning an attention-like interaction between token embeddings of ICL examples, followed by a differentiable sampling routine involving the Sinkhorn algorithm. The adversarial optimization routine enables the P-net to be discarded at inference time, not requiring any pre- or post-processing of model inputs and outputs. Meanwhile, many adversarial robustness methods do incur additional inference-time overhead.\n\n1. Clarity and presentation: the paper is clearly written and easy to follow, despite minor points referenced below."
            },
            "weaknesses": {
                "value": "1. The authors work only with Llama 3 8B. It would be relevant to assess the model\u2019s generalizability to include other models; in particular Llama 2 7B and 13B (from the previous generation), Mistral 7B v0.2 and Gemma 7B.\n\n1. Lack of evaluations for many-shot settings: recent work in adversarial robustness has highlighted the vulnerability of LLMs to many-shot adversarial attacks (Anil et al. 2024). Hence, it would strengthen the paper to include additional evaluations where the number of shots is much larger than 5 (e.g. 32, 64 and 128 shots). In particular, the setup used by Anil et al. 2024 could enable the authors to derive direct consequences of their work when it comes to model safety. In addition, the many-shot setting is also relevant for agentic workflow and RAG applications. Hence, including such evaluations could further enhance the applicability of the findings for practitioners.\n\n1. Unclear connection with Optimal Transport: the authors use the Sinkhorn algorithm, but, as someone without a background in optimal transport, it is unclear that this alone qualifies the method as learning an optimal transport map between the uniform distribution on permutations and the \u201cdistribution of hard permutations\u201d (for which a precise definition is not given). It would be helpful if the authors could clarify this, e.g. in the appendix.\n\n1. Minor points about formatting:\n\n    - The use of the absolute value in Equation (1) confused me at first, as it seemed to suggest that the ASR metric also factors in cases where permuting the inputs increases performance. This is of course not the case, since the worst-case performance is never greater than the average case performance. Still, I believe it would improve clarity to remove the absolute value in Equation (1), since its argument is anyways always non-negative.\n\n    - In Algorithm 1, the authors refer to \u201cascending the gradient\u201d of functions labeled $L_D$ and $L_G$. I found this confusing, as the naming of the functions seems to suggest these are loss functions, in which case one would descend their gradients.\n\n    - Also, the nomenclature for the functions in 4.3 is different from that of Algorithm 1. I believe it would help clarity if these matched."
            },
            "questions": {
                "value": "1. As presented in the methods section, the matrix W does not need to be positive definite, meaning that $(h_1, h_2) \\mapsto h_1^T W h_2$ need not define an inner product in the model\u2019s embedding space. Do you have an intuition or empirical results as to whether constraining W to be positive would harm performance?\n\n1. Do you have an explanation for why the ERM+DS and ERM+IM baselines make average-case performance worse across all numbers of shots in Table 3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper address the problem that the order of demonstrations in few-shot learning influences the task performance. The authors propose an adversarial optimisation training scheme, whereby an additional model mines hard permutations while fine-tuning the original model to perform well on them."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper offers a simple, straightforward, intuitive and, most importantly, well-performing solution to the problem of permutation-sensitivity.\n2. The paper is clearly written and well-presented.\n3. It is not immediately obvious how one can handle the combinatorial explosion for the hard permutation mining with a neural network but the authors propose an elegant solution using the Sinkhorn operator and Gumbel sampling."
            },
            "weaknesses": {
                "value": "1. The evaluations seem to be restricted to the 3-, 4- and 5-shot cases. However, these settings are quite small and one could even enumerate them and try each permutation without needing the P-Net at all. Currently, models have very large contexts and can have hundreds if not thousands of demonstrators (e.g., Many-Shot In-Context Learning, Agarwal et al., 2024). It is not clear from the paper whether such larger sets of demonstrations also exhibit such permuntation-sensitivity. Furthermore, it is not clear whether the proposed approach can scale to the many-shot setting (or more than 5 examples) and what the benefit would be in that case. I think a key question that needs to be answered is whether the proposed method is better than simply adding extra demonstrators (in random order).\n2. The paper does not compare how the proposed method fares against baselines such as selecting the most performant permutation (without specially fine-tuning the model to be permutation-invariant) and selecting the demonstrators that result in maximal performance, e.g. the (Lu et al., 2022) reference from the paper."
            },
            "questions": {
                "value": "1. The separator line between 2-shot and 3-shot in Table 3 is offsetted.\n2. Do you need a separate P-Net for every number of demonstrators? Eq. (4) seems to hint at that. If that's the case, I think the authors should discuss the implications of this approach on scalability and practical implementation.\n3. If one wants demonstrator-wise permutation invariance, can\u2019t one just mask the examples in a way that no example can attend to another example, and thus making the model permutation-invariant by design?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper considers the tendency for the performance of LLMs to decrease given changes to the ordering of in-context (IC) examples. The authors propose an approach based on distributionally robust optimization (DRO), which they then run on both synthetically generated data as well as standard benchmarks. They show that performance against adversarial modifications of the ordering of the IC examples tends to increase when fine-tuning using their DRO approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper has a relatively strong set of experiments. If you agree that the threat model is worth studying, then you would certainly conclude that the method provides clear robustness against this kind of attack.\n* Aside from a few minor typos, the paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "**Threat model**\n\n* \"This paper shows that this vulnerability can be exploited to design a natural and completely imperceptible attack. . .\" -- This claim seems untrue. The attack is perceptible; the attack changes the order of the IC examples, and a keen user could very well notice this. Or, if the argument is that the user never sees the IC examples, then of course it's imperceptible. But *any* attack that is hidden from the user by construction would be imperceptible, so this doesn't seem like a distinguishing aspect of the algorithm.\n* \"Consequently, there remains a significant need for methods that can fundamentally enhance LLMs\u2019 inherent ability to manage the vast combinatorial space of possible input permutations.\" -- I'm not sure I'm convinced. Why is the need so pronounced, and in what realistic setting do you imagine that an adversary will have access to the IC examples? It seems quite unlikely that an adversary would have access to the same context window as a user. Moreover, even if the IC examples go in the system prompt, it's unclear in what settings an adversary would have this kind of access, particularly if one tends to use proprietary models, which tend not to expose their system prompts to users. Indeed, this accounts for the vast majority of use cases, so calling the need \"significant\" seems too strong. My advice: The authors should motivate their threat model and clearly explain why it corresponds to a real-world setting. Perhaps a demonstration of this attack happening in the wild, especially in the first few sections of this paper, would make sense.\n\n**Experiments.**\n* \"We select two tasks. . . We test 100 samples from each task.\" -- Is 100 samples sufficiently many? I'd be concerned that you may not get stable results (i.e., the variance would be relatively high) on datasets this small. Did you run multiple independent trials? How were these 100 examples selected?\n* It would be worth using more than one LLM, especially in the experiments described in Figure 3. The authors use Llama 3.1, which is indeed one of the strongest one-source models. It seems that one could also simulate this attack on proprietary models by putting these examples in the context window before the user's prompt. Is this the case, or have I misunderstood something?  I understand that the defense proposed later on requires fine-tuning the model, but even here, it would be worth extending the score of the analysis to include other open-source models to get a better sense for how this vulnerability impacts models outside of the Llama family.\n* The drop in performance seems to be a factor not only of $\\tau$, but also of the number of examples, since (as the authors point out), larger $n$ would tend to mean that the lower bound on performance is lower.\n* In eq. (1), how are $\\mu_i$ and $w_i$ calculated? Is this meant to be a general overview of the method, or something more specific to the experiments in Figure 1. Indeed, it may also be worth characterizing $R$ as a function of $D$ and $\\tau$, i.e., $R = R(D, \\tau)$.  \n* I didn't understand Figure 2. In the ERM problem in eq. (2), the optimization  $\\theta$ is learned, but the text indicates that both ERM and DRO learn \"probability distributions of models.\" This doesn't make sense to me, because neither (2) nor (3) deal with a distribution over models. For this reason, the plots in Figure 2 are hard to interpret; what is $\\hat{P}$ (as far as I know, this isn't defined in the text), is this different from $\\hat{p}$ (referred to in the caption), and what is $P_\\theta$? It would be worth explaining the example more as well. It's unclear why only indices 2,3, and 5 are considered here.  Moreover, the bars in both plots also look exactly the same, which I'd assume isn't the right message to take away.\n* The paper would benefit from a clearer explanation regarding how the hyperparameters are chosen.\n* It would be worth describing how the Rogue-L metric is calculated and why it's a good metric to use here.\n\n**Proposed method.**\n* It'd be worth saying more about why this instantiation of the problem is NP-Hard. Is it simply because I would need to enumerate all of the options in $\\mathcal{Q}$ (assuming that's what $\\mathcal{Q}$ is, since it's rigorously defined in the text)?\n* The [CLS] token seems to appear from nowhere. What does CLS stand for?\n* It would be worth describing more why the proposed approach corresponds to *adversarial* training. The approach seems to involve minimizing a certain loss function, rather than the typical zero-sum formulation of adversarial training which is standard in the computer vision/adversarial examples literature.\n* The problem is a tad bit underspecified. The authors should be precise about what the dimensionality of $X$ is and what $N$ is in the formulation on page 6.  It's also unclear why the authors want to add randomness (c.f., \"To introduce randomness and control...\"); why is randomness needed in this algorithm, and why, in particular, is the current method the right way to introduce it? Also, why do we need to \"control the discreteness?\" I am having a hard time understanding what this means and/or why the authors are trying to do this.\n* The authors say that their formulation allows the user to transform \"the input permutation distribution into any permutation distribution.\" Again, the paper should be more clear about whether we are dealing with distributions over perturbations or perturbations themselves. You'd imagine that there is a primal view of (3) where we deal over the objects themselves, rather than distributions over these objects. \n* In (8), it's unclear where the sinkhorn operator factors in. And how does this formulation connect to the DRO formulation in (3). From the perspective of the reader, this problem in (8) seems to appear out of thin air, and I'd imagine that readers will get stuck at this point.\n* Unless I missed something, the variable $G$ is undefined in Algorithm 1. Is that the P-Net?\n* \"To prevent the P_Net from exploiting trivial solutions--such as outputting even matrices that average out the semantic content...\" -- I'm not sure what this means. What is semantic content here? Isn't the P-Net outputting a permutation?\n* I didn't understand the steps between 9 and 10. Consider that if we plug in the losses to\n\n$$\\min_{\\theta,\\phi} L(\\theta){\\text{LM}} + L(\\phi)_{P-Net}$$\n\nthen objective becomes:\n\n$$ \\mathbb{E}[\\ell(\\theta; \\phi; (\\Pi \\cdot p, x, y)) - \\ell(\\theta, \\phi; (\\pi\\cdot p, x, y)) + \\beta L(\\theta, \\phi)_\\text{ent}] $$\n\nwhich is equal to \n\n$$ \\mathbb{E}[ \\beta L(\\theta, \\phi)_\\text{ent}] $$\n\nsince the first two terms cancel.  Could the authors clarify this, because it seems like the loss reduces to an entropy penalty.\n\n**Discussion of related work.**\n* Outside of the related work section, there is very little discussion of related work. It would be worth justifying claims using citations, e.g., \"Under appropriate assumptions, learning theory guarantees that models trained via ERM perform well on the test distribution given sufficient training data.\" Since this wasn't studied in the paper under review, it would be worth citing learning theoretic papers, which, if the reader had inclination, would enable looking into this matter more closely.\n* It would also be worth citing representative works from papers that seek to generalize to data that is outside of the training distribution. A particularly related piece of related work is the GroupDRO paper (https://openreview.net/pdf?id=ryxGuJrFvS), which uses a similar approach in classification to the approach outlined by the authors here.\n\n**Miscellaneous.**\n* \"Most existing studies on ICL primarily aim to enhance the general performance on few-shot learning.\" -- The paper would benefit from more precise language. \"General performance\" is fairly vague, and it's a bit unclear how one would distinguish this from the setting considered by the authors. Does this paper not also consider the \"general performance\" of ICL?\n* Figure 1 -- the colors are too difficult to tell apart. It would be worth considering colors outside of the blue family."
            },
            "questions": {
                "value": "* The authors consider average and worst-case orderings of IC examples. But what about best-case ordering? This also seems like a reasonable baseline. Given that performance significantly drops between the average and worst orderings, should we expect performance to increase given a best ordering?  This almost seems like a more realistic model of LLM use than the adversarial setting: A user wants to maximize their performance, and one way of doing that is by finding a good ordering of IC examples. \n* Another missing (easy) baseline is the impact of picking a *random* permutation in Figure 1. Or, is this how the average performance was calculated?\n* Line 171 -- is the upper bound \"simulated,\" or is this the actual upper bound? My understanding is that if you were to enumerate all of the permutations of the IC examples, you would be able to calculate the *true* upper bound."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}