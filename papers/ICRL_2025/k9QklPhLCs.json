{
    "id": "k9QklPhLCs",
    "title": "Subspace Node Pruning",
    "abstract": "Efficiency of neural network inference is undeniably important in a time where commercial use of AI models increases daily. Node pruning is the art of removing computational units such as neurons, filters, attention heads, or even entire layers to significantly reduce inference time while retaining network performance. In this work, we propose the projection of unit activations to an orthogonal subspace in which there is no redundant activity and within which we may prune nodes while simultaneously recovering the impact of lost units via linear least squares. We identify that, for effective node pruning, this subspace must be constructed using a triangular transformation matrix, a transformation which is equivalent to and unnormalized Gram-Schmidt orthogonalization. We furthermore show that the order in which units are orthogonalized can be optimised to maximally reduce node activations in our subspace and thereby form a more optimal ranking of nodes. Finally, we leverage these orthogonal subspaces to automatically determine layer-wise pruning ratios based upon the relative scale of node activations in our subspace, equivalent to cumulative variance. Our proposed method reaches state of the art when pruning ImageNet trained VGG-16 and rivals more complex state of the art methods when pruning ResNet-50 networks across a range of pruning ratios.",
    "keywords": [
        "Node Pruning",
        "Subspaces",
        "Efficient AI"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We describe the projection of neural activities to a subspace in which they are orthogonal but from which they can still be (node) pruned. This subspace enables: pruning only non-redundant information, sensible ranking of nodes, network-wide pruning.",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=k9QklPhLCs",
    "pdf_link": "https://openreview.net/pdf?id=k9QklPhLCs",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a post-training pruning procedure for neural networks.  This procedure proceeds layer-wise, removing nodes whose activity is well explained by activity of other nodes in that layer.  To implement this heuristic, similarity of activation patterns across a dataset is used to compute an orthogonal basis for a subspace capturing node activity.  In finding this basis, a variant of ZCA is used to first rank nodes by importance, defining an ordering for Gram-Schmidt orthogonalization.  The resulting ordering and lower-triangular transformation matrix permit explicit pruning of the least important original nodes.  A global cutoff on variance in activity patterns calibrates pruning across all layers."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The approach appears well-motivated.\n\nGlobal calibration of the pruning order of nodes across all layers is an attractive property.\n\nExperiments demonstrate competitive performance compared to alternative pruning approaches on VGG networks (VGG-11, VGG-16, VGG-19), and ResNet."
            },
            "weaknesses": {
                "value": "The standard design of VGG networks may make them artificially good candidates for pruning.  Specifically, the first fully-connected (FC) layer of VGG learns weights that take a 7x7x512 feature tensor to a 4096-dimensional vector; this involves 7*7*512*4096 = 102.76 million parameters just for that single layer.  This is an incredibly inefficient design as an entire VGG-19 network has only 144 million parameters total.  Modern CNNs do more gradual reduction of spatial size (e.g., 8x8 to 4x4 to 2x2 to 1x1 instead of 7x7 directly to 1x1) and more gradual changes in feature dimensionality in order to avoid allocating an inordinate number of parameters to a single layer.\n\nAn consequence is that VGG is probably trivial to prune, especially for a method that globally prunes the network: simply remove parameters from the unnecessarily large 102M parameter layer.  Unfortunately, this also means results on VGG are not representative of how a pruning method might perform on other, more reasonably parameterized neural networks.  Thus, Table 1 (results on VGG-16) cannot be taken as representative of performance in general.\n\nResults on ResNet-50 (Table 2) show both pruned accuracy and speedup similar to many competing methods; the proposed SNP-ZCA is not even the best in accuracy.\n\nMissing is experimental validation beyond the VGG and ResNet CNNs.  Pruning is also highly relevant for networks with different components, such as transformer layers, and across a more diverse range of tasks, such as LLMs and generative models (e.g., GANs, diffusion).  I think a far more diverse experimental regime is needed to demonstrate the viability of the proposed approach."
            },
            "questions": {
                "value": "Additional experimental results across a diverse set of neural network architectures and tasks are needed in order to validate the proposed pruning method.  Please address this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an orthogonalization-based pruning strategy. Additionally, they use importance score-based pre-ordering for the orthogonalization, which provides a simple layerwise pruning ratio. The proposed importance score-based ordering performs better than existing importance score methods. The approach is tested on vgg and resnet architectures on imagenet classification."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The GS-based orthogonalization for node pruning is neat and the importance score-based pre-ordering makes sense.\n2. Experiments show the benefit of the proposed pre-ordering strategy.\n3. The results are better than the compared method albeit marginally, with and without retraining."
            },
            "weaknesses": {
                "value": "1. The improvements are marginal and not tested exhaustively on the latest architectures or other relevant pruning strategies. \n2. Related to the above, low-rank pruning must be a baseline, given the similarity to the proposed method.\n3. The novelty is limited unless there is a significant improvement over during SVD and low-rank-based pruning. Also, it is not clear which part of the method provides the most benefit (orthogonalization or the importance score)."
            },
            "questions": {
                "value": "1. Could you clarify the benefits of the proposed method over SVD-based low-rank pruning?\n2. Could it be extended to transformers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a pruning approach for reducing the effective weights in a neural network by calculating the effective subspaces of weights in a network and using an importance scoring to ensure the correct ordering of the pruning approach. They primarily show this approach on VGG and further on ResNet."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper presents their approach in a clear fashion and layout the theory behind their approach nicely. The motivation and why this works is also clearly shown. The experiments show an improvement, and the overall story is well tied together."
            },
            "weaknesses": {
                "value": "There's no proof of generalization to general networks or different architectures like the transformers, it is also not shown the sensitivity of this approach to language vs vision. Overall the paper seemed rushed and not well structured.\n\nAlso a side note on formatting and section distribution: This seems very different from general papers submitted, and while it doesn't take away from the content, the lack of formal formatting and some inter-spread typos highlight that this was rushed."
            },
            "questions": {
                "value": "* Needs further exploration on language and larger diverse datasets"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method to prune/discard nodes (or units) from a neural network. The authors propose to project intermediate representations to a subspace such that the subspace makes the nodes or units orthogonal. Moreover, the authors provide a method to re-order nodes before orthogonalization. They also provide a global pruning saliency scheme based on the orthogonalization used. Empirical investigation is performed on the ImageNet dataset with VGGs and ResNets with and without retraining. The results indicate that the method performs reasonably well agains the selected baselines. The ZCA reordering scheme does not seem to be much useful."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1. The problem of structured pruning, with and without pruning is hard and important.\n\nS2. The empirical results on the selected baselines seem encouraging."
            },
            "weaknesses": {
                "value": "W1. [Writing] The paper is poorly written, and often imprecise. It is difficult to comprehend critical sections. Citing a few instances now\n1.  The writing flow needs to improved in section 2.\n  - What is meant by least possible impact on dynamics [line 141]?\n  - Why is it important for the subspace projections to be orthogonal?\n  - Line 159. \"..we wish for the final dot product between each pair of vectors..\" Which vectors (row vectors or column vectors of X? Isn't the orthogonalisation dependent on the samples used? How do you tackle this challenge?\n  - The justification for requiring a lower-triangular matrix seems non-coherent.\n2. Line 204 is supposed to be an importance score.\n  - Is the matrix a diagonal matrix per definition of diag in line 161?\n  - How is this different from importance in line 279? what is the relation between the two?\n3. The title of the manuscript is not quite indicative of the manuscript.\n\nW2. [Experiments] The empirical evidence is not sufficient for the following reasons.\n1. More recent baselines have been left out which are quite strong; such as [a], and [b].\n2. Experiments (especially those without retraining) do not compare against other weight re-compensation based methods.\n3. How sensitive is the method to the number of samples (s) selected? And how sensitive is the method to the samples used.\n4. It is not clear how computationally intensive is the method.\n5. Inference time improvements upon structured pruning aren't measured, which is a stated motivation for this problem.\n6. Experiments are not demonstrated on the CIFAR-10 and CIFAR-100 datasets which is a minimal expectation within the pruning literature. Moreover, effects of this pruning method on other architectures, such as densenets and mobilenets, have not been studied.\n7. Why are SNP-SAW results not reported in Table 2?\n\nW3. [Unaddressed items] Please refer to the questions below.\n\n\nSome typos:\n- Line 206. Do you mean Number of nodes to \"keep\" n?\n\n[a] Structural Pruning via Latency-Saliency Knapsack. Shen et al. NeurIPS 2022\n\n[b] DFPC: Data flow driven pruning of coupled channels without data. Narshana et al. ICLR 2023"
            },
            "questions": {
                "value": "Q1. As pointed in appendix A, if fundamentally, your weight re-compensation scheme is similar to that derived from LLS, then what is the novel insights are your bringing on the table?\n\nQ2. How do you extend the analysis of your work from feed-forward networks to CNNs? How computationally expensive is this method for CNNs?\n\nQ3. In Appendix F, it is not clear how you compute grouped/joint importances/saliencies for ResNets. Natural extension of this method can be done in many ways. But, what do you do precisely?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}