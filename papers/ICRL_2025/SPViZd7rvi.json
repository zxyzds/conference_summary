{
    "id": "SPViZd7rvi",
    "title": "Bypassing Skip-Gram Negative Sampling: Dimension Regularization as a More Efficient Alternative for Graph Embeddings",
    "abstract": "A wide range of graph embedding objectives decompose into two components: one that attracts the embeddings of nodes that are perceived as similar, and another that repels embeddings of nodes that are perceived as dissimilar. Without repulsion, the embeddings would collapse into trivial solutions. Skip-Gram Negative Sampling (SGNS) is a popular and efficient repulsion approach that prevents collapse by repelling each node from a sample of dissimilar nodes. In this work, we show that when repulsion is most needed and the embeddings approach collapse, SGNS node-wise repulsion is, in the aggregate, an approximate re-centering of the node embedding dimensions. Such dimension operations are much more scalable than node operations and yield a simpler geometric interpretation of the repulsion. Our result extends findings from self-supervised learning to the skip-gram model, establishing a connection between skip-gram node contrast and dimension regularization. We use this observation to propose a flexible algorithm augmentation framework that improves the scalability of any existing algorithm using SGNS. The framework prioritizes node attraction and replaces SGNS with dimension regularization. We instantiate this generic framework for LINE and node2vec and show that the augmented algorithms preserve downstream link-prediction performance while reducing GPU memory usage by up to $33.3$% and training time by $22.1$%. Further, for graphs that are globally sparse but locally dense, we show that removing repulsion altogether can improve performance, but, when repulsion is otherwise needed, dimension regularization provides an effective and efficient alternative to SGNS.",
    "keywords": [
        "graph embeddings",
        "negative sampling",
        "skip gram",
        "dimension regularization"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=SPViZd7rvi",
    "pdf_link": "https://openreview.net/pdf?id=SPViZd7rvi",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a regularization approach over embedding dimensions as an alternative to the conventional negative sampling strategy (i.e. noise contrastive estimation) adapted in the skip-gram-based graph representation learning methods. The authors apply the approach to the objectives of Node2Vec and LINE and evaluate the performance across multiple real-world datasets. The results demonstrate improvements in both computational efficiency, including reduced runtime and lower GPU memory requirements during optimization."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Proposes a novel framework aimed at reducing running time and GPU memory requirements.\n- Supports the proposed framework with theoretical analysis."
            },
            "weaknesses": {
                "value": "- Given the current dominance of Graph Neural Networks (GNNs) in the graph representation learning domain, the practical relevance of the proposed framework may be limited.\n- Application of the framework to Node2Vec and LINE results in significant performance drops (notably for Node2Vec), raising concerns about its practical effectiveness.\n- The theoretical analysis provided lacks detail, making some claims difficult to follow."
            },
            "questions": {
                "value": "**Comments**\n\n**Theoretical Analysis:**\n- The provided theoretical analysis lacks rigor and clarity in certain parts:\n\n- - For instance, in the proof of Proposition 2.2, it is not clear how we obtain the equation given in Line 730 from Equation 27. Although $C$ is defined as the minimum of the inner product of embedding vectors, $C$ is considered a constant, but embeddings depend on time and vary at every step, so I think $C$ also should depend on time. In Lines 731-732, the authors state that the constriction is monotonically increasing, since it remains positive. But this argument doesn\u2019t guarantee that we will obtain $C\\geq c$ for any given $c$. The series of the reciprocals of powers of 2 is monotonically increasing as well and convergent, so it can be given as a counter-example. The learning rate is also assumed to be convergent to 0 in Proposition, but it is ignored in the proof.\n\n- - In Lines 257-259, the authors state that we can approximate $N_{SG}$ with the objection $N_{SG}^{\u2019}$ for large sparse graphs, and then they provide Proposition 2.3 to validate this point. However, Equation 11 holds since the denominator term goes to infinity so it does not establish the convergence of the gradients of $N_{SG}$ and $N_{SG}^{\u2019}$\n- - The derivation from Equation 37 to Equation 35 is not clearly explained.\n- - A more detailed explanation in proofs and theoretical analysis would enhance the paper's quality.\n\n**Experimental Validation:**\n- Applying the proposed method to additional approaches like VERSE [1] could strengthen the experimental validation.\n\n*[1] Tsitsulin, Anton, et al. \"Verse: Versatile graph embeddings from similarity measures.\" Proceedings of the 2018 world wide web conference. 2018.*\n\n**Notations and Clarity:**\n- The notation in Defn. 2.1, $min_{i,j \\in n x n}$ might be replaced by $min_{i,j \\in [n] x [n]}$ to represent $[n]$ as a set of integers from $1$ to $n$.\n- In Line 357, $j$ is the index representing batch, but it is overwritten in Line 358.\n- In Equation 30, there should be a delta before $N_{SG}$.\n\n\n**Questions:**\n- Given that only positive samples are used (i.e. $II^0$), it is interesting that embeddings do not collapse and that models (especially LINE) achieve comparable performance to vanilla versions. Could this be related to the choice of a small number of epochs?\n- Since the approach assumes unweighted edges, could the authors discuss its applicability to weighted networks?\n\nI vote for a score of 3 because of my concerns regarding the originality of the proposed approach, theoretical contributions, and experimental evaluation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an alternate repulsive force for skipgram (instead of negative sampling) based on dimension regularization.  At its core, it takes results from self-supervised learning (dimension mean regularization) and applies them to skip gram (but mostly graph embedding).  The authors argue that the proposed negative loss is more efficient (and therefore more scalable) than skipgram with negative sampling.  Experiments to evaluate the proposed regularization are performed on a handful of mostly citation datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+  The proposed method's formalization seems nice, especially in terms of how it performs in the presence of dimensional collapse.\n+  The paper's writing is generally good, and the author's methods are clear."
            },
            "weaknesses": {
                "value": "-  There are some questions about novelty - in that the proposed regularization perhaps \"already exists\" in the SSL community and replacing SGNS is a seemingly obvious application.  However I'm not aware of work that actually does this (... but have not extensively looked for it).\n- The core argument of the paper is that the proposed method is more efficient than SGNS and therefore \"more scalable\".   While the efficiency of the method is definitely better, its not obvious that it's an \"online\" loss, and the true scalability of SGNS comes from the fact that the loss can be computed (and parallelized) online.  \n- There are not many (or really any) baselines used in the paper's experiments.  Since skipgram is well studied, it seems like more modifications of negative sampling or other related work might aide in understanding the proposed method better.\n- Finally, there is an apparent performance loss that comes from the proposed method.  This is present in the SBM experiments (Figure 2).  This is discussed some in the paper, but it seems like its the most important part and should be investigated more."
            },
            "questions": {
                "value": "Please see weaknesses.  In addition, here are some more thoughts/questions.\n\n- What's the primary draw of the proposed method?  If its truly efficiency, then we need to see how an online version of your method performs vs SGNS.\n\n- Using downstream tasks (especially just a handful of graphs (4 of which are citation networks)) to prove the goodness of an embedding method just doesn't cut it these days.\n\n- I would love extra experiments in the vein of Figure 2d.  Especially, how does SGNS compare to the full gradient solution (which presumably is better than SGNS, since it is itself an efficiency play).  What can we say about when each loss is better, worse, etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a theoretical reinvestigation of skip-gram models, with particular focus on their dimensional properties. The authors demonstrate that while the skip-gram negative sampling (SGNS) loss function itself does not act as a dimension regularization technique, the preservation of dissimilarity between embeddings can be effectively achieved through dimensionality reduction methods. Building on these theoretical insights, the authors propose two key modifications to existing SGNS-based algorithms: (1) a mechanism to balance the trade-off between similarity and dissimilarity preservation, and (2) a strategic approach to introducing dimensionality regularization only after detecting embedding collapse."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper has a pretty solid mathematical interpretation (section 2). The proof flow is very good  and quite makes sense. \n2. The paper is pretty well-written and easy to understand."
            },
            "weaknesses": {
                "value": "1. Limited practical impact. The proposed augmentation shows consistent performance degradation in node2vec embeddings by approximately 5% (Table 2).  The authors also fail to present potential use cases where their modifications would be beneficial for graph embedding algorithms nowadays.\n2. Dataset selection is inadequate: while the key point of proposed modifications lies in potentials in scalability, the dataset selection is Limited to small and medium-scale datasets and misses evaluation on large-scale datasets. \n3. Limited experiments: the experiments is mostly done on LINE and node2vec, which are pretty old graph embedding methods. It would be great if authors can provide some experiment results to justify how this can be potentially useful for  skip-gram based language models."
            },
            "questions": {
                "value": "1. Could you clarify the intended use cases for your proposed augmentation algorithm? The practical applications seem limited if the primary goal is scaling up LINE/node2vec. What other potential applications could benefit from this approach?\n2. Could the proposed algorithm be used to scale up skip-gram based language models? \n3. Why in table 2 and 3, the key performance metric is AUROC instead of MRR/Hits@K? Given that link prediction tasks typically rely on ranking metrics, wouldn't MRR/Hits@K be more appropriate?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}