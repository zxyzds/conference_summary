{
    "id": "5fS03oP3q6",
    "title": "C-MELT: Contrastive Enhanced Masked Auto-Encoders for ECG-Language Pre-Training",
    "abstract": "Accurate interpretation of Electrocardiogram (ECG) signals is pivotal for diagnosing cardiovascular diseases. Integrating ECG signals with their accompanying textual reports holds immense potential to enhance clinical diagnostics through the combination of physiological data and qualitative insights. However, this integration faces significant challenges due to inherent modality disparities and the scarcity of labeled data for robust cross-modal learning. To address these obstacles, we propose C-MELT, a novel framework that pre-trains ECG and text data using a contrastive masked auto-encoder architecture. C-MELT uniquely combines the strengths of generative with enhanced discriminative capabilities to achieve robust cross-modal representations. This is accomplished through masked modality modeling, specialized loss functions, and an improved negative sampling strategy tailored for cross-modal alignment. Extensive experiments on five public datasets across diverse downstream tasks demonstrate that C-MELT significantly outperforms existing methods, achieving 15% and 2% increases in linear probing and zero-shot performance over state-of-the-art models, respectively. These results highlight the effectiveness of C-MELT, underscoring its potential to advance automated clinical diagnostics through multi-modal representations.",
    "keywords": [
        "Multi-modal Representation Learning",
        "Contrastive Masked Auto-Encoders",
        "ECG-Text Pre-Training"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5fS03oP3q6",
    "pdf_link": "https://openreview.net/pdf?id=5fS03oP3q6",
    "comments": [
        {
            "summary": {
                "value": "The proposed approach aims to integrate generative pre-training with robust cross-modal representation learning. The paper extends the previous MERL[1] methodology developed for multimodal learning on ECG records and associated reports, providing zero-shot classification capabilities. The main contributions are the integration of predicting masked segments for both ECG signal and textual reports, specialized loss functions, and an improved negative sampling strategy for cross-modal alignment. The performance is evaluated for diverse datasets and demonstrates superior performance to existing SSL (Self-supervised learning) approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The strength of the model is the ability to provide improved performance compared to MERL[1], achieved through the specialized loss functions."
            },
            "weaknesses": {
                "value": "The method may not be directly comparable to SSL methodologies as the pretraining is exposed to cardiac diagnostic texts enhancing the performance for related tasks. Performance for novel features, unrelated to the context of the reports (e.g. age and sex of the subject) may be impacted adversely."
            },
            "questions": {
                "value": "Methodology:\n\nPlease clearly list the main contributions of the current work in the context of the previous literature, with an accurate formulation of the specific problem addressed.\n\nPlease explicitly state whether the primary contribution is zero-shot classification or learning generalized ECG features, and how your work advances the field.\n\nThe introduction mentions the scarcity of labeled data but integrating textual reports cannot be regarded as truly \u201cunlabelled\u201d since converting textual descriptions to labels is a trivial task, looking at the examples provided. Please clarify the definition of \"unlabeled data\" in this context and discuss how the approach differs from traditional supervised learning using text-derived labels.\n \nPlease categorize your work as self-supervised, semi-supervised, or supervised representation learning and explain why it fits that category, given the use of textual reports.\n\nDoes the implementation necessitate multi-modal data or can it also incorporate a mix of ECG-only and multi-modal data? \n\nThe methodology may also depend on the accuracy and the distribution of the textual reports. Are the reports in the pretraining dataset automatically generated or written by cardiologists? \n\nThe addition of \u201cdiagnostic data\u201d introduces the possibility of learning data bias. What has been done to ensure generalization? \n\nIt would be a good test for generalization to predict concepts not addressed in the textual reports e.g. if age and sex are not mentioned in the reports then a supervised task can be performed for these tasks, as these labels are readily available in most datasets.\n\nHow does the proposed approach differ from other hybrid approaches combining generative and contrastive methodologies e.g. [2]?\n\nIs the text encoder frozen or fine-tuned?\n\nThe method may not be directly comparable to SSL methodologies as the pretraining is exposed to cardiac diagnostic texts enhancing the performance for related tasks. Performance for novel features, unrelated to the context of the reports (e.g. age and sex of the subject) may be impacted adversely. \n\nPlease explain in further detail about where the N3S is applied as there are two cross-modal alignment losses. Is the cross-model alignment performed with single or multiple negative samples? If N3S avoids similar negative reports then the use of several positives and negatives in the batch can be incorporated in future work to avoid an expensive search for dissimilar negatives and looking explicitly for more distant reports is counter-intuitive to the goal of contrastive learning.\n\nWhat is the difference between SigLEP and ETM losses? Don't they both serve the same purpose? Clearly explain whether the alignment is performed between projections of both modes, joint representation, or the inputs and how the positive and negative pairs are defined.\n\nDoes the application also provide the possibility of automatic ECG report generation?\n\nThe comparative approaches are not exposed to labels while textual reports may include the labels or similar concepts during pre-training.\n\nMinor suggestions:\n\nPage 1 line 23: Abstract: \u201cachieving 15% and 2% increases.\u201d Please mention the specific context of the percentages mentioned.\n\nPage 2 lines (55-56): Introduction \u201cWhile some recent efforts (Liu et al., 2024; Lalam et al., 2023; Li et al., 2024)\u201d. Is the Li et al., 2024 repeated or have you missed the correct reference?\n\nPage 2 lines (73-76): \u201cAdditionally, we introduce a nearest-neighbor negative sampling \u2026 contextually relevant and challenging.\u201d How is the sampling \u201ccontextually relevant and challenging\u201d?\n\nPage 2 lines (77-85): No need to explain the test setup in the introduction. Please move to Section 4.1. \n\nPage 3:  Siglep is mentioned without explaining the acronym and the cited paper only describes \u201cSigLIP\u201d. Is it Siglep author\u2019s implementation based on \u201cSigLIP\u201d or used in previous literature? If the latter, then please reference the correct source.\n\nPage 4 line 165: Please explain the masking and reconstruction details e.g. is the masking only performed on particular leads similar to RLM in [3] and then the leads are reconstructed? Or is it also applied to segments of the same lead? \n\nPage 4 line (206-215): Is the MEM loss computed in the feature space, not the signal space? If the former then how does the reconstruction loss take into account the quality of the generated signals? There may be data leakage from the features that are input to the decoder as the network may learn trivial reconstruction. \n\nPage 6 line (273-274): \u201cThis makes the negative samples to be both challenging and distinct for effective contrastive learning\u201d. The N3S technique for finding negative samples looks specifically for the most distinct reports. How does it make it more \u201cchallenging\u201d? Also, the implementation is not very clear. Is the contrastive loss not considering the rest of the batch as negatives?\n\nPage 6 line 313: \u201cOur proposed model is developed based on the fairseq-signals framework in our work\u201d The meaning of the sentence is not clear and the reference for the fairseq-signals framework is not included. If it means that is the author\u2019s prior work then it might be a violation of the double-blind review process as the fairseq-signals framework is implemented on Git Hub.\n\nPage 8 line 429: Have you tested removing the ETM loss?\n\nPage 8 line (429-438): What is the supervised task?\n\nPage 9 line (470): Have you considered using more than 8 eight transformer layers? How does the model size vary for the different architectures?\n\nTables: The particular metrics used are not specified in the table captions throughout the paper. Please mention the labeled dataset, the training configuration (linear probe or fine-tune), and the score in all table captions.\n\nTables: Are the comparisons in Tables 1 to 4 based on the author's implementation of other approaches? If so the performance for other works properly optimized for training hyperparameters for both the pre-training and supervised training? If not then do the cited references include the particular evaluation? \n\nTable 5: What is the metric being compared and why is the student seemingly better than a cardio resident? Please verify the scores.\n\nResults: Some of the classes in the PTB-XL dataset have very few samples so for 1% and 10% random training samples there may not be sufficient positive samples in the train split. That can explain why SSL methodology loses performance significantly for labels other than superclasses. \n\nFigures: Please improve the figure captions and describe in more detail what is being shown.\n\nReferences:\n\n[1] Che Liu, Zhongwei Wan, Cheng Ouyang, Anand Shah, Wenjia Bai, and Rossella Arcucci. Zero-shot ecg classification with multimodal learning and test-time clinical knowledge enhancement. arXiv preprint arXiv:2403.06659, 2024. \n\n[2] Song, J., Jang, J. H., Lee, B. T., Hong, D., Kwon, J. M., & Jo, Y. Y. (2024). Foundation Models for Electrocardiograms. arXiv preprint arXiv:2407.07110.\n\n[3] Jungwoo Oh, Hyunseung Chung, Joon-myoung Kwon, Dong-gyun Hong, and Edward Choi. Lead-agnostic self-supervised learning for local and global representations of electrocardiogram. In Conference on Health, Inference, and Learning, pp. 338\u2013353. PMLR, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces C-MELT, a cross-modal pre-training framework for self-supervised learning of ECG signals and textual reports. C-MELT combines the strengths of generative and contrastive learning through masked language and ECG modeling to reconstruct data. It also leverages contrastive loss functions and a nearest-neighbor negative sampling strategy to improve cross-modal alignment. Extensive experiments across multiple public datasets demonstrate that C-MELT significantly outperforms existing methods, especially in downstream tasks like zero-shot learning and linear probing, showcasing strong generalization capabilities and diagnostic potential."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Overall, this paper makes clear contributions, especially the comparative interpretation with MERL, which further promotes the development of multimodal ECG research.\n- N3S provides an effective negative sample selection strategy to effectively select negative text reports in medical datasets."
            },
            "weaknesses": {
                "value": "- The ablation experiment does not discuss the role of different components in the reconstruction method. The contrast method is currently the main method for multimodal alignment in ECG multimodal researches. This paper proposes a hybrid method to enhance the performance, but does not discuss the ablation experiment of the added reconstruction method in more detail, which leads to doubts about the effectiveness of the added reconstruction method.\n- The layout of Table 2 is somewhat inadequate, as it differs significantly from the original table in MERL, leading to some confusion when reading. Given that this table may serve as a benchmark in the future, it should be adjusted for consistency to facilitate easier comparison."
            },
            "questions": {
                "value": "- Please provide more details on the performance improvement achieved by the reconstruction approach.\n- In Section 3.1, positional encoding is added at the end. How is sequence information ensured during the calculation of multi-head attention according to the content of the article?\n- If ECG-Text Matching (ETM) promotes the alignment of corresponding ECG-Text pairs, why does it not enhance the discriminative ability of the encoders?\n- Conversely, if Siglep Loss can accomplish contrastive learning for multimodal alignment, what is the purpose of retaining ETM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The present manuscript proposes multiple interesting ideas for enhancing ECG-language pre-training by combining contrastive learning and reconstruction.  By doing so, considerable improvements over unimodal pretraining and a recent multimodal approach are shown on multiple downstream datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Strengths of the manuscript include the introduction of multiple novel ideas, in particular the combination of learning via contrastive and generative generation. Furthermore, strong results are obtained across datasets. The authors clearly explain the various parts of the approach."
            },
            "weaknesses": {
                "value": "The manuscript could benefit from a clearer rationale behind the choice of methods, as theoretical justifications are somewhat limited. While several ideas are presented, a more thorough exploration of the reasoning behind each would strengthen the approach. Additionally, although some ablation analyses have been performed, it would be helpful to clarify which components (e.g., reconstruction vs. no reconstruction) play an essential role in the downstream tasks."
            },
            "questions": {
                "value": "For the experiments:\n-\tWhereas the proposition to combine reconstruction and contrastive learning is very worthwhile, to my understanding this matter is not directly addressed in the paper. While the Siglep loss is ablated, I believe in this case the model still is not trained solely via reconstruction, as ECG-Text Matching (ETM) is active. I believe the manuscript would be considerably improved if results could be shown under a setting of \u2018reconstruction only\u2019 and \u2018without reconstruction\u2019. Ideally, under the \u2018reconstruction only\u2019 setting, additional information is provided about the utility of text or ECG reconstruction. In the \u2018without reconstruction\u2019 setting, it would be interesting to ablate either the Siglep loss and/or ETM. The authors are free to provide alternative analyses than specifically those considered here, but again, gaining an understanding of the contribution of the individual parts of the approach would be important.\n-\tWhy did the authors choose not to compare against other methods for ECG-text pretraining such as the cited Lalam et al. 2023 or Liu et al. 2023 (https://arxiv.org/pdf/2309.07145)?\n\nIn the text, the following sections could improve with clearer justifications of the authors work:\n- Lines 60-68: Here the authors propose to combine contrastive and generative approaches to leverage their complementary strengths, but it is unclear what these complementary strengths are expected to be. As no clear argumentation is provided why these approaches should be combined, there is no framework in place to later interpret the results. I would therefore recommend to provide a more precise description of the working hypothesis.\n-\tLines 240-243: \u201cThis can hinder the model\u2019s capability to learn discriminative features (\u2026)\u201d. What is being referred to here? The fact that generative approaches perform reconstruction and contrastive learning distinguishes between data pairs is, to my understanding, precisely what causes them to learn discriminative features to the extent that they do. \nNext, it is suspected that ETM could serve as a contrastive loss but that it may be insufficient as it performs binary classification with fused features. As these characteristics of ETM were introduced with little justification, one is left wondering why the Siglep loss and ETM are both needed. Specifically, the hypothesis that alignment of both unfused and fused features is beneficial is currently not answered in the paper, as ablations do not seem to include ETM.\n-\tLines 274-275: I would like to ask why the authors claim nearest-neighbour negative sampling makes negative samples challenging? Does the approach not make negative samples easier by selecting less similar reports as negatives? (And as reports with high cosine distance are selected, is this not opposite to the principle of \u2018nearest-neighbours\u2019?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes a multimodal ECG learning framework with multiple objective alignments and evaluates it across various downstream tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Many empirical studies encompassing a wide range of datasets and methods."
            },
            "weaknesses": {
                "value": "- Lack of novelty: MEM, ETM, and MLM losses are very similar to the BLIP [1] work, which was proposed in 2022. This work directly reimplements the BLIP loss in the ECG domain, which is not novel enough.\n- Ambiguous results: In Table 3, C-MELT claims that the zero-shot classification performance across all downstream datasets is **77.71**, which is higher than MERL [2]. However, in the ablation results Table 7, the average zero-shot result is **72.5\u00b19.1**, which *is not consistent* with the author-reported performance. Additionally, in MERL [2]'s original paper, the reported average zero-shot performance is **75.24\u00b11.7** in Tables 5-9, which is much higher than C-MELT's performance.\n- The reproducibility concern is made worse by the fact the authors don't appear willing to share their code.\n\n[1] Li, Junnan, et al. \"Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.\" International conference on machine learning. PMLR, 2022.\n[2] Liu, Che, et al. \"Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement.\" Forty-first International Conference on Machine Learning."
            },
            "questions": {
                "value": "- Can the authors clarify the main differences between C-MELT and BLIP? Even though the data is different (ECG vs. Image), the framework and optimization objectives appear too similar.\n- How do the authors explain the discrepancy between the reported results in Table 3 and Table 7?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}