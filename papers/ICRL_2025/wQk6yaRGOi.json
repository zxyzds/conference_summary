{
    "id": "wQk6yaRGOi",
    "title": "Improving Discrete Diffusion with Schedule-Conditioning",
    "abstract": "In research on discrete diffusion generative models, one long-standing mystery is the dominance of the masking state corruption process.\nIn masking diffusion, all data points collapse to a sequence of mask tokens without any transitions between non-mask tokens, ruling out small edits from one unmasked token to another. By contrast, in image modeling, the dominant corruption process is Gaussian noise, which encourages gradual movements in pixel space. In this paper, we propose that masking diffusion dominates due to knowledge of when corruptions occurred. When it makes predictions, it does so conditional on the schedule of previous corruptions; this allows it to devote less capacity to inferring whether a corruption has occurred and more capacity to modeling relationships between tokens. \nWe use this insight to build knowledge of corruptions into other discrete diffusion models; we call our method schedule-conditioned diffusion (SCUD). We show that SCUD generalizes classical discrete diffusion and masking diffusion.\nWe show that applying SCUD to models with different corruption processes leads to improved perplexities on images, text, and protein sequences; Finally, by applying SCUD to models with corruption processes with ``gradual'' structure, we build diffusion models that outperform masking.",
    "keywords": [
        "discrete diffusion",
        "image generation",
        "language model"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wQk6yaRGOi",
    "pdf_link": "https://openreview.net/pdf?id=wQk6yaRGOi",
    "comments": [
        {
            "summary": {
                "value": "The choice of forward process has great performance implications for (discrete) diffusion models. This paper theoretically studies the improved performance of the masking forward process in discrete diffusion models. To this end, the authors introduce the notion of an event schedule, which describe times along the forward process where transitions take place, and separate the \"when\" of transition from the \"where\" of transition. \n\nThe authors further derive the training objectives corresponding to conditioning on these event schedules, and apply their proposed method SCUD, to the tasks of image generation, language generation and protein sequence generation. Across the different tasks, their proposed method is able to outperform equivalent forward processes, but without the conditioning using less training examples."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The main strength of the paper is the firm theoretical footing to understand different forward processes in discrete diffusion, and how they relate to the \"when\" and \"where\" of the corresponding transitions. Through the notion of event / schedule conditioning, the paper attempts to disentangle the influence of \"when\" and \"where\", which leads to corresponding modifications to the ELBO objective. The authors also emphasize the connections of their method to previous discrete diffusion methods. \n\n2. The paper is very well written, and makes a strong effort to coherently explain the different moving parts. \n\n3. To connect the method to practice, the authors also propose different tricks such as reversing multiple events jointly, and an efficient loss formulation for high-dimensional data. Experiments across image, protein and language domain show favorable improvements for the same forward process, conditioned on event schedule. The experiments on proteins are especially interesting, using 2 orders less of training data."
            },
            "weaknesses": {
                "value": "There are no glaring weaknesses in the paper. But there is some minor feedback:\n\n1. The formal notion of event schedule is only introduced in Proposition 4.2. This should instead be moved to before Proposition 3.1, so the readers already know what the event schedule captures, and the corresponding equations become easier to follow.\n\n2. There are claims in the paper regarding SCUD outperforming masking, but evidence of this is not visible in the experiments. It is unclear whether the SCUD conditioning with Gaussian / Uniform will outperforming existing discrete diffusion (e.g. D3PM) with masking, given equal compute."
            },
            "questions": {
                "value": "1. In light of the point 2 in weaknesses, could the authors run the D3PM model with masking diffusion with the same number of training samples as SCUD? -- The reviewer hopes this is feasible within the available compute budget of the authors. An alternative would be to also consider small versions of D3PM and SCUD that can be trained for equivalent number of samples.\n\n2. How is B defined in the language and protein experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents Schedule Conditioned Diffusion (SCUD), a new method to enhance discrete diffusion models for conditional sequence generation. The authors identify that existing structured and state-dependent approaches are often outperformed by the simpler \"masking diffusion,\" due to its effective alignment of forward and backward transition schedules. They introduce a decomposition of the Evidence Lower Bound (ELBO) that addresses this mismatch and demonstrate efficient training strategies for SCUD. The findings indicate that SCUD achieves similar performance to masking diffusion, with the authors releasing their code for further exploration."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well written, equations are nicely embedded and overall presentation is good (marking will be raised to good once the page limit is achieved).\n\n- The paper effectively addresses limitations of existing discrete diffusion models, specifically the dominance of masking diffusion over more gradual and structured processes. It provides a solid theoretical foundation to argue for the introduction of SCUD.\n\n- Introducing the SCUD model, which conditions on the transition schedule, is a novel approach. This model adapts diffusion frameworks by incorporating structured forward processes, potentially expanding discrete diffusion's applications across data types.\n\n- The paper includes a rigorous theoretical framework, with proofs and mathematical propositions that justify the SCUD approach.\n\n- The experiments span various data types, including images, text, and protein sequences. Results on CIFAR-10, LM1B, and UniRef50 datasets convincingly show that SCUD improves performance compared to other non-masking processes.\n\n- The paper compares SCUD with state-of-the-art discrete diffusion models, showing how SCUD better captures transition schedules and can leverage structured processes."
            },
            "weaknesses": {
                "value": "# Presentation (Minor)\n\nI will re-adjust my mark for presentation from poor to good once the following urgent concern has been addressed by the authors:\n\n- The text of the main paper exceeds the 10-page limit. Please move your remarks regarding Reproducibility to the appendix, ICLR will likely be strict about enforcing the 10-page limit, exceeding the limit may lead to your work being rejected down the line.\n\n\n# Content (Major)\n\n- The paper's complexity could limit practical adoption. SCUD requires intricate parameterizations and careful handling of components like the rate parameter $\\gamma$ and the transition matrix $K$ in particular, potentially increasing computational cost.\n\n-  While SCUD reduces training samples, the discussion on costs associated with complex matrix operations, schedule conditioning, and increased dimensionality in high-dimensional data is minimal.\n\n- The qualitative analysis of CIFAR-10 images suggests that SCUD models lack clear object formation. Though the focus is on likelihood scores, this limitation in sample quality could affect its utility in image generation tasks."
            },
            "questions": {
                "value": "In general, I am willing to raise my score, if my concerns and questions are addressed with compelling evidence. \n\nBuilding on the aforementioned weaknesses, I pose the following questions:\n\n1. Can you provide O-Notation w.r.t. the data-dimensionality for the added computational cost arising from $K$? Furthermore mentioning the GPU hours of the different methods in your work could help put the computational cost of different methods into perspective.\n\n2. In addition to 1.: Could you provide a quantitative comparison of the computational costs associated with SCUD versus standard discrete diffusion methods in terms of memory usage and processing time?\n\n3. Given the increased complexity SCUD introduces, what specific strategies could one use to manage computational demands when applying SCUD to larger datasets or higher-dimensional inputs?\n\n4. Could you elaborate on how sensitive SCUD is to the selection of the rate parameter $\\gamma $? How does this parameter interact with other hyperparameters in practice on datasets other than CIFAR-10 as shown in figure 2?\n\n5. Are there particular domains where SCUD could have inherent advantages over masking diffusion?\n\n6. Although SCUD improves likelihood scores, how do you plan to address the relatively low quality of visual samples generated in CIFAR-10? Could modifications to SCUD enhance sample fidelity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose that masking diffusion succeeds because it leverages information about when corruption events occurred, enabling it to focus on modeling relationships between tokens rather than inferring corruption events. Building on this insight, they introduce a new method called schedule-conditioned diffusion (SCUD), which incorporates corruption schedule information into discrete diffusion models. Experimental results demonstrate that SCUD generalizes masking diffusion and outperforms classical discrete diffusion methods on various datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "\u200b1. **Innovative Insight**: This paper provides a novel explanation for the success of masking diffusion, linking it to the modeling of corruption schedules, which is a valuable addition to the understanding of discrete diffusion processes.\n\n\u200b2. **Methodology**: The introduction of SCUD is rigorous, with mathematically supported schedule conditioning, which extends masking diffusion and further enhances its performance.\n\n\u200b3. **Empirical Evidence**: Experiments on image, text, and protein data show that SCUD outperforms standard discrete diffusion models, supporting its claims of enhanced generative capability."
            },
            "weaknesses": {
                "value": "\u200b1. **Lack of Motivation Explanation**: The authors seem to focus heavily on explaining how SCUD works, with less emphasis on why this approach is chosen and what its ultimate goal is. This may make it difficult for readers to follow the authors\u2019 line of reasoning.\n\n\u200b2. **Over-Reliance on Appendix for Proofs**: Many key proofs and details are placed in the appendix, which disrupts the main text\u2019s coherence and could interfere with readers\u2019 logical understanding.\n\n\u200b3. **Inadequate Training/Sampling Procedure Description**: The description of the training and sampling processes is not sufficiently detailed, making it difficult to understand SCUD\u2019s training and sampling mechanics without referring to supplementary materials or  external resources."
            },
            "questions": {
                "value": "\u200b1.\tCould the authors clarify the role of $\\Delta t$ in discrete Markov process? This is not clearly defined in the \u201cBackground\u201d section, nor are there references provided for further reading.\n\n\u200b2.\tWould it be possible to simplify some of the proofs to make them more accessible in the main text?\n\n\u200b3.\tWhy didn't provide the complete training and inference process in algorithmic form in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper attempts to understand the empirical finding that discrete diffusion models using a masking corruption process outperform other noising processes such as uniform or Gaussian noising processes in which tokens are \"mutated.\" The authors hypothesize that masking diffusion dominates due to having knowledge of when corruptions occur, which is not the case for the non-masking noise processes. With this hypothesis, the authors propose a modification to standard non-masking discrete diffusion models that allows the model to condition on information about the schedule of the noising process. The authors derive a new version of the ELBO used to train discrete diffusion models into one that explicitly marginalizes over different noise schedules. This allows them to propose a new training objective and parameterization of the denoising process that conditions on noise schedules. The authors evaluate their approach on images, text, and proteins and demonstrate that their approach consistently improves over models using the same noising process, but without schedule conditioning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is tackling an original problem, namely understanding why masking discrete diffusion outperforms other noising processes including those that have domain-specific inductive biases. While I have some concerns with the hypothesis (see section Weaknesses), I agree that conditioning on noise schedules should help mitigate some of the advantages that masking diffusion models have. The paper contains a rigorous derivation that shows how to incorporate the noise schedules into the modeling framework. The empirical results demonstrate that conditioning on the noise schedule tends to improve likelihood-based metrics over models that use the same noise process but do not incorporate the noise schedule."
            },
            "weaknesses": {
                "value": "I am not convinced by the primary hypothesis outlined in the abstract that says \"...we propose that masking diffusion dominates due to knowledge of when corruptions occur.\" Unless I am misunderstanding something, masking diffusion models have no knowledge of when an event occurs, only that an event occurred. To put a different way, the identity of the state (masked or not) tells us whether it has been noised. However, we have no idea when the noising occurred. This seems like an important point for motivating the use of noise schedules as input.\n  \nThe primary weakness for me was the experimental results. In particular, the paper seems to be lacking several key details about the experiments. I will reiterate this in the \"Questions\" section, but I did not see any explanation for why the authors chose to model Cifar10 with both 128 and 256 pixel values. Based on the lack of explanation, I am left feeling that the B=128 experiments were done simply because this was a setting where the authors found they can get Gaussian SCUD to improve over Masking. However, to me, this makes the results appear cherry-picked. Furthermore, in Figure 2, it appears Gaussian is already outperforming Masking, without SCUD (although perhaps not with statistical significance), diminishing the novelty of the result. There should be some commentary on this since.\n\nAll of the experiments appear to be done with different training data than the baselines. Thus, I am not sure any of the numbers are truly valid in terms of comparing between methods. Most concerningly, I didn't see any explanation of this in the text or appendix. Baselines should be redone using the same training data.\n\nFor the baselines, it seems a crucial baseline would be to use all of the same hyperparameters as SCUD, but without conditioning. This would use the same noising process (e.g. Gaussian or Uniform), the same architecture, and the same training data but without using any conditioning information (e.g. just pass in a constant $s$ every time).\n\nThe authors do not describe how BPD/Perplexity are computed. Since there appears to be an additional variable to marginalize out (the noise schedule), these details are important and need to be explained and justified.\n\nPractically speaking, the field seems to be moving away from discrete diffusion models and more towards discrete flow matching. The latter has a much simpler objective function and training procedure while improving results. However, I am very sympathetic to the fact that field is moving so quickly and therefore do not penalize the authors for this. However, any more discussion about how this can be extended to flow-matching would be welcome and it seems like a straightforward extension?\n\nThe writing and presentation could be improved for more clarity. There were a few instances where the writing was too imprecise for me to understand what was meant. For example, around lines 235-236, the authors write \"...define pr(x_t^d) as the last event in dimension d...\". It was not clear to me what is meant by and \"event\" and what \"last\" is referring to. I think what is meant is pr(x_t^d) is the state of x_t before the last noise event?\n\nAnother place where the writing could have been more clear is in the \"Efficient Loss\" section when the authors say \"... and then add a weight representing how likely an event is to occur at the instant t.\" Here I think it would be helpful to write what the term is as I was left confused about which terms in Equation 5 were the weight. I believe the weight term is both the term involving Betas and the multiplication of s_t. As a reader, I was expecting this to be more clearly laid out for me."
            },
            "questions": {
                "value": "1) Can the authors clarify my point about masking diffusion models not having knowledge of when corruptions occur?\n\n2) Why were experiments done with B=128 and B=256? Was is due to the fact that only for B=128 did SCUD outperform masking?\n     \n3) Why were different training data used in the baselines? \n\n4) How were the metrics computed? Specifically, how did the authors handle marginalizing the noise schedule?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}