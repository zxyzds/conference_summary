{
    "id": "VvxuD3cdJx",
    "title": "AutoScale: Combining Multi-Task Optimization with Linear Scalarization",
    "abstract": "Multi-task learning is favored due to its efficiency and potential transfer learning achieved by sharing networks across tasks. While a series of multi-task optimization algorithms (MTOs) have been proposed to solve MTL optimization challenges and enhance performance, recent research claims that simple linear scalarization, which sums per-task loss with a carefully searched weight set, is sufficient, casting doubt on the added value of more complex MTO algorithms. In this paper, we provide a novel perspective that linear scalarization and MTOs are closely related and can be combined to yield high performance and efficiency. We show, for the first time, that a well-performing linear scalarization exhibits specific characteristics of certain optimization metrics proposed by MTOs, such as high task gradient magnitude similarity and low condition number, via an extensive empirical study. We then propose AutoScale, an efficient pipeline that leverages these influential metrics to guide the search for optimal linear scalarization weights. AutoScale shows superior performance than prior MTOs and performs close to the searched weight performance consistently across different datasets.",
    "keywords": [
        "Multi-task Learning",
        "Autonomous Driving"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=VvxuD3cdJx",
    "pdf_link": "https://openreview.net/pdf?id=VvxuD3cdJx",
    "comments": [
        {
            "summary": {
                "value": "This paper explores the connection between multi-task optimization (MTO) and linear scalarization, proposing the use of multi-task learning metrics such as gradient magnitude similarity and condition number to guide the determination of weights in linear scalarization. The proposed approach, named AutoScale, includes detailed discussions on the initial multi-task learning phase and various strategies for optimizing the weights, which gives insights into its implementation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Multi-task optimization and learning is essential in machine learning. The approach of using multi-task learning metrics to determine the weights in the linear scalarization seems novel. The proposed method is computationally more feasible than an exhaustive grid search of weights."
            },
            "weaknesses": {
                "value": "1. While using MTL metrics to estimate the optimal weight appears reasonable based on the results shown in Figure 2, it is unclear whether these findings would hold consistently across other datasets. Additionally, the evaluation metric\u2014a single average of the accuracy of multiple tasks\u2014may not fully capture the nuanced definition of \"good\" and \"bad\" results, especially in a multi-task learning context. More careful discussions are needed to justify the use of MLT metrics.\n2. The paper could benefit from more comparisons with linear scalarization approaches that directly use predictive performance metrics as criteria for determining weights.\n3. Beyond the unitary and grid-searched weights discussed, the authors might consider exploring other weight determination methods, such as those surveyed by Royer et al. (2024), to provide a broader context and benchmark for AutoScale."
            },
            "questions": {
                "value": "In Table 1, the iteration time is reported. Could the authors also provide the total computational time? Additionally, what is the computational time for each method shown in Table 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies the relationship between MTO and linear scalarization, which is over debate in the literature. Authors use some metrics to demonstrate the positive relationship. Based on this observation, a new two-phase pipeline for MTL is proposed and demonstrated in multiple datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This work is the first to explore relationship between MTO and linear secularization.\n\nExperiments show the benefit of AutoScale in terms of evaluation metrics and running time."
            },
            "weaknesses": {
                "value": "Notations are not given before their formal definitions, which creates barriers of reading.\n\nSome explanations are not given, for example, why are those cost functions defined in that way?\n\nResults (figure/table) are not clearly explained."
            },
            "questions": {
                "value": "What are G1, G2, G3, \u2026, B1 in Figure 2?\n \nI feel the whole structure needs to be rewritten. For example, Delta m is in Figure 3 and authors wrote on Line 257 that Figure 3 shows clear correlations. However, Delta m is not defined until Section 5. Key definitions need to be given in the main text.\n \nIn Section 4.1, authors propose three cost functions. Are these cost functions related to the literature. Are there any reasonings on choosing these? Why are these functions helpful to the problem? Authors should explain the intuitions.\n \nIn Section 4.2, what is the fifth method? Exponential fit? What are the conclusions from Figure 4?\n\nIs Time s/iter the running time? For example, the last column of Table 1. This should be defined clearly.\n\nAuthors show the running time, different values of alpha (Figure 6) etc. If these are to demonstrate the generalization or robustness, authors should show them in all datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes AutoScale, a new approach that combines MTO with linear scalarization for efficient and effective MTL. The authors explore the synergy between MTOs and linear scalarization, proposing that certain MTO metrics can guide weight selection to improve scalarization efficiency. AutoScale operates in two stages: an exploration phase to gather gradient and loss data using an MTO method, and a scalarization phase that leverages optimized weights derived from key MTO metrics. Extensive experiments demonstrate that AutoScale outperforms prior MTO methods and nearly matches the performance of grid-searched scalarization weights, without the associated search costs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-organized and the idea is clear.\nThe presentation is overall clear, and the experiments seem extensive and convincing."
            },
            "weaknesses": {
                "value": "1. The technical contributions seem limited since the proposed methodology is pretty straightforward.\n2. In the experiments, the number of tasks are too small (i.e. up to 4 tasks), which are not common in multi-task learning settings.\n3. The code is unavailable, and it is impossible to reproduce the results.\n4. Some typos. For example, Line 306 Exploration -> exploration\nLine 311: Linear Scalarization -> linear scalarization"
            },
            "questions": {
                "value": "1. Can authors discuss why linear scalaization correlates with some MTO metrics while is independent of others. What are commonality of these correlated MTO metrics?\n\n2. What are benefits of Autoscale compared with traditional MTO algorithms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}