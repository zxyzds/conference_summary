{
    "id": "6zAIFLgayn",
    "title": "Open Eyes, Then Reason: Fine-grained Visual Mathematical Understanding in MLLMs",
    "abstract": "Current multimodal large language models (MLLMs) often underperform on mathematical problem-solving tasks that require fine-grained visual understanding. The limitation primarily arises from inadequate perception of geometric primitives during image-level contrastive pre-training (e.g., CLIP). Current efforts to enhance MLLM performance have focused on scaling up mathematical visual instruction datasets and employing stronger LLM backbones, yet these approaches often neglect persistent visual recognition errors in MLLMs. In this paper, we systematically evaluate the visual grounding capabilities of state-of-the-art MLLMs and uncover a negative correlation between their visual grounding accuracy and problem-solving performance. Notably, even advanced models like GPT-4o demonstrate a significant error rate (70\\%) when identifying geometric entities, highlighting that fine-grained visual understanding remains a crucial bottleneck in visual mathematical reasoning. To address this, we propose a novel approach, SVE-Math (Selective Vision-Enhanced Mathematical MLLM), featuring a geometric-grounded vision encoder and a feature router that dynamically adjusts the contribution of hierarchical visual feature maps. Our model recognizes accurate visual primitives and generates precise visual prompts tailored to the language model's reasoning needs. In experiments, SVE-Math-7B outperforms other 7B-parameter models by 5.5\\% on MathVerse and even surpasses larger models on MathVista. Despite being trained on smaller datasets, SVE-Math-7B matches the performance of models trained on significantly larger datasets on GeoQA benchmark. Our findings provide critical insights for future research, highlighting the need for more effective integration of fine-grained visual understanding in MLLMs.  We will release model weights, code, and instructions upon acceptance.",
    "keywords": [
        "Multimodal Large Language Models (MLLMs);Mathematical Reasoning;Fine-grained Visual Understanding;Visual Grounding"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6zAIFLgayn",
    "pdf_link": "https://openreview.net/pdf?id=6zAIFLgayn",
    "comments": [
        {
            "summary": {
                "value": "The paper first identifies visual recognition errors prevalent of current MLLMs by a pilot study. Then the paper introduces GeoGLIP, a vision encoder specifically trained to identify geometric elements in the image. The feature from the trained geometric vision encoder is later merged with the feature of the original CLIP vision encoder, aiming at more precise geometry perception. The authors prove the effectiveness of their method by evaluating on various benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The paper proposes a novel perspective that the errors of visual mathematical problems come from poor visual perception.\n+ The three training tasks of GeoGLIP closely matches the analysis in Fig. 1. The paper is self-contained and well-written."
            },
            "weaknesses": {
                "value": "+ The major concern is whether addressing the visual perception error is sufficient for the mllm to correctly solve these tasks. The visual mathematical questions also require advanced reasoning capability, especially merging both the visual and textual information. Only correctly identifying the graph seems to be far enough to solve a mathematical problem. Detecting the texts, shapes or curves in the graph does not necessarily suggest the model understands the element. How much GeoGLIP actually helps in understanding and reasoning seems marginal. The pilot study shown in Fig. 1 also only analyze the error of visual descriptions, while neglecting other potential core problems of MLLM for visual mathematical questions. \n+ The effectiveness of the proposed GeoGLIP is not validated. The authors need to report the performance of the model trained with same instruction data only without the GeoGLIP encoder to illustrate the improvement brought by it. Otherwise, the improvement may be from the Geo170K data.\n+ The overall performance advantages of SVE-Math compared to previous works are not very obvious."
            },
            "questions": {
                "value": "+ How much more computational cost and inference time is introduced by GeoGLIP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "To address the limitations of multimodal large language models (MLLMs) in solving math problems involving images, this paper proposes a Selective Vision-Enhanced Mathematical MLLM. It leverages a geometric-grounded vision encoder and a feature router to help MLLMs better comprehend mathematical image features, thereby improving their performance on math problems with visual components."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper clearly articulates the problem it aims to address, and the overall writing is easy to follow.\n\n2. The paper enhances MLLM's ability to recognize mathematical images and solve math problems by introducing geometry-rich visual information, achieving improvements on several benchmarks."
            },
            "weaknesses": {
                "value": "1. Using more detailed visual features for solving math problems is an intuitive idea, as is combining geometric and semantic features at different levels. However, you should conduct additional ablation studies to validate the effectiveness of this approach. For instance, consider using vision encoders from other similar models on your dataset/training your model on the training data of other models.\n\n2. In Table 1, some experimental results differ from those provided in the official MathVerse table. For example, you show the cot-e score for SPHINX-Plus and the w/o score for SPHINX-MOE. When comparing with other models on the same benchmark, you should ensure thorough variable control.\n\n3. You mention using synthetic data, but the paper does not include any description, details, or examples of the synthetic data generation process.\n\n4. The paper does not present any output examples from the model.\n\n5. As a \u201cdata collection-model training-benchmark testing\u201d type of paper, the performance improvements on benchmarks are minimal in the absence of novelty."
            },
            "questions": {
                "value": "1. In terms of writing, the paper\u2019s section distribution could be improved. You should allocate some space to introduce synthetic data, dedicate more space to ablation studies to validate the method's effectiveness, and reduce the length of the Methods section.\n\n2. Please provide more details and examples of the synthetic data.\n\n3. Please provide examples of the model\u2019s outputs to demonstrate its ability to recognize geometric elements and Chain-of-Thought (CoT), as you compared cot-e performance with some models in Table 1.\n\n4. In the Introduction, you mentioned a finding: instructing MLLMs with fine-grained visual information improves top-1 accuracy compared to providing only worded questions, while providing all visual cues for solving a math question decreases accuracy. How does your approach\u2014primarily by introducing more geometry-rich visual information\u2014address the issue highlighted by this finding?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces SVE-Math, a Multimodal Large Language Model (MLLM) designed for mathematical question answering. It incorporates a GeoGLIP module to enhance the visual encoder's perception of mathematical elements and utilizes a routing module to prioritize features from CLIP. The training process for SVE-Math consists of three stages: GeoGLIP training, cross-modal alignment, and instruction tuning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.The approach of enhancing the visual encoder for improved mathematical performance is both innovative and logical.\n\n2.The routing module is well-designed and demonstrates significant performance improvements in the ablation studies. However, I believe that the routing module is not specifically designed for mathematical reasoning tasks and can be applied to a wider range of scenarios.\n\n3.The paper is well-structured and easy to understand."
            },
            "weaknesses": {
                "value": "1.My main concern is the performance results, which are not particularly impressive. While SVE-Math achieves competitive scores on several benchmarks, the improvements over the previous works are marginal, raising questions about the effectiveness of the approach.\n\n2.Building on the first point, I believe a significant portion of the performance improvement in MLLMs stems from the data used. The scale and quality of training data are critical for MLLMs. Could you elaborate on any unique handling or augmentation techniques applied to the training data? \n\n3.Could the authors provide more explanation of why the routing module is specifically designed for mathematical reasoning tasks? Relying solely on empirical evidence is not sufficient to substantiate this claim."
            },
            "questions": {
                "value": "Please refer to weakness. Can the proposed methods be applied to other mathematical problems beyond geometric figures and problems?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes the SVE-Math-7B model to improve the math reasoning skills of current MLLMs. The authors start by analyzing the performance on mainstream models' math reasoning tasks to show the geometric information's effectiveness. Based on the observation, the author proposes the architecture of SVE-Math with a pre-trained GeoGLIP, a fusing connector with dual visual encoders, and further fine-tuning the baseline models. The authors conduct experiments on mainstream math-relative benchmarks such as MathVerse and MathVista and show improvements compared with baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper discovers and discusses the math-solving problem of MLLMs, which is a significant and widely-concerned problem for current MLLMs. The solution with GeoGIP and math-relevant fine-tuning is efficient for the problem.\n2. The analysis in Figure 1 clearly shows the drawbacks of LLaVA and GPT-4o, and shows the effectiveness of geometric information. \n3. The methods and experiment periods are well organized and easy to follow. The authors conduct experiments on mainstream math datasets and clearly show the results."
            },
            "weaknesses": {
                "value": "1. The main weakness is that the ablation analysis is not sufficient to demonstrate the improvements of all the components. The author proposes the GeoGLIP, dual visual encoder connector, math-specific finetuning with Geo170K, MathV360K datasets. However, the analysis of such aspects is lacking. The authors only conduct experiments on the design of connectors, which is not the key claim for the contributions, as many papers have used similar fusing approaches for visual encoders. I think the authors could clearly explain where the improvements come from, especially for the GeoGLIP and the math-relevant training datasets. \n2. Although the authors show improvements over baselines, the performance for SVE-Math-7B is significantly behind the state-of-the-art models (e.g. more than 60 accuracy on MathVista). I assume the approach proposed by the author is universal, therefore the results of state-of-the-art models are lacking. \n3. The effectiveness of GeoGLIP is not confirmed. I wonder how the tiny visual encoder with less than 50M parameters can help the overall learning results. As shown in the visualization results, directly providing geometric-relevant information in a proper manner may also lead to similar performance. The authors could conduct sufficient experiments to explain this issue."
            },
            "questions": {
                "value": "As stated in the weakness periods, clarifying the issues can better demonstrate the conclusion of the paper. \n1. What are the improvements with math-specific datasets? \n2. Why using GeoGLIP based on Swin-T is effective for results? As illustrated in the visualization results, the usage of the models provides geometric information, so the authors may provide more comparisons by providing direct geometric results, or directly using GLIP. \n3. The results for current models are somehow out-of-date. The authors are encouraged to equip proposed approaches on state-of-the-art level MLLMs. \n4. The math problems may already be solved with better data curation or reasoning processes, as many papers have done on such problems. The author could provide explanations and superiority for the proposed methods and provide comparisons with other methods on math problems.\nTherefore, based on the weaknesses and questions stated above, I think the paper is below the acceptance threshold in the current situation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}