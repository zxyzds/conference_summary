{
    "id": "zo049dh2r9",
    "title": "3DRealCar: An In-the-wild RGB-D Car Dataset with 360-degree Views",
    "abstract": "3D cars are commonly used in self-driving systems, virtual/augmented reality, and games. However, existing 3D car datasets are either synthetic or low-quality, presenting a significant gap toward the high-quality real-world 3D car datasets and limiting their applications in practical scenarios. In this paper, we propose the first large-scale 3D real car dataset, termed 3DRealCar, offering three distinctive features. (1) \\textbf{High-Volume}: 2,500 cars are meticulously scanned by 3D scanners, obtaining car images and point clouds with real-world dimensions; (2) \\textbf{High-Quality}: Each car is captured in an average of 200 dense, high-resolution 360-degree RGB-D views, enabling high-fidelity 3D reconstruction; (3) \\textbf{High-Diversity}: The dataset contains various cars from over 100 brands, collected under three distinct lighting conditions, including reflective, standard, and dark. Additionally, we offer detailed car parsing maps for each instance to promote research in car parsing tasks. Moreover, we remove background point clouds and standardize the car orientation to a unified axis for the reconstruction only on cars without background and controllable rendering. We benchmark 3D reconstruction results with state-of-the-art methods across each lighting condition in 3DRealCar. Extensive experiments demonstrate that the standard lighting condition part of 3DRealCar can be used to produce a large number of high-quality 3D cars, improving various 2D and 3D tasks related to cars. Notably, our dataset brings insight into the fact that recent 3D reconstruction methods face challenges in reconstructing high-quality 3D cars under reflective and dark lighting conditions. \n\\textcolor{red}{\\href{https://3drealcar.github.io/}{Our dataset is available here.}}",
    "keywords": [
        "3D reconstruction",
        "Car reconstruction",
        "Car dataset"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zo049dh2r9",
    "pdf_link": "https://openreview.net/pdf?id=zo049dh2r9",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a new dataset of 2.5k real cars scanned using modern\niPhones, resulting in high-resolution images as well as sparse LiDAR point\nclouds (~200 views per car). This setting provides additional training and\nbenchmarking opportunities in novel view synthesis (NVS).\n\nKey tasks which can be researched and benchmarked with this dataset include 3D\nreconstruction, relighting, and parsing of car parts.\n\nThe dataset is further motivated with experiments demonstrating that using its\n3D cars to perform image data augmentation can lead to perception improvements\nin unusual scenarios. The dataset is also shown to help with part segmentation,\n3D generative modeling, and 3D neural rendering.\n\nOne of my main questions regarding the paper, as elaborated in the Questions\nSection, is whether any car is captured in more than one lighting condition. I\nthink this is a major research gap, so the presence of this data is pivotal in\nassessing the impact of this dataset."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- [S1] The dataset includes diverse car types, including sedans, sports cars,\n  and small trucks, captured in high resolution, and annotated with part\n  information (e.g., doors, hood, wheels). This has applications in benchmarking\n  3D reconstruction and in developing simulators for autonomous driving.\n- [S2] The experimental section analyses multiple tasks such as neural rendering\n  and 3D generative modeling, as well as camera-based perception in corner-case\n  scenarios. In the latter example, the authors demonstrate that synthesizing\n  corner-case images (e.g., cars swerving off road) using their proposed 3D car\n  assets can help improve real data perception on these scenarios.\n- [S3] The dataset is already openly available online!"
            },
            "weaknesses": {
                "value": "- [W1] The LiDAR used is the iPhone 14 model, which is sparser than typical\n  automotive LiDARs. While denser LiDAR can of course be re-simulated from dense\n  3D models, this will require additional engineering effort while also\n  suffering from some domain gap. This should be clarified in the intro: \"3D\n  scanner\" makes me think of automotive or survey-grade LiDAR, not smartphone.\n- [W2] Some minor suggestions for additional references and discussions: Please\n  consider mentioning DeepMANTA [0] as related work since, while old, it also\n  proposed similar part-level annotations.\n- This isn't a weakness but as a tip: It can be helpful to provide some usage\n  examples and tutorials for the dataset next to its SDK. For example \"how to\n  run gaussian splatting on 3DRealCar\", since this can help more people learn to\n  be familiar with the dataset, which can increase its impact.\n- Other small suggestions:\n  - For the \"Stable Zero123\" reference please surround the author name in curly\n    braces in the .bib file, so the citation shows up as \"Stability AI, 2023\"\n    instead of just \"AI, 2023\".\n  - For those curious (such as myself!) it would be helpful to add some\n    low-level details on the data collection in the appendix. For example, I\n    have no idea how to get raw LiDAR points from an iPhone, so a brief\n    discussion would be interesting.\n  - L325: \"the car is well-lighting\" -> \"the car is well-lit\"\n  - L417: \"Dreamcract3D\" contains a typo\n- References:\n  - [0]: Chabot, Florian, et al. \"Deep manta: A coarse-to-fine many-task network\n    for joint 2d and 3d vehicle analysis from monocular image.\" Proceedings of\n    the IEEE conference on computer vision and pattern recognition. 2017."
            },
            "questions": {
                "value": "- [Q1] Is the proposed dataset restricted to academia or is commercial use\n  allowed?\n- [Q2] I looked at the sample car reconstruction provided on the 3DRealCar\n  dataset, and noticed that the COLMAP output mesh, =textured_output.obj=, is\n  missing the upper half of the car. Is this expected?\n- [Q3] Are any of the vehicles in the dataset captured in more than one\n  environment? It is valuable to have the same vehicle captured in multiple\n  environments, with varying lighting conditions, because it creates ground\n  truth for relighting. Relighting is challenging and it is otherwise very\n  difficult to get non-synthetic ground truth, so having some of the cars\n  captured in 2+ lighting conditions would be valuable. However, I cannot tell\n  whether this is the case in the proposed dataset.\n- [Q3] Will the dataset also include pre-computed dense reconstructions like\n  splats, or dense meshes extracted from a NeRF-like method? As mentioned above, it\n  seems like the COLMAP mesh outputs can be incomplete.\n- [Q4] On L346, \"2D Car Parsing\", why are car parsing maps given as input to\n  segmentation? Or does \"S\" refer to just a binary mask of the vehicle? (Vehicle\n  vs. background?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a comprehensive 3D car dataset featuring high-quality 360-degree scans. The dataset comprises 2,500 cars, each scanned with 3D scanners to produce 200 high-resolution RGB-D views. Additionally, the dataset includes variations across three different lighting conditions: reflective, standard, and dark.\n\nThe authors detail the data collection methodology extensively. Data capture involved the use of RGB-D sensors, followed by the recovery of camera poses using Colmap Structure from Motion (SfM), and mask extraction utilizing Grounding-DINO. The 3D models were then reconstructed with 3DGS.\n\nThe paper discusses extensive 2D and 3D downstream experiments conducted using the dataset, which include 2D detection and segmentation, depth estimation, point cloud completion, and 3D car generation. The experiment results demostrate the effectiveness of the collected data on various tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The structure of the paper is well-organized and the content is clearly presented;\n2. The effectiveness of the dataset is demonstrated through its application to various downstream tasks, including various 2D and 3D tasks.\n3. This paper introduces a novel 360-degree real car dataset. Previous literature has not extensively covered. Both the quality and the diversity of the dataset are commendable."
            },
            "weaknesses": {
                "value": "1. The data collection and processing method is not novel and is a standard approach to reconstructing 3D assets.\n2. The paper does not highlight its uniqueness and irreplaceability, especially in terms of improving 2D parsing and 2D detection performance.\n3. The results of the NVS task (Fig.8 and 9) are considerably inferior to the state of the art.\n4. Many previous papers, including CADSim (https://arxiv.org/pdf/2311.01447), GeoSim (Chen et al., CVPR'21), and other related works, have demonstrated the capability to reconstruct cars from real-captured data, which I believe is a more extensible way to reconstruct car assets. The authors claim that their data is of higher quality, but the paper does not demonstrate the necessity of such high quality, especially for downstream tasks.\n\nAlthough the paper provides a very useful dataset and makes a significant contribution, the downstream tasks and data collection processes are not novel and do not meet the acceptance standards of ICLR."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes 3DRealCar, a large-scale 3D real car dataset with RGB-D images and point clouds of 2,500 cars captured in real-world environments. It addresses the limitations of existing car datasets that usually use synthetic data or have low-quality data. This dataset includes cars under three lighting conditions (standard, reflective, and dark), promoting research in 3D car reconstruction, parsing, and novel view synthesis. Benchmarks with state-of-the-art methods demonstrate that existing methods struggle in reflective and dark lighting conditions, emphasizing the dataset\u2019s value for improving 3D reconstruction methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper addresses the limitations of current car datasets by capturing real-world car data with diverse samples, high-quality images, and point clouds.\n2. The idea of introducing 3 different lighting conditions is interesting and makes up for the shortcomings of existing datasets\n3. The extensive experiments demonstrate the effectiveness of this dataset on various tasks."
            },
            "weaknesses": {
                "value": "1. Though collecting data is costly, the contribution of this work is only on the dataset and lacks technical contribution for 3D/2D car understanding. \n2. This dataset captures cars under three lighting conditions, including reflective, standard, and dark; however, it seems to lack data of the same car under all three conditions, which limits the exploration of the effects of lighting on car appearance."
            },
            "questions": {
                "value": "Data of the same car under different lighting conditions could be added to enhance the analysis of lighting effects. Additionally, including metadata such as environmental maps or more detailed car materials could improve the dataset's versatility."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}