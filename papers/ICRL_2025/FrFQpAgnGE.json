{
    "id": "FrFQpAgnGE",
    "title": "Language Models Implicitly Learn a Unified Representation Space",
    "abstract": "Modern language and multimodal models can process a wide variety of inputs across different languages and modalities. We hypothesize that models acquire this capability through learning a *unified representation space* across heterogeneous data types. We first show that model representations for semantically equivalent inputs in different languages are similar in the intermediate layers, and that this space can further be interpreted using the model\u2019s dominant pretraining language (when it has one) via the logit lens. We also find that models show a similar tendency when processing other kinds of data, including code and visual/audio inputs. Interventions in the unified representation space further affect model outputs in expected ways: for example, replacing the image representations in a vision-language model with language token representations leads to output changes consistent with the language token semantics, suggesting that the unified representations space is not simply a byproduct of large-scale training on broad data, but something that is actively utilized by the model during input processing.",
    "keywords": [
        "interpretability",
        "representation",
        "multilingual",
        "multimodal"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=FrFQpAgnGE",
    "pdf_link": "https://openreview.net/pdf?id=FrFQpAgnGE",
    "comments": [
        {
            "summary": {
                "value": "This paper aims to show that models process inputs from different languages/modalities primarily by first projecting them into a unified representation space, before projecting them out into their language/modality for the output. They do so for multilingual, code, visual and audio settings. They then perform interventions in the unified space across those settings to support the hypothesis that the models operate in that space in intermediate layers."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "* The paper builds on Wendler et al. (2024), which only looked at Llama trained pre-dominantly on English, to show that models trained predominantly on other languages do indeed have that language as their intermediate representation space\n* They show that this also extends to multimodal input, which is what supports the claim of a 'unified' representation space.\n* They perform some intervention experiments to support the hypothesis, and show that intervening in the unified representation space can affect outputs in other modalities/languages."
            },
            "weaknesses": {
                "value": "* In Line 108, they mention that 'absolute similarity measures are generally difficult and unintuitive to interpret in high dimensional spaces' and hence choose relative similarity measures instead. This is a key choice in the experimental setup, and I would hope to see more discussions on whether prior work have chosen to use relative over absolute similarity measures.\n* The intervention experiments seem to be somewhat weak. For them to strongly support the unified representation space hypothesis, I would expect a comparison showing that  dominant data type steering is comparable or more effective than non-dominant steering. This doesn't seem to be the case as monolingual steering seems to be on the whole better than crosslingual steering (Table 1). However, I do think that the fact that English steering works at all weakly suggests that hypothesis is true."
            },
            "questions": {
                "value": "* Section 2: Notation wise, it is not clear what $M_{LM}$ is. It strictly reads as just the embedded tokens in this section (L102), but it also looks like a general, vague reference to mid-layer representations later in the paper.\n* Equation 1 needs to be explained: is this the formulation of your hypothesis? If so, it should be introduced like: \"Formally, our hypothesis can be formulated as:\"\n* What do you mean by \"scaffolded\" as used throughout the paper? The term is not defined but used early on in the introduction.\n* Typos: (1) L288: Should this refer to Figure 8? (2) All references to Llava should be 'LLaVA' as per the original paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a hypothesis that models can learn a unified representation space across multiple modalities, such as language and vision, as well as across multiple languages. Specifically, the authors hypothesize that: (1) Semantically related concept pairs from different modalities or linguistic spaces are represented more closely than unrelated pairs. (2) When given an input, the model continues in the dominant modality (e.g., language for a language model), even when the input format is symbolic, such as code.\n\nTo validate the first hypothesis, the authors calculate the cosine similarity between paired data (e.g., parallel translations, images and captions) and non-paired data, observing that semantically paired data have higher similarity scores. For the second hypothesis, they evaluate whether tokens that match the input context have a higher probability than random tokens. The results suggest that the model favors continuations in the dominant modality.\n\nThe authors further examine cross-modal correspondence by intervening in one modality and observing changes in others, covering multilingual, code, image, and video data. For example, by replacing image hidden states corresponding to one color with the text embedding for another color, they find that the model correctly predicts the new color with 80% accuracy when replacements are made from the first layer onward.\n\nIn conclusion, the authors present empirical evidence through similarity analysis and cross-modal interventions to support that models can learn a semantically unified space across languages and modalities."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The topic of interpretability in multi-modal representations is highly relevant, and the hypotheses and conclusions presented in this paper\u2014along with the supporting experiments\u2014are likely to inspire further research. \n2. The paper is well-written and easy to follow. \n3. The authors validate their hypotheses across multiple modalities, including multilingual settings, text-code, language-vision, and language-audio, providing a thorough examination of their proposed unified representation space."
            },
            "weaknesses": {
                "value": "1: Sub-hypothesis 1, \u201csemantically related concepts are closer than unrelated concepts,\u201d is not novel, as it aligns with the standard training objective of many multi-modal models. This alignment is already an expected outcome, though recent research has highlighted its limitations and proposed methods to improve it [1, 2, 3].\n\n2: Sub-hypothesis 2 suggests that inputs will continue in the dominant space, even if the input format is not in that space. The authors support this by observing that, in models like Llama, intermediate representations shift to the dominant language (e.g., English) when the input is Chinese, with the representation returning to Chinese dominance in later layers. However, this hypothesis seems unrelated to the main idea of a unified representation space, and its findings are largely anticipated. A deeper analysis is needed, potentially identifying specific transformation mechanisms, such as attention heads or MLPs, that facilitate cross-lingual translation. Insights into how these transformations occur would add value, as explored in the Induction Head blog[3], which discusses how output circuits transform intermediate representations into the output space.\n\n3: Lack of discussion of related work in unified representation[1,2] and information flow, especially the intermediate representation to input/output space [3]  \n\nReferences: \n\n[1] Achieving Cross Modal Generalization with Multimodal Unified Representation\n\n[2] UNIFIED-IO: A Unified Model for Vision, Language, and Multi-modal Tasks\n\n[3] A Mathematical Framework for Transformer Circuits"
            },
            "questions": {
                "value": "1. The description of Figure1, and Figure2 is missing?\n\n2. In Figure 6, the intermediate tokens shown are predominantly English words rather than the matched label token. Could the authors clarify whether the tokens from the last layer align with the matched token? If so, the results may not be surprising, as the intermediate layers likely serve to associate the input with internal knowledge in the dominant space, while the later layers have output circuits that transform this dominant representation back into the output format. This aligns with my earlier point about the expected behavior of the model\u2019s translation mechanism (see Weakness Point 2)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes the Unified Representation Hypothesis, which predicts that\u2014regardless of the type of data of the inputs being processed\u2014the latent representations of a given concept will be aligned with the dominant data type that the model was trained on. For example, given a multimodal language model trained primarily on English text, the latent representations will reliably align with English text, even if the input is only an image, or in another language.\n\nThe study tests and finds consistent support for this hypothesis in cross-lingual and cross-modal settings, including text-audio, text-image, and multilingual models. These alignments are largely verified using the logit lens, but many experiments also entail counterfactual interventions to hidden states."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The study advances an idea that has been more casually discussed in past work: that the representation space of models representing many types of data will be biased toward the dominant data type. This paper proposes and presents evidence for a stronger version: that, when given data of any type, the model\u2019s representation of what to predict next will be more closely aligned with the dominant data type (even in the absence of any data of the dominant type in the input).\n* The ideas are presented in a precise way. While it may take more effort to parse than verbal intuitions, it makes the theoretical assumptions underlying the work clearer.\n* Thorough experimentation. Convincing evidence (often causal) from many models and in many settings.\n* Table 2: It\u2019s nice that the quality of the outputs is evaluated along multiple axes, rather than just overlap with respect to a reference."
            },
            "weaknesses": {
                "value": "* The proposed idea does not seem distinct from that proposed in the Platonic Representation Hypothesis paper (Huh et al., 2024), which is cited. That paper also presented similarly diverse and thorough empirical evidence in favor of unified representations across modalities. It would be helpful to explicitly contrast the Unified Representation Space Hypothesis from the Platonic Representation Hypothesis in the paper, or at least to contextualize the main idea and findings here more thoroughly with it.\n* Due to the quantity of experiments in the paper, it was probably difficult to fit the necessary details for each in such a way that makes them all clear. Most of them are well-defined, but I was sometimes not sure what, exactly, was being evaluated. See Questions/Suggestions for details.\n* L108-L113: Relative similarity captures a notion of distance, but it doesn\u2019t capture whether there is underlying structural similarity (e.g., isotropy). Token distance is a nice start, but I have a hunch that going deep rather than wide would yield even more significant insights.\n* L257-258: Speculative. It would be interesting to test this by projecting the inputs onto the space of the dominant data type, and then generating."
            },
            "questions": {
                "value": "* Figure 11: The experimental setup should be clarified. Is this a single comparison of the similarity between the logit lens and the mean representation across all nouns in the caption? Or is this an average similarity across separate similarities per noun? Or something else?\n* L360-368: This is pretty light on details.  appendix explaining the experimental settings would be very helpful.\n* Figure 8: There is plenty of vertical space here; consider enlarging this figure. Also: this looks pretty noisy. It could be revealing to add a color for a control setting, where the hidden representation is closer to a token other than these two possibilities. I hypothesize that much of this graph will be filled with the control color, but also that the rows with blue color will be much more consistent across examples.\n* Figures 12 and 13: These values seem small on an absolute scale, and the difference between baseline and the correct answer is also small on a relative scale."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates whether multimodal LLM  process diverse input types using a unified representation space. The authors utilized \"logitlens\" as an interpretability tool to extract representations from each layer of the LLM. They show that model representations for semantically equivalent inputs from different modalities are similar in intermediate layers and that interventions in this space lead to predictable changes in model behavior."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.  Extends prior research (Wendler et al.) from multilingual to multimodal settings on unified representation in the middle layers of LLM, offering new insights.\n2. The experiments are interesting and thorough across modalities, including text, code, image, audio, adding robustness to its claims.\n3. Good experiments on intervention in the unified representation space which shows the space causally steers model output."
            },
            "weaknesses": {
                "value": "1. The paper is largely an empirical study, and the findings may heavily depend on experimental settings. Some details are unclear, e.g., in multilingual experiment 1, the baseline for non-parallel texts is not well explained.\n2. The paper consistently use the last token's representations over layers for measuring similarities interpretation, which maybe a limitation.\n3. In Figure 9, the cosine similarity increase over baseline is marginal, with a maximum of only 0.03.\n4. In line 435, the paper hypothesizes that the unified representation space is language-agnostic, but the experiments primarily use the English-dominant Llama-3. This needs more justification.\n5. Lack of related work for representation alignment, such as alignment through relative space [1] and its application in LLMs [2].\n\n[1] Moschella et al. \"Relative representations enable zero-shot latent space communication\"  \n[2] Wu et al. \"Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models\""
            },
            "questions": {
                "value": "1. Why Bloom is not a english-dominant LLM but Llama is english-dominant? Does it mean that they were trained differently?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}