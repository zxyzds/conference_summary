{
    "id": "TCFtGBTxkq",
    "title": "Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training and Unimodal Deployment",
    "abstract": "Building reliable speech systems often requires combining multiple modalities, like audio and visual cues. While such multimodal solutions frequently lead to improvements in performance and may even be critical in certain cases, they come with several constraints such as increased sensory requirements, computational cost, and modality synchronization, to mention a few. These challenges constrain the direct uses of these multimodal solutions in real-world applications. In this work, we develop approaches where the learning happens with all available modalities but the deployment or inference is done with just one or reduced modalities. To do so, we propose a Multimodal Training and Unimodal Deployment (MUTUD) framework which includes a Temporally Aligned Modality feature Estimation (TAME) module that can estimate information from missing modality using modalities present during inference. This innovative approach facilitates the integration of information across different modalities, enhancing the overall inference process by leveraging the strengths of each modality to compensate for the absence of certain modalities during inference. We apply MUTUD to various audiovisual speech tasks and show that it can reduce the performance gap between the multimodal and corresponding unimodal models to a considerable extent. MUTUD achieves this while reducing the model size and computing compared to multimodal models by almost 80%.",
    "keywords": [
        "audiovisual learning",
        "speech processing",
        "multimodal learning",
        "efficiency"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "a new approach where you train with multimodal data but infer using only one modality",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TCFtGBTxkq",
    "pdf_link": "https://openreview.net/pdf?id=TCFtGBTxkq",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes MUTUD, which enables unimodal inference (deployment) by leveraging multi-modal training (specifically audio-visual training in this work). The TAME module is proposed to enable cross-modal transfer and is proven to be effective. It achieves better results than single-model training while using fewer parameters than multimodal training across three speech tasks: speech recognition, active speaker detection, and speech enhancement."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The presentation is quite clear, and the design of TAMU is simple and easy to understand.\n2. While there are few works focused on efficient unimodal deployment, this appears to be the first adaptation in audio-visual speech tasks. The motivation is therefore innovative.\n3. The experiments are sufficiently extensive."
            },
            "weaknesses": {
                "value": "Major Concerns:\nWhile I understand that multi-modal approaches can improve performance, they also increase costs, which motivated MUTUD's development. I am curious about practical utility in industry applications - would we really use audio-visual data for performance boosting? I would love to see actual industry statistics. Though the motivation is quite clear, I would appreciate seeing the \"motivation behind the motivation.\"\nAudio and video data, though time-synchronized, aren't always perfectly aligned per frame. I'm not sure if you have any insights to handle this. Would time-accurate alignment in TAME lead to error accumulation? \nAdditionally, I'm uncertain about how general TAME is. What if we have different video modalities like face or lip?\n\n\nMinor Concerns:\nOn line 36, the claim that \"visual modality is the most widely used in these speech tasks\" needs evidence. From my perspective, text appears to be more prevalent.\nRegarding lines 45, 49, and 54, the three bullet points seem to apply specifically to audio-visual multi-modal scenarios, while speech-text multimodal situations might be different. One important missing challenge is that multimodal input doesn't always bring improvement - it can be noisy and confusing. For example, face and speech are not \"perfectly\" matching.\nOn line 137, the related work section should consider \"ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Enhancement.\" The literature review needs to be more comprehensive."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an approach to leverage multiple modalities in model training to improve performance when only a single or a subset of modalities are present at test time.  This is achieved by estimating the missing modalities based on their correlation with the known modalities and using those estimates in the inference process.  A codebook based approach to model the modality correlation is presented.  Results of three audio-visual tasks \u2014 speech enhancement, speech recognition, and active speaker detection \u2014 demonstrate that by using both modalities at training time while only one at test time results in a significant gains over baseline single modality performance on each of the three tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* A novel approach to utilize information from two or more modalities at model training time to improve run-time performance when only a single or a subset of modalities are present.  Proposed approach relies on estimating, in a time aligned manner, modality features that are not present at run-time.\n* Proposed model is parameter and compute efficient\n* Results in a significant accuracy gains without adding too many parameters over the baseline unimodal model."
            },
            "weaknesses": {
                "value": "* Missing baselines: Using multiple modalities at training time only to improve performance under a single or a subset of modalities has been studied before.  E.g. [1].  References & comparison with those methods needs to be carried out to fully assess the merits of the proposed model.\n\n[1] Abavisani et al., \u201cImproving the Performance of Unimodal Dynamic Hand-Gesture Recognition with Multimodal Training\u201d\n\n* Only the case of audio-visual ASR / Speech enhancement / Active speaker selection is studied.  This provides a limited, bi-modal view of general multimodal tasks.  A more general multi-modal setting will make this work much more attractive."
            },
            "questions": {
                "value": "* Typo on Line 058: defnitely\n* Lines 195-196: define \u2018D\u2019"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors proposed a Multimodal Training and Unimodal Deployment (MUTUD) framework to train the model on audio and visual modalities, but only use audio modality for inferecnce. The authors expected the model to imagine visual information from extracted audio features to help downstream audio tasks, with a Temporally Aligned Modality feature Estimation (TAME) module. TAME comprises individual codebooks for audio and video, and is trained using reconstruction loss and KL divergence loss to associate the different modalities. During inference, the audio features are used to retrieve the corresponding video features through the codebooks. Other training objectives are used for specific downstream tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors present a potential method to guide the projection from audio to visual representation when visual input is absent. However, the experiments are insufficient and lack persuasiveness.  The writing is clear and well-structured."
            },
            "weaknesses": {
                "value": "1. The author conducted experiments solely on the audio-visual dataset, comparing the performance of the multi-modal, audio-only, and MUTUD frameworks. However, no independent validation and comparison with other models were carried out on out-of-domain audio-only datasets. Such validation is crucial to checkout whether the model merely overfits to specific biases within the audio-visual dataset, or genuinely learns the projection from audio features to video features, thereby enhancing uni-modal tasks.\n\n2. Lack of novelty. Similar methods have already been explored, such as \"Multi-modality Associative Bridging through Memory: Speech Sound Recollected from Face Video,\" which predicts audio features from visual features.\n\n3. The performance gain is quite limited compared to the audio-only model.\n\n4. The experiment is not sufficient. Lack of comparison with some recent models on the three tasks.\n\n5. Lack of validation for the effectiveness of the proposed TAME method on more advanced backbone."
            },
            "questions": {
                "value": "1. Could you provide the no independent validation and comparison with other uni-modal models on audio-only tasks? Such as speech enhancement, ASR, speaker detection.\n\n2. The authors have built the models and baselines using relatively older models (e.g., GCRN, 2019). How does TAME perform when integrated with the latest backbone models?\n\n3. What are the performance outcomes and discussions in comparison with the latest state-of-the-art works across the three tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tackles the interesting task of training on multimodal data and then evaluating using only one modality. This is achieved by predicting the other modality. Their method focuses on the use of code books to quantize audio and visual inputs, and they apply various loss functions in order to align the quantized modalities of the code books. They also align the dimensions of the audio and video via their interleaving method. They beat their benchmarks in several tasks while maintaining a small model footprint - measured by parameters and MACs."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper presents a useful approach to multimodal-to-unimodal distillation through its use of codebooks for modality alignment. The technical work includes benchmarking across different datasets and tasks, with testing under various noise conditions and SNR levels. The authors provide ablation studies on codebook sizes and architectural choices, and show how their method works with different baseline architectures. The experimental results are well-documented, and the paper's structure makes it easy to follow their methodology and findings. \n\nFrom a practical standpoint, the work offers a solution for running multimodal systems with single modality inputs during deployment. Their implementation reduces model size by 80% and requires less computation compared to full multimodal systems. The authors' method for aligning audio and visual dimensions works effectively, and they demonstrate good performance across benchmarks while keeping the model compact in terms of parameters and MACs. The results suggest their approach could be useful for applications where resource efficiency matters."
            },
            "weaknesses": {
                "value": "There are several weaknesses to this paper. To summarise briefly: (1) the paper lacks significant contribution(s), (2) experiments do not support the authors\u2019 claims. However, there are additional issues that I will discuss below. Firstly, point (1): \n\n- A main motivation of this paper (motivation 2) is rather weak. A video camera available in even the cheapest smartphone is not \u201ccomplex sensory devices working together seamlessly\u201d. Other modalities, like text (unexplored in this paper) can be acquired via free open-source transcription models. There are many instances where multi-modal data is expensive and/or hard to obtain, but they do not tackle such use cases. I feel it would be better to focus the narrative on resource constraints in deployment settings (such as handling sensor failures). \nThey propose a framework that ideally maps from m training modalities to n inferences modalities, where n<m, but they focus on audio-visual training with only audio during inference time. It is unclear how MUTUD could generalise to other settings, when they do not even consider the reverse (only video at inference time). This is particularly true given their extensive usage of loss functions, which seem very task specific.  \n- The methods section is extremely short, and the majority of the work in this section is not the authors\u2019 work. The largest section, about the codebooks, is simply defining what a codebook is. There is no additional contribution, and they do not cite the original work, -- it is unclear what the contribution of this section is. Similarly, the loss functions are not their work but are uncited. I believe the overall technique is unique, but a simple, direct application of others work is not a significant enough contribution to meet the requirements of ICLR.  \n\nPoint (2): \n\n- They compare to two methods: one adapted audio method from 2019, and one adapted audio-visual method from 2023. Experiments and resulting claims should be backed up by extensive experimentation that compares to many baselines. A 5 year old model that has been adapted is not sufficient to meet the requirements of ICLR. \n- There is no empirical justification as to why estimating visual features from audio features is better than simply using only the audio features. In order to support this claim, their codebook methodology should be used in the audio only setting, the audio-visual setting, and the audio-visual setting with only audio at inference time. Comparing to other models means it's impossible to tell if the improvements are due to the audio-visual training, or simply because codebooks solve the task better.  \n- They only compare to adapted baselines. The tasks, such as AVSR and AVSE, are very common and well explored in the literature. They should compare to a list of contemporary methods, at least 6 contemporary audio-visual/audio only methods in total (i.e. 3 audio only, 3 audio visual). \n- The number of parameters and MACs are interesting metrics, but have very little influence on inference speed. A large part of the paper\u2019s narrative is discussing efficiency, yet none of the tables contain inference times. In modern deep learning, the biggest challenges are inference times and memory, reducing arbitrary values has little real-world utility. \n- They do not break down specifically how the parameters and MACs are allocated, for example, how many MACs/Params for the audio code book, the video codebook, training size, evaluation size (on 1 modality).  \n- The tables are not well labelled, it is often unclear (without digging through paragraphs of experimental detail) what the evaluation task is for each table, on which dataset."
            },
            "questions": {
                "value": "You have audio to video but no video to audio, is this not included in the model? \nFrom my understanding, codebooks are slow unless a transformer or similar architecture is applied in order to efficiently assign indexes in the codebook (i.e. https://arxiv.org/abs/2210.13438). Can you provide inference times in your tables?\n\nPlease additionally address the comments in the weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}