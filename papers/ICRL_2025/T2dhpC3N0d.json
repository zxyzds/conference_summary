{
    "id": "T2dhpC3N0d",
    "title": "ER2Score: An Explainable and Customizable Metric for Assessing Radiology Reports with LLM-based Rewards",
    "abstract": "In recent years, the automated generation of radiology reports (R2Gen) has seen considerable growth, introducing new challenges in evaluation due to its complex nature. Traditional metrics often fail to provide accurate evaluations due to their reliance on rigid word-matching techniques or their exclusive focus on pathological entities, leading to inconsistencies with human assessments. To bridge this gap, we introduce ER2Score, an automatic evaluation metric designed specifically for R2Gen that harnesses the capabilities of Large Language Models (LLMs). Our metric leverages a reward model and a tailored design for training data, allowing customization of evaluation criteria based on user-defined needs. It not only scores reports according to user-specified criteria but also provides detailed sub-scores, enhancing interpretability and allowing users to adjust the criteria between clinical and linguistic aspects of reports. Leveraging GPT-4, we generate extensive evaluation data for training based on two different scoring systems, respectively, including reports of varying quality alongside corresponding scores. These GPT-generated reports are then paired as accepted and rejected samples to train an LLM towards a reward model, which assigns higher rewards to the report with high quality. Our proposed loss function enables this model to simultaneously output multiple individual rewards corresponding to the number of evaluation criteria, with their summation as our final ER2Score. Our experiments demonstrate ER2Score's heightened correlation with human judgments and superior performance in model selection compared to traditional metrics. Notably, our model's capability to provide not only a single overall score but also scores for individual evaluation items enhances the interpretability of the assessment results. We also showcase the flexible training of our model to varying evaluation systems. We will release the code on GitHub.",
    "keywords": [
        "Radiology Report Generation",
        "Auto Evaluation Metrics",
        "Reward Model",
        "LLM",
        "RLHF"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=T2dhpC3N0d",
    "pdf_link": "https://openreview.net/pdf?id=T2dhpC3N0d",
    "comments": [
        {
            "withdrawal_confirmation": {
                "value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."
            },
            "comment": {
                "value": "We disagree with the reviewers' comments and have submitted responses addressing our perspective. At this time, we have decided to formally withdraw our paper from consideration."
            }
        },
        {
            "title": {
                "value": "A general comment"
            },
            "comment": {
                "value": "Among my numerous paper submission experiences, this is the first time that I received scores of 1 and 2 for the presentation, possibly due to we are working on an unconventional research topic: developing human-consistent evaluation metrics for automated medical report generation methods. Hopefully, ICLR can better match reviewers next time."
            }
        },
        {
            "comment": {
                "value": "Thanks for your feedback, to address your concerns, we have outlined the following points:\n\nWe have to point out our approach represents a significant improvement in providing human-consistent evaluation metric for R2Gen task: Our Kendall Tau's correlation improves from 0.63 (the second best) to 0.75, and our Spearman correlation improves from 0.81 (the second best) to 0.91 on ReXVal dataset.  Please note that, to advance R2Gen research, it is essential to develop evaluation metrics that are highly correlated with human assessments. Currently, many methods still focus on improving existing non-trainable metrics, but these metrics often have weak correlation with human evaluations. Relying on them as a guide for improvement may misdirect R2Gen research. Therefore, constructing metrics closely aligned with human judgment not only enables more accurate quality assessment of generated reports but also provides more effective guidance for research and model development in this field. Meanwhile, this is a very challenging task. We cannot hope to get a 100% correlated scoring system as humans in a single shot, but this research has to start from somewhere and we have demonstrated promising solutions. \n\nQuestions:\n\n1. Please note that the dataset of 50 samples is only used for the sanity-check of the training data; the validation of our model is conducted on two datasets with distinct criteria, comparable (indeed more than) to the scale of the existing works in this domain.\n\n2. Each predicted report and its corresponding ground truth report must be combined as input for the evaluation process, as all evaluation metrics require both. The ground truth report serves as the reference for assessing the accuracy and quality of the predicted report. \n\n3. The Rad-100 dataset has unique 100 ground truth reports.\n\n4. The \"Total\" here represents the correlation of the report's overall score between the metrics and the human-rated total score. In contrast, the \"Individual\" scores indicate the correlation between each specific criterion\u2019s score from the model and the corresponding criterion score given by humans. While the individual scores themselves are summed to produce the total score, the correlations (Total and Individual) do not have an additive relationship\u2014only the scores do.\n\n5. Yes, for table 2, all other NLG metrics need both generated report and ground truth report to calculate the score"
            }
        },
        {
            "comment": {
                "value": "Thank you for the feedback\n\n1.\nThere are no such datasets publicly available. We would like to emphasize the scale of the dataset involved in our work is at the same scale of relevant works in the literature. \n\n2.\nDifferent from existing methods directly predicting the overall score only, our method predicts the subscores and adds them to calculate the overall score. This indeed improves the model generalization capacity, reducing the risk of overfitting to data, because it is a more challenging task to get both the subscores and the overall score to be highly correlated to human ratings. Meanwhile, using our current training dataset, our model has significantly outperformed existing evaluation metrics in correlating to human ratings.\n\n3. \nPlease note that we used zero-shot prompting to generate the training samples. It is reasonable to expect that incorporating few-shot prompts could further enhance our model's performance, as generally agreed in this field.\n\n4. Please note that the test dataset ReXVal involves 6 human raters and the ground-truth scores are based on inter-human agreement.\n\nQuestions. \n1. Please note that the ground-truth reports are taken from MIMIC-CXR, while the quality of the reports is controlled by the dataset. We use the same evaluation datasets as existing comparable methods in this domain.\n\n2. About computational cost: The conventional NLG metrics and the existing clinic-scores do not need training, however, they correlate poorly with human ratings and they have no ways to customize to different evaluation criteria. These are the challenges that this work wants to address.  Compared with other trainable evaluation metrics like GREEN, our model is computationally very efficient: GREEN\u2019s inference time is 1.06 seconds per sample, while ours is only 0.04 seconds per sample. GREEN has a much higher training cost: it was trained on 8x NVIDIA A100 Tensor Core GPUs with 40GB VRAM each, using a batch size of 2,048 for 12 epochs. In contrast, our model was trained on a single A6000 GPU with 40GB VRAM, using a batch size of 6 for 4 epochs.\n\n3. Our training loss curve converges on the two tasks."
            }
        },
        {
            "comment": {
                "value": "Thanks for your feedback\nPlease note that, among your suggested methods, FineRadScore and G-Rad are not formally published methods and they are not directly comparable to our method, as they rely on online LLMs, which raises privacy issues and is inviable for practical use. In contrast, our model is an offline model.\n\nCompared with GREEN, our method has multiple advantages. First, our Kendall\u2019s tau score is higher than GREEN\u2019s (Ours: 0.751, GREEN: 0.640). GREEN adds interpretability through free-text analysis but sacrifices scoring accuracy due to the task complexity. Furthermore, GREEN\u2019s fine-tuning of LLMs lacks a dedicated loss function as ours, limiting its sensitivity to nuanced quality differences. Second,  our inference speed is significantly faster. GREEN\u2019s inference time is 1.06 seconds per sample, while ours is only 0.04 seconds per sample. Third, GREEN has a much higher training cost: it was trained on 8x NVIDIA A100 Tensor Core GPUs with 40GB VRAM each, using a batch size of 2,048 for 12 epochs. In contrast, our model was trained on a single A6000 GPU with 40GB VRAM, using a batch size of 6 for 4 epochs.\n\nAs mentioned above, FineRadScore is not comparable to our model, as it only leverages the capabilities of large models (GPT4 and Claude-3 ) without fine-tuning, and scores each report by comparing sentences individually. Notably, it cannot provide an offline model to cater for the privacy concerns for evaluation. \n\nAs mentioned above, similar to FineRadScore, G-Rad relies solely on an online GPT model as its evaluator, which is both costly and requires online access."
            }
        },
        {
            "comment": {
                "value": "Thank you for the feedback, \n1. \nFirst, we\u2019d like to emphasize that the dataset scale in our work is comparable to that of comparable studies in the literature. Evaluating the scoring system's correlation with human ratings requires ground-truth scores from radiologists, which naturally limits the size of the test datasets. Nevertheless, we have evaluated two datasets, including the public ReXVal dataset, using two different scoring criteria, a notable advancement over existing works that rely solely on ReXVal and test only one criterion (e.g., RadCLiQ or MRScore). Could you please provide any other examples that cover more criteria or datasets than ours?\n\n\\\n2. Please note that RadCLiQ and MRScore are currently the only available scoring systems explicitly provided in the literature of relevant works. Also note that, no existing methods have evaluated both scoring systems as we did. If additional systems were available, we would apply our method to them as well. By using two distinct scoring systems, we show that our model can be easily customized to various evaluation criteria. \n\nWhile our model requires retraining to adapt to new scoring systems, this represents a big leap compared to existing NLG metrics and clinical scores, which have NO WAY to adapt to different evaluation standards.\n\n\n3. \nIn Table 1, although the correlation values vary across different sub-criteria, they are all statistically significant, and the overall score, which is the addition of all sub-scores, shows very high correlation. The variation of correlation values may be related to the distribution of training samples according to the sub-criteria. For example, in MIMIC-CXR, there are only a small amount of reports that have clinic history records, which may contribute to the relatively low correlation of the \u201comission of comparison that notes a change from a previous study.\n\n4. \nPlease note that the margin is NOT a hyperparameter, but was calculated using the difference in the scores of the training pairs (i.e., accepted and rejected reports) generated by GPT-4 . If we do not use margin (i.e., set margin = 0), the Kendall-tau correlation will drop from 0.751 to 0.731 and the Spearman correlation will drop from 0.910 to 0.895."
            }
        },
        {
            "comment": {
                "value": "Thank you for your feedback. To address your concerns, we have outlined the following points:\n\n1. Our model is significantly different from RadCliQ and MRsore. \n\nFirst, neither method can conduct fine-grained evaluation, e.g.,  by combining sub-criteria, we can clearly identify the reasons for a report\u2019s poor quality, e.g., whether due to incorrect lesion location, incorrect severity of findings, or omission of findings, which is not achievable by either RadCliQ or MRScore.  Notably, simultaneously predicting subscores accurately is a more challenging task requiring a carefully designed optimization process. In contrast, RadCliQ combines non-trainable scores linearly, offering limited improvement and flexibility, while MRScore can only predict an overall score. Even for the overall score, we improved the Kendall's Tau from 0.63 (the second best) to 0.75 and Spearsman correlation from 0.81 (the second best) to 0.91 on the public ReXVal dataset. That is a significant improvement in any means.\n\nSecondly, we design completely different loss functions from existing LLM-based scoring systems, beyond enabling subscore evaluation. \n\nThirdly, it is the first time that an LLM-based R2Gen scoring system can demonstrate its adaptability across different evaluation criteria. \n\nMoreover, as for \"the customization feature highlighted as a key contribution lacks sufficient experimental support, with limited results in Table 2 that do not fully demonstrate its claimed flexibility\", please note that Table 2 alone is not used to demonstrate the flexibility of customization feature!!! The customization feature is demonstrated by our experiments on two datasets with different scoring systems (results presented in Table 2 and Table 4).\n\n2. To score existing R2Gen models, we need to access the reports generated by these methods, which are absent in most cases. Also, although this is an interesting work, it totally goes beyond the scope of this paper, which focuses on designing a new evaluation method and comprehensively evaluating its performance."
            }
        },
        {
            "summary": {
                "value": "This study proposes ER2Score, an evaluation framework for automated radiology report generation, grounded in the RadCliQ and MRScore scoring systems. To address the limitations of traditional metrics, ER2Score leverages pairs of \u201caccepted\u201d and \u201crejected\u201d reports with corresponding scores obtained from GPT-4. These pairs are then used to train a Llama3 model, allowing it to assign quality scores to individual reports during inference. ER2Score demonstrates the potential to capture the semantic content and clinical significance of radiology reports, achieving a high correlation score with human evaluations. The authors suggest that this method offers a more fine-grained assessment compared to existing evaluation approaches. \n\nHowever, while ER2Score builds on RadCliQ and MRScore, the improvements remain incremental, potentially limiting its overall contribution."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Development of a Model with Strong Human Alignment: ER2Score demonstrates high alignment with human evaluations by employing GPT-4 to generate high- and low-quality report samples for a large set of ground truth reports, with GPT-4 assigning scores based on the RadCliQ and MRScore evaluation criteria. These scored pairs are then used as input for training Llama3, introducing a structured approach to calculating both total and individual scores, resulting in a model that closely aligns with human assessments.\n\n2. Rad-100 Dataset for Comprehensive Evaluation: The Rad-100 dataset, comprising 100 reports scored by an experienced radiologist following the MRScore framework, was created to provide additional validation of ER2Score\u2019s effectiveness."
            },
            "weaknesses": {
                "value": "1. This study builds on RadCliQ and MRScore, yet the improvements are minimal, with only slight modifications to the loss function, making it difficult to view ER2Score as a truly fine-grained evaluation metric. Additionally, the customization feature highlighted as a key contribution lacks sufficient experimental support, with limited results in Table 2 that do not fully demonstrate its claimed flexibility.\n\n2. The study would benefit from evaluating state-of-the-art models using ER2Score, providing further evidence of its practical advantages beyond metric-to-metric comparisons."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces ER2Score to automated radiology report generation evaluation. \nER2Score utilizes a reward model based on GPT-4, for more nuanced and human-aligned evaluations, and provides sub-scores across various criteria. The authors also train a reward model using ER2score."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- ER2Score provides detailed sub-scores for specific report components, making it easier to identify deficiencies in generated reports.\n- The metric is highly agreeable with human evaluation standards"
            },
            "weaknesses": {
                "value": "- Testing is restricted to a narrow set of benchmarks on a total of 200 + 100 reports, which is quite small and introduces concerns of overfitting.\n- ER2Score bases its scoring criteria on established scoring systems like RadCliQ and MRScore, which are limited in scope and may not fully encompass the complexity of radiological reporting. The model lacks flexibility to adapt to other or emergent scoring needs without significant retraining.\n- ER2Score's accuracy varies across sub-criteria, performing well on some (e.g., omission of findings) but poorly on others (e.g., location or position accuracy). More in-depth analysis would be interesting to see why this is the case.\n- Margin-based scoring criteria for distinguishing between accepted and rejected reports is interesting, but lacks an in-depth analysis of the impact of margin size on model sensitivity. Margin thresholds should be rigorously analyzed for borderline cases."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the limitations of traditional metrics for evaluating automated RRG and introduces ER2Score, a metric based on LLMs. ER2Score attempts to improve evaluation accuracy by using a reward model that can be tailored to user-defined criteria and provides detailed sub-scores for enhanced interpretability."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Many experiments and ablation studies. Nicely presented paper with clear figures and methodology. I commend the authors for doing many experiments and evaluation studies to validate their metric."
            },
            "weaknesses": {
                "value": "It seems like the candidate metrics selected for comparison are not the most recent ones or relevant ones, especially given ER2Score's LLM-based nature. It is expected for ER2Score to outperform the traditional lexical/semantic/factuality metrics; however, I cannot judge on its performance without comparisons to other more recent LLM-based metrics for RRG.\n\nI recommend the authors do experiments with GREEN (https://arxiv.org/html/2405.03595v1), FineRadScore (https://arxiv.org/html/2405.20613v2), and G-Rad (https://arxiv.org/html/2403.08002v2), as these are more relevant. They also use similar ReXVal dataset and error counts, as well as correlation evaluation with Kendall's Tau."
            },
            "questions": {
                "value": "Recommend authors do further comparison and validation studies with the suggested metrics to fully demonstrate/prove that ER2Score is superior."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces ER2Score, a novel metric for evaluating automated radiology report generation. ER2Score leverages LLMs, specifically GPT-4 and Llama 3, to address the limitations of existing metrics that rely on n-gram overlap or predefined clinical entities. \n\nGPT-4 generates training data by scoring synthetically generated reports against reference reports based on predefined customizable criteria (RadCliQ and MRScore). Llama 3 is then LoRA fine-tuned as a reward model to predict sub-scores for each criterion, which are summed to produce the final ER2Score. The authors claimed ER2Score's improved correlation with human judgments and superior model selection capabilities compared to traditional metrics on two datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The generation of sub-scores for individual evaluation criteria offers valuable insights into report quality, potentially better interpretability, and potentially enabling targeted improvements.\n\nThe ability to train the metric with different scoring systems (RadCliQ and MRScore) demonstrates flexibility and adaptability to diverse evaluation needs."
            },
            "weaknesses": {
                "value": "The relatively small size of the evaluation datasets (ReXVal and Rad-100) raises concerns about the generalizability of the results. Larger-scale evaluations are needed to validate the robustness of ER2Score.\n\nRelying on GPT-4 generating training data, certain combinations of sub-scores might be rare, leading to data sparsity issues. This can hinder the model's ability to learn effectively and generalize to unseen examples, affecting convergence. Also, introduce a performance cap. \n\nThe supplementary material provides example prompts, but more details on the prompt engineering process and its impact on GPT-4's scoring would be beneficial. How sensitive is the performance to prompt variations?\n\nLacking inter-human agreement."
            },
            "questions": {
                "value": "How sensitive is ER2Score to the quality of the ground truth reports used for training and evaluation?\n\nWhat are the computational costs associated with training and using ER2Score? How does it compare to other metrics?\n\nWhat are the limitations of using reward modeling for this task? Is the reward guaranteed to converge due to the multi-dimensionality of the grading criteria? \n\nOther: \n\nline 77, citation (Meta, 2024) missing space afterward.\n\nline 142, 143 \"Kendall's tau of 0.735\" - needs to be consistent capitalization: \"Kendall's Tau.\"\n\nReally inconsistent capitalization on Dataset: for example, line 288 \"ReXVal Dataset\" but \"MIMIC dataset\" on following lines.\n\nVarious instances: Inconsistent capitalization of \"RadCliQ\" and \"MRScore.\"\n\nAnd many awkward phrasings. Careful proofreading and editing are required going further."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a method for training language models to evaluate generated radiology reports across a variety of metrics. The authors create a dataset of generated radiology reports of varying quality and train a reward model to assign scores to radiology reports using this \u201cground-truth\u201d data, producing fine-grained evaluations across a range of relevant sub-categories. The authors find that their approach produces results that align better with radiologists than other common approaches. As new approaches to automated radiology report generation are developed, this approach could be an effective method of generating quick feedback on the effectiveness of these approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The approach is well motivated taking an approach to evaluation that can score reports across a range of clinically valid dimensions. The described loss function seems mathematically sound with different terms well justified and the usage of human evaluation when scoring the method is important in validating the approach beyond automated metrics."
            },
            "weaknesses": {
                "value": "Despite the method outperforming other approaches, the final performance of ER2Score still seems fairly low. The manuscript needs more justification in light of this low performance to justify why this method will still be useful. It would also be useful to study the ReXVal criteria where performance was particularly low in more detail, to understand why agreement with radiologists might have been poor.\n\nAdditionally, there were areas where the approach was unclear and requires more clarification. Please see the questions."
            },
            "questions": {
                "value": "* The sample-size of 50 used to validate/spot-check the model generated radiology reports is fairly low. Did the authors perform any kind of power analysis to verify that this is a sufficiently large sample to validate their automated report generation process?\n* I\u2019m not sure I fully understand the pairing rule. From figure 2 it appears that each report in a predicted report pair was also paired with the ground-truth report? Is this the case or did a pair simply consist of 2 predicted reports? If the former then what was the reason for also providing the ground-truth report?\n* For the Rad-100 dataset was there a 1-to-1 correspondence between a report in the dataset and ground-truth MIMIC report it was based on? Or are there several reports based on a single report?\n* In Table 1 what does Total mean? These values seem very different to the individual criteria score\n* How are our NLG metrics computed for table 2? Are these based on BLEU/ROUGE between the generated report and a ground-truth report?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}