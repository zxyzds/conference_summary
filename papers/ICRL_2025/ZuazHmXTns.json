{
    "id": "ZuazHmXTns",
    "title": "Problem-Parameter Free Federated Learning",
    "abstract": "Federated learning (FL) has garnered significant attention from academia and industry in recent years due to its advantages in data privacy, scalability, and communication efficiency. However, current FL algorithms face a critical limitation: their performance heavily depends on meticulously tuned hyperparameters, particularly the learning rate or stepsize. This manual tuning process is challenging in federated settings due to data heterogeneity and limited accessibility of local datasets. Consequently, the reliance on problem-specific parameters hinders the widespread adoption of FL and potentially compromises its performance in dynamic or diverse environments. To address this issue, we introduce PAdaMFed, a novel algorithm for nonconvex FL that carefully combines adaptive stepsize and momentum techniques. PAdaMFed offers two key advantages: 1) it operates autonomously without relying on problem-specific parameters, making it, to our knowledge, the first FL algorithm to achieve such problem-parameter-agnostic adaptation; and 2) it manages data heterogeneity and partial participation without requiring heterogeneity bounds. Despite these benefits, PAdaMFed provides several strong theoretical guarantees: 1) It achieves state-of-the-art convergence rates with a sample complexity of $\\mathcal{O}(\\epsilon^{-4})$ and communication complexity of $\\mathcal{O}(\\epsilon^{-3})$, even using constant learning rates; 2) these complexities can be improved to the best-known $\\mathcal{O}(\\epsilon^{-3})$ for sampling and $\\mathcal{O}(\\epsilon^{-2})$ for communication when incorporating variance reduction; 3) it exhibits linear speedup with respect to the number of local update steps and participating clients at each global round. These attributes make PAdaMFed highly scalable and adaptable for various real-world FL applications. Extensive empirical evidence on both image classification and sentiment analysis tasks validates the efficacy of our approaches.",
    "keywords": [
        "Adaptive federated learning",
        "problem-parameter free",
        "arbitrary data heterogeneity",
        "adaptive stepsize"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ZuazHmXTns",
    "pdf_link": "https://openreview.net/pdf?id=ZuazHmXTns",
    "comments": [
        {
            "title": {
                "value": "Point-to-Point Responses"
            },
            "comment": {
                "value": "**We sincerely appreciate the reviewer for recognizing our contributions and for the constructive comments. Our point-to-point responses to concerns on Weaknesses and Questions are given below.**\n\n**Reply to Weakness 1:**  Thank you for the comment. While we appreciate the observation regarding SCAFFOLD, we respectfully disagree that PAdaMFed is merely a direct extension with momentum. Our work has a fundamentally different objective and makes distinct technical contributions:\n\n1) Our primary goal is to design a truly parameter-free federated learning algorithm that eliminates dependency on all problem-specific parameters while maintaining state-of-the-art convergence. This is achieved through the careful integration of three essential components:\n- Local gradient normalization for adaptive step-sizes;\n- Client-side momentum for heterogeneity bounding;\n- Control variates for \"client-drift\" control.\n\n2) The theoretical analysis of our algorithm follows a significantly different path from SCAFFOLD's proof technique. Our analysis requires novel theoretical tools to handle the interplay between gradient normalization, momentum, and control variates, while establishing convergence guarantees without requiring knowledge of problem parameters.\n\nWhile both algorithms use control variates, PAdaMFed's parameter-free nature and incorporation of adaptive mechanisms represent fundamental innovations rather than incremental extensions. Notably, our algorithms also eliminate the need for data heterogeneity bounds, further broadening their applicability.\n\n\n**Reply to Weakness 2 and Question 1:** Thank you for the comment. The proof of our algorithm presents several unique challenges compared to SCAFFOLD's analysis: \n1) Handling normalized gradients: Our use of gradient normalization introduces new mathematical complexities. We need to carefully track how normalization affects the relationship between consecutive model updates. Special techniques are required to analyze the interaction between normalized gradients and momentum.\n2) Momentum analysis: The introduction of momentum terms creates additional coupling between iterations. We need to bound the accumulated effects of momentum across multiple updates. The interaction between momentum and control variates requires novel analysis techniques.\n3) Parameter-free guarantees: Proving convergence without relying on problem-specific parameters requires different analytical tools. We need to show that gradient normalization effectively compensates for the lack of parameter-dependent step sizes. The analysis must demonstrate that convergence holds across different problem settings without parameter tuning. \n\nThese challenges required developing new proof techniques that differ significantly from SCAFFOLD's analysis approach.\n\n**Reply to Question 2:** \n1) The gain of our PAdaMFed algorithm stems from the synergistic integration of three indispensable components: \n- Gradient normalization serves as an adaptive learning rate scheme, automatically adjusting step sizes based on the local optimization landscape. This design automatically allows larger steps in regions with small gradients (where more aggressive exploration is beneficial) and smaller steps in steep regions (where careful progress is needed). \n- Client-side momentum helps accelerate convergence while maintaining stability. First, it helps overcome local irregularities in the loss landscape by accumulating gradients over clients and iterations; Second, it accelerates progress in directions of consistent gradient agreement. \n- Furthermore, control variates align local updates with the global objective, reducing variance in gradient estimates and ensuring more consistent updates across heterogeneous client data. \n\nCollectively, these techniques ensure that the stepsizes are independent of problem-specific parameters such as smoothness parameters and heterogeneity bounds, simplifying the tuning process and enhancing robustness across diverse FL applications. \n\n\n2) We have provided a detailed comparison in terms of the convergence rate/communication complexity between PAdaMFed, PAdaMFed-VR, the SCAFFOLD algorithm and related algorithms as the Table 2 in the SCAFFOLD paper."
            }
        },
        {
            "title": {
                "value": "Point-to-Point Responses"
            },
            "comment": {
                "value": "**Reply to Weakness 4:** We appreciate these thoughtful suggestions about the experimental evaluation. Our responses to each point of the comment is listed below:\n\n1) Regarding FedAvg's performance on EMNIST: Our results are consistent with previous studies in the literature. For instance,  references [1] and [2] reported similar convergence rates for FedAvg under comparable settings. The performance of \nWe emphasize that all baseline algorithms were implemented under identical experimental conditions as PAdaMFed to ensure fair comparison.\n2) We will enhance the experimental section in the revised version by:\n- Including additional experiments on CIFAR-10 dataset;\n- Conducting comprehensive ablation studies to analyze the individual contributions of adaptive stepsizes and momentum;\n- Providing detailed analysis of how each component affects the overall performance.\n3) To ensure reproducibility and facilitate further research, we will release our code on GitHub upon acceptance.\n\n[1] J. Pei, W. Li and S. Mumtaz, \"From Routine to Reflection: Pruning Neural Networks in Communication-efficient Federated Learning,\" in IEEE Transactions on Artificial Intelligence, 2024, doi: 10.1109/TAI.2024.3462300. \n\n[2] V. S. Mai, R. J. La, T. Zhang, Y. Huang and A. Battou, \"Federated Learning With Server Learning for Non-IID Data,\" 2023 57th Annual Conference on Information Sciences and Systems (CISS), Baltimore, MD, USA, 2023, pp. 1-6.\n\n**Reply to the following comment in the Weaknesses:**\n>Overall, this paper targets a meaningful problem and makes a substantial contribution to algorithm design in federated learning, supported by theoretical rigor and empirical evidence. However, its novelty may be limited by reliance on existing algorithmic techniques and proof strategies (e.g., variance reduction and SCAFFOLD).\n\n**Reply:**  We sincerely appreciate the reviewer for recognizing our contributions. However, we would like to emphasize that our algorithm introduces significant innovations aimed at overcoming specific challenges in federated learning.\n\n1) Our primary goal is to design a truly parameter-free federated learning algorithm that eliminates dependency on all problem-specific parameters while maintaining state-of-the-art convergence. This is achieved through the careful integration of three essential components:\n- Local gradient normalization for adaptive step-sizes;\n- Client-side momentum for heterogeneity bounding;\n- Control variates for \"client-drift\" control.\n\n2) The theoretical analysis of our algorithm follows a significantly different path from SCAFFOLD's proof technique. Our analysis requires novel theoretical tools to handle the interplay between gradient normalization, momentum, and control variates, while establishing convergence guarantees without requiring knowledge of problem parameters.\n\nWe believe these contributions advance the field by providing a more robust and adaptable solution to federated learning challenges. We appreciate your recognition of the meaningful problem we address and the substantial contributions we make.\n\n\n**Reply to Question 1:** Thank you for this question. Our answer can refer to the Reply to Weakness 4.\n\n**Reply to Question 2:**  Thank you for the valuable suggestion regarding an ablation study. We agree that isolating the contributions of adaptive stepsizes and momentum can provide deeper insights into their individual impacts on the performance of PAdaMFed. We will conduct an ablation study.\n\n**Thank you once again for your thoughtful review and constructive feedback.**"
            }
        },
        {
            "title": {
                "value": "Point-to-Point Responses"
            },
            "comment": {
                "value": "**We sincerely appreciate the reviewer for recognizing our contributions and for the constructive comments. Our point-to-point responses to concerns on Weaknesses and Questions are given below.**\n\n**Reply to Weakness 1:**  Thank you for this valuable suggestion. We have added more explanations in Section 4.2 of our paper on how our algorithm achieves hyperparameter independence. The reviewer is right that the gradient normalization in Step 8 is indeed the key mechanism enabling our algorithm to operate without problem-specific parameters. This gradient normalization serves as an adaptive learning rate scheme, automatically adjusting step sizes based on the local optimization geometry \u2014 taking larger steps when gradients are small and smaller steps when gradients are large. It also provides us the convenience on quantifying the distance between consecutive models in our theoretical analysis, maintaining that $||\\theta_i^{t, k+1}-\\theta_i^{t,k}||= \\eta $ for all $i, k, t$. \n\nMomentum and control variables are also indispensable to achieve the satisfactory performance of our algorithm. Momentum helps accelerate convergence while maintaining stability. First, it helps overcome local irregularities in the loss landscape by accumulating gradients over clients and iterations; Second, it accelerates progress in directions of consistent gradient agreement. Control variates align local updates with the global objective, reducing variance in gradient estimates and ensuring more consistent updates across heterogeneous client data. Collectively, these techniques ensure that the stepsizes are independent of problem-specific parameters such as smoothness parameters and heterogeneity bounds, simplifying the tuning process and enhancing robustness across diverse FL applications.\n\n**Reply to Weakness 2:**\n\n\n**Reply to Weakness 3:** Thank you for the comment. The strong performance of our algorithm can be attributed to two key design elements that provide inherent adaptivity:\n\n1) Gradient normalization: Gradient normalization serves as an adaptive learning rate scheme, automatically adjusting step sizes based on the local optimization landscape. This design automatically allows larger steps in regions with small gradients (where more aggressive exploration is beneficial) and smaller steps in steep regions (where careful progress is needed). This per-step adaptivity can potentially achieve better performance than fixed learning rates, even when the latter are optimized through grid search.\n\n2) Momentum: Our momentum-based update design helps accelerate convergence while maintaining stability. The momentum term provides two key benefits: it helps overcome local irregularities in the loss landscape by accumulating gradients over multiple iterations, and it accelerates progress in directions of consistent gradient agreement.\n\nThese adaptive mechanisms enable our algorithm to achieve strong performance without manual tuning, as they continuously optimize the learning dynamics throughout the training process.\n\n\n\n**Reply to Question 1:** Thank you for the question. The use of momentum helps eliminate the requirement for data heterogeneity bounds. At each global round, the momentum is updated as follows:\n$$g^{t} = \\frac{\\beta}{SK}\\sum_{i\\in S_t}(\\sum_{k=1}^K\\nabla F(\\theta_i^{t,k};\\xi_i^{t,k})-\\mathbf{c}_i^{t-1})+\\beta\\mathbf{c}^{t-1}+(1-\\beta)g^{t-1}.$$\nThis accumulates the gradient directions across clients and iterations, providing an \"anchoring\" effect that effectively mitigates the \u201cclient-drift\u201d phenomenon. In the extreme case where $\\beta=0$, all clients remain synchronized in their local updates, eliminating the drift caused by data heterogeneity in the vanilla FEDAVG. By appropriately choosing the coefficient $\\beta$, our algorithm achieves state-of-the-art convergence while removing the necessity for data heterogeneity assumptions.\n\n**Thank you once again for your thoughtful review and constructive feedback.**"
            }
        },
        {
            "title": {
                "value": "Point-to-Point Responses"
            },
            "comment": {
                "value": "**Reply to Question 4:**  Thank you for the comment.  The strong performance of our algorithm without grid search can be attributed to two key design elements that provide inherent adaptivity:\n\n1) Gradient normalization: Gradient normalization serves as an adaptive learning rate scheme, automatically adjusting step sizes based on the local optimization landscape. This design automatically allows larger steps in regions with small gradients (where more aggressive exploration is beneficial) and smaller steps in steep regions (where careful progress is needed). This per-step adaptivity can potentially achieve better performance than fixed learning rates, even when the latter are optimized through grid search.\n\n2) Momentum: Our momentum-based update design helps accelerate convergence while maintaining stability. The momentum term provides two key benefits: it helps overcome local irregularities in the loss landscape by accumulating gradients over multiple iterations, and it accelerates progress in directions of consistent gradient agreement.\n\nThese adaptive mechanisms enable our algorithm to achieve strong performance without manual tuning, as they continuously optimize the learning dynamics throughout the training process.\n\n**Reply to Question 5:**   Thank you for this valuable suggestion.  We have included the complexity bounds in Table 1 in the revised manuscript. \n\n**Thank you once again for your thoughtful review and constructive feedback.**"
            }
        },
        {
            "title": {
                "value": "Point-to-Point Responses"
            },
            "comment": {
                "value": "**We sincerely appreciate the reviewer for recognizing our contributions and for the constructive comments. Our point-to-point responses to concerns on Weaknesses and Questions are given below.**\n\n**Reply to Weakness 1:** Thank you for the comment.  To the best of our knowledge, our algorithm is the first to be truely problem-parameter-free in FL, regardless of whether diminishing or constant step sizes are employed. In our approach, the gradient normalization mechanism compensates for conservative step sizes, which provides two key benefits:\n- It automatically adjusts the effective step size based on the local optimization landscape;\n- It compensates for conservative nominal step sizes through adaptive scaling.\n\nThe effectiveness of this approach is validated by both our theoretical analysis and empirical results, where our parameter-free algorithm achieves competitive or superior performance compared to manually-tuned baselines. This demonstrates that careful algorithmic design can overcome the potential limitations of conservative step sizes. \n\n**Reply to Weakness 2:**  Thank you for the comment. \n1) We will release our complete codebase upon acceptance via GitHub,  including thorough documentation, running scripts, and configuration files. \n2) All baselines are trained using the same model architecture as our algorithm: (1) a convolutional neural network (CNN) with three convolutional layers and two fully connected layers for image classification on the EMNIST dataset, and (2) a long short-term memory (LSTM) model for textual sentiment analysis on the IMDB dataset. The gradient search ranges from 1e-1 to 1e-5. Moreover, we have presented the performance of all baseline algorithms versus stepsize in Fig 2 for the image classification task, and the test accuracy in Fig 1 of all those baselines corresponds to the best stepsizes shown in Fig. 2. \n\n**Reply to Question 1:**  Thank you for the insightful question regarding varying client participation. While our current theoretical analysis assumes a fixed number of participating clients $S$ per round for analytical tractability, our algorithm can be naturally adapted to accommodate variations in client participation. For scenarios with fluctuating client participation, we can define a variable $S_{min}$ to represent the minimum expected number of participants per round. The analysis remains valid by replacing $S$ with $S_{min}$.\n\u200b\n\n**Reply to Question 2:** Thank you for this valuable suggestion. Test accuracy indeed serves as the primary measure of generalization performance, which is a core objective in machine learning applications. While optimization metrics (e.g., gradient norm) can indicate convergence during training, they do not necessarily reflect model performance on unseen data. Nonetheless, we agree that optimization metrics provide useful insights into algorithmic behavior. In the revised version, we will include gradient norm trajectories in Fig. [X] and training loss curves in Fig. [Y].\n\n**Reply to Question 3:**  Thank you for the comment. \n\n1) The gradient normalization in Line 8 enables adaptive step-sizes of our algorithm by automatically adjusting to the local optimization landscape. While the normalization only focus on the gradient direction, our algorithm maintains competitive convergence through two complementary mechanisms:\n- The momentum term accumulates gradient information across iterations, helping to preserve magnitude-related information over time\n- The control variates help reduce variance and maintain alignment with the global objective\n\n2) It's important to note that our convergence analysis relies on the Lipschitz constant of the gradient (which is acutally the smoothness parameter $L$ in our paper). The gradient normalization, combined with our carefully designed control variates and momentum, ensures that we fully exploit the problem structure without requiring explicit knowledge of these constants. Our theoretical guarantees show that the design of our algorithm still maintains start-of-the-art convergence properties while achieving problem-parameter free operation."
            }
        },
        {
            "title": {
                "value": "Point-to-Point Responses"
            },
            "comment": {
                "value": "**We sincerely appreciate the reviewer for recognizing our contributions and for the constructive comments. Our point-to-point responses to concerns on Weaknesses and Questions are given below.**\n\n**Reply to Weakness 1:**  We appreciate this thoughtful question about the \"problem-parameter free\" property. Let us clarify two key points:\n\n1) The term \"problem-parameter free\" refers to algorithms that do not require problem-specific parameters (such as smoothness constant $L$) for tuning in practice. In our algorithm, the learning rates are explicitly determined by only system-defined constants: the number of participating clients $S$, local update iterations $K$, and communication rounds $T$. These are configuration parameters that are known and set by the system administrator, rather than problem-specific parameters that would need to be estimated or tuned.\n\n2) While our theoretical analysis includes assumptions involving the smoothness constant $L$ (e.g., Assumption 1), these assumptions are standard regularity conditions that establish the problem setting, rather than parameters needed for algorithm execution. Importantly, $L$ remains independent of both $K$ and $T$, and knowledge of $L$ is not required to run the algorithm. The practical implementation of PAdaMFed depends only on the system-defined constants ($S, K, T$), making it truly parameter-free from an operational perspective. For reference, our Assumption 1 is cited below:\n\n>**Assumption 1.** *Given any $\\xi$, the sample-wise loss function $F(\\boldsymbol\\theta; \\xi)$ is $L$-smooth, i.e., $||\\nabla F(\\boldsymbol\\theta; \\xi)-\\nabla F(\\boldsymbol\\delta; \\xi)|| \\leq L||\\boldsymbol\\theta-\\boldsymbol\\delta||$ for all $\\boldsymbol\\theta,\\boldsymbol\\delta \\in \\mathbb{R}^{d}$.*\n\n**Reply to Weakness 2:**  Thank you for the comment. Our algorithm achieves parameter-free operation through careful algorithmic design that incorporates automatic adaptivity. Specifically, the gradient normalization mechanism in Step 8 serves as the key adaptive component:\n\n1) The normalization automatically adjusts step sizes based on the local optimization landscape, effectively providing adaptive learning rates without requiring manual tuning. This design ensures:\n- Larger steps in regions with small gradients where more aggressive exploration is beneficial;\n- Smaller steps in steep regions where careful progress is needed;\n- Uniform update magnitudes across clients, preventing any single client from having disproportionate influence.\n\n2) This adaptive mechanism, combined with momentum and control variates, creates a truly self-tuning system where:\n- Update magnitudes are automatically calibrated based on local gradients;\n- The effective step size becomes proportional to the ratio between learning rate and gradient norm;\n- Client heterogeneity is naturally handled without requiring explicit bounds or parameters.\n\nOur approach thus achieves parameter-free operation through algorithmic adaptivity rather than manual tuning, making it robust across diverse federated learning scenarios.\n\n**Reply to Weakness 3:**  We appreciate the concern about communication efficiency. We would like to clarify the actual communication overhead of our algorithm:\n\nUplink (client \u2192 server): Each client transmits two vectors per round: $\\boldsymbol\\theta_i^{t,K}$ and $\\boldsymbol{c}_i^t$.\n\nDownlink (server \u2192 clients): Originally appears to require three vectors: $\\boldsymbol\\theta^t$, $\\boldsymbol{c}^t$, and $\\boldsymbol{g}^t$. However, we can optimize this by combining $\\boldsymbol{c}^t$ and $\\boldsymbol{g}^t$ into a single term $\\beta\\boldsymbol{c}^t + (1-\\beta)\\boldsymbol{g}^t$. This optimization reduces downlink communication to two vectors.\n\nWith this implementation, our algorithm achieves the same communication overhead as SCAFFOLD, making it communication-efficient while maintaining its parameter-free advantages."
            }
        },
        {
            "summary": {
                "value": "This paper proposed a new federated learning algorithm called PAdaMFed that is problem-parameter free. The main idea is to combine SCAFFLOD and momentum. The authors also designed a modified version of PAdaMFed to reduce the variance, called PAdaMFed-VR  The authors showed that PAdaMFed-VR can achieve state-of-the-art convergence performance. The performance of PAdaMFed and PAdaMFed-VR is also verified using experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed PAdaMFed is independent of the problem parameters such as gradient divergence. \n\n2. The author also designed PAdaMFed-VR to reduce the variance of the federated learning algorithm.\n\n3. The authors showed the convergence upper bound of PAdaMFed and PAdaMFed-VR analytically."
            },
            "weaknesses": {
                "value": "1. The idea of PAdaMFed is a direct extension of SCAFFOLD with momentum considered at each client. \n\n2. The challenge in the proof is unclear."
            },
            "questions": {
                "value": "1. What are the challenges in the proof compare to the proof in the SCAFFOLD algorithm?\n\n2. The assumptions used in this paper are similar to those used in proving the convergence rate of the SCAFFOLD algorithm, so the gain in terms of convergence analysis must coming from using momentum and variance reduction. Please provide a detailed explanation on the gain of the proposed algorithms. Please provide a detailed comparison in terms of the convergence rate/communication complexity between PAdaMFed, PAdaMFed-VR, the SCAFFOLD algorithm and related algorithms as the Table 2 in the SCAFFOLD paper.\n\nKarimireddy, Sai Praneeth, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. \"Scaffold: Stochastic controlled averaging for federated learning.\" In International conference on machine learning, pp. 5132-5143. PMLR, 2020."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces PAdaMFed, a federated learning algorithm that uses momentum and adaptive learning rates to address client heterogeneity, similar to SCAFFOLD-M. The method is problem-parameter-free and does not require parameter tuning. The paper provides a convergence bound, without the standard assumption about gradient dissimilarity."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1- The paper studies important problems. Both data heterogeneity and hyperparameter tuning are existing issues for FL systems. The method has problem-independent hyperparameters which makes it more useful in practice.\n\n2- There are not many works in the literature that study problem-independent parameters; this work is one of the first.\n\n3- The convergence bound and analysis is novel, and has minimal assumptions. The authors proved convergence without assumptions about gradient dissimilarity, a standard assumption that most of the literature's works require."
            },
            "weaknesses": {
                "value": "1- The convergence bound is not optimal compared to SCAFFOLD-M. Also, for the case when  $K$ (the number of local updates) is a function of $T$ (total rounds), a fair comparison would involve the equivalent formulation for SCAFFOLD-M under the same conditions.\n\n2- Empirical results are limited. Experiments for different architectures and datasets can help improve the paper and show its effectiveness.\n\n3- Missing related work: There are some similarities between your work and [1] which is not cited. Can you compare your method and results with this work. \n\nMinor: Most of related works uses $\\|\\nabla f(x)\\|^2 \\le \\epsilon$ and not  $\\|\\nabla f(x)\\| \\le \\epsilon$ for the definition of $\\epsilon$-stationary and this formulation makes the paper a bit confusing.\n\n[1] Li, Jiaxiang, et al. \"Problem-Parameter-Free Decentralized Nonconvex Stochastic Optimization.\" arXiv preprint arXiv:2402.08821 (2024)."
            },
            "questions": {
                "value": "1- Can you recover the optimal bound with the extra assumption and different learning rates?\n\n2- Can you design an experiment in which the gradient dissimilarity assumption does not hold and other methods fail to converge?\n\nsuggestion: since $c^{t-1}$ and $g^{t-1}$ are used only as $\\beta c^{t-1} + (1- \\beta) g^{t-1}$ you can only send the weighted sum and not each one to the clients and have the same communication per round as SCAFFOLD."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an algorithm called PAdaMFed, where the hyperparameters don't depend on the problem-specific parameters such as smoothness constant, stochastic gradient variance bound, etc. PAdaMFed applies a client-drift control mechanism similar to SCAFFOLD but also uses normalized updates at the client level. It also has a momentum-based update rule at the global level. A variance-reduced version of PAdaMFed, called PAdaMFed-VR is also proposed. PAdaMFed and PAdaMFed-VR have communication complexities of $\\mathcal{O}(\\epsilon^{-3})$ and $\\mathcal{O}(\\epsilon^{-2})$, respectively, for converging to an $\\epsilon$ stationary point. The convergence results don't rely on data heterogeneity bounds, which is also interesting. Empirical results on EMNIST and IMDB (in the appendix) show the efficacy of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**1.** This paper seems to be the first one to provide an algorithm whose hyperparameters are fully independent of the problem-specific parameters such as smoothness constant, stochastic gradient variance bound, etc. However, I'm not up to speed on all the relevant literature.\n\n**2.** It is interesting that the results don't rely on any kind of heterogeneity bound. \n\n**3.** Empirical results show that the proposed method is better than SCAFFOLD."
            },
            "weaknesses": {
                "value": "**1.** It is not clear to me what exactly is enabling the algorithm to work with hyperparameters that don't depend on problem-specific parameters. This aspect should be explained better. For instance, does normalization in step 8 of Algorithm 1 enable the hyperparameters to be independent of the smoothness constant? \n\n**2.** How exactly the variation reduction step in PAdaMFed-VR leads to variance reduction should be explained more precisely. The current discussion in Section 3.2 is not very satisfactory. Is this step of PAdaMFed-VR inspired by the update rule of STORM [1]? Also since the relative weights of the gradients at $\\theta_i^{t,k}$ and $\\theta_i^{t-1}$ are different ($1$ and $1-\\beta$, respectively), it'd be helpful to have some discussion on how $\\beta$ should be chosen to reduce the variance and also retain a sufficient amount of client drift control (w/ weight $\\beta$).\n\n**3.** The empirical results show that PAdaMFed is better than SCAFFOLD even after the hyperparameters of SCAFFOLD are tuned. However, from what I understood, the derived convergence results don't indicate this. Can the authors please explain this?\n\n---\n\n[1]: Ashok Cutkosky and Francesco Orabona. \"Momentum-based variance reduction in non-convex SGD\". *Advances in neural information processing systems, 32, 2019*."
            },
            "questions": {
                "value": "**1.** Is there any intuition for why there is no requirement for any kind of heterogeneity bound for deriving the results of this paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the convergence of federated averaging algorithms with momentum. Its main contribution is to establish the convergence and convergence rate of these algorithms when the step-sizes and momentum parameters in the local and aggregation algorithms are tuned independently of problem-specific parameters, such as the Lipschitz constant of the gradients. Instead, these parameters depend solely on the number of local and global iterations and the number of nodes sampled in each iteration."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The topic is important and well motivated. It is indeed a problem that for ensuring theoretical convergence one typically needs parameters like Lipschitz constant on the gradient, which are typically not known. \n\nThe paper is well written and the results look correct, at least the main steps in the proofs look correct."
            },
            "weaknesses": {
                "value": "I have some concerns with the setup. Many algorithms already permit diminishing step-sizes that do not require knowledge of the Lipschitz constant of the gradient, such as step-decay or similar approaches, which also come with theoretical guarantees. These methods have the added advantage that step-sizes can start relatively large and decrease over time, while in this approach, the step-size is inversely based on the total number of global and local iterations, which may be large. As a result, the step-sizes in this method are consistently small. There is no comparison to such methods here.\n\nThe experiments are not reproducible, the code is not provided and there is no information about hyper parameter tuning or other details for other algorithms, except that a grid search has been used."
            },
            "questions": {
                "value": "Since the number of sampled nodes is used in the parameter selection, what to do if the number of sampled nodes changes between iterations?\n\nIn the numerical results, why only considering accuracy? I get that accuracy, or some similar Machine Learning metrics are important to evaluation for Machine Learning applications. However, all the theory is related to optimization metrics, the size of the gradient norm. It would be good to also show how the numerical results align with the theoretical results in the paper.\n\nIn line 8 of Algorithm 1, why do you take scale the gradient direction with the gradient norm? This means that you don't use the magnitude of the gradient, only the direction of the gradient. This is probably why you don't need Lipschitz constant of the gradient, even in deterministic optimziation algorithms such algorithms converge without using Lipschitz constants, but if we don't use gradient magnitude then convergence rate will be worse worse and compatible to sub-gradient methods, since we are not exploiting the smoothness.\n\nGiven that for your algorithm you did not do any grid search to tune hyperparameters, while grid-search was used for the other algorithms, I am a bit surprised how much better results you get. In my experience, the hyperparameters obtain from the theory are usually not good in practice, usually one can find much better parameters from doing grid search. Would this be the case for your work? \n\nIn Table 1, it would be good if you could also include the complexity bounds."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces PAdaMFed, a new federated learning (FL) algorithm that removes dependence on problem-specific parameters, a significant advancement for FL where hyperparameter tuning is often hindered by data heterogeneity and limited local dataset accessibility. By combining adaptive stepsizes and momentum, PAdaMFed aims to manage arbitrary data heterogeneity and partial client participation while achieving state-of-the-art convergence and communication complexities. Empirical results validate the theoretical benefits of PAdaMFed across multiple tasks, demonstrating robustness and scalability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The parameter-free design in FL addresses a critical challenge, making this an interesting and important contribution.\n2. Theoretical guarantees, including sample and communication complexities of \\(O(\\epsilon^{-4})\\) and \\(O(\\epsilon^{-3})\\) respectively for the standard version (with further improvements under variance reduction), are rigorously derived and well-supported."
            },
            "weaknesses": {
                "value": "1. The \"problem-parameter free\" claim is somewhat unclear. Although the authors suggest that PAdaMFed has no dependency on problem-specific parameters like the smoothness constant \\(L\\), it seems that implicit conditions on \\(L\\) might still affect the learning rate. For example, Assumptions A3 and A4 seem to require some condition on \\(L\\) related to \\(K\\) and \\(T\\), suggesting that the learning rate still indirectly depends on \\(L\\).\n  \n2. From my understanding, achieving a truly problem-parameter free algorithm would typically involve hyperparameter adaptivity based on problem characteristics. Simply incorporating variance reduction and momentum may not fully achieve this goal. In problem-parameter or hyper-parameter free algorithms in centralized learning, the key is to design the adaptivity of these hyper-parameters .\n\n3. The algorithm requires communication of three vectors per round, which is substantial. Since communication often constitutes a major bottleneck in FL, this increase is likely not marginal, especially given the lack of evidence provided to support this claim.\n\n4. I have concerns regarding the experiment results. The baselines, particularly FedAvg, appear to underperform, with an accuracy of only 0.8 on EMNIST IID data. The experimental section would be stronger if it included additional datasets and models, along with an ablation study to isolate the contributions of adaptive stepsizes and momentum, as proposed in the PAdaMFed, to the overall performance.\n\nOverall, this paper targets a meaningful problem and makes a substantial contribution to algorithm design in federated learning, supported by theoretical rigor and empirical evidence. However, its novelty may be limited by reliance on existing algorithmic techniques and proof strategies (e.g., variance reduction and SCAFFOLD)."
            },
            "questions": {
                "value": "1. I'd like to know the performance of these baselines, particularly FedAvg, which seems to underperform.\n2. I suggest an ablation study to isolate the contributions of adaptive stepsizes and momentum in the PAdaMFed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}