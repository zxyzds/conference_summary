{
    "id": "HVblmL5Rws",
    "title": "The Two-Hop Curse: LLMs trained on A\u2192B, B\u2192C fail to learn A\u2192C",
    "abstract": "While LLMs excel at answering multi-hop questions like \u201cWho is the spouse of the performer of Imagine?\u201d by thinking out loud (chain-of-thought), they perform surprisingly poorly when required to reason in their latent space and answer without chain-of-thought. This observation was previously referred to as the compositionality gap, implying that although language models are less reliable at two-hop latent reasoning, they still perform it sometimes. In this paper, we introduce a controlled setting for investigating the compositionality gap. We run a series of experiments finetuning a large language model (Llama-3-8B-Instruct) on synthetic facts expressed in English. We attempt to elicit two-hop reasoning in three ways: (i) fine-tune on a data mixture designed to incentivize two-hop reasoning, (ii) force facts to be stored in layers in the correct order, and (iii) use an auxiliary loss to provide activation-level supervision for two-hop reasoning. We show that LLaMA 3 8B successfully learns to answer two-hop questions about synthetic facts using CoT, but completely fails without CoT, achieving chance-level accuracy and chance-level test loss. Failures of LLMs in our controlled setting cast doubt on the purported ability of present LLMs to perform multihop latent reasoning and lead us to conjecture that, rather than a reasoning gap, current language models might exhibit a two-hop reasoning curse \u2014 a complete lack of ability rather than a relative weakness. This is the Two-Hop Curse.",
    "keywords": [
        "latent reasoning",
        "two-hop reasoning",
        "chain of thought",
        "LLMs",
        "question answering",
        "llama",
        "fine-tuning",
        "fact representation",
        "knowledge representation",
        "world models"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We show that LLMs (LLaMA-3-8B) fail to learn how to combine two facts to answer a two-hop question, even despite being finetuned to do so",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=HVblmL5Rws",
    "pdf_link": "https://openreview.net/pdf?id=HVblmL5Rws",
    "comments": [
        {
            "summary": {
                "value": "This paper studies the compositionality gap in LLMs: why do LLMs fail to answer two hop questions directly but can do so with CoT."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper studies an interesting problem, the compositionality gap, although with relatively shallow experiments (restricted to one model on a simple synthetic dataset). It tries different approaches to address this, although the motivation for the auxiliary objectives need to be strengthened (currently feels very ad-hoc)."
            },
            "weaknesses": {
                "value": "1. This work is quite incremental given the existing literature on compositionality gap (Press et al, 2023).\n2. Further the experiment is carried out only in a very simple synthetic domain.\n3. The experiments are performed using only one model, so this and the previous points brings the generalizability of this study into question.\n4. The motivation for this work needs to be clarified, as the LLMs do perfectly well with CoT.\n5. Several phrases used without proper definitions of them: \"two-hop circuitry\", \"Goldilocks zone\""
            },
            "questions": {
                "value": "What is the motivation behind this study given CoT does well?\nWhy is the study restricted to only a simple synthetic dataset and only one model?\nWhat is the key differentiating factors between this work and the several prior works on compositionality gap of LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The author proposes a series of experiments to explore the \"two-hop curse\" observed in large language models (LLMs). Using atomic data in different configurations, they select two types of two-hop data: one without chain-of-thought (CoT) reasoning and one with CoT reasoning, to fine-tune the Llama3-8B model. This approach effectively allows control over the data that influences the LLM's parameters. The author finds that adding two-hop CoT data increases accuracy for two-hop questions; however, it still fails to improve performance on two-hop questions without CoT. Additionally, two intervention tests were conducted to assess their impact, but both showed minimal effect. Overall, this paper provides a detailed analysis of the phenomenon known as the \"compositionality gap.\""
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The study employs fine-grained control over the training data and conducts a series of experiments to meticulously examine the compositionality gap in large language models."
            },
            "weaknesses": {
                "value": "This is an analytical paper that conducts various experiments and research on a specific phenomenon. However, it does not present particularly impressive conclusions or unique perspectives. The results obtained from intervention2 and intervention3 are not positive, but we should see more reasons that lead to the occurrence of the phenomenon in the main papaer, rather than this \"process of elimination.\" In addition, the author's motivations and explanations for their interventions are not convincing."
            },
            "questions": {
                "value": "- Intervention 2: We observe that one-hop accuracy from layer-selective experiments also declined. Could you elaborate on how this affects the decrease in two-hop reasoning performance?\n\n- Additionally, could you explain your motivation for choosing these two specific interventions over other possible options?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The work sets out to investigate the compositional reasoning gap of LLMs. The authors design three different approaches to elicit two-hop reasoning of pre-trained LLaMA-3-8B: fine-tuning on mixed data, staged fine-tuning to force the first-hop facts and the second-hop facts to be stored in different layers and leveraging additional supervision signal to encourage the emerge of bridge entity in the middle layers. They offer a converged conclusion: LLaMA-3-8B completely fails to learn to generalize to compositional reasoning cases without chain-of-thought prompting."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The presentation, logic-flow of the paper is good. The paper is overall well writen and easy-to-follow.\n2. The topic of the work \"the limitations of the compositional reasoning in large language models\" is interesting and important as well.\n3. The experiments designed in the paper are quite multi-faceted, offering some insightful results to readers."
            },
            "weaknesses": {
                "value": "1. Though the experiments presented in the paper, it only explore a few settings (fine-tuning on mixed data, staged fine-tuning to force the first-hop facts and the second-hop facts to be stored in different layers and leveraging additional supervision signal to encourage the emerge of bridge entity in the middle layers). Negative results on such settings might be insufficient to claim that LLMs exhibit a near-complete failure of two-hop latent reasoning.\n2. As the authors stated in the Limitation section, the paper mainly focus on making LLMs acquire knowledge via fine-tuning, different from pre-training (where typically we do). This may weaken the insights brought by the work.\n3. The variation of the data is quite limited: only covering factual knowledge data and only two semantic templates (spouse and birth city), which may prevent the model from learning some general composition skills."
            },
            "questions": {
                "value": "1. One of the contributions claim that the experimental setup can alleviate memorization or reasoning short cuts. How do the dataset settings control the memorization or reasoning short cuts? I may overlook some details? Did you use the counter-factual (or virtual) data to conduct the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the limitations of LLMs in performing \"two-hop\" reasoning in their latent space. The authors create a controlled setup using Llama 3 8B, where they fine-tune the model with three strategies aimed at eliciting two-hop reasoning without CoT: data mixtures that encourage two-hop reasoning, layer-ordering of facts to align with logical steps, and activation-level supervision. Despite these methods, LLMs could not reliably perform two-hop reasoning without CoT, failing to exceed chance-level accuracy. This suggests that LLMs may lack fundamental latent reasoning capabilities, potentially highlighting an intrinsic limitation of current transformer models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.The authors address a highly intriguing problem, investigating weaknesses in LLMs and pointing to directions for future optimization.\n\n2.The experimental design minimizes the impact of the model's pre-existing knowledge on the results, thereby increasing the reliability of the conclusions."
            },
            "weaknesses": {
                "value": "1. The paper lacks novelty, as previous works, such as arxiv.org/pdf/2406.12775 and arxiv.org/pdf/2402.16837, have already investigated the limitations of LLMs in multi-hop reasoning. The authors should further discuss the distinctions between their study and these prior works.\n\n2. The study identifies the \"two-hop curse\" phenomenon through experimental analysis but does not delve into the underlying causes of this limitation, nor does it propose any effective methods to alleviate it.\n\n3. The experimental design lacks sufficient depth; the constructed dataset contains only one pattern (\u201cThe spouse of e1 is e2. The birth city of e2 is e3\u201d), without covering other relational structures. Additionally, only the Llama 3 8B model is evaluated, leaving open the question of whether larger models or different architectures would also experience the two-hop curse.\n\n4. Several details remain unclear, such as specific hyperparameters for the training setup (e.g., learning rate, warmup ratio), and some methods need further theoretical explanation, particularly Inventions 2 and 3. There is approximately a page and a half of space that could be used to expand on these aspects.\n\nMinor Issues:\n\n1. The color differentiation in Figure 1 is minimal, making it difficult to discern details."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}