{
    "id": "uZFXpPrwSh",
    "title": "Zero-shot Model-based Reinforcement Learning using Large Language Models",
    "abstract": "The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. \nIn reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. \nIn this paper, we investigate how pre-trained LLMs can be leveraged to predict in-context the dynamics of continuous Markov decision processes. \nWe identify handling multivariate data and incorporating the control signal as key challenges that limit the potential of LLMs deployment in this setup and propose several ways to address them. \nOur experimental results demonstrate that our approach produces well-calibrated uncertainty estimates, suitable for proprioceptive control tasks. \nWe further present proof-of-concept applications in two reinforcement learning settings: model-based policy evaluation and data-augmented off-policy reinforcement learning, supported by theoretical analysis of the proposed methods.",
    "keywords": [
        "Model-based Reinforcement Learning",
        "Large language models",
        "Zero-shot Learning",
        "In-context Learning"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "We investigate in-context learning using LLMs for dynamics learning of Markov Decision Processes with applications in off-policy reinforcement learning.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=uZFXpPrwSh",
    "pdf_link": "https://openreview.net/pdf?id=uZFXpPrwSh",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a new approach Disentangled In-Context Learning (DICL) to generalize LLM-based in-context learning to the domain of continuous-state-space reinforcement learning. This paper then analyzed the theoretical properties of this new DICL approach, and reports empirical experiments results from policy evaluation and data-augmented off-policy RL to the advantages in sample efficiency and performance of DICL in comparison to baseline methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper has good originality in that it innovatively proposes a novel DICL method to generalize ICL to continuous-state-space RL. This paper also includes solid mathematical derivations and proofs and detailed experiment results to support the claims in the paper."
            },
            "weaknesses": {
                "value": "There is a lot of room of improvement for the clarity, writing and presentation of this paper. Multiple places in the paper are not very clearly explained and the general theme of the paper is a little bit hard to follow in its writing. For example, throughout the whole paper, there is no explicit explanation or demonstrations on how exactly the LLM prompts for DICL are constructed. One or more concrete prompt examples would be very helpful in the paper to help readers understand the core technical details of the proposed DICL method.\n\nThis paper has very good potential, but its current form could benefit a lot from a systematic revision that improves its clarity and presentation."
            },
            "questions": {
                "value": "1. Typo - on Line 63, should it be \u2018deferred\u2019 instead of \u2018differed\u2019?\n2. Typo - on Line 322, should it be \u2018The goal is to improve \u2026\u2019?\n3. In the DICL-SAC algorithm, what would be the optimal value for \\alpha? Are there any intuitions for choosing the optimal \\alpha value?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores the utilization of LLMs in the realm of RL, specifically targeting continuous state spaces that have been understudied in the context of LLM integration. It introduces a novel approach termed Disentangled In-Context Learning (DICL), designed to leverage pre-trained LLMs for predicting the dynamics of continuous Markov decision processes. The paper is supported by theoretical analysis and extensive experiments, demonstrating the potential of LLMs in model-based policy evaluation and data-augmented off-policy RL, and shows that LLMs can produce well-calibrated uncertainty estimates."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper presents a method to integrate state dimension interdependence and action information into in-context trajectories within RL environments, enhancing the applicability of LLMs in continuous state spaces.\n2. It provides a theoretical analysis of the policy evaluation algorithm resulting from multi-branch rollouts with LLM-based dynamics models, leading to a novel return bound that enhances understanding in this area.\n3. The paper offers empirical evidence supporting the benefits of LLM modeling in two RL applications: policy evaluation and data-augmented offline RL, showcasing the practicality of the proposed methods.\n4. It demonstrates that LLMs can act as reliable uncertainty estimators, a desirable trait for MBRL algorithms."
            },
            "weaknesses": {
                "value": "1. The paper does not extensively discuss how the proposed method generalizes across different environments or tasks, that is, more discussion about the application of this method is needed.\n2. While DICL simplifies certain aspects of RL, the integration of actions and the handling of multivariate data present ongoing challenges. More discussion about the introduced aspects of the DICL is needed.\n3. The experiments are somewhat simplistic, and it would be worthwhile to conduct more in-depth analyses, such as discussing when they become ineffective and more results will be beneficial.\n4. The writing style of the paper is a bit convoluted, making it less fluent to read."
            },
            "questions": {
                "value": "Why does Principal Component Analysis (PCA) decouple features, isn\u2019t it primarily for feature selection? If it is for feature selection, are there any special constraints needed for article-related scenarios?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to leverage large language models (LLMs) to achieve model-based reinforcement learning (MBRL). Specifically, it explores using the in-context learning (ICL) capability of LLMs to autoregressively predict the next state and reward, denoted as $<s_t, r_t>$, based on a given prior states and actions.\n\nThe authors' first approach, vanilla ICL (vICL), simplifies the objective by predicting $s_t$ solely based on prior states $s_{1:t-1}$, without considering the action. The second approach, DICL, uses Principal Component Analysis (PCA) to map the state-action vector to a latent space, where ICL is applied to the latent vectors. Additionally, they apply their LLM-based dynamics learner to augment the replay buffer of the Soft Actor-Critic (SAC) algorithm.\n\nAnother important contribution of this work is demonstrating the bound on the difference between true dynamics and LLM-based dynamics under a multi-branch rollout setting."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(please also see the summary)\n1. This work systematically studies LLM-based MBRL.  \n2. The method is clearly presented and easy to follow."
            },
            "weaknesses": {
                "value": "The \u201czero-shot\u201d claim is potentially misleading. If \"zero-shot\" is defined at the trajectory level, it is true that no trajectory-level examples were shown to the LLM during prediction. However, as shown in Section 4 (theoretical analysis), it appears necessary to use true dynamics to predict the transition and reward for steps $t < T$. These transitions, such as from $<s_{t-1}, a_{t-1}>$ to $s_t$, effectively serve as state-level few-shot examples. In my understanding, a true \u201czero-shot\u201d setting would require that all previous transitions be predicted by the LLM itself autoregressively.\n\nAnother related concern is that the experimental setup is difficult to understand. For instance, Figure 3b was unclear to me. Given that the authors claim their method is zero-shot, I was unsure why, in the zero-shot setting, the agreement between the ground truth and the LLM-based method improves over time significantly. Wouldn\u2019t error accumulation or distributional shift cause the LLM\u2019s performance to degrade as more time steps are taken? After reviewing the theoretical analysis section, it seems likely that the result in Figure 3b is due to true dynamics being incorporated into the predictions, which was not mentioned. Please let me know if my understanding is inaccurate.\n\nFinally, the ICL of LLM leverages the whole information in the previous context to predict the next transition. However, the MLP method can only accept the information of previous step as the input. The comparison is somehow unfair."
            },
            "questions": {
                "value": "n/a"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates zero-shot reinforcement learning (RL) for continuous control tasks using large language models (LLMs) and introduces \"Disentangled In-Context Learning\" (DICL) to improve LLMs' handling of continuous MDPs. This approach separates state and action features to better utilize in-context learning, with experiments validating its efficacy."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The authors proposed an innovative integration of LLMs into continuous reinforcement learning tasks.\n2. Clear theoretical foundation supporting the DICL framework is shown.\n3. The authors conduct extensive experiments to verify the effectiveness of the method across diverse RL scenarios.\n4. The authors also did insightful analysis of zero-shot capabilities in complex MDPs."
            },
            "weaknesses": {
                "value": "Unclear experimental settings. The authors mentioned that they used 3 Llama3 models for the experiments, but all the experiment figures only show one result without stating which model is used. Also, an ablation study about the effect of different LLMs is missing."
            },
            "questions": {
                "value": "You mentioned that you change the update step of SAC to accommodate LLM's requirements. Is this unfair? Have you compared with a baseline SAC with update step 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}