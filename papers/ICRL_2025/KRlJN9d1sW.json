{
    "id": "KRlJN9d1sW",
    "title": "Reinforcement Learning via Lazy-Agent for Environments with Random Delays",
    "abstract": "Real-world reinforcement learning applications are often hampered by delayed feedback from environments, which violates the fundamental assumption of the Markovian property and introduces significant challenges. While numerous methods have been proposed for handling environments with constant delays, those with random delays remain largely unexplored owing to their inherent complexity and variability. In this study, we explored environments with random delays and demonstrated that these environments can be transformed into equivalent constant-delay environments by introducing a simple agent called the \\textit{lazy-agent}. This approach overcame the challenges posed by the variability of random delays, enabling the application of conventional constant-delay approaches to random-delay environments. The empirical results reveal that our lazy-agents trained in random-delay environments performed almost comparably to the agents trained in constant-delay environments, significantly outperforming other baseline algorithms in terms of asymptotic performance and sample efficiency.",
    "keywords": [
        "Delayed system",
        "Markov decision process",
        "reinforcement learning"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We introduce a simple yet effective reinforcement learning agent, referred to as the lazy-agent, which can be applied in environments with random delays.",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=KRlJN9d1sW",
    "pdf_link": "https://openreview.net/pdf?id=KRlJN9d1sW",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a derivation and a small-scale empirical analysis that shows random-delay MDPs can be transformed to nearly equivalent constant-delay MDPs by employing a lazy-agent that assumes that all states are (constantly) delayed by the maximum delayed times."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Soundness\n======\n The approach is sound, and, albeit a small increment over existing approaches, well demonstrated empirically. The paper lacks an in-depth discussion of the benefits and limitations of the approach. \n\t\nSignificance & Related work\n=========\nThere is no related work section in the paper and thus the significance of the work is not clarified.  \n\nExperimentation\n=========\nThe experimental analysis shows the equivalence of RDMDPs to CDMDPs in BPQL in the MuJoCo environments. \n\n\nPresentation\n=========\nThe paper is well written."
            },
            "weaknesses": {
                "value": "Soundness\n======\nThe paper lacks an in-depth discussion of the benefits and limitations of the approach. \n\t\nSignificance & Related work\n=========\nThere is no related work section in the paper and thus the significance of the work is not clarified.  \n\nExperimentation\n=========\n\nThere is a need, however, for more in-depth ablation experiments that evaluate the impact of assuming states are processed in order, and of highly varied random delays."
            },
            "questions": {
                "value": "* What is the performance of the approach in the other MuJoCo environments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a way to transform random delay environments into constant delay environments, by imposing the specified maximum delay onto all observed states, regardless of their actual delay. Experiments show that agents trained under this transformation achieve similar results to agents trained in constant delay environments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Nice visual representations e.g. Figure 1 help with understanding\n2. The paper proposes a straightforward way to apply an algorithm for constant-delay environments to environments with random delays\n3. Convincing number of environments and comparison algorithms used in experiments"
            },
            "weaknesses": {
                "value": "1. Still requires a maximum number of delay time steps to be specified, which may be unrealistic in some environments (e.g. if observations must be sent over a high-latency network, if the observations require lengthy processing). This would admittedly be a challenging setting to address.\n2. Only addresses the issue of large augmented state dimensions if the specified max delay is small.\n3. I found the explanation of BPQL a bit confusing, as I don\u2019t see how it avoids state space explosion if you are still training a policy on the augmented state."
            },
            "questions": {
                "value": "1. Unsure why you need to wait for $t>2_{o_{max}}$ in line 19 of Algorithm 1, instead of just $t>o_{max+2}$, if the purpose is just to wait until you have the next state/augmented state?\n2. In section 3.2 it is stated that the state dimension could become infinite in infinite-horizon MDPs, wouldn\u2019t this only occur if $O_{max}$ was infinite? \n3. Could you provide more details on how BPQL mitigates the state space explosion problem, particularly given that the policy is still trained on the augmented state?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Lazy-Agent for handling reinforcement learning in environments with random delays.\nTraditionally, state-of-the-art RL techniques assume constant delays, simplifying the learning process but failing to reflect real-world scenarios' complexity.\nA Lazy-Agent operates under the assumption that all feedback, such as states or rewards, is delayed by the maximum possible time, even though the actual delay is random.\nThis method allows the transformation of random-delay environments into constant-delay ones, making it possible to apply conventional RL techniques designed for constant delays.\nRather than reacting immediately, the Lazy-Agent waits for the maximum delay before making decisions.\nThis simplifies the management of random and unpredictable delays by treating them as constant delays, thereby facilitating the use of existing reinforcement learning methods for delayed feedback.\nThe paper extends the belief projection-based Q-learning (BPQL) framework, introducing Lazy-BPQL."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors demonstrate that using Lazy-Agent can convert random-delay Markov decision processes (RDMDPs) into constant-delay MDPs (CDMDPs). This transformation allows established constant-delay solutions to be applied to random-delay environments.\n- The proposed method is simple and easy to implement."
            },
            "weaknesses": {
                "value": "- For certain tasks (such as Ant, Hopper, Humanoid, and Pendulum), the performance gains from using Lazy-BPQL are minimal.\nThe reported improvements in performance, when considering standard deviation, are negligible.\n- There is no significant advantage of Lazy Agents over traditional methods in all benchmarks, which raises concerns about the broader applicability of the Lazy Agent model."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}