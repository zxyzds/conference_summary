{
    "id": "96beVMeHh9",
    "title": "Causal Identification for Complex Functional Longitudinal Studies",
    "abstract": "Real-time monitoring in modern medical research introduces functional longitudinal data, characterized by continuous-time measurements of outcomes, treatments, and confounders. This complexity leads to uncountably infinite treatment-confounder feedbacks, which traditional causal inference methodologies cannot handle. Inspired by the coarsened data framework, we adopt stochastic process theory, measure theory, and net convergence to propose a nonparametric causal identification framework. This framework generalizes classical g-computation, inverse probability weighting, and doubly robust formulas, accommodating time-varying outcomes subject to mortality and censoring for functional longitudinal data. We examine our framework through Monte Carlo simulations. Our approach addresses significant gaps in current methodologies, providing a solution for functional longitudinal data and paving the way for future estimation work in this domain.",
    "keywords": [
        "Causal Inference",
        "Stochastic Process",
        "Longitudinal Data; Functional Data",
        "Continuous Time."
    ],
    "primary_area": "causal reasoning",
    "TLDR": "We propose a novel causal identification framework for functional longitudinal data.",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=96beVMeHh9",
    "pdf_link": "https://openreview.net/pdf?id=96beVMeHh9",
    "comments": [
        {
            "comment": {
                "value": "5. Demonstration of Theorem 4\nThank you for highlighting the importance of Theorem 4. We agree that it is a key contribution of the paper, and we understand the desire for its further validation. However, it is important to note that Theorem 4 establishes a theoretical foundation rather than a practical computation. Specifically, the theorem demonstrates that our framework is dense in the probability measure space, which implies that in real-world scenarios, one cannot practically encounter a measure that violates it. As such, there is no established practice or meaningful way to numerically verify this property.\n\nInstead, Theorem 4 serves as the cornerstone for future work, ensuring that any estimation procedure\u2014whether parametric or non-parametric for nuisance functions\u2014can operate freely within our framework. This generality is crucial for enabling subsequent developments in functional data analysis for causal inference, including estimation, inference, and applications to real-world datasets like MIMIC-IV.\n\nWhile we acknowledge the importance of practical validation, the contribution of Theorem 4 is primarily theoretical, laying the groundwork for future empirical applications and methodological advancements.\n\n6. Definition of \"Functional\" Data\n\nThank you for your observation regarding the definition of \"functional\" data. We will incorporate the feedback from other reviewers and refine our definition, drawing from established references such as Wang, Chiou, and M\u00fcller (2016). Specifically, we will clarify that functional data refers to data arising from continuous-time processes, often characterized by high-dimensional, smooth trajectories or curves, and distinguish our focus within this framework. This will help to more rigorously position our approach while emphasizing its relevance to causal inference in complex scenarios.\n\n7. Assumption of Functional Continuity\n\nThe assumption of functional continuity reflects the population-level nature of our framework. The causal effect of a drug, for instance, operates in a continuous-time manner, even if real-world data are observed discretely. How sample-level data are recorded\u2014whether densely, sparsely, or irregularly\u2014is a topic for future work focusing on estimation. These considerations will include the minimum observation density required for consistent estimators and the potential shift to partial identification when non-dense observations are present.\n\nIn this paper, our focus is on rigorously defining the estimand in a way that reflects the underlying continuous process at the population level. This definition is essential for future developments in estimation and inference, ensuring the framework remains robust and grounded in theoretical principles."
            }
        },
        {
            "comment": {
                "value": "Thank you for your detailed and constructive feedback, and for your realization of the strengths of this work. We appreciate the time you dedicated to our manuscript and the opportunity to address your concerns. Below, we respond to the key points you raised.\n\nWeaknesses\n\n1. Relation to \"Causality for Complex Continuous-Time Functional Longitudinal Studies with Dynamic Treatment Regimes\":  \n   We acknowledge the connection with this paper, which indeed represents a subsequent work to the present one. While the later work generalizes identification results to include semiparametric frameworks, it lacks numerical examples and practical verifications. Our current submission provides a focused exploration of nonparametric identification, coupled with simulation-based numerical validation. Incremental contributions like this allow for a deeper understanding of specific aspects, offering clarity and actionable insights while building toward more comprehensive frameworks.\n\n2. Academic and Practical Significance of Solving More Complex Problems:  \n   The introduction of variables such as \\( T \\), \\( X \\), and \\(\\Delta\\) addresses complexities inherent in real-world medical applications. Medical studies often involve truncated follow-up due to death or censoring, which these variables explicitly model. Furthermore, in healthcare, interest frequently lies in the entire process of progression (e.g., \\( Y(t) \\)), rather than just a terminal outcome. By distinguishing between these components, our framework supports broader and more clinically relevant causal queries.\n\n3. Why is the Non-Parametric Property Important in a Functional Data Framework?\nThe non-parametric property aligns with the recent assumption-lean efforts in the causal inference community (see references below). Traditional approaches often rely on parametric or semi-parametric modeling assumptions, such as smoothness or sparsity, to facilitate analysis and reduce dimensionality. However, these assumptions are typically made for mathematical convenience rather than being grounded in prior knowledge. As a result, inferences drawn from such models may reflect the assumptions as much as, or more than, the data itself.\n\nFor functional data, where the complexity of continuous, infinite-dimensional outcomes makes it even harder to justify any specific model, relying on parametric assumptions becomes especially unrealistic. Our framework deliberately separates modeling assumptions from identification, focusing purely on structural assumptions necessary for causal inference. This ensures that the framework extracts information only from the data, avoiding the risk of introducing unwarranted or misleading conclusions based on arbitrary assumptions.\n\nBy adopting a non-parametric approach, we provide a more flexible and adaptable methodology that reflects the complexities of real-world data, particularly in healthcare settings where data rarely adhere to idealized models. This choice strengthens the framework\u2019s robustness and relevance, particularly for functional data analysis.\n\nVansteelandt, Stijn, and Oliver Dukes. \"Assumption-lean inference for generalised linear model parameters.\" Journal of the Royal Statistical Society Series B: Statistical Methodology 84.3 (2022): 657-685.\n\nVansteelandt, S., Dukes, O., Van Lancker, K., & Martinussen, T. (2024). Assumption-lean Cox regression. Journal of the American Statistical Association, 119(545), 475-484.\n\n4. More explanations on v and G\nYes, we are addining more explanations and examples around them, thanks for pointing out!\n\"Can Equation (1) be understood as a general expression representing the average treatment effect, the averaged treatment outcome, or a transformed form of these?\" Yes, our characterization can accommodate all things you've mentioned here because we've allowed G to be a signed measure, so it does not have to be positive, so for instance it allows for 1(\\bar A = \\bar 1) - 1(\\bar A = \\bar 0) (the difference of two delta measures), representing the average treatment effect of always-treated vs never-treated."
            }
        },
        {
            "comment": {
                "value": "Thank you for your thoughtful and detailed feedback. Your comments provide valuable insights that will help us improve both the presentation and clarity of our manuscript. Below, we address your specific concerns and questions.\n\nClarification of Summary\n\nWe appreciate your summary and would like to clarify that our focus is on the underlying curve data \\( X(t) \\), as introduced in Wang et al. (2016). At the population level, our framework abstracts away the sparsity or regularity of sample-level observations. In future work, we plan to extend our framework to sample-level data, where factors such as sparsity or irregularity could influence the consistency of estimators. For example, investigating how the number of observed time points \\( p_n \\) scales with the sample size \\( n \\) in densely observed data could provide valuable insights.\n\nWeaknesses\n\n1. Connection Between \\( T \\) and \\( Y(t) \\):  \n   We will improve the explanation of how \\( T \\) (the event time) relates to \\( Y(t) \\) (the outcome). Intuitively, \\( T \\) serves as a \"Cemetery Point\" where all other stochastic processes cease to evolve due to the individual\u2019s death. For instance, if \\( Y(t) \\) represents disease progression and \\( T \\) represents death, \\( Y(t) \\) is fixed as \\( Y(T) \\) for \\( t > T \\). Both \\( Y(t) \\) and \\( T \\) are practically relevant, so we distinguish them in our framework. This distinction aligns with prior work such as Rytgaard et al. (2022).\n\n2. Improving Readability:  \n   We will follow the concrete steps suggested by other reviewers, including simplifying notations, adding examples, and providing more intuitive explanations throughout the manuscript. This will make the connection between key components clearer.\n\nQuestions\n\n1. When \\( A(t) \\) is a Function:  \n   The results in our paper remain unchanged if \\( A(t) \\) is a function, vector, or scalar. This is because our framework is built using measure theory, which generalizes across these cases. Theorems and results retain their form regardless of the specific nature of \\( A(t) \\).\n\n2. Why \\( Y(t) \\subseteq L(t) \\):  \nThis is purely for notational simplicity. Including \\( Y(t) \\) separately would make the measures and derivations more cumbersome without adding clarity. The framework does not assume \\( Y(t) \\) must impact treatment assignment but allows this dependency to exist or not, reflecting scenarios like disease progression influencing treatment decisions."
            }
        },
        {
            "comment": {
                "value": "Thank you for your constructive feedback and thoughtful questions. We appreciate your recognition of the contributions and potential impact of our work. Below, we address the key points you raised.\n\nWeaknesses\n\n1. Numerical Experiment Simplification:  \n We acknowledge the limitations of the current simulations and will incorporate additional scenarios accounting for mortality and censoring in the revised manuscript.\n\n2. Causal Structure and \\( Y(t) \\subseteq L(t) \\):  \nThis notation was chosen purely for simplicity. Including \\( Y(t) \\) separately would make the measures and derivations more complex without adding clarity. We are not assuming \\( Y(t) \\) must affect treatment assignment but instead allow this dependency to exist or not. This flexibility is critical as in many cases (e.g., disease progression), outcomes can influence treatment adjustments, and therefore acting as a confounder as well.\n\n3. Intuitive Explanations for Propositions:  \n We agree with this suggestion and will include intuitive explanations and examples following definitions and propositions, addressing similar feedback from other reviewers.\n\nQuestions\n\n1. Interventional Distribution (Equations (7)-(10)):  \n   We will expand this section to bridge the conceptual gap from Equations (2)-(6) to (7)-(10). Specifically:\n   - Equations (4)-(5) describe the probability of censoring within or beyond \\([t_j, t_{j+1}]\\).\n   - Intervening to a pseudo-world where censoring always happens after \\(t_{j+1}\\) leads to terms like \\( {1 - 1(x \\leq t_{j+1}, \\delta = 0)} \\).  \n   - Similarly, treatment distributions are intervened into \\(G\\) as shown in Equation (10).  \n   For additional context, we will reference Rytgaard et al. (2022) in the reference of our paper, particularly Definitions 1 and 2, to clarify intervention-based causal inference.\n\n2. Extension to Dependent Censoring:  \n   Yes, our framework can be extended to handle dependent censoring. If the dependency is explained by observed factors, this is already addressed under Assumption 2. For unobserved dependency, methods like proxy variables (e.g., Ying, A. (2024). Proximal survival analysis to handle dependent right censoring. Journal of the Royal Statistical Society Series B: Statistical Methodology, qkae037.) can be adapted to extend our framework. Similarly, this approach could be generalized for unmeasured confounders."
            }
        },
        {
            "comment": {
                "value": "Thank you for your detailed and thoughtful feedback. Your comments provide valuable guidance for improving our manuscript. Below, we address the key points you raised. The draft is under changing now to incorporate all reviewers' suggestions but here is a preliminary reply for what we will do and also answer some of your questions.\n\nGeneral Revisions\n\nWe will implement all suggested improvements in the \"Weaknesses\" section to enhance clarity and accessibility:\n1. Terms such as \"g-computation\" and \"counterfactual time-to-event endpoint\" will be clearly defined with examples. We will also add reviews of some terms in the classical discrete-time case in the appendices.\n2. The literature review will be expanded to include detailed textual explanations, replacing Figure 1.\n3. Section 3.1 will be streamlined with concrete examples (e.g., for \ud835\udf08 and \ud835\udc3a) and unnecessary notations removed.\n\nSpecific Responses to Weaknesses\nExperiments Limited to Simulation Data:\n\nThe primary aim of our paper is to address the identification problem for functional longitudinal data. As you and other reviewers noted, this is a novel area in the field, and our focus in this work is strictly on theoretical identification rather than estimation or inference. Given the 10-page limit and the complexity of the problem, our approach represents an incremental but crucial step in developing a solid foundation for future estimation frameworks. As described in the paper, the g-formula, inverse probability weighting (IPW), and doubly robust (DR) formulas yield identical results at the population level under identification, all equal to (1). Since all three are theoretically equivalent on the population level, verifying the g-formula suffices as a sanity check for our purposes. Additionally, the g-formula is the most computationally feasible for simulation because its implementation does not require knowledge on the measures themselves, unlike IPW or DR (unlike in discrete-time and non-functional cases, one have the knowledge and can compute the density, here we need to compute measures, which is far from trivial). This also addresses Question 2: At the population level, the three formulas are theoretically equivalent under our framework. Their finite-sample differences stem from estimation, which will be a focus of future research. \n\nWith that said, we realized that our simulation can be oversimplified, as noted by other reviewers as well. Therefore, we will make our simulations more complex by adding more covariates, separating from the outcomes, adding mortality and censoring as well.\n\nClarity Regarding the Statement on Density Functions (Line 141):\n\nIn infinite-dimensional spaces, the concept of density is not applicable as in finite-dimensional Euclidean spaces. Instead, one must resort to measure-theoretic approaches. For example, the measure \ud835\udc43(\ud835\udc51\\bar{\ud835\udc4e}\ud835\udc51\\bar{\ud835\udc59})) referenced in the paper pertains to the probability distribution over the paths of stochastic processes, which is characterized directly via measures rather than densities. We will revise this section to clearly explain why density functions are unsuitable in this context and why measures are used instead."
            }
        },
        {
            "summary": {
                "value": "In this paper, authors consider causal inference on time-varying data (functional longitudinal data). They generalize the classical g-computation, inverse probability weighting and doubly robust formulas to the time-varying setting subject to censoring and mortality. The g-computation formula is simulated using Monte-Carlo on a toy dataset, achieving promising results."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Treatment effect on functional longitudinal data seems to be an understudied subject. This research nicely fills the gap of existing works. \n\nThe resulting G-computation can be quite straightforwardly approximated using observations under simulation settings."
            },
            "weaknesses": {
                "value": "The way this paper is written obscures its main ideas (at least to a general, non-expert reader). There are many terms and phrases used without clear explanation (e.g., \"g-computation\", \"counterfactual time-to-event endpoint\"). This restricts the range of potential readers of this paper.  \n\n**The experiments are limited to only simulation data and only validate the G-computation formula**. \n\nThe literature review of this paper (section 2) does not seem to provide much information of existing works as without proper explanation, readers may be unclear what \"temporal aspect\", \"point exposure\" and \"end-of-study outcome\" means. I recommend removing Figure 1 and expanding on each of the subsections, providing more details of existing works. \n\nThe preparation in Section 3.1 is quite long. Without concrete examples, it is hard for readers to understand what they actually mean. I suggest skip some unnecessary notations, and explain them as the paper progresses. \n    - Some symbols are better explained with examples. For instance, authors could give an example of nu and G, around equation (1)."
            },
            "questions": {
                "value": "Line 141, authors mentioned \"note this is not a density function\". Then please specify what this is. \n\nLine 325, why is it sufficient to evaluate the approximation of the G-computation formula? I don't think on population level, the values of three formulas are numerically equal. Even if they are equal, they may have quite different finite-sample behaviors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a causal identification framework that bridges classical causal inference framework, continuous-time longitudinal analysis and functional data analysis. In this framework, the parameter of interest is the marginal mean of counterfactual outcomes under a measure that allows randomly assigned treatments, with absence of censoring. Leveraging the tools in stochastic process, the authors then demonstrate the identification results for three classical estimation strategies in causal inference: g-computation, IPW and doubly robust estimation. The authors further claims that the identification framework also has non-parametric property."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper establishes a new causal identification framework for continuous-time longitudinal studies with functional data, and provides clear and concise theoretical demonstration. I believe that this framework will be of interest to causal inference and machine learning communities."
            },
            "weaknesses": {
                "value": "1. The numerical experiment might be an over-simplification of the survival analysis scenario since neither mortality nor censoring are taken into consideration. \n2. What is the causal structure that the framework is focusing on? Specifically, why set $Y(t)$ (outcome of interest) to be a subset of $L(t)$ (measured confounders)? I might misunderstood but are we assuming that previous outcome will impact the current treatment assignment (since confounders, from my understanding, will impact treatment assignment)?\n3. I guess it would be helpful to attract readers in a wider community if more intuitive explanation could be added after stating definitions/propositions."
            },
            "questions": {
                "value": "1. Why is the interventional distribution ((7)-(10)) formulated in this way? Specifically, I\u2019m curious about where the term $\\{1 -   \\mathbb{1}(x \\leq t_{j+1}, \\delta=0) }$ comes from.\n2. Can this framework be extended to dependent censoring?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper is challenging to follow, so I may have misunderstood some parts. My understanding is that the \"functional longitudinal data\" investigated here are conventional functional data, as described by Wang et al. (2016), which can be measured intensively, sparsely, or irregularly. However, this paper focuses solely on the ideal (hypothetical) setting where continuous-time measurements are available for each experimental subject, resulting in infinite-dimensional data. If this interpretation is correct, the goal of this paper is to explore causal identification for infinite-dimensional functional (time-varying) outcomes that are subject to mortality and censoring by generalizing the classical g-computation, inverse probability weighting, and doubly robust formulas.\n\nReference: \nWang, Chiou and M\u00fcller (2016). Functional data analysis. Annual Review of Statistics ands its application."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The approach is nonparametric and it accommodates functional treatment processes A(t) and functional confounders L(t), as well as functional response Y(t)."
            },
            "weaknesses": {
                "value": "The paper is hard to follow and the connection of the event-time T to the outcome Y(t) is unclear."
            },
            "questions": {
                "value": "Could you elaborate on the situation when A(t) is a function? \n\nWhy should Y(t) be a subset of L(t), and what does it mean?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a nonparametric causal identification framework for functional longitudinal data, exemplified by the MIMIC-IV dataset, which includes complexities such as events like death. By leveraging stochastic process theory and measure theory, the framework generalizes g-computation, inverse probability weighting, and doubly robust formulas, effectively handling time-varying outcomes with mortality and censoring. Monte Carlo simulations verifies it."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper addresses the problem of causality identification in complex scenarios inherent in functional longitudinal data, a highly general and advanced form of data. If it can overcome the criticisms and shortcomings pointed out by other reviewers and get published, it could serve as a milestone, taking one step forward in functional data analysis for causal inference."
            },
            "weaknesses": {
                "value": "This paper is more abstract than necessary, making it harder to read than other papers. Even after spending more time on it than on other review papers, the preliminaries, equations, and theorems presented in the paper are not easily understood at once. As a result, I had to refer to the cited papers, and I found two papers that share many aspects with this one. One is \"Causality for Functional Longitudinal Data,\" which is cited by (Ying, 2024) in this paper, and the other is \"Causality for Complex Continuous-Time Functional Longitudinal Studies with Dynamic Treatment Regimes,\" which is not cited in this paper. The title of this paper is \"Causal Identification for Complex Functional Longitudinal Studies,\" suggesting that the most significant update in this paper is the \"complex\" aspect in the title. The paper \"Causality for Complex Continuous-Time Functional Longitudinal Studies with Dynamic Treatment Regimes\" seems to have updated the dynamic treatment regimes element one step further.\n\nSpecifically, in the preparation section, the time-to-event endpoint $T$, $C$, $X$, and $\\Delta$ are newly defined, assuming a more complex situation where the study may be forcibly terminated due to an event like death before the study is completed. This updates Theorems 1, 2, and 3 from the previous paper. I would like to ask about the academic and practical significance of solving the more complex problem introduced by those new variables. The most important contribution of this paper seems to be the introduction of the non-parametric property through Theorem 4. Why is the non-parametric property important in a functional data framework? The paper states that it makes the model more flexible and adaptable to various data, but doesn't the continuous functional data, which is more extensive, lead to increased computational and implementation complexity, a common drawback of non-parametric models? Doesn't this also create issues with the interpretability required for healthcare data analysis?\n\nOne of the most important equations in this paper seems to be Equation (1). The rest of the paper is dedicated to finding another representation of Equation (1). However, it is not easy for readers to immediately understand what Equation (1) means and why we should be interested. Additionally, it is not straightforward to grasp what $\\mathbb{G}$ represents. By referring to the paper by Ying (2024), I could somewhat understand $\\mathbb{G}$ through the following example:\n*When the causal outcome under a specific regime $\\bar{a}$ is of interest, for instance, all patients were under treatment, the point mass (delta) measure $\\mathbb{G} = \\mathbb{1}(\\bar{A} = \\bar{a})$ can be considered.*\nIncluding this example in this paper would help in understanding. Furthermore, providing a concrete example of what $\\nu$ represents would help comprehend Equation (1). Can Equation (1) be understood as a general expression representing the average treatment effect, the averaged treatment outcome, or a transformed form of these?\n\nIf the non-parametric property is a major contribution of the paper, it should be demonstrated through more concrete experimental examples, such as using the MIMIC-IV data mentioned in the introduction. The experimental section currently numerically verifies Theorem 1, which has already been proven in the Appendix, but a demonstration of Theorem 4 seems more necessary. However, under Theorem 4, it is only mentioned that \"we have not achieved the full nonparametric paradigm.\"\n\nAdditionally, there is a need to clearly and definitively define loosely defined \u201cfunctional\u201d data. The abstract of this paper describes it as \"characterized by continuous-time measurements,\" while another cited paper describes it as \"characterized by continuous-time processes and high-dimensional measurements.\" I believe \"continuous\" alone is not sufficient to be called functional. What is the rationale for developing a framework that assumes functional continuity in the model even though real-world healthcare data does not have mathematically rigorous time continuity and does not observe over an infinite time? (The previous paper assumed up to time $\\tau$, but this paper assumes up to $\\infty$.) What is the justification for this assumption?"
            },
            "questions": {
                "value": "Please provide additional explanations for the questions raised in the Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Figure 1 is directly taken from Figure 2 of the paper \"Causality for Complex Continuous-Time Functional Longitudinal Studies with Dynamic Treatment Regimes\" submitted to the Annals of Statistics (https://arxiv.org/pdf/2406.06868). A citation should be included."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}