{
    "id": "jtY2OHKj4a",
    "title": "Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented Large Language Models",
    "abstract": "Large-scale language models (LLMs) have achieved remarkable success across various language tasks but suffer from hallucinations and temporal misalignment. To mitigate these shortcomings, Retrieval-augmented generation (RAG) has been utilized to provide external knowledge to facilitate the answer generation. However, applying such models to the medical domain faces several challenges due to the lack of domain-specific knowledge and the intricacy of real-world scenarios.\nIn this study, we explore LLMs with RAG framework for knowledge-intensive tasks in the medical field. To evaluate the capabilities of LLMs, we introduce MedicineQA, a multi-round dialogue benchmark that simulates the real-world medication consultation scenario and requires LLMs to answer with retrieved evidence from the medicine database. MedicineQA contains 300 multi-round question-answering pairs, each embedded within a detailed dialogue history, highlighting the challenge posed by this knowledge-intensive task to current LLMs. We further propose a new \\textit{Distill-Retrieve-Read} framework instead of the previous \\textit{Retrieve-then-Read}. Specifically, the distillation and retrieval process utilizes a tool calling mechanism to formulate search queries that emulate the keyword-based inquiries used by search engines. With experimental results, we show that our framework brings notable performance improvements and surpasses the previous counterparts in the evidence retrieval process in terms of evidence retrieval accuracy. This advancement underscores the framework's potential to effectively address the inherent challenges of applying RAG models to the medical domain.",
    "keywords": [
        "Large Language Models",
        "Medication Consultation",
        "Retrieval-Augmented Generation",
        "Tool Call"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "This study introduces a benchmark containing multi-round dialogues for medication consultation and a novel retrieval-augmented framework with the tool calling mechanism.",
    "creation_date": "2024-09-14",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=jtY2OHKj4a",
    "pdf_link": "https://openreview.net/pdf?id=jtY2OHKj4a",
    "comments": [
        {
            "summary": {
                "value": "This paper built a multiple-round dialogue benchmark MedicineQA and proposed a distill-retrieve-read framework for RAG improved medication consultation capability of LLM. The dataset size is limited, and the multiple-round dialogues were generated by GPT-4 but not from real-world dialogue; this may affect the value of the application for the proposed framework. More details about the dataset should be introduced, such as the number of dialogue rounds.  It seems the dialogue distilling tool was trained on the general domain synthetic dataset but not for the medical domain, and the details of the dialogue distill tool training are not clear (which base model? data size? training settings? list of data sources). The proposed method lacks novelty; it applies the dialogue distilling module in the RAG setting and tests on the small MedicineQA dataset. Experiments are only based on a single dataset.\nTypo? The manuscript mentioned \"Table 3.2,\" but this is not Table 3.2; I guess it should be Table 1.\nQuestion: Will the medicine database be released to the public?"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper proposed a new dataset for medication consultation."
            },
            "weaknesses": {
                "value": "The dataset size is limited, and the multiple-round dialogues were generated by GPT-4 but not from real-world dialogue; this may affect the value of the application for the proposed framework. More details about the dataset should be introduced, such as the number of dialogue rounds.  It seems the dialogue distilling tool was trained on the general domain synthetic dataset but not for the medical domain, and the details of the dialogue distill tool training are not clear (which base model? data size? training settings? list of data sources). The proposed method lacks novelty; it applies the dialogue distilling module in the RAG setting and tests on the small MedicineQA dataset.  Experiments are only based on a single dataset."
            },
            "questions": {
                "value": "Typo? The manuscript mentioned \"Table 3.2,\" but this is not Table 3.2; I guess it should be Table 1.\nQuestion: Will the medicine database be released to the public?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a benchmark, database and method for describing medications in dialogues with LLMs. The benchmark is collected automatically and reviewed by medical experts and the database is curated to support retrieval of medication information. The paper evaluates the proposed approach of converting conversations with their histories into a set of keywords to query the medical database and the results indicate favourable outcomes, based on GPT-4- and human-based evaluations of the generations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The collected resources seem helpful and, even if collected automatically, are validated by domain experts. The proposed approach seems to improve over LLM-based baselines.\n\nThe experiments seem sound in what they report/investigate.\n\nThe overall writing quality is good."
            },
            "weaknesses": {
                "value": "My main concerns is the lack of meaningful comparison and under-specification of some of the details.\n\n- While the paper suggests that the proposed distill-then-retrieve framework is better than RAG methods, I did not find meaningful comparisons with either general RAG frameworks as e.g. discussed in lines 117-126, or with domain-specific RAG approaches, such as e.g. BioReader (https://aclanthology.org/2022.emnlp-main.390.pdf) RAFT (https://arxiv.org/abs/2403.10131).\n\n- The experimental setting is somewhat under-specified, see my questions to the authors. In summary, I don't understand the exact settings for the comparisons in Tables 2 and 3.\n\n- There is very little detail on the construction of the medical database, which seems to be the key contribution enabling the retrieve-then-distill framework. Particularly, data sources, data quality measures etc are missing.\n\nI gravitate towards rejection but if my queries are answered with sufficient detail, I am open to re-evaluating my decision."
            },
            "questions": {
                "value": "1) What is the use of the question annotations in MedicineQA? If I understand correctly, it is the explicit reformulation of the questions from the conversation history (which might be ambiguous and contain superfluous information, as shown in \"Dialogue History\" in Figure 3). I understand that the synthetic dataset collected and described in lines 257-264 is used to convert questions into keywords, but during inference, what is actually used as input? Is it the dialogue history, of the extracted questions. If the latter, how are they extracted from dialogue history?\n\n2) Could you please indicate how you measure the H@K for \"baseline models\". From reading lines 300-306, It is not clear to me what the baselines are actually doing. Are they generating the medication names from their parameters? Are they generating queries that are run against the database? \n\n3) Furthermore, could you please elaborate on what you mean by \"we immediately adopt the attached corresponding medicine information as the context to guide the generation [...]\" in line 323. Is it the ground-truth medicine annotations from MedicineQA? If so, I find this result highly counter-intuitive and contradictive, as the paper states later, in lines 389 ff, that the improvements of RagPULSE are due to the proposed RAG framework rather than due to domain fine-tuning of the PULSE LLM. But since RagPULSE's retrieval is far from optimal (e.g. H@k rates in Tables 2 and 3), it would suggest that imperfect retrieval somehow leads to better generations. Similarly, could you please indicate what \"instruction used for baseline models\" in the description of Table 3 means: \"We also prompt RagPULSE by the instruction used for baseline models, which are denoted as \u2020\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper has introduced a benchmark called 'MedicineQA' to evaluate llms in medical consultation. Their suggested benchmark is focusing on rag for multi-turn dialogues in medication consultation. The authors also propose RagPULSE framework to deal with limitations of standard RAG frameworks. This framework utilizes a tool-calling mechanism to enhance retrieval accuracy by summarizing dialogue history into search queries and retrieving evidence from a medicine database."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper shows an approach to enhancing LLMs in the medical domain by reformulating rag for multi-round medication consultations. Also, using a tool-calling mechanism for search query formulation is a novel method of tailoring retrieval processes in dialogue-based tasks in healthcare. The paper shows good quality in methodology. Also, the authors did around 600 experiments to validate the effectiveness of ragpulse and provided comparisons with existing open-source and commercial models. The paper has clear objectives, methods, and findings sections written. The study addresses a research gap in applying LLMs to the medical field and introduced MedicineQA, ragpulse. Their work shows good retrieval and response generation capabilities."
            },
            "weaknesses": {
                "value": "I feel that the number of multi-round dialogues (300) are very less and that makes it difficult in terms of generalizing the results. So, maybe increase the dataset size by expanding the set of topics like for various specific conditions like breast cancer, diabetes, chronic pain.\n\nAnother thing is that the study should discuss retrieval errors for the cases where the model fails to retrieve accurate responses. The authors can try to include 'error analysis' section (maybe) that show various retrieval failures types or examples. This analysis will certainly go a long way in understanding if any adjustments should be made in distillation process."
            },
            "questions": {
                "value": "1. How ragpulse deals with retrieval errors (if any)? Also, did you see any failure modes commonly found and can you discuss them, please?\n\n2. The benchmark is available in English and Chinese as per the paper. but how well do we know if MedicineQA can be adapted to other languages (maybe low-resource languages)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}