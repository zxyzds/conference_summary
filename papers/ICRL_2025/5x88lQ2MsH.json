{
    "id": "5x88lQ2MsH",
    "title": "Bonsai: Gradient-free Graph Distillation for Node Classification",
    "abstract": "Graph distillation has emerged as a promising avenue to enable scalable training of GNNs by compressing the training dataset while preserving essential graph characteristics. Our study uncovers significant shortcomings in current graph distillation techniques. First, the majority of the algorithms paradoxically require training on the full dataset to perform distillation. Second, due to their gradient-emulating approach, these methods require fresh distillation for any change in hyperparameters or GNN architecture, limiting their flexibility and reusability. Finally, they fail to achieve substantial size reduction due to synthesizing fully-connected, edge-weighted graphs. To address these challenges, we present Bonsai, a novel graph distillation method empowered by the observation that *computation trees* form the fundamental processing units of message-passing GNNs. Bonsai distills datasets by encoding a careful selection of *exemplar* trees that maximize the representation of all computation trees in the training set. This unique approach imparts Bonsai as the first linear-time, model-agnostic graph distillation algorithm for node classification that outperforms existing baselines across $6$ real-world datasets on accuracy, while being $22$ times faster on average. Bonsai is grounded in rigorous mathematical guarantees on the adopted approximation strategies making it robust to GNN architectures, datasets, and parameters.",
    "keywords": [
        "Graph Neural Networks",
        "Machine Learning",
        "Data Distillation",
        "Graph Distillation",
        "Dataset Distillation",
        "Sustainable AI"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "An unsupervised and model/hyper-parameter agnostic graph distillation algorithm for Node CLassification",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5x88lQ2MsH",
    "pdf_link": "https://openreview.net/pdf?id=5x88lQ2MsH",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a novel graph distillation method empowered by the observation that computation trees form the fundamental processing units of message-passing GNNs. This paper specifically addresses the issue of overly dense edges in graph distillation. Experiments on various datasets verify the effectiveness of the method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Compared to previous works, BONSAI is novel.\n2. The experimental results look very good, especially regarding the training time.\n3. The theoretical analysis is solid."
            },
            "weaknesses": {
                "value": "1. This paper is not easy to understand.\n2. In some cases, BONSAI does not perform the best, such as with citeseer.\n3. Regarding table 5, can you provide experimental results for other compression rates?\n4. PPR and RkNN involve many parameters, and the ablation study in Fig. 4(b) is insufficient."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies critical limitations in existing graph distillation methods. It introduces a new approach that optimizes the training of GNNs by generating smaller, representative datasets without requiring full training on the original data. The proposed method leverages a computation tree-based approach to create a distilled representation that is efficient, adaptable, and capable of maintaining high accuracy across varied datasets and models. Experimental results have demonstrated its effectiveness and efficiency."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed gradient-free approach bypasses the need for computationally expensive gradient calculations, resulting in a significantly faster distillation process. This efficiency makes Bonsai highly scalable even for large datasets.\n2. This model-agnostic method is interesting and saves efforts in hyperparameter tuning when changing condensation models.\n3. It is the first distillation method that retains the original node features and synthesizes graphs with unweighted edges, which more faithfully represent the original graph structure."
            },
            "weaknesses": {
                "value": "1. The idea of Bonsai is very similar to MIRAGE[1], as both methods select frequent trees. This similarity makes Bonsai appear to be a minor adaptation of MIRAGE. Furthermore, much of the theoretical analysis, such as Graph Isomorphism, is borrowed from MIRAGE. Although these two works focus on different tasks, it's strongly recommended to discuss the differences between Bonsai and MIRAGE in the related work section.\n2. This paper claims that \"distilling to fully-condensed graph\" is a problem for previous work. However, most prior methods include a sparsification step, setting a threshold to obtain a sparse graph. Consequently, the number of edges reported in Table 2 is inaccurate. For correct edge counts, please refer to Table 5 in the GCond paper [2].\n3. Although this paper presents comprehensive deductions in the theoretical section, some hypotheses appear to be too strong and lack support. For example, Hypothesis 1 (line 201) and Logical Progression 2 (line 215) may not hold true.\n4. In Fig. 2, the authors empirically demonstrate the correlation between GNN embedding and WL embedding. When the threshold is large, almost all node pairs are considered, leading to a low correlation. How does this observation inform the method design?\n5. In line 235, **Diversity** is highlighted. Which part of the method addresses this concern?\n6. To select the trees with top representativeness, why do the authors choose Reverse K-NN? Would it be possible to simply adopt clustering instead?\n7. The experimental settings differ from commonly used ones in the following ways: (a) Different dataset split (i.e., training/validation/test set split) (b) Different metric for compression rate. The authors are suggested to clarify the reasons for choosing a different setting.\n\n\n### Minor\n1. The details of PPR starting from line 332 could be simplified or moved to the Appendix as people are familiar with it.\n2. Why is Herding OOT (out of time) for Reddit? In my understanding, this method should be efficient.\n\n### References\n[1] Mridul Gupta, Sahil Manchanda, HARIPRASAD KODAMANA, and Sayan Ranu. Mirage: Model-\nagnostic graph distillation for graph classification. In ICLR 2024\n\n[2] Wei Jin, Lingxiao Zhao, Shichang Zhang, Yozen Liu, Jiliang Tang, and Neil Shah. Graph condensa-\ntion for graph neural networks. In ICLR, 2021."
            },
            "questions": {
                "value": "Please see the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents Bonsai, a gradient-free graph distillation method for node classification that overcomes limitations in existing approaches. By selecting exemplar computation trees rather than synthesizing fully connected graphs, Bonsai efficiently distills graphs without needing full dataset training or specific GNN architectures. This approach achieves higher accuracy and faster distillation across various datasets and GNN models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The\u00a0performance\u00a0of\u00a0bonsai\u00a0is\u00a0impressive,\u00a0including\u00a0the\u00a0cross-arch\u00a0results.\n2. The\u00a0proof\u00a0of\u00a0maximizing\u00a0the\u00a0representative\u00a0power\u00a0of\u00a0exemplars\u00a0is\u00a0NP-hard\u00a0is\u00a0simple\u00a0but\u00a0attracting."
            },
            "weaknesses": {
                "value": "See\u00a0questions."
            },
            "questions": {
                "value": "1. Could the authors clarify their specific contributions to Mirage [A]? While Mirage highlights the importance of the computation tree in graph dataset condensation, their primary focus is on the graph level (with node-level experiments included in the appendix). Thus, extending their approach to the node level by following the 1-WL test may not represent a substantial novelty.\n2. Regarding Fig. 4(b), although the authors emphasize the significance of the RkNN and PPR components, the random exemplar selection still yields considerable results. How do the authors interpret this outcome? Could they also provide a performance comparison for $\\mathbf{S_r}$ using different exemplars on datasets like Cora, Ogbn-arxiv, and Reddit? Including results for random selection would further clarify the comparison.\n3. From my perspective, selecting datasets using a sampling strategy seems more akin to traditional graph reduction, sparsification, or coarsening methods, rather than directly aligning with the field of graph condensation. Therefore, it\u2019s challenging to accept the significant improvements claimed over random or herding baselines. Could the authors provide an intuitive example to support their approach?\n\n[A] Mridul Gupta, Sahil Manchanda, HARIPRASAD KODAMANA, and Sayan Ranu. Mirage: Model\u0002\nagnostic graph distillation for graph classification.\u00a0ICLR\u00a02024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Bonsai, a linear-time, model-agnostic graph distillation algorithm for node classification. It observe three limitations of previous works, Full Gnn training, Distilling to a fully-connected graph and Model-specific. To address these limitations, Bonsai  aims to identify a set of b exemplar computation trees that optimally represent the full training set."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1: The problem is important in this field.\n\nS2: Presentation is good. Good model name.\n\nS3: The solution looks novel to me."
            },
            "weaknesses": {
                "value": "W1: The experiments are not very standard. \n\nW2: Some recent works are not included as baseline."
            },
            "questions": {
                "value": "Q1: Why the authors choose 0.5%, 1% and 3% as $S_r$? This setting does not align with previous works.\n\nQ2: Recently, a lot of works have studied the efficiency of graph condensation, e.g., Exgc [1] and GC-SNTK [2]. These two methods should be included as baselines when comparing condensation time. By the way, it would be better to present time comparison via table than figure.\n\nQ3: Other graph condensation methods should be included for accuracy comparison, e.g., SGDD [3], SFGC [4], GEOM [5].\n\n[1] Exgc: Bridging efficiency and explainability in graph condensation\n\n[2] Fast Graph Condensation with Structure-based Neural Tangent\n\n[3] Does graph distillation see like vision dataset counterpart\n\n[4] Structure-free graph condensation: From large-scale graphs to condensed graph-free data\n\n[5] Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}