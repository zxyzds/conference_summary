{
    "id": "P5icyaAReM",
    "title": "Stochastic Approximation to Contrastive Learning",
    "abstract": "Contrastive learning is a powerful paradigm that has been crucial for self-supervised representation learning. While there is evidence for its effectiveness, these methods typically rely on arbitrary definitions of positive and negative pairs. Most existing contrastive learning methods require large batch sizes during training due to their rigid control over the tradeoff between the two contrastive terms. Consequences are that, substantial computational resources are wasted on negative pairs that provide minimal learning signals. To address this issue, this work present a novel method. We reformulate contrastive learning as a matrix approximation problem using I-divergence, a non-normalized form of Kullback-Leibler divergence. Our proposed objective function is decomposable across instance pairs, enabling the development of efficient stochastic approximation algorithms from neighbor embeddings which perform well with fewer negative samples. Additionally, we generalize the scaling factor beyond normalization, allowing it to adaptively emphasize positive pairs that carry more learning signals, thereby reducing the computational waste associated with negative pairs. Experimental results on visual representation learning benchmark datasets such as CIFAR and ImageNet demonstrate major improvements over other contrastive learning methods, particularly when using small batches and with only one negative pair.",
    "keywords": [
        "contrastive learning",
        "self-supervised learning",
        "unsupervised learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "New contrastive learning method which works well with one negative sample.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=P5icyaAReM",
    "pdf_link": "https://openreview.net/pdf?id=P5icyaAReM",
    "comments": [
        {
            "comment": {
                "value": "Contrary to the authors' claim, SACLR has not been comprehensively benchmarked against SimSiam, SogCLR, and iSogCLR. Specifically, SimSiam results are missing from Tables 1 and 2, iSogCLR results are absent from Tables 2 and 3, and SogCLR is not included in Table 3.\n\nIn response to the authors' question regarding additional contrastive learning methods to incorporate, it is recommended to incorporate the top-performing benchmarks from Table 1 in [A], which is already referenced in your manuscript. Notably, the results for SimCLR, MoCo v2, SimSiam, SwAV, InfoMin Aug, OBoW, BYOL, SwAV, Barlow Twins, and VICReg in Table 1 of [A] surpass those presented in Table 2 of the authors' manuscript. Additionally, referencing Tables A2 and A3 in [B] is encouraged, as they demonstrate that SimCLR can achieve higher performance than the results shown in your Table 2. For ImageNet-100 results in Table 1, it would be valuable to include results from [C].\n\nFinally, to substantiate claims of your method\u2019s superiority, it is strongly recommended to conduct additional experiments in semi-supervised learning, transfer learning for image classification, and transfer learning for object detection and instance segmentation (see Tables 2, 3, and 4 in [D] for reference).\n\n[A] Bardes, A., Ponce, J., & LeCun, Y. (2021). Vicreg: Variance-invariance-covariance regularization for self-supervised learning. arXiv preprint arXiv:2105.04906.\n\n[B] Chen, T., Luo, C., & Li, L. (2021). Intriguing properties of contrastive losses. Advances in Neural Information Processing Systems, 34, 11834-11845.\n\n[C] Kalantidis, Y., Sariyildiz, M. B., Pion, N., Weinzaepfel, P., & Larlus, D. (2020). Hard negative mixing for contrastive learning. Advances in neural information processing systems, 33, 21798-21809.\n\n[D] Zbontar, J., Jing, L., Misra, I., LeCun, Y., & Deny, S. (2021, July). Barlow twins: Self-supervised learning via redundancy reduction. In International conference on machine learning (pp. 12310-12320). PMLR."
            }
        },
        {
            "title": {
                "value": "Which additional contrastive learning methods would you like us to include in the comparison?"
            },
            "comment": {
                "value": "Regarding your third point in the Weakness section: \"From an experimental standpoint, there is also an inadequate comparison of SACLR\u2019s performance across diverse benchmarks. Many studies have introduced methods that significantly improve upon SimCLR, yet these methods are omitted from the benchmark comparisons in this study.\" We have already included SimSiam, SogCLR, and iSogCLR. Could you please specify which additional contrastive learning methods should be incorporated into the empirical comparison? We are eager to hear your suggestions and will work on adding the corresponding experiments within the next two weeks."
            }
        },
        {
            "summary": {
                "value": "This work proposes an approach to reformulate contrastive learning, used in many self-supervised learning methods such as SimCLR, in terms of matrix approximations. This reformation enables stochastic approximations of contrastive learning to enable good performance with smaller batch sizes. The authors validate the idea with experiments on CIFAR and ImageNet by measuring linear evaluation classification accuracy as well as embedding projections. The authors compare the method against standard self-supervised training method such as SimCLR as well as stochastic approximation methods such as SogCLR that also aim to reduce the need for large batch sizes in self-supervised training."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors study the important problem of revisting contrastive learning's reliance on large batch sizes. The authors present an interesting reformulation of contrastive learning by taking inspiration from stochastic cluster embeddings. This point of view is an interesting perspective on standard contrastive learning \nExperimentally, the authors train SACLR on CIFAR and ImageNet using standard setup and compare against a reasonable set of baselines. I appreciate the author's notes on which libraries were used for the method implementations, aiding reproducibility (although I do hope the authors also release code for the proposed method)."
            },
            "weaknesses": {
                "value": "# Presentation\n* The introduction is quite lengthy and rehashes broadly well-known statements about the role of deep learning. I'd recommend the authors trim this lengthy introduction. \n* With this gained space, I recommend 1) the authors describe in more detail the similarities and differences to existing stochastic methods such as SogCLR that also tackle the same challenge of large batch sizes in contrastive learning. 2) the authors consider illustrating the method visually with a method figure. Right now the method as presented in Section 3 and Algorithm 1 requires parsing many variables, with nested summations that muddy the overall idea and intuition. A high-level intuitive figure to illustrate the method can help a great deal in this regard.\n* The performance gains on ImageNet are marginal compared to SogCLR (and I'd be curious how they compare to iSogCLR as well). While this doesn't diminish the contribution in itself, I do recommend the authors downtone the claims made in the introduction and abstract accordingly. \n\n# Claims\n* The authors claim our method is \"more computationally efficient\" (line 72), \"signficant computaitonal resources are wasted on negative pairs that offer little learnign signal\" (line 61), and again in the abstract (line 24). This claim is missing exactly what the efficiency is relative to\u2014is it existing efficient methods such as SogCLR or standard SimCLR? This needs clarification and evidence. To support this claim the authors should provide evidence of the GPU training hours, flops, or amount of memory saved, which right now I could not find.\n\n* The authors state SCE requires fixed input similarites and cannot generalize to data points outside the training set (lines 177-179), after which they suggest their method overcomes these limitations with three modification (using a deep neural network, using augmented view and row-ise vector approximation), however, no further discussion of how these three modification address the two limitations highlighted is provided. Can you the authors respond provide a detailed justification for this claim?\n\n- The authors only provide classification accuracy figures using linear evaluation. I recommend the authors also include other evaluation common approaches such as finetuning, KNN-evaluation, and retrieval (see for example SogCLR) to provide a more complete picture of the methods' benefits."
            },
            "questions": {
                "value": "- Performance on ImageNet100, I'm curious why the authors suspect the matrix versus row method difference is much larger on ImageNet100 compared to ImageNet?\n- I'm confused by what the comparison in Figure 1 shows. Is t-SNE panel illustrating standard SimCLR feature projections compared to SACLR? If so, maybe a better label is warranted?\n- Which dataset is used in the ablations for figure 2? I'm also curious how the linear evaluation accuracy compares for SACLR-1 versus SACLR-all."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the contrastive learning paradigm, which is a typical and important representation learning method. Concretely, this paper focuses on its substantial computation resources and inspired by the Stochastic Cluster Embedding (SCE) method, authors reformulate contrastive learning as a matrix approximation problem, thus the objective function can be decomposable over instance pairs and generalize well for smaller batchsize. Extensive experiments are conducted on the CIFAR and ImageNet datasets to validate the effectiveness of the proposed loss."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ The investigated problem is meaningful. \n+ The writing is good. The derivation of the mathematical formula is convincing. \n+ The experimental results are comprehensive."
            },
            "weaknesses": {
                "value": "Overall, this paper is readable and the proposed loss is convincing. But I still have some concerns as follows. \n\n- The logic behind the proposed loss over the robustness to the fewer negative samples is unclear. Since B refers to the batchsize, I do not see the analysis over various B in the formula or theorem. \n\n- The computational complexity should be analyzed or empirically investigated since it needs to calculate s in Eq.(6). \n\n- The baseline methods are sort of weak. Since it focuses on the smaller batchsize, some other competing methods in terms of smaller batchsize should be included as well. For example, ReSSL (NeurIPS 2021) achieves 69.9% (Table 6) on ImageNet with ResNet50 which is much higher than that in this paper."
            },
            "questions": {
                "value": "Please refer to the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the problem of large batch sizes required for training networks under contrastive loss. The authors use ideas from Stochastic Cluster Embedding to represent the contrastive loss as a matrix approximation problem with approximation quality measures by I-divergence, an unnormalized form of KL divergence. To adapt the Stochastic Cluster Embedding setup to representation learning under contrastive loss, the authors represent the embedding using a neural network. The authors provide two algorithms: one with a row-wise scaling/normalization factor (similar to SimCLR under uniform normalization) and one with a single scaling/normalization factor over the whole matrix. In both cases, the authors treat the scaling factor as a constant during gradient computation, with the scaling factor updated after every iteration using a weighted sum of the values of the embedding matrix. The weighted formulation of the scaling factor allows the objective to emphasize the positive samples as training progresses.\n\nThe authors also apply the proposed methods on CIFAR and Imagenet datasets to demonstrate the superior performance of the proposed methods compared to other current SOTA Contrastive Learning methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper also addresses an important problem concerning the scalability of Contrastive Learning algorithms. The paper draws inspiration from the Stochastic Cluster Embedding paper to propose a novel formulation of the constrastive learning loss function.\nThe proposed weighting function $w$ to calculate the scaling factor $s$ during training provides a novel and interesting way to adaptively trade off the weight of signals from the positive and negative pairs during training."
            },
            "weaknesses": {
                "value": "While the theoretical motivation of the paper is quite interesting, it is not clear why the scaling quantity $s$ is considered to be a constant in the loss function. As seen later in the paper, $s$ is a function of $q^{uv}_{ij}$ and changes during training. \n\nWhile the experiments on CIFAR and Imagenet show a slight performance improvement, the experiments do not include any variance bounds on the reported metrics. Without any confidence bounds, the proposed methods' relative performance boost is unclear. \n\nFurthermore, the paper doesn't include any experimental data on the runtime or memory improvements the proposed method provides. It is unclear how much efficiency can be gained using $M=1$ instead of $M>1$ negative samples in a batch. It would be great to see the performance differences as well as runtime differences between the baselines and the proposed method under different values of $M$."
            },
            "questions": {
                "value": "Please refer to the Weakness Section for the questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This study proposes a method, SACLR, inspired by Stochastic Cluster Embedding (SCE), a similarity-based nonlinear dimensionality reduction (NLDR) technique. SACLR reformulates contrastive learning as a matrix approximation problem using I-divergence, an unnormalized form of Kullback-Leibler divergence."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This study proposes the SACLR method, which enhances existing contrastive learning approaches. Representations learned through SACLR are optimized to support effective clustering."
            },
            "weaknesses": {
                "value": "This study lacks novelty in its methodology. SACLR appears to be a straightforward application of SCE, incorporating a few modifications to SCE. However, the study does not sufficiently explain why these modifications are important or what roles they play within the model framework.\n\nTheorem 1, the only theoretical result presented in this study, fails to justify why SACLR would be expected to outperform SimCLR. Since Theorem 1 does not highlight any specific advantages of SACLR over existing contrastive or non-contrastive methods, including SimCLR, the theoretical contribution appears limited in its significance.\n\nFrom an experimental standpoint, there is also an inadequate comparison of SACLR\u2019s performance across diverse benchmarks. Many studies have introduced methods that significantly improve upon SimCLR, yet these methods are omitted from the benchmark comparisons in this study.\n\nAdditionally, the study lacks a comprehensive ablation analysis. Although it claims that adjusting the scaling factor can reduce computational waste, there is neither a theoretical explanation for why this adjustment reduces computation nor a rigorous ablation study to support this claim. Similarly, while the study asserts that SACLR performs well with small batch sizes, it does not provide an extensive ablation analysis to substantiate this claim."
            },
            "questions": {
                "value": "In Figure 1, this study demonstrates that SACLR achieves effective clustering. However, does strong clustering necessarily indicate a good representation? Additionally, is there evidence that SACLR performs well on various downstream tasks beyond clustering?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}