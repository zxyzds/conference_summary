{
    "id": "cho9iE9POr",
    "title": "Low-Budget Simulation-Based Inference with Bayesian Neural Networks",
    "abstract": "Simulation-based inference methods have been shown to be inaccurate in the data-poor regime, when training simulations are limited or expensive.\nUnder these circumstances, the inference network is particularly prone to overfitting, and using it without accounting for the computational uncertainty arising from the lack of identifiability of the network weights can lead to unreliable results.\nTo address this issue, we propose using Bayesian neural networks in low-budget simulation-based inference, thereby explicitly accounting for the computational uncertainty of the posterior approximation.\nWe design a family of Bayesian neural network priors that are tailored for inference and show that they lead to well-calibrated posteriors on tested benchmarks, even when as few as $O(10)$ simulations are available.\nThis opens up the possibility of performing reliable simulation-based inference using very expensive simulators, as we demonstrate on a problem from the field of cosmology where single simulations are computationally expensive. We show that Bayesian neural networks produce informative and well-calibrated posterior estimates with only a few hundred simulations.",
    "keywords": [
        "simulation-based inference",
        "approximate Bayesian inference"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=cho9iE9POr",
    "pdf_link": "https://openreview.net/pdf?id=cho9iE9POr",
    "comments": [
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "> Why is the convergence to the NPE solution so slow in terms of the number of simulations? Can the prior be altered to avoid this problem, rather than introducing a temperature parameter?\n\nThe prior we define is the reason why we obtain calibrated approximations. Altering the prior would then probably negatively affect the coverage results, as seen in Figure 1. Using a temperature that is too low could also affect the coverage results; it should be tuned to have the appropriate trade-off. \n\n---\n\nOverall, we think we have addressed each concern raised in your review and implemented changes accordingly. While these improvements appear to resolve the identified issues, we'd value your perspective on whether any aspects still fall short of expectations. If the changes adequately address your concerns, we'd appreciate having this reflected in the evaluation."
            }
        },
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "> The BNN-NRE method appears to perform worse than NRE in terms of mass placed on true parameters, even for low simulation budgets (100-1000 in Figure 2), which is the domain where the method is proposed to be beneficial. Similarly, the results for the NPE case are not particularly convincing in terms of the mass placed on the true parameters. The coverage properties do appear to have improved, but coverage alone is not indicative of a good posterior estimate.\n\nIndeed, this is a limitation that our method might perform worse in terms of mass placed on true parameters, but this is not what we aim for. This work follows the philosophy highlighted by Hermans et al., 2022, where they argue that to be useful for scientific application, posterior approximations must be conservative before anything else. If that is not the case, then they cannot be used reliably for downstream scientific tasks. In this work, the main objective is to build SBI methods that are conservative for low simulation budgets. In that regard, our method provides significant improvement. If one is only interested in placing the mass on true parameters for their application and not in conservativeness, then our method is probably not the method to use.\n\nJoeri Hermans, Arnaud Delaunoy, Francois Rozet, Antoine Wehenkel, Volodimir Begy, and Gilles Louppe. A crisis in simulation-based inference? beware, your posterior approximations can be unfaithful. Transactions on Machine Learning Research, 2022\n\n> The results don't align with an intuitive understanding of epistemic uncertainty. For example, in figure 3, in NPE, we can see 4000 simulations produces a reasonable posterior estimate, whereas BNN-NPE is still massively conservative, even with 65536 simulations, suggesting overestimation of epistemic uncertainty. This suggests simple alternative approaches such as training posterior estimates on subsets of the data with standard methods, such as NPE, and mixing the resulting posteriors, would likely be more effective. I understand the temperature parameter is introduced to limit this problem, but this then introduces another hard to choose hyperparameter (along with the Gaussian process parameters introduced).\n\nIf the model is very uncertain about where to put the mass, then providing a large credible interval means that epistemic uncertainty has been correctly estimated. A good estimation of epistemic uncertainty does not mean providing the correct posterior. In opposition, for a budget of 256, we see that NPE provides a smaller interval than BNN-NPE, but the interval produced by NPE is completely wrong. This means that BNN-NPE estimated the epistemic uncertainty correctly by outputting a very uncertain posterior approximation, while NPE outputs an overconfident posterior approximation. We should also keep in mind that Figure 3 only provides an example, and the correct estimation of epistemic uncertainty cannot be grasped from a single example. To have a correct view of how well epistemic uncertainty is captured, one should look at the coverage curves.\n\n> How can we be sure that any benefits are from the BNN modelling epistemic uncertainty, and not the altered initialization? For example, if we \"pretrained\" NPE/NRE to prior samples (and \n simulated from some noise distribution), such that at \"initialization\" the posterior estimate would be approximately equal to the prior for any, it would be interesting to see if there is still be any benefit to the introduced method.\n \nThe benefits sure come first from the fact that we have an appropriate prior over weights, which is the main contribution. Then, either the maximum a posteriori could be used or the full Bayesian posterior over weights. In both settings, we benefit from having a better prior, but to capture epistemic uncertainty, using the full Bayesian posterior is needed.\n\n> Most the benefits seem to be in the very small data regime (<100 simulations). I am not convinced that this scenario is of particular interest the scientific community, could you give an example of a practical case where this is useful?\n\nFirst, it should be noted that what happens for <100 simulations on our benchmarks could happen for more simulations on much more complicated simulators. Solving the issue for very low budgets on our benchmarks is hence desired. Second, the cosmological application has been chosen to be one of such examples. 35 million CPU hours have been dedicated to generating $44,000$ simulations. This is roughly equivalent to 33 CPU-days for **each single** simulation. This is not something manageable without having access to large computing facilities. Before that massive amount of computing was dedicated to those simulations, a too low amount of simulations were available. With our method, it would have been possible to produce calibrated approximations with a reasonable amount of computation."
            }
        },
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "First, thank you for your review. We are happy to hear that you found the contribution broadly useful to the scientific community, sensible, novel and clever, that the paper mostly well-structured and that the experiments interesting and that the choice of metrics is appropriate.\n\nWe adress your concerns bellow.\n\n> Lack of method summary: There's no clear summary of the method, e.g. in the contributions section, an algorithm, or the start of section 3. A brief overview of 3-4 sentences would be very beneficial for readers. It's only towards the end of section 3.2, that the overarching method begins to come together on a first read. In my opinion, the abstract also does not include enough information on the method.\n\nThanks for pointing this out. We have added the following sentence at the beginning of section 3 to guide the reader.\n\"In this section, we propose a novel SBI algorithm based on Bayesian neural networks with tuned prior on weights. The algorithm first optimizes a prior on weight to have desirable conservativeness properties. This tuned prior is then used to compute an approximate posterior on weights that is itself used to make predictions through Bayesian model averaging.\"\n\n> Lack of clear definitions: The concept of an \"a priori-calibrated Bayesian model\" should be defined more clearly when it is first introduced. I presume that an example of a well calibrated a priori model, would be one for which the Bayesian model average at initialization is equal to the simulator parameters prior $p(\\theta)$ for any $x \\ in \\mathcal{X}$? Presumably this includes a large variety of useful and non-useful models for modelling epistemic uncertainty (as suggested in equation 8).\n\nAn a-priori calibrated Bayesian model is defined as a prior over weights that lead to a calibrated Bayesian model average. If this Bayesian model average is the prior $p(\\theta)$ for all $x$, this is indeed a calibrated model. We added the following sentence to make things clearer:\n\"This means that the Bayesian model average computed using the prior normal on weights $p(w)$ is not calibrated.\"\n\n> Figures: The credibility figure in Figure 1 is confusing, either the standard deviations are not in order, or there is a typo one of the standard deviations. Figure 3 should have a labeled legend. Many of the font sizes are too small.\n\nIndeed, there are two standard deviations that are not in order, this has been changed. We also added the meaning of the legend of Figure 3 in the caption.\n\n> Limited experimental runs: Only 3 runs are performed for each experiment, and only the median is reported. This makes it hard to assess if differences between methods are significant. Repeating with more runs would be beneficial, although I am sympathetic to limitations in computational budget.\n\nWe agree that more runs would have been preferable. However, we estimated the computational cost of 3 runs to be approximately 25,000 GPU hours. This is because each point in the reported curves corresponds to 3 full training and testing procedures, and testing with Bayesian neural networks involves the approximation of the Bayesian model average for each test sample. Unfortunately, we do not have the computational power to do many more runs. That being said, we see from Figure 2 that all the methods but BNN-NPE show a coverage AUC below -0.1 for low simulation budgets on at least one benchmark. BNN-NPE shows only positive or very slightly negative coverage AUC. We believe we can then conclude that using BNN improves the conservativeness even if we have only 3 runs."
            }
        },
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "> As a potential user, I would find it very worrisome that BNN-NRE has significantly lower nominal log-posterior than standard methods. Indeed, for some tasks, it seems to require 5 orders of magnitude (for many other tasks 3 orders of magnitude) more simulations than standard methods.\n\nIt depends on what kind of application one is working on and what we are trying to achieve. This work follows the philosophy highlighted by Hermans et al., 2022, where they argue that to be useful for scientific application, posterior approximations must be conservative before anything else. If that is not the case, then they cannot be used reliably for downstream scientific tasks. In this work, the main objective is to build SBI methods that are conservative for low simulation budgets. In some settings, people might indeed not care that much about conservativeness and focus on statistical power. Our work does not aim to address this setting, and in those cases, other methods might indeed be preferred. We would also like to highlight that we focus on conservative simulation-based inference in the low simulation budget regime, a regime where other methods fall short. For large datasets, others methods like standard or balanced NPE/NRE might be prefered.\n\nJoeri Hermans, Arnaud Delaunoy, Francois Rozet, Antoine Wehenkel, Volodimir Begy, and Gilles Louppe. A crisis in simulation-based inference? beware, your posterior approximations can be unfaithful. Transactions on Machine Learning Research, 2022\n\n---\n\nOverall, we think we have addressed each concern raised in your review and implemented changes accordingly. While these improvements appear to resolve the identified issues, we'd value your perspective on whether any aspects still fall short of expectations. If the changes adequately address your concerns, we'd appreciate having this reflected in the evaluation."
            }
        },
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "First, thank you for your review. We are happy to hear that you found the contribution novel, interesting, potentially impactful, elegant and rigorous. \n\nWe adress your concerns bellow.\n\n> The paper overstates its claims. My main issue with this paper is that it overstates its claims and does not acknowledge the weakness of empirical results. Looking at Figure 2, and in particular for NRE: BNN-NRE never reaches the log posterior of the other methods (even for 1M simulations). Yet the authors state that the nominal log posterior density is on par with other methods for very high simulation budgets. Where does this claim come from. Similarly, in Figure 2 (NRE), the authors do not acknowledge that the AUC is not particularly much higher for BNN-NRE as compared to NRE, even for 10 simulations. Indeed, for three of the four tasks NRE has a higher coverage AUC than BNN-NRE for 10 simulations. On top of this, for many tasks and simulation budgets, BNN-NRE does have a negative AUC, but the authors simply claim that they show positive coverage AUC. Finally,\n\nWe totally agree that our method has limitations. We have tried to be as transparent as possible regarding those limitations by including a limitation paragraph at the end of the conclusion highlighting all the mentioned limitations. The differences between BNN-NPE and BNN-NRE are also highlighted in lines 416-418. We understand that some sentences might sound like overstatements, and we have now modified those. We have modified the sentence about log posterior density as follows: \n\"We also observe that the nominal log posterior density is on par with other methods for very high simulation budgets, on most benchmarks and with an appropriate temperature, but that more samples are required to achieve high values.\"\nRegarding the coverage AUC of NRE and BNN-NRE, in our opinion, BNN-NRE has better coverage values. We observe that NRE reaches coverage AUC values of -0.05 on SLCP, -0.1 on Two Moons, almost -0.1 on Lotka-Volterra, and almost -0.05 on Statial SIR. In opposition, BNN-NRE shows coverage AUC that is at worse, around -0.03, and is much more often positive. The coverage curves of BNN-NRE are above NRE at least 90\\% of the time. We agree that stating that BNN-NRE always shows positive coverage is, strictly speaking, wrong. We have updated the claim as follows:\n\"Figure 2 compares simulation-based inference methods with and without accounting for computational uncertainty. We observe that BNNs equipped with our prior and without temperature show positive, or only slightly negative, coverage AUC even for simulation budgets as low as $O(10)$.\"\n\n> The empirical results are weak.\n\nWe agree that our method shows empirical limitations as it requires higher simulation budgets to reach similar log posterior density. However, the empirical results clearly show that using Bayesian neural networks with our prior leads to better coverage AUC. All other methods are shown to have dangerously low coverage AUC in low simulation budget regime. We lose in statistical power to avoid risking false inference, which is the main objective of this work. \n\n> Second, to me, the empirical results are difficult to interpret. (A) Many of the curves shown in Figure 2 are very noisy and irregular. It might be beneficial to average across more seeds to observe clear trends. \n\nWe agree that more runs would have been preferable. However, we estimated the computational cost of 3 runs to be approximately 25,000 GPU hours. This is because each point in the reported curves corresponds to 3 full training and testing procedures, and testing with Bayesian neural networks involves the approximation of the Bayesian model average for each test sample. Unfortunately, we do not have the computational power to do many more runs. That being said, as mentioned above, even if the curves are a bit noisy, they are sufficient to validate the fact that using Bayesian neural networks with our prior leads to better coverage AUC."
            }
        },
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "> The results in Figure 3 seem to indicate that NPE is indeed outperforming BNN-NPE here. Would the authors argue that BNN-NPE is preferable over NPE for this task?\n\nIf we have a close look to what happens for a budget of 256, we observe that BNN-NPE takes into account the epistemic uncertainty and provide a large posterior. However, NPE provides a narrower posterior with its mass centered at the wrong place. NPE is then confident and wrong, while BNN-NPE is not overconfident. This behavior of NPE is what we are trying to avoid with BNN-NPE, and it works for that example.\n\n> What do the numbers in the legend in Figure 3 mean?\n\nThey are the simulation budgets. We have updated the caption to make it clearer.\n\n---\n\nOverall, we think we have addressed each concern raised in your review and implemented changes accordingly. While these improvements appear to resolve the identified issues, we'd value your perspective on whether any aspects still fall short of expectations. If the changes adequately address your concerns, we'd appreciate having this reflected in the evaluation."
            }
        },
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "First, thank you for your review. We are happy to hear that you found the contribution novel, interesting, and very relevant to the community, that the motivation and derivation of the tailored prior is particularly well done and convincing, that the method seems to empirically achieve the motivated goal and that the paper is well-written and easy to follow. We address your concerns below.\n\n> The authors evaluate their method with the expected coverage (EC). In Figure 2, it is in my opinion very difficult to draw conclusions which methods works best. This is likely due to the fact that only 3 runs have been evaluated. Given the complexity of SBI and the high variance of the inferential results, I think they should do at least 10 evaluations and report these.\n\nWe agree that more runs would have been preferable. However, we estimated the computational cost of 3 runs to be approximately 25,000 GPU hours. This is due to the fact that each point in the reported curves corresponds to 3 full training and testing procedures and that testing with Bayesian neural networks involves the approximation of the Bayesian model average for each test sample. Unfortunately, we do not have the computational power to do many more runs. That being said, we see from Figure 2 that all the methods but BNN-NPE show a coverage AUC below -0.1 for low simulation budgets on at least one benchmark. BNN-NPE shows only positive or very slightly negative coverage AUC. We believe we can then conclude that using BNN improves the conservativeness even if we have only 3 runs.\n\n> The authors use as a second evaluation metric the expected posterior log density (EPLD). It is not clear what a good value should look like here or even if a high log density should be desired. \nis to have a conservative posterior, is a extremely high EPLD a good measure? The authors should, despite possible drawbacks, have used some divergence to the true posterior, such as MMD, in addition to the other two and report this.\n\nTo evaluate the conservativeness of a posterior approximation, coverage is the only relevant measure. We reported EPLD to compare the methods as to how close they are to the real posterior. EPLD is actually the training objective of NPE methods and can be derived from the expected KL divergence between the approximate posterior and true posterior. \n\n$ w^* = \\arg \\min_w \\mathbb{E}_{p(x)} \\left[ KL\\left[ p(\\theta | x) || \\hat{p}_w(\\theta | x) \\right] \\right]$\n\n$ w^* = \\arg \\min_w \\mathbb{E}_{p(\\theta, x)} \\left[ \\frac{p(\\theta | x) }{\\hat{p}_w(\\theta | x)}\\right]$\n\n$ w^* = \\arg \\max_w \\mathbb{E}_{p(\\theta, x)} \\left[\\hat{p}_w(\\theta | x)\\right]$\n\nWe indeed do not know what a good EPLD value is, but we know that a method with high EPLD is closer to the real posterior than one with low EPLD, according to the KL divergence.\n\n> It is not clear how crucial hyperparameters such as the temperature T or the covariance functions of the reference GP are chosen in practice.\n\nIf you are referring to the temperature $T$ applied on the prior during the posterior approximation, we have given guidelines on lines 408-409. If you are referring to the parameters variance and lengthscale of the kernel, those are indeed hard to choose. The lengthscale is chosen to be a quantile of observed distances when taking two samples from the measurement set. The variance should be guided by domain knowledge as to how close the posterior is expected to be from the prior. In this setting, the priors are uniform and the prior density is then equal on all the parameter space. We have set the standard deviation to half that density value but more refined choices could be made with domain knowledge. This is explained in lines 686-692 in Appendix A.\n\n> The authors propose to use cold posteriors, e.g., using a temperature of T=0.01. The motivation of this is not clear, given that the entire idea is to use a prior that enforces calibration. Why would it be desirable to in fact reduce the impact of the prior?\n\nIndeed, this is counterintuitive. To obtain conservative approximations, it is better not to use a temperature. However, we have observed that in some cases, we were not able to reach satisfactory EPLD values and proposed using temperature as a way to set a trade-off between conservativeness and EPLD values. Note that tempering the prior is something that is often done in practice in Bayesian deep learning. \n\n> Figures 2 should show the standard errors.\n\nThey are shown in Figure 11. We didn't include those in Figure 2 because it makes the figure hard to read.\n\n> Figure 3 should show the reference posterior and not only the true parameter value.\n\nThe reference posterior is unfortunately unknown, but we can assume that NPE with a simulation budget of 1 million samples should not be far from the reference posterior."
            }
        },
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "> the experimental results are difficult to interpret because they show the media of only three repetitions. More repetitions and error bars would be better here.\n\nWe agree that more runs would have been preferable. However, we estimated the computational cost of 3 runs to be approximately 25,000 GPU hours. This is due to the fact that each point in the reported curves corresponds to 3 full training and testing procedures and that testing with Bayesian neural networks involves the approximation of the Bayesian model average for each test sample. Unfortunately, we do not have the computational power to do many more runs. That being said, although a bit noisy, we belive that those 3 runs are sufficient to demonstrate that using BNNs with our prior leads to better coverages in the low-simulation regime, which is our main claim.\n\n> the high underconfidence of the BNN approach in how-data regimes is concerning. Additional evaluation of the posterior predictive distributions would be appropriate.\n\nWe would argue that it depends on what we are trying to achieve. This work follows the philosophy highlighted by Hermans et al., 2022, where they argue that to be useful for scientific application, posterior approximations must be conservative before anything else. If that is not the case, then they cannot be used reliably for downstream scientific tasks. In this work, the main objective is to build SBI methods that are conservative for low simulation budgets. Of course, we still wish to maintain reasonable statistical power but only if conservativeness is observed in the first place. In that regard, overconfidence is a much bigger issue than underconfidence and we mainly aim to avoid overconfidence. We agree that in some settings, people might not care that much about conservativeness and focus on statistical power. Our work does not aim to address this setting and other methods might be preferred for such applications.\n\nJoeri Hermans, Arnaud Delaunoy, Francois Rozet, Antoine Wehenkel, Volodimir Begy, and Gilles Louppe. A crisis in simulation-based inference? beware, your posterior approximations can be unfaithful. Transactions on Machine Learning Research, 2022\n\n> the choice and construction of the prior from simulated should be explained more clearly. A better explanation ideally will resolve the questions on the general procedure and on Figure 1 above.\n\nWe hope our clarifications above made everything clearer. We dedicated an entire page (section 3.2) to this. Could you point out specific elements that would be interesting to add or clarify in your opinion?\n\n> NPE / NRE ensembles seems to perform similarly well compared to the BNN approach. How does it perform on the cosmology example? How does it compare to the BNN approach in terms of computational budget? Do you think ensembles could be a good alternative when computational budgets are limited? What are the disadvantages of ensembles compared to BNN for SBI?\n\n> More generally, what is the computational complexity of the BNN approach compared to NPE (ensembles)?\n\nFrom Figure 2, we can conclude that they do not perform similarly well, as the NPE ensemble has bad coverage for low simulation budgets, which is the setting of interest in this work, on all the benchmarks. BNN-NPE does not show the same behavior. We believe that the same observations would be made on the cosmological examples as it has been observed on all the benchmarks. In terms of computational budget, ensembles need more computational power to train than Bayesian neural networks with variational inference. For an ensemble of $N$ members, $N$ training procedure have to be performed, while for BNNs, only one training procedure outputing the posterior on weights has to be performed. At inference time, approximating the Bayes model average requires $M$ neural network evaluation if $M$ neural networks are used for the Monte-Carlo approximation. The two methods are then equivalent on that point if $M=N$.\n\n---\n\nOverall, we think we have addressed each concern raised in your review and implemented changes accordingly. While these improvements appear to resolve the identified issues, we'd value your perspective on whether any aspects still fall short of expectations. If the changes adequately address your concerns, we'd appreciate having this reflected in the evaluation."
            }
        },
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "First, thank you for your review. We are happy to hear that you found the contributions valuable and that it addresses a timely problem, that you think that the technical contributions are sound and that the experimental results support the claims, and that you found the paper well-written and clearly structured. We address your concerns below.\n\n> The only paper with a related approach that seems to be missing in the discussion is Lueckmann et al. 2017, section 2.2, who propose performing SVI on the neural network weights for continual learning across SNPE rounds. Also, it seems a bit odd to me that the benchmark used are not cited (except for the spatial SIR by Hermans et al.). The SLCP benchmark was introduced by Papamakarios et al. (SNLE paper), two moons by Greenberg et al.; and the overall set of SBI benchmarking tasks was introduced in Lueckmann et al. 2021.\n\nThanks for pointing it out. We were not aware of the work by Lueckmann et al. (2017), and we now discuss it in the introduction. The benchmarks are appropriately cited in Appendix B.\n\n>In section 3, especially in section 3.2, it should be explained more clearly why and how the prior has to be tuned to be used for the BNN approach. In the classical SBI setting, the prior is usually set a priori using expert knowledge. This step usually does not involve using simulated data for an optimization procedure. In the BNN approach, it seems that the prior has to be tuned with actual simulations (e.g., lines 254-261). Do these simulations have to be run in addition to the training data? Are they accounted for the in the budgets in the benchmarks?\n\n> Do these simulations for prior tuning have to be run in addition to the training data? Are they accounted for the in the budgets in the benchmarks?\n\nFirst, we would like to clarify what kind of prior we are talking about. In the classical SBI setting, a prior over the simulator's parameters $p(\\theta)$ is set a priori using expert knowledge. From that, an approximate posterior model based on a neural network with fixed weights $w$ is computed $p(\\theta | x, w)$. In this paper, we are also using a prior over simulator's parameters $p(\\theta)$ that is set using expert knowledge and not learned or optimized in any way. However, we use a Bayesian neural network as an approximate model, and the approximate posterior becomes $\\hat{p}(\\theta | x) = \\int p(\\theta | x, w) p(w | D) dw$. The term $p(w | D)$ requires the introduction of a prior $p(w)$ that is, this time, over the neural network's weight and not the simulator's parameters. We optimize this prior to match the behavior of a Gaussian process in the functional domain. We mention in lines 258-265 several ways to construct a measurement set for the optimization. In the experiments, we only use simulations to derive the support of the distribution, and the simulations used are the same as the training simulations. No additional simulations are then used, and all the simulations that are used are taken into account in the simulation budget.\n\n> Figure 1 needs a couple of clarifications. The caption says it's a visualization of the tuned prior. Then, the next sentence says the left panels show posterior functions sampled from the tuned prior prior over the neural network's weights. How are these samples obtained and what are thet supposed to show? Are they obtained before or after SBI training? Similarly question for the last panel: is the calibration calculated after the full SBI training or only after the prior tuning?\n\nAgain, the confusion might come from the fact that there are two types of priors used in this work. This figure shows a visualization of the tuned prior over weight $p(w|\\phi)$. To show this prior, we plot the approximate posterior function $p(\\theta | x, w)$ with $w \\sim p(w)$. This is then before SBI training, as during SBI training, we compute the posterior over weights $p(w | D, \\phi)$. Approximate posterior samples after SBI training would then be $p(\\theta | x, w), w \\sim p(w | D, \\phi)$. We added some precisions in the caption to avoid any confusion.\n\n> In general, it seems that a better explanation of the steps involved in setting up and training the BNN-NPE (NRE) approach. I suggest adding more details in the next and adding an algorithm scheme in the text or appendix.\n\nWe added a few sentences at the beginning of Section 3 indicating the different steps involved. There are three main steps. The first step is to optimize an appropriate prior $p(w | \\phi)$. The second step is to use that prior to compute a posterior over weights $p(w | \\phi, D)$ and the third step is to use that posterior over weight to compute an approximate posterior over simulator's parameters $\\hat{p}(\\theta | x) = \\int p(\\theta | x, w) p(w | D, \\phi) dw$."
            }
        },
        {
            "summary": {
                "value": "The paper intodruces a new method for simulation-based inference (SBI) that uses\nBayesian neural networks (BNN) to better account for uncertainty in posterior estimates\ndue to limited training data. The authors demonstrate that previous attempts of\ncombining BNN with SBI showed limited success due to the choice of prior on the BNN\nweights and they propose an alternative prior more suitable for the SBI setting. To\nevaluate the proposed method, the authors show on four benchmarking tasks and on a SBI\nuse-case from cosmology that the resulting SBI posterior estimates are well-calibrated\neven in the low-data regime."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "### Originality\n\nThe idea of using BNN for SBI is not new (as discussed in the paper). However, showing\nwhy previous approaches did not work so well and the proposal of a new type of BNN prior\nmore suitable for the SBI setting is a valuable contribution. Most of the previous work\non better uncertainty quantification in SBI is discussed adequately in the introduction.\nThe only paper with a related approach that seems to be missing in the discussion is [Lueckmann et al.\n2017, section\n2.2](https://proceedings.neurips.cc/paper/2017/hash/addfa9b7e234254d26e9c7f2af1005cb-Abstract.html),\nwho propose performing SVI on the neural network weights for continual learning across\nSNPE rounds. Also, it seems a bit odd to me that the benchmark used are not cited\n(except for the spatial SIR by Hermans et al.). The SLCP benchmark was introduced by\nPapamakarios et al. (SNLE paper), two moons by Greenberg et al.; and the overall set of\nSBI benchmarking tasks was introduced in Lueckmann et al. 2021.\n\n### Quality\n\nThe technical contributions of the paper appear as sound and the selected methods for\napplying BNN to SBI are appropriate. The experimental results tend to support the\ninitial claim of obtaining well-calibrated posteriors in low-data regime. However, the\nnumber of performed experiments is quite low (three) and the results appear quite noisy.\n\nIn general, it should be made clearer how strong the trade-off is between posterior\ncalibration and posterior accuracy. At the moment it seems that the BNN approach tends\nto be quite underconfident in some scenarios (e.g., BNN-NRE on all benchmarks). I\ntherefore suggest that more experiments should be run and results should be reported\nwith error bars (e.g., 5-10 runs with error bars showing the standard error of the mean\nperformance). Additionally, I think it would be good to also check the posterior\npredictive distribution for the cases where the BNN has low nominal log posterior values\neven in high-data regimes.\n\n### Clarity\n\nOverall, the paper is well-written and clearly-structured. I have a couple of remarks\nthat should be addressed to improve the clarity.\n\n- In section 3, especially in section 3.2, it should be explained more clearly why and\n  how the prior has to be tuned to be used for the BNN approach. In the classical SBI\n  setting, the prior is usually set a priori using expert knowledge. This step usually\n  does not involve using simulated data for an optimization procedure. In the BNN\n  approach, it seems that the prior has to be tuned with actual simulations (e.g., lines\n  254-261). Do these simulations have to be run in addition to the training data? Are\n  they accounted for the in the budgets in the benchmarks?\n- Figure 1 needs a couple of clarifications. The caption says it's a visualization of\n  the tuned prior. Then, the next sentence says the left panels show posterior functions\n  sampled from the *tuned prior prior over the neural network's weights*. How are these\n  samples obtained and what are thet supposed to show? Are they obtained before or after\n  SBI training? Similarly question for the last panel: is the calibration calculated\n  after the full SBI training or only after the prior tuning?\n\nIn general, it seems that a better explanation of the steps involved in setting up and\ntraining the BNN-NPE (NRE) approach. I suggest adding more details in the next and\nadding an algorithm scheme in the text or appendix.\n\n### Significance\n\nUncertainty quantification in neural SBI methods on the posterior approximation itself\nis an important and timely problem. Especially in low-data regimes, common neural SBI\nmethods like NPE struggle. The proposed BNN approach with a Gaussian process prior\nas proposed here appears as an important contribution for addressing this issue."
            },
            "weaknesses": {
                "value": "I outlined several concerns and questions above. To summarize to most important points:\n\n- the experimental results are difficult to interpret because they show the media of\n  only three repetitions. More repetitions and error bars would be better here.\n- the high underconfidence of the BNN approach in how-data regimes is concerning.\n  Additional evaluation of the posterior predictive distributions would be appropriate.\n- the choice and construction of the prior from simulated should be explained more\n  clearly. A better explanation ideally will resolve the questions on the general\n  procedure and on Figure 1 above."
            },
            "questions": {
                "value": "1) Do these simulations for prior tuning have to be run in addition to the training data?\n2) Are they accounted for the in the budgets in the benchmarks?\n3) NPE / NRE ensembles seems to perform similarly well compared to the BNN approach. How\n  does it perform on the cosmology example? How does it compare to the BNN approach in\n  terms of computational budget? Do you think ensembles could be a good alternative when\n  computational budgets are limited? What are the disadvantages of ensembles compared to\n  BNN for SBI?\n4) More generally, what is the computational complexity of the BNN approach compared to\n  NPE (ensembles)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a novel approach for simulation-based inference for low-budget problems using Bayesian neural networks. They additionally propose to use tailored priors for the neural network weights which generally leads to more conservative posterior estimates. They evaluate their method on several experimental models against multiple baselines demonstrating promising results for SBI when the computational budget is limited."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper proposes a novel contribution for simulation-based inference. The method is in my opinion very relevant to the community and interesting as a low-budget solution for inferential problems.\n- The motivation and derivation of the tailored prior is in my opinion particularly well done and convincing.\n- Empirically, the method seems to achieve the motivated goal: having calibrated posteriors for low simulation budgets.\n- The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "The evaluations and presentations of the results could in my opinion be improved.\n\n- The authors evaluate their method with the expected coverage (EC). In Figure 2, it is in my opinion very difficult to draw conclusions which methods works best. This is likely due to the fact that only 3 runs have been evaluated. Given the complexity of SBI and the high variance of the inferential results, I think they should do at least 10 evaluations and report these.\n- The authors use as a second evaluation metric the expected posterior log density (EPLD). It is not clear what a good value should look like here or even if a high log density should be desired. When the goal is to have a conservative posterior, is a extremely high EPLD a good measure? The authors should, despite possible drawbacks, have used some divergence to the true posterior, such as MMD, in addition to the other two and report this.\n- It is not clear how crucial hyperparameters such as the temperature $T$ or the covariance functions of the reference GP are chosen in practice. \n- The authors propose to use cold posteriors, e.g., using a temperature of $T=0.01$. The motivation of this is not clear, given that the entire idea is to use a prior that enforces calibration. Why would it be desirable to in fact reduce the impact of the prior? \n\n#### Minor\n- Figures 2 should show the standard errors.\n- Figure 3 should show the reference posterior and not only the true parameter value."
            },
            "questions": {
                "value": "- The results in Figure 3 seem to indicate that NPE is indeed outperforming BNN-NPE here. Would the authors argue that BNN-NPE is preferable over NPE for this task?\n- What do the numbers in the legend in Figure 3 mean?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors aim to produce well-calibrated posterior in simulation-based inference, in particular in cases where the simulator is very expensive and, therefore, few simulations can be used as training data. To achieve this, the authors propose to use Bayesian neural networks. They develop a novel prior which leads to a-priori well-calibrated posteriors. They apply their method to several toy simulators and to a physics simulator."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The idea to generate neural network weight priors which lead to a-priori well-calibrated posteriors is novel, interesting, and potentially impactful. The methodology which the authors employ to achieve this is novel, elegant, and rigorous. I thoroughly enjoyed reading these parts of the paper. In addition, the authors demonstrate that the method can be applied across methods (NPE & NRE) and they evaluate the method on a series of useful tasks."
            },
            "weaknesses": {
                "value": "(1) The paper overstates its claims.\nMy main issue with this paper is that it overstates its claims and does not acknowledge the weakness of empirical results. Looking at Figure 2, and in particular for NRE: BNN-NRE _never_ reaches the log posterior of the other methods (even for 1M simulations). Yet the authors state that `the nominal log posterior density is on par with other methods for very high simulation budgets`. Where does this claim come from. Similarly, in Figure 2 (NRE), the authors do not acknowledge that the AUC is not particularly much higher for BNN-NRE as compared to NRE, even for 10 simulations. Indeed, for three of the four tasks NRE has a higher coverage AUC than BNN-NRE for 10 simulations. On top of this, for many tasks and simulation budgets, BNN-NRE does have a negative AUC, but the authors simply claim that they `show positive coverage AUC`. Finally, \n\n(2) The empirical results are weak.\nSecond, to me, the empirical results are difficult to interpret. (A) Many of the curves shown in Figure 2 are very noisy and irregular. It might be beneficial to average across more seeds to observe clear trends. (B) As a potential user, I would find it very worrisome that BNN-NRE has _significantly_ lower nominal log-posterior than standard methods. Indeed, for some tasks, it seems to require 5 orders of magnitude (for many other tasks 3 orders of magnitude) more simulations than standard methods."
            },
            "questions": {
                "value": "No questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors wish to provide a method for simulation-based inference for expensive simulators that accounts for the epistemic uncertainty, which tends to be high when available data (simulations) is low. They wish to do so by using a Bayesian neural network in combination with the SBI methods NRE and NPE. They note issue with current priors for Bayesian neural networks in simulation based inference, stating they are not calibrated a priori. They note that using a Gaussian process, with the prior distribution as its mean, can be used to define a prior which is well-calibrated a priori. By optimizing the Bayesian networks prior on the weights to approximate the Gaussian process prior, the Bayesian networks prior becomes approximately well-calibrated. They perform experiments, and show this often leads to better calibrated posteriors in the low-data (low simulation budget) regime."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Being able to perform reliable simulation-based inference with fewer simulations would be broadly useful to the scientific community. The idea to use a prior over the weights of a BNN that is compatible with the simulators prior distribution is sensible, and using a Gaussian process to achieve this is to the best of my knowledge novel and clever. The paper is well structured overall (except for where noted below) and includes some interesting experiments, using a good choice of metrics assessing reliability of the posteriors."
            },
            "weaknesses": {
                "value": "Structure/Presentation:\n- Lack of method summary: There's no clear summary of the method, e.g. in the contributions section, an algorithm, or the start of section 3. A brief overview of 3-4 sentences would be very beneficial for readers. It's only towards the end of section 3.2, that the overarching method begins to come together on a first read. In my opinion, the abstract also does not include enough information on the method.\n- Lack of clear definitions: The concept of an \"a priori-calibrated Bayesian model\" should be defined more clearly when it is first introduced.  I presume that an example of a well calibrated a priori model, would be one for which the Bayesian model average at initialization is equal to the simulator parameters prior $p(\\theta)$ for any $x \\in \\mathcal{X}$? Presumably this includes a large variety of useful and non-useful models for modelling epistemic uncertainty (as suggested in equation 8).\n- Figures: The credibility figure in Figure 1 is confusing, either the standard deviations are not in order, or there is a typo one of the standard deviations. Figure 3 should have a labeled legend. Many of the font sizes are too small.\n\nExperiments:\n- Limited experimental runs: Only 3 runs are performed for each experiment, and only the median is reported. This makes it hard to assess if differences between methods are significant. Repeating with more runs would be beneficial, although I am sympathetic to limitations in computational budget.\n- The BNN-NRE method appears to perform worse than NRE in terms of mass placed on true parameters, even for low simulation budgets (100-1000 in Figure 2), which is the domain where the method is proposed to be beneficial. Similarly, the results for the NPE case are not particularly convincing in terms of the mass placed on the true parameters. The coverage properties do appear to have improved, but coverage alone is not indicative of a good posterior estimate.\n- The results don't align with an intuitive understanding of epistemic uncertainty. For example, in figure 3, in NPE, we can see 4000 simulations produces a reasonable posterior estimate, whereas BNN-NPE is still massively conservative, even with 65536 simulations, suggesting overestimation of epistemic uncertainty. This suggests simple alternative approaches such as training posterior estimates on subsets of the data with standard methods, such as NPE, and mixing the resulting posteriors, would likely be more effective. I understand the temperature parameter is introduced to limit this problem, but this then introduces another hard to choose hyperparameter (along with the Gaussian process parameters introduced)."
            },
            "questions": {
                "value": "How can we be sure that any benefits are from the BNN modelling epistemic uncertainty, and not the altered initialization? For example, if we \"pretrained\" NPE/NRE to prior samples (and $x$ simulated from some noise distribution), such that at \"initialization\" the posterior estimate would be approximately equal to the prior for any $x$, it would be interesting to see if there is still be any benefit to the introduced method.\n\nMost the benefits seem to be in the very small data regime (<100 simulations). I am not convinced that this scenario is of particular interest the scientific community, could you give an example of a practical case where this is useful?\n\nWhy is the convergence to the NPE solution so slow in terms of the number of simulations? Can the prior be altered to avoid this problem, rather than introducing a temperature parameter?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}