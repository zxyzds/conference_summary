{
    "id": "NQTrARs2pz",
    "title": "HomieBot: an Adaptive System for Embodied Mobile Manipulation in Open Environments",
    "abstract": "Embodied Mobile Manipulation in Open Environments (EMMOE) is the challenge that agents understanding and executing long-horizon everyday tasks in home environments. This challenge encompasses task planning, decision-making, navigation and manipulation, and is crucial to develop a powerful home assistant capable of automatically completing daily tasks. However, the absence of a holistic benchmark, data incompatibility between large language models (LLMs) and mobile manipulation tasks, the lack of a comprehensive framework, and insufficient dynamic adaptation mechanisms all continue to hinder its development. To address these issues, we propose EMMOE, the first unified benchmark that evaluates high-level planners and low-level policies simultaneously. Additionally, We manually collect EMMOE-100, the first everyday task dataset featuring detailed decision-making processes, Chain-of-Thought (CoT) outputs, feedback from low-level execution and a trainable data format for Large Multimodal Models (LMMs). Furthermore, we design HomieBot, a sophisticated agent system which integrates LMM with Direct Preference Optimization (DPO) as the high-level planner, small navigation and manipulation models as the low-level executor. HomieBot can get a 31.8% success rate in training tasks and 20% in test tasks.",
    "keywords": [
        "Embodied Agent",
        "Embodied AI Benchmark",
        "Mobile Manipulation"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=NQTrARs2pz",
    "pdf_link": "https://openreview.net/pdf?id=NQTrARs2pz",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces three main contributions:\n\n1. EMMOE: A unified benchmark for evaluating both high-level planners and low-level policies in embodied mobile manipulation tasks.\n\n2. EMMOE-100: A dataset with 100 complex everyday tasks featuring task-planning processes and COT outputs\n\n3. HOMIEBOT: An intergrated agent system combining high-level planner and low-level executors."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The new evaluation metrics are novel, potentially providing a valuable resource for the research community and practical applications."
            },
            "weaknesses": {
                "value": "1. **Insufficient Methodology Details**\n   - The data processing and augmentation methods are not adequately explained:\n     * The \"fixed-format conversation data\" conversion process in SFT Augmentation is not described\n     * No clear explanation of how the uniform script processes the EMMOE-100 data\n     * The DPO Augmentation section lacks clear algorithmic description or flowcharts\n\n2. **Limited Experimental Evaluation**\n   - Inadequate baseline comparisons and empirical analysis:\n     * No evaluation of existing RL algorithms or LM-based agents on the EMMOE benchmark\n     * Only one system (HOMIEBOT) is evaluated, lacking comparative analysis\n     * The relatively low success rates (31.8% training, 20% testing) are not thoroughly analyzed\n     * Missing ablation studies or detailed error analysis to understand performance bottlenecks"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work focuses on studying embodied AI instruction following mobile manipulation in simulated environments. To this end, the paper proposes the EMMOE-100 dataset and benchmark and HOMIEBOT as a system integration solution to it. Some unique contributions involve that it is a dataset of human-controlled embodied agent trajectories:\n1. Decomposition into subtasks\n1. Annotated reasoning process with every execution\n1. Replanning for some failed subtasks\nThe paper also introduces metrics to measure Task Progress, Success end rate, and success replan rate and presents results for SFT and SFT+DPO on the Video-LlaVA model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper introduces the human controlled embodied agent trajectories, unlike the use of PDDL in ALFRED and other RL benchmarks.\n1. The focus seems to be on open-ended long horizon questions, multiple ways of solving it, annotating sub-goals and deliberately collecting replanning trajectories."
            },
            "weaknesses": {
                "value": "1. Overclaim: it does not seem like the first to unify the high-level and low-level. This has been discussed in Robotics Task and Motion planning (TAMP) literature and has been introduced as baselines in previous simulator benchmarks. Connection with previous work and clarity on the unique contribution of this work is needed.\n1. Lack of failure analysis: The paper does not discuss the limitations and failures modes of the combined high-level and low-level modules. \n1. Limited discussion: It is unclear why the success rate for HOMIEBOT is so low in training and test tasks. \n1. Clarity: Paper is not well formatted and has lots of typos (line 180 control, 185-186 between)\n1. Limited novelty: with discrete action space, the work looks very similar to ALFRED (Shridhar et al.) and FiLM (Min et al.) with a few enhancements."
            },
            "questions": {
                "value": "1. Why not use continuous action space? Habitat Replica allows for continuous action space. HM3D \n2. How is the Success end rate, and success re-plan rate measured? How is the count for end and replan computed for each trajectory?\n3. Why is \u201ctrainable data format\u201d listed as an important contribution compared to the existing datasets? \n4. How does the high-level planner know when and where low-level execution fails?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a benchmark of 100 tasks in home robotics, collected within the Habitat simulator using human demonstrations annotated with detailed reasoning explanations. The benchmark evaluates models using success rate, success end rate, and success replan rate. The authors train Video-LLaVA on this dataset with either SFT (supervised fine-tuning) or DPO (Direct Preference Optimization) and find that SFT+DPO performs best on the train set across all metrics, while SFT-only achieves the highest scores on the test set."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Introduction of a new benchmark for home robotics with a diverse set of 100 tasks.\n- Collection of detailed annotations to capture the reasoning process and a diverse set of tasks in home environments.\n- Strong metrics to evaluate embodied execution, focusing on nuanced aspects of embodied execution.\n- Propose a dataset for SFT and DPO fine-tuning from egocentric trajectories."
            },
            "weaknesses": {
                "value": "- Limited baseline comparisons, which reduces the clarity of contributions relative to existing methods.\n- Lack of diverse model comparisons, including text-only and zero-shot multi-image baselines (e.g., Qwen2VL, GPT-4o), and no modular vs. end-to-end performance analysis.\n- Insufficient analysis of task-specific challenges and bottlenecks within the benchmark; error analysis and reasoning failure modes are explored to some degree, but not in detail.\n- No discussion of potential real-to-sim discrepancies for the manipulation models, which could impact model generalization."
            },
            "questions": {
                "value": "- Could the authors provide stronger baseline comparisons, especially with text-only and zero-shot multi-image models?\n- How do modular models compare to end-to-end approaches on this benchmark?\n- Can the authors analyze the primary failures observed by the model, and give qualitative analysis of what are the major challenges in the benchmark for existing models?\n- Is there any observed real-to-sim gap affecting the manipulation models, and if so, how does it impact performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a unified benchmark named \u201cEMMOE\u201d designed to simultaneously evaluate both high-level planning and low-level control abilities of embodied agents. This benchmark comprises 100 complex everyday tasks based on scenarios from the Replica dataset, including long-horizon tasks, open-ended tasks with multiple possible outcomes, logical tasks, etc. To better assess agent performance, the authors propose three new metrics: Task Progress, Success End Rate, and Success Re-plan Rate. Additionally, the paper provides training data using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), featuring Chain-of-Thought (CoT) outputs and replan processes. A baseline agent, HomieBot, is introduced, featuring a Large Multimodal Model (LMM) for high-level planning and several task-specific models for low-level control. HomieBot achieves a 31.8% success rate on training tasks and 20% on test tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The objectives of the study are clearly stated.\n2. The related work cited in this article is extensive.\n3. The provision of the datasets (including CoT, replanning, etc.) is beneficial for future embodied AI research."
            },
            "weaknesses": {
                "value": "1. The article lacks clear organization.\n     - In Section 3, it is inappropriate to describe the dataset for model training (Section 3.2, Data Augmentation for SFT and DPO) here. This dataset is intended for training the embodied agents, not for constructing the benchmark tasks.\n     - The work aims to evaluate high-level planning and low-level execution abilities. However, there is no individual assessment of these abilities (e.g., the success rate of high-level planning, low-level execution, etc.). The relationship between the proposed evaluation metrics (TP, SER, SRR) and these abilities is unclear.\n    - The problem is not well defined. It would be beneficial to provide a problem formulation regarding high-level planning and low-level execution in the paper.\n   - Some content in the article is unclear. For instance, on page 4, line 207, the statement \u201cif subtask i fails but subtask i + 1 succeeds\u201d lacks clarity regarding the relationship between subtask i and subtask i+1. From my understanding, subtask i and i+1 are the same task but with different model responses (actions). This needs to be clarified.\n2. There are a lot of mistakes in the paper,\n   - In Section 3.3, Equation 1 appears to be incorrect. For different keypaths $k_i$, the nodes to be checked are also different. Therefore, the equation should be $TP=max_{{k_i}\\in K_T} (\\frac{len(k_i^{check})}{len(k_i)})$.\n   - On page 6, line 278, M is defined as the set of all tasks, and $t \\in M$, so $t$ should be a single task in the task space. However, the author also states \u201ct is a task trajectory,\u201d which conflates the concepts of task and trajectory.\n   - The criterion \u201cSuccess End Rate\u201d is not reasonable. For example, if an agent never outputs \u201cEnd\u201d but can complete some tasks, the equation will be infinite. The correct equation should be $\\frac{\\text{number of successful tasks whose last output is End}}{\\text{number of successful tasks}}$'.\n   - On page 4, line 318, $i$ is not defined.\n   - Spelling errors: On page 4, \u201ccontroll\u201d should be \u201ccontrol\u201d.\n3. The experimental section needs improvement. There is even no evaluation of the model before fine-tuning."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}