{
    "id": "sULAwlAWc1",
    "title": "One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs",
    "abstract": "Safety alignment in large language models (LLMs) is increasingly compromised by jailbreak attacks, which can manipulate these models to generate harmful or unintended content. Investigating these attacks is crucial for uncovering model vulnerabilities. However, many existing jailbreak strategies fail to keep pace with the rapid development of defense mechanisms, such as defensive suffixes, rendering them ineffective against defended models. To tackle this issue, we introduce a novel attack method called ArrAttack, specifically designed to target defended LLMs. ArrAttack automatically generates robust jailbreak prompts capable of bypassing various defense measures. This capability is supported by a universal robustness judgment model that, once trained, can perform robustness evaluation for any target model with a wide variety of defenses. By leveraging this model, we can rapidly develop a robust jailbreak prompt generator that efficiently converts malicious input prompts into effective attacks. Extensive evaluations reveal that ArrAttack significantly outperforms existing attack strategies, demonstrating strong transferability across both white-box and black-box models, including GPT-4 and Claude-3. Our work bridges the gap between jailbreak attacks and defenses, providing a fresh perspective on generating robust jailbreak prompts.",
    "keywords": [
        "large language model",
        "jailbreak attack",
        "robustness"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=sULAwlAWc1",
    "pdf_link": "https://openreview.net/pdf?id=sULAwlAWc1",
    "comments": [
        {
            "summary": {
                "value": "This paper propose an automatic attack framework (ArrAttack) for generating robust jailbreak prompts designed to bypass defenses in LLMs. The authors propose two main components: a rewriting-based prompt generation technique and a robustness judgment model. \n\nArrAttack first uses an undefended LLM to generate jailbreak prompts. These prompts are then evaluated using a robustness judgment model trained to assess their resistance to defenses. The final model combines both components to generate highly effective jailbreak prompts that perform well against multiple defence methods. Through extensive experimentation, the authors show that ArrAttack achieves higher success rates and transferability compared to other baseline approaches across various LLMs and defense mechanisms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper notices the limitations of existing jailbreak attacks on defended LLMs and presents a unique approach to jailbreak prompt generation by combining a robustness judgment model with a rewriting-based generation technique.\n\n2. The paper is well-structured, with a clear problem formulation and a detailed description of the proposed methods.\n\n3. The experiments are comprehensive, evaluating multiple models with diverse architectures and defense mechanisms. The authors employ various evaluation metrics, including attack success rate, semantic similarity, and perplexity, which adds depth to the assessment."
            },
            "weaknesses": {
                "value": "1. The authors focus on jailbreak attacks in defense scenarios. However, this has not been thoroughly validated across all defense types, which may limit generalizability. The defense methods tested in the paper are all system-level, focusing on input-level defenses. The authors could strengthen the study by including model-level defense mechanisms, such as unlearning [1] and adversarial fine-tuning.\n\n2. The authors should provide more detailed case studies for the robustness judgment model and prompt generation model, particularly with samples of generated prompts in defended scenarios.\n\n3. From the ablation studies in Tables 4 and 5, it is evident that the robustness judgment model and prompt generation model are the most critical components. Therefore, the authors should delve further into optimizing these models. In Sections 3.3 and 3.4, the training parameters are briefly mentioned, but the authors provide little detail on the training methods and rationale behind parameter choices that contribute to model quality.\n\n\n[1] Yao, Yuanshun, Xiaojun Xu, and Yang Liu. \"Large language model unlearning.\" arXiv preprint arXiv:2310.10683 (2023)."
            },
            "questions": {
                "value": "1. In Section 3.3, the robustness judgment model is trained with a single defense mechanism (SmoothLLM). Why was this particular defense chosen? How does it ensure transferability across different defenses and models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel jailbreak attack method, ArrAttack, aimed at circumventing defenses in large language models (LLMs). ArrAttack leverages a rewriting-based attack mechanism and a robustness judgment model to generate robust jailbreak prompts, capable of defeating various defense strategies. Experimental results suggest that ArrAttack exhibits strong transferability and success across multiple defenses, including GPT-4 and Claude-3."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe paper presents a new approach by integrating a robustness judgment model with the generation of jailbreak prompts, enhancing both the efficiency and robustness of attacks. However, similar concepts have already been proposed, such as PAIR[1] and TAP[2], which utilize LLMs to iteratively rewrite adversarial prompts to achieve a high ASR.\n2.\tThe paper is generally well-structured, providing clear explanations of the proposed method, its components, and the evaluation criteria.\n\n[1] Jailbreaking Black Box Large Language Models in Twenty Queries\n[2] Tree of Attacks: Jailbreaking Black-Box LLMs Automatically"
            },
            "weaknesses": {
                "value": "1.\tWhile ArrAttack improves robustness, the core attack methodology\u2014particularly the rewriting-based approach\u2014is not entirely new, as similar strategies have been explored in previous works like PAIR and TAP.\n2.\tThe focus on defense-enhanced LLMs is limited by incomplete experimentation. Several key model-level defenses [3], such as safety training and unlearning, have not been thoroughly examined.\n3.\tThe paper lacks comparison with similar baselines, such as PAIR and TAP, making it difficult to fully assess the attack performance of ArrAttack.\n4.\tThe use of GPTFuzz\u2019model, as an evaluation metric is questionable due to its inherent bias and limited generalizability in complex scenarios. I recommend adopting GPT-4 or using more recent jailbreak evaluation frameworks to reassess the metric.\n\n[3] Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs"
            },
            "questions": {
                "value": "1.\tCan the robustness judgment model be fine-tuned or adapted for use in other languages or LLMs trained on non-English datasets? If so, what modifications would be required?\n2.\tHow would ArrAttack perform against real-time defense mechanisms that evolve based on user feedback or system updates?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "na"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose ArrAttack, a brand-new attack to jailbreak the defended LLMs. Two steps are included. Firstly, it trains a universal robustness judgement model. Then, this model serves as a filter to select powerful prompts for generation model training."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1 The soundness of this paper is good.\n\n2 In Table 1, the results indicate that ArrAttack can not only achieve high ASR, but also obtains PPL score.\n\n3 The transferability of jailbreak prompts generated by ArrAttacks is good."
            },
            "weaknesses": {
                "value": "1 I think the writing of this paper is not satisfying, especially the method and the experimental section. To be honest, I think ArrAttack itself is not so hard to understand. But I indeed try hard to follow the writers' chain of thought. In Section 3, the authors should introduce how ArrAttack is motivated and how the pipeline of ArrAttack works rather than the detailed settings of the hyperparameters. In Section 4, instead of combining all results into a huge subsection \"RESULTS\", you should divide them part by part to make it clearer to readers.\n\n2 ArrAttack requires to train a generation model, which needs additional computational cost. In PAIR [1] and TAP [2], they propose to craft the jailbreak prompts with GPT-4, which makes them more flexible and adptive to various defenses. However, unfortunenately, I do not see the comparison with those methods. Further experiments are needed to demonstrate ArrAttacks' outstanding performances compared to previous works.\n\n3 ArrAttack needs a large amount of training data compared to those of existing attacks, such as AutoDAN (zero-shot). In contrast, ArrAttack combines three datasets to perform attacks. As far as I know, the diversity of the training set is a key factor to the generalizable of the methods. Thus, more ablation studies should be performed when the training data are limited.\n\n4 The settings for evaluation is rare. Actually there are a lot of benchmarks such as harmbench that evaluate the performances of jailbreak attacks on various LLMs. Comparing with previous attack methods on existing benchmarks not only makes the effectiveness of the method more intuitive but also enhances the impact of the work."
            },
            "questions": {
                "value": "1 Although bypassing SMP, DPP, RPO and PAR are much appreciated, how about evading more stronger defenses, such as [3-5]?\n\n2 In the experimental setups section, the authors propose to evaluate the ASR with the \"GPTFuzz\" model. Much more details are needed to provide in the Appendix to see how it works because the ASR metric is sensitive to various settings. \n\n[1] Jailbreaking black box large language models in twenty queries\n\n[2] Tree of attacks: Jailbreaking black-box llms automatically\n\n[3] Safedecoding: Defending against jailbreak attacks via safety-aware decoding\n\n[4] Fight back against jailbreaking via prompt adversarial tuning\n\n[5] RAIN: Your Language Models Can Align Themselves without Finetuning"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}