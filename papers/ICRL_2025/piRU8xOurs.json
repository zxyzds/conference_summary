{
    "id": "piRU8xOurs",
    "title": "TreeTop: Topology-Aware Fine-Tuning for LLM Conversation Tree Understanding",
    "abstract": "While Large Language Models (LLMs) have dominated a wide diversity of natural language tasks, improving their capabilities on \\emph{structured} inputs such as graphs remains an open challenge. We introduce $\\texttt{TreeTop}$, a pre-training framework for LLMs that significantly improves their ability to understand and reason over structural relationships in multi-party, threaded discussions, such as those found on social media platforms. $\\texttt{TreeTop}$ is a novel set of 17 QA-style tasks specifically designed to allow LLMs to selectively focus on both the structure of and content in discussion graphs. We find that LLMs fine-tuned with $\\texttt{TreeTop}$ outperform their counterparts in every setting: zero-shot/few-shot performance on unseen pretraining tasks as well as downstream social media inference tasks (e.g.rumor detection), as well as fine-tuned performance on the downstream tasks, including their challenging \"early-detection\" variants. In particular, $\\texttt{Gemini Pro}$ fine-tuned with $\\texttt{TreeTop}$ and further fine-tuned on downstream tasks surpasses both vanilla $\\texttt{Gemini Pro}$ and state-of-the-art GNN baselines. Our framework paves the way for LLMs with enhanced capabilities on heavily-structured inputs.",
    "keywords": [
        "Conversation Trees",
        "Social media",
        "Large language models"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We study and develop the ability of large language models (LLMs) to understand and reason over structural relationships in conversations, especially on social media platforms.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=piRU8xOurs",
    "pdf_link": "https://openreview.net/pdf?id=piRU8xOurs",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces TreeTop, a fine-tuning framework designed to enhance the performance of LLMs in understanding and reasoning over conversation trees\u2014structured, multi-party discussions typically found on social media. The framework provides a collection of 17 structural tasks, aiming to improve LLMs' capacity to process both content and the structure of conversation trees. Experimental results show that LLMs fine-tuned with TreeTop outperform baseline models, including state-of-the-art GNNs, on multiple social media inference tasks, such as controversy and rumor detection."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper tackles a novel challenge by focusing on conversation trees, which have unique characteristics such as directed, acyclic, and temporal structures. This addresses an important gap in LLM capabilities.\n- TreeTop shows potential across a range of tasks (e.g., controversy detection, rumor detection) relevant to social media, making the framework broadly useful."
            },
            "weaknesses": {
                "value": "- Some tasks are too easy to be meaningful. This work is proposed as a benchmark for LLMs and what's the point if a LLM can already achieve 100% accuracy?\n- Most datasets used in this study are derived from Reddit, which may limit the generalizability of TreeTop\u2019s effectiveness on other social media platforms or discussion types.\n- The paper briefly touches on potential issues such as over-moderation or promoting echo chambers but does not provide a detailed discussion on these risks or propose safeguards.\n- The paper only includes results from one model (Gemini), which is bluntly telling the authors work for a certain company. Also not including other models fail to set meaningful baselines.\n- Format: This paper uses a wrong font."
            },
            "questions": {
                "value": "See \"Weaknesses.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a fine-tuning method on conversation-tree related tasks. \nConcretely, the conversation-tree for the LLM to understand and the corresponding tasks in the form of yes/no questions are encoded in a structured prompt and used for fine-tuning. This method is named as TreeTop. They show that TreeTop fine-tuned LLMs have better performance than their not fine-tuned counterparts across various tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- a new method is proposed to encode the tree structure of the conversation-trees and is showed to be effective \n- designed some structural tasks to evaluate the topological structure of the conversations trees \n- the proposed fine-tuning method does not only boost the performance on fine-tuned tasks but also on unseen tasks\n- extensive experiments were conducted and the ablation studies are also detailed"
            },
            "weaknesses": {
                "value": "See questions"
            },
            "questions": {
                "value": "- For the primitive structural tasks, why is TT not compared against the baseline where the LLM is fine-tuned with non TreeTop encoded conversations and tasks? Please point it out if I understand it incorrectly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the author proposed TreeTop, a fine-tuning framework improving the reasoning capabilities of LLMs over structured conversation tree data. TreeTop incorporates 17 structural tasks to train LLMs on the unique topological and content-driven aspects of conversation trees. Experimental results show that LLMs fine-tuned with TreeTop outperform traditional GNNs and other baselines on social media inference tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Adapting LLMs for graph-structured conversation trees are distinct graph problem-solving methods from traditional, static graphs in their directed, acyclic nature and temporal aspects.\n- The evaluation on various social media tasks is comprehensive. Fine-tuning on structural tasks and subsequent performance comparisons are sufficient. The ablation analysis is well-set up and indicates the stability of the proposed method."
            },
            "weaknesses": {
                "value": "- This framework may not sufficiently address how to perform with longitudinal data, where multiple conversation trees evolve over time or space.\n- The framework could benefit from considering the integration of user characteristics across different conversation trees, especially in social media data where the same user may participate in multiple sessions.\n- The details of integrating 17 structural tasks to fine-tune LLMs, such as considering the insightful associations and importance weights between these tasks, are not yet fully clarified.\n- In terms of comparisons, only traditional GNNs are used as the main baselines, lacking comparisons with more recent graph-based reasoning methods.\n- The reliance on task-specific prompts requires more results to clarify the extent to which various prompt styles across tasks influence TreeTop\u2019s effectiveness."
            },
            "questions": {
                "value": "- How are the 17 structural tasks weighted during training? Is their importance dynamically adjusted based on downstream application requirements?\n- Can TreeTop model the associations between multiple conversation trees, especially based on longitudinal data or same user scenarios?\n- How does TreeTop differentiate between structure- and content-related inferences when solving a reasoning task?\n- How does TreeTop compare to other fine-tuning based methods on general graph-based reasoning tasks, such as edge existence prediction or node classification?\n- How sensitive are the model outputs to prompt design, and is there a mechanism to automatically adjust prompts for different tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "TreeTop is a fine-tuning framework designed to enhance LLMs in understanding structured inputs like conversation trees on social media. Featuring 17 tasks, TreeTop focuses on improving LLMs' ability to reason about structural and content relationships in these discussions. LLMs fine-tuned with TreeTop outperform baseline models, including SOTA GNNs, in generalizing to new tasks and excelling in social media inference tasks, such as controversy detection, even in early-detection scenarios. This framework advances LLMs' capabilities in processing structured data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This work presents a new conversation tree encoding method. It would be beneficial to the field of multi-turn multi-party conversation.\n\n2. It reveals the shortcoming of existing LLMs on understanding such kinds of conversation tree, and also presents a fine-tuning-based solution."
            },
            "weaknesses": {
                "value": "1. I think the writing should be improved. For example, I did not catch how you finetune LLMs with the TreeTop framework. It is better to clarify it clearly in the main content.\n\n2. It seems that the baselines and related works are out-of-the-date. Are there any other more recently works of LLM on conversation tree?\n\n3. I wonder how the conversation tree benefits the downstream dialogue tasks, such as dialogue response generation. Generating and understanding the conversation tree should be the endpoints of exploring this technique. Constructing the conversation tree, I think, should facilitate the quality of the response generation in multi-party conversations. So, I think it is important to highlight how the conversation tree will help the field of dialogue system."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}