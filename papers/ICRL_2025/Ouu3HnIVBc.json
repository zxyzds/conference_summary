{
    "id": "Ouu3HnIVBc",
    "title": "ADAM: An Embodied Causal Agent in Open-World Environments",
    "abstract": "In open-world environments like Minecraft, existing agents face challenges in continuously learning structured knowledge, particularly causality. These challenges stem from the opacity inherent in black-box models and an excessive reliance on prior knowledge during training, which impair their interpretability and generalization capability. To this end, we introduce ADAM, An emboDied causal Agent in Minecraft, that can autonomously navigate the open world, perceive multimodal contexts, learn causal world knowledge, and tackle complex tasks through lifelong learning. ADAM is empowered by four key components: 1) an interaction module, enabling the agent to execute actions while documenting the interaction processes; 2) a causal model module, tasked with constructing an ever-growing causal graph from scratch, which enhances interpretability and diminishes reliance on prior knowledge; 3) a controller module, comprising a planner, an actor, and a memory pool, which uses the learned causal graph to accomplish tasks; 4) a perception module, powered by multimodal large language models, which enables ADAM to perceive like a human player. Extensive experiments show that ADAM constructs an almost perfect causal graph from scratch, enabling efficient task decomposition and execution with strong interpretability. Notably, in our modified Minecraft games where no prior knowledge is available, ADAM maintains its performance and shows remarkable robustness and generalization capability. ADAM pioneers a novel paradigm that integrates causal methods and embodied agents in a synergistic manner.",
    "keywords": [
        "embodied agent",
        "causality",
        "large language model",
        "interpretability",
        "vision language navigation",
        "cross-modal application",
        "cross-modal information extraction",
        "multimodality"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Ouu3HnIVBc",
    "pdf_link": "https://openreview.net/pdf?id=Ouu3HnIVBc",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces ADAM, an autonomous agent for open-world environments like Minecraft that builds a causal graph from scratch to improve interpretability and performance without relying heavily on pretrained knowledge. Through a combination of interaction, causal reasoning, planning, and multimodal perception, ADAM outperforms existing agents in task success and adaptability, even in modified game settings. ADAM\u2019s approach establishes a new standard for causal reasoning in embodied agents."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe figures in the paper are well-done and enhance clarity, making the content easier to understand.\n2.\tThe experiments conducted in Minecraft show a higher success rate than those achieved by Voyager."
            },
            "weaknesses": {
                "value": "1.\tThe paper appears to be hastily prepared, as it contains numerous typos and minor errors, such as inconsistencies between \u201cFig.\u201d and \u201cFigure\u201d references and improper usage of quotation marks in Table 4\u2019s caption. I recommend that the authors carefully review and correct these issues.\n2.\tThe major issue lies in the extensive use of pretrained language models that already incorporate substantial knowledge of Minecraft. Since language models may internally form a comprehensive causal graph of the game world, primarily in linguistic form, the proposed additional causal graph construction might be redundant. I suggest that the authors explore scenarios with completely altered world rules in Minecraft to test the validity of models like GPT in such modified environments, perhaps using a setting like \u201cMars.\u201d Alternatively, they could consider using a language model entirely devoid of Minecraft knowledge, though this may be challenging to achieve.\n3.\tThe agent\u2019s modular design is nearly identical to Voyager, with the primary addition being the causal graph. Experimentally, however, it does not show significant advantages over Voyager, as it does not complete tasks that Voyager was unable to.\n4.\tThe causal graph generated by the model ADAM is quite similar to the hybrid knowledge graph in memory described in [1]. The authors should clarify the differences.\n5.\tAdditionally, the current causal graph is entirely object-centric. In open-world Minecraft, there are many open-ended tasks, such as building and farming, which are not strictly object-centric. This limitation restricts ADAM\u2019s generalization capability in open-ended tasks.\n6.\tSeveral relevant works are not cited, including:\n\n\t[1] Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks\n \n\t[2] Mars: Situated Inductive Reasoning in an Open-World Environment, NeurIPS 2024\n \n\t[3] OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents, NeurIPS 2024"
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an architecture for agents that play the game of Minecraft, based on combining Large Language Models with causal inference. Featuring different modules (e.g., planner, actor, perception), the method is based on inferring causal graphs related to the various crafting dependencies of Minecraft's technology tree, and on enabling an agent to use the knowledge about these dependencies for progressing in the game. The method is evaluated against similar LLM-based methods (albeit using a different action space)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The method seems to be the first approach that combines casual inference with LLM agents in code-based action spaces, which is a potentially very important direction for future research.\n- The method performs quite well for inferring causal graphs on Minecraft, and it seems to provide a way for agents to take advantage of those causal graphs."
            },
            "weaknesses": {
                "value": "- The presentation of the method is quite high-level and does not help the reader understand how the method actually works in practice. When the different \"modules\" are introduced, it is not clear a priori what they actually are. Are they just prompts and specifications to a GPT4 model? If so, it could be beneficial to show one of the prompts earlier, to guide the understanding of the rest of the paper.\n- The comparisons in the paper are unclear, due to the choice of a specific action space that is different from the one used in previous work. Indeed, while the paper mostly discusses the \"observation space\" difference compared to the setting usually employed in reinforcement learning papers, one crucial difference is the one in action space. For instance, Voyager works in an extremely more high-level action space compared to DreamerV3. This is not accurately depicted in the current version of the paper.\n- It could be surprising to observe that an off-the-shelf open model is accurately describe an observation to the level of providing enough information for an actor to take the optimal action, especially in an environment that is as visually rich as Minecraft. An ad-hoc evaluation of this specific capability would strengthen the paper.\n- The method seems to be highly Minecraft-specific, and the paper does not extensively discuss how it could be generalized to other domains."
            },
            "questions": {
                "value": "- Would the method generalize to other environments? If so, what are the assumptions and requirements for the application of the method to a new environment?\n- How does the method compare to approaches trained with reinforcement learning? Is it possible to train an agent with reinforcement learning on the same action space that ADAM uses?\n- What is the captioning performance of the perception module? What are its failure cases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces ADAM (An emboDied causal Agent in Minecraft) - an agent architecture that autonomously explores, learns causal world knowledge, and executes complex tasks in Minecraft from multimodal inputs. The system consists of four main components: interaction module, causal model module, controller module, and perception module. The interaction module samples actions and records observations. The causal model module infers causal relationships and constructs causal subgraphs for each action.\nThe key innovation is integrating causal discovery methods with embodied exploration, enabling the agent to learn accurate causal relations from scratch without relying on prior knowledge."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The incorporation of causal discovery methods in a modular framework is a novel in LLM-based embodied exploration, and it does not rely on privileged information unlike prior work\n2. The paper demonstrate strong empirical results with well-designed experiments, that led to significantly faster discovery of skills in Minecraft. Performance in modified environments where prior knowledge is invalid did not degrade performance too much demonstrates causal learning is indeed effective. Method also does not require meta-data.\n3. The experiments are solid with multiple baselines and includes comprehensive ablation studies as well as detailed analysis of failure cases"
            },
            "weaknesses": {
                "value": "1. One concern is whether ADAM scales with more complex world and causal graph for intervention-based causal discovery (CD).\n2. Interestingly the paper proposes a multimodal agentic framework but all the baselines compared to are text-based frameworks. It would be good to have at least one multi-modal baseline, e.g. [1] as this is also cited by the authors.\n\n[1] Wang, Z., Cai, S., Liu, A., Jin, Y., Hou, J., Zhang, B., ... & Liang, Y. (2023). Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. arXiv preprint arXiv:2311.05997."
            },
            "questions": {
                "value": "See previous"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a Minecraft agent called Adam that relies on a combination of (M)LLM inference and causal discovery. At the core of the method is a causal graph that represents the agent's expertise on the environment logic. The causal graph is proposed by an LLM and each relation in the graph is confirmed/disproved by environment interactions. The proposed method obtains diamonds faster and more reliably than prior methods. Moreover,"
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- the paper is well structured and clearly written\n- the combination of LLM-prompting with CD seems to be quite novel\n- the intervention-based refinement of the causal graph seems meaningful and practical"
            },
            "weaknesses": {
                "value": "- unsupported claims: the authors claim their method has \"excellent interpretability\" and that their agent \"closely aligns with human gameplay;\" yet, I cannot find any empirical evidence supporting these claims. \n- a runtime/memory analysis (be it theoretical or empirical) is completely missing, i.e., it is not clear at which cost the claimed SOTA results come\n- the results presented in figure 1 are based on a modified causal graph claiming that this removes prior knowledge from LLMs; yet, it is unclear in how far this claim is true; it would be interesting to see an analysis akin to appendix A for the modified environment\n- the choice of the acronym Adam is at best unfortunate as it coincides with one of the most influential machine learning papers (https://arxiv.org/abs/1412.6980) and could be mistaken for an attempt to tap unwitting citations as both paper titles start with \"Adam: ...\"\n- insufficient reproducibility due to missing source code"
            },
            "questions": {
                "value": "- ad interpretability claim: how does the interpretability of the presented method relate to the interpretability of baseline methods, e.g., voyager? isn't the interpretability of the method compromised by the lack of interpretability of the used LLMs?\n- the proposed method is quite complex as it consists of 4 modules consisting of several submodules each and the presented ablation studies seem insufficient to rigorously justify such a complicated system; so how was the method designed? how much inspiration was drawn from prior work? \n- Adam is claimed to be a \"generalizable framework\"; why not back this claim with a complementary application in another environment? \n- what are possible limitations of the method? (the paper does not mention any)\n- what do the error bars in tables 1, 2, 3 mean?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}