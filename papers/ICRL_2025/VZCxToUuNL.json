{
    "id": "VZCxToUuNL",
    "title": "HyperSHAP: Shapley Values and Interactions for Hyperparameter Importance",
    "abstract": "Hyperparameter optimization (HPO) is a crucial step in achieving strong predictive performance, particularly for deep learning with hyperparameters controlling the neural architecture and learning behavior. However, the impact of some hyperparameters on model generalization can vary significantly depending on the dataset and performance measure, making it challenging to generalize their importance. Gaining a better understanding of the importance of hyperparameters is therefore important to deepen our understanding of machine learning and to leverage this knowledge in future downstream HPO tasks, especially if training is expensive and HPO needs to be as efficient as possible.\nTo address these challenges, we propose a game theoretic framework based on Shapley values and interactions for HPO. These methods offer an additive decomposition of a performance measure across hyperparameters, enabling both local and global explanations of hyperparameter importance and interactions. Our framework, named HyperSHAP, provides insights into ablation studies, tunability of specific hyperparameter configurations, and entire configuration spaces. Through experiments, we demonstrate that focusing on the hyperparameters deemed important by our framework improves performance during subsequent hyperparameter optimization, while ignoring important hyperparameters or interactions degrades performance. This validates the effectiveness of our approach in enhancing model performance and providing interpretable explanations of hyperparameter importance.",
    "keywords": [
        "explainability",
        "Shapley",
        "interaction",
        "hyperparameter optimization"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We propose HyperSHAP, a framework for quantifying hyperparameter importance based on Shapley values and interactions.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=VZCxToUuNL",
    "pdf_link": "https://openreview.net/pdf?id=VZCxToUuNL",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a framework that uses Shapley values and interactions to quantify the importance of hyperparameters. It starts with a detailed description of what the Shapley values and interactions are, and then uses these in the context of hyperparameter optimization to define the HyperSHAP framework. It includes an experimental evaluation on LCBench to show how it can be used and how it can lead to obtaining stronger performance by suitably reducing the search space based on the identified hyperparameter importances."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The paper introduces a new well-motivated strategy to quantify the importance of hyperparameters, including their interactions. The strategy uses Shapley values and interactions for this goal and there is relatively extensive contribution in terms of framing these in a way suitable for the problem of hyperparameter optimization.\n* The paper is well-written with precise explanations and well-made figures to explain the findings.\n* The paper shows how the framework gives support to finding that only lower-order interaction is\ntypically sufficient to explain most of the performance improvements when conducting hyperparameter optimization. This is in line with the mentioned literature and is good to confirm via HyperSHAP.\n* The paper shows the framework can be utilized in practice to focus on important hyperparameters and reduce the search space, improving anytime performance in constrained budget settings."
            },
            "weaknesses": {
                "value": "To me the main weakness is the limited experimental evaluation of the framework. It is evaluated on selected datasets within LCBench for neural networks (with rbv2 ranger benchmark in the appendix for random forests). The experimental evaluation works mainly as a way to showcase how the framework can be used and that it can be successfully used in at least some cases.\n\nIt would be quite valuable to see a more extensive evaluation, via a larger number of benchmarks with diverse sets of hyperparameters used - with some summary findings reported. Some of these could focus on fine-tuning settings (e.g. of large language models) as these are often of interest. Various pre-computed benchmarks exist so such significantly more rigorous evaluation may be manageable to conduct. For example, the PD1 benchmark could be a suitable example. Such more rigorous evaluation would help better establish the usefulness of the proposed methodology.\n\nPD1 benchmark: Pre-trained Gaussian Processes for Bayesian Optimization. Wang et al., JMLR\u201924\nhttps://arxiv.org/pdf/2109.08215"
            },
            "questions": {
                "value": "* Am I correct in understanding that the paper uses pre-computed benchmarks for evaluating the performance of sampled hyperparameters? I.e. it does not need to run long experiments with the sampled configurations.\n* HPO is often studied also in the form of neural architecture search. Does the framework also apply to this setting and is interesting to use there? For example to find good architectures in benchmarks such as NAS-Bench-201.\n\nNAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search. Dong et al., ICLR\u201920\nhttps://openreview.net/forum?id=HJxyZkBKDr"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes HyperSHAP, a game-theoretic framework for explaining hyperparameter importance and interactions during Hyperparameter Optimization (HPO) of ML models. The framework considers three instances where hyperparameter importance deserves to be assessed:\n- Ablation: Importance of hyperparameters in ablation studies.\n- Tunability: Importance of hyperparameters for a given optimizer. This can be used with several datasets to test the importance across different datasets. It corresponds to the classical HPO setting.\n- Optimizer bias: Importance of hyperparameters when considering several optimizers.  Can assess weaknesses of a given optimizer.\nExperiments show the benefits of HyperSHAP in an HPO setting by identifying the importance of individual hyperparameters and their higher-order interactions; and emphasizing optimizers' flaws."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors clearly define three instances of application for HyperSHAP that corresponds to concrete situations that ML practitioners regularly encounter.\n- The experiment effectively illustrates the claim that hyperparameters' importance can be different and that their interactions may impact the final validation error under different scenarios. \n- The paper is well structured."
            },
            "weaknesses": {
                "value": "Even if the paper is well structured, it is difficult to follow :\n- Much information that is required to understand what is going on is deferred to the appendix (e.g., how to read the plots, how the argmax are computed - there is not even a link to the corresponding appendix - for definitions 2 and 4). We should not have to resort to the Appendix to understand the results presented.\n- l.421. All of a sudden, the authors use FSII, whereas it was barely mentioned earlier, and we do not know what it is.\n- I find that the take-aways of the experiments are not clearly stated in Section 6\n- Sometimes the link between the conclusions and the experiment's results is not clear. For instance \n\t- l.348 \"For lower budgets, weight decay is not considered important.\" really? It seems equally important regardless of the number of evaluations in Figure 2. \n\t- l.350 \"the optimizer leverages the strong positive interaction between weight decay and learning rate\" There never seem to be any interaction between those two in Figure 2.\n\nAlthough the framework identifies that hyperparameters can be of different importance, as well as their interaction, I am not convinced of its practical utility. \n- The computational budget of this study is a serious concern for me. Indeed, evaluating SI involves a combinatorial number of model evaluations. The authors specify (in the appendix - it should be in the manuscript in my opinion) that they are evaluated with thousands of random evaluations, but it has a cost that is not taken into account e.g. in Section 6.3 in the x-axis reporting the number of evaluations. As a result, it is not sure that it effectively enhances the overall efficiency of HPO.\n- What is the point of evaluating the weaknesses of a given optimizer if it implies to evaluate $\\underset{\\lambda}{\\operatorname{argmax}} VAL_u(\\lambda, D_i)$ l.360, i.e. basically solving the problem of HPO?"
            },
            "questions": {
                "value": "Can you provide an updated comparison of RS and SMAC vs RS + SI and SMAC + SI with the same computational budget?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces HyperSHAP, a novel framework that utilizes Shapley values from cooperative game theory to assess and interpret hyperparameter importance and interactions in machine learning models. HyperSHAP provides a structured approach to hyperparameter optimization (HPO) by defining three \"hyperparameter importance (HPI) games\" to evaluate the impact of individual hyperparameters and their interactions. The framework supports three levels of analysis: (1) Ablation, which compares performance changes by switching individual hyperparameters; (2) Tunability, assessing the potential for performance improvement via tuning; and (3) Optimizer Bias, examining any systematic biases in hyperparameter optimization. HyperSHAP's efficacy is demonstrated through experiments that highlight its potential to improve HPO efficiency and enhance model performance by focusing on influential hyperparameters."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The framework targets explainable hyperparameter optimization, a critical aspect of machine learning that directly impacts model performance and efficiency, and that is usually important for practitioners.\n* HyperSHAP introduces Shapley values in a unique way to evaluate hyperparameters, capturing both individual and interaction effects, thus contributing to a more comprehensive understanding of hyperparameter importance.\n* The results validate HyperSHAP\u2019s utility, showing that focusing on high-impact hyperparameters the framework identifies can lead to better model performance and optimization efficiency."
            },
            "weaknesses": {
                "value": "* The experiments were limited in terms of search spaces and data modalities. Testing HyperSHAP on diverse configurations would improve its generalizability and robustness.\n* The study would benefit from a comparison with other baseline explainable HPO techniques to assess HyperSHAP\u2019s relative performance and computational efficiency.\n* Calculating Shapley values and interaction indices can be computationally intensive. Including performance and scalability insights would be helpful, especially for real-world applications where computational cost is a concern.\n* The paper is difficult to understand in some parts. For example, the Figure 2 is difficult to follow."
            },
            "questions": {
                "value": "* How expensive in time and resources is to compute the shape values?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces HyperSHAP, a method for explaining hyperparameter importance and interactions using Shapley values and Shapley interaction indices from game theory. HyperSHAP analyzes hyperparameter importance at three levels: hyperparameter configuration, hyperparameter search space, and hyperparameter optimizer behavior. Through experiments on a downstream task, the paper claims the value of focusing on the important hyperparameters identified by this method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Explaining hyperparameter importance using Shapley values and Shapley interaction indices appears to be an unexplored approach.\n- The paper provides sufficient background explanations for Shapley values and Shapley interaction indices."
            },
            "weaknesses": {
                "value": "- To demonstrate the practical usefulness of HyperSHAP, its time cost should be clearly presented and compared.\n  - As stated in the paper, Definition 2-4 require multiple HPO runs for argmax operations. Even with the use of an approximation, the time burden of HyperSHAP can still be significant.\n  - The time burden of HyperSHAP raises concern about whether applying HyperSHAP before other HPO algorithms provides any real benefit.\n  - More extensive and fair comparisons of HPO algorithms, with and without HyperSHAP, are necessary. Additionally, the time cost of HyperSHAP should be clearly reported.\n\n- The paper does not include any comparisons with other methods specifically designed to measure hyperparameter importance or interactions.\n\n- For HyperSHAP to be a reliable interpretability technique, its results on hyperparameter importance and interactions should match real benchmarks.\n  - The experiments in Section 6.2 are quite limited in scope, analyzing only two types of toy optimizers on a single dataset."
            },
            "questions": {
                "value": "Please refer to the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}