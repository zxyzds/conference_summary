{
    "id": "4S9bBbX1be",
    "title": "DriveArena: A Closed-loop Generative Simulation Platform for Autonomous Driving",
    "abstract": "This paper introduces DriveArena, the first high-fidelity closed-loop simulation system designed for driving agents navigating real-world scenarios. DriveArena comprises two core components:  Traffic Manager, a traffic simulator capable of generating realistic traffic flow on any global street map, and World Dreamer, a high-fidelity conditional generative model with infinite auto-regression. DriveArena supports closed-loop simulation using road networks from cities worldwide, enabling the generation of diverse traffic scenarios with varying styles. This powerful synergy empowers any driving agent capable of processing real-world images to navigate in  DriveArena's simulated environment. Furthermore,  DriveArena features a flexible, modular architecture, allowing for multiple implementations of its core components and driving agents. Serving as a highly realistic arena for these players, our work provides a valuable platform for developing and evaluating driving agents across diverse and challenging scenarios.  DriveArena takes a significant leap forward in leveraging generative models for driving simulation platforms, opening new avenues for closed-loop evaluation of autonomous driving systems.",
    "keywords": [
        "Autonomous Driving",
        "Diffusion Model",
        "Closed-loop Simulation"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "DriveArena is  a pioneering closed-loop autonomous driving simulator based on conditional generative models for training and testing driving agents.",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4S9bBbX1be",
    "pdf_link": "https://openreview.net/pdf?id=4S9bBbX1be",
    "comments": [
        {
            "summary": {
                "value": "The submission introduces DriveArena, a high-fidelity closed-loop simulation platform designed for testing and developing autonomous driving agents in real-world scenarios. The platform consists of two main components: the Traffic Manager and the World Dreamer. The Traffic Manager is responsible for generating realistic traffic flow on any global street map, while the World Dreamer is a high-fidelity conditional generative model that creates infinite autoregressive simulations. DRIVEARENA enables the generation of diverse traffic scenarios with varying styles and allows driving agents that can process real-world images to navigate within its simulated environment."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This submission targets an important problem in the field of autonomous driving: how to properly evaluate the performance of end-to-end systems. The submission introduces a closed-loop evaluation method, which is more reflective of real-world driving conditions compared to open-loop evaluations.  It will be useful for practical applications.\n2. The platform utilizes road networks from cities worldwide and allows for the generation of diverse traffic scenarios with varying styles, which is essential for training and evaluating driving agents across different driving environments. \n3. The submission provides a clear and detailed explanation of the technical aspects of DriveArena. The figures, tables, and appendices enhance the understanding of the system's components and their interactions."
            },
            "weaknesses": {
                "value": "1. The submission lacks novelty. The proposed DriveArena is just a combination of several existing methods: LimSim for traffic simulation and condition generation; DriveDreamer/Vista for generating images from the conditions; NAVSIM and Carla for closed-loop evaluation. \n2. The World Dreamer model is trained primarily on the nuScenes dataset, which may not capture diverse driving scenarios. To improve the model's generalizability, it would be beneficial to incorporate additional datasets that represent different geographical locations, driving cultures, and road conditions.\n3. This submission fails to address an important issue for closed-loop evaluation: the model should be able to generate the same scene captured from different positions (similar to actual scenarios of driving differently in the same scene). No visualization was found addressing such an issue. \n4. Experiments are not enough. The submission primarily focuses on the simulation platform itself rather than an in-depth evaluation of various driving agents within the platform. Expanding the experimental section to include a broader range of driving agents and more extensive testing can help provide a clearer picture of DRIVEARENA's capabilities and limitations.\n5. Minor issues in writing and presentation. For example, The figures are not vectorized for zooming in and they are suggested to be replaced."
            },
            "questions": {
                "value": "Can authors provide visualizations of generating consistent scenes based on slightly different conditions (e.g., the ego car moves differently), which is a key aspect for closed-loop evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents a generative simulation framework for autonomous driving. In particular, a layout-conditional diffusion model is proposed as a sensor simulator, with bounding boxes and road graphs serve as underlying world state."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The topic of generative simulation is an especially timely one for the AV field, with better and better generative video models coming out regularly and frameworks such as this one being able to capitalize on these parallel investments.\n\nWhile all of the individual pieces of this framework have existed before, connecting them all together into a usable simulation framework for the broader community to use is appreciated and presents a potential path towards practical generative simulation.\n\nThe paper is written well and it is easy to follow the core ideas as presented.\n\nLeveraging layout-conditional generation is a sound and sensible idea for maintaining consistency across time."
            },
            "weaknesses": {
                "value": "The core weakness is that any geometric or semantic consistency is not guaranteed. Layout conditioning certainly helps, but in the linked webpage's videos there are clear inconsistencies across timesteps (e.g., car color and types changing over time). This is something that is not brought up in Lines 190 - 198, but perhaps should be as it is the core reason why works leverage NeRF or 3D Gaussian Splatting (for their geometric/semantic/temporal consistency over purely-2D generative models).\n\nWhile static images of generations appear to be of good quality, there are significant temporal consistency issues when viewed as part of a video on the linked project page (most videos appear to be static even with the ego-vehicle theoretically moving forward in the world). Do the authors have any idea for why that is? It almost appears that the video diffusion model suffers from mode collapse when tasked with generating building walls (taking example from the linked webpage).\n\nThe AV results in Tables 1 and 2 still show a significant gap to real data, indicating that, while the core points of DriveArena are sensible, there is still much work to be done to leverage it for practical AV development."
            },
            "questions": {
                "value": "In Lines 261-263 it is written \"We also verify that extending to a multi-frame auto-regressive version (using multiple past frames as reference and outputting multi-frame images) and adding additional temporal modules can enhance temporal consistency.\" - How does this work in practice? In closed-loop, the ego-vehicle can drive however it wants and so there can still be inconsistencies between generated videos at time t and t+1, right? Further, how are the generated videos used? Are T frames predicted but only the 1st one is shown to the ego-policy? And then a new prediction is made (similar to model-predictive control)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents DriveArena, an image-based high-fidelity simulator for closed-loop simulation of agents for autonomous driving. DriveArena consists of a Traffic Manager and a World Dreamer, and its modular design allows for easy replacement of each of the components. Traffic Manager enables the dynamic control of all traffic agents and supports various HD maps, both of which are inputs to World Dreamer, which uses conditional diffusion to generate realistic images. The diffusion model is conditioned on map and object layout and generates images autoregressively for temporal consistency and variable length simulations.\n\nDriveArena is evaluated in two ways. First, its fidelity is evaluated, among others, with UniAD's performance. Then, open- and closed-loop evaluation of VAD and UniAD models is performed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Highly relevant research direction: The work correctly argues for the need of closed-loop evaluation of autonomous driving behavior models.\n* Novelty: High-fidelity closed-loop image-based simulation with clear controllability (e.g. via text prompts).\n* Performance: Evaluation of sim-to-real gap shows superiority over MagicDrive and reasonable results for open-loop and closed-loop behavior eval.\n* Well presented: The paper is easy to follow and all information is well presented."
            },
            "weaknesses": {
                "value": "* Evaluation of sim-to-real gap: The presented evaluation is pretty short and for example lower L2 distances do not necessarily imply higher quality images. Additional evaluation of the fidelity would be helpful. Are there perception metrics such as FID that could be used or other metrics that compare statistics between ground truth and generated images? Otherwise, user studies are another possibility to judge the quality.\n* Unclear takeaway from VAD vs. UniAD open- and closed-loop comparison: In open-loop, UniAD performs better on nuScenes but worse on DriveArena than VAD. This difference is explained with better open-loop generalization of VAD. However, it's unclear what role the fidelity of DriveArena plays. Is it possible to e.g. run an experiment with different DriveArena datasets, some that are closer and some that are further from nuScenes? In closed-loop eval, UniAD outperforms VAD in DriveArena. It's unclear whether these differences are due to open- / closed-loop model gaps or issues in DriveArena. I acknowledge that this difficulty of correct attribution is inherent to research in this area but you might be able to shed more light on this. For example, would it be possible to evaluate the models on various levels of fidelity in DriveArena to disentangle open- / closed-loop eval from it?"
            },
            "questions": {
                "value": "* Evaluation of sim-to-real gap: How are the videos generated? Is this in open- or closed-loop? For correctness, it should be closed-loop. If so, do you notice any suffering from DAgger issues?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a traffic simulation platform for testing autonomous driving algorithms. The simulation architecture is built based on LimSim, using Monte Carlo tree search for vehicle motion planning. A diffusion-based renderer is applied to achieve realistic and controllable surround images for vision-based driving agents. The platform supports driving scenario construction from nuScenes and OSM; codes and tools are open-source for the community to use."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Compared to previous work using video generation methods as world models to achieve realistic traffic simulation, this work uses a two-step pipeline, including rule-based traffic simulation in geometric space and diffusion-based video generation conditioned on trajectory layouts of vehicles. I believe this approach can achieve better physical realism and temporal consistency.\n\nText prompts are introduced to achieve diverse driving scenarios and plenty of demos are presented to clearly show the generation results.\n\nThe codes are well organized with a modularized design and the whole platform is open-source to better support downstream tasks in research of autonomous driving."
            },
            "weaknesses": {
                "value": "The author says that one of the main contributions is scalability, which means simulation on any region can be achieved by using map info from OSM. As far as I know, OSM only contains road-level map information, and extra efforts like completing lane topology and planning vehicle OD are needed to construct simulations based on it, this part of the work seems unclear in this paper.\n\nAs the dreamer is the most important part of this paper, it would be better if the author could provide some indicators that can directly evaluate the generated results, like FID.\n\nMinor note: it seems that Figure 3 is not in vector format."
            },
            "questions": {
                "value": "+ About constructing traffic scenarios from OSM.\n  + How is the HD map built from the OSM data, and how is traffic demand generated in this kind of scenario? \n  + OSM maps are not high quality in many areas, is there a way to solve this?\n  + Is this part of the work mainly based on the tools provided by SUMO or LimSim?\n+ Are there any indicators to evaluate the generated results directly, like FID or FVD?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}