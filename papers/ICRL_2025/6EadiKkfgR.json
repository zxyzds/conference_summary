{
    "id": "6EadiKkfgR",
    "title": "Contrastive Learners Are Semantic Learners",
    "abstract": "In this work, we explore the definition of semantic equivalence to establish a connection between contrastive tasks and their downstream counterparts. Specifically, we investigate when a contrastive dataset can learn representations that encode semantic relations for a specific downstream task. In our analysis, we recover a surprising hypothesis resembling the distributional one\u2014dubbed distributional alignment hypothesis. Under this assumption, we demonstrate that a simple contrastive learning procedure can generate representations that encode semantic relations for the downstream task. Furthermore, we support the theory with a series of experiments designed to test the presented intuitions.",
    "keywords": [
        "contrastive learning",
        "self-supervised learning",
        "embedding"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "This work proposes the \"distributional alignment hypothesis,\" showing that contrastive learning can effectively capture semantic relations for downstream tasks, and supports the theory through experimental validation.",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6EadiKkfgR",
    "pdf_link": "https://openreview.net/pdf?id=6EadiKkfgR",
    "comments": [
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "Thank you for your response! The outline of the theorem's reverse direction looks correct to me. The proposed new experiment would also better address my concerns. If the authors revise the paper to include the reverse direction of Corollary 4.3, and add the new proposed experiment to the paper, I will raise my score.\n\n I am still concerned about the strength of the assumptions made in the paper. Is there anything that can be said about the \"closeness\" of embeddings with very similar (but not equal) conditional distributions?"
            }
        },
        {
            "comment": {
                "value": "We appreciate the reviewer\u2019s feedback and hope that the following considerations might lead you to reconsider your evaluation of our paper.\n\n### Weakness 1\n\n> Limited empirical evidence to a single contrastive framework.\n\nWe have limited our experiments to the SimCLR variant scenario; however, we acknowledge that the contrastive learning landscape is much broader. Including experiments with additional frameworks would provide a more comprehensive understanding of the applicability of Corollary 4.3.\n\nWe propose the following experiments:\n1. Replace the infoNCE loss with triplet loss in Section 5. This will offer preliminary insight into whether the theoretical results extend to different loss functions.\n2. Replace the SimCLR contrastive framework with Barlow Twins [2]. This will offer preliminary insight into whether the theoretical results extend to different frameworks.\n\nBoth of these changes alter the assumptions made in the theoretical analysis. However, we still expect to observe empirical results similar to those seen in the SimCLR variant scenario.\n\nIf the reviewer agrees with this proposal, we will incorporate these experiments into the revised manuscript.\n\n### Weakness 2\n\n> Semantic equivalence is based on an analogy to programming languages, which may not translate perfectly to the nuances of different data modalities.\n\nWe respectfully disagree with the reviewer\u2019s assessment. We believe that the proposed definition of semantic equivalence is applicable across all data modalities.\n\nWe regard it as a general principle that symbols (which, depending on the task, may refer to an image patch, a sentence, a token, etc.) that do not change the outcome regardless of context should be encoded with the same embedding. This principle is effectively formalized (in Def.2.1) through the definition of semantic equivalence.\n\nTo illustrate this principle, consider the case of a cat-vs-dog image classification task, an image patch of clear sky should be encoded with the same embedding as an image patch of a cloudy sky, since the presence of clouds should not affect the classification outcome.\n\n### Weakness 3\n\n> Limited practical applicability.\n\nWe believe that our analysis provides novel theoretical insights into the relationship between pretraining contrastive datasets and downstream tasks. It also characterizes a property (Corollary 4.3) that the optimal embedding function must satisfy when applying SimCLR (under the appropriate hypotheses). Ultimately, we believe that the proposed theory represents a valuable and novel contribution to the field.\n\nAdditionally, we speculate that future work may explore ways to estimate the degree to which the alignment hypothesis holds in real-world data. This could enable more practical applications of the proposed theory.\n\nWe will revise the manuscript to more clearly communicate these points in the *Conclusion* section.\n\n[1] Schroff et al. FaceNet: A Unified Embedding for Face Recognition and Clustering. 2015.\n\n[2] Zbontar et al. Barlow Twins: Self-Supervised Learning via Redundancy Reduction. 2021."
            }
        },
        {
            "comment": {
                "value": "We appreciate the reviewer's feedback highlighting important extensions and connections to related fields. We hope that the following considerations may enhance the paper's quality.\n\n### Weakness 1\n\n> Parallels with the MultiTask Learning (MTL) literature.\n\nWe agree that there is a parallelism between our work and the suggested field, specifically the work of [1]. The shared representation function $h$ is required to encode the information that is relevant to all tasks. Similarly, here, we want an embedding function that encodes the relevant information for the downstream tasks. \n\nThe main difference lies in the fact that, during the downstream training, we allow for the embedding (or representation) function $\\mathcal{E}$ (or $h$) to specialize for each downstream task. However, if we were to freeze its parameters, the methodology would reduce to learn a single $h$ for all $g_k$, similarly to [1].\n\nWe speculate that the optimal $h^{*}$ should encode semantically equivalent relationships for the target tasks (under the right hypotheses). \n\nWe will make sure to address this parallelism in the following revision.\n\n### Weakness 2\n\n> The work is limited to token representation.\n\nWe want to stress that the embedding function $\\mathcal{E}$ (mapping symbols to symbol-embedding) and the encoder function $E$ mapping context to context-embeddings are not subject to restriction in either data modality nor architecture. \n\nFor example, we may have an embedding function, implemented as a Convolutional Neural Network, generating symbol-embedding from a single image-patch. Meanwhile, we may have an encoder function, implemented as Vision Transformer, generating the context-embedding from the remaining image-patches. \n\nSimilarly, we could have an embedding function, implemented as a Long Short Term Memory, generating symbol-embeddings from a natural language sentence, Meanwhile, we may have the encoder function, implemented as a classical Transformer, generating the context-embedding from the remaining document.\n\nWe will revise the manuscript to make these points clear.\n\n### Weakness 3\n\n> Weaker semantic relation might broaden the applicability.\n\nWe agree. We speculate that, using a weaker relation (namely semantic similarity) would lead to similar but more general results. Formally, we could define semantic similarity as follows:\n\n$$u \\circeq_\\epsilon v \\iff d(p(Y|u,\\mathrm{P}),p(Y|v,\\mathrm{P})\\leq \\epsilon$$\n\nWhere $d$ is a distance function (e.g. Euclidean or Weisserstein). We anticipate that two semantically similar symbols must be encoded in close embedding by the optimal embedding function. \n\n> Weaker alignment might broaden the applicability.\n\nWe agree. Again, we speculate that using a weaker hypothesis would lead to similar but more general results. A weaker definition of alignment could be made as follows:\n\n$$d(p_{\\mathcal{D}}(y|u,\\rho),p_{\\mathcal{D}}(y|v,\\rho)) \\leq \\epsilon \\iff d(p_{\\mathcal{C}}(\\rho|u),p_{\\mathcal{C}}(\\rho|v)) \\leq \\epsilon $$\n\nHere, $d$ is a distance function (e.g. Euclidean or Weisserstein). We speculate that when this weaker hypothesis hold, one could derive a weaker version of the Corollary 4.3:\n\n$$u \\circeq_{\\epsilon_0} v \\iff d(\\mathcal{E}^*(u),\\mathcal{E}^*(v)) \\leq \\epsilon_1 $$\n\nHere, $d$ is a distance function (e.g. Euclidean).\nWe will include this consideration as potential avenues for future work in the revised manuscript.\n\n[1] Maurer et al. The Benefit of Multitask Representation Learning. 2016."
            }
        },
        {
            "comment": {
                "value": "Thanks for your feedback. We believe that many of the concerns arise from overloading the term semantic with both a linguistic and a technical meaning. In the following revision, we will clarify these issues. We hope that the following considerations may lead you to reconsider your evaluation of the paper.\n\n### Weakness 1\n\n> The alignment hypothesis (AH) is too strong.\n\nWe agree. However, we would like to point out that, in practice, the conditions under which pretraining data can be useful for a downstream task are also quite strong. For instance, randomly generated data is unlikely to benefit any downstream task. Similarly, using genomic data as contrastive data would likely be ineffective for a sentiment classification task.\n\n> The AH is not useful in practice.\n\nVerifying AH in practice is not feasible. However, it can explain why some contrastive data are more effective than others for a particular downstream task.  \n\n> Not novel intuitions\n\nWe respectfully disagree with the reviewer\u2019s assessment. While the intuition that contrastive learning can capture semantics (here, as a linguistic term) is well established, our work aims to formalize the conditions under which this occurs for the formal definition of semantics. We believe this adds a novel contribution to the field.\n\n### Weakness 2\n\n> Too broad claims\n\nWe agree, and we appreciate this feedback. We acknowledge that the title may have contributed to this impression, and we are considering alternatives such as \"When Can a Contrastive Learner Capture Semantics?\". Would this title be more appropriate?\n\nWe will revise the claims in the abstract, introduction, and discussion to more accurately reflect the theoretical findings.\n\n### Weakness 3\n\n> Loose terminology.\n\nWe apologize for any confusion caused by the terminology. Our concept of \"semantics\" is based on Def.2.1, which is derived from classical formal semantics. We briefly reference the denotational notation [3]. In short, we define $u$ and $v$ as **semantically equivalent/similar** if they can be interchanged in any/most contexts $\\rho$ without affecting the outcome distribution $y$. We recognize that using \"semantics\" as both a linguistic and technical term may have led to confusion. \n\n> Unclear meaning of \"semantic learner\"\n\nWe apologize for the confusion. By \"semantic learner,\" we refer to a model that encodes semantic equivalence relationships (as defined in Def.2.1) within its embeddings. \n\n> We already know that contrastive learning (CL) can learn semantics from the literature.\n\nWe agree that it is well established in the literature that CL can capture semantics (here intended as a linguistic term). However, our work aims to identify and formalize the theoretical conditions under which this occurs for the formal definition of semantics. We believe this adds a novel contribution to the field.\n\n> Unclear meaning of \"semantic relations\"\n\nWe apologize for the confusion. Here, \"semantic relations\" specifically refers to the semantic equivalence relations in Def.2.1.\n\n> Not novel RQ2.\n\nWe agree that the question \"Can we train embeddings that effectively encode semantic relations?\" may appear unoriginal if interpreted with the linguistic meaning of semantics. However, in our work, \"semantic relations\" refers to formal semantic equivalence, as in Def.2.1. \n\nWe will address all the previous terminology issues in the revised manuscript. \n\n### Weakness 4\n    \n> Findings not clearly stated / too broad claims.\n\nWe apologize for the lack of clarity. We will revise the *Findings* paragraph in Section 1 to more clearly state the contributions of the paper and avoid overly broad claims.\n\nAdditionally, we will introduce a paragraph at the end of the discussion to address the implications of Corollary 4.3. In the meantime, here is a brief explanation:\n\n- Suppose you have a cheap CL dataset, $\\mathcal{C}$, which could be a set of images.\n- Suppose you have an expensive downstream dataset, $\\mathcal{D}$, which could be a set of image-label pairs.\n- Suppose we use a CL method as described in Sect.3.\n\nWe aim to learn useful embeddings from $\\mathcal{C}$ to be used for learning $\\mathcal{D}$. Formally, we desire \n\n$$u \\circeq v \\iff \\mathcal{E}(u) = \\mathcal{E}(v) \\text{ ( denoted as \u2605)}$$\n\nWhat conditions on $\\mathcal{C}$ and $\\mathcal{D}$ are required to achieve \u2605? For once, we require an AH (Def.4.1) on the data. Further, when the AH holds, we can say that an optimal model must encode semantic equivalence relations in its embeddings, which characterizes a key property of the optimal model.\n\n\n[1] Goldberg, et al. Neural word embedding as implicit matrix factorization. 2014.\n\n[2] Arora, et al. A theoretical analysis of contrastive unsupervised representation learning. 2019.\n\n[3] Tennent et al. The denotational semantics of programming languages. 1976."
            }
        },
        {
            "comment": {
                "value": "Thank you for carefully reviewing the mathematical aspects of our paper. We hope that the following considerations may lead you to reconsider your evaluation of the paper.\n\n---\n### Weakness 1\n\nWe confirm that Cor. 4.3 indeed represents an iff relationship. We agree that the backward direction was not explicitly demonstrated. The proof is simple, and we will add a full derivation in the next revision. In the meantime, here is the outline:\n\n- As you mentioned, from [2] we have $f_i = c \\frac{p(\\rho, y | \\sigma_i)}{p(\\rho, y)}$ or, using our notation, $\\mathcal{E}^*(u) = c \\frac{p(\\rho, y | u)}{p(\\rho, y)}$.\n- A similar relationship holds for $v$, so $\\mathcal{E}^*(v) = c \\frac{p(\\rho, y | v)}{p(\\rho, y)}$.\n- By hypothesis, $\\mathcal{E}^*(u) = \\mathcal{E}^*(v)$, which implies $p(\\rho, y | u) = p(\\rho, y | v)$.\n- Using $p(\\rho | u) = p(\\rho | v)$, we get $p(y | \\rho, u) = p(y | \\rho, v)$, fulfilling $u \\circeq v$.\n\nIf the constant $c$ raises concerns, the same steps apply from the end of Lemma A.1 as in Th. 3.1.\n\n### Weakness 2\n> Assumption $f_i=c\\frac{p(\\rho,y|\\sigma_i)}{p(\\rho,y)}$.\n\nThis assumption arises from a model optimality condition rather than being directly borrowed.\n\n> Assumption $p(\\rho,y|u)=p(\\rho,y|v)$.\n\nWe agree with the reviewer that the assumption $p(\\rho,y|u)=p(\\rho,y|v)$ is quite strong. However, we want to stress that this assumption stems from the definition of semantic equivalence ($p(y|u,\\rho)=p(y|v,\\rho)$) and the assumption $p(\\rho|u)=p(\\rho|v)$. While the former is the relationship under study, the latter is introduced to make the problem tractable. However, we want to emphasize that such assumption is quite reasonable even in practice. If it were not to hold, we could find ourselves in a scenario where a context always appears together with one symbol but never with the other. In such cases, it would be impossible to recognize the two symbols as semantically equivalent. \n\n> The encoder that maps every input to 0 also satisfies this semantical equivalence property.\n\nThe forward implication of Cor. 4.2 is significant, even without the backward implication. While a zero-encoder satisfies the forward implication, it is unlikely to be optimal except in degenerate data scenarios. We find it compelling that, if two symbols are semantically equivalent in classification data, the optimal encoder for contrastive data (under Def.4.1) must reflect this equivalence in its embeddings, facilitating easier training on the classification data.\n\nWe will address these points in the manuscript.\n\n### Weakness 3\n> The ModAdd experiment lacks complexity and noise seen in real data.\n\nOur goal is to provide a theoretical explanation for how semantically similar objects are encoded with similar embeddings using contrastive learning and joint embedding methods, a concept well-supported in the literature. As such, we limited our experiments to a simplified example, but we agree that a larger-scale experiment would enhance the study.\n\nWe propose the following experiment inspired by word2vec:\n\n- Use SimCLR to learn word representations from natural text.\n- Randomly select $k$ words and generate semantically equivalent variants for each.\n- Randomly swap occurrences of the original word with its variant (e.g., swapping \"cat\" with a new token, $\\text{cat}_1$ and $\\text{cat}_2$, with probability $p = 0.5$).\n\nSince these tokens are semantically equivalent, we expect the model to bring their embeddings closer. We can also analyze how varying $p$ affects the model's ability to learn this relationship.\n\nPlease let us know if this experiment would better address your concerns. In this case, we will include it in the revised manuscript.\n\n### Remark 1\n> In Figure 2(a), the semantically equivalent pairs do not converge to 0. Why?\n\nAt a certain point during training, the contrastive loss becomes so small that further changes in the embeddings are minimal, even after 10,000 epochs.\n\nWe will include this explanation in the revised manuscript.\n\n### Remark 2\n> Basis assumption motivation.\n\nThe assumption of a basis is made so that the only possible solution for the optimal model is to encode semantically equivalent symbols in the same embedding. Furthermore, note that any slight perturbation of $E^*(\\rho_{i_0},y_{i_0}), \\dots, E^*(\\rho_{i_d},y_{i_d})$ would establish a basis almost surely. This hypothesis is also employed in the context of invertible neural networks [1]. \n\nHowever, we recognize the mentioned manuscript as a missed related work that will be included in the next revision.\n### Minors\n> - line 176: mentions \"both\" but only lists Theorem 3.1.\n> - line 385: \"potetial\"\n> - Appendix A.9 Title: \"SLIGTHLY\"\n\nThank you for pointing out these typos. We will correct them in the revised manuscript.\n\n[1] Finzi, Marc, et al. Invertible convolutional networks. 2019.\n\n[2] Aaron van den Oord, et al. Representation Learning with Contrastive Predictive Coding. 2018."
            }
        },
        {
            "summary": {
                "value": "This paper explores the theoretical foundation of contrastive learning, a popular self-supervised technique used to generate high-quality embedding representations across various data modalities (e.g., images, audio, text). While contrastive learning has shown empirical success in encoding semantically similar objects into close embedding representations, a formal understanding of this process is lacking. To address this gap, the authors propose a formalization of semantic equivalence in contrastive learning, inspired by principles from programming language theory. They introduce the distributional alignment hypothesis, which posits that the alignment of distributions in contrastive tasks is essential for effective downstream performance. Through analysis of the SimCLR method, they demonstrate that contrastive learning can inherently encode semantically equivalent symbols in close proximity within the embedding space."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.The paper provides a theoretical perspective on contrastive learning, bridging the gap between empirical success and formal understanding.\n\n2.Introducing the concept of semantic equivalence in the context of contrastive learning is innovative, borrowing ideas from programming languages to define how two symbols can be considered equivalent in the embedding space.\n\n3.The proposal of the distributional alignment hypothesis offers a new framework for understanding when contrastive tasks are effective for downstream applications, potentially guiding future work in contrastive learning model design."
            },
            "weaknesses": {
                "value": "1.While the theoretical findings are compelling, the paper might benefit from empirical experiments to validate the proposed hypotheses, particularly the distributional alignment hypothesis, across different contrastive learning frameworks. As the study is heavily focused on theoretical formalism, which may limit its immediate applicability for practitioners who are looking to implement contrastive learning solutions without deep theoretical knowledge.\n\n2.The paper\u2019s formalism of semantic equivalence is based on an analogy to programming languages, which may not translate perfectly to the nuances of different data modalities (e.g., images vs. text), potentially limiting its generalizability across tasks.\n\n3.There have been a number of analysis to study the effectiveness of contrastive learning, but it is hard to say how the new perspective would help the improvement of contrastive learning method."
            },
            "questions": {
                "value": "Please refer to the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The core argument is that if a pretraining task is \"distributionally aligned\" with a downstream task, this alignment benefits the downstream task. The concept of distributional alignment relies on \"semantic equivalence\" -- where 2 features are interchangeable for a prediction task without altering the target label's probability (e.g., synonyms). Two tasks are then distributionally aligned if they share these equivalences; that is, if tokens are semantically equivalent in one task, they remain so in another. The paper suggests that pretraining with a contrastive loss facilitates this by encouraging the model to capture these contextual equivalences on  context prediction task, making it beneficial for downstream tasks (when this alignment holds)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper presents an interesting theoretical exploration, backed by set of empirical experiments that, while somewhat limited, suppor t and illustrate the main theoretical arguments. Overall, the paper is clearly written, and the theoretical results, though potentially unsurprising, appear to offer novel insights."
            },
            "weaknesses": {
                "value": "Comments\n\n1. This framework seems to have parallels with the learning theory in multitask learning. For instance, Maurer et al. (2016) (multitask subspace learning) argue that multitask learning can be advantageous when task-specific functions can be decomposed into shared and unique components, i.e., f_k = g_k cdot h, where function h is shared across tasks. Drawing on these connections with LT for MTL may be beneifical, and may clarify the novelty. Currently, previous research on MTL is not discussed in the paper.\n\n2. The focus on token representations (word embeddings) as the primary benefit of pretraining is interesting, but it raises a question: could this analysis extend beyond token-level representations to contextualized models as a whole? It would be valuable to explore if and how these insights might generalize.\n\n3. Lastly, the assumption of strict equivalence across all predictions might be too strong. A more nuanced setting, where only some predictions share semantic equivalences with the downstream task, would broaden the applicability. For example, if predicting continuations like \"thumbs_up\" and \"thumbs_down\" shares equivalences with a downstream sentiment classification task, but other alternative may not share them, the theory should ideally capture this partial alignment.  The intuition is that probably some of the pretrainintg decision share equivalencess, some unrelated, some require more 'fine-grain' equivalences' or more coarse-ones, this still works.\n\nReference: Maurer, A., Pontil, M, Romero-Paredes, B. (2016). The Benefit of Multitask Representation Learning. JMLR. https://jmlr.org/papers/volume17/15-242/15-242.pdf"
            },
            "questions": {
                "value": "See the three comments above, I'd appreciate authors view on these points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores contrastive learning and investigates under which conditions contrastive learning can be effective for downstream tasks. The paper introduced a \"distribution alignment hypothesis\": if the data distributions used in contrastive learning and downstream tasks are aligned, then contrastive learning will learn semantic representations that good for the task. To support the hypothesis, the paper provides a controlled experiment using a mod-addition task."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "At a high level, the work is well motivated: the match between the data used in contrastive learning and final downstream task is crucial for good empirical performance."
            },
            "weaknesses": {
                "value": "- The main limitation I see is similar to that the authors identify in Section 7: the alignment hypothesis introduced is very strong and verifying it is not possible in practice. On the other hand, the intuitions that it sheds light on are not novel: despite the claims made, it's been known for a long time that one can learn similarity from this type of data (please see work such word2vec, or subsequent papers such as Levy and Goldberg, showing the relation to the classic distributional semantics work)   \n- The paper considers a very specific form of contrastive learning using a specific augmentation function. In general the term contrastive learning is much wider, and the data used varies widely, from data with labels in classification, to question answer pairs in retrieval. I would recommend being much more specific in the claims made.  \n- The paper is very loose with the terminology that is at the basis at its main questions and claims, leading to vague or in-accurate statements. For example the answer to RQ2 in intro is clearly yes: we can train embeddings to reflect semantic similarity (see more in suggestions below). Similarly, what is a semantic learner? Semantics is a very wide term (see the field of semantics in linguistics) and yes, speaking in general terms, we already know we can learn meaning from raw data."
            },
            "questions": {
                "value": "The paper needs to provide definitions or be very specific about the terms such as \"semantic relations\". For example in RQ2 (line 47): \"can we train embeddings that effectively encode semantical relations?\" What do you mean by semantic relations here? And the answer to this question is yes, clearly more than ten years of research in the field of distributional semantics have shown that we can train them to reflect semantic similarity.    \nI would also encourage the authors to try and get to core of their findings and explain them better: I struggled to figure out if there is actual content to the definitions and theorem given, or I was simply walked through re-writing of formulas. For this, the paper would benefit from focusing on the actual setup (for example the SimCLR variants used) and less on making wider claims about the findings."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents an analysis that contrastive loss (InfoNCE) learns representations that encode semantic relationships effective for downstream tasks. The paper proves that, under certain assumptions, semantically equivalent symbols have the same learned embeddings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper borrows frameworks from programming languages to analyze theoretical properties of contrastive embeddings, which is an original and interesting perspective.\n2. The paper is written clearly. The exposition is very good for building up reader intuition and the assumptions are clearly stated for each proof."
            },
            "weaknesses": {
                "value": "> Weakness 1. Corollary 4.3 states an **iff** relationship between the alignment of symbols and their respective embeddings. However, the authors only prove the forward direction.\n\nConditional on Corollary 4.3 being a typo, and only the forward direction holding: $u \\doteq_{\\mathcal{D}} v \\Rightarrow \\mathcal{E}^*(u) = \\mathcal{E}^*(v)$, why is this conclusion valuable? An encoder that maps every input to 0 also has this property.\n\n> Weakness 2. The assumptions are unrealistic and lead trivially to the Theorems 3.1 and 4.2.\n\nI believe the proof for Theorem 3.1 is a bit obfuscated. The authors essentially make two assumptions:\n\n1. that contrastive encoders learn the following probability ratio up to a multiplicative constant: $f_i = c \\cdot \\frac{p(\\rho, y \\mid \\sigma_i)}{p(\\rho, y)}$. (taken from van den Oord et al. (2018))\n2. that $p(\\rho, y \\mid u) = p(\\rho, y \\mid v)$, which is a trivial result of (1) $u \\doteq_{\\mathcal{D}} v$, which implies to $p(y \\mid u, \\rho) = p(y \\mid v, \\rho)$,  and (2) $p(\\rho \\mid u) = p(\\rho \\mid v)$.\n\nBecause of Assumption 2, we can immediately conclude that $f_u = f_v$, and use the basis argument to conclude that $u$ and $v$ must have the same embeddings.\n\nThe direct assumption that $p(\\rho, y \\mid u) = p(\\rho, y \\mid v)$ seems quite strong and substantially simplifies the analysis, as the desired result follows almost immediately from this condition. Also, it is unclear that this result is meaningful in any way for learnability. As stated previously, the encoder that maps every input to 0 also satisfies this property. It is only this property in conjunction with the reverse direction *(that semantically dis-similar symbols are not mapped to the same encoding)* that is interesting. I may be misunderstanding, but I do not see any proof of the reverse direction in the paper.\n\nTheorem 4.2 follows the same basic structure, and the same concerns apply.\n\n > Weakness 3. Limited experimentation\n\nThe ModAdd experiment, while very illustrative of the beneficial properties of contrastive learning, can be solved symbolically. It lacks complexity and noise seen in real data. Simple experiments in language or vision would better illustrate the developed theoretical results. Possibly an evaluation on an MLM task (as shown as an example in Section 2.1)?\n\n---\nReferences:\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Predictive Coding. 2018."
            },
            "questions": {
                "value": "Mentioned in weaknesses.\n\nSome additional remarks:\n1. In Figure 2(a), the semantically equivalent pairs on average converge to a lower euclidean distance compared to non-semantically equivalent pairs. However, it does not converge to 0, as the previously developed theory would suggest. Why is this the case?\n \n2. Minor remark: There should be better ways of motivating the assumption $E^*(\\rho_i, y_i)$ forms a basis for $\\mathbb{R}^d$. There exists prior work on hyper-spherical contrastive learning that proves certain uniformity results that seem related to the author's assumption of uniformity [1]. \n\n*Minor*\n- line 176: mentions \"both\" but only lists Theorem 3.1.\n- line 385: \"potetial\"\n- Appendix A.9 Title: \"SLIGTHLY\"\n\n---\nReferences:\n\n[1] Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. Tongzhou Wang, Phillip Isola 2022"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}