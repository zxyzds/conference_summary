{
    "id": "X8aFMdXk3N",
    "title": "Ensuring Fair Comparisons in Time Series Forecasting: Addressing Quality Issues in Three Benchmark Datasets",
    "abstract": "Time series forecasting (TSF) is critical in numerous applications; however, unlike other AI domains where benchmark datasets are meticulously standardized, TSF datasets often suffer from data inconsistencies, missing values, and improper temporal splits. These issues have an impact on model performance and evaluation. This paper addresses these challenges by proposing inconsistency-free versions of three well-known TSF datasets. Our methodology involves identifying and correcting data inconsistencies using a combination of linear interpolation and context-aware imputation strategies. Additionally, we introduce a novel cycle-inclusive data splitting method, which respects the longest cycle in each dataset, ensuring that models are evaluated over meaningful temporal patterns. Through extensive testing of multiple transformer-based models, we demonstrate that our revised datasets and cycle-inclusive splitting lead to more accurate and interpretable forecasting results, as well as fairer comparison of TSF models. Finally, our findings highlight the need for proper dataset refinement and tailored data splitting strategies in TSF tasks, and pave the way for future work in the development of more robust forecasting benchmarks.",
    "keywords": [
        "Time Series; Dataset Quality; Fair Comparisons; Benchmark Datasets"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "Current Time Series Forecasting Datasets suffers from data inconsistencies, we propose cleaner version of three datasets in order to ensure fairer comparisons.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=X8aFMdXk3N",
    "pdf_link": "https://openreview.net/pdf?id=X8aFMdXk3N",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes three new benchmark datasets for time series forecasting. The proposed datasets aim to address data inconsistencies, missing values and improper temporal splits, facilitating fair evaluations for methods designed for forecasting task."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1)  Creating comprehensive and accurate benchmark dataset, especially for time series analysis, which is of high real-world importance is a critical and important task.\n\n2) The paper conducts extensive experiments"
            },
            "weaknesses": {
                "value": "1) The contribution of the paper is incremental, as it applies existing methods to three established datasets to create new benchmarks. The limitations of current benchmark datasets are well-known, and various solutions have already been developed to address these issues.\n\n2) The paper lacks a systematic method for creating benchmark datasets, which limits its applicability. This raises the question: can the approach presented in the paper be applied to any existing dataset to establish it as a fair benchmark?\n\n3) As noted in the paper, the approach relies heavily on domain expert knowledge, which limits its broader applicability. Additionally, if domain knowledge is to play a significant role, it could also be used to design new benchmark datasets.\n\n4) Given the limited novelty and the real-world application of the paper, I believe the paper may be a better fit for a benchmark track."
            },
            "questions": {
                "value": "1) Can the approach presented in the paper be applied to any existing dataset to establish it as a fair benchmark?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper motivates the need for high quality datasets in benchmarks of Time Series Forecasting models, and through careful data analysis, provide cleaned versions of 3 datasets. By comparing existing models on the new datasets, they establish the importance of the dataset version on method relative performances."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1) Extensive data analysis is systematically presented in Appendix, analyzing seasonality in a multivariate context. This is precious for practitioners looking to analyze models on such datasets.\n2) The task of providing clean datasets to the forecasting community is both significant and difficult, as currently each new paper benchmarks previous work in its own fashion. Standardizing training-validation-test split via data analysis is valuable in such a context.\n3) Code, datasets, and figures are provided and well structured."
            },
            "weaknesses": {
                "value": "1) There is overall very few citations in introduction, section 2.1, section 2.3 and section 3. \nThe paper makes claims on the general approaches to data processing (eg line 147). Additionally, the paper tracks variants of the three datasets through different papers. Overall, the first four pages are about existing works, concerns and approaches, yet few citations and no paper collection methodology are presented.\n2) The main paper version is very vague on the proposed data correction methods. The protocol for FFT application was described in Appendix, but this would be better included in the main paper. Imputation on the other hand appears simplistic. It would be for the best if the paper could try or discuss other methods and why specifically linear interpolation and neighbourhood average should be chosen. Surveys: https://arxiv.org/abs/2402.04059, https://arxiv.org/abs/2011.11347\n3) Three different methods might not be enough to see strong changes on method ranking. Additionally, 3 runs is insufficient to provide confidence on results significance."
            },
            "questions": {
                "value": "Seeing the correlation analysis results, would the authors say that correlation analysis is useful to measure dataset quality? I could not come to a conclusion, but I did not spend much time comparing the original and corrected plots.\n\n(Appendix, 1870, \"we can observe\" and not \"we can observed\", this one caught my eye). As most of the paper/appendix is text without formulas, I suggest going through a spell and grammar checker.\n\nIs there any way to measure seasonal differences on the ECL dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a solution to address inconsistencies and unreasonable data splits in time series forecasting evaluation, facilitating more accurate assessments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "As far as I know, data inconsistency is a common issue. The use of imputation in this paper is reasonable. The paper provides comprehensive experiments comparing the original datasets with the revised ones."
            },
            "weaknesses": {
                "value": "1. **Generality of the problem:** The paper discusses inconsistencies and data splitting issues using only three datasets. Are these issues commonly observed across other datasets?  \n2. **Real-world significance:** While I agree that data inconsistency is relevant in practice, I disagree with the periodic splitting approach. As far as I know, most real-world time series applications [1-2] do not align with periodic splitting settings.  \n3. **Lack of related work discussion:** Many studies have discussed time series forecasting (TSF) benchmarking. At least, [3] also aim s to fair evaluation. The authors should discuss the difference in motivation and in evaluation results.  \n4. **Limited novelty and dataset contribution:** Overall, I have concerns about the novelty of this work. Imputation is a common preprocessing technique, and the rationale for periodic splitting requires further discussion. From a dataset perspective, only three datasets are processed, and the benchmarking uses very few methods.\n\n**References:**  \n[1] Dugas, Andrea Freyer, et al. \"Influenza forecasting with Google Flu Trends.\" *PLoS One* 8.2 (2013): e56176.  \n[2] Singh, Ritika, and Shashi Srivastava. \"Stock prediction using deep learning.\" *Multimedia Tools and Applications* 76 (2017): 18569-18584.  \n[3] Qiu, Xiangfei, et al. \"Tfb: Towards comprehensive and fair benchmarking of time series forecasting methods.\" VLDB 2024"
            },
            "questions": {
                "value": "Please check limitations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper advocates for standardizing multivariate time-series forecasting evaluation, by removing inconsistencies and missing values, and by splitting data with cycle-preserving ratios. It further manually analyzes and releases cleaned versions of three commonly used datasets, and compares three recent models (NLinear, Informer and iTransformer) on them."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper tackles an **important problem**, the problem of ensuring a reliable, reproducible and consistent evaluation of time-series models.\nThe contributions are clearly stated, and I agree that the time-series community would benefit from standardized pre-processing and evaluation."
            },
            "weaknesses": {
                "value": "**The paper falls short in its analysis** of the impact of lack of standardization, for the following reasons:\n1. It considers only three datasets, while evaluations are commonly carried out with many more datasets, as listed in Table 1 in the supplementary material. The same analysis and proprocessing should be carried out at least on the 10 most commonly used dataset for the work to have a real impact.\n2. The paper\u2019s evaluation of Section 6 does not highlight any worrying problems with using the publicly available versions of the datasets. For instance, model rankings do not change when comparing models on the old dataset versions and when comparing them on the newly proposed versions, and in general model's performance is not significantly impacted by the standardization. More strikingly, models look equally impacted by the changes which hinders the claim that current evaluation practices are not fair.\n3. The paper\u2019s evaluation of Section 6 does not provide new insights into model performance, e.g., their weaknesses and strengths wrt capturing cycles, multivariate relationships, robustness to noise, and other factors that can impact performance.\n4. Only three and very recent models are considered, while there exists an extensive literature on the time-series forecasting, see e.g., survey [1].\n\n**The proposed methodology is not original**, as it encompasses commonly used techniques, such as interpolation for missing value imputation and spectral analysis for cycle detection.\nIt is also heavily relies on expert knowledge and visual inspection, which hinders its applicability to new datasets.\n\n**The work is not well positioned with respect to existing literature on standardizing time-series forecasting evaluation**. There exist at least two works, and related libraries [3][4], that provide access to standardized model implementations and datasets to facilitate a reliable, reproducible and consistent evaluation. How does the paper differ and improve upon these works? \n\n\n[1] Dama, Fatoumata, and Christine Sinoquet. \"Time series analysis and modeling to forecast: A survey.\" arXiv preprint arXiv:2104.00164 (2021).\n[2] Wang, Yuxuan, et al. \"Deep time series models: A comprehensive survey and benchmark.\" arXiv preprint arXiv:2407.13278 (2024).\n[3] Alexandrov, Alexander, et al. \"Gluonts: Probabilistic and neural time series modeling in python.\" Journal of Machine Learning Research 21.116 (2020)."
            },
            "questions": {
                "value": "1. Two cleaned versions of the datasets are proposed. Which ones should the community use? In the effort of standardizing evaluation, it is confusing to release more than a single version.\n2. Cycle-preserving splits are motivated to ensure train, validation and test sets are representative of the data distribution. This however does not account for shifts, which also create discrepancies. How should they be handled? \n3. Releasing a library is a convenient way of ensuring that pre-processing and evaluation are automatically standardized. Is there a particular reason why the authors decided not to provide one?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}