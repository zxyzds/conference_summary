{
    "id": "ve5Omkxc13",
    "title": "Latent Trajectory: A New Framework for Actor-Critic Reinforcement Learning with Uncertainty Quantification",
    "abstract": "Uncertainty quantification for deep neural networks is crucial for building reliable modern AI models. This challenge is particularly pronounced in deep reinforcement learning, where agents continuously learn from their interactions with stochastic environments, and the uncertainty of the value function is a key concern for ensuring reliable and robust RL applications. The complexity increases in actor-critic methods, as the training process alternates between optimizing the actor and critic networks, whose optimization nature makes the uncertainty of the value function hard to be quantified. \nTo address this issue, we introduce a novel approach to RL training that conceptualizes transition trajectories as latent variables. Building on this framework, we propose an adaptive Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) algorithm for training deep actor-critic models. This new training method allows for the implicit integration of latent transition trajectories, resulting in a trajectory-independent training process. We provide theoretical guarantees for the convergence of our algorithm and offer empirical evidence showing improvements in both performance and robustness of the deep actor-critic model under our Latent Trajectory Framework (LTF). Furthermore, this framework enables accurate uncertainty quantification for the value function of the RL system, paving the way for more reliable and robust RL applications.",
    "keywords": [
        "Reinforcement learning",
        "Stochastic gradient MCMC",
        "Bayesian sampling",
        "Uncertainty quantification"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ve5Omkxc13",
    "pdf_link": "https://openreview.net/pdf?id=ve5Omkxc13",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces the latent trajectory framework (LTF) that implicitly models the uncertainty of Q-functions by drawing multiple samples of critic parameters, essentially forming a distribution over Q-values."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper provides theroretical justification for the convergence of the proposed method.\n- The insight of conditional independence between the critic parameters and past actor parameters given the current state trajectory is particularly interesting."
            },
            "weaknesses": {
                "value": "## Lack of related works and comparison\nIt is hard to position this paper in the context of prior work as it lacks a discussion of how this work relates to existing RL algorithms that model the uncertainty of Q-values. The paper should discuss how this work is different from distributional RL methods, which also model the uncertainty of Q-values. The paper should also discuss why SGMCMC is the suitable method for the problem at hand, in contrast to prior work. This made the paper particularly hard to understand as a reader.\n\n## The problem of uncertainty quantification is not well-motivated\nThe paper does not provide a clear motivation for why it is important to model the uncertainty of Q-values. It mentions \"accurately quantifying the uncertainty of the value function has been a critical concern for ensuring reliable and robust RL applications\", but does not provide any concrete examples of why this is the case or precisely in what scenarios this is important. Ideally, the paper should analyze the limitations of existing RL algorithms that do not model the uncertainty of Q-values and provide examples of scenarios where this leads to suboptimal performance, and how the proposed method addresses these limitations in experiments.\n\n## No results on PPO\nThe paper mentions that PPO suffers from severe miscalibration issues in both the actor and critic, but does not provide any results on PPO to demonstrate this.\n\n## Connecting uncertainty quantification to performance\nWhile the escape environment discusses the relationship of how LTF leads to better MSE of value functions, how this translates to better performance in the escape environment is not clear. Conversely, on other environments, the paper does not provide a clear analysis of how the uncertainty quantification of Q-values leads to better performance.\n\nIt is also not so clearly apparent that LT-A2C has smaller seed variability than A2C, as the paper claims. The performance difference is largely imperceptible in the plots, considering the confidence intervals."
            },
            "questions": {
                "value": "Can you build a connection to prior works in uncertainty quantification?\n\nHow would you make this paper more approachable to a wider audience? Currently, it requires knowing a lot of prior work to make sense of the motivation, algorithm, and convergence proofs. The paper should be largely self-sufficient when reading."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Uncertainty quantification of the value function is crucial for a robust reinforcement learning algorithm. This work considers the challenging actor-critic setting. The introduced latent trajectory framework (LTF) is built upon the adaptive Stochastic Gradient Markov chain Monte Carlo (SGMCMC) by treating the transition trajectory and the value function parameter as latent variables for policy optimization. The proposed method is theoretically proved to be able to converge under mild conditions. The experiments on indoor escape environments  and PyBullet environment show that the proposed method has better performance compared with baseline A2C algorithms."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Theoretical results: The proposed LTF is grounded by theoretical convergence results. The paper contains most of the proof details, and the logic in the writing is easy to follow. \n- Experiments: The experiments show multiple metrics for evaluating the performance of the proposed LTF. For instance, the KL and MSE can help evaluate the performance from different perspectives. The results in HalfCheetah show that the proposed method can effectively improve the performance compared with A2C."
            },
            "weaknesses": {
                "value": "- The proposed LTF only compares with the vanilla A2C method, while there have been many  AC-based methods proposed, such as [W1,W2]. I recognize that the proposed LTF introduces a new perspective by treating the value parameters as a latent variable, while the experiments are lacking. In particular, the experiments in Section 4.1 are conducted on rather new environments (by Liang et al.). The lack of comparison with other AC methods (e.g., in experiments and/or related works section) makes the results and performance gain less convincing. If the comparison is not possible or not necessary, please clarify the reasons. \n- The proposed LTF is largely based on SGMCMC algorithms that are proposed in Liang et al., 2022a; Deng et al.,2019, while the major contribution of the LTF is less clear. From my understanding, the main contribution is on the A2C settings, which pose unique challenges for uncertainty quantification. It will be very beneficial for the authors to explicitly state the key challenges of applying SGMCMC to A2C settings and how their approach addresses these challenges. \n\n\n\n\n[W1] Zhou et al. \"Natural actor-critic for robust reinforcement learning with function approximation.\" Advances in neural information processing systems 36 (2024).\n\n[W2] Wu, et al. \"Uncertainty weighted actor-critic for offline reinforcement learning.\" arXiv preprint arXiv:2105.08140 (2021)."
            },
            "questions": {
                "value": "- What is the parameter $\\delta_j$ in Eqn. 10?\n- What are the relation between $\\epsilon_{k,l}$ and $\\epsilon_{k} $ in Algorithm 1\n- How to select $\\mathcal{N}$ in practice?\n- Why the variance in Figure 7 for LT-A2C is not significantly reduced if the uncertainty is effectively evaluated during update (e.g., HopperBullet, Evaluation reward)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a novel method for characterising uncertainty in value functions using a stochastic gradient MCMC algorithm. They carry out a convergence analysis of their method before evaluating PyBullet and Gridworld environments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The theoretical analysis of the proposed algorithm seems sounds from a cursory readying. Convergence guarantees are always welcome in papers."
            },
            "weaknesses": {
                "value": "My main concern relates to lack of positioning of the paper. Bayesian RL offers a precise way to characterise uncertainty in an MDP. At every timestep, the posterior over uncertain variables updates according to a Bayesian Bellman operator. Uncertainty can be characterised in the state-reward transitions as in model-based approaches or other sufficient variables like value functions or Bellman operators as in model-free approaches[4].  There is already a wealth of literature in uncertainty quantification in value functions. See my question below to authors.\n\nThe authors claim `Notably, uncertainty quantification for value-functions is generally beyond the reach of conventional iterative optimization algorithms used to train actor-critic models'. This is not true. Methods such optimistic actor-critic [1], BBAC[2] and EVE[3] (to name but a few) have been able to quantify uncertainty in value functions when used in continuous control for some time now as uncertainty quantification is essential to their exploration methods. There also exist analyses of the various approximate inference tools used to quantify uncertainty [5]. See [6] for a recent comparison of state of the art continuous control using uncertainty quantification evaluated in a variety of domains. \n\nEmpirical weaknesses:\n\nThere is a significant lack of comparison to other methods that quantify uncertainty in value functions. A comparison of these methods seems essential to evaluate the contribution of the proposed method. Moreover, the authors don't indicate number of timesteps in their evaluations so it is difficult to gauge the worth of their approach in comparison to similar Bayesian methods. \n\n[1] Coisek et al., Better Exploration with Optimistic Actor-Critic, 2019, https://arxiv.org/pdf/1910.12807\n[2] Fellows et al., Bayesian Bellman Operators, 2023, https://arxiv.org/pdf/2106.05012\n[3] Schmitt et al., Exploration via Epistemic Value Estimation, 2023, https://arxiv.org/pdf/2303.04012\n[4] Fellows et al., Bayesian Exploration Networks, 2024 https://arxiv.org/pdf/2308.13049\n[5] Coisek et al., Conservative Uncertainty Estimation By Fitting Prior Networks, 2020 https://openreview.net/pdf?id=BJlahxHYDS\n[6]  Tasdighi et al., Deep Exploration with PAC-Bayes, 2025, https://arxiv.org/pdf/2402.03055"
            },
            "questions": {
                "value": "How does the proposed method fit into the general model-free Bayesian RL framework that precisely characterises uncertainty in value functions? How does $\\pi_\\mathcal{N}(\\psi\\vert \\theta)$ relate to the posterior over $g_\\psi$ under these frameworks? If it differs from existing characterisations, ie [1]-[4], what theoretical, algorithmic or empirical advantages does it offer over a full Bayesian approach for characterising uncertainty in value functions? \n\nCan the authors approach be used to derived Bayes-optimal policies? If not, why does their uncertainty quantification prevent this? \n\nCan the authors extend their empirical evaluation to include other methods that quantify uncertainty in their value functions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a Latent Trajectory Framework (LTF) to improve uncertainty quantification in deep actor-critic reinforcement learning, addressing the challenge of value function uncertainty in stochastic environments. Using an adaptive Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) algorithm, the method enables trajectory-independent training, backed by theoretical convergence guarantees and empirical performance improvements. This approach enhances both the robustness and reliability of RL applications by integrating latent transition trajectories."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Theoretical Analysis: The paper provides a theoretical analysis of the Latent Trajectory Framework, attempting to establish convergence and benefits for uncertainty quantification in RL."
            },
            "weaknesses": {
                "value": "**$.1**\n- L47: Why is the actor considered \"unknown\"? We can access its weights during training, so we should be able to evaluate it on any state-action pair. Even if the actor were unknown, how does that affect uncertainty quantification?\nGeneral: Could you clarify the type of uncertainty you\u2019re addressing?\n\n**$.2**\n- L100: What is \u03c0(x\u2223\u03b8)? I thought \u03c0 represented the policy, a distribution over the action space.\n- L103: What is \u03c0(\u03c8\u2223\u03b8)?\n- L105: What does \"pseudo-population size\" mean? Is N not equal to the batch size n?\n- L107: Similar to above, this line is unclear.\n- Eq(3): Could you provide the intuition behind this learning objective?\n\n**$.3**\n- How does this approach differ from the SGMCMC method discussed in Shih & Liang (2024)?\n\n**$.4**\n- $4.1: The writing lacks organization. For instance, metrics are introduced at L323, but the computation details are only explained two paragraphs later. Why is coverage rate the chosen metric for uncertainty quantification?\n- $4.2: Since actor-critic methods are typically used for continuous action spaces, why not use Mujoco benchmarks for Fig. 7? Additionally, can LT be extended to SAC or other recent methods?\n\n**Overall**\n- This paper heavily relies on prior work for explanations and notations, which makes it challenging for readers unfamiliar with the domain to follow."
            },
            "questions": {
                "value": "none"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}