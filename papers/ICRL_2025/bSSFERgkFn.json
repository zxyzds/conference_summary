{
    "id": "bSSFERgkFn",
    "title": "Counterfactual Techniques for Enhancing Customer Retention",
    "abstract": "In this paper, we introduce a novel counterfactual reasoning method using eBERT embeddings to convert customers from an e-commerce company who frequently add items to their cart but don\u2019t proceed to checkout. We demonstrate that our method i) outperforms existing techniques such as DiCE, GANs, and CFRL in key metrics such as coverage, while also maintaining a low latency; ii) balances high coverage and low latency by adjusting the number of nearest unlike neighbors, highlighting a trade-off between these competing goals; and iii) allows customization of  mutable features, improving the practical applicability of our counterfactual explanations.",
    "keywords": [
        "Counterfactual Explanations",
        "BERT",
        "e-commerce"
    ],
    "primary_area": "causal reasoning",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=bSSFERgkFn",
    "pdf_link": "https://openreview.net/pdf?id=bSSFERgkFn",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a framework to generate counterfactual explanations for e-commerce applications. The proposed method aims to generate counterfactual explanations with low latency while maintaining high coverage."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper has an interesting setting; \n\nthe application targeted in this paper (e-commerce) is novel."
            },
            "weaknesses": {
                "value": "1) The paper\u2019s contribution is highly incremental, primarily building on the NICE framework. The only difference is the use of an embedding space, which is also generated by an existing method (eBERT).\n\n2) The paper claims that the proposed method has low latency; however, it is unclear why this is the case. The paper needs to provide more detail on this aspect, especially since low latency is presented as one of its main contributions.\n\n3)  The paper is not well-structured and contains several instances of incorrect and imprecise language. For example, contrary to what is stated in the introduction, counterfactual reasoning aims to answer \"what would have happened\" questions, not \"what if\" questions. Additionally, I would like to point out that the framework is not actually designed for counterfactual reasoning, as claimed; instead, it is intended for generating counterfactual explanations, which is a different focus."
            },
            "questions": {
                "value": "1) How does the proposed method addresses the latency issue? How does does it ensure low latency?\n\n2) What is the difference between the proposed framework and methods designed for causal algorithmic recourse?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new counterfactual reasoning method using eBERT embedding. By converting data into embeddings and identifying nearest unlike neighbors, this approach outperforms existing methods like DiCE and GANs in their study for an e-commerce company."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "A strength of the paper is its use of BERT embeddings to capture the semantic meaning of customer features, allowing for more nuanced and contextually accurate counterfactuals. This captures not just the value but the broader context and meaning of each feature within customer behavior."
            },
            "weaknesses": {
                "value": "A key weakness of the paper is its reliance on a proprietary dataset, with no access to the data or code, which limits reproducibility. Benchmarking on widely used datasets\u2014such as the Adult Income [Dua and Graff, 2017], FICO [Holter et al., 2018], and German Credit [Dua and Graff, 2017] datasets\u2014would allow for direct comparison with existing methods and validate performance claims in real-world scenarios. Without standard datasets and shared code, it\u2019s difficult for others to verify or build upon the findings, reducing the transparency and impact of the research.\n\nAnother limitation of the current implementation is its reliance on discretizing numerical features (e.g., categorizing PRICE and SHIPPING FEES into four broad buckets), which can lead to a loss of information. This simplification creates a trade-off between model complexity and the quality of the counterfactual explanations, which calls for more rigorous analysis. For example, with PRICE categories set in ~$20 increments, the buckets may be too wide to offer precise, actionable insights. \n\n\n\nReference\n- Dua, D. and Graff, C. (2017). UCI Machine Learning Repository.\n\n- Holter, S., Gomez, O., and Bertini, E. (2018). FICO Explainable Machine Learning Challenge."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a counterfactual reasoning method tailored to e-commerce with the goal of improving customer retention. It employs eBERT embeddings to identify \u201cnearest unlike neighbours\u201d optimising counterfactuals for actionability and speed by supporting feature mutability and balancing between latency, coverage, and customisability. The evaluation includes metrics such as coverage, reconstruction error, and latency, framing the technique as viable for real-time applications."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The flexibility to customise mutable features enhances the tool\u2019s practicality, allowing for more realistic recommendations tailored to business needs.\n- With a reported latency of 0.49s per counterfactual, the method aligns well with real-time applications\n- Employing eBERT embeddings to capture domain-specific semantics is a useful approach that could improve the plausibility of counterfactuals"
            },
            "weaknesses": {
                "value": "- While eBERT is reasonable for this context, its use is not critically evaluated against other potential embedding methods, such as Sentence-BERT (Reimers & Gurevych, 2019) or domain-adaptive pre-training models, which might yield similar or better results depending on the dataset specifics.\n\n- The paper\u2019s comparison omits several recent advancements, such as FACE (Poyiadzi et al., 2020) and CEM (Dhurandhar et al., 2018), that directly address challenges in feature mutability and interpretability. Including these would provide a more complete and relevant comparison of performance and mutability capabilities.\n\n- Certain parameters, particularly the nearest unlike neighbours k, are minimally justified. A benefit to this work would be providing a clearer rationale for parameter choices.\n\n- The greedy heuristic search for counterfactual generation is insufficiently explained, it lacks theoretical backing and reproducibility. Comparisons to alternative optimisation techniques, like those used in MACE, would contextualise this approach.\n\n- Evaluations focus solely on coverage, reconstruction error and latency, missing aspects critical for practical application such as interpretability and user-facing plausibility. A deeper study, similar to that in the DiCE paper, would provide stronger validation.\n\n- Despite claims of real-time suitability, no real-world testing or industry feedback is provided\n\n- Certain sections, particularly data processing, are challenging to follow and inadequately detailed for reproducibility. The writing quality is inconsistent, which hinders clarity.\n\n- Algorithm 1 occupies excessive space without proportional content, and could likely be condensed to half a page or less, enabling a more concise and readable presentation.\n\n- The results do not convincingly demonstrate a significant improvement over existing techniques. The reported gains seem marginal, and there is limited discussion on the implications of these small differences.\n\n- The study feels incomplete, lacking essential implementation details and more in-depth discussions on the method\u2019s performance and limitations.\n\nMinor things:\n\n- It appears the paper doesn't use the ICLR citation formatting, e.g. \\citep{} \n\n- The caption for Table 1 lacks clarity, providing no insight into the evaluation metrics or comparative values. Clearer captions would improve readability."
            },
            "questions": {
                "value": "- What guided the selection of  k  (nearest unlike neighbours)?\n\n- Why was eBERT chosen over other embedding methods like Sentence-BERT or OpenAI embeddings? How would other embeddings have affected the results or scalability?\n\n- Is there any comparative reasoning for why greedy heuristic was chosen over other optimisation methods?\n\n- Has the proposed method been piloted or tested in a live e-commerce setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel counterfactual reasoning method for enhancing customer retention in e-commerce, using eBERT embeddings to generate more plausible and actionable counterfactuals. \n\nKey contributions include:\n\nDeveloping a new counterfactual reasoning algorithm that improves upon state-of-the-art methods like NICE, DiCE, and GAN-based approaches by balancing coverage, latency, and plausibility.\n\nIntroducing an embedding-based approach using eBERT to generate highly plausible counterfactuals that better reflect customer behavior in e-commerce settings.\n\nEnsuring the method supports customizable and mutable features, allowing businesses to specify which factors can be realistically adjusted.\n \nOptimizing the system for real-time deployment, focusing on low latency and high scalability.\n\nEvaluating the effectiveness of the proposed method across various scenarios, comparing it against existing techniques using metrics like coverage, reconstruction error, and L1 distance.\n\nThe experiments conducted on 200,000 shopping sessions showed that the proposed method outperforms existing counterfactual generation methods in terms of coverage, reconstruction error, and L1 distance, while maintaining lower latency (0.49 seconds per counterfactual). The method also demonstrated consistent performance when tested with limited sets of mutable features.\n\nThe paper highlights the importance of the hyperparameter k (number of nearest unlike neighbors) in balancing coverage and computational speed. Overall, the results show that this approach enhances plausibility, provides the best coverage, and outperforms other methods offering mutability across all evaluation metrics."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality:\n\nThe paper introduces a novel approach by combining eBERT embeddings with counterfactual reasoning, which is an innovative application of natural language processing techniques to e-commerce data analysis.\n\nThe method creatively extends the NICE algorithm by adding support for mutable features and using advanced embedding techniques, addressing limitations of existing approaches.\n\nQuality:\n\nThe research methodology is robust, with experiments conducted on a large dataset of 200,000 shopping sessions, providing a solid empirical foundation for the claims.\n\nThe comparative analysis against multiple state-of-the-art methods (DiCE, GANs, CFRL) across several metrics demonstrates thorough evaluation.\n\nThe exploration of the hyperparameter k and its impact on coverage and latency shows depth in understanding the trade-offs involved.\n\nClarity:\n\nThe paper is well-structured, with clear explanations of the methodology, algorithms, and experimental results.\nThe use of figures (e.g., Figure 1 illustrating nearest unlike neighbors, Figure 2 showing coverage and latency variations) effectively supports the textual explanations.\nTables presenting comparative results (Tables 2 and 3) are clear and informative.\n\nSignificance:\n\nThe proposed method addresses a crucial challenge in e-commerce: improving customer retention through actionable insights.\nThe balance achieved between coverage, latency, and plausibility makes the approach highly relevant for real-world applications.\nThe support for mutable features enhances the practical applicability of the method, allowing businesses to focus on actionable changes.\nThe low latency (0.49 seconds per counterfactual) demonstrates the method's potential for real-time deployment in production systems.\n\nAdditional Strengths:\n\nInterdisciplinary approach: The paper effectively combines techniques from natural language processing (eBERT), machine learning (counterfactual reasoning), and e-commerce, showcasing a valuable interdisciplinary approach.\n\nScalability: The method's ability to handle a large dataset and maintain low latency indicates good scalability, which is crucial for real-world e-commerce applications.\n\nCustomizability: The support for specifying mutable features allows for tailoring the counterfactual generation to specific business needs and constraints.\n\nComprehensive evaluation: The paper provides a thorough evaluation across multiple metrics (reconstruction error, L1 distance, latency, coverage) and compares against several baseline methods, giving a holistic view of the method's performance.\n\nPractical implications: The focus on generating actionable counterfactuals for customer retention has direct practical implications for e-commerce businesses, bridging the gap between academic research and industry application."
            },
            "weaknesses": {
                "value": "Paper's weaknesses:\n\nLimited theoretical foundation:\nThe paper could benefit from a more rigorous theoretical analysis of why eBERT embeddings are particularly suitable for this task. While the empirical results are promising, a deeper exploration of the theoretical underpinnings would strengthen the contribution. For instance, the authors could discuss how the semantic relationships captured by eBERT relate to the concept of counterfactuals in e-commerce data.\n\nLack of ablation studies:\nThe paper would be stronger if it included ablation studies to isolate the impact of different components of the proposed method. For example, comparing the performance with and without the eBERT embeddings would help quantify their specific contribution to the overall improvement.\n\nLimited discussion on feature importance:\nWhile the method allows for specifying mutable features, there's little discussion on how to determine which features are most important or impactful for generating meaningful counterfactuals. A feature importance analysis could provide valuable insights for practitioners implementing this method.\n\nGeneralizability concerns:\nThe experiments are conducted on a single, albeit large, e-commerce dataset. The paper would be stronger if it included tests on multiple datasets from different e-commerce domains to demonstrate the method's generalizability. This could address potential concerns about overfitting to the specific characteristics of the dataset used.\n\nLack of user studies:\nGiven that the goal is to provide actionable insights for customer retention, the paper would benefit from user studies or expert evaluations to assess the practical utility and interpretability of the generated counterfactuals. This would provide valuable real-world validation beyond the quantitative metrics.\n\nLimited exploration of edge cases:\nThe paper focuses on average-case performance but doesn't thoroughly explore edge cases or potential failure modes of the proposed method. A discussion of scenarios where the method might underperform would provide a more balanced view and guide future improvements.\n\nInsufficient comparison with recent techniques:\nWhile the paper compares against several baseline methods, it misses comparison with some recent advancements in the field. For instance, comparing with methods like MACE (Model-Agnostic Counterfactual Explanations) or GeCo (Generative Counterfactual Method) would provide a more comprehensive evaluation.\n\nLimited discussion on computational resources:\nWhile latency is discussed, there's limited information on the computational resources required for training and deploying the model. This information would be valuable for practitioners considering implementing this method at scale.\n\nLack of discussion on ethical implications:\nGiven the potential impact on customer behavior, a discussion on the ethical implications of using such a system for influencing purchasing decisions would be valuable. This could include considerations of privacy and the potential for manipulation.\n\nInsufficient details on eBERT fine-tuning:\nThe paper mentions using eBERT fine-tuned on the company's product titles but doesn't provide details on this process. More information on the fine-tuning procedure, dataset size, and any challenges encountered would be helpful for reproducibility.\n\nAddressing these weaknesses would significantly strengthen the paper, providing a more comprehensive and robust contribution to the field of counterfactual reasoning in e-commerce."
            },
            "questions": {
                "value": "Could you provide more details on the eBERT fine-tuning process? Specifically:\n\nWhat was the size and composition of the dataset used for fine-tuning?\nWhat were the hyperparameters and training duration?\nDid you encounter any challenges during fine-tuning, and if so, how were they addressed?\n\n\nHave you conducted any ablation studies to isolate the impact of eBERT embeddings? It would be informative to see a comparison of your method with and without these embeddings to quantify their specific contribution.\n\nThe paper focuses on a single e-commerce dataset. Have you tested the method on datasets from different e-commerce domains or non-e-commerce datasets? This would help demonstrate the generalizability of your approach.\n\nCould you elaborate on how to determine which features are most important or impactful for generating meaningful counterfactuals? A feature importance analysis could provide valuable insights for practitioners.\n\nHave you considered conducting user studies or expert evaluations to assess the practical utility and interpretability of the generated counterfactuals? This could provide valuable real-world validation beyond the quantitative metrics.\n\nThe paper doesn't discuss potential failure modes or edge cases. Could you provide insights into scenarios where your method might underperform?\n\nYour comparison doesn't include some recent techniques like MACE or GeCo. Have you considered comparing your method against these approaches? If so, what were the results?\n\nCould you provide more information on the computational resources required for training and deploying your model at scale? This would be valuable for practitioners considering implementation.\n\nGiven the potential impact on customer behavior, have you considered the ethical implications of using such a system for influencing purchasing decisions? Could you discuss any privacy concerns or potential for manipulation?\n\nThe hyperparameter k (number of nearest unlike neighbors) seems crucial for balancing coverage and latency. Could you provide more insights into how to optimally select k for different scenarios or datasets?\n\nYour method achieves a good balance between coverage, latency, and plausibility. Could you elaborate on the trade-offs involved in achieving this balance? Were there any unexpected challenges or insights during this optimization process?\n\nThe paper mentions that future work will add functionality to specify a range of values for each feature. Could you provide more details on how you envision this implementation and its potential impact on the method's performance and usability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "While the paper presents valuable research on improving e-commerce customer retention through counterfactual reasoning, there are several privacy and safety considerations that warrant further examination:\n\nData privacy: The study uses a large dataset of 200,000 shopping sessions, which likely contains sensitive personal information. The paper doesn't provide details on data anonymization or protection measures. There's no mention of whether customer consent was obtained for using this data for research purposes.\n\nPotential for manipulation: The goal of generating counterfactuals to influence customer behavior, while potentially beneficial for businesses, raises ethical concerns about consumer manipulation. The paper doesn't address the fine line between helpful recommendations and potentially exploitative practices.\n\nLack of bias discussion: The paper doesn't explore potential biases in the data or model that could lead to unfair treatment of certain customer groups. This is particularly important given the use of machine learning techniques on consumer data.\n\nAbsence of ethical guidelines: There's no discussion on ethical guidelines for implementing this technology in real-world e-commerce systems. Given its potential to influence consumer behavior, such guidelines are crucial.\n\nSecurity considerations: While the paper focuses on improving customer retention, the same techniques could potentially be misused for more nefarious purposes if the system falls into the wrong hands. The paper doesn't address security measures to prevent misuse.\n\nTransparency to consumers: There's no mention of how this system would be disclosed to consumers. Customers may not be aware that their behavior is being analyzed to this degree to influence their future actions.\n\nLong-term societal impact: The paper doesn't consider the broader societal implications of widespread adoption of such technologies in e-commerce. There could be unforeseen consequences on consumer behavior and market dynamics."
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}