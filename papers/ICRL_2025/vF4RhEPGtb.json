{
    "id": "vF4RhEPGtb",
    "title": "Typography Leads Semantic Diversifying: Amplifying Adversarial Transferability across Multimodal Large Language Models",
    "abstract": "Recently, Multimodal Large Language Models (MLLMs) achieve remarkable performance in numerous zero-shot tasks due to their outstanding cross-modal interaction and comprehension abilities. However, MLLMs are found to still be vulnerable to human-imperceptible adversarial examples. In the exploration of security vulnerabilities in real-world scenarios, transferability, which can achieve cross-model impact, is considered the greatest threat posed by adversarial examples. However, there is currently no systematic research on the threat of cross-MLLMs adversarial transferability. Therefore, this paper as the first step to provide a comprehensive evaluation of the transferability of adversarial examples generated by various MLLMs. Furthermore, leveraging two key factors that influence transferability performance: 1) The strength of information diversity involved in the adversarial generation process; 2) Editing across vision-language modality information. We propose a boosting method called Typography Augment Transferability Method (TATM) to investigate the adversarial transferability performance across MLLMs further. Through extensive experimental validation, our TATM demonstrates exceptional performance in real-world applications of \"Harmful Word Insertion\" and \"Important Information Protection.\"",
    "keywords": [
        "Adversarial Transferability; Multimodal Large Language Models; Data Augmentation"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vF4RhEPGtb",
    "pdf_link": "https://openreview.net/pdf?id=vF4RhEPGtb",
    "comments": [
        {
            "summary": {
                "value": "The paper investigate the threat of cross-MLLMs adversarial transferability. The paper proposes a boosting method, TATM, leveraging the strength of information diversity involved in the adversarial generation process and editing across the modality information."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper provides comprehensive evaluation on various surrogate model architectures, such as BLIP2, InstructBLIP, LLaVA, MiniGPT4. \n\n2. Provides different attack baselines, which makes the evaluation stronger. \n\n3. The paper is easy to understand."
            },
            "weaknesses": {
                "value": "1. Does the proposed attack method resistant to the defense mechanism? Are there any analysis or the defense baselines evaluation?\n\n2. The selected tasks are generation-based tasks. Will the attack also works on the classification tasks?"
            },
            "questions": {
                "value": "Please check the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the transferability of adversarial examples across MLLMs. Although MLLMs excel in cross-modal interaction and comprehension, they remain vulnerable to transferable adversarial attacks, posing significant real-world risks. Leveraging two key factors\u2014information diversity in adversarial generation and cross-modal editing\u2014the authors propose the Typography Augment Transferability Method (TATM), which enhances adversarial transferability across MLLMs. Experiments demonstrate TATM\u2019s effectiveness in applications such as \"Harmful Word Insertion\" and \"Important Information Protection."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. the authors propose an innovative approach to enhance transferability by leveraging cross-modal data augmentation and semantic diversification through typography-based adversarial examples.\n2. The introduction of the Multi-semantic Angular Deviation Score (MADs) as a metric for quantifying information diversity also reflects the technical rigor of the study."
            },
            "weaknesses": {
                "value": "1. There is a lack of in-depth analysis on the transferability of adversarial examples. For instance, to further understand how the targeted adversarial example influences response generation, the authors can compute the relevancy score of image patches related to the input question using GradCAM to obtain a visual explanation for both clean and adversarial images.\n2. Reference missing, such as \"On Evaluating Adversarial Robustness of Large Vision-Language Models\""
            },
            "questions": {
                "value": "1. Although the authors have validated the proposed method on multiple models, the experiments are limited to fixed-size models and scenarios. Expanding the diversity of experiments is recommended, such as testing larger models like Qwen2-VL-8B, CogVLM-17B, and Yi-VL-34B, to further assess the method's effectiveness.\n\n2. What is the impact of the perturbation budget on the transferability of adversarial examples in applications like \"Harmful Word Insertion\" when evaluated on larger models? Generally, larger MLLMs exhibit more robust visual understanding.\n\n3. \"On Evaluating Adversarial Robustness of Large Vision-Language Models\" is likely the first work investigating the adversarial robustness of MLLMs. Could this method be used as a baseline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates vulnerabilities in Multimodal Large Language Models to transferable adversarial attacks. The authors introduce the Typography Augment Transferability Method (TATM), which uses typographic augmentation and cross-modal editing to enhance adversarial transferability across models. TATM proved highly effective in tasks like harmful word insertion and information protection."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The transferability of adversarial attacks across MLLMs is a timely and essential area of study, especially as MLLMs are increasingly integrated into commercial products.\n\n- Comprehensive experiments involving a substantial number of models have been conducted, which is critical for advancing research in transferability studies."
            },
            "weaknesses": {
                "value": "- The paper requires significant proofreading; several errors disrupt readability and make the reading process frustrating.\nThe Abstract contains grammatical issues, for example: \"Therefore, this paper as the first step to ...\" should likely read \"this paper serves as the ...\". Also, \"Furthermore, leveraging two key factors that influence transferability performance: 1) The strength of information diversity involved in the adversarial generation process; 2) Editing across vision-language modality information. We propose a boosting method ...\" lacks fluency.\nIn the Introduction, citations, particularly those listed in groups, should be enclosed in brackets for clarity.\nIn the Background section, adequate spacing is needed between method names and author names, e.g., \"Projected Gradient Descent (PGD)Madry et al. (2017)\" should read \"Projected Gradient Descent (PGD) Madry et al. (2017).\"\n\n- Numerous acronyms and newly introduced terms make the paper challenging to follow.\n\n- Transferability studies should ideally test transferability on black-box production models, such as GPT-4, Gemini, and real deployed systems.\n\n- The method\u2019s dependence on typographic augmentation may restrict its applicability to certain scenarios or datasets. Exploring other forms of semantic augmentation could improve its generalizability and broaden potential applications."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper does not appear to address ethical considerations, particularly given the focus on transferability, which could have real-world implications for production models."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the security vulnerabilities of Multimodal Large Language Models (MLLMs), with a focus on the transferability of adversarial examples. The authors introduce the Multi-semantic Angular Deviation Score (MADS), a quantitative metric for analyzing the adversarial transferability of different image samples. Additionally, they propose the Typography Augment Transferability Method (TATM), which enhances adversarial transferability by leveraging information diversity and cross-modal editing. Through experiments, the authors demonstrate the effectiveness of TATM in improving adversarial transferability."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The strengths of this paper include:\n- The authors' proposal of a white-box attack method to enhance the transferability of adversarial examples. They conducted experiments on various tasks, such as \"Harmful Word Insertion\" and \"Important Information Protection,\" thoroughly analyzing the performance of different methods across various MLLMs, including those with fixed vision encoders and cross-vision encoders. \n\n- The introduction of the Multi-semantic Angular Deviation Score (MADS) contributes to improving the interpretability of adversarial examples, offering a valuable tool for understanding their behavior."
            },
            "weaknesses": {
                "value": "The weaknesses of this work include:\n- The authors introduce two real-world applications: \"Harmful Word Insertion\" and \"Important Information Protection.\" However, the paper does not provide a clear explanation of the specific setups for these applications. Both Figure 1 and the accompanying text fail to clarify these aspects. Additionally, the authors do not justify the choice of \"Suicide\" and \"Unknown\" as target outputs, leaving their rationale unclear. Moreover, the explanations for the baseline methods, including DIM, BC, SIM, SIA, and TIM, are also insufficiently detailed.\n\n- The proposed adversarial example generation method is essentially a white-box attack, where the image is initialized using typographic words. However, the advantage of this initialization is not demonstrated. The authors neglect to provide ablation studies, such as comparing adversarial training using original image for initialization or image patch for initialization.\n\n- The authors' exploration of the different word types of typographic words embedded in images lacks clear justification. Intuitively, if the target output is \"suicide,\" the typographic words used in the images should be contextually relevant words or related images. Without such relevance, the significance of testing transferability across arbitrary word types becomes questionable.\n\n- The paper also suffers from numerous typos, indicating a need for significant improvement in writing quality. For instance:\n\n1. Line 124: \"Furthermore, data-augmentation methods Data augmentation has received more attention because of the ease and efficiency of implementation.\"\n2. Line 195: Multi-semantic Angular Deviation Score (MADS), and Line 206: \"we present the Mean Absolute Deviation Scores (MADS)\"\u2014the inconsistent use of different full forms for the same acronym is unacceptable.\n3. In Algorithm 1, there are multiple issues with inconsistent capitalization, symbol notation, and typographical errors, including \"TypoT.\""
            },
            "questions": {
                "value": "Refer to Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}