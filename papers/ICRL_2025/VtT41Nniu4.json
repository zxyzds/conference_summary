{
    "id": "VtT41Nniu4",
    "title": "GT-Mean Loss: A Simple Yet Effective Solution for Brightness Mismatch in Low-Light Image Enhancement",
    "abstract": "Low-light image enhancement (LLIE) aims to improve the visual quality of images captured under poor lighting conditions. In supervised LLIE tasks, there exists a significant yet often overlooked inconsistency between the overall brightness of an enhanced image and its ground truth counterpart, referred to as brightness mismatch in this study. Brightness mismatch negatively impact supervised LLIE models by misleading model training. However, this issue is largely neglected in current research. In this context, we propose the GT-mean loss, a simple yet effective loss function directly modeling the mean values of images from a probabilistic perspective. The GT-mean loss is flexible, as it extends existing supervised LLIE loss functions into the GT-mean form with minimal additional computational costs. Extensive experiments demonstrate that the incorporation of the GT-mean loss results in consistent performance improvements across various methods and datasets.",
    "keywords": [
        "Low-light image enhancement",
        "loss function",
        "GT-mean"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We propose GT-mean loss, a flexible loss function that can extend existing supervised LLIE loss functions into the GT-mean formulation, consistently improving model performance with minimal computational cost.",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=VtT41Nniu4",
    "pdf_link": "https://openreview.net/pdf?id=VtT41Nniu4",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces GT-Mean Loss, a novel loss function designed to address the issue of brightness mismatch in low-light image enhancement (LLIE). GT-Mean Loss aims to align the brightness of enhanced images with their ground truth counterparts by dynamically adjusting the loss during training, ensuring that the model focuses on factors beyond brightness mismatches."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Proposes a straightforward loss function that effectively mitigates brightness mismatches in LLIE.\n2. Demonstrates performance improvements across various LLIE models, supporting the generalizability of GT-Mean Loss.\n3. The approach adds minimal computational overhead, making it easy to integrate into existing models without significant resource costs."
            },
            "weaknesses": {
                "value": "The authors claim that \"brightness mismatch dominates the PSNR values,\" suggesting that brightness inconsistency heavily biases this commonly used metric, leading to inaccurate quality evaluations. However, this assertion may oversimplify the limitations of PSNR, as the issue presented seems to be a specific type of counterexample rather than an overarching flaw in PSNR itself. PSNR is limited primarily in its sensitivity to perceptual qualities rather than simply brightness mismatch, and as such, is inherently less reliable for subjective quality evaluation. It\u2019s possible to generate similar counterexamples through methods like the MAD competition, which reveal PSNR\u2019s broader limitations in capturing perceptual quality accurately. Further, rather than focusing only on brightness mismatch, utilizing perceptually-oriented image quality assessment (IQA) metrics, such as LPIPS and DISTS, would provide a more holistic and accurate quality differentiation. These metrics are specifically designed to capture perceptual differences that metrics like PSNR or SSIM may overlook. By comparing GT-Mean Loss to metrics like LPIPS and DISTS, the authors could strengthen the argument for their loss function's contribution to enhancing perceptual quality in LLIE tasks, rather than solely addressing the limited case of brightness mismatch."
            },
            "questions": {
                "value": "1. Although the paper presents GT-Mean PSNR and GT-Mean SSIM as enhanced metrics, the evaluation lacks comparison using perceptual quality metrics such as LPIPS, DISTS, Q-Align, and LIQE, which could better reflect subjective image quality.\n\n2. The method\u2019s focus on brightness mismatch as the sole training issue may overlook other complex image degradation factors in low-light settings, such as the noise, the color bias, the unaccurate white balance,  limiting the applicability of the method for more holistic quality enhancement.\n\n3. The performance improvements presented in the paper, while consistent, appear modest in scale. The GT-Mean Loss achieves incremental gains in metrics like PSNR and SSIM, but these enhancements are relatively small and may not justify the added complexity of implementing a new loss function focused on brightness alignment alone. For real-world applications, such minor improvements could be seen as insufficient.\n\n4. In addition to quantitative metrics, conducting a user study on the enhanced images could provide a more reliable and insightful demonstration of GT-Mean Loss\u2019s impact on perceptual quality. Objective metrics, especially pixel-based ones like PSNR and SSIM, do not fully capture human perception\n\n5. The captions in the paper\u2019s figures would benefit from additional context, allowing them to convey key insights independently of the main text. Currently, they lack sufficient detail to stand alone, which can make it challenging to grasp the full significance of the visual data without referring back to the text. \n\n6. The motivation section, particularly the second paragraph of the Introduction, would benefit from a clearer and more robust explanation. Currently, the rationale behind the proposed GT-Mean Loss is somewhat limited."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a GT-mean loss to address the brightness mismatch in low-light image enhancement. The authors first define the brightness mismatch and describe its impact on evaluation and training. Subsequently, the GT-mean loss and weight design are introduced. The evaluation is performed on several backbones with three low-light enhancement datasets. The GT-mean loss can consistently improve performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe proposed loss leads to consistent performance for several backbones on three low-light image enhancement datasets.\n2.\tThe paper is well-motivated and elaborately develops a loss function for the observation."
            },
            "weaknesses": {
                "value": "1.\tIn Fig.2, it seems more like a metric problem. If PSNR is not sensitive to this mismatch, why can the proposed loss function improve the PSNR performance? When there is a resolution difference between the original GT and the scaled image, how do we compute the PSNR value of the scaled one? Moreover, how is the brightness mismatch problem illustrated in Fig.2?\n2.\tThe paper couples the original loss and the introduced one using W. How do we confirm that these two loss functions have any relationships?\n3.\tTab. 1 shows that perceptual loss can also identify the scaled image as better. Can this loss make a more significant improvement than the proposed one when applied to algorithms that initially did not use this loss?\n4.\tSome typos. Lv1->LOLv1. The citation for ZeroDCE is missing."
            },
            "questions": {
                "value": "Please see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose to align overall brightness of the enhanced image with ground-truth images for supervision in low-light enhancement models. Experiments have shown that the proposed GT-mean loss results in consistent performance gain in PSNR."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed method is well-motivated.  The MSE/L1-based metrics, say PSNR or RMSE, do have a problem identifing noise from lightness bias in RGB color space.\n2. The experiment results are reasonable."
            },
            "weaknesses": {
                "value": "1. Novelty. The GT-mean metric was first proposed in KinD[1], and has been adopted by many previous methods[2,3]. I don't think the effectiveness of GT-mean **metric**, can be considered as a contribution.\n2. Formulation of global lightness. The author models the global brightness with normal distribution, which breaks in most real-world scenarios.\n\n[1] Zhang et. al, Kindling the Darkness: A Practical Low-light Image Enhancer, ACM MM 2019.\n\n[2] Wang et. al, Low-Light Image Enhancement with Normalizing Flow, AAAI 2022.\n\n[3]Cui et. al, Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement, ICCV 2023."
            },
            "questions": {
                "value": "1.  Estimating W with the average of two KL divergences is confusing. Why? More in-depth analysis should be conducted.\n2.  In Figure 3, the authors plotted average brightness. Is this average brightness distribution obtained from images? In my experiments, the brightness distribution of images (widely visualized as a histogram) doesn't always come with a perfect Gaussian distribution.\n3. With the proposed gt mean loss, the models are still aligning input with reference images. However, as an **enhancement** task, there exists no ground truth image. It is not surprising that gt-mean alignment helps align the output with reference images, and thus improves the performance on reference-based metrics, say, PSNR and SSIM. Please report the non-reference metrics MUSIQ[1] and Q-align[2] for DICE, MEF, LIME, NPE, and VV datasets.\n\n[1]Ke et. al,  MUSIQ: Multi-Scale Image Quality Transformer, ICCV 2021.\n\n[2] Wu et. al, Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels, ICML 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Low-light image enhancement (LLIE) aims to improve the visual quality of images captured under poor lighting conditions. In supervised LLIE tasks, there exists a significant yet often overlooked inconsistency between the overall brightness of an enhanced image and its ground truth counterpart, referred to as brightness mismatch in this study. Brightness mismatch negatively impact supervised LLIE models by misleading model training. However, this issue is largely neglected in current research. In this context, we propose the GT-mean loss, a simple yet effective loss function directly modeling the mean values of images from a probabilistic perspective. The GT-mean loss is flexible, as it extends existing supervised LLIE loss functions into the GT-mean form with minimal additional computational costs. Extensive experiments demonstrate that the incorporation of the GT-mean loss results in consistent performance improvements across various methods and datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper propose GT-mean loss, a simple yet effective loss function directly modeling the mean values of images from a probabilistic perspective."
            },
            "weaknesses": {
                "value": "* The authors in this paper define a new term, \u201cbrightness mismatch\u201d which refers to the \u201cinconsistency\u201d between enhanced image and the ground-truth image (lines  028-030). However, it is quite unclear as to how it impacts the model training as mentioned in lines 030-032. The authors are trying to imply that the outcome of the trained model affects the training of the model? It is somewhat difficult to follow. The same has several occurrences in the manuscript, but it is not quite clear how the brightness mismatch is affecting the model training. The authors are requested to provide clear and crisp information regarding the claims made. \n* Figure 2 and the explanation provided is somewhat misleading/not easy to understand. The \u201cscaled\u201d version of the GT image supposedly has \u201cinfinite\u201d GT-mean PSNR Values; this seems quite untrue. This brings to next discrepancy/confusion in the paper, the GT-mean PSNR. The manuscript seems to not have a said definition for GT-mean PSNR. The authors are requested to provide more information and clarity on this. There is no information as to how the GT-mean PSNR is computed to recreate the Figure 2, and also how the result of infinity is achieved. Is it for some kind of demonstration purpose? \n* Lines 086-094 are not quite clear. Several Methods in literature make use of non-PSNR like metrics such as perceptual loss [1], color fidelity losses [2] which somewhat aim at solving the \u201cbrightness mismatch\u201d issue as claimed by the authors. Towards this the authors are requested to provide adequate information for the claims made. \n* Lines 230-234 are extremely difficult to follow. For example., what is the meaning of \u201cAt this stage, the GT-mean loss value measures the fidelity between the two images by excluding brightness mismatch in advance considering brightness mismatch, thereby sustaining effective model training\u201d? The writing style and the information provided at this point makes it quite difficult to comprehend the manuscript. \n* The overall contribution of this paper is supposedly GT-mean \u201closs\u201d but the authors (in lines 122-125) describe it as \u201cextension\u201d to any existing loss function pushing the reader in the direction of if this is really a loss function or rather a regularization term. \n* The manuscript overall is rather difficult to follow and understand. \n* The qualitative results in section E show least/minimal differences between the baseline results and the proposed GT-mean version. There is no drastic change in the results shown and supposedly a non-issue due to the availability of losses like perceptual loss, color loss, cosine similarity loss for minimizing color deviations. The observed improvements seem to not fit for the level to claims made.\n\n\n\nReferences\n[1] Desai, Chaitra, et al. \"Lightnet: Generative model for enhancement of low-light images.\" Proceedings of the IEEE/CVF international conference on computer vision. 2023.\n[2] Ignatov, Andrey, et al. \"Dslr-quality photos on mobile devices with deep convolutional networks.\" Proceedings of the IEEE international conference on computer vision. 2017."
            },
            "questions": {
                "value": "Authors to consider major concerns mentioned."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a simple and effective GT-mean loss for low-light image enhancement. The GT-mean loss aims to align the average image pixel value intensities of the enhanced image to that of GT image. The authors provide an analysis from the point of view of probabilistic modeling. A design of the balancing weights between L1 loss and GT-enhance loss based on KL divergence is designed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "(a) The presentation of this paper is well-dressed. The writing is good and easy-to-follow. Although the proposed method is simple, but I found some insights in the weight design part. \n\n(b) The experimental results are good and solid. The authors have validated the effectiveness of the proposed GT-mean loss on seven methods on three datasets in both terms of normal enhanced results and GT-mean enhanced results. The Visual comparison also looks good. The ablation studies can demonstrate the effectiveness of the proposed techniques."
            },
            "weaknesses": {
                "value": "(a) The novelty is poor. The GT-mean enhancement is actually a cheat. It was first used by KinD [1]. However, KinD does not achieve very high results. Thus, this trick does not attract much attention. Later work LLflow [2] uses this trick to achieve a very high result on LOL-v1 dataset. Since then on, more and more methods used this trick to pursue better PSNR and SSIM. Although Retinexformer pointed out this is wrong and bad, it cannot stop the community from using this trick. From my point of view, using the average value of GT image pixels to achieve higher numeric results does not make sense. It is not essential. And this is just a small trick. I do not think this trick is worth an ICLR publication.\n\n[1] Kindling the Darkness: A Practical Low-light Image Enhancer. ACM MM 2019.\n\n[2] Low-Light Image Enhancement with Normalizing Flow. AAAI 2022.\n\n(b) The proposed GT-Mean Loss does not work in any case, especially for the out-of-distribution scenes. For example, in lines 1123 - 1129, \nthe original LLformer performs better without the GT-Mean Loss. This is because the correction of GT-Mean has a bias on the trained dataset. This is the main technical drawback of the proposed method.\n\n(c) Algorithm 1 is unnecessary since it is very simple. There is no need to draw the algorithm table.\n\n(d) Code and pre-trained weights are not submitted, the reproducibility cannot be checked."
            },
            "questions": {
                "value": "The proof in Appendix A is just the normal definition and formulation of the KL divergence. Why write it? it seems unnecessary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}