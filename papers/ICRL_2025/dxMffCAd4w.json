{
    "id": "dxMffCAd4w",
    "title": "CLF: Curve Line Fitting Neural Network Based On Bezier Curve",
    "abstract": "The Multilayer Perceptron (MLP) serves as a fundamental architecture in deep learning, leveraging the universal function approximation theorem through linear regression combined with activation functions. Despite its widespread use, the inclusion of activation functions contributes to the inherent nature of MLPs as ``black boxes,\" limiting their interpretability. In this paper, we propose a novel Curve Line Fitting (CLF) network, which introduces Bezier curve fitting to directly address nonlinear distributions. By replacing traditional linear regression with Bezier curve regression, the CLF network offers a more efficient means of fitting target distributions. Additionally, the removal of activation functions makes the CLF model fully interpretable, enabling clear insights into the relationships between input dimensions and target distributions, as well as the interdependencies across different dimensions. (Sample code for the CLF model will be made available on GitHub.)",
    "keywords": [
        "CLF",
        "MLP",
        "Interpretable Neural Networks"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "Propose a network, CLF, directly fits nonlinear relations, eliminating the need for activation functions.  It is fully explainable and capable of clearly demonstrating the relationships it learn.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=dxMffCAd4w",
    "pdf_link": "https://openreview.net/pdf?id=dxMffCAd4w",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes to use Bezier curve regression to replace the linear regression in a multi-layer perceptron (MLP), and also remove all activations, in order to realise (1) more efficiently fitting target distributions, and (2) \u201cinsights into the relationships between input dimensions and target distributions, as well as the interdependencies across different dimensions\u201d I quoted."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Pros:\n\n- The paper presents detailed introduction and explanation of the proposed method, including estimator of the label, optimisation, forward function, initialisation, training process for cases of single node, single layer, and multiple layers. The paper is compressive, well-structed, and easy to follow. \n\n- The paper present comprehensive experiments for cases of single node, single layer, and multiple layers, in terms of efficiency, capability, mapping from input to the distribution estimated, and interaction between different dimensions.\n\n- I find the paper makes conceptual contributions of proposing a new family of neural networks that could have advantages."
            },
            "weaknesses": {
                "value": "Cons:\n\n- \u201cExtraordinary claims require extraordinary evidence.\u201d Proposing a new family of neural networks needs strong evidence in their advantages with existing methods, in either theoretical or empirical aspect, or more ideally, in both. Unfortunately, this paper fails to persuade me they are better. The experiments are too simple \u2013 only comparing the method with MLP using MNIST. No theoretical analysis is provided. I would suggest including experiments on at least CIFAT and ImageNet, comparing with CNN, ResNet, and transformer. For theoretical studies, I would encourage analyse the generalisability, approximation, and stability of the proposed method. For example, does the hypothesis space of CLF has smaller approximation error than existing methods? Does the model leaned by CLF has a smaller generalisation bound?\n\n- The paper claims that the CLF is \u201cfully interpretable\u201d. I challenge this. First, I guess the authors meant to say \u201cexplainable\u201d rather than \u201cinterpretable\u201d. The two terms may have similar lay-man meanings, but in machine learning, \u201cinterpretable\u201d is usual for interpreting model mechanisms (such as the theory of deep learning), but \u201cexplainable\u201d is for explaining different data features\u2019 contributions. Second, this paper attributes the explainable issues of MLP to nonlinear activations, and thus CLF is fully explainable because it removes activations. This is groundless. To render a solid discussion, please (1) define what is explainable, (2) explain why activation make it unexplainable, and (3) show how to explain the results of CLF."
            },
            "questions": {
                "value": "Please address the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a Curve Line Fitting (CLF) network, which uses Bezier curve fitting to fit nonlinear relations compared to linear regression with activation functions in MLP. The authors claim that this CLF model is more interpretable than MLP."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- propose to replace traditional linear regression in MLP with Bezier curve regression, which can remove the using of activation functions and be more interpretable.\n- Some initial experiments are conducted to show the efficacy of the proposed method."
            },
            "weaknesses": {
                "value": "- The experiment is only performed on small synthetic datasets and MNIST. The test accuracy on MNIST is very low compared with current models that can achiever almost perfect accuracy.\n- I don't see how the proposed CLF is more interpretable than MLP. Instead of learning linear combinations of input in MLP, CLF is learning quadratic functions over each dimension and then sum over all dimensions. The nonlinear function of each dimension is itself not interpretable.\n- See some other question below"
            },
            "questions": {
                "value": "- The Bezier Curve is not introduced in the paper. I think the authors use the Quadratic B\u00e9zier curves? It would be better to have some introduction of Bezier Curve.\n- A single node CLF can only calculate quadratic functions of each dimension of the data and there is no interaction between different dimensions. Does it have enough expressive power to represent complex functions?\n- Line 211 $\\hat{Y}$ is not defined. Line 212, $Y \\in R^M$ and $\\hat{Y}_{all} \\in R^{M \\times N}$. How do you calculate $Y - \\hat{Y}\\_{all}$?\n- What is $Relation(i, j)$ and $Cov$ operation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The author(s) introduced a novel hierarchical method to fit the functions. In their main contribution, the authors emphasizes that their model  doesn't rely on the nonlinearity from the activation function as the conventional multilayer neural networks. Instead, they developed a sequential fitting algorithm with Bezier Curve. In each step, they tune the control points to minimizes the regression error. The author compares the performance of CLF and MLP in synthetic data and real world data."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper wraps up a sound theory by developing a sequential fitting algorithm with Bezier Curve. It details the construction of the Bezier Curve and make a systematic comparison between CLF and MLP in synthetic data and real world data."
            },
            "weaknesses": {
                "value": "The paper needs some improvements in writing, including \n1. The algorithm in section 2.1.6 should format in an algorithm setup. It also should include a short sentences for each of the abbreviations, like LR for learning rate and etc.\n2. Nonlinear distribution. The paper first mentions the terminology 'nonlinear distribution' in line 087 without correct reference or definition  for the term. Usually, distribution refers to the probability distribution, while in this paper it seems like 'nonlinear function class' would be a better description.\n3. The abuse of the notations. In line 120, the author(s) uses $P_1'$ to denote the derivative with respect to $P_1$, which is not rigorous. The variable should be a subscript conventionally. \nThe paper lacks of evidence why the sequential CLF is interested,\n1. CLF doesn't show theoretical soundness compared with MLP. The modern approximation theory for MLP can (partially) explain why and how the neural networks works, including the universal approximation theory, the sample complexity and etc.. The lack of fully developed theory is not a solid reason for developing a new method. Also, in a lot statistical problems, the performance of traditional methods are comparable with that of the neural networks. \n2. The results demonstrated in table 5 only indicates the comparable performance of CLF (or even worse). It lacks of the evidence when and why CLF is superior than MLP."
            },
            "questions": {
                "value": "I might miss something though I have several concerns about the paper,\n1. If by increasing the number of control points, CLF can achieve better regression results. Then what's the motivation to develop multiple layer CLF? Also, In traditional fitting problem, dividing the domain into subdomain and solve the same fitting problem on the subdomain correspondingly will yield a better regression, is it necessary to discuss the reason why choose to use hierarchical structure instead of the windowed fitting?\n2. Does CLF also have universal approximation theory? What's the convergence rate with respect to the number of control points. \n3. Can CLF generalize the results to more complicated function class, beyond the smooth functions? like the function with regularities.\n4. The proposed CLF seems like to address the interpretability difficulty of MLP, but what exactly the difficulty does CLF try to solve? like sample complexity or the convergence rate, or the meaning of the parameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work aims to learn and construct a nonlinear mapping using B\u00e9zier curves, departing from the traditional paradigm of neural network activation functions. The study is both novel and interesting. I encourage further exploration in this area; however, the overall research appears to lack rigor, and several issues remain unaddressed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The questions are both profound and challenging."
            },
            "weaknesses": {
                "value": "1. Several critical issues remain unaddressed.\n2. The experiment was insufficient and lacked rigor.\n3. Please refer to the questions."
            },
            "questions": {
                "value": "1.\tIn line 98, it states that \u201cthe input space X is evenly divided into five segments, governed by control points labeled A through G.\u201d However, I cannot find the corresponding five segments in Figure 1. Please clearly label or highlight the five segments in Figure 1.\n2.\tIn line 116, how is $P_{12x}$ defined?\n3.\tIs this approach susceptible to overfitting issues? How does CLF address overfitting in its implementation? Authors could add a specific section discussing overfitting in CLF, including any experiments or comparisons with traditional neural networks in terms of overfitting behavior.\n4.\tNeural networks can effectively manage the decoupling relationship between multiple features, eliminating the need for complex feature engineering. How are multiple feature inputs handled in this method? From the text's description, it seems to involve simple summation at the output. CLF does not appear to be well-equipped to handle multi-feature tasks efficiently. It is suggested that the authors provide a detailed explanation or illustration of how CLF handles multiple features, possibly with a comparison to how traditional neural networks manage this.\n5.\tIn section 2.3, a multi-layer CLF is mentioned. However, based on the description, this approach does not seem to be end-to-end. Such a method may not be effective in practical applications, particularly for constructing deeper networks. \n6.\tCan CLF be extended to classification tasks? How is the loss function defined for classification?\n7.\tI believe the operational complexity of this approach could be significant when dealing with high-dimensional inputs and outputs. For instance, in the MINST experiment mentioned in the paper, how does the running complexity of CLF compare to that of multilayer perceptron models? We suggest that the authors include a detailed complexity analysis section, comparing the time and space complexity of CLF to traditional neural networks for various input dimensions, particularly focusing on the MNIST experiment mentioned in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}