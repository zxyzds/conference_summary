{
    "id": "V7PYbRzD0h",
    "title": "Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step",
    "abstract": "Text-based image generation models, such as Stable Diffusion and DALL-E 3, hold significant potential in content creation and publishing workflows, making them the focus in recent years.\nDespite their remarkable capability to generate diverse and vivid images, considerable efforts are being made to prevent the generation of harmful content, such as abusive, violent, or pornographic material.\nTo assess the safety of existing models, we introduce a novel jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises image generation models through a step-by-step editing process.\nSpecifically, for malicious queries that cannot bypass the safeguards with a single prompt, we intentionally decompose the query into multiple sub-queries. The image generation models are then prompted to generate and iteratively edit images based on these sub-queries.\nTo evaluate the effectiveness of our CoJ attack method, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine safety scenarios, three types of editing operations, and three editing elements.\nExperiments on four widely-used image generation services provided by GPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack method can successfully bypass the safeguards of models for over 60\\% cases, which significantly outperforms other jailbreaking methods (i.e., 14\\%).\nFurther, to enhance these models' safety against our CoJ attack method, we also propose an effective prompting-based method, Think Twice Prompting, that can successfully defend over 95\\% of CoJ attack.\nWe will release our dataset and code to facilitate the AI safety research.",
    "keywords": [
        "Jailbreak Attack",
        "Text-to-Image Models",
        "Safety"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We introduce a novel Chain-of-Jailbreak (CoJ) attack method, which compromises image generation models through a step-by-step editing process.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=V7PYbRzD0h",
    "pdf_link": "https://openreview.net/pdf?id=V7PYbRzD0h",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces the \"Chain-of-Jailbreak\" (CoJ) attack, a method to bypass safeguards in text-based image generation models like GPT-4V and Gemini 1.5. By breaking down malicious queries into harmless sub-queries and using iterative editing, CoJ can generate harmful content that would otherwise be blocked. The study highlights significant vulnerabilities in current models, achieving a 60% success rate in bypassing safeguards. To counter this, the authors propose \"Think Twice Prompting,\" a defense strategy that prompts models to internally evaluate the safety of the content before generation, successfully defending against 95% of CoJ attacks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Innovative Methodology: The introduction of the Chain-of-Jailbreak (CoJ) attack is a significant advancement. By decomposing malicious queries into harmless sub-queries and using iterative editing, the paper presents a novel approach to bypassing safeguards in text-based image generation models.\n\nComprehensive Evaluation: The authors have conducted extensive experiments across multiple models (GPT-4V, GPT-4o, Gemini 1.5, and Gemini 1.5 Pro) and scenarios. This thorough evaluation demonstrates the robustness and effectiveness of the CoJ attack, achieving a high success rate of 60%.\n\nProposed Defense Mechanism: The paper doesn't just identify vulnerabilities but also proposes a practical solution. The \"Think Twice Prompting\" defense strategy, which prompts models to internally evaluate the safety of the content before generation, shows a high defense success rate of 95%."
            },
            "weaknesses": {
                "value": "Method Robustness: The authors propose using Edit Operations and Edit Elements to break down a malicious query into sub-queries. In the implementation, they manually apply this approach to five seed queries and leverage a large language model (LLM) to generalize these examples to other queries. However, my concern is whether the model consistently adheres to the principles of the proposed Edit Operations and Edit Elements. It would be helpful if the authors could elaborate on the reliability of this approach.\n\nNumber of Sub-Queries: As illustrated in Figure 1, the malicious query is divided into three sub-queries. This raises the question of how many sub-queries would be optimal for other queries. Is there a \u201cbest\u201d decomposition, and how can it be identified? While the authors rely on an LLM for this task, I am concerned about the LLM\u2019s ability to consistently find the optimal decomposition.\n\nChoice of LLM: The authors specify using Mistral-Large-2 for modifying malicious queries automatically. It would be informative to know whether other models were considered and if similar performance could be achieved with smaller models requiring less computational power. This consideration is especially relevant for attackers with limited resources who may not have access to high-powered computational hardware."
            },
            "questions": {
                "value": "1. How reliably does the model follow Edit Operations and Edit Elements across queries?\n\n2. What is the optimal number of sub-queries, and can the LLM consistently find it?\n\n3. Were other models tested, and can similar results be achieved with less computationally intensive options?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method to make text-to-image models generate images with harmful content. The harmful image is defined as an image with harmful words in it. The idea is to decompose the prompt into multiple sub-queries, which gradually generate a harmful image. A benchmark of queries for such harmful images are also collected and used for evaluation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Safety of text-to-image models is an important topic. \n\nA method is proposed to generate a specific type of harmful images. \n\nA benchmark is collected."
            },
            "weaknesses": {
                "value": "The scope is a little bit limited. Only images with harmful words can be generated by this method. This should be made clear from paper title and abstract. \n\nTo generate such images, more straightforward approach may be applicable. E.g., directly merge harmful words with images. Note that, an attacker jailbreaks a GenAI model to generate harmful images, and still needs to propagate them to cause real harms for other people. To generate the types of harmful images considered in this work, an attacker may not need a text-to-image model and thus may not need the proposed attack. \n\nComparison with baseline methods is missing. What are the alternative approaches to generate such harmful images? Does the attacker have to use a text-to-image model? Even if text-to-image model is needed, any other baseline methods can be used to generate such harmful images? It is not clear in the current paper."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel method called Chain-of-Jailbreak (CoJ) attack that bypasses safeguards in image generation models. The method proposes a decomposition method that breaks down malicious prompts into a series of sub-queries. The experiments demonstrate that such a method is able to jailbreak open-source black-box models like GPT-4V, GPT-4o, and Gemini 1.5 by generating harmful contents in various malicious categories. The authors also introduce a dataset named \u201cCoJ-Bench\u201d, which consists of various jailbreaking prompts in a wide range of safety scenarios. The paper claims to achieve high jailbreak success rates (~60%), which outperforms traditional prompt-based attacks. In addition, the authors introduce a defense mechanism against the proposed attack called \u201cThink Twice Prompting\u201d, which asks the model to check the safety implications once more before generating the images."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper proposes a novel yet simple approach to bypass image generation model safeguards, which are known to be built stronger than open-source models such as Stable Diffusion. It also demonstrated a huge outperformance against previous jailbreaking methods in text-to-image generation. \n\n2. The paper also introduces the edit operation in the decomposition process is structured systematically rather than simply tokenizing the input prompt. The diversity of the attack method showed the effectiveness of jailbreaking these models.\n\n3. The CoJ benchmark covers a wide range of safety scenarios, with an even more detailed division in the subgroups of each safety category of the benchmark, which enhances reproducibility and evaluation standards."
            },
            "weaknesses": {
                "value": "1. The defense method is not clearly explained and not realistic enough. How and when are the defense prompts inputted to the image generation? \n\n2. A threat model of this type of attack is not specified. When are these attacks be utilized in a real-life scenario?"
            },
            "questions": {
                "value": "1. What could be the reason behind why \u201cInsertion-then-Delete\u201d is the most effective attack? In general, why was decomposition effective compared to traditional attack methods?\n\n2. The traditional attack methods are not cited as well as not explained in the background section.\n\n3. In the automatic evaluation, what is the purpose of observing the response of LLM? How does GPT-4 responding No indicate that the image generation models not refusing the malicious query?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Although the paper discusses harmful contents by jailbreaking these models, it provided a warning in the beginning of paper as well as in the ethical concern section."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}