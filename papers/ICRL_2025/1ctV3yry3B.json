{
    "id": "1ctV3yry3B",
    "title": "MazeNet: An Accurate, Fast, & Scalable Deep Learning Solution for Steiner Minimum Trees",
    "abstract": "The Obstacle Avoiding Rectilinear Steiner Minimum Tree (OARSMT) problem, which seeks the shortest interconnection of a given number of terminals in a rectilinear plane while avoiding obstacles, is a critical task in integrated circuit design, network optimization, and robot path planning. Since OARSMT is NP-hard, exact algorithms scale poorly with the number of terminals, leading practical solvers to sacrifice accuracy for large problems. We propose and study MazeNet, a deep learning-based method that learns to solve the OARSMT from data. MazeNet reframes OARSMT as a maze-solving task that can be addressed with a recurrent convolutional neural network (RCNN). A key hallmark of MazeNet is its scalability: we only need to train the RCNN blocks on mazes with a small number of terminals; mazes with a larger number of terminals can be solved simply by replicating the same pre-trained blocks to create a larger network. Across a wide range of experiments, MazeNet achieves perfect OARSMT-solving accuracy, with significantly reduced runtime compared to classical exact algorithms, and with the ability to handle larger numbers of terminals than state-of-the-art approximate algorithms.",
    "keywords": [
        "Recurrent Convolutional Neural Networks (RCNNs)",
        "Obstacle-Avoiding Rectilinear Steiner Minimum Tree (OARSMT)",
        "Deep learning for maze-solving",
        "Search algorithm for termination condition",
        "Graph-to-image transformation"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1ctV3yry3B",
    "pdf_link": "https://openreview.net/pdf?id=1ctV3yry3B",
    "comments": [
        {
            "summary": {
                "value": "The authors propose a neural network-based framework named Mazenet for the Obstacle Avoiding Rectilinear Steiner Minimum Tree problem, an important combinatorial problem associated with circuit routing. \n\nMazenet is derived from an image classification perspective. The algorithm involves mapping an input graph and set of terminals to an image. An recurrent convolutional network is then trained on synthetic data to sequentially predict elements of the steiner tree. A termination condition module is trained to detect once a candidate path is detected. \n\nThe authors demonstrate that Mazenet recovers the OARSMT faster than classical exact algorithms and highlight its ability to generalize to problem settings beyond its training set. Some ablation experiments detailing Mazenet\u2019s  test accuracy and training time are provided. Superior runtimes are reported and perfect test accuracy."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The authors propose a novel image-based pipeline for the OARSMT problem\n- The synthetic dataset generation is interesting\n- Superior runtimes are reported on a variety of synthetic benchmarks compared to classic methods"
            },
            "weaknesses": {
                "value": "- weak experimental results. The authors evaluate their method on synthetic benchmarks and compare to old methods.\n- some confusing results. figure 14 does not imply perfect test accuracy despite the claims made in the paper.\n- the authors may consider a more rigorous evaluation with the current state of the art, FLUTE or any number of other recent methods, e.g. Chen et al., A Reinforcement Learning Agent for Obstacle-Avoiding Rectilinear Steiner Tree Construction, 2022, Kahng et al., NN-Steiner: A Mixed Neural-algorithmic Approach for the Rectilinear Steiner Minimum Tree Problem, 2023, etc.\n- evaluation on real datasets is critical to understand the performance benefit of the proposed method."
            },
            "questions": {
                "value": "can the authors comment on how does the method compare to other recent works?\n\ncan the authors clarify the discrepancy between figure 14 and the perfect accuracy claims made in the main text\n\n_our method reaches the solution in very few iterations, as seen in Figure 15. This contrasts with the competing methods, which often rely on loops that repeat for many more iterations to arrive to a solution_ - I could not understand the significance of this claim. Can the authors provide additional insight?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes MazeNet, a learning-based algorithm that leverages a recurrent convolutional neural network to predict a single-channel binary matrix iteratively, thereby solving the Obstacle Avoiding Rectilinear Steiner Minimum Tree (OARSMT) problem. The algorithm is evaluated on different mazes with 2-8 terminals, showing 100% test accuracy and competitive planning speed."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper formulates the OARSMT into a binary image prediction problem, which is easy to understand and reasonable.\n\n2. The experimental results show that MazeNet is able to achieve an impressive 100% test accuracy.\n\n3. The experimental results show that MazeNet scales well with an increasing number of terminals."
            },
            "weaknesses": {
                "value": "1. The mazes that MazeNet is evaluated on are too small, of only 11 x 11 kernels. There is not strong evidence that MazeNet can perform well on larger mazes. \n\n2. This work only compares MazeNet with classical solvers like Dijkstra, Mehlhorn and Kou, etc. However, there are some more recent algorithms that are either learning-based or CPU-based, e.g., [1], [2]. [3]. Comparison with more and stronger baselines is needed to consolidate the conclusion.\n\n3. It is not new to learn to predict the future images, e.g., [4] also formulated the grid-like motion planning problem into a video prediction problem. From this paper, I can not see how the specific domain knowledge from OARSMT is incorporated into the network design.\n\n\n[1] Lin, Zhenkun, et al. \"Obstacle-Avoiding Rectilinear Steiner Minimal Tree Algorithm Based on Deep Reinforcement Learning.\" 2023 International Conference on Artificial Intelligence of Things and Systems (AIoTSys). IEEE, 2023.\n\n[2] Chen, Po-Yan, et al. \"A reinforcement learning agent for obstacle-avoiding rectilinear steiner tree construction.\" Proceedings of the 2022 international symposium on physical design. 2022.\n\n[3] Huang, Tao, and Evangeline FY Young. \"An exact algorithm for the construction of rectilinear Steiner minimum trees among complex obstacles.\" Proceedings of the 48th Design Automation Conference. 2011.\n\n[4] Zang, Xiao, et al. \"Robot motion planning as video prediction: A spatio-temporal neural network-based motion planner.\" 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2022."
            },
            "questions": {
                "value": "1. How is the threshold 0.65 decided as the TC threshold? Is there ablation study to find the optimal value?\n2. What is the step size of the solver, i.e., how many cells are the trees extended in each iteration? How many one entries are contained in the predicted binary matrix?\n3. Curious what is the performance of MazeNet on large mazes, e.g., 256 x 256?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "MazeNet, a recurrent convolutional neural network (RCNN) for the Obstacle Avoiding Rectilinear Steiner Minimum Tree (OARSMT) problem, shows promise with 100% accuracy in initial tests but requires further validation on larger grids and more terminals to confirm scalability. Questions remain on its novelty, given similar RCNN applications in maze-solving, and on its high training time (48.12 hours on four GPUs), along with the need to reduce training data complexity and evaluate the TC module's computational overhead. Additional context through a more detailed literature review would also strengthen the work."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. MazeNet is designed for scalability and adaptability, making it effective for solving mazes of varying sizes and numbers of terminals that need connection.\n\n2. While RCNNs alone may struggle to identify and verify a correct solution to terminate the process, MazeNet addresses this by incorporating a search-based algorithm that reliably detects a correct solution. This approach combines the speed of graph-based approximate algorithms with the precision of exhaustive graph-based methods.\n\n3. RCNNs provide step-by-step interpretability of the method\u2019s operations, as the head module can be applied at any iteration, allowing for observation of intermediate solution stages. These stages can be visualized as image outputs, providing insight into the solution process at each step."
            },
            "weaknesses": {
                "value": "1. The proposed approach of using a recurrent convolutional neural network (RCNN) to solve the Obstacle Avoiding Rectilinear Steiner Minimum Tree (OARSMT) problem may lack novelty, as RCNNs have previously been applied to similar maze-solving problems.\n\n2. Although MazeNet demonstrated 100% accuracy in the reported experiments, additional proof is needed to confirm it can consistently achieve this level of accuracy across all problem instances.\n\n3. The experimental setup appears limited; testing just on a grid of 11 \u00d7 11 nodes with up to 8 terminals may not be sufficient to thoroughly assess MazeNet\u2019s performance, particularly regarding its scalability.\n\n4. While the TC module improves MazeNet's accuracy, it introduces significant computational overhead, which has not yet been systematically evaluated.\n\n5. The paper lacks a dedicated related work section, and a more comprehensive discussion of relevant literature would strengthen the context for this research."
            },
            "questions": {
                "value": "1. In what ways does the proposed method differ from prior work that applies Recurrent Convolutional Neural Networks (RCNNs) to solve maze-related problems?\n\n2. Does MazeNet require separate training for different grid and terminal configurations, such as an 11\u00d711 versus a 9\u00d79 node grid, or can a single model handle multiple setups?\n\n3. What strategies can be employed to reduce the time and computational complexity involved in generating training data?\n\n4. Training MazeNet reportedly took around 48.12 hours across four GPUs, which is considerable. How does training time scale with increased problem complexity and size, and what optimizations could help reduce this duration?\n\n5. In Figure 8, is the runtime of MazeNet measured with parallelization applied?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The article establishes a MazeNet model to solve the OARSMT problem. Specifically, it first converts the graph representation of the maze into image representation, then processes the image data using the RCNN model, and finally reduces the model's running time through a termination condition."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The application is interesting."
            },
            "weaknesses": {
                "value": "1.\tThe motivation is not clear, as the article does not explicitly outline the problems with previous solutions to the OARSMT problem, nor does it explain how this article addresses these issues.\n2.\tThe experimental evaluation metric design is unreasonable. The OARSMT problem is an NP-hard problem. However, the evaluation metric used in this article's experimental section is accuracy. While for small-scale problems, the shortest path can be obtained using Dijkstra's algorithm for comparison to calculate precision, for large-scale problems, it is challenging to solve using Dijkstra's algorithm. \nFurthermore, the second part of the article clearly states that the optimization goal is to minimize path length. However, the evaluation metric in the experimental section does not use path length as a measure, which is confusing.\n3.\tIn line 164 of the text, it is stated that \"However, these problems were in domains where traditional methods are both fast and accurate, leaving open the question of whether RCNNs can provide similar advantages for more complex graph-based problems.\" Given that traditional algorithms can achieve good results, what is the significance of this research? Moreover, the question of whether RCNNs can provide similar advantages for more complex graph-based problems remains unresolved. How does this study address or prove this issue?\n4.\tThe resolution of figures 2b and 2c is too low. Although the generated data size is 48x48, clear images should still be placed in the article.\n5.\tThe author's proficiency in English is lacking, and the translation traces are too obvious.\nThe innovation in this article is weak. Regardless of whether it is RCNN or the conversion of graph representation to image representation, the innovation is very limited. From both a writing and experimental perspective, it resembles more of an experimental report and is not suitable for publication as a research paper."
            },
            "questions": {
                "value": "1. The article only mentions the number of samples in the test set. What is the number of samples in the training set?\n2. In terms of problem scale, for instance in the field of chip design where there are tens of thousands of nodes with connections that must adhere to certain constraints, can this algorithm achieve good results in larger-scale tasks?\n3. The testing accuracy can reach 100%, could this be a result of overfitting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the Obstacle Avoiding Rectilinear Steiner Minimum Tree (OARSMT), which seeks to find a set of horizontal and vertical connections between a set of points while avoiding obstacles using the minimum overall connection length. The paper's technical approach is to convert OARSMT graphs to images then use a Recurrent Convolutional Neural Network (RCNN) to iteratively highlight the solution. RCNN-based solutions to OARSMT were introduced in previous work, but this paper uniquely extends RCNN-based maze solving to larger maze domains with more terminals where traditional methods are computationally inefficient. In addition, this paper develops a termination condition to avoid both premature termination and excessive runtimes. Finally, this paper includes experimental results with 2-7 terminals in 11x11 mazes with 100% accuracy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Approach for converting Obstacle Avoiding Rectilinear Steiner Minimum Tree (OARSMT) problem to image-based Recurrent Convolutional Neural Network (RCNN) with extensible training images and more than 2 terminals.\n\n100% empirical accuracy on test cases (40,000 total mazes for 2-5 terminals and 3,000 mazes for 6-8 terminals). Alternatively, graph-based approximation methods of Kou et al. 1981 and Mehlhorn 1988 have errors with 3 or more terminals.\n\nMazeNet is computationally faster than Dijkstra's algorithm when 5 or more terminals are used. \n\nMaze figures are straightforward and informative (e.g., Figure 4)."
            },
            "weaknesses": {
                "value": "Several details technical details are unclear (see specific feedback below).\n\nDoes not provide any limitations or failure cases. For example, what happens if >> 8 terminals are used? This is only discussed as future work. Does algorithm run indefinately for unreachable terminals?\n\nA lot of overlap with Schwarzschild et. al. 2021, but with additional terminals and the terminal condition module.\n\nThe paper emphasizes that their approach is parallelizable (L23, L155, L315) but does not provide key details on how this approach works or report accuracy of experimental results on larger mazes to verify it's utility. Instead, the paper provides a vague description of the parallelization process (Section 3.4, L315) and reports only on runtime performance from parallelization on larger mazes (Figure 9, L466)."
            },
            "questions": {
                "value": "## Questions\n\nHow would researchers replicate your work?\n\nL111 How is O(T!) permutations determined for exhaustive methods?\n\nWhat is the purpose of the paragraph at L174-182? Is the progressive training algorithm of Bansal et al. used in this work? If so, be explicit and state that.\n\nAt L224, \"...position, indicating a cycle, it is terminated to prevent redundant processing.\" After finding a cycle and terminating, which single path is chosen?\n\nAlgorithm 1 L245-250 is a bit difficult to follow. \"junction found\" can only be understood by referencing back to the text. Also, what if the \"Move to the direction with highest 'whiteness'\" is in the backwards direction?\n\nL269 Why are mazes of 2, 3, or 4 terminals chosen for training? (e.g., as opposed to 5, 6, 7)\n\nL293-295 reference random variables n,k. What distributions are these sampled from?\n\nParallelization for Scalability Section 3.4 is missing specific details.\nHow many sections are images divided into? (L320)\nHow many pixels are \"sufficient\" overlap? (L322)\nFor a section with two or more terminals, what is the incentive to find additional paths to other unknown sections?\nWhat is the goal of a section with only one terminal?\nHow does parallelization work for sections without terminals?\n\nL378 What does \"20 MazeNet iterations\" refer to? Earlier sections indicated that 30 module iterations are used before checking terminal conditions (L261) and 16 training epochs are used (L310). There is no explanation in the text or table.\n\n## Feedback\n\nL55 describes a 11x11 maze, but the paper does not clarify what \"11\" refers to until L125 in Section 2.1. Explain what 11x11 means at L55 (e.g., \"11x11 node graph\").\n\nFigure 5 is first referenced at L266 but provides almost no detail or context for what the \"Projection,\" \"Batch,\" and \"Head\" blocks are. Projection was referenced once at L176 when discussing another paper's work. Multiple configurations of the batch and head modules are referenced earlier, but all blocks are uniformly labeled without any specification of the differences between them. For example, the first \"Batch\" represents 30 RB iterations and subsequent \"Batch\" represents 10 iteration (L261) but these are labeled as the exact same module in Figure 5. As another example, L177-180 reference a \"Head\" module that produces the output and a \"final head module\" that transforms the network's output to single-channel prediction. Why not add these details to Figure 5 to be more informative and accurate?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}