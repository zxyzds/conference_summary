{
    "id": "P5UETqZXqT",
    "title": "Model Collapse in the Chain of Diffusion Finetuning: A Novel Perspective from Quantitative Trait Modeling",
    "abstract": "The success of generative models has reached a unique threshold where their outputs are indistinguishable from real data, leading to the inevitable contamination of future data collection pipelines with synthetic data. While their potential to generate infinite samples initially offers promise for reducing data collection costs and addressing challenges in data-scarce fields, the severe degradation in performance has been observed when iterative loops of training and generation occur---known as ''model collapse.'' This paper explores a practical scenario in which a pretrained text-to-image diffusion model is finetuned using synthetic images generated from a previous iteration, a process we refer to as the ''Chain of Diffusion.'' We first demonstrate the significant degradation in image qualities caused by this iterative process and identify the key factor driving this decline through rigorous empirical investigations. Drawing on an analogy between the Chain of Diffusion and biological evolution, we then introduce a novel theoretical analysis based on quantitative trait modeling. Our theoretical analysis aligns with empirical observations of the generated images in the Chain of Diffusion. Finally, we propose Reusable Diffusion Finetuning (ReDiFine), a simple yet effective strategy inspired by genetic mutations. ReDiFine mitigates model collapse without requiring any hyperparameter tuning, making it a plug-and-play solution for reusable image generation.",
    "keywords": [
        "generative",
        "diffusion",
        "model collapse"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=P5UETqZXqT",
    "pdf_link": "https://openreview.net/pdf?id=P5UETqZXqT",
    "comments": [
        {
            "summary": {
                "value": "This paper identifies CFG as the major factor leading to model collapses on synthetic data, and analyses it through the perspective of quantitative trait modeling. Then, the paper proposes a novel mitigation strategy: conditional drops during finetuning, and decayed CFG scaling during sampling."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The observations are interesting.\n* The analysis of quantitative trait modeling is novel.\n* The method is easy to implement and shows promising results."
            },
            "weaknesses": {
                "value": "* No discussion on the potential tradeoff of the method: specifically, how would the generation quality be affected if using the proposed CFG scheduling?\n* Unclear how the method behaves on other CFG scales and decay rates, especially a lower CFG scale, as I only found 5 to 10 in the paper.\n* The paper summarizes prior arts as \u201cfocused on the reduction of diversity\u201d. The paper could have shown if the proposed method maintains sampling diversity as well. \n* The connection between the proposed method and analysis is not convincing. How would a decaying CFG schedule correspond to smoothing truncations?"
            },
            "questions": {
                "value": "* How would the generation quality be affected if using the proposed CFG scheduling?\n* How does the method behave on other CFG scales and decay rates?\n* Does the proposed method maintain sampling diversity?\n* How would a decaying CFG schedule correspond to smoothing truncations?\n* Another work [1] sheds light on how CFG increases quality but reduces variation. It would be interesting to see if using their method would alleviate model collapse.\n* What if different synthetic images have different CFG scales?\n\n[1] Tero Karras, Miika Aittala, Tuomas Kynk\u00a8a\u00a8anniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding a diffusion model with a bad version of itself. arXiv preprint arXiv:2406.02507, 2024a."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates an emerging challenge in machine learning: the potential contamination of training data for future models by AI-generated content. The authors introduce and analyze the concept of \"model collapse\" - a degradation in model performance caused by training on self-generated data. Their key contributions are identifying classifier-free guidance as a primary factor in this degradation process, developing an analytical framework inspired by genetic biology to study this phenomenon, and proposing ReDiFine, a new inference method to mitigate model collapse."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper addresses a timely and critical research question, particularly relevant given the rapid proliferation of foundation models and AI-generated content.\n- The work demonstrates foresight in identifying and analyzing a challenge that could significantly impact future model development.\n- The authors provide clear explanations supported by effective visual diagrams.\n- The biological inspiration for the analytical framework offers a novel perspective on the problem."
            },
            "weaknesses": {
                "value": "- The analysis of the observed phenomena lacks sufficient depth and causal investigation. For instance, the relationship between low CFG scales and low-frequency degradation could be explained by the absence of concept generation in subsequent iterations (those trained without real data), as a CFG scale of 1.0 effectively neutralizes the guidance. However, this hypothesis cannot be verified without samples of generated images used as dataset for successive training iterations.\nIn this connection, not having the concept in the training images of successive iterations could lead to dilution of the concept getting trained.\n- Conversely, the link between high CFG scales and high-frequency degradation could be seen as just concept overfitting. Due to high guidance, crude copies of the previous training images could compose the next the training dataset. This again cannot be verified since training sets for each iteration are not shown. The paper analysis would benefit from a more systematic investigation of these mechanisms, which should form the foundation for developing more robust solutions to the model collapse problem."
            },
            "questions": {
                "value": "How does the proposed framework extend to multi-domain training scenarios? While early work need not address all complexities, some preliminary insights into situations where multiple concepts are trained simultaneously would better align with the paper's broader motivation of understanding model training with aggregated domains.\n\nSpecifically:\n- How do different concepts interact during the collapse process?\n- Does the rate of degradation vary across different domains?\n- How might ReDiFine's effectiveness vary across different concept types?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This  work simulates a recycled pipeline of synthetic images for the training of generative models, and named \"self-consuming chain\"\u3002\nAnd most importantly, image degradation will happen in such recycle chain. So authors design a metric to evaluate whether this model is collapse.\nAuthors check and conclude (among many factors) that  higher CFG scale will be the key factor to accelerate the model collapse.\nAnd a new strategy named ReDiFine is proposed to mitigate model collapse, including dropping condition during finetuning and dynamically adjusting the CFG scale along the chain."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Clear clirify and writing. Good representation.\n\n2. Systematically exploring the self-consuming chain, with designed metric and listed factors.\n\n3. Successfully mitigates the issue proposed by their own."
            },
            "weaknesses": {
                "value": "1. The methods and the paper is well designed and very serious. And I admire this is a good paper.\nBut my main concern is the motivation, which seems to be an impractical and pseudo demands:\n\na. Data hungury now happens in LLM  and many AI models leverage the synthetic data to train. But this work choose the visual/image generation (visual signals are sufficient currently). It might be better to discuss such a topic in the scenario of language?\n\nb. For the training of a large-scale vision model (such as SAM), we have the semi-supervised strategy like human-in-the-loop for SAM.  We can get a huge amounts of raw data with human annotated prompts, for the training of visual generation models.\n\nCombing with the above two reasons, it seems that we do not need using the synthetic data with the same prompts to re-train a visual generation model.\n\nCurrently I offer my rating to 5 and I am willing to raise my rating after your replying my main concern."
            },
            "questions": {
                "value": "1. Could please provide the Recall score in the \"self-consuming chain\"? A common sense is that higher CFG fator (Within a reasonable range) can lead to better FID but the worse Recall. And recall means the generation diversity.  So I consider that the high CFG is the shallow reason but the worse  and worse generation diversity  brought by high CFG is the key factor in the \"self-consuming chain\" .\n\nBesides, I recommend you the high-score work CADS[1] in the last ICLR, which is a more elegant CFG to improve both the FID and the Recall score. You can leverage such an idea to sovle the collapse problem in  ReDiFine.\n\n[1] Sadat, Seyedmorteza, et al. \"CADS: Unleashing the diversity of diffusion models through condition-annealed sampling.\" ICLR 2024"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the phenomenon of \"model collapse\" and identifies significant degradation in image quality caused by the iterative process of generation and training. The key factor contributing to this degradation is found to be the CFG. The paper introduces a novel theoretical analysis based on quantitative trait modeling to explain this phenomenon and proposes ReDiFine, a new method to mitigate model collapse without the need for hyperparameter tuning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The observation that the direction of degradation changes with different CFG values is intriguing and provides new insights.\n2. The proposed ReDeFine method effectively reduces the collapse rate without the need for meticulous tuning. Additionally, this mitigation strategy produces reusable images for future training.\n3. The theoretical perspective offers a novel approach to explaining model collapse."
            },
            "weaknesses": {
                "value": "1. The statement in the abstract that \"their outputs are indistinguishable from real data\" is too absolute. As noted by [a], the distributions generated by diffusion models still differ significantly from real data from the perspective of classifiers.\n2. The experimental setting in this paper differs from existing studies. Here, the authors fine-tune the base model with newly generated data in each iteration, whereas previous studies fine-tune the new model with newly generated data in each iteration. Thus, I am concerned that the analysis presented in this paper may not be directly applicable to prior research.\n\n[a] You Z, Zhang X, Guo H, et al. Are Image Distributions Indistinguishable to Humans Indistinguishable to Classifiers?[J]. arXiv preprint arXiv:2405.18029, 2024."
            },
            "questions": {
                "value": "1. Different fine-tuning approach:\n- Why does this paper fine-tune the base model with newly generated data in each iteration, while existing studies fine-tune the new model in each iteration? I believe this difference makes it distinct from the \"model collapse\" phenomenon discussed in prior research.\n2. Clarification on \u2018clip\u2019 metric (line 208):\n- What does the 'clip' metric refer to in this context? Could you provide more details?\n3. Effectiveness of guidance interval:\n- Could the guidance interval proposed by [b] effectively slow down the collapse rate?\n4. Alternative fine-tuning strategy:\n- Could you try fine-tuning the new model instead of the original model and show some example results?\n5. Choice of k=6:\n- Why was k=6 chosen for the experiments? Could you elaborate on the reasoning behind this selection?\n6. Meaning of symbols in Fig. 2:\n- What do the symbols C, W, S, L, and CLIP skip represent in Fig. 2?\n7. Fine-tuning the text encoder (Fig. 2):\n- Why is fine-tuning the text encoder considered an option in Fig. 2? In my view, the text encoder should not be fine-tuned. Could you explain the rationale for this choice?\n\n[b] Kynk\u00e4\u00e4nniemi T, Aittala M, Karras T, et al. Applying guidance in a limited interval improves sample and distribution quality in diffusion models[J]. arXiv preprint arXiv:2404.07724, 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}