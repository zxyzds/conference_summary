{
    "id": "1olDGAXncb",
    "title": "$f$-Divergence Policy Optimization in Fully Decentralized Cooperative MARL",
    "abstract": "Independent learning is a straightforward solution for fully decentralized learning in cooperative multi-agent reinforcement learning (MARL). The study of independent learning has a history of decades, and the representatives, such as independent Q-learning and independent PPO, can obtain good performance in some benchmarks. However, most independent learning algorithms lack convergence guarantees or theoretical support. In this paper, we propose a general formulation of independent policy optimization, $f$-divergence policy optimization. We show the generality of such a formulation and analyze its limitation. Based on this formulation, we further propose a novel independent learning algorithm, TVPO, that theoretically guarantees convergence. Empirically, we show that TVPO outperforms state-of-the-art fully decentralized learning methods in three popular cooperative MARL benchmarks, which verifies the efficacy of TVPO.",
    "keywords": [
        "multi-agent",
        "reinforcement learning",
        "fully decentralized learning",
        "policy optimization",
        "convergence",
        "independent learning"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1olDGAXncb",
    "pdf_link": "https://openreview.net/pdf?id=1olDGAXncb",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes TVPO for cooperative Markov games, with the update rule of each agent as $\\pi^i_{t+1}=\\arg\\max_{\\pi^i} \\sum_{a_i} \\pi^i(a_i | s)Q_i^{\\pi_t}(s,a_i)-\\omega D_{TV}(\\pi^i(\\cdot|s)|| \\pi_t^i(\\cdot|s) )$ and shows that the algorithm can converge monotonically to the NE of the game. Moreover, TVPO with the adaptive $\\beta$ in PPO shows superior empirical performance over previous algorithms."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The empirical performance of TVPO is superior to previous SOTA\n- The writing is clear except for several typos (see weaknesses)\n- The proofs are easy to follow\n- Compared to previous algorithms, TVPO is easy to implement"
            },
            "weaknesses": {
                "value": "## Comparison to Related Work\nMy major concern is that this paper seems to miss several relevant literature. For instance, [1], [2] both proposed algorithms for independent learning in potential Markov games, which include the cooperative Markov games investigated in this paper. Further, [1] proposed a policy gradient algorithm and [2] proposed a policy iteration algorithm, which is highly relevant to this paper.\n\nMoreover, the algorithm in [2] can also use the adaptive $\\beta$ in PPO. Therefore, I'm wondering if TVPO will be superior to [2] when both using an adaptive $\\beta$.\n\n## Writings\n- $i$ is superscript for $\\pi$ but subscript for $V,Q$\n- The $M$ in Proposition 4.2 and Section 4.2 differs\n- Line 152: such as...\n\nI would be happy to raise the score if the author can resolve the issues above.\n\n[1] Leonardos, Stefanos, et al. \"Global convergence of multi-agent policy gradient in markov potential games.\" arXiv preprint arXiv:2106.01969 (2021).\n\n[2] Fox, Roy, et al. \"Independent natural policy gradient always converges in markov potential games.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2022."
            },
            "questions": {
                "value": "- Is the $V^*$ in Theorem 4.6 the stationary point instead of the value function corresponding to the optimal policy?\n- In the second line of Eq (23), it seems to be $\\Rightarrow$ instead of $\\Leftrightarrow$. Because $f$ is convex instead of strongly convex"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores independent learning in the multi-agent reinforcement learning (MARL) setting and introduces f-divergence policy optimization. The authors analyze the limitations of the method with an illustrative example and propose defining the f-divergence as the total variation distance. Theoretical and experimental results confirm the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Detailed related work in the Fully Decentralized Learning field.\n2. The paper introduces a well-grounded technique for achieving monotonic improvement in multi-agent optimization through decentralized learning.\n3. The paper is well-structured and easy to follow."
            },
            "weaknesses": {
                "value": "1. The relevant work of CTDE is incomplete and lacks recent work, such as HASAC[a] and MAT[b].\n2. Assuming global information might influence the impact of this work.\n3. While the experiment results appear promising, the contribution is slightly insufficient compared with existing work[c,d].\n\n\na. Liu, Jiarong, et al. \"Maximum Entropy Heterogeneous-Agent Reinforcement Learning.\" The Twelfth International Conference on Learning Representations.\n\nb. Wen, Muning, et al. \"Multi-agent reinforcement learning is a sequence modeling problem.\" Advances in Neural Information Processing Systems 35 (2022): 16509-16521.\n\nc. Grudzien, Jakub, Christian A. Schroeder De Witt, and Jakob Foerster. \"Mirror learning: A unifying framework of policy optimisation.\" International Conference on Machine Learning. PMLR, 2022.\n\nd. Su, Kefan, and Zongqing Lu. \"Decentralized policy optimization.\" arXiv preprint arXiv:2211.03032 (2022)."
            },
            "questions": {
                "value": "1. Why use different metrics for SMAC (win rate) and SMACv2 (return)?\n2. Due to the assumption of the global state, I suggest using Markov games [a] as the multi-agent framework.\n\na. Littman, Michael L. \"Markov games as a framework for multi-agent reinforcement learning.\" Machine learning proceedings 1994. Morgan Kaufmann, 1994. 157-163."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper utilizes f-divergence, specifically the total variation, to generalize the KL divergence in independent policy optimization."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The presentation is clear and easy to follow."
            },
            "weaknesses": {
                "value": "- The application of f-divergence in policy optimization is not new; a comprehensive analysis of various distance constraints in policy gradients has been provided in [1].\n\n- Extending existing single-agent analysis to the multi-agent setting is reasonable, but some assumptions are questionable. Specifically, the approach assumes full observability in MARL making the setting difficult to distinguish from single-agent reinforcement learning. Under full observability, what meaningful difference remains between centralized and decentralized control?\n\n- The performance improvement appears marginal. With full observability, IPPO has already demonstrated near-optimal performance on SMAC and Multi-Agent MuJoCo. Were the baseline hyperparameters tuned to achieve their optimal reported performance?\n\n- Why is win rate not used as the evaluation metric for SMAC-v2 tasks?\n\n\n[1] Zhang, Junyu, et al. \"Variational policy gradient method for reinforcement learning with general utilities.\" Advances in Neural Information Processing Systems 33 (2020): 4572-4583."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}