{
    "id": "EdMb9TqqDY",
    "title": "Long-horizon Visual Instruction Generation with Logic and Attribute Self-reflection",
    "abstract": "Visual instructions for long-horizon tasks are crucial as they intuitively clarify complex concepts and enhance retention across extended steps. \nDirectly generating a series of images using text-to-image models without considering the context of previous steps results in inconsistent images, increasing cognitive load.  Additionally, the generated images often miss objects or the attributes such as color, shape, and state of the objects are inaccurate.\nTo address these challenges, we propose LIGER, the first training-free framework for Long-horizon Instruction GEneration with logic and attribute self-Reflection. LIGER first generates a draft image for each step with the historical prompt and visual memory of previous steps. This step-by-step generation approach maintains consistency between images in long-horizon tasks. Moreover, LIGER utilizes various image editing tools to rectify errors including wrong attributes, logic errors, object redundancy, and identity inconsistency in the draft images. Through this self-reflection mechanism, LIGER improves the logic and object attribute correctness of the images.\nTo verify whether the generated images assist human understanding, we manually curated a new benchmark consisting of various long-horizon tasks. Human-annotated ground truth expressions reflect the human-defined criteria for how an image should appear to be illustrative. \nExperiments demonstrate the visual instructions generated by LIGER are more comprehensive compared with baseline methods. The code and dataset will be available once accepted.",
    "keywords": [
        "text to image generation",
        "visual instruction generation"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We propose a training-free framework for long-horizon visual instruction generation with logic and attribute self-reflection.",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=EdMb9TqqDY",
    "pdf_link": "https://openreview.net/pdf?id=EdMb9TqqDY",
    "comments": [
        {
            "summary": {
                "value": "This manuscript introduces an innovative task named\"Visual Instructions for Long-Horizon\" which aims to generate a series of continuously aligned visual images corresponding to textual instructions. The manuscript proposes four self-reflection methods that leverage both visual and historical prompts. To prevent cumulative deviation and help generate along the correct trajectory, an approach termed Inversion-Based Visual Memory Calibration is proposed. The proposed method is noteworthy for its approach to addressing attribute errors and inconsistencies by utilizing existing mLLM tools."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe proposed task and solution are indeed novel. Ablation experiments validate the contributions of the various proposed modules, including the visual prompt, historical prompt, self-reflection mechanism, and inversion-based memory calibration.\n2.\tNumerous qualitative experiments demonstrate that the proposed tool-based self-reflection method maintains alignment with textual instructions, ensuring that the generated images adhere to contextual logic, thereby validating the overall efficacy of the approach."
            },
            "weaknesses": {
                "value": "1.\tThis manuscript would benefit from a more comprehensive comparative analysis. I recommend including comparisons with additional train-free methods, such as [1] and [2], as well as approaches that share similar concepts, like [3], and non-tool-based methods, such as [4].\n2.\tFurthermore, not all metrics proposed by the authors are original, and the use of multimodal large language model (LLM) reasoning to evaluate images also not original. It is advisable to revise the relevant statements in the contributions section to reflect this accurately.\n3.\tWhile the research method is innovative, the reliance on several pre-designed strategies and external models for image refinement may not be particularly efficient in practical applications. This could be seen as a limitation of the manuscript and the authors are encouraged to provide an appropriate discussion of this issue.\n\n+ [1] Coherent Zero-Shot Visual Instruction Generation\n+ [2] Training-Free Consistent Text-to-Image Generation\n+ [3] Consistent self-attention for long-range image and video generation\n+ [4] StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation"
            },
            "questions": {
                "value": "Why is there no evaluation of the effect of \u201c+self-reflection\u201c alone in the ablation experiments? It appears to be directly combined with memory calibration in the final method without independent verification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method for visual story generation that is completely training-free but consists of many steps. The main parts of the method include keeping a historical prompt and a visual memory for consistency and self-reflection for refinement. The method shows improved results over the previous methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. the paper is written well. \n2. the method section reads clearly. \n3. the paper shows improved results over the baselines."
            },
            "weaknesses": {
                "value": "1. the method has a lot of moving parts. i would have liked to see some error analysis regarding how does the approach work if one of the components makes a mistake. for example, what happens if gpt-4o misses some details? \n2. the current work is overly reliant on the closed-source gpt-4o. there are also plenty of open source models available. some ablation on using open source vlms could be useful and beneficial for the community. \n3. error bars are not reported. \n4. how expensive is this approach. if the sequence length is too long of the tasks. would that mean that we will need to store a lot of 'visual memory'? some discussion on this could have been helpful."
            },
            "questions": {
                "value": "i have some serious concerns about the motivation of this task. what can be the differences of this particular task with semantically consistent video generation for a scene? \n\nplease also look at the weaknesses. overall i like the paper. but if these concerns are addressed i can update my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on long-horizon text-to-image generation tasks and proposes a train-free framework. Long-horizon text-to-image tasks face two main challenges: temporal consistency and the accumulation of attribute errors. To address this, the work injects historical text descriptions and visual tokens into a Diffusion Model, then leverages GPT-4 and the segmentation large model LISA to detect and locate errors in the images. Image editing tools are subsequently employed to correct these errors. Meanwhile, the framework uses DDIM inversion to obtain the features of the edited images. Additionally, the paper introduces an evaluation dataset of over 500 samples to assess the effectiveness of long-horizon image generation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well-structured and smoothly written, making it easy for readers to understand.\n- The figures and tables in the paper are well-designed, making them easy to understand.\n- The motivation is well-defined. To address the temporal consistency issue in long-horizon visual instruction generation, it proposes injecting historical text and visual information into the diffusion model. To tackle the attribute error problem, it introduces a method of using MLLMs to detect errors and then calling an agent to correct them, which demonstrates a certain level of novelty."
            },
            "weaknesses": {
                "value": "- This paper only validates its effectiveness on a self-proposed evaluation dataset, which is somewhat unfair. It is recommended to find more suitable and fair evaluation datasets for verification. \n- The evaluation dataset used in this paper is limited to cooking scenarios, lacks generality, and is relatively small, with only about 500 samples."
            },
            "questions": {
                "value": "- In Section 3.1, \"Visual Memory Sharing\" uses an attention module to inject historical visual information into the Diffusion model. However, the paper claims that the method is train-free, so where are the weights for this attention module loaded from? If this attention module is not trained on the specific task, can it really be used in a zero-shot manner? I have doubts about its performance.\n- The symbol $O_i$ in Eq.3 may lack an explanation, which might leave readers wondering how $O_i$ is used in the Diffusion model.\n- In Section 3.2, it is recommended to briefly explain how the image editing tools mentioned (such as DragonDiffusion, SD inpainting Rombach, LAMA, etc.) are used in this method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a training-free framework for generating long-horizon visual instructions with logic and attribute self-reflection. It drafts consistent images for each step, using historical prompts and visual memory, and corrects errors with image editing tools. Experiments show it produces more comprehensive visual instructions than baseline methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.  The training-free framework and self-reflection mechanism of LIGER provide a novel approach to visual instruction generation for long-horizon tasks.\n2.  The writing is clear and well-structured, making the concepts easy to understand.\n3. The manually curated benchmark tests effectively demonstrate the advantages of the images generated by LIGER in terms of comprehensibility."
            },
            "weaknesses": {
                "value": "1. In Automatic evaluation, the authors didn't evaluate the quality of the generated images, only assessing alignment and consistency.\n2. In this paper, the benchmark's narrow focus on cooking might not capture the full spectrum of complexities and variations present in long-horizon tasks across different industries or activities. It would be beneficial for the authors to provide more details on the types of tasks included in the benchmark."
            },
            "questions": {
                "value": "1. How does LIGER's self-reflection mechanism ensure that the identification and correction of errors in images are accurate and error-free? Is there a possibility of over-correction or failure to recognize certain types of errors? How you balance Over-consistent and Identity inconsistent?\n2. How much time consumption does LIGER introduce?\n3. Given the benchmark's focus on cooking tasks, how does LIGER address the generalization to other long-horizon tasks, and are there plans to broaden the benchmark's scope? Like engineering and sports?\n4. Is there a specific template or structure that the benchmark follows for the steps involved in the tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}