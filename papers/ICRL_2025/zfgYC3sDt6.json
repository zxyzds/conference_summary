{
    "id": "zfgYC3sDt6",
    "title": "Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models",
    "abstract": "Confidence calibration is critical for the safe deployment of machine learning models in the real world.\nHowever, such issue in vision-language models like CLIP, particularly after fine-tuning, has not been fully addressed.\nIn this work, we demonstrate that existing prompt tuning methods usually lead to a trade-off of calibration between base and new classes:\nthe cross-entropy loss in CoOp causes overconfidence in new classes by increasing textual label divergence, whereas the regularization of KgCoOp maintains the confidence level but results in underconfidence in base classes due to the improved accuracy.\nInspired by the observations, we introduce Dynamic Outlier Regularization (DOR) to ensure the confidence calibration on both base and new classes after fine-tuning. \nIn particular, we propose to minimize the feature deviation of novel textual labels (instead of base classes) sampled from a large vocabulary.\nIn effect, DOR prevents the increase in textual divergence for new labels while easing restrictions on base classes.\nExtensive experiments demonstrate that DOR can enhance the calibration performance of current fine-tuning methods on base and new classes.",
    "keywords": [
        "Vision-Language Models",
        "Confidence calibration",
        "Outlier Regularization",
        "Prompt Tuning"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We propose Dynamic Outlier Regularization to improve CLIP\u2019s calibration under various evaluations without compromising vanilla fine-tuning.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zfgYC3sDt6",
    "pdf_link": "https://openreview.net/pdf?id=zfgYC3sDt6",
    "comments": [
        {
            "summary": {
                "value": "This paper, through CoOp and KgCoOp, observe that when a model undergoes adaptation via prompt tuning, CoOp tends to be overconfident in novel classes, while KgCoOp, on the contrary, becomes underconfident. To address this tendency towards overconfidence/underconfidence, this paper propose a Dynamic Outlier Regularizer (DOR) term, which demonstrates performance improvements when the proposed regularizer is added to the algrorithms for prompt learning of VLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. A new blind point was identified (CoOp and KgCoOp becomes over/under confident)\n2. Experimental results show performance improvement when the proposed regularizer is added."
            },
            "weaknesses": {
                "value": "1. The motivation was inferred from CoOp and KgCoOp, but in fact, these two algorithms lack adequate consideration for novel classes. For an effective analysis of this phenomenon, algorithms that directly account for novel classes should be utilized, such as CoCoOp, MaPLe, PromptSRC, DEPT, and TCP. If similar tendencies are observed in these algorithms, it would strongly support the authors' claim.\n\n2. It appears that further explanation is needed regarding the concept of \"Texture divergence.\" From my understanding, this divergence is due to the diversity of textual representation arising from CoOp's prompt learning method. However, finding concrete evidence to confirm this explanation is challenging. Additionally, a more detailed explanation of the notation, particularly the keywords emphasized in the paper, would enhance reader's understanding if the study.\n\n3. More comparisons with other algorithms are necessary. Given the large number of prompt learning algorithms, further experimental comparisons are needed to confirm whether the effect of this regularizer is generalizable. (Similar with the first weakness statement)"
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Dynamic Outlier Regularization (DOR) to improve confidence calibration in prompt-tuned vision-language models (VLMs), particularly in CLIP. The authors argue that current prompt-tuning methods lead to miscalibration by creating a trade-off between base and new classes, with CoOp overconfident on new classes and KgCoOp underconfident on base classes. DOR aims to resolve this trade-off by introducing textual outliers to regularize model predictions, promoting consistent calibration across both base and new classes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The paper identifies a relevant issue in the domain of prompt tuning and confidence calibration in VLMs, an area of growing importance.\n- The authors introduce a novel idea of using dynamically sampled textual outliers to address calibration inconsistencies, and the approach shows effectiveness across various datasets.\n- DOR\u2019s flexibility in working with multiple prompt-tuning methods is a potential advantage."
            },
            "weaknesses": {
                "value": "- The paper does not include comparisons with several recent and closely related methods, such as CoPrompt and PromptSRC, which also address calibration issues and trade-offs between base and new classes. Without these comparisons, it is unclear whether DOR provides any substantial advantage over the state of the art, especially since these methods were specifically designed to tackle the same calibration challenges.\n\n- The primary claim\u2014that prompt-tuning methods like CoOp and KgCoOp introduce calibration trade-offs between base and new classes\u2014has already been extensively studied in prior works. For instance, CoPrompt effectively handles these issues and includes mechanisms specifically designed to manage calibration across both class types. As such, the problem statement lacks novelty, and the paper provides insufficient rationale for why DOR would be preferable to these existing methods.\n\n\n- While the paper offers some empirical evidence for DOR\u2019s effectiveness, it lacks analysis that explains why the use of textual outliers should systematically address calibration trade-offs. \n\n- The proposed solution, while conceptually interesting, lacks practical guidelines on how to effectively select and implement outliers in a real-world setting. Given that the efficacy of DOR relies on appropriate textual outlier selection, more detailed criteria or algorithms for selecting these outliers would be necessary for practitioners to adopt this method.\n\n- Interestingly, some of the latest methods show less improvement with the proposed solution compared to some of the earlier methods like CoOp. This indicates that the latest methods are already capable of handling the problem and don't require such a solution proposed in this paper. Again, the paper lacks a comparison to the latest method, making it difficult to understand if it has any usage."
            },
            "questions": {
                "value": "See the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Dynamic Outlier Regularization (DOR), a method to improve confidence calibration in fine-tuned VLMs by controlling textual feature divergence (also can be extended to visual tuning) through the use of selected outliers, thereby enhancing model reliability on both base and novel classes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. DOR integrates smoothly with popular prompt-tuning methods (e.g., CoOp, KgCoOp) without requiring major architectural changes, making it easy to adopt in existing pipelines. \n2. The authors provided insightful analysis of how current prompt-tuning methods impact confidence calibration, with clear explanations for why overconfidence or underconfidence arises in certain settings. The motivation is clear.\n3. The manuscript is well-written and in a good logic."
            },
            "weaknesses": {
                "value": "1. The ablation study is insufficient, e.g. how sensitive DOR is to the choice of outliers and whether different selection strategies could yield better results?\n2.  The experiments are largely limited to standard benchmarks without applying the method to domain-specific tasks (e.g., medical imaging or autonomous systems as the authors mentioned in the Introduction), where calibration is especially critical."
            },
            "questions": {
                "value": "1. When you calculate the semantic similarity between textual labels in WordNet and the base classes, do you use cosine similarity? Please Clarify. Would using a different metric (e.g. Euclidean distance) impact results?\n2. The outliers\u200b are selected based on the top-K, but there\u2019s no mention of a specific similarity threshold. Would setting a threshold affect performance?\n3. How frequently are the outliers updated during training? Does the frequency affect DOR\u2019s calibration performance?\n4. In page 8, line 407-408, is it a typo (should be \"CoCoOp\" ranther than \"CoOp\") or a wrong statement (the number is wrong if you campred to the zero-shot CLIP)? Please correct it.\n5. What criteria were used to select visual outliers from ImageNet-1K? How to ensure these outliers are sufficiently distinct from base classes without introducing irrelevant noise?\n6.  How does DOR influence the feature space of base classes when incorporating visual outliers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the calibration performance of CLIP following fine-tuning. The author observes a trade-off in calibration between base and new classes and proposes a method called Dynamic Outlier Regularization (DOR). DOR samples categories unrelated to the base class from a large vocabulary to minimize the feature deviation of novel textual labels. Empirical results demonstrate that this approach outperforms the standard fine-tuning method across various settings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The author conducted robust experiments that demonstrate how prompt fine-tuning prior to happiness can lead to a decline in the model's calibration performance.\n\n2. The paper introduces an efficient normalization method designed to enhance the calibration performance of both base and novel classes.\n\n3. The paper provides performance results across multiple calibration evaluation metrics and a range of experimental settings.\n\n4. The writing is clear and well-structured, making it easy to read."
            },
            "weaknesses": {
                "value": "1. Previous works [1,2] have examined the calibration performance of pre-trained CLIP after fine-tuning. However, your paper lacks experimental results comparing your method with these studies. We recommend that you include such comparisons in your work.\n2. Given that your method is based on experimental observations from CoOp and KgCoOp, we have concerns about its generalizability. For example, in Table 1, your method underperforms compared to Vanilla TCP in half of the settings.\n3. This article primarily selects outliers from WordNet. We are curious whether using different lexical databases significantly affects the results.\n\nIf you can include relevant experiments and address my questions, I will consider increasing the score.\n\n[1] Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models.\n\n[2] Towards Calibrated Robust Fine-Tuning of Vision-Language Models."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}