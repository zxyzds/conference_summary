{
    "id": "NbbsRnPBoS",
    "title": "Faster Gradient Descent in Deep Linear Networks: The Advantage of Depth",
    "abstract": "Gradient descent dynamics in deep linear networks has been studied under a wide range of settings. These studies have reported some negative results on the role of depth, in that, gradient descent in  deep linear networks: (i) can take exponential number of iterations to converge, (ii) can exhibit sigmoidal learning, i.e., almost no learning in initial phase followed by rapid learning, (iii) can delay convergence with increase in depth. Some of these results are also under stronger assumptions such as whitened data and balanced initialisation. These messages from prior works suggest that depth hurts the speed of convergence.\n\nIn this paper, we argue that the negative role of depth in the prior works is due to certain pitfalls which can be carefully avoided. We give a positive message on the role of depth, i.e., seen as an additional resource, depth can always be used to speed up convergence. For this purpose, we consider scalar regression with quadratic loss. In this setting, we propose a novel aligned gradient descent (AGD) algorithm for which we show that (i) linear convergence is always possible (ii) depth accelerates the speed of convergence. In AGD, feature alignment happens in first layer and the deeper layers accelerate by learning the right scale. We show acceleration in AGD happens in finite time for unwhitened data. We provide insights into the {acceleration} mechanism and also show that acceleration happens in phases. We also demonstrate the acceleration due to AGD on synthetic and benchmark datasets. Our main message is not propose AGD as a new algorithm in itself, but to demonstrate that depth is an advantage in linear networks thereby dispelling some of the past negative results on the role of depth.",
    "keywords": [
        "Deep Linear Network; Gradient Descent; Faster Convergence in Finite Time"
    ],
    "primary_area": "optimization",
    "TLDR": "Prior works have shown that depth plays a negative role in convergence in deep linear networks. We show that depth in fact speeds up convergence.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=NbbsRnPBoS",
    "pdf_link": "https://openreview.net/pdf?id=NbbsRnPBoS",
    "comments": [
        {
            "title": {
                "value": "Questions to Reviewer REzx"
            },
            "comment": {
                "value": "1) All prior works for deep linear networks (in venues comparable to ICLR)  have essentially considered width = 1. Yet, they not only failed to improve speed of convergence using depth, but also gave a message (based on a rather heuristic gradient flow analysis) that depth hurts. We have also considered width = 1, and showed (in theory and experiments) that depth helps to speed up convergence in finite time. Why does this not amount to ICLR standards for theoretical novelty? What is our fault here? doing the right thing to fix an issue? \n\n2) In optimising any objective in a continuous domain, a unique global solution is a single point whose measure is zero. The non-trivial part is to actually find this measure zero global solution. Our objective is to use depth to increase the speed of convergence and we indeed find such a solution for the objective. What is our fault here? finding a solution for the objective? \n\n3) Nowhere in the paper we are talking about deep non-linear networks. Further, we have clarified in the introduction (lines 58-63 in our paper) that our objective is to use depth to increase speed of convergence in deep linear networks. We wonder then why our work is being held against what it may or may not imply for deep non-linear networks. Does the reviewer consider every work on deep linear networks unrealistic because no one uses them in practice?"
            }
        },
        {
            "title": {
                "value": "Point by Point Response to other comments by reviewer REzx"
            },
            "comment": {
                "value": "**Limited scope of the investigation/practical relevance** : We **partially agree**. Our work does not immediately translate into a practice recommendation for deep non-linear networks.  However, from prior results, it is natural to conjecture that even for deep linear networks depth hurts, i.e., no deep linear network can outperform a one layer linear network in speed of convergence. Our work invalidates such a conjecture, i.e., negative role of depth is not a settled case, and the hope is that in the future one can come up with initialisations and algorithms such that depth helps even optimisation in deep non-linear networks.\n\n**Failed Objective** : We **disagree**. Our objective is to use depth as a resource to increase the speed of convergence in deep linear networks when compared to one layer model. Our work indeed achieves this objective.\n\n**Lack of Theoretical novelty** : We **disagree**. AGD is a novel algorithm. There are two important novelties, the initialisation and the adaptive scaling both of which contribute towards the increased speed of deep linear networks in comparison to one layer network.\n\n**Lack of empirical support**: We strongly **disagree**. Subsections 4.1, 4.2, 4.3, 4.4, 4.5 and Figures 1 to 4 are entirely empirical in nature and support the theory and the objective.\n\n**Unrealistic Initialisation**: We **disagree**. One can call our initialisation non-standard and not widely used in practice. Such widely used realistic initialisations are not useful for our objective. This is the very reason we came up with our initialisation. \n\n**Unrealistic Assumption**: We strongly **disagree**. If at all, we are making very minimal assumptions on the dataset. We have already explained initialisation and the width = 1 aspects in the previous points.\n\n**No enhancement of our understanding of the phenomena**: We **disagree**. The paper clearly separates the issue of poor initialisation and the possible unstable behaviours that occur due the training dynamics. In fact, ours is the first work to demonstrate how unwhitened data causes GD to be unstable even for two layer width 1 linear networks. \n\n**Computational increase**: We **agree**. The increase limited to 5L is only due to architecture. We will modify this in our revision.\n\n**Terminology used to describe shallow networks**: Point taken. We will change this in our revision.\n\n**Redundancy**: Point taken. We will change this in our revision."
            }
        },
        {
            "title": {
                "value": "Many thanks to reviewer REzX (in this comment, we point out the factual errors/misconceptions in the review)"
            },
            "comment": {
                "value": "**We are very thankful for the detailed review and also to the reviewer's openness for a discussion**. The review makes points which are useful as a devil\u2019s advocate. However, there are critical factual errors and misconceptions in the review which make the overall assessment incorrect. We first point out these errors and misconceptions, then provide a point by point response to the review, and then present our questions to the reviewer.\n\n**In what follows, to make our position clear, we have indicated where and why we disagree. That said, at the end of the day, we find the points raised by Reviewer REzx very valuable in making the contribution stronger and we are very thankful for the same.**\n\n---------------------\n----------------------\n\n**Disagreement with the Summary** : The summary is also incorrect due to the factual inaccuracies which we point out below. \n\n**Factual Error 1 in the Review  (multiple points on all the layers are the identity except one; reducing it to standard linear regression)**: We strongly disagree. We are training all the layers, so for $t>0$ none of the layers are identity. So, our models are not equivalent to standard linear regression with one layer for time $t>0$. In particular, the dynamics of gradient descent in our models is not the same as one layer model. \n\n**Factual Error 2 in the Review  (point on GD is unstable)**: We strongly **disagree**. The failure case, i.e., unstable behaviour of GD is also for other standard (in the market) initialisation schemes namely He and Normal under a small constant learning rate. This has been already mentioned explicitly in the title of Section 4.1, and in boldface text which reads \u201ccomparison with other initialisations\u201d. The instability of GD is further explained in Section 4.4 where the analysis does not make use of any specific initialization (i.e., all layers identity except one).\n\n**Factual Error 3 in the Review  (depth killed to a point of no effect)** : We **disagree**. Since we are training all the layers, depth has a huge effect in the gradient dynamics, and if not addressed properly (like we do in AGD), depth can cause instability. \n\n**Misconception 1 in the Review  ( point on contrived example; overly simplistic case)**: We strongly **disagree**. We would like to re-emphasise that the negative results on depth in prior works (Saxe et al. (2013), Arora (2018), Shamir (2019)) are essentially arguments based on width = 1 networks. Here, we (us and prior works) are studying the phenomenology of depth and width = 1 is sufficient for the phenomena to emerge. \n\n$\\bullet$ Saxe et al (2013) (https://arxiv.org/abs/1312.6120) use connectivity modes, and reduce the model to width = 1. We quote lines from page 7 above equation (14) of their work \u201ceach connectivity mode in the $N_l$ layered network can be described by $N_{l \u2212 1}$ scalars $a^1 , . . . , a^{N_{l} \u22121}$.\n\n$\\bullet$ Shamir (2019) (http://proceedings.mlr.press/v99/shamir19a/shamir19a.pdf) studies only scalar networks. We quote the first line of the abstract \"We study the dynamics of gradient descent on objective functions of the form $f(\\prod^k_{\ud835\udc56=1}w_i)$ (with respect to scalar parameters $w_1,\u2026,w_k$)\".\n\n$\\bullet$ Arora et al (2018) (https://proceedings.mlr.press/v80/arora18a/arora18a.pdf) note in Section 8 (page 7, column 1, paragraph 2) \u201cSpecifically, it shows that in the evaluated set-ting, hidden layers of size 1 (scalars) suffice in order for the essence of overparameterization to fully emerge\u201d, i.e., they are considering the width =1 case.  Further, they also go on to add, we quote from (page 7, column 2, paragraph 1, line 1) \u201cAs can be seen, convergence of deeper networks is (slightly) slower in the case of l2 loss. This falls in line with the findings of Saxe et al. (2013)\u201d.\n\n**Misconception 2 in the Review  (measure zero)** : Considering deep linear networks with arbitrary initialisation as a class, we pick the right member (not an average representative) that achieves our objective of using depth to increase the speed of convergence. \n\n**Misconception 3 in the review  (this does not imply that deeper networks may be trainable)** : We have not made any claims for deep non-linear networks at all in our paper. If the reviewer meant deep linear networks, then we strongly **disagree**. We are indeed showing that deep linear networks are trainable, and even a stronger claim that they are instance-wise (dataset and learning rate) faster than one layer networks."
            }
        },
        {
            "summary": {
                "value": "This work studies optimization in deep linear networks, in particular the effect of depth on convergence.\nIt proposes a novel algorithm, aligned gradient descent (AGD), that solves the issue of slow convergence of linear networks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The results seem correct."
            },
            "weaknesses": {
                "value": "This work considers a very narrow problem that, in my opinion, is of very little interest.\nFirst of all, the problem of deep linear networks is very narrow.\nHowever, even worse than that, the authors motivate their work from trivial observations.\nSection 3.1 describes a trivial situation in which the neural network is initialized at a very special value of the parameters, that is known to converge to a saddle point. Any initialization that is sufficiently far from that special case would not suffer from the limitations described by the authors. Also, depth plays no role in this section, contrary to what seems to be the main motivation of the authors.\nSimilarly, section 3.2 describes another trivial situation where the neural network is initialized very near the saddle point. Again, any initialization that is far away enough from that special initialization would not suffer from the problems described.\nIn section 3.3, the authors desribe problems in the case of p-norm loss, but that is also a narrow case of very little interest. \nThe novel algorithm, AGD, is quite complicated and is limited to deep linear networks.\nIt remains unclear why that algoroithm may be useful or interesting in any other (non-linear) case."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper tackles the optimization issues induced by higher depth.\n\nThe authors show that in the case of 1) width 1, 2) a very precisel and unrealistic initialization where all the layers but one are initialized at 1 and the others at 0, a version of GD converges exponentially as the standard OLS.\n\nThe authors conclude that this implies that some negative optimization results attributed to the depth are not actually attributable to the depth."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is clearly written and well organized. The problem is well introduced."
            },
            "weaknesses": {
                "value": "The paper does not meet the ICLR standards for theoretical novelty or practical relevance. The issues with unrealistic assumptions, limited scope of the investigation, and lack of empirical support are significant enough that a strong reject recommendation is given.\nThe paper has a huge number of problems, but those are not even the point. It completely fails in its objective. The conclusions drawn are misleading and not supported by the computations on the toy models provided.\n\nThe claims of non-detrimental effects of depth are based on a single, highly contrived example where all layers but one are essentially bypassed by setting their weights to 1, reducing the model to a standard linear regression. This approach does not reflect the complexities or realities of deploying deep networks in practice. The authors provide precisely one of those few cases (of measure zero) in which depth is not detrimental for the optimization and they claim that thus depth may not be detrimental. \n\nThis does not imply that deeper networks may be trainable. They are actually cooking up an example in which they kill the effect of depth to say that depth there has no effect. Not only this cooked up example is very far from practice, but everywhere else in the parameter space, depth has an effect.\n\nIt is also misleading to suggest that the computational increase is limited to 5L, which results solely from choosing a network width of one\u2014a characteristic of the architecture, not the algorithm itself. Computational demands typically scale with network width.\n\nMoreover, even in this overly simplistic case they show that GD is unstable and that is the reason why they change algorithms. In practice they are telling us that even on this instance of deep network in which all the layers are the identity except one, GD would not behave well as the depth scales.\n\nAdditionally, the terminology used to describe shallow networks as standard linear regressions contradicts the literature referenced, where shallow networks are generally recognized as having a single hidden layer.\n\nOverall, this paper does little to enhance our understanding of the phenomena it intends to explore and fails to address its central thesis effectively.\n\nA minor point of critique is the redundancy in the text; the last paragraph of page 1 is essentially repeated with identical wording at line 168 and again in section 5. This redundancy could have been avoided to streamline the content and enhance clarity."
            },
            "questions": {
                "value": "Am I wrong about my assessment? I really do not see how this analysis can lead to prove your claims but happy to discuss it with the authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Whether the depth provides an advantage in training deep networks has been a key question discussed in prior research. In response to this, the present study constructs a gradient method (along with the initialization strategy) that achieves fast convergence even as the number of layers increases, positively answering this question. The distinct feature compared to previous studies is that un-whitened data is allowed, as well as the use of the standard L2 loss function."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors constructed a concrete and solvable example where a specific gradient method achieves fast convergence in a deep linear network. Notably, this work demonstrates that depth can be an advantage in optimization even in the general case with un-whitened data.\n\n- The intuition behind the speed-up of convergence is well explained, including examples from previous works on acceleration."
            },
            "weaknesses": {
                "value": "**Limitation on the Width**  \nAs far as I understand, the analysis is limited to a width of 1 for hidden layers, and it is not obvious whether it can be extended to networks with general widths. If this is true, it is a very restricted scenario.\n\n**Necessity of AGD**  \nWhile it is clear that AGD contributes to the acceleration of convergence, the necessity of using AGD specifically is unclear. For instance, why wouldn\u2019t Adam or Newton\u2019s method work?"
            },
            "questions": {
                "value": "**Limitation on the Width**  \n- Why do the authors not generalize the results to networks with general widths? A width of 1 is very restrictive. The authors mention some prior work as being limited due to the assumption of whitened data, but to me, restricting the width to 1 seems more idealized and far removed from modern *over-parameterized* deep networks.  \n- What is the width used in the experiments on datasets in Section 4.5?\n\n**Necessity of AGD**  \nAGD has an adaptive learning rate scaled by $(\\Theta_t^{(2:L)})^2$.  Is it necessary to use the square, or can this be generalized to $(\\Theta_t^{(2:L)})^p$ ($p>0$)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}