{
    "id": "PQzZrRNynC",
    "title": "PSformer: Parameter-efficient Transformer with Segment Attention for Time Series Forecasting",
    "abstract": "Time series forecasting remains a critical challenge across various domains, often complicated by high-dimensional data and long-term dependencies. This paper presents a novel transformer architecture for time series forecasting, incorporating two key innovations: parameter sharing (PS) and Spatial-Temporal Segment Attention (SegAtt). We also define the time series segment as the concatenation of sequence patches from the same positions across different variables. The proposed model, PSformer, reduces the number of training parameters through the parameter sharing mechanism, thereby improving model efficiency and scalability. The introduction of SegAtt could enhance the capability of capturing local spatio-temporal dependencies by computing attention over the segments, and improve global representation by integrating information across segments. The combination of parameter sharing and SegAtt significantly improves the forecasting performance. Extensive experiments on benchmark datasets demonstrate that PSformer outperforms popular baselines and other transformer-based approaches in terms of accuracy and scalability, establishing itself as an accurate and scalable tool for time series forecasting.",
    "keywords": [
        "Time Series Forecasting",
        "Transformer",
        "Parameter Sharing"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PQzZrRNynC",
    "pdf_link": "https://openreview.net/pdf?id=PQzZrRNynC",
    "comments": [
        {
            "summary": {
                "value": "The article introduces a novel Transformer architecture, PSformer, for time series forecasting, incorporating parameter sharing (PS) and Spatial-Temporal Segment Attention (SegAtt). The authors conducted experiments on various benchmark datasets, comparing PSformer with baseline methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The design ideas and motivations behind the model are very interesting.\n\n2. The way the article is written is very good, making it very easy to follow."
            },
            "weaknesses": {
                "value": "1. The article has some minor errors, for example, I couldn't seem to find Table 27.\n\n2. The article lacks research on important references, such as the author's focus on iTransformer at ICLR 2024, but does not consider contemporaneous models like TimeMixer[1], and FITS[2]. Furthermore, this still lacks a comparison with GNN-based methods like CrossGNN[3] and FourierGNN[4] from NeurIPS 2023 and other methods such as MICN[5] at ICLR 2023.\n\n[1] Wang, S., Wu, H., Shi, X., Hu, T., Luo, H., Ma, L., ... & ZHOU, J. (2024). TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting. In The Twelfth International Conference on Learning Representations.\n\n[2] Xu, Z., Zeng, A., & Xu, Q. (2024). FITS: Modeling Time Series with $10 k $ Parameters. In The Twelfth International Conference on Learning Representations.\n\n[3] Huang, Q., Shen, L., Zhang, R., Ding, S., Wang, B., Zhou, Z., & Wang, Y. CrossGNN: Confronting Noisy Multivariate Time Series Via Cross Interaction Refinement. In Thirty-seventh Conference on Neural Information Processing Systems.\n\n[4] Yi, K., Zhang, Q., Fan, W., He, H., Hu, L., Wang, P., ... & Niu, Z. FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure Graph Perspective. In Thirty-seventh Conference on Neural Information Processing Systems.\n\n[5] Wang, H., Peng, J., Huang, F., Wang, J., Chen, J., & Xiao, Y. (2023). Micn: Multi-scale local and global context modeling for long-term series forecasting. In The Eleventh International Conference on Learning Representations.\n\n3. The experimental comparisons are insufficient. The methods mentioned in W2 and TimesNet[6] were also not compared by the authors, therefore, it cannot be concluded that PSformer achieves SOTA performance.\n\n[6] Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., & Long, M. (2023). TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis. In The Eleventh International Conference on Learning Representations.\n\n4. The author seems to have not provided details about the experimental platform. Additionally, it appears that the author did not specify the parameter search space for the compared methods. Has the author completed the work of determining the best model parameters on the validation set? If this work has not been done, it would not be fair to compare the performance of all models, as the specific choices of the experimental platform and model parameters can have a significant impact on the experimental conclusions. It is hoped that the author can clarify this point."
            },
            "questions": {
                "value": "See the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a Transformer architecture for time series forecasting that highlights parameter sharing and cross-channel patching. Specifically, it applies attention across both channels and patches for spatio-temporal information fusion, and aligns the parameters of linear projections within an encoder block. Experiments show that these two designs contributes to the overall performance improvement on popular benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The method proposed in the paper has clear motivation and solid intuition.\n2. The idea is presented with clarity and easy to follow.\n3. Sufficient analysis is done to highlight the efficacy of the proposed updates on existing Transformer architectures."
            },
            "weaknesses": {
                "value": "1. The accuracy improvement is relatively marginal, especially in ablation studies, and variances in metrics should be reported to support the significancy of the contribution from the proposed ideas.\n2. An analysis about why PSformer does not work well on exchange would be helpful."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This article presents a framework for multivariate time series forecasting, integrating\nparameter sharing and a Spatial-Temporal Segment Attention mechanism. Extensive\nexperiments demonstrate that this approach consistently achieves higher accuracy and\nefficiency compared to other state-of-the-art models"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "a) This paper presents an innovative SegAtt mechanism and a parameter-sharing\napproach, effectively advancing methods in time series forecasting and aligning well with\nthe field\u2019s objectives.\n\nb) The experimental evaluation is notably thorough, offering readers a well-rounded\nperspective on the framework\u2019s performance and the contributions of its various\ncomponents.\n\nc) The writing is clear and of high quality, making the paper accessible and easy to\nunderstand."
            },
            "weaknesses": {
                "value": "a) While SegAtt demonstrates notable strengths, it would be advantageous to evaluate the\nselection of segmentation numbers across a more diverse range of datasets beyond\nETTh1 and ETTm1. Providing further guidance on practical approaches for selecting\nsegmentation numbers would also be valuable.\n\nb) The experimental section could be enhanced by further validating the framework\u2019s\nparameter-saving capacity and examining its implications for pre-trained models.\n\nc) To strengthen the robustness of the findings, incorporating results with multiple\nrandom seeds would provide additional confirmation of the framework\u2019s superior\nperformance.\n\nd) A deeper exploration of channel-mixing techniques could enrich the analysis. A\ncomparative discussion, similar to PatchTST A.7, on the benefits of channel\nindependence versus channel-mixing strategies would be particularly insightful.\n\ne) Regarding computational efficiency, could you clarify which specific components\nwithin the framework contribute to its computational advantages over other state-of-the-\nart models?"
            },
            "questions": {
                "value": "In the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces PSformer, a novel transformer-based architecture for time series forecasting that incorporates parameter sharing techniques and a Spatial-Temporal Segment Attention mechanism to capture local spatio-temporal dependencies."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(1) The introduction of parameter sharing techniques in transformer-based models demonstrates its effectiveness and potential in the field as validated by experimental results.\n\n(2) Experimental results demonstrate that PSformer achieves state-of-the-art performance in the most of the datasets."
            },
            "weaknesses": {
                "value": "(1) There are several writing errors present in the article.\n\n(2) Although the parameter sharing techniques demonstrated effectiveness according to the ablation study, the authors did not provide detailed analysis or empirical studies to further elucidate this technique, such as comparisons of convergence rates with and without parameter sharing or analysis of how parameter sharing affects model capacity."
            },
            "questions": {
                "value": "(1) Referring to **Weaknesses (2)**, could you provide a more in-depth analysis or empirical studies to illustrate the effectiveness of parameter sharing in time series forecasting?\n\n(2) According to the authors, attention is applied within each segment to enhance the extraction of local spatio-temporal relationships. However, in PSformer, a token represents a down-sampled sequence along a channel. This might be confusing because it suggests that the attention is applied to capture the global dependencies across channels. Could you provide a more detailed explanation of how the segmentation process preserves local temporal information while allowing for cross-channel interactions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "PSFormer suggests a new transformer-based model for multivariate time-series forecasting. This model utilizes parameter sharing(PS Block) and spatial-temporal segment attention(SegAtt) to decrease computational complexity while effectively modeling time and feature-wise dependency in data. SegAtt is designed to group patches located at the same positions across different variables, enabling efficient capture of local spatio-temporal dependencies. PSBlock allows the model to maintain linear path(residual connection) but also perform nonlinear transformation. As this block is shared, the model can reduce its parameter when it comes to the whole model architecture. With these advantages, PSFormer achieves strong parameter efficiency and enhances predictive accuracy across various benchmark datasets, showing greater scalability and forecasting performance than conventional Transformer models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. PSFormer reduces the number of parameters by parameter sharing, which allows to maintain both size and representational ability of the model. This design enhances the model\u2019s scalability and helps mitigate overfitting in data-scarce scenarios.\n2. PSFormer demonstrates shorter running time and smaller model size, which empirically shows efficacy of parameter sharing. \n3. The SegAtt mechanism effectively models spatio-temporal dependencies in multivariate time series by incorporating inter-variable information, boosting prediction accuracy."
            },
            "weaknesses": {
                "value": "1. The model\u2019s performance varies with different hyperparameters, making hyperparameter tuning appear crucial. \n2. While SegAtt improves performance, it may underperform in cases of univariate time series or where there is little dependency between variables.\n3. The paper briefly mentions the necessity of positional encoding, but additional experiments are needed to assess its impact on the generalization of sequences that require a strong temporal order."
            },
            "questions": {
                "value": "1. Are there any specific settings where PSFormer performs particularly well? For example, do they work better on datasets with strong correlations between variables? When looking at the benchmark datasets, it seems that the Electricity and Exchange datasets exhibited weaker performance compared to others. I am curious if there is any reason for this. \n2. Could the authors provide an ablation study comparing PSFormer\u2019s performance with and without SAM, as well as with other optimization methods such as Adam or SGD? This would clarify the specific impact of SAM on performance and provide a basis for its inclusion in the PSFormer architecture.\n3. The authors briefly explained the influence of positional encoding; however, have you conducted more specific experiments on the effects of this positional encoding on the model? I am curious about additional analysis results for various time series data. Could the authors perform experiments comparing the model\u2019s performance with and without positional encoding on different time series types (e.g., seasonal vs. non-seasonal, stationary vs. non-stationary)? Such an analysis could clarify the benefits of positional encoding in diverse time series forecasting contexts.\n4. In the ablation study, I observed the analysis results regarding the influence of the number of encoder layers and the number of segments. It seems that tuning these hyperparameters has a significant impact on the model\u2019s performance. Could the authors detail their hyperparameter tuning process, or alternatively, discuss recommended tuning approaches (e.g., grid search, random search, Bayesian optimization) tailored to PSFormer? \n5.  Could the authors provide a detailed analysis of how parameter sharing in the PS Block affects the model\u2019s temporal pattern capture? For instance, they could compare learned representations across layers with and without parameter sharing, illustrating its influence on time series modeling."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a Transformer-based model for time series forecasting (PSformer), which primarily consists of two core components: the Parameter Sharing Technique and the Spatial-Temporal Segment Attention mechanism. The former is designed to reduce the complexity of the model, while the latter is used to simultaneously model temporal dynamics and channel correlations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1: PSformer experiments involve a wide range of datasets.\n\nS2: PSformer and baselines are compared in terms of training parameters and running time."
            },
            "weaknesses": {
                "value": "W1: The paper overall lacks innovation. Parameter Sharing is merely a simple module parameter-sharing technique to reduce complexity. Additionally, Spatial-Temporal Segment Attention simply merges the channel dimension and patch size dimension together before modeling it with a Transformer.\n\nW2: What is the purpose of including the PS Block in the Segment Attention? Why not just add the PS Block after the output of the Attention, which would eliminate the need for parameter sharing? Additionally, why can two SegAttn be viewed as one FFN layer?\n\nW3: The integration of temporal and channel information in Spatial-Temporal Segment Attention may lead to incomplete capture of both time and spatial information, potentially resulting in negative effects. Could you further explain how this approach differs from the separated modeling methods like Crossformer and iTransformer, and visualize the advantages of PSformer in modeling time and channel?\n\nW4: Why is there such a significant difference between the results of PatchTST in Table 1 and the original PatchTST? Please explain.\n\nW5: Please add new baselines for comparison, such as PDF (ICLR2024) and Time-LLM (ICLR2024).\n\nW6: There are many writing errors in the paper, such as \"Moment()\" on line 125 and \"GPT4TS uses BERT as the backbone\" on line 127."
            },
            "questions": {
                "value": "See W1-W6."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}