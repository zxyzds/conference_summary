{
    "id": "uMLeOlzlZ2",
    "title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval",
    "abstract": "Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences, where reliability and reproducibility are crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and often biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can dynamically and recursively interact with computational and experimental data from the Materials Project (MP) and run atomistic simulations via high-throughput workflow interface. Without fine-tuning, LLaMP demonstrates strong tool-usage ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structure and elastic tensor), and streamline complex tasks in computational materials and chemistry. We propose a metric combining uncertainty and confidence estimates to evaluate the self-consistency of responses by LLaMP and vanilla LLMs. Our benchmark shows that LLaMP effectively mitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli, electronic bandgaps, and formation energies that seem to derive from mixed data sources. We also demonstrate LLaMP\u2019s capability to edit crystal structures and run annealing molecular dynamics simulations using pre-trained machine-learning interatomic potentials. The framework offers an intuitive and nearly hallucination-free approach to exploring and scaling materials informatics and paves the way for knowledge distillation and fine-tuning of future language models.",
    "keywords": [
        "information retrieval",
        "language model",
        "database",
        "materials informatics"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "LLaMP is a multimodal retrieval-augmented generation (RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can dynamically and recursively interact with Materials Project to ground LLMs on high-fidelity materials informatics",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=uMLeOlzlZ2",
    "pdf_link": "https://openreview.net/pdf?id=uMLeOlzlZ2",
    "comments": [
        {
            "comment": {
                "value": "We appreciate the reviewer's time and comment. Our methodological advance is the implementation of large-scale API orchestration and hierarchical agent itself, which resolves the challenges when grounding LLM on one of the largest and the most coherent source of materials database. There are many MP API endpoints having more than ten arguments https://api.materialsproject.org/docs, and the logic is highly challenging to a layman.\n\nWe stress that implementing such a large and reliable tool is far beyond trivial and important for reproducibility and pointing the field toward the right path, which many previous and concurrent work do not fulfill. This work relies on retrieval-augmented generation instead of fine-tuning/training on the MP database, and we prove that such a method is necessary to generate high fidelity/quality response that both scientists and ML practitioners should care about for the materials research pipeline. We believe that ICLR is the right stage for pointing the community to clearly see this purpose and benefiting the future work, e.g. fine tuning LLM on MP database of diverse chemistry and multimodal data (tensors, 3D crystals).\n\nThere is no training in our framework. LLaMP performs RAG on API response (so called in-context grounding/learning). We did a manual API request as our MP ground truth data, and LLaMP performs API requests autonomously without human intervention. For the bandgap of common materials, LLaMP successfully retrieves the correct data for all materials over 5 trials and hence the high confidence and low MAE, thanks to the coherence of MP database and API requests. We could see that since LLM generation is still probabilistic, LLaMP for less common cases sometimes fetches other polymorphic crystals (still valid materials satisfying the description in the query, e.g. valid chemical formula). This induces variance seen in other properties in Table 1 when such an ambiguity is always present if the query is not precise enough. This is an opportunity and limitation simultaneously, as it offers a great flexibility in user query to access diverse data on MP while if the user is not descriptive and vigilant in their query, LLaMP might request different valid data from time to time. With that said, we are to the best of our knowledge the first to quantify such variance and demonstrate that LLaMP effectively improves this.\n\nThank you again for your comment. We have added more text and self-explanatory example and revised the limitation section accordingly to address the point mentioned above. We will upload the updated manuscript all together with other revisions shortly."
            }
        },
        {
            "summary": {
                "value": "The manuscript describes the introduction and test of LLaMP, an LLM fine tuned to interact with the materials science data from the Materials Project database and run atomistic simulations via a workflow interface."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The introduced LLM is very likely of great interest for the materials science community, in particular because it is ready to be used and it is fine tuned on one of the largest and most consistent corpus of materials-science data, especially computational (atomistic-simulation) data. \nResults suggest it would be a useful tool used for materials-science research."
            },
            "weaknesses": {
                "value": "There is no methodological advance worth highlighting here. The described \"hierarchical agent framework\" is a simple device to design and implement, while the metric assessing consistency is reasonable for reporting and discussing results but does not strike as a major advance.\nIn view of these strengths and weaknesses, the paper seems more indicated for a materials-science specialized journal such as npj Computational Materials."
            },
            "questions": {
                "value": "- Figure 2, which reports with Table 1 the main results of the manuscript, is barely readable. \nBesides working harder on contrasting colors overall visibility, the authors should explain all symbols' choices and consider either reducing the amount of data shown or make the figure in more panels (possibly reporting some crucial example in the main text and the rest in an appendix).\n- in Table 1, confidence scores equal exactly to 1 (at least to the third digit precision), especially if combined with MAE equal to 0.000 (see electronic band gaps of common elements for LLaMP) look unlikely good and suggest the prediction use training data. This aspect would need discussion in presenting the results, and probably a different design of the performance tests."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose LLaMP, a multimodal retrieval-augmented generation (RAG) framework leveraging hierarchical reasoning-and-acting (ReAct) agents to dynamically interact with Materials Project (MP), arXiv, Wikipedia, and atomistic simulation tools. To reduce the hallucination of Large Language Models (LLMs), the framework provides LLMs with high-fidelity material informatics derived from various sources. Consequently, LLaMP leverages hierarchical planning and correctly retrieves higher-order materials data and also combines different modalities to perform complex, knowledge-intensive inferences and operations essential for real-world research applications. LLaMP manifests enhanced performance in predicting key material properties, real-world applications in materials science, and language-driven simulations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper presents a well-designed hierarchical ReAct agentic framework that dynamically manages multiple agents, each specialized in distinct tasks. This structured approach is highly modular, improving the model's accuracy and efficiency in handling complex workflows.\n\n1. The authors provide a comprehensive discussion of LLaMP's strengths and limitations, offering valuable insights into its potential and constraints in materials science. This balanced analysis enhances the paper's utility for related researchers by clarifying LLaMP's practical applicability and areas for further development. \n\n1. The self-correcting agent mechanism enables real-time error correction during tool usage and API interactions. This design minimizes the propagation of errors and enhances reliability in complex tasks. \n\n1. The paper not only presents benchmarking results and analysis but also discusses the real-world applications in materials science. This applied focus, combined with LLaMP's use of high-fidelity data sources, makes it highly relevant and impactful for researchers."
            },
            "weaknesses": {
                "value": "1. The framework relies heavily on data from the Materials Project, which may restrict its application to new or less-explored materials. Although MP is comprehensive, the paper acknowledges that the database's coverage is not exhaustive, particularly for certain magnetic and bandgap configurations.\n\n1. While the paper demonstrates LLaMP\u2019s applications in materials science, the analysis is primarily based on one or two examples for each application. A more comprehensive experiment or in-depth discussion of success and failure cases should be helpful for exploring real-world applications. \n\n1. The statistics for precision and CoP in Table 1 seem not consistent with the equation in Section 4.2. See the questions for details."
            },
            "questions": {
                "value": "1. According to Section 4.2, $CoP = exp(\u2212Precision)$. However, a lot of Precision and CoP results in Table 1 do not match this relationship. For example, The Precision of LLaMP on bulk modulus is $2.698$, then $exp(-2.698) = 0.067$, but the reported value is $0.900$. Can the authors clarify that?\n\n1. Have the authors tried LangGraph instead of LangChain for a more structured agentic workflow instead of a linear one? Langgraph will probably be beneficial as it may enhance the hierarchical planning in LLaMP, potentially improving agent coordination and task prioritization, especially for complex, multi-step processes.\n\n1. In the example in Figure A.1, it seems that the supervisor gives mp-9258 as the answer to the query even though the two assistant agents never return any of the relevant information. So can I interpret that the example ends up with a hallucination? Is there any method to prevent this, and why is there not an observation step to capture the mismatch between the requested and returned MP IDs and to require the refinement of the action?\n\n1. While LLaMP shows strength in typical systems (e.g., Si-O, Li-based compounds), how does it perform on less common or highly complex systems, such as multinary oxides or intermetallic compounds?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new method for LLMs (LLAMP) applied to materials science based on retrieval-augmented generation (RAG) and hierarchical reasoning-and-acting (ReAct) that addresses language-based materials science tasks. The papers starts by motivating LLAMP based on the fact that many LLMs lack access to up-to-data for solving timely materials challenges and are prone to hallucination. As such, LLAMP aims to alleviate those challenges by providing a way to infuse external data sources when solving materials science tasks. The paper claims five contributions: 1. the LLAMP method; 2. metrics for understanding self-consistency for LLMs; 3. a study for materials property prediction; 4. showcasing LLAMP capabilities in synthesis and materials structure generation; 5. demonstrating high-throughput atomistic simulations using LLAMP.\n\nNext, the paper describes related work in Section 2 and Section 3 respectively focused on the materials project, NLP in science as well as the application of prompting and tool use to solve materials science challenges. Section 4 the describes the main pieces of the LLAMP-ReAct framework, including the supervisor agent and assistant agents that help perform relevant tasks. Section 4 also outlines the metrics used in subsequent analysis, including Prediction, Coefficient of Precision, Confidence and the Self-consistency of Response (SCoR). The primary aim of the metric is the understand the consistency and confidence of LLM responses to certain queries where reliability is important. Section 5 provides the main experiments with the main focus on including materials property prediction. The paper then provides an analysis based on the metrics proposed in Section 4. The analysis generally shows that LLAMP provides lower MAE in property prediction and usually second best in SCoR. Section 5.2 also showcases a study on data retrieval for materials property analysis where LLAMP with its ReAct framework generally outperforms vanilla LLMs relying on implicit knowledge. Section 5.3 provides a brief description summarization of synthesis procedures as well as crystal generation and language-drive simulation. The discussion of the experiments in Section 5.3 is brief with much reference to the appendix for supporting information. The general claims in Section 5.3 is that LLAMP outperforms vanilla LLMs.\n\nThe paper concludes in Section 6 with a discussion on robustness, which ties into one of the main motivations outlined in the introduction, as well as limitations and a broader summary of the work and potential future work."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper introduces a new framework to relevant challenges in materials science. \n* The tools and knowledge bases included in the LLAMP framework could be useful to build upon."
            },
            "weaknesses": {
                "value": "In its current form, the paper has substantial weaknesses related to limited machine learning novelty, scant evidence of the initial claims and missing discussion of relevant related work.\n\n* Machine Learning Novelty: the paper does not provide good evidence for its machine learning novelty. The background and related work provided limited discussion of RAG and tool-calling in general and miss relevant work in related domains. As such it is difficult to understand the novelty of the LLAMP. \n* Evidence for claims: In the current form, the main addition novelty related to LLAMP focuses on RAG and ReAct and many of the presented experiments show that a RAG-based performs better a RAG-based tasks (e.g., information retrieval) compared to a non-RAG model. As such, the only claim that is somewhat properly supported is that a RAG-based model is somewhat better and more consistent on tasks where RAG is useful, which is not particularly surprising given prior success of RAG. Additionally the paper provides scant evidence for claims 4 & 5 in the main text with much reference for the appendix. This is not (in my opinion) best practice and weakens the initial claims made in the introduction.\n     * The claims related to the ReAct framework could also be strengthened by adding an ablation on tool calling - especially since the paper mentions flat tool calling.\n* Related work: The paper provides a limited discussion of relevant work related to machine learning for relevant parts of the proposed method, including RAG and tool-calling. This extends to LLMs for chemistry and materials science where methods, including agent methods, have been proposed [1] [2]. The section on NLP for science could also benefit from a broader inclusion of work on applying LLMs for question-answering tasks in materials science [3] [4] [5] along with additional work on LLMs for materials science data extraction [6] [7 - mara].\n\n[1] Ghafarollahi, Alireza, and Markus J. Buehler. \"AtomAgents: Alloy design and discovery through physics-aware multi-modal multi-agent artificial intelligence.\" arXiv preprint arXiv:2407.10022 (2024).\n\n[2] Zhang, Huan, et al. \"Honeycomb: A flexible llm-based agent system for materials science.\" arXiv preprint arXiv:2409.00135 (2024).\n\n[3] Yu Song, Santiago Miret, and Bang Liu. 2023. MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3621\u20133639, Toronto, Canada. Association for Computational Linguistics.\n\n[4] Zaki, Mohd, and NM Anoop Krishnan. \"MaScQA: investigating materials science knowledge of large language models.\" Digital Discovery 3.2 (2024): 313-327.\n\n[5] Yu Song, Santiago Miret, Huan Zhang, and Bang Liu. 2023. HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5724\u20135739, Singapore. Association for Computational Linguistics.\n\n[6] Hira, Kausik, et al. \"Reconstructing the materials tetrahedron: challenges in materials information extraction.\" Digital Discovery 3.5 (2024): 1021-1037.\n\n[7] Schilling-Wilhelmi, Mara, et al. \"From text to insight: Large language models for materials science data extraction.\" arXiv preprint arXiv:2407.16867 (2024)."
            },
            "questions": {
                "value": "* How are the agents distinguished from each other? Do they receive different prompts, different context? It seems like not all of them are LLM based - it would be good to describe more details on what is considered an agent and what is considered a tool.\n* Have you compared to other RAG methods? It seems a bit unfair to have to benchmark LLMs solely on implicit knowledge. It would also be good to get additional context on how your proposed tool calling method compares to other methods [8]\n* How limited are the proposed methods to Materials Project? What would be needed to apply it to other materials science knowledge sources?\n* Could you add the main conclusions in the captions of figures and tables for greater clarity?\n\n[8] Qin, Yujia, et al. \"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs.\" The Twelfth International Conference on Learning Representations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes LLaMP, a RAG framework based on hierarchical ReAct agents. The supervisor agent handles high level logic while assistant agents interact with Materials Project, arXiv, Wikipedia and atomistic simulation tools. \n\nThe authors then introduce a metric, the Self-Consistency of Response (SCoR), which aggregates the model's confidence in answering with the variability of the answers. \n\nExperiments show that LLaMP achieves a low MAE and high SCoR in generating Bulk Modulus, Formation Energy, and Electronic Bandgap while LLMs and other RAG frameworks achieve higher MAEs and comparable or lower SCoR. \n\nLLaMP is also able to predict the magnetic orderings and total magnetisation, retrieve inorganic synthesis recipes, edit crystal structures and drive simulations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**Originality**\nLLaMP is original in its use of hierarchical agents and the combination of agents used. The metrics and combination of tasks is also original. \n\n**Quality**\nThe hierarchical orchestration is well thought out and seems to perform well. The variety of agents makes the framework useful for materials scientists. The SCoR is mostly well justified. The experiments are convincing. The variety of tasks used is a bonus.\n\n**Clarity**\nThe paper is easy to read  and well organised. Figures and Tables are informative (except Figure 2, see Weaknesses).\n\n**Significance**\nAccuracy of answers and reducing hallucinations is important when using LLMs in a scientific context. LLaMP is a significant step towards addressing these issues and thus is a valuable contribution to the community."
            },
            "weaknesses": {
                "value": "1) The name precision in SCoR feels misleading. When SCoR = 1, the model generates the same response to queries, meaning that the standard deviation is zero and also the precision is zero. This is counter-intuitive as I think of precision as True Positives / (True Positives + False Positives).\n2) Figure 2 is unclear. How should I read the values produced by each method? What is the ground truth?\n3) (Minor) I can't find a citation for LangChain."
            },
            "questions": {
                "value": "1) Can you please elaborate on how the standard deviation in SCoR is calculated? I assume that it only works for numerical answers and it's the standard deviation of the population.\n2) Can you please clarify Figure 2. What are the ground truth values?\n3) Can you provide code to reproduce the framework? This would add value to your submission."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}