{
    "id": "eIJfOIMN9z",
    "title": "Language Representations Can be What Recommenders Need: Findings and Potentials",
    "abstract": "Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields.\nHowever, in the recommendation domain, it remains uncertain whether LMs implicitly encode user preference information. Contrary to prevailing understanding that LMs and traditional recommenders learn two distinct representation spaces due to the huge gap in language and behavior modeling objectives, this work re-examines such understanding and explores extracting a recommendation space directly from the language representation space.\nSurprisingly, our findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance.\nThis outcome suggests the possible homomorphism between the advanced language representation space and an effective item representation space for recommendation, implying that collaborative signals may be implicitly encoded within LMs.\nMotivated by the finding of homomorphism, we explore the possibility of designing advanced collaborative filtering (CF) models purely based on language representations without ID-based embeddings.\nTo be specific, we incorporate several crucial components (i.e., a multilayer perceptron (MLP), graph convolution, and contrastive learning (CL) loss function) to build a simple yet effective model, with the language representations of item textual metadata (i.e., title) as the input.\nEmpirical results show that such a simple model can outperform leading ID-based CF models on multiple datasets, which sheds light on using language representations for better recommendation.\nMoreover, we systematically analyze this simple model and find several key features for using advanced language representations:\na good initialization for item representations, superior zero-shot recommendation abilities in new datasets, and being aware of user intention.\nOur findings highlight the connection between language modeling and behavior modeling, which can inspire both natural language processing and recommender system communities.",
    "keywords": [
        "Collaborative filtering",
        "Language-representation-based recommendation",
        "Language models",
        "Language model representations"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=eIJfOIMN9z",
    "pdf_link": "https://openreview.net/pdf?id=eIJfOIMN9z",
    "comments": [
        {
            "summary": {
                "value": "This paper explores the relationship between language space and behavior space in the context of recommendation systems, investigating the potential of using language representations for this purpose. The proposed method, AlphaRec, demonstrates excellent performance in zero-shot recommendation scenarios."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow.\n2. Leveraging textual information for recommendations and exploring zero-shot performance is both novel and interesting.\n3. The experiments are comprehensive and effectively demonstrate the proposed method's effectiveness."
            },
            "weaknesses": {
                "value": "1. It would be beneficial to conduct an additional ablation study to evaluate different types of text embedding models on AlphaRec."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper challenges the idea that Language Models (LMs) and traditional recommenders exist in separate representation spaces. It proposes deriving a recommendation space directly from language representations. By mapping item representations from advanced LM outputs, it shows improved recommendation performance, hinting at a homomorphism between language and item representation spaces. The paper introduces collaborative filtering models based solely on language representations, excluding ID-based embeddings. Components like multilayer perceptrons, graph convolution, and contrastive learning are integrated into the model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The authors propose using linear mapping of LLMs' representations to directly model user preferences, which is both efficient and effective.\n\n2. The authors conducted comprehensive experiments discussing the performance of AlphaRec in full-shot, zero-shot, and other scenarios, comparing it with baseline methods from various research lines."
            },
            "weaknesses": {
                "value": "1. From Table 1, it can be observed that the performance of AlphaRec is heavily dependent on the performance of the chosen language model. When selecting smaller language models (such as BERT, RoBERTa, etc.), the performance is less than ideal. Additionally, the costs associated with state-of-the-art methods (i.e., SFR-Embedding-Mistral, text-embeddings-3-large, etc.), including inference costs or API call expenses, cannot be overlooked, which weakens the applicability of this method.\n\n2. The authors omitted a method that performs well in zero-shot scenarios, VQ-Rec [1], when exploring zero-shot capabilities.\n\n3. In the ablation study section, it can be observed that AlphaRec without Contrastive Learning (CL) performs significantly worse than Linear Mapping, indicating that simply adding MLP and GCN on top of Linear Mapping does not lead to effective performance improvements.\n\n4. If LLMs' representations and MLP are applied to state-of-the-art ID-based methods (e.g., XSimGCL) to replace the original ID embeddings, would it achieve results comparable to those of AlphaRec?\n\n[1] Hou, Yupeng, et al. \"Learning vector-quantized item representation for transferable sequential recommenders.\" Proceedings of the ACM Web Conference 2023. 2023."
            },
            "questions": {
                "value": "Please refer to the issues mentioned in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the potential of leveraging advanced language models (LMs) for recommendation tasks, exploring whether language representations can encode user preferences and collaborative signals. Traditionally, CF-based recommendation systems use ID-based embeddings to model user-item interactions. The authors propose a method that directly maps language representations of item metadata (like titles) into a behavior space to serve as effective item representations. They introduce a linear mapping approach, followed by the AlphaRec model, which combines language embeddings with collaborative filtering elements such as graph convolution and contrastive loss.\n\nGenerally, this is a very good paper provides new insights in the community: homomorphism exists between LM- based language space and user behavior space for recommendation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The topic on studying the relationship between the language space and behavior space is important to the community. The authors provide new insights to show the homomorphism between the two spaces.\n\n2. The paper\u2019s core contribution, mapping language representations into a behavior space, is thoroughly tested and demonstrates strong empirical performance. This novel approach could reduce reliance on large datasets of historical behavior data, making the method highly applicable.\n\n3. Followed by the observation through linear mapping, the authors also propose a stronger model, AlphaRec, through non-linear mapping and GNN. Its ability to perform zero-shot recommendations and its efficient training process are well-demonstrated in multiple datasets, addressing common challenges in recommendation systems where user or item IDs change frequently. \n\n4. The paper\u2019s evaluation covers a range of experimental setups, comparing multiple LMs, testing robustness to input noise, and conducting ablation studies. The discussion on the potential of language-based user representation (Section 5) is promising."
            },
            "weaknesses": {
                "value": "1. While the paper presents a compelling case for using language representations in recommendation, a discussion on how AlphaRec compares to existing fine-tuned LLM approaches would strengthen it further. For example, it would be helpful to understand whether the authors see AlphaRec as complementary or competitive with fine-tuned LLM-based approaches (e.g., [1]), and if they anticipate similar benefits, limitations, or unique strengths compared to those models.\n\n[1] Tennenholtz et al. Demystifying Embedding Spaces using Large Language Models. ICLR 2024.\n\n\n2. The authors assume that a single linear mapping can capture collaborative signals for all users, which may overlook the diversity in individual user preferences and linguistic nuances. Different users might interpret language in varied ways, and a one-size-fits-all mapping could limit the model\u2019s ability to fully capture such personalization and diversity. How might the authors address this challenge within their framework? Exploring the possibility of user-specific mappings or personalization approaches could be a valuable direction for future work, potentially enhancing the model\u2019s adaptability across diverse user groups."
            },
            "questions": {
                "value": "The discussion on the potential of intent-aware ability is interesting. However, as indicated in Appendix D.3.1, a fixed user intention query is generated for each item in the dataset. I would argue that this is not optimal, as different users may have different intents even when consuming the same item. Could the authors consider extending their approach to allow for dynamic query generation or user-specific intent modeling, potentially improving recommendation relevance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the relationship between the language space generated by LLMs and the behavior space produced by CF models. The authors found that the two spaces exhibit a pattern of homomorphism. Based on this finding, they proposed AlphaRec, which uses item embeddings exclusively from the language space and employs a trainable projector to map them into the behavior space for recommendations. The results demonstrate that with language space-initialized embeddings, the model significantly outperforms traditional ID-based methods and other LLM-based approaches in general recommendation, zero-shot recommendation, and intention-aware recommendation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The questions explored in this paper are novel and meaningful. While many existing works have analyzed the recommendation capabilities of language models, very few have empirically investigated the differences between the representation spaces of LMs and CF models.\n\n- The evaluations are comprehensive. The authors not only investigate the recommendation capabilities of AlphaRec but also examine its performance in zero-shot and intention-aware recommendations. Additionally, the visualization of the LM and behavior spaces is a plus."
            },
            "weaknesses": {
                "value": "- There are some concerns that affect the soundness and the conclusions presented in the paper (see point 2 of weakness and questions below).\n\n- Some of the comparisons are not entirely fair. For example, in Figure 3 and Table 3, it is evident that when using BPR loss (which is used in LightGCN), AlphaRec\u2019s performance is a bit worse than that of LightGCN. Given that AlphaRec shares a similar graph convolution architecture with LightGCN in its nonlinear projection, it appears that AlphaRec\u2019s advantage over LightGCN comes from the InfoNCE loss rather than the language representations. I am wondering whether such result can support the power of language representations. By the way, I suggest aligning the implementations of KAR and RLMRec closer with AlphaRec, including the loss function and some of the modules. Without a fair comparison, it is hard to conclude that the combination of ID-based embeddings and language representations is less suitable than using pure language representations."
            },
            "questions": {
                "value": "- Since MovieLens and Amazon Review are classical and widely used datasets publicly available on the Internet, I am concerned that they might already be part of the pretraining data for some LLMs, hence the user preference information encoded in LMs could be a potential data leakage. This could explain why BERT-style models perform worse than more recent LMs. To verify this assumption, can authors evaluate AlphaRec\u2019s performance on a private or newly published recommendation dataset?\n\n- If authors want to claim the collaborative signals are already encoded in the language space, I wonder if authors can show the results using original language representations without any linear or nonlinear transformations? If there is valid information can show user behaviour in language space, the performance should be comparable to some traditional CF methods (like MF) or heuristic methods (e.g., Pop)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}