{
    "id": "NPDnRLFhc0",
    "title": "EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers",
    "abstract": "We study the task of finding evidence for a hypothesis in the biomedical literature. Finding relevant evidence is a necessary precursor for evaluating the validity of scientific hypotheses, and for applications such as automated meta-analyses and scientifically grounded question-answering systems. We develop a pipeline for high quality, sentence-by-sentence annotation of biomedical papers for this task. The pipeline leverages expert judgments of scientific relevance, and is validated using teams of human annotators. We evaluate a diverse set of language models and retrieval systems on the benchmark, which consists of more than 400 fully annotated papers and 700k sentence judgments. The performance of the best models still falls significantly short of expert-level on this task. To show the scalability of our annotation and hypothesis generation pipeline, we created a larger EvidenceBench-100K containing 100,000 fully annotated papers with hypotheses and 150 million sentence judgments. Empirically, we show that fine-tuning embedding and language models on EvidenceBench-100k significantly improved their performance on the original EvidenceBench, reaching state-of-the-art embedding model performance and closely trails behind Large Language Models. By providing a standardized benchmark and evaluation framework, this work will support the development of tools which automate evidence synthesis and hypothesis testing, as well as the long-context global reasoning and instruction-following abilities for Large Language Models (LLM) and embedding-based IR systems. The dataset is available at \\href{https://github.com/EvidenceBench/EvidenceBench}{https://github.com/EvidenceBench/EvidenceBench}.",
    "keywords": [
        "Biomedical Benchmark",
        "Scientific Information Retrieval",
        "Scientific Information Extraction",
        "Large Language Models",
        "BioNLP"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "A large-scale BioNLP benchmark for evaluating and training models on the task of Evidence Retrieval for hypotheses",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=NPDnRLFhc0",
    "pdf_link": "https://openreview.net/pdf?id=NPDnRLFhc0",
    "comments": [
        {
            "summary": {
                "value": "This paper presents EvidenceBench, which is a benchmark for finding arguments supporting or against a hypothesis. They find hypotheses inside survey, and find supporting arguments from the same survey, and the original paper. \nThis paper also proposes metrics for the task and compares many LLMs' performance on the task. In addition, this paper also provides the fine-tuning results on the benchmark."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Identifying arguments supporting or against an argument is an important task.\n2. This paper proposes an annotation pipeline, which might be helpful for future tasks."
            },
            "weaknesses": {
                "value": "1. The paper spends too much effort on constructing the benchmark, but not many insights are provided through the experiment section.\n2. The writing can be largely improved. There's many places in the writing that are vague and not clear. \nFor example, the second paragraph in the introduction section: \n\"We consider the goal of understanding what is known in the literature about a scientific hypothesis.\nThis can be broken into several stages: searching for relevant papers; extracting information from\nthese papers; and aggregating this information. Our work focuses on the second stage.\"\nMy questions are:\na). what is precisely the research goal in terms of \"understanding what is known in the literature about a scientific hypothesis\"?\nb). why it can be broken into these three stages?\n\nIn line 047~048: \"These annotations are judgments which are ordinarily made by domain experts, and the benchmark should be faithful to these judgments\": what does it mean by \"the benchmark should be faithful to these judgements\"?\n\nIn line 048~050: \"Third, despite the complexity of annotation, the benchmark construction process should be scalable, providing a sufficient number of examples to accurately measure system performance\": what does it mean by \"should be scalable\"? what does it mean by \"providing a sufficient number of examples to accurately measure system performance\"? what is the relation between these two arguments?\n\nI found it very hard to read through the paper. I would suggest the authors for a major revision at least in terms of the writing."
            },
            "questions": {
                "value": "Can you provide more insights or knowledge that we can learn?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the task of finding evidence for a hypothesis. The authors develop a pipeline for annotating biomedical papers for this task. Using the annotation pipeline, the authors build a benchmark of more than 400 papers. Additionally, the authors create a larger dataset containing 100K papers.\nThe authors also run experiments to evaluate the effectiveness of different approaches to the proposed task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The authors propose a practical task and evaluate the effectiveness of existing approaches to the task\n* A useful resource for benchmarking LLMs on the proposed task\n* The paper is well-structured and easy to follow, although some presentations can be further improved"
            },
            "weaknesses": {
                "value": "There is no strong reason to reject the paper, although some issues related to clarity and presentation need to be improved.\n\n* The concept of `study aspect` is confusing, and I am unsure how the evaluation procedure considers it. For example, if a sentence is retrieved for the wrong aspect or multiple aspects.\n* Based on the task definition, the hypothesis is given. However, this may not be the case in the real world. It would be nice to see these tested models' sensitivity to the modified (paraphrasing) hypothesis.\n* The difference between `EvidenceBench` and `EvidenceBench-100k` is unclear (seems with or without human validation?)\n* The authors exclude figure and table, which might be very relevant and important for the proposed task."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces EvidenceBench, a scalable annotation pipeline designed for extracting and aligning evidence with specific hypotheses in biomedical literature. Study aspects and hypotheses are initially extracted from systematic reviews. A specialized alignment annotator then performs sentence-level annotations to link each piece of evidence directly to the corresponding hypothesis. To validate this approach, the authors use the pipeline to generate the expansive EvidenceBench-100k benchmark. Fine-tuned on this benchmark dataset, embedding models showed improved performance in the 'Result ER@Optimal' task, showing this standardized benchmark and evaluation framework will support the development of tools for automate evidence synthesis and hypothesis testing."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "o\tThe original EvidenceBench evaluated the performance of selected LLMs and embedding models on evidence retrieval tasks and compared different prompting strategies, which revealed that GPT-4o is the SOTA LLM, and that embedding models underperform due to a lack of context awareness.\n\no\tThe EvidenceBench-100k fine-tuned E5-v2 model and Llama3-8B significantly improved on the result evidence retrieval task but trailed behind larger models, validating the effectiveness of the benchmark dataset.\n\no\tThe author presented the topic and their framework well, with detailed descriptions and clear figures illustrating the overall problem and their area.\n\no\tThis paper innovatively developed the pipeline for evidence retrieval for a given hypothesis and further annotated biomedical papers at the sentence level for better meta-analysis.\n\no\tThe authors conducted comprehensive experiments with both open-sourced and closed-sourced LLMs, and a small language model. Human experts were involved to validate study aspects generation and automate sentence annotation to enhance trustworthiness."
            },
            "weaknesses": {
                "value": "o\tThe authors claimed that their method using the SOTA LLMs reduces construction time from over 3,000 human hours for EvidenceBench to 3 hours. However, there is no evidence provided regarding how the 3 hours were concluded. Additionally, GPT4-0125, GPT4-o-mini, and Claude3-Opus are used during data generation without explanations of when to choose which.\n\no\tNo ablation study regarding how topics can influence the study aspect and hypothesis extraction is provided. The subsets used for experiments are randomly selected without considering the distribution of topics.\n\no\tThe experiments are conducted on a subset of the test dataset. Although EventBench-100K is comprehensive and large, only 300 data points are used to evaluate LLMs, which is a very small portion."
            },
            "questions": {
                "value": "o\tHow is the 3 human hour for construction using this pipeline concluded? Why are different LLMs (GPT4-0125, GPT4-o-mini, and Claude3-Opus) selected as the tool at different stage of benchmark data creation?\n\no\tThe summary extraction from study monographs is performed by LLMs, without human inspection. The summaries are used as input to LLMs for the recovery of hypotheses and decomposition of aspects. Although the aspect decomposition is inspected by humans, how is the summary extraction validated to avoid error propagation?\n\no\tIn Task Definition, both versions of the task define the desired sentences as evidence for or against a hypothesis. It is difficult to discern whether sentences classified as counter-evidence in relation to one hypothesis might be more appropriately considered as supportive of an alternative hypothesis. Given the structured nature of the tasks, are there any experiments or human validations conducted to address and clarify these categorizations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper released a new dataset for evidence extraction from biomedical papers, which requires some ethic evaluations of the dataset creation."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new task of finding evidence for a hypothesis. The authors built a large-scale dataset with reasonable costs using existing survey monographs and LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The large-scale datasets are constructed using LLMs with fewer budgets.\n- Several evaluations of existing and fine-tuned models are provided and compared, showing the usefulness of the benchmark dataset for evaluating current LLMs' abilities."
            },
            "weaknesses": {
                "value": "- It needs to be clarified how the hypotheses from survey monographs are generally helpful.\n- The authors expect to provide immediate value to scientists as the first desiderate of the benchmark (lines 44-45), and the dataset creation involves several experts. Still, no manual analyses are provided for the results, so whether the results benefit scientists is unclear. \n- The proposed task focuses on the limited part of the practical problem; the task expects the candidate pool and needs to consider cases with evidence and with (the retrieval results of) the considerable paper pool (e.g., the entire PubMed database).\n\nTypo:\n- Line 509, Figure 3 should be Table 6."
            },
            "questions": {
                "value": "- Hypotheses are taken from survey monographs, so they can differ from natural hypotheses the experts consider. Is there any evaluation of the dataset that evaluates the suitableness of the instances for evidence retrieval?\n- What happens if the experts think of the hypotheses with no evidence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}