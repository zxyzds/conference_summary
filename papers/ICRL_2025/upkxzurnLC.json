{
    "id": "upkxzurnLC",
    "title": "Learning on One Mode: Addressing Multi-Modality in Offline Reinforcement Learning",
    "abstract": "Offline reinforcement learning (RL) seeks to learn optimal policies from static datasets without interacting with the environment. A common challenge is handling multi-modal action distributions, where multiple behaviours are represented in the data. Existing methods often assume unimodal behaviour policies, leading to suboptimal performance when this assumption is violated. We propose Weighted Imitation Learning on One Mode (LOM), a novel approach that focuses on learning from a single, promising mode of the behaviour policy. By using a Gaussian mixture model to identify modes and selecting the best mode based on expected returns, LOM avoids the pitfalls of averaging over conflicting actions. Theoretically, we show that LOM improves performance while maintaining simplicity in policy learning. Empirically, LOM outperforms existing methods on standard D4RL benchmarks and demonstrates its effectiveness in complex, multi-modal scenarios.",
    "keywords": [
        "Offline reinforcement learning",
        "weighted imitation learning",
        "multi-modality."
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We propose weighted imitation Learning on One Mode (LOM) to address the multi-modality problem in offline reinforcement learning.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=upkxzurnLC",
    "pdf_link": "https://openreview.net/pdf?id=upkxzurnLC",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces Weighted Imitation Learning on One Mode (LOM), a novel approach for offline reinforcement learning that addresses the challenges of multi-modal action distributions by focusing on the single mode with the highest expected return. Using a Gaussian mixture model to identify modes and a hyper Q-function for mode selection, LOM significantly improves performance in multi-modal environments, outperforming other methods on standard benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper is well-organized and easy to follow.\n2. The paper proposes an innovative approach to handling multi-modal action distributions in offline reinforcement learning, which provides valuable insights for research of muti-modality in RL, by focusing on the most promising action mode and using a Gaussian mixture model to identify modes.\n3. This paper presents extensive experiments validating the effectiveness of LOM across diverse benchmarks. The results demonstrate that LOM outperforms state-of-the-art offline RL algorithms in multi-modal settings."
            },
            "weaknesses": {
                "value": "1. Appendix is missing. What I read is a 13-page paper without an appendix. However, the paper states that the proofs of the theorems are in the appendix, so I guess the author made an error when uploading their paper. It is hard to evaluate the quality of this paper.\n2. For the theoretical perspective, the result that $V^{\\pi^L}(s) \\geq V^{\\pi^{\\zeta_g}}(s) \\geq V^{\\pi^b}(s)$ is relatively weak, Is it possible to provide a lower bound for the gap between the value functions, or could the author explain the difficulties in doing so?\n3. The effectiveness of LOM depends on the choice of the number of Gaussian components (M) in the Gaussian Mixture Model. Have the author tried any automatic model selection techniques for GMMs, such as the Bayesian Information Criterion or cross-validation, and how these might be applied to LOM?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents LOM, an offline imitation learning approach designed to address the multi-modality of batch datasets. The method employs a Gaussian mixture model to capture the behavior policy, selecting the optimal mode based on expected returns. Subsequently, it estimates the advantage by leveraging information from the selected mode and incorporates this advantage into a weighted imitation learning objective using an exponential weighting scheme. The approach demonstrates competitive performance against baselines on the D4RL benchmark."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow. \n\n2. It effectively leverages the multimodal nature of the data to design a practical method."
            },
            "weaknesses": {
                "value": "1. While the method appears broadly applicable and versatile, it closely resembles existing techniques, such as weighted advantage imitation learning and multi-modal policy networks. The approach mainly combines established methods, making the contribution appear incremental. \n2. Furthermore, although the author analyzes how the number of mixture components impacts the results, the experimental findings require additional clarification. For instance, in Section 6.3, the statement \u201cWhen M is too large, it results in overly narrow domains of action learning, which may exclude potentially useful actions from the dataset\u201d would benefit from a more thorough explanation, including potential reasons behind this effect. Without such elaboration, the analysis may feel incomplete."
            },
            "questions": {
                "value": "1. In mode selection, the greedy policy is applied. Is this the optimal strategy for choosing the mode? I wonder if a more conservative approach might yield better results. Could the author elaborate more about this choice?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the multi-modality problem in offline reinforcement learning (RL). It proposes to first perform model selection and then further improve the greedy hyper policy that captures the best mode."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I enjoy reading this paper. I think its key contribution is learning on one mode. By doing this, the KL-divergence term in offline policy improvement would not burden the policy to cover the entire support distribution generated by the behavior policy.\n\nThe key problem lies in the fact that, in the behavior data, the action distribution has some clear modes that should be represented by discrete values and some continuously distributed probability mass within each mode. The paper proposes dealing with these two aspects using different approaches. For the first aspect, it involves selecting between different modes to ensure the learned policy remains within the offline data distribution. For the second aspect, it involves using offline policy improvement with KL-divergence to constrain the continuous action distribution."
            },
            "weaknesses": {
                "value": "No clear weaknesses."
            },
            "questions": {
                "value": "I am curious about the theoretical aspect. Clearly, this paper employs a two-step improvement process. Empirically, it seems reasonable that this approach would perform better than doing just the second step without the mode selection. However, I am wondering if there is any theoretical analysis comparing the two-step improvement with only performing offline policy improvement in the second step. Which approach do you think would yield better results theoretically?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an offline reinforcement learning algorithm that accounts for the multimodal distribution of actions within a given dataset. In the proposed algorithm, the behavior policy is modeled as a Gaussian mixture policy, and the mode yielding the highest expected return is identified based on an approximated Q-function associated with the behavior policy. The policy is then learned using an advantage-weighted actor-critic approach, guided by actions generated by the mode of the behavior policy that maximizes the expected return.\n\nThe proposed method was evaluated on standard D4RL tasks and custom tasks based on Fetch tasks with multimodal datasets. Experimental results show that the proposed method outperformed baseline methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The idea is interesting and reasonable. While existing methods often focus on enhancing the flexibility of the policy model, the proposed approach aims to extract a unimodal distribution from a multimodal dataset.\n\n- In the experiments, the proposed method outperformed the baseline methods."
            },
            "weaknesses": {
                "value": "- The comparison with related methods is insufficient. While the proposed approach is interesting for its aim to extract a unimodal action distribution from a given dataset, several methods enhance policy expressiveness to handle multimodal action distributions. Using a Gaussian mixture model is a straightforward and intuitive approach. A recent study explores Gaussian mixture policies and mixtures of deterministic policies based on AWAC (https://openreview.net/forum?id=zkRCp4RmAF). Additionally, diffusion models offer a promising approach (https://openreview.net/forum?id=AHvFDPi-FA). Comparisons with these methods should be included in the experiments.\n\n- Hyperparameters used in the experiments are not provided. Please provide a table or appendix section detailing the hyperparameters used for each experiment, including any tuning process."
            },
            "questions": {
                "value": "- Why are LAPO, WCGAN, and WCVAE absent from the experiments with extremely multimodal datasets? These methods should be compared to evaluate their capability in handling multimodal action distributions.\n\n- Is it possible to include Diffusion QL and AWAC with a Gaussian policy for comparison? These methods should also be included in the experiments with extremely multimodal datasets.\n\n- In the attached code, I noticed that the loss function for the GMM might be incorrect. The provided code shows the loss as $\\mathcal{L} = \\sum_i ( \\log w_i + \\log p_i$ ) where $p_i$ is a Gaussian component.\nHowever, the likelihood in a GMM is given by $\\sum w_i p_i $ and its log likelihood is $\\log ( \\sum w_i p_i )$, which differs from $ \\sum_i ( \\log w_i + \\log p_i$ )$.  Was this modification made intentionally or by mistake? If it was intentional, please provide the rationale.\n\n- How sensitive is the proposed algorithm to the number of Gaussian components? Please provide an ablation study or sensitivity analysis showing how performance changes with different numbers of Gaussian components across various tasks or datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}