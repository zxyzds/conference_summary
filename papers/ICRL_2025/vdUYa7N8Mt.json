{
    "id": "vdUYa7N8Mt",
    "title": "The Rate-Distortion-Perception Trade-Off with Algorithmic Realism",
    "abstract": "Realism constraints (or constraints on perceptual quality) have received considerable recent attention within the context of lossy compression, particularly of images. Theoretical studies of lossy compression indicate that high-rate common randomness between the compressor and the decompressor is a valuable resource for achieving realism. On the other hand, the utility of significant amounts of common randomness at test time has not been noted in practice. We offer an explanation for this discrepancy by considering a realism constraint that requires satisfying a universal critic that inspects realizations of individual compressed images, or batches thereof. We characterize the optimal rate-distortion-perception trade-off under such a realism constraint, and show that it is asymptotically achievable without any common randomness, unless the batch size is impractically large.",
    "keywords": [
        "lossy compression",
        "perceptual quality",
        "rate-distortion-perception trade-off",
        "randomization",
        "universal critics"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vdUYa7N8Mt",
    "pdf_link": "https://openreview.net/pdf?id=vdUYa7N8Mt",
    "comments": [
        {
            "summary": {
                "value": "The paper concerns with the rate-distortion-perception tradeoff (RDP) in the context of lossy compression and argues that previous theoretical results, which suggest that common randomness between the encoder and the decoder is crucial for good performance, do not accurately reflect how humans perceive realism. To address this, the authors reformulate the RDP with reaslim constraints by adopting the concept of universal critic that generalizes no-reference metrics and divergences and insecpt batches of samples. Under this framework, they prove that near-perfect realism is achievable without common randomness unless the batch size is impractically large and the proposed realism measure reduces to a divergence."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper provides a novel perspective on the rate-distortion-perception tradeoff by adopting the concept of universal critics.\n* The paper presents rigorous theoretical analysis and proofs to support its claims.\n* The theoretical finding that near-perfect realism is achievable without common randomness has significant practical implications for lossy compression."
            },
            "weaknesses": {
                "value": "While the paper presents a novel and potentially impactful contribution, its clarity and accessibility are hindered by a dense presentation style. The heavy use of technical notation and the lack of illustrative examples make it challenging to grasp the core concepts and implications of the proposed framework.\n\nSpecifically, the paper would benefit from:\n\n* More explanatory discussions: For instance, a concise discussion following Definition 3.3 would clarify the meaning and significance of the new formulation in comparison to the original RDP framework.\n\n* Illustrative examples: Simple case studies or visual examples would help readers understand the practical implications of the theoretical results. The authors could consider drawing inspiration from the original RDP paper by Blau & Michaeli, which effectively uses examples to convey its ideas.\n\nAddressing these issues would make the paper more accessible to a wider audience and increase its impact. While the core contribution merits acceptance, I strongly encourage the authors to revise the paper with a focus on clarity and illustrative examples."
            },
            "questions": {
                "value": "1. Does common randomness offer any benefits beyond perceptual realism in lossy compression? For example, stability/robustness? \n2. How does the achievable rate-distortion-perception tradeoff change as a function of the batch size used by the universal critic?  Does this analysis offer any insights into selecting an appropriate batch size for training generative compression models that aim to satisfy perceptual quality constraints?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The study addresses a core issue in lossy image compression: achieving high perceptual quality in the decompressed images while minimizing distortion and compression rate. A unique aspect of this paper is its focus on algorithmic realism \u2014 a concept that considers human perception and aims to create compressed images that appear realistic to a critic. This builds on prior work on the rate-distortion-perception (RDP) trade-off, but instead of relying heavily on common randomness, it introduces a framework that reduces or eliminates the need for shared randomness between encoder and decoder in practical settings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Interesting Insight into Realism Constraints: By redefining perceptual realism through an algorithmic lens, the paper provides a fresh perspective on the RDP trade-off and its practical applications in lossy compression.\n2. Reduced Dependency on Common Randomness: The finding that common randomness is only needed in impractically large batches addresses a significant gap in previous theoretical predictions versus experimental observations.\n3. Good Theoretical Foundation: The study provides rigorous proof and aligns well with information theory, making it a valuable resource for researchers interested in theoretical advances in compression."
            },
            "weaknesses": {
                "value": "While the paper provides rigorous theoretical derivations and proofs, one significant limitation is the lack of practical illustrations or implementations that could help readers appreciate the impact and contributions of the proposed framework in real-world applications. The authors claim that algorithmic realism simplifies the practical attainment of the rate-distortion-perception (RDP) trade-off by reducing the dependency on common randomness between encoder and decoder. However, without practical visualizations or demonstration attempts, it becomes challenging for readers to intuitively evaluate the work\u2019s contributions.\n\nThough I acknowledge the value of theoretical derivations, the paper appears incomplete and, consequently, less persuasive without empirical validation. I strongly recommend that the authors complement their theoretical results with practical experiments, such as specific implementations, visual examples, or a demonstration. This would significantly enhance the paper\u2019s credibility and provide readers with a tangible understanding of the theory\u2019s implications.\n\nTo make these points more specific, I propose the following questions:\n\n **1. Evaluation of Practical Applicability**\n\nThe paper offers extensive theoretical proofs, yet there is no concrete implementation provided to demonstrate how this framework could be integrated into real-world image compression tasks. Could the authors consider validating the proposed approach on an actual compression system to illustrate its practical efficacy?\n\n **2. Feasibility of Reducing Common Randomness.**\n\nWhile the theory is sound, it would benefit from an empirical investigation to verify that reducing common randomness does not detract from visual quality. Without experimental validation, how can readers assess the applicability of these theoretical findings to practical compression systems?\n\n   **3. Experimental Support for Theory-Practice Connection**\n\n The paper\u2019s theoretical framework is detailed but lacks experimental applications or use cases. Could the authors consider providing experiments to demonstrate the balance of visual quality and compression rate achieved by the proposed approach?\n\n   **4. Inclusion of Visual Case Studies**\n\n Given the claims of practical feasibility, is it possible to provide specific examples of compressed and decompressed images to offer readers a more direct perception of the quality improvement achieved by the proposed approach?\n\nThese additions would substantially enhance the paper by bridging the gap between theoretical results and their practical impact, allowing readers to more fully appreciate the contributions of this work."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a new rate-distortion-perception function and proves its achievabiliy and converse, in both zero-shot and asymptotical setting. The proposed RDP function replace the P from divergence to a realism measure defined by authors. The propose RDP function is achievable without common randomness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "It is great to have a RDP which is achievable without randomness. Afterall, the human eye distinguishs images in a per-image setting without randomness. The proposed RDP is better aligned to human perception in this sense. I have not went through the details of proofs due to the complex notation. However, I am in general glad to see a new RDP function with achievability & converse, zero-shot & asymptotic."
            },
            "weaknesses": {
                "value": "The reason why I am not willing to give this paper a higher rating is that the authors have not shown how the proposed RDP can guide perceptual compression / super-resolution, not even a toy example.\n\nThe RDP function in [Blau & Michaeli 2019] has many disadvantages, which this paper does not have:\n* [Blau & Michaeli 2019] does not prove the converse.\n* [Blau & Michaeli 2019] does not distinguish zero-shot and asymptotic function.\nThose issues have not been fixed until [A coding theorem for the rate-distortion-perception function].\n\nHowever, those weakness does not stop [Blau & Michaeli 2019] being popular. This is because [Blau & Michaeli 2019] has clear application in perceptual compression / super-resolution. It explains why previous work using GAN for perceptual compression; It aligns very well with the practically used \"real vs. fake\" test; It even guides later works in diffusion based image compression.\n\nICLR is a machine learning venue, not a pure information theory venue such as ISIT / TIT. It is better to have numerical examples (even toy size) and suggestions for later application works, so that the later works in ICLR can benefits from this paper more.\n\n(minor) It is better to move the converse to the main paper, as this year we have 10 page budget. It is strange to have a 8 page paper and 20 page appendix. At least for me, the converse is as important as achievability."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new mathematical formulation for the rate-perception-distortion tradeoff. Specifically, in the previous rate-perception-distortion formulation, the perceptual quality constraint is a constraint on the statistical divergence between the distribution of the decoded images and that of the clean images. In theory, this typically leads to randomized decoders, which produce many different decoded images given an encoded one. However, in practice, high-perceptual-quality compression-decompression algorithms rarely incorporate such randomness.\nTo explain this phenomenon, the authors replace the perceptual quality constraint with a new interesting concept called the \"universal critic\", which poses a perceptual quality constraint on individual images (or on a batch of images).\nThe new rate-perception-distortion formulation leads to solutions which do not incorporate randomness. This is a sensible result given the fact that now there is no constraint on the *distribution* of the decoded images."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "This paper is incredibly interesting, and written very well. The theoretical results are interesting and serve a highly important contribution to the community of information theorists."
            },
            "weaknesses": {
                "value": "1. There are no experiments, demonstrations, simulations, presented evidence, etc. This paper contains only theoretical results, which is not necessarily a bad thing, but I am not sure whether it's a fit for the ICLR community (most of which are practitioners). I would expect to see this paper in a theoretical journal.\n\n2. There is no discussion/limitation section discussing the possible future continuation of this work."
            },
            "questions": {
                "value": "1. I am aware that universal critics cannot be implemented practically, but still, is there a way to somehow simulate/demonstrate the new tradeoff on simple examples (perhaps a Bernoulli source)?\n\n2. Is there a way to demonstrate on previous works that less randomness is indeed attributed to better universal critic scores? Namely, is it possible to demonstrate that one may benefit from better rate&distortion by avoiding randomness, but still achieving high-perceptual-quality in the sense of universal critics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}