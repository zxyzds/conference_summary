{
    "id": "Txxz9fBPcJ",
    "title": "Can LLMs Enhance Performance Prediction for Deep Learning Models?",
    "abstract": "Accurate performance prediction of Deep Learning (DL) models is essential for efficient resource allocation and optimizations in various stages of the DL system stack. While existing approaches can achieve high prediction accuracy, they lack ability to quickly adapt to new hardware environments or emerging workloads. \nThis paper leverages both Graph Neural Networks (GNNs) and Large Language Models (LLMs) to enhance the accuracy and adaptability of DL performance prediction. Our intuition is that GNNs are adept at capturing the structural information of DL models, naturally represented as graphs, while LLMs provide generalization and the ability to quickly adapt to various tasks thanks to extensive pre-training data.\nWe empirically demonstrate that using GNN-derived graph embeddings as inputs to an LLM outperforms traditional representations, including high-level text summary and lossless semi-structured text (e.g., JSON), for this task. Furthermore, we propose a structured pre-training strategy to enable model adaptation to new hardware environments, significantly reducing the need for extensive retraining. Our experiments validate the effectiveness of this approach, showing an 8.8 percentage-point improvement in accuracy over a state-of-the-art GNN baseline. Notably, when adapted to new hardware with few samples, our method achieves a remarkable 30--70 percentage-point increase in accuracy compared to the GNN baseline.",
    "keywords": [
        "Graph Neural Networks",
        "Graph Tokens",
        "Large Language Models"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Txxz9fBPcJ",
    "pdf_link": "https://openreview.net/pdf?id=Txxz9fBPcJ",
    "comments": [
        {
            "title": {
                "value": "Regarding Ethical concern"
            },
            "comment": {
                "value": "Dear Reviewer,\n\nThank you for your valuable feedback. I would like to clarify that the paper you referenced (https://openreview.net/forum?id=bpS4vaOg7q#discussion) is a non-archival version presented at the ICML 2024 WANT workshop. According to ICLR guidelines, this does not constitute plagiarism. I kindly request the removal of the ethical concern raised in your review.\n\nI will address the other points you've mentioned as soon as possible. Thank you once again!"
            }
        },
        {
            "summary": {
                "value": "This work combines GNN and LLM for predicting the performance of a DL model in a certain hardware environment. It first verifies that using graph embeddings serve as effective input tokens for an LLM for performance prediction. A GNN foundation model is further proposed to avoid retraining. A specialized dataset verifies that GNN + LLM achieves higher accuracy and better efficiency than pure text-based methods."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well-organized and easy-to-follow.\n\n- The proposed method does not require retraining for a new model or a new hardware environment.\n\n- A new dataset is brought about.\n\n- Experimental results support that the proposed method achieves better advantages."
            },
            "weaknesses": {
                "value": "This work follows Perozzi et al. (2024)'s finding. What's the technical differences between the two works? Without such a comparison, it is hard to judge the technical novelty of the second contribution (line 108-109), which is the key contribution in my opinion."
            },
            "questions": {
                "value": "Will the code and dataset be released?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a deep learning-based system for predicting the performance of deep learning models. It combines a GNN encoder for producing a representation of the model's computational graph with an LLN which incorporates hardware details and the like. To adapt to novel hardware architectures and/or models, fine-tuning is employed. A comprehensive series of experiments demonstrates that this approach outperforms competing approaches and can transfer well with limited data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper presents a novel approach to performance modeling for deep learning which outperforms existing approaches.\n2. Improved performance modeling is highly impactful for many parties trying to decide whether to invest in new hardware, or budget for model deployment.\n3. A comprehensive set of experiments demonstrates the benefit of the approach.\n4. The paper is clear and well-written."
            },
            "weaknesses": {
                "value": "1. The dataset considered for evaluation (NNLQP) focuses exclusively on vision tasks, and almost all models considered are CNNs (there is one study which shows good results for low-data transfer to ViTs). The paper would be stronger if it considered models from additional modalities, most notably transformers on text tasks.\n2. Likewise, the hardware considered does not include recent GPU architectures, e.g., H100 GPUs, and is mostly focused on dedicated inference devices. While this is valuable to include, it neglects server-class GPUs which are probably more widely deployed, and which have very different hardware characteristics.\n3. Many models these days require multiple GPUs even for inference. It is not clear how the approach handles this situation.\n\nMinor:\nTypo: Figure 2, stage 2: \"Prompt Summaries the graph\": \"Summaries\" should be \"Summarizes\"."
            },
            "questions": {
                "value": "Please see comments/questions under weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel approach that combines Graph Neural Networks (GNNs) and Large Language Models (LLMs) to predict the inference latency of deep learning (DL) models. DL models are first converted into graphs with feature matrices via the Open Neural Network Exchange (ONNX) format. These graphs are then processed by a GNN to generate graph embeddings, which are concatenated with token embeddings from the LLM. The authors propose a three-stage training pipeline: (1) GNN pre-training, where the GNN is trained using Graph Masked Auto Encoder techniques; (2) Graph-Text Adaptation, which aligns the GNN's graph embeddings with the LLM\u2019s text embeddings using a projection layer and LoRA (Low-Rank Adaptation); and (3) Performance Prediction Fine-Tuning, which simultaneously trains both models. The results indicate that this multi-modal approach achieves lower Mean Absolute Percentage Error (MAPE) and higher accuracy compared to baseline methods and adapts well to new DL architectures and hardware configurations with limited training data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "-\tThe paper's method, which utilizes GNN embeddings as inputs for LLMs, is novel for performance prediction tasks. Unlike previous works, this approach combines GNN and LLM in a structured, multi-stage training pipeline to optimize both models for this task, showing high adaptability to new architectures and hardware.\n\n-\tThe proposed method shows considerable improvement in performance prediction metrics (e.g., MAPE, accuracy) compared to baselines, demonstrating strong potential for adaptability to new hardware configurations and DL architectures with limited training samples.\n\n- The presentation of the paper and explanation of the method is very clear. Although the method consists of several stages and different modules, the writing makes it easy to understand how each module is integrated in the training pipeline. \n\n\n\n- The authors propose a new training pipeline that can effectively optimize two different neural networks, GNN and LLM, to the task of performance prediction. Empirical results show that the resulting model has high adaptability to new hardware configurations or DL graphs given only a small set of training data."
            },
            "weaknesses": {
                "value": "-\tIn Table 3, the proposed method is compared against a single GNN baseline, which is insufficient to assess the enhancement from the method. The authors should add existing works such as [1] and [2] to their experiments for fair comparison with SOTA methods. \n\n-\tWhile this work demonstrates the feasibility of using GNN and LLM together to predict the performance of DL models, it lacks a thorough discussion on the specific challenges and characteristics that make this problem difficult to address using LLMs. In many fields, the effectiveness of LLMs is well established, and multi-modal language models are widely used. To strengthen the contribution of this paper, an analysis of why this problem requires an LLM-based approach and how it can be effectively addressed should be included.\n\n-\tthe baseline accuracy of 51.72% reported in Table 3 is questionable, as it is significantly lower than the accuracy reported in the original paper (Average of Acc(10%): 59.73% in the original paper). Additional discussion on why the baseline shows lower accuracy compared to the original paper is needed.\n\n-\tThe \u201cJustification for the Proposed Architecture\u201d section 4.2 is difficult to understand and confusing. Are the authors suddenly presenting justification for using LLMs by showing suboptimality of their early designs that use smaller LMs? Or are they presented as enhanced baselines which simply integrate language models to the GNN baseline? \n\n\n[1] Yu, G. X., Gao, Y., Golikov, P., & Pekhimenko, G. (2021). \"Habitat: A Runtime-Based Computational Performance Predictor for Deep Neural Network Training.\" Proceedings of the 2021 USENIX Annual Technical Conference (USENIX ATC 21).\n\n[2] Zhang, Y., Li, Y., & Wang, Y. (2023). \"Runtime Performance Prediction for Deep Learning Models with Graph Neural Network.\" Proceedings of the 2023 IEEE International Conference on Big Data (Big Data)."
            },
            "questions": {
                "value": "-\tHow do other baselines (e.g., [1], [2]) perform with the evaluation suite you used?\n\n-\tWhat are the specific challenges of using LLMs to predict the performance of DL models? There are already several works that use GNN and LLM together to encode the graph and make it understandable for LLMs [3], [4]. \n\n-\tWhy is the Acc(10%) of the GNN baseline much lower than the figure reported in the original paper?\n\n[3] Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi (2024), \u201cTalk like a Graph: Encoding Graphs for Large Language Models\u201d, The Twelfth International Conference on Learning Representations (ICLR 2024)\n\n[4] Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow (2024), \u201cLet Your Graph Do the Talking: Encoding Structured Data for LLMs\u201d, https://arxiv.org/abs/2402.05862"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes to use both graph neural networks (GNNs) and Large Language Models (LLMs) for a more accurate and generalizable performance prediction framework for deep learning models. The proposed method leverages a GNN-based encoder to generate graph tokens as inputs to the language model and achieves notable improvements against GNN-only or LLM-only baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed method, compared to baselines, takes network structure into consideration to achieve a better prediction accuracy while also maintaining a manageable computational cost."
            },
            "weaknesses": {
                "value": "- The LLM-based baselines provided in Table 1 all have very obvious drawbacks and therefore are not strong enough to serve as main baselines to illustrate the novelty of the proposed methods. The 'text' method does not include a network structure at all, which naturally leads to bad performance. The 'JSON' method uses a huge input in the LLM with possibly redundant information that significantly increases the training runtime and harms the accuracy.\n- The breakdown results for different model architectures are missing.\n- There seem to be different results reported in Table 6 of the original NNLQP paper, which reports a 79.51% accuracy."
            },
            "questions": {
                "value": "1. Can you provide a stronger LLM-only baseline? E.g., instead of using direct JSON input which can contain useless information, a more compact text-based and structured information can be passed to the LLM. \n2. What are the 'hardware details' in the prompt?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethic review needed"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new method that combines Graph Neural Networks (GNNs) and Large Language Models (LLMs) to predict the performance of deep learning models. GNNs can help capture the structural information in the model architectures, while LLM can help enhance the generalization and adaptation ability of the prediction model. To seamlessly combine them two, the proposed method pre-trains the GNN first and then uses two different stages to fine-tune the combined model. For the two-stage fine-tuning, the graph-text adaptation stage uses the\u00a0text-adaption dataset to tune the LLM and the projection layer between GNN and LLM, and the performance prediction finetuning stage uses a multi-platform dataset to tune the whole model, including the GNN and LLM. Both of the two stages use LoRA and soft prompting methods for efficient model training. As a result, the proposed method can achieve better prediction accuracy than the previous GNN-based methods, especially for the adaptation to new hardware with few samples."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper is well-written and easy to follow.\n- This paper introduces a novel method that combines a GNN and an LLM for performance prediction tasks. It can help inspire many other GNN+LLM researches on those structured data.\n- The evaluation clearly shows the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- Although the total training time can be greatly reduced, the inference time will increase due to the computation of LLM. There will be a new inference performance issue when applying this method to the DL system stack. There is no discussion about the inference time.\n- The proposed method heavily depends on multi-stage training, which must pre-train GNN and LLM first and then use two separate fine-tuning stages to finish the training. Each step needs a careful design for the training."
            },
            "questions": {
                "value": "Question about the results in Table 8. It seems that the GNN baseline can perform better on all of the fp32 platforms, and this result is contrary to the conclusion in the experiments section. But there is no discussion and any result about fp32 in the main body. It is better to have an explanation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}