{
    "id": "VYWBMq1L7H",
    "title": "MrT5: Dynamic Token Merging for Efficient Byte-level Language Models",
    "abstract": "Models that rely on subword tokenization have significant drawbacks, such as sensitivity to character-level noise like spelling errors and inconsistent compression rates across different languages and scripts. While character or byte-level models like ByT5 attempt to address these concerns, they have not gained widespread adoption\u2014processing raw byte streams without tokenization results in significantly longer sequence lengths, making training and inference inefficient. This work introduces MrT5 (MergeT5), a more efficient variant of ByT5 that integrates a token deletion mechanism in its encoder to dynamically shorten the input sequence length. After processing through a fixed number of encoder layers, a learnt delete gate determines which tokens are to be removed and which are to be retained for subsequent layers. MrT5 effectively \"merges\" critical information from deleted tokens into a more compact sequence, leveraging contextual information from the remaining tokens. In continued pre-training experiments, we find that MrT5 can achieve significant gains in inference runtime with minimal effect on performance. When trained on English text, MrT5 demonstrates the capability to transfer its deletion feature zero-shot across several languages, with significant additional improvements following multilingual training. Furthermore, MrT5 shows comparable accuracy to ByT5 on downstream evaluations such as XNLI and character-level tasks while reducing sequence lengths by up to 80%. Our approach presents a solution to the practical limitations of existing byte-level models.",
    "keywords": [
        "NLP",
        "ByT5",
        "T5",
        "tokenization",
        "byte-level language models",
        "character-level language models"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "MrT5 improves the efficiency of byte-level models like ByT5 by introducing a token deletion mechanism in its encoder, reducing the sequence length during processing.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=VYWBMq1L7H",
    "pdf_link": "https://openreview.net/pdf?id=VYWBMq1L7H",
    "comments": [
        {
            "title": {
                "value": "Response to Reviewer cEPL (continued)"
            },
            "comment": {
                "value": "### Questions\n1. > When training from scratch, what prevents the model from learning to set G=-30 (minimizing gating loss) and inflating QK^T for all tokens to offset the impact? It seems this would give the model additional capacity without increased loss.\n\n**Answer:** Our use of $\\text{softmax}_1$ in the attention mechanism prevents the model from learning to set $G=-30$ (see 208-213). With $\\text{softmax}_1$, setting $G=-30$ for all tokens in the sequence would cause the sum of attention scores to go to zero, emulating the effect of deleting all tokens. This reflects an impact on the loss, even with soft deletion.\n\n2. > Does the specific value of $k$ impact results when fine-tuning existing models, or does $k=-30$ seem to always work well? \n\n**Answer:** We found that $k=-30$ worked well for all our experiments. Larger values of $k$ downweight tokens but do not emulate deletion very well (we tried $k=-10$), causing discrepancies between hard and soft deletion performance. We did not observe discrepancies between hard and soft deletion when using $k = -30$ (see the table provided in the response to Reviewer 91Fx above).\n\n3. > If the models are trained until convergence does the performance gap between MrT5 and ByT5 increase?\n\n**Answer:** See (5) above.\n\n4. > Why was $\\text{softmax}_1$ used in the ByT5 baseline? Wouldn\u2019t the unmodified ByT5 perform better?\n\n**Answer:** We employ $\\text{softmax}_1$ consistently across all models to ensure that any performance variations are not attributed to $\\text{softmax}_1$, but rather to other components of the architecture. Additionally, we evaluated the impact of $\\text{softmax}_1$ on the ByT5 baseline. Our results showed that the loss for ByT5 with $\\text{softmax}_1$ (0.7805) was nearly identical to the loss for the unaltered ByT5 (0.7815), further reinforcing our decision to use $\\text{softmax}_1$ across all baselines.\n\n5. > The paragraph beginning at Line 180 suggests that a sequence\u2019s compression rate depends on the least compressed sequence in the batch. Are the rates reported in the paper based on this batch-dependent compression rate or the rate you\u2019d get with a batch size of 1?\n\n**Answer:** The compression rates reported in the paper are batch-independent (i.e. the rate with a batch size of 1). However, any runtime speed-ups that we report (such as Table 3) are batch-dependent using the batch sizes specified in the appendices. These results demonstrate that we can achieve significant speed-ups despite the dependence on the least compressed sequence in the batch.\n\n6. > Similarly, are the performance metrics reported using this explanation of batch processing, or are they reported as if sequences were fully compressed?\n\n**Answer:** If by \u201cperformance\u201d the reviewer means inference-time speed-up, these are reported on entire batches, as explained in Q5 above. If by \u201cperformance\u201d the reviewer means task loss or accuracy, these are entirely independent of the batch size and would not change based on the batch size. \n\n7. > In Figure 3, why is the English-only trained model doing better than English on many of the other languages?\n\n**Answer:** The \u201cEnglish-only\u201d models are still trained on top of a pre-trained ByT5 Small, which was trained on multiple languages and has better CE loss on certain languages other than English.\n\nWe are grateful to the reviewer for the thorough feedback on our work, and we hope that our response addresses your concerns and questions."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer cEPL"
            },
            "comment": {
                "value": "We thank reviewer cEPL for the thoughtful comments on our work. We appreciate that the reviewer found our paper to be clearly written, well-organized, and supported by thorough experiments. We would like to address the reviewer\u2019s concerns and questions in detail.\n\n### Weaknesses\n1. The reviewer sees our use of a regularizer with an $\\alpha$ hyperparameter as a limitation, and cites the binomial loss of Nawrot et al. (2023) as an alternative method. The main problem is that Nawrot et al.'s regularizer requires binary decisions, which would not apply to our \u201csoft\u201d deletion method. Furthermore, we see our regularizer as a more flexible variation; without the controller, MrT5 can discover its own compression rate, and with a controller, MrT5 can be pushed toward a specific compression rate. It is also worth noting that the binomial loss of Nawrot et al. (2023) also includes a hyperparameter that must be tuned, and their boundary predictor is less lightweight, requiring 1M additional parameters compared to our 4k parameter gate. Our method is also effective with only a minimal amount of continued pre-training. We do not believe that their method will compete in the same continued pre-training setup.\n2. The reviewer says that merging of contextualized tokens needs to happen later in the network. This is not the case, and we place the gate at an early layer for the most significant compute gain. For the span corruption and downstream tasks, the gates were placed at an early layer (l = 3, which is 1/4 of the ByT5 Small encoder), which resulted in deletions with minimal impact on task performance. Deleting at any reasonable layer (l >= 3) did not result in a performance impact, as shown in Figure 4. We only placed the gate at later layers for the simulations, and this was due to the simple nature of those tasks, where even a T5 model only developed non-trivial attention patterns at later layers. The synthetic experiments were designed to demonstrate that contextual deletions can be learned, rather than to optimize inference runtime at an early layer.\n3. It is not the case that the delete gate only targets the correct characters when there is too much deletion. We show that MrT5 is capable of achieving the ground truth deletion rate as specified by the synthetic task. The only synthetic model that had a steep drop in accuracy was the MrT5 model with $\\alpha$ = 1e\u20133 trained on the Sequence Merge task. This model was actually deleting more characters than specified for the task, resulting in a performance drop, as noted in the description given in Table 2c. These are copy tasks, so dropping characters that must be copied makes the task significantly harder. The other models that targeted relevant patterns did not see a significant performance drop, showing that MrT5 can be tuned to have the appropriate amount of deletion for a task.\n4. We appreciate the reviewer\u2019s suggestion to explore additional baselines. The Toucan model (Fleshman and Van Durme, 2023) speeds up the cross attention in decoding. This is negligible for improving the inference runtime of ByT5, which carries the majority of the computation in the encoder. We will be actively working on experiments adapting the boundary predictor of Nawrot et al. (2023) for encoders and will report our findings in another revision.\n5. We fine-tune all models on MNLI for 4,000 steps, which is ~10 epochs. The ByT5 authors fine-tune the model for 262,144 steps, which would be ~650 epochs, given their setup. We find this to be excessive for MNLI, and this is what we intended to convey in footnote 4. Due to resource constraints, we do not train for as many epochs. However, none of the claims in our paper depend on this comparison to the fine-tuning in the ByT5 paper. Our focus is on speeding up ByT5."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer 91Fx"
            },
            "comment": {
                "value": "We thank reviewer 91Fx for the insightful comments on our work. We are happy that the reviewer found our hard/soft deletion method to be novel, and that they found the paper to be well-written. We address the reviewer\u2019s comments and questions below.\n\n### Weaknesses\n1. We appreciate the reviewer\u2019s suggestion to include orthogonal methods as baselines; we will be actively working on experiments using the boundary predictor/pooling of Nawrot et al. 2023 and will report our findings in another revision. We note that the boundary predictor of Nawrot et al. is less lightweight, requiring 1M additional parameters compared to our 4k parameter gate. Our method is also effective with only a minimal amount of continued pre-training. We do not believe that their method will compete in the same continued pre-training setup.\n2. Our goal in this work is to enhance the efficiency of ByT5 while preserving its task performance. We do not claim to compete with or outperform subword models. Nonetheless, we provide accuracy comparisons with a subword-based T5 model on character-level tasks: under identical training settings, our model surpasses T5's performance, which achieves 47.98% accuracy on spelling correction and 72.90% on word search, as reported by Huang et al. (2023). However, we reiterate that our focus remains on optimizing ByT5\u2019s speed rather than benchmarking against subword-level models.\n3. We do not claim that the model is stronger than ByT5. We provide significant inference runtime improvements to ByT5 while maintaining its task performance. The runtime gains after the token deletion operation are provided in Table 3, and they are quite significant.\n\n### Questions\n1. >Consider discussing the relation between this work and GemFilter (https://arxiv.org/pdf/2409.17422) as both pertain to token deletion.\n\n**Answer:** We appreciate the reviewer\u2019s reference to GemFilter, and we will add it to the related work. We would like to note that GemFilter is concurrent work, as it was made public on September 25.\n\n2. > The proposed method seems readily transferable to decoder-only models, which is the most widely used architecture nowadays. Would like to see some experiments, or at least some discussions about this direction.\n\n**Answer:** Our approach is transferable to decoder-only models, but the deleted tokens must be re-inserted in order to perform next-token prediction\u2014a nontrivial modification to our method. We are exploring this direction, but we found this to be out-of-scope of the current work that seeks to speed up ByT5. It is a great next step, though.\n\n3. >150-151: \"(1) we want to avoid the overhead of executing the deletion algorithm multiple times;\" This motivation is better discussed from the perspective of \"trade-off\". If executing the algorithm multiple times can reduce the number of tokens/positions to process in later layers even more without compromising generation quality, then there is no reason to not do it.\n\n**Answer:** By \u201coverhead\u201d we are referring to additional time required to execute the algorithm. We agree that this can be discussed as a trade-off, and we will incorporate this in the revision.\n\n4. > The regularizer loss only seems to encourage the increase of the number of deleted tokens, but does not encourage the gate output to converge to the extreme values (i.e., the min or max gate value). This could make hard deletion during inference less effective because merely setting a threshold may delete some \"somewhat useful\" tokens.\n\n**Answer:** Throughout our experiments, we thoroughly tested that loss and task metrics for hard and soft deletion matched; in other words, soft deletion during training served as a true proxy for hard deletion at inference. We found that an additional regularizer that pushes the gate to its extreme values was unnecessary, and ultimately omitted it for simplicity. Here, we provide accuracy comparisons for hard and soft deletion on our downstream tasks. The results clearly show that there is little or no difference between the two.\n\n| Task                   | Hard Deletion Accuracy | Soft Deletion Accuracy |\n|------------------------|------------------------|-------------------------|\n| XNLI (English)         | 78.88                  | 78.84                   |\n| XNLI (All Languages)   | 49.63                  | 49.65                   |\n| Spelling Correction    | 56.07                  | 56.05                   |\n| Word Search            | 74.30                  | 74.25                   |\n\nWe also thank the reviewer for the presentation and phrasing suggestions, which we will incorporate in the revision. We are grateful for the reviewer\u2019s comments, and we hope that our response addresses your concerns and questions."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer uaeo"
            },
            "comment": {
                "value": "We thank reviewer uaeo for the thoughtful comments on our paper. We appreciate that the reviewer found our method to be simple yet effective for improving the inference speed of ByT5 while maintaining its performance. Below, we address the reviewer\u2019s comments and concerns.\n\n### Weaknesses\nWhile tokens are deleted softly during training, our continued pre-training approach requires only a relatively small number of steps (3,000) to effectively adapt the existing ByT5 model. Since models are typically used far more for inference than for training, optimizing inference runtime yields the most significant gains in practical utility.\n\nDue to resource constraints, we only performed experiments on moderate model sizes. With additional resources, we hope to scale up our approach in future work.\n\n### Questions\nThe MrT5 gating mechanism only adds 4,417 parameters to ByT5 Small, so both models have ~300M parameters. The additional parameter count is almost negligible, especially when compared to related approaches. For example, the boundary predictor of Nawrot et al. 2023 introduces 1M additional parameters.\n\nWe would again like to thank the reviewer for their comments, and we hope that our response addresses any concerns and questions."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer T4gu"
            },
            "comment": {
                "value": "We thank reviewer T4gu for their thorough feedback on our work. We are pleased that the reviewer found our work to be clear, acknowledging that it is a lightweight addition to ByT5 that significantly improves its inference runtime. Below, we address the constructive comments provided by the reviewer.\n\n### Weaknesses\n1. We recognize that our model\u2019s zero-shot performance for non-English languages has a larger drop compared to English. While perfect zero-shot transfer from English to all other languages is not expected, MrT5's deletion gate\u2014trained on minimal English span corruption data\u2014demonstrates impressive zero-shot transfer capabilities across several languages. Furthermore, with just a small amount of additional pre-training on multiple languages (3,000 steps), the model's cross-lingual performance improves substantially.\n2. We agree with the reviewer's suggestion to provide a more detailed analysis of the variance across languages in the XNLI task experiments to improve our discussion of the trade-offs. We will incorporate a more in-depth discussion of the multilingual results in the main text.\n\n### Questions\n1. >There is not much difference in performance between MrT5 and ByT5 for Chinese XNLI in Appendix Table 6, and there is ~20% length reduction - why do you think there is a difference to Chinese in the top half of Figure 3 with only a ~2% length reduction? Similarly, for Swahili, there seemed to be a significant difference in CE in Figure 3 zero-shot, but a smaller gap in Table 6. Why do you think that is?\n\n**Answer:** The reviewer mentions great points regarding our multilingual span corruption and XNLI task results that we clarify here. For the XNLI task discussed in Section 6, we first train the MrT5 model using multilingual span corruption data. (This is the bottom model in Figure 3). After this continued pre-training, we fine-tune the model specifically on the MNLI task, which uses only English data. This approach means that the model\u2019s gating mechanism has been trained on multilingual data, but solely within the continued pre-training phase.\n\nNext, we evaluate the model\u2014fine-tuned on English-only MNLI\u2014on the XNLI task, which spans 15 languages. This evaluation is therefore a zero-shot transfer *with respect to the XNLI task*, as the model has not seen these languages during fine-tuning. Our results demonstrate that effective fine-tuning on downstream tasks can be performed in English only, yielding strong zero-shot performance across other languages.\n\n2. > I suggest addressing these inconsistencies in the main text and providing possible explanations for the differences between training and downstream task performance.\n\n**Answer:** In Section 6, we state that we use a MrT5 model that was tuned with multilingual span corruption data before fine-tuning it on English MNLI data. We will emphasize this more and include a more thorough discussion of multilingual results.\n\n3. > How $\\alpha$ was set was not super clear to me. L196: \"For most of our experiments, we set $\\alpha$ by hand, which allows the model to dynamically set the deletion ratio [based on the loss].\" This paragraph says $\\alpha$ was set by hand most of the time, but that it is easier to allow $\\alpha$ to dynamically change. Do any of the experiments in the main text allow $\\alpha$ to change? (It doesn't seem so?) Why not, if it is easier?\n\n**Answer:** For the downstream task experiments (XNLI and character-level tasks) we use a controller to allow $\\alpha$ to dynamically change in order to target a specific deletion ratio. For the synthetic experiments and span corruption task, we set an initial $\\alpha_0$ and do not let it dynamically change, which nudges the model to delete, but ultimately allows it to discover its own deletion ratio. We found this to be the appropriate approach for those exploratory experiments, to study the behavior of MrT5 without being pushed toward a specific deletion ratio.\n\nWe appreciate the reviewer for the suggested citations of Cherry et al. 2018 and Limisiewicz et al. 2024, and we will include these in the related work of the revised paper.\n\nWe would like to thank the reviewer again for their insightful comments on our paper. We hope that our response has addressed your concerns and questions."
            }
        },
        {
            "summary": {
                "value": "This paper presents MrT5 (Merge-T5), a more efficient variant of ByT5 that introduces dynamic sequence shortening through a learned token delete gate after the first few encoder layers. Though it is called Merge-T5, no tokens are merged together, only deleted or preserved. \nThe method is a lightweight addition to a typical transformer architecture, with fully-differentiable soft deletion used during training via attention-masking and a regularizer with a tuned weight to adjust the degree of deletion, and hard deletion at inference time. \nPerformance is shown to be comparable to ByT5 (+/- 2% accuracy) while reducing sequences by up to 80% and runtime up to ~55%. \nOnly 4,000 steps are necessary to adapt a ByT5 model to a MrT5 model. \n\nExperiments are well designed and illustrate the properties of the method in different settings.  \nFirst, synthetic tasks (vowel deletion, token merging) are used to understand learned deletions with controlled settings and small 31M parameter models.  \nNext, they introduce token deletion in continued pre-training of ByT5-small both on English and multilingually, ablating components of the mechanism and different degrees of deletion. The task is span corruption loss, which for byte-level input may affect character and word boundaries. \nMrT5 is (generally) able to delete bytes in other scripts even if they have been unseen (if the model has been pretrained on English only), but results in slightly higher losses than ByT5. Multilingual pretraining reduces but does not remove the loss difference between MrT5 and ByT5. For Chinese, though, token deletion does not occur zero-shot.  \n--> It is not clear the significance of the differences in CE in Figure 2 & 3 here - it would be nice if this could be made clear.  \nFinally, MrT5 is evaluated on downstream tasks, XNLI and 2 character-level tasks.  \nFor English, MrT5 outperforms ByT5 with a reduction in sequence length and inference time of ~50%, while averaged across all languages there is a slight performance degradation of ~2% (though a significant speedup).   \nOn two English character-level tasks (contextual spelling correction and word search), MrT5 again saw ~2% accuracy drop compared to ByT5 in exchange for a 30-55% runtime decrease.  \nWhen comparing different layer placements, it seems that a middle layer (3) balances training stability and deletion level, allowing contextual representations to be learned before deleting tokens while still pruning sufficient tokens for efficiency gains. \n\nIt would be good if it were made clearer what the possible performance cost of introducing the deletion gate is, and some sense of the variability across languages, on explicit task performance as well as the loss presented in Figure 3."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "A straightforward and lightweight addition to ByT5 models which can provide significant efficiency improvements (up to 80%) with a small trade-off in accuracy (~2%) compared to ByT5 (and often small improvements for English)."
            },
            "weaknesses": {
                "value": "Efficiency gains (though generally significant) can come at a slight performance cost for non-English languages, and it is not clear how much variance in this cost there may be. \n\n- Without continued training in non-English scripts, there can be performance drops for non-English languages with less efficiency improvements. Figure 3 suggests that for Chinese for example, there may be almost no reduction in seq length zero-shot for the span corruption task, and some languages have relatively large drops in CE compared to ByT5\n- For the downstream experiments, the results are reported in aggregate for non-English languages in the main text. While the results are broken out by languages in the Appendix, a mention of the variance across languages and a comment on the relationship between these results and those in the previous section would make it clearer what the potential trade-offs are and when they arise"
            },
            "questions": {
                "value": "Questions: \n- I'd like to see the 'all languages' split for the downstream tasks discussed in the main text to match the CE analysis -> is it the case for example that you see a performance hit and no sequence length reduction for Chinese when the deletion gate is used zero-shot on a task, following Figure 3? Essentially, it would be nice to clearly state what the possible performance cost of introducing the deletion gate is, and some sense of the variability across languages, on explicit task performance as well as the loss presented in Figure 3. \n  - There is not much difference in performance between MrT5 and ByT5 for Chinese XNLI in Appendix Table 6, and there is ~20% length reduction - why do you think there is a difference to Chinese in the top half of Figure 3 with only a ~2% length reduction? Similarly, for Swahili, there seemed to be a significant difference in CE in Figure 3 zero-shot, but a smaller gap in Table 6. Why do you think that is? I suggest addressing these inconsistencies in the main text and providing possible explanations for the differences between training and downstream task performance. \n\nSuggested citations:  \n- [Cherry et al 2018](https://arxiv.org/abs/1808.09943) learns to delete characters for temporal compression with character-level models in MT\n- [Limisiewicz et al 2024](https://arxiv.org/pdf/2403.10691) uses morphologically inspired compression for byte sequences to create MyT5, a ByT5 variant \n\n\nPresentation nits:\n- L99: \"the main the limitations\" -> \"the main limitations\"\n- How \u03b1 was set was not super clear to me. L196: \"For most of our experiments, we set \u03b1 by hand, which allows the model to dynamically set the deletion ratio [based on the loss].\" This paragraph says \u03b1 was set by hand most of the time, but that it is easier to allow \u03b1 to dynamically change. Do any of the experiments in the main text allow \u03b1 to change? (It doesn't seem so?) Why not, if it is easier?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work builds on ByT5 and introduces a new token deletion mechanism that dynamically determines how to remove unnecessary tokens from the sequence without compromising performance.\n\nThe authors incorporate additional neural layers to learn the optimal token deletion process. During training, a loss function is introduced to encourage the model to progressively delete tokens (softly) or to achieve a specific ratio. During inference, the layers identify and discard unimportant tokens in a hard manner.\n\nControlled experiments on synthetic tasks demonstrate that the method, MrT5, can effectively learn to compress the input sequence. Further results on downstream tasks, such as XNLI, also support the authors' claims."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* A simple yet effective method that enables models to dynamically learn how to delete tokens from byte-level inputs.\n* Controlled experiments and results on downstream tasks support the authors' claims.\n* MrT5 demonstrates competitive inference speed compared to ByT5."
            },
            "weaknesses": {
                "value": "* If I understand correctly, during training, the byte tokens are deleted softly, meaning there are still significant burdens for byte-level language models given that standard attention has quadratic time complexity, which limits their scalability to larger sizes.\n* The experiments presented utilize moderate model sizes, which may constrain the overall persuasiveness of the proposed method."
            },
            "questions": {
                "value": "If possible, I am interested in comparing the effectiveness of additional parameters introduced by MrT5 in learning to delete tokens."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work improves on top of ByT5 by adding a delete gate at a certain layer to remove unimportant tokens. The proposed model trains a soft deletion layer which is fully differentiable, and then uses the same layer for hard (i.e., discrete) deletion during inference. The model only requires unsupervised data for training. \n\nThe model is evaluated on monolingual and cross-lingual pretraining tasks, showing a trade-off between cross-entropy loss and sequence length reduction. On two evaluation tasks, the model is also shown to be competitive with ByT5 while reducing sequence length."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "\u2022 Clear and illustrative figure\n\n\u2022\u00a0Very well-written and easy to read\n\n\u2022\u00a0Demonstrate that the proposed method performs competitively with ByT5 on XNLI and Spelling Correction.\n\n\u2022 The soft and hard deletion switch can be considered novel, at least for applying to this line of work."
            },
            "weaknesses": {
                "value": "\u2022 The baselines for MrT5 are not properly constructed (Section 5). To see how well MrT5 does, one should compare it with other orthogonal methods/models (e.g., pooling) that reduce sequence length and see how much increase in x-entropy loss they incur.\n\n\u2022 For downstream tasks we should compare with non-byte-level models to see the gap: how far are we in terms of accuracy? What's the run-time comparison after this token deletion optimization? These questions are left unanswered in the paper.\n\n\u2022\u00a0Just evaluating on XNLI and Spelling Correction is not enough to claim that the model is stronger than ByT5, let alone comparing comprehensively with models equipped with traditional tokenizers."
            },
            "questions": {
                "value": "Suggestions:\n\n\u2022 034-035: \"... via algorithms such as byte-pair encoding (Sennrich et al., 2016) or SentencePiece (Kudo & Richardson, 2018) ...\"\nConsider rephrasing this sentence as BPE is part of SentencePiece.\n\n\u2022 Consider discussing the relation between this work and GemFilter (https://arxiv.org/pdf/2409.17422) as both pertain to token deletion.\n\n\u2022 The proposed method seems readily transferrable to decoder-only models, which is the most widely used architecture nowadays. Would like to see some experiments, or at least some discussions about this direction.\n\n\u2022 150-151: \"(1) we want to avoid the overhead of executing the deletion algorithm multiple times;\"\nThis motivation is better discussed from the perspective of \"trade-off\". If executing the algorithm multiple times can reduce the number of tokens/positions to process in later layers even more without compromising generation quality, then there is no reason to not do it.\n\n\u2022 The regularizer loss only seems to encourage the increase of the number of deleted tokens, but does not encourage the gate output to converge to the extreme values (i.e., the min or max gate value). This could make hard deletion during inference less effective because merely setting a threshold may delete some \"somewhat useful\" tokens. Please refer to a very old paper on NALU to see how they do this: https://arxiv.org/pdf/1808.00508. Preferably this work can discuss the motivation of why or why not there is no such regularization term.\n\n\u2022 It's an interesting choice to combine experimental setup and results into one section, but I still think it's better to present them separately."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces MrT5, a ByT5 variant incorporating a learned delete gate for dynamically reducing the byte sequence length during the encoding process. By removing tokens from the sequence, the model is encouraged to implicitly merge the information from the deleted tokens into those which remain. The authors find that this reduction in sequence length leads to significant gains in runtime efficiency at inference time, with minimal differences to baseline performance. They show the deletion mechanism can reduce sequence length in zero-shot scenarios with new languages and see further improvements when training with multi-lingual data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Originality**\nMost previous work in this area focus on algorithms for learning \u201cwhat to keep\u201d when performing sequence compression. This paper provides an interesting alternative by framing the problem as learning \u201cwhat can be removed\u201d. The proposed deletion mechanism can be added to existing architectures, making their solution easy to integrate into existing models via fine-tuning.\n\n**Quality**\nThe authors conducted several levels of experimentation including simple tasks for gaining an intuition about the mechanism of their approach, a more difficult span completion task to compare against baselines, and down-stream tasks to assess cross-lingual semantic understanding and sensitivity to character-level manipulations.\n\n**Clarity**\nThe paper is well-organized and clearly written. The ideas, methodology, and findings were easy to follow and understand.\n\n**Significance**\nThe paper focuses on improving tokenization-free language models, which is a very relevant topic of interest to the community. Tokenization leads to several issues in LMs, including those listed by the authors as motivation: sensitivity to character-level noise (spelling errors), number representation for mathematical reasoning, and inconsistent compression rates across languages."
            },
            "weaknesses": {
                "value": "**1** The results are sensitive to the $/alpha$ hyperparameter. The gate regularization loss does not directly relate $\\alpha$ to the desired compression rate, leading the authors to manually tune $\\alpha$ or use the proposed P-controller to optimize for a desired compression during training. Previous work such as those cited by the authors (Nawrot et al., 2023) use a binomial loss to directly incorporate the desired compression rate into the regularization term. The same approach could be used here to remove the additional complexity.\n\n**2** The location of the gate in a specific network also needs to be tuned. The authors show that for implicit merging of contextualized tokens the gate needs to be placed later in the network, reducing the cost-savings of the approach.\nThe results of the synthetic experiments were not convincing. The deletion gate seemed to only target the correct characters when sequence length was significantly reduced, resulting in a steep drop in sequence-level accuracy.\n\n**3** There were no comparisons with other token-merging models in any of the experiments. It would have been nice to see a head-to-head comparison of this approach with the unsupervised boundary predictor from the cited (Nawrot et al., 2023) paper or the faster Toucan model from (Fleshman and Van Durme, 2023). Stronger baselines are needed throughout.\n\n **4** The models aren\u2019t trained to convergence (footnote 4), and the potential 15% improvement in the baseline through continued training might not translate to MrT5, increasing the performance gap."
            },
            "questions": {
                "value": "**Q1** When training from scratch, what prevents the model from learning to set G=-30 (minimizing gating loss) and inflating QK^T for all tokens to offset the impact? It seems this would give the model additional capacity without increased loss.\n\n**Q2** Does the specific value of k impact results when fine-tuning existing models, or does k=-30 seem to always work well?\n\n**Q3** If the models are trained until convergence does the performance gap between MrT5 and ByT5 increase?\n\n**Q4** Why was softmax_1 used in the ByT5 baseline? Wouldn\u2019t the unmodified ByT5 perform better?\n\n**Q5** The paragraph beginning at Line 180 suggests that a sequence\u2019s compression rate depends on the least compressed sequence in the batch. Are the rates reported in the paper based on this batch-dependent compression rate or the rate you\u2019d get with a batch size of 1?\n\n**Q6** Similarly, are the performance metrics reported using this explanation of batch processing, or are they reported as if sequences were fully compressed?\n\n**Q7** In Figure 3, why is the English-only trained model doing better than English on many of the other languages?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}