{
    "id": "74vnDs1R97",
    "title": "Understanding Visual Concepts Across Models",
    "abstract": "Large multimodal models such as Stable Diffusion can generate, detect, and classify new visual concepts after optimizing just the prompt. How are prompt embeddings for visual concepts found by prompt tuning methods different from typical discrete prompts? We conduct a large-scale analysis on three state-of-the-art models in text-to-image generation, open-set object detection, and zero-shot classification, and find that prompts optimized to represent new visual concepts are akin to an adversarial attack on the text encoder. Across 4,800 new embeddings trained for 40 diverse visual concepts on four standard datasets, we find perturbations within an $\\epsilon$-ball to any prompt that reprogram models to generate, detect, and classify arbitrary subjects. These perturbations target the final-layers in text encoders, and steer pooling tokens towards the subject. We explore the transferability of these prompts, and find that perturbations reprogramming multimodal models are initialization-specific, and model-specific. Code for reproducing our work is available at the following site: https://anonymous-visual-words.github.io.",
    "keywords": [
        "Deep Learning"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=74vnDs1R97",
    "pdf_link": "https://openreview.net/pdf?id=74vnDs1R97",
    "comments": [
        {
            "summary": {
                "value": "This paper examines how fine-tuned prompt embeddings for visual concepts affect text-to-image generation, object detection, and classification. It reveals that these embeddings act as model-specific adversarial perturbations, altering behavior without needing extensive retraining."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper provides a novel perspective on prompt tuning across multiple models and tasks.\n- The paper is logically structured, clearly presenting methodologies and findings."
            },
            "weaknesses": {
                "value": "- The application of the transfer function and its relationship with Section 4 should be clearly articulated, especially regarding its role in the findings and conclusions of the study.\n- The potential impact of the differences in the datasets used for the experiments on the results and conclusions should be discussed in detail. Currently, the datasets appear to exhibit considerable similarity. The diversity and representativeness of these datasets remain limited. It raises the question of whether the findings from a model trained on large-scale data are genuinely necessary for the conclusions drawn in this study. The application of the findings in domains with greater diversity could yield more valuable insights. A comparative analysis that includes varied domains, such as remote sensing, would enhance our understanding of the generalizability of the findings."
            },
            "questions": {
                "value": "- The analysis of the transfer function employs a straightforward linear transformation. \n1. Given that X and Y are finite observations, the effectiveness of the linear transformation is influenced by the sample size (n), quality, and representativeness of the samples. Under these circumstances, it is critical to evaluate whether the derived transfer function T is representative and adequately supports the subsequent conclusions. \n2. Considering the potential noise in the observations, it would be worthwhile to explore the use of penalized least squares. Additionally, for linear transformations between two spaces, incorporating sparse regularization could significantly impact the subsequent analysis and conclusions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates prompt embeddings in large multimodal models, such as Stable Diffusion, to understand how these embeddings differ from traditional discrete prompts for generating and classifying new visual concepts. Through a large-scale analysis across text-to-image generation, object detection, and zero-shot classification, the authors discover that prompts optimized for new concepts function similarly to adversarial attacks on the text encoder. Testing with 4,800 embeddings, they find that these adversarial perturbations specifically target the final layers of text encoders, influencing models to respond to specific subjects, but these effects are model-specific and dependent on initialization."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper presents an interesting perspective for integrating specific concepts into a sequence of multimodal models.\n- The paper proposed straightforward methods for transferring the soft prompts in a source domain into various target tasks, with thorough analysis of its effect."
            },
            "weaknesses": {
                "value": "- The paper presumes the linearity between the text embedding space between domains. While models sharing the similar text encoders might be suitable to presume the linear relationship, the models using text encoders with totally different text embedding space might rather collapse when representing the soft prompt of the target domain with linearity.\n- Generalizability of transform function: the paper used 40 visual concepts for transferring experiments, which seem to be limited. Scaling up the visual concepts would be required to see if the transform function can generalize to any of visual concepts.\n- the motivation behind transferring the soft prompt over various tasks: the authors suggested that transferring the soft prompt into other tasks eliminates the need for retraining prompts for each task. Performance comparison between transferring prompt vs prompt-tuning each task would be required to see if the performance gap is negligible while lowering the training overhead.\n- The paper needs re-ordering, where the main goal of why transferring soft prompts is needed stated in the last with a few sentences, which might give not seamlessly connected\n- The paper needs reorganization, as the empirical questions and observations take a large portion of the introduction and abstract in the front, while the main goal\u2014explaining why transferring soft prompts is necessary\u2014is briefly mentioned at the end with a few sentences, which might make the flow disjointed. Also, the details (e.g., derivation of (2)) are often absent, which would rather be better if kindly provided in supplementary materials."
            },
            "questions": {
                "value": "See above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies the following question: How are prompt embedding for visual concepts found by prompt tuning methods different from typical discrete prompts? It evaluates through transfer learning tasks such as zero-shot detection."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is well-written (while I have a hard time to understand the motivation, see below) but the overall presentation is good. Authors did a large-scale analysis on the defined problem as well."
            },
            "weaknesses": {
                "value": "I actually have a hard time to understand the motivation of this work, and as a result, my judgement may be incorrect.\n\nSpecifically, it's not clear to me what's the motivation of finetuning prompts like <black_dog> or <orange_cat>? I have seen work doing similar things for personalized generation like DreamBooth but what's actually the motivation for prompts like <black_dog> or <orange_cat>?\n\nFollowing previous point, I don't get the motivation to understand the difference between <black_dog> and \"black dog\" in the embedding space. For what purposes should we care about this? I think the analysis makes sense but it's not clear to me why should we care about this problem."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors are motivated to learn prompts or tune prompts across different vision tasks. They mostly try to learn prompts for generative tasks and then transfer these learned prompts to discriminative tasks such as classification and detection."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The idea has practical significance and learning soft prompts that can transfer for various tasks is valuable.\n2. The proposed linear layer learned is simple to understand and the implementation details are properly explained."
            },
            "weaknesses": {
                "value": "1. I question the value of this setup if both the generative and discriminative VLMs use the same text encoder. The authors chose Stable Diffusion 2, which uses OpenCLIP L14 for generation, and for classification, they use CLIP L14. However, if OpenCLIP L14 were also used for discriminative tasks, the problem setup might not hold.\n2. Additionally, the paper lacks a discussion on the limitations of their problem setup and proposed methods, which would be helpful for understanding the approach's constraints.\n3. The vocabulary in some sections feels unnecessarily complex and could be simplified for better clarity."
            },
            "questions": {
                "value": "1. What if you were to reverse the setup, learn prompts for discriminative tasks and transfer to generative tasks would the results hold?\n2. Did you finetune any layers of the models? To the best of my knowledge, they seemed to have been left frozen."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores the transferability of prompt embeddings learned through prompt tuning across models trained on different tasks. It concludes that these prompt embeddings are not transferable. The study finds that prompt embeddings function similarly to adversarial perturbations, with multiple effective prompt solutions possible within close proximity to text embeddings of unrelated concepts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Well-written and easy to follow. \n2. Extensive analysis clearly establishes the non-transferability of learned prompt embeddings across models. \n3. Perturbation analysis shows that learned embeddings constrained to random prompt anchors can perform equally well as those near related prompt anchors."
            },
            "weaknesses": {
                "value": "1. Identifies that prompt-tuned input embeddings resemble adversarial examples and lack transferability but does not propose mitigation strategies. \n2. Recent works [2] [3] focuses on final-layer convergence for multimodal representations; it\u2019s unclear if this semantic alignment exists in input embeddings. Applying metrics like CKA [1] or CKNNA [2] could reveal input embedding similarities, potentially isolating adversarial behavior as the primary non-transferability factor. \n3. Lacks a control setup in the linear transformation + MSE loss analysis where the transfer works; tuning on one generative model and testing on another could serve as a useful comparison. Is the non transferability because of this Linear+MSE setup?\n4. Limited transformation methods and losses explored\u2014considers only linear transformation and MSE loss. Trying Nonlinear transformations and/or CLIP loss could offer further insights. \n5. Missing citations for recent work on vision and language representation convergence, such as [3] \n\n[1] Kornblith, Simon, et al. \"Similarity of neural network representations revisited.\" International conference on machine learning. PMLR, 2019.\n[2] Huh, Minyoung, et al. \"The platonic representation hypothesis.\" arXiv preprint arXiv:2405.07987 (2024).\n[3] Maniparambil, Mayug, et al. \"Do Vision and Language Encoders Represent the World Similarly?.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\nI am willing to raise my score if more insights are provided."
            },
            "questions": {
                "value": "1. How semantically similar (measured using CKA or CKNNA) are the input embedding representations? Are the text encoders identical across the three models? If not, do they exhibit high semantic similarity for common vocabulary but not for learned concepts? \n\n2. Why was only MSE loss considered for learning transformations across modalities? Is the failure due to difficulty of transferring between tasks, or could it be attributed to the limitations of using a linear transform or the MSE loss?\n\n3. What happens when the prompt is learnt for one generative model and transferred to another generative model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}