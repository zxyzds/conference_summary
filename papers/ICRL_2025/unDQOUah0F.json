{
    "id": "unDQOUah0F",
    "title": "VideoWebArena:  Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks",
    "abstract": "Videos are often used to learn or extract the necessary information to complete tasks\nin ways different than what text and static imagery alone can provide. However,\nmany existing agent benchmarks neglect long-context video understanding, instead\nfocusing on text or static image inputs. To bridge this gap, we introduce VideoWe-\nbArena (VideoWA), a benchmark for evaluating the capabilities of long-context\nmultimodal agents for video understanding. VideoWA consists of 2,021 web agent\ntasks based on manually crafted video tutorials, which total almost four hours of\ncontent. For our benchmark, we define a taxonomy of long-context video-based\nagent tasks with two main areas of focus: skill retention and factual retention.\nWhile skill retention tasks evaluate whether an agent can use a given human demon-\nstration to complete a task efficiently, the factual retention task evaluates whether\nan agent can retrieve instruction-relevant information from a video to complete\na task. We find that the best model achieves 13.3% success on factual retention\ntasks and 46.0% on factual retention QA pairs, far below human performance\nat 73.9% and 79.3%, respectively. On skill retention tasks, long-context models\nperform worse with tutorials than without, exhibiting a 5% performance decrease\nin WebArena tasks and a 10.3% decrease in VisualWebArena tasks. Our work\nhighlights the need to improve the agentic abilities of long-context multimodal\nmodels and provides a testbed for future development with long-context video\nagents.",
    "keywords": [
        "agents",
        "benchmark",
        "video understanding",
        "multimodal agents"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=unDQOUah0F",
    "pdf_link": "https://openreview.net/pdf?id=unDQOUah0F",
    "comments": [
        {
            "summary": {
                "value": "This paper presents VideoWebArena, a novel, open-sourced video-based benchmark, designed to evaluate the capabilities of long-context multimodal agents in video understanding tasks. The dataset consists of 2,021 tasks based on four hours of video tutorials across six domains. Moreover, the paper conducts experiments to validate that the current intelligent agents do not perform well on most the task, and is important to the relevant research field."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThis paper provides a novel video benchmark that is very welcome to the research community.\n2.\tThe benchmark offers a well-defined taxonomy that focuses on two main areas: factual retention and skill retention, which test different facets of a model's abilities to retrieve information from videos and efficiently apply learned skills.\n3.\tThe paper conducts experiments to validate that the current intelligent agents do not perform well on most the task, leading to future improvement."
            },
            "weaknesses": {
                "value": "1.\tThe used LLMs are all closed-source, thus may cause obstacles to reproduction. Experiments on open-sourced intelligent agents may be included.\n2.\tThis paper does not provide a comparison or discussing over itself and other similar video benchmarks, like MVBench[1], with respect to some tasks like temporal reasoning, etc.\n\n[1] Li, K., Wang, Y., He, Y., Li, Y., Wang, Y., Liu, Y., ... & Qiao, Y. (2024). Mvbench: A comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 22195-22206)."
            },
            "questions": {
                "value": "1.\tCan new tasks or new domain be easily added into the benchmark?\n2.\tThrough analysis, what is the possible aspects that the existing agent can be improved?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper present VideoWebArena (benchmark) to evaluate a model's ability to process long video sequences alongside text and images to complete tasks that require memory retention, information retrieval, multimodal reasoning, and skill retention. Moreover, this papaer show that these models are still a far reach from human levels of performance, highlighting a wide gap in the information retrieval and agentic abilities of current state-of-the-art long-context models.\n\nStrengths:\n+ This paper construct a benchmark called VideoWebArena.\n+ The idea of this paper is novel and interesting.\n+ This paper test many retention tasks, I think it is a hard task and the author complete this task.\n \nWeakness:\n+ Can author release the source code for this paper, I would like to try this agent."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "See Summary"
            },
            "weaknesses": {
                "value": "See Summary"
            },
            "questions": {
                "value": "See Summary"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents the VideoWebArena benchmark to evaluate the capabilities of long context multimodal agents. The tasks are divided into factual retention and skill retention. The authors evaluate GPT-4o and Gemini-1.5-pro on their benchmarks and provide an in-depth analysis."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ The proposed benchmark is valuable to the community.\n+ The task designs make sense to me (divide into skill retention and factual retention), which challenge current modes\u2019 capabilities to extract information from a demonstration video to complete tasks.\n+ The experimental findings are insightful, especially Table 7, which suggests that current models struggle with \u201clearning\u201d from tutorial videos, in contrast to humans."
            },
            "weaknesses": {
                "value": "+ The introduction lacks references.\n\n+ While the related works section reviews a few agent benchmarks and long-context benchmarks, it is suggested to include a dataset comparison table of VideoWA with existing benchmarks. This would clarify VideoWA\u2019s distinctiveness, highlighting differences in task diversity, domain, video length, etc., against existing ones. \n\n+ To improve clarity, it would be helpful for the authors to explicitly discuss how VideoWebArena differs from VisualWebArena and WebArena in terms of environment and task design. Are there methodological innovations in this paper beyond extending these benchmarks to long videos?\n\n+ Could the authors further clarify the difference among video agent, frames agent and summary agent in terms of their input? Does the difference lie in audio input (gemini directly takes audio while gpt-4o takes audio transcriptions)? What about the summary agent?\n\n+ In Table 4, the example provided for temporal reasoning appears more like a counting task; substituting it with a clearer example focused on temporal reasoning might improve clarity.\n\n+ The benchmark only evaluates Gemini and GPT-4o. I wonder why the authors do not include open-source VLMs such as LongVILA, and LLAVANextVideo (as mentioned in the intro)."
            },
            "questions": {
                "value": "Overall, this paper offers a useful benchmark with valuable insights for the community.  However, the current presentation is not clear and many clarifications are needed. Please refer to weaknesses for my questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors present VideoWA, a benchmark designed to assess the capabilities of long-context multimodal agents, particularly their ability to understand and utilize video information to accomplish tasks on web-based platforms. The paper emphasizes the gap in existing benchmarks, which predominantly focus on static image or text inputs, and seeks to bridge this with comprehensive video-based task evaluation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is clearly written, with well-structured sections that guide the reader through the motivation, methodology, and results.\n2. The experimental setup and evaluation in the paper are robust, employing a range of state-of-the-art models such as GPT-4o and Gemini 1.5 Pro. The results are well-documented and provide clear evidence of the limitations of current models in handling long-context multimodal inputs. \n3. The significance of this work lies in its potential to drive advancements in the development of long-context multimodal models. Although some tasks may not fully justify the necessity of video tutorials, identifying more suitable tasks that would effectively leverage the richness of video input remains a challenge. Nevertheless, the benchmark's foundation provides a valuable starting point for refining task design and exploring the benefits of video-based learning in AI agents."
            },
            "weaknesses": {
                "value": "Major problems I have found are as follows:\n1. One significant concern is that the contributions of VideoWA appear to be relatively incremental compared to prior work such as WebArena and VisualWebArena. While the inclusion of video content does enhance task complexity and data richness, the paper does not convincingly demonstrate how these video tutorials meaningfully improve agent learning or task performance. Specifically, some tasks, like \u201cbuying the cheapest item\u201d as presented in Table 4, do not seem to justify the necessity of video input. Agents could reasonably complete these tasks with textual intent or static images. This raises doubts about whether the chosen tasks fully exploit the potential advantages of video tutorials or merely add unnecessary complexity.\n2. Another perplexing issue is the low success rate of human performance on seemingly straightforward tasks. The authors report that these metrics were gathered using the authors themselves, who should be well-acquainted with both the data and the tasks. The review questions how the human success rate could be so limited (e.g., 73.9% on factual retention), suggesting a potential shortcoming in task design or evaluation methodology. If even familiar humans struggle, it may reflect an underlying flaw in the task construction or evaluation criteria, which could compromise the benchmark's real-world applicability and reliability.\n3. The paper presents an unexpected and concerning trend where multimodal models, even when provided with video tutorials, perform worse than models without them. This result contradicts the assumption that video tutorials should aid skill retention, as evidenced by the drop in performance on WebArena and VisualWebArena tasks (5% and 10.3% decreases, respectively). The authors should address whether the noise introduced by video information or ineffective agentic reasoning mechanisms is the primary cause of these shortcomings. Without further investigation or hypothesis testing, the paper leaves this critical aspect unresolved."
            },
            "questions": {
                "value": "Please refer to the weakness 1-3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}