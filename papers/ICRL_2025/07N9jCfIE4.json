{
    "id": "07N9jCfIE4",
    "title": "The Complexity Dynamics of Grokking",
    "abstract": "We investigate the phenomenon of generalization through the lens of compression. In particular, we study the complexity dynamics of neural networks to explain \\emph{grokking}, where networks suddenly transition from memorizing to generalizing solutions long after over-fitting the training data. To this end we introduce a new measure of intrinsic complexity for neural networks based on the theory of Kolmogorov complexity. Tracking this metric throughout network training, we find a consistent pattern in training dynamics, consisting of a rise and fall in complexity. We demonstrate that this corresponds to memorization followed by generalization. Based on insights from rate--distortion theory and the minimum description length principle, we lay out a principled approach to lossy compression of neural networks, and connect our complexity measure to explicit generalization bounds. Based on a careful analysis of information capacity in neural networks, we propose a new regularization method which encourages networks towards low-rank representations by penalizing their spectral entropy, and find that our regularizer outperforms baselines in total compression of the dataset.",
    "keywords": [
        "Compression",
        "Complexity",
        "Generalization",
        "Grokking",
        "Minimum Description Length"
    ],
    "primary_area": "learning theory",
    "TLDR": "We track the complexity dynamics of neural networks during training to understand grokking, using insights from the theory of Kolmogorov complexity.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=07N9jCfIE4",
    "pdf_link": "https://openreview.net/pdf?id=07N9jCfIE4",
    "comments": [
        {
            "title": {
                "value": "Response 1.3"
            },
            "comment": {
                "value": "> Does your regularization technique always lead to lower test accuracy compared to weight decay?\n\nBoth regularization methods achieve perfect test accuracy.\n\n__\n\n> Figures 3 and 5 are not analyzed in the text, can you add some insights on the result they present?\n\nWe have updated the draft with additional discussion of the figures, and new figures. In addition to the rate\u2013distortion curves we mentioned earlier, we also added complexity dynamics plots for the unregularized network, so you can see how the complexity stays high at all times after memorization occurs, with no generalization following. We also added effective rank plots, which show how our spectral entropy method encourages models toward low effective rank. Interestingly, weight decay also seems to encourage effective low-rank representations, though not as strongly as ours.\n\nRegarding your question on the permutation of hypotheses. Indeed one is free to choose any prior one wishes - if we understand your point correctly, your question amounts to whether there is a unique, canonical notion of \u201ccomplexity\u201d. Firstly, consider that the permutation (bijection) you apply to the hypotheses has itself a non-zero Kolmogorov complexity, so you're not leaving the complexity invariant by permutation. However, this leads us to the question of a unique ordering on the hypotheses/natural numbers. This is an interesting and deep question, but it is beyond the scope of our work. While there exist invariance theorems for Kolmogorov complexity, they only hold up to an arbitrary constant, so we can only make strong statements about asymptotic complexity. We recommend the textbook on Kolmogorov complexity by Li and Vitanyi if you are interested in the mathematical foundations of algorithmic complexity.\n\n[1] Dingle, Kamaludin, Chico Q. Camargo and Ard A. Louis. \u201cInput\u2013output maps are strongly biased towards simple outputs.\u201d Nature Communications 9 (2018)\n\n[2] Johnston, Iain G., Kamaludin Dingle, Sam F. Greenbury, Chico Q. Camargo, Jonathan P. K. Doye, Sebastian E. Ahnert and Ard A. Louis. \u201cSymmetry and simplicity spontaneously emerge from the algorithmic nature of evolution.\u201d Proceedings of the National Academy of Sciences of the United States of America 119 (2021)\n\n[3]: Power, Alethea, Yuri Burda, Harrison Edwards, Igor Babuschkin and Vedant Misra. \u201cGrokking: Generalization Beyond Overfitting on Small Algorithmic Datasets.\u201d ArXiv abs/2201.02177 (2022)\n\n[4]: Lotfi, Sanae, Marc Finzi, Yilun Kuang, Tim G. J. Rudner, Micah Goldblum and Andrew Gordon Wilson. \u201cNon-Vacuous Generalization Bounds for Large Language Models.\u201d ArXiv abs/2312.17173 (2023)\n\n[5] Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Annual Conference Computational Learning Theory, 1993."
            }
        },
        {
            "title": {
                "value": "Response 1.2"
            },
            "comment": {
                "value": "> For instance, using the actual test accuracy would be very informative, to see whether the proposed regularization leads to better performance.\n\nAs we mentioned above, and in the paper, all regularized nets achieve perfect test accuracy, so there is no value in comparing the test accuracy.\n\n__\n\n> Does the generalization bound of Equation (4) only hold for finite hypothesis spaces? If yes is that a realistic assumption in practical learning settings? Moreover, could you be more precise as to why the choice of Solomonoff prior should lead to tighter bounds than other priors, such as the uniform prior over H?\n\nThe generalization bound of Equation 4 comes from Lotfi et al [4]. The Solomonoff prior is defined over all finite strings considered as programs. This is the most generic possible assumption that one could make for a computer model of some data.\n\nThe question of why the Solomonoff prior is a good one, is very deep and interesting. There is ongoing research into the apparent simplicity bias found in nature [1, 2]. It appears to be the case that nature simply has a bias towards simpler structures, hence the Solomonoff prior is superior to a uniform prior. The ultimate nature of why this is the case is not yet clear.\n\n__\n\n> Line 181: Why can the empirical risk be understood as the entropy of the data under the model? Is there a way to formalize this fact?\n\nLotfi et al discuss this in their work which produces the bound we use. We refer you to their work to understand the nuances of the finite hypothesis bounds. In particular, they show how to adapt entropy measures for the risk (they have to make some small changes to ensure the entropy stays bounded).\n\nIn practice, one can take the risk to be the cross-entropy loss used in training. This is the intriguing link between compression and generalization: both the MDL principle and generalization bounds like Equation 4 suggest that we should take the model which minimizes the sum of data entropy and model complexity. The model which compresses the data best is the one which generalizes best. This deep fact underpins our work.\n\n__\n\n> Is it possible to obtain a formal statement relating the information capacity (Equation (9)) to generalization?\n\nThe information capacity can be seen as the largest upper-bound on the model complexity. That is, the model complexity can be no greater than its information capacity. For example, a model might have 100 parameters of 10 bytes each (total 10 bytes times 100 =  1KB). However, imagine all the parameters are zero (or 1, or any constant). That model is very simple, its complexity is low. We can also imagine that each of the 100 parameters is as complex as possible (e.g. uniform random), and that there is no discernable pattern in the parameters considered as a whole: the complexity of this model is large, but it is certainly no larger than the total capacity of the model (1KB). So yes, the capacity bounds the complexity, but it is the loosest possible bound. However, one can see that an effective model compression scheme might be to try distilling a larger model into a smaller one: then if the capacity of the smaller model is low enough, we may be able to produce quite tight complexity bounds.\n\n__\n\n> To what size and precision do the parameters \u03bb and \u03b4 (Section 4) refer to in practice?\n\nIt depends on model representation specifics. E.g. models trained in float32 vs float16 will have different effective max ranges and precisions, and so on. We control the max size \\lambda through weight decay, and the precision \\delta through the noisy weight scheme which we discuss in section 4.1\n\n__\n\n> How would the training accuracy be affected by the addition of Gaussian noise in practical deep learning settings?\n\nThere is no general answer to this question, as it can depend on the specifics of the data, model, loss function, and so on. However, adding noise to the weights is an effective regularization scheme. We cited [5] in our discussion of this scheme, and recommend it for further insight on using noisy weights as a regularization method.\n\n__\n\n> Can you define more precisely the notations used in Algorithm 2, such as BO.SUGGESTPARAMETERS()? More generally, can you provide more details on the Bayesian optimization procedure?\n\nBayesian optimization is a generic black-box optimization procedure. Say you have input-output access to some function f(x), and you want to find its maximum value. Bayesian optimization is a generic method which receives a history of (x, f(x)) pairs, and suggests a new x each time by refining a model of the function under evaluation. In our case, we are trying to minimize the compressed size of the network, so we maximize -compressed_size. Our inputs parameters are the quantization level \\Delta and the parameter \\tau introduced in section 4.2 which controls the degree of rank-decomposition. We used a standard python BO, found [here](https://github.com/bayesian-optimization/BayesianOptimization)."
            }
        },
        {
            "title": {
                "value": "Response 1.1"
            },
            "comment": {
                "value": "Thank you for your review.\n\n>Several notions are mentioned repeatedly but without being formally defined, such as capacity, distortion or (\u03bb,\u03b4) (Equation (9)). It would improve the paper to include additional theoretical background and more formal definitions.\n\n**Formal definitions**: We formally defined all of these. The distortion is defined in Equation 6: it is the absolute difference in loss between the original and coarse-grained weights on the data. \\lambda and \\delta are defined just before Equation 9. They are the max size and precision of a single parameter. Imagine a parameter which can take values as large as 100 (size), but only in multiples of 10 (precision). Then its information capacity is log(100/10) = log(10) = log(number of bins).\n\n__\n> It should be made clearer how the quantities introduced in Sections 3.1 and 4 are related to generalization. For instance, is it possible to write down a theorem with explicit dependence on these quantities, or are their consideration partially based on intuitions? Can the link of these quantities with Kolmogorov complexity be made more formal?\n\nThe algorithmic rate\u2013distortion function returns the Kolmogorov complexity of the coarse-grained model at the distortion level \\epsilon, under the distortion function. This means that the generalization performance can be bounded by Equation 4, which links Kolmogorov complexity with expected risk. The dependence is explicit, not based on intuition. In the revised paper we have added a plot of the rate\u2013distortion curve (Fig 3), so that you can see the complexity levels K at different levels of distortion \\epsilon. This plot shows that our method Pareto-dominates weight decay at all distortion levels. That is, our method results in more compressible models compared to weight decay at every distortion level.\n\n__\n\n> Despite the lack of formal theorems and proofs, the experiments are done on very simple arithmetic tasks. Therefore, it is not clear (neither theoretically nor empirically) whether the results may be generalized to more complex settings. I think that at least one experiment on a small dataset like MNIST or CIFAR10 could improve the paper.\n\nAs mentioned above, our bounds are explicit *and universal*, unlike other complexity proxies like L2 norm. It is not a limitation of our work that we test on \u201csimple tasks\u201d, it is a choice. We are trying to explain grokking, which was originally demonstrated on these modular arithmetic tasks [1]. They are an excellent test-case because they provide a clear, delayed transition from memorization to perfect generalization. This is a theory paper. Not every work benefits from scale: some ideas are best demonstrated in simple settings. While we intend to scale up our work, and have already begun follow-up work which does this, here we are focused on the basic science of learning, complexity, and generalization.\n\n__\n\n> It would be useful to include an experiment comparing the performance (in terms of accuracy) with and without the proposed regularization scheme. Indeed, we see that it reduces the MDL and the generalization bound, but, if I am correct, it is not clear whether it achieves better performance overall.\n\nThis is already included in the original draft\u2019s appendix, in the final figure. As mentioned, all regularized models grok (that is, they transition from memorization (100% train accuracy, low test accuracy) to generalization (perfect test accuracy)), and the unregularized models remain in the memorization phase. The full train and test accuracy plots are shown in the final figure, in the Appendix, which we referenced in the Experiments section. There is no performance difference whatsoever between weight decay and our method, since both generalize perfectly. This is not the point of our work. Since the train and test entropy are effectively zero, the model complexity dominates the total description length, and so our method (which drives learning towards less complex structures) achieves a better total compression of the dataset, as demonstrated in the total description length plot.\n\n__\n> We see in Figure 4 that the proposed regularization scheme achieves the lowest complexity. However, the complexity is computed by Algorithm 2 and the proposed regularization is precisely penalizing the quantity computed by algorithm 2. Therefore it does not seem surprising that it is the lowest. As an ablation study, it would be interesting to make the comparison using other complexity notions. \n\nIt\u2019s the other way around: because our complexity measure is a proper complexity metric, we want to optimize it directly. We cannot, though, since the final step is not differentiable (zipping the weights). The spectral entropy penalty encourages the network to have low effective rank, but like L2, or any other information capacity proxy, it is not a complexity metric. It is only one way the network can be complex, but there are many such ways (all possible representation spaces)."
            }
        },
        {
            "title": {
                "value": "Response 1.2"
            },
            "comment": {
                "value": "> What practical implications does this paper have? I would consider a method practically useful if: (1) it can speed up grokking and/or (2) it can compress real-world datasets better than baselines.\n\nThis is a **theory paper**, submitted to the learning theory track. We are interested in the basic science of learning, complexity, and generalization. Our primary concern is to understand the nature of complexity and generalization more precisely. To this end, our method provides a universal, computable complexity measure that can be (and already is) used to study complexity in any parameterized mode. Furthermore, our theory connects complexity with information capacity in a fundamental way, through both quantization and low-rank decomposition. Questions of key importance to ML practitioners include: How much can we quantize our model (e.g. an LLM)? How much can a model be distilled into a smaller one (e.g. what is the lowest-rank model which can achieve this performance)? How results can help answer these questions. Even more intriguingly, in our view, is the question of emergence. What kinds of abstractions/representations emerge during learning? Are they simple or complex? What does complex even mean? Will what my model learned generalize to new examples? These are the sorts of questions that our work asks, and contributes to answering.\n\nIn terms of practical results, our method causes grokking to happen faster than with weight decay alone (see final figure in the appendix). Because the models it produces are less complex than the alternatives, it also achieves better compression on its datasets, since, as we discussed, the model size must be considered as part of the total compressed size of the dataset.\n\n__\n\n> the paper reads like a collection of ok-ish results but none of them is impressive enough.\n\nNo one has yet explained grokking in a way the community accepts. We think that understanding the grokking phenomenon in terms of the network complexity dynamics sheds light on the nature of abstraction formation in neural networks, and which has the potential to fundamentally change how the community understands the dynamics of complexity in learning models."
            }
        },
        {
            "title": {
                "value": "Response 1.1"
            },
            "comment": {
                "value": "Thanks for your review.\n\n**Key Motivation**: The motivation is to understand the nature of complexity and generalization better. Why and when do neural networks generalize? How can we know if they\u2019ve learned a good explanation? What kinds of abstractions emerge in the networks? Do they learn complicated explanations, or simple ones?\nThe grokking phenomenon is the clearest example we know of where during training the networks undergo a clear phase transition between memorization and generalization, so it is a perfect test-case to understand the relationship between generalization and complexity. Our results demonstrate that networks are highly complex when they have simply memorized their training data, and that when they generalize, they become much simpler. Our complexity dynamics plots demonstrate this transition from high to low complexity clearly.\n\n**Comparison with previous work**: As discussed in the related work section, Liu et al produce a complexity proxy, but their measure cannot be used to construct a generalization bound. They do not prove that their measure guarantees generalization behavior. The same is true of Humayun et al. In contrast, our method bounds the Kolmogorov complexity, which results in an explicit generalization bound in turn. Furthermore, our construction is universal, and can be applied to any kind of parameterized model.\n\nRE: Del\u00e9tang et al, this work shares our view of sequence modeling as compression, but does not study grokking. Their work is focused on dataset compression with LLMs, but not on the model complexity. Our work demonstrates that model complexity must be considered jointly with data compression to understand generalization.\n\n__\n\n> Could you elaborate on the comparison with bzip2? What is being compressed, problem setup, compressed file size, etc.?\n\n**Relationship to zip**: It turns out that all else being equal, more compressible models provably generalize better, so there is a deep connection between how compressible a model is and how well it generalizes. Ultimately, one can bound the complexity of the network by its information content (e.g. its filesize). Hence, we want to know: how much can we compress this model? If we simply try to zip the weights, we don\u2019t achieve any meaningful compression because of the random information in the network, so we don\u2019t get insight into the model complexity. In this work, we presented a way to get rid of the noise in the weights, and we made this procedure formal using rate\u2013distortion theory, which is the same theory that underlies related compression schemes, such as JPEG. So in our work, we give a formal theory of network compression, and connect that theory to equations which tell us how well a network will perform on unseen examples (its generalization performance). Our compression scheme has multiple steps to get rid of noise (quantization, low rank approximation), and the final step is to zip the de-noised weights, to get a complexity measure in bytes. A proper complexity measure like ours allows us to give an explicit generalization bound which is universal.\n\n__\n\n> The papers claim a 30-40x improvement in compression ratio, but I did not find and details or data.\n\nWe have added the naive bzip2 filesizes of the networks, shown in Figure 9 in the appendix. Note the y-axis scale in comparison to Figs 1 and 2. The final complexity of the regularized networks as measured by our method is 30-40x smaller than with naive bzip2."
            }
        },
        {
            "title": {
                "value": "Response 1.2"
            },
            "comment": {
                "value": "> While entropy regularization surely helps in compressing the model, I expect that both the usual L2 regularization and the entropy regularization will achieve perfect test accuracy. Could you think of a scenario where the proposed regularization technique offers a clear performance advantage over L2 regularization?\n\nYes, one can achieve perfect test accuracy with almost any regularization method. It is not difficult to get these models to generalize. The point of this work is not to propose a new regularization scheme which performs better than weight decay across a range of tasks: the point is to clarify the relationship between complexity and generalization in neural networks. As we discuss in the paper, L2 is not a valid complexity metric since networks can be arbitrarily rescaled. A proper complexity metric must have units of information. Using our complexity metric, one can construct explicit generalization bounds (Equation 4), unlike prior works which study complexity in grokking.\n\n__\n\n> Will entropy regularization also help in training larger models with more complicated datasets, where they often do not have simple representations as one-dimensional numbers?\n\nYes, the spectral entropy regularization will always penalize models towards low-rank solutions. Of course, if the regularization strength is too large, this can lead to model collapse, just like any other regularization method. There is no particular relationship between the fact that grokking occurs on modular arithmetic equations, and the complexity of the models.\n\n__\n\n> Could the computational overhead of low-rank optimization become significant, especially when applied to large models? If so, how could we mitigate them?\n\nUltimately, in this work we want to track the complexity as closely as possible to get a sharp picture of the complexity dynamics throughout training, to illustrate the phase transition from memorization to generalization. In real-world applications, one probably does not need to get complexity estimates this densely, and if one is only interested in the final performance of the model, they could get a complexity estimate once at the end of training."
            }
        },
        {
            "title": {
                "value": "Response 1.1"
            },
            "comment": {
                "value": "> While the complexity explanation of grokking is interesting, it seems to overlap with the circuit efficiency explanation proposed by Varma et al. (2023). Although the authors acknowledge that model complexity is not exactly identical to efficiency or parameter norms, the added insights in this area feel somewhat limited.\n\n**Formal Complexity Measure**: We strongly disagree that our added insights are limited. We were indeed inspired by Varma et al to study network complexity. The issue is that the L2 norm, which they appeal to to explain grokking, is not a complexity measure. This is a widespread misunderstanding in the ML community, which has led to much confusion. We aim to clarify that confusion with this work. Aaronson et al connect effective complexity with coarse-graining, but give no formal justification for this connection. We have formalized Aaronson et al\u2019s insight using algorithmic rate\u2013distortion theory, and then demonstrated the effectiveness of our proper universal complexity measure by applying it to explain grokking in terms of the fundamental information content inside a network. We believe our work has far-reaching implications for understanding generalization, model compression, and quantization schemes. Understanding the nature of generalization and its relationship to complexity is a question of **central importance** in machine learning.\n\n__\n\n> The proposed model compression methods are quite similar to existing techniques on quantization and low-rank approximations, which raises questions about the novelty of the approach. Spectral entropy-based regularization is an interesting idea, but concerns about potential computational overhead and their applicability in more complex settings remain.\n\n**Relationship to prior work**: It is true that others have studied quantization and low-rank approximation in machine learning. These are topics of fundamental importance, and we make no claim whatsoever about our use of these elements being novel per se. Our contribution is to connect these fundamental ideas with another: complexity. Why can models be quantized to different degrees? Why can they sometimes be distilled into smaller networks? Our work takes a key step toward clarifying these questions by illuminating the relationship of these basic ideas with complexity and generalization. **Can you be more specific about what concerns you have about the spectral regularization method being applied in \u201cmore complex settings\u201d?** \n\n**Followup work**: Now that we have this complexity measure, we are producing follow-up work applying these insights to other domains and scaling our method up. We\u2019re excited to share these results with the community, but this first paper laying out the conceptual framework and demonstrating it on the grokking tasks is, in our view, the appropriate first step.\n\n__\n\n> Lastly, the applicability of entropy regularization techniques in more complex problems beyond the modular arithmetic task raises some concerns. Additional evidence or analysis demonstrating how this technique can advance the complexity-performance Pareto frontier in more difficult tasks will strengthen the paper.\n\n**Pareto frontier**: Because we are introducing a lot of conceptually new pieces in this work, we want to stay completely focused on the clearest possible example of complexity dynamics and generalization: grokking. However, we absolutely agree regarding the question of a Pareto frontier. We have updated the paper to include a plot (Fig 3) showing how the complexity of the models differ at different distortion levels, and find that our method represents a Pareto improvement over weight decay.\n\n__\n\n> How did you set the learning rates for experiments? Does the performance of entropy regularization vary with different learning rates?\n\nAs mentioned in the experiments section of the paper, we use exactly the same settings as the original grokking work, to avoid any additional complication or changes. Because of this, we only used the default rate of 1e-3. We have added a hyperparameter table to the appendix."
            }
        },
        {
            "comment": {
                "value": "Thank you for your review. \n\n**Chosen Datasets**: We intend to study more complex datasets in our next work, but we have kept this paper focused on the original grokking tasks (without making any changes), to maintain clarity and comparability with the key prior work. \n\n\n**Discussion of What Constitutes an Explanation**: It\u2019s an interesting philosophical question whether this constitutes an \u201cexplanation\u201d of grokking, as you mention! What might one mean by an explanation? If we use different networks, with different activation functions, different topologies, different regularizers, the learned representations will, in all likelihood, be different. That is, the microscopic dynamics/representation of the network weights might not be the appropriate level of description, and so might not constitute an \u201cexplanation\u201d. The point here is that some abstraction emerges in the network which lets it generalize. To use an analogy: the dynamics of both water and air are governed by the hydrodynamic equations (they follow the same emergent macroscopic phenomenon), but their microscopic components are completely different (H20 vs a mix of gases). What constitutes an \u201cexplanation\u201d of their behavior? Does one want to appeal to the microscopic dynamics (quantum mechanics), or is it enough to show that the same abstraction emerges? Our argument in this work is that we can actually ignore the microscopic weight structures, and *explicitly* bound the generalization performance (which is what one generally cares about in ML) using the complexity, which is properly thought of as a macroscopic phenomenon in the context of coarse-graining. Ultimately, it will be up to the community to decide what constitutes an explanation for grokking.\n\n**RE: Bzip, and compression time**: Yes, it\u2019s true these are not ideal. One could imagine some specialized compression scheme for weight matrices performing better here. However, the point of this work was not to produce the tightest possible complexity estimates at all costs, but to develop the conceptual framework and give a simple implementation. For our purposes, a simple off-the-shelf compressor like bzip2 is sufficient. We also experimented with gzip for the final compression step, which produced similar results with slightly worse compression ratios. When understanding the exact generalization performance is necessary for critical systems, one would want to use the best possible compression scheme within one\u2019s compute budget.\n\n**Followup Work**: Now that we have this complexity measure, we are producing follow-up work applying these insights to other domains and scaling our method up. We\u2019re excited to share these results with the community, but this first paper laying out the conceptual framework and demonstrating it on the grokking tasks is, in our view, the appropriate first step.\n\n**Additions to updated draft**: We have added a number of plots to the updated version of the paper. In particular we would like to point out the plots of the rate\u2013distortion curves for the different regularization methods. You can see that our method Pareto-dominates weight decay at every distortion level, indicating the strong performance of our regularizer. Furthermore, we\u2019ve added complexity plots for the unregularized network, so that you can see the complexity dynamics in the case where generalization does not occur. Finally, we also added plots showing the effective rank of the networks with different regularizers, which demonstrates how our spectral entropy regularizer enforces an effective low-rank penalty, helping us outperform weight decay in complexity."
            }
        },
        {
            "title": {
                "value": "Response 1.2"
            },
            "comment": {
                "value": "> in line 358 you state \"..we show that regularizing the spectral entropy leads to grokking..\" Is this an overstatement? How exactly is grokking defined quantitatively?\n\nNo, this is not an overstatement. Regularizing the spectral entropy alone does cause grokking, which is defined as perfect generalization after overfitting. It is not difficult to cause grokking with different regularizers, however, this is not a central claim of our work, so we have changed this line to better make our point: we now merely state that our regularization method also causes grokking. The final plot in the appendix shows grokking induced by our regularization method. Grokking is defined by a distinct memorization phase where train accuracy is 100%, and test accuracy is low (<30%, often 0%), followed by a generalization phase where test accuracy goes to 100%. We plotted these accuracy curves in the final figure in the appendix to demonstrate that all regularized networks grok, and unregularized networks do not grok.\n\n\n> In Figure 3, you compare your regularization technique with weight decay. What is the dependence of the proposed spectral entropy regularization on the regularization weight? What behavior do you notice as you apply more or less spectral regularization? It would be nice to see the effect as the regularizaiton of the spectral entropy gradually increases.\n\nLike any regularization method, one ideally wants to find a good hyperparameter for each new model class, loss function, and optimizer. There is no universally correct regularization weight, generally. We apologize that a hyperparameter table was missing from the original submission. We have included a hyperparameter table in the updated draft.\n\nAs we show in the updated draft, Fig 8 in the appendix, the effect of the spectral entropy regularization can be seen clearly as a decrease in the effective rank of the matrix.\n\n>Does Figure 4 include multiple seeds? Why are error bars not visible in this plot?\n\nYes, the total description length plots are produced with multiple seeds. The lack of error bars was an oversight on our part, which we have remedied in the updated draft.\n\n> in Figure 2. Why include the \"ours\" distinction when all plots are \"ours\".\n\nWe have updated this figure to remove \u201cours\u201d, and only mention it in the caption. We have also added the complexity and accuracy plots for unregularized networks, so that you can see the relationship of the complexity dynamics with train and test accuracy in the case where generalization does not occur.\n\n**Conclusion**: Overall, we wish to emphasize that prior works which study complexity in grokking do not use true complexity measures, only proxies of complexity. Our complexity metric is based on the Kolmogorov complexity, which is universal and so can be applied to any model class. Our use of Kolmogorov complexity results in explicit generalization bounds. \n\nWe agree with your point that one wants to know whether the complexity measure being used guarantees generalization performance, vs merely correlates with generalization. This is *why* one wants an explicit generalization bound, and is a particular strength of our work vs previous works. Because they only study proxies of complexity, they cannot, in general, produce such bounds, whereas we can since we use a universal measure of complexity.\n\nWe are excited to share this development in the theory of complexity and generalization with the community, and think that many people will be interested.\n\n[1]: Varma, Vikrant, Rohin Shah, Zachary Kenton, J'anos Kram'ar and Ramana Kumar. \u201cExplaining grokking through circuit efficiency.\u201d ArXiv abs/2309.02390 (2023)\n\n[2]: Nakkiran, Preetum, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak and Ilya Sutskever. \u201cDeep double descent: where bigger models and more data hurt.\u201d Journal of Statistical Mechanics: Theory and Experiment 2021 (2019)\n\n[3]: Power, Alethea, Yuri Burda, Harrison Edwards, Igor Babuschkin and Vedant Misra. \u201cGrokking: Generalization Beyond Overfitting on Small Algorithmic Datasets.\u201d ArXiv abs/2201.02177 (2022)"
            }
        },
        {
            "title": {
                "value": "Response 1.1"
            },
            "comment": {
                "value": "Thanks for your review.\n\n**Comparison with other complexity measures**: The most commonly used proxy of complexity in the machine learning community is the L2 norm (followed by parameter count) [1,2]. As we discuss in the paper in the Related Work and elsewhere, the L2 norm is not a proper complexity metric, and its use as a complexity measure has led to a substantial amount of confusion. Indeed, one cannot construct a generalization bound from the L2 norm alone. To see this, note that a network can be re-scaled arbitrarily: We can multiply all our weights by an arbitrarily large or small constant. If a network\u2019s weight are all 10^5 or all 10^-5, the L2 norm reports vastly different \u201ccomplexities\u201d, whereas both are in fact simple. A true measure of complexity must have units of information.\n\nOn the other hand, Kolmogorov complexity is universal, and can be explicitly connected with generalization, as we show in Equation 4. \n\nIn a few very specific cases of particular statistical hypothesis classes, one can construct alternative complexity measures. However, these measures simply do not apply to generic neural networks, and so have no relevance in this setting. In addition to demonstrating the complexity dynamics that occur during grokking, we are trying to clarify the situation regarding model complexity by giving an explicit upper bound on a universal complexity measure via compression.\n\n**Correlation vs Causation**: You point out that it is unclear whether our measure only \u201ccorrelates\u201d with generalization, and that seeing it compared to other measures would help. In fact, this is the point of Equation 4: it guarantees a bounding relationship between the Kolmogorov complexity and the generalization performance. In related works like [1], they cannot guarantee any generalization performance\u2013this is because their measures are not true complexity measures.\n\n**Cost of Computing Complexity**: We use a Bayesian optimizer to search for coarse-graining settings (ways of reducing the information content of the network) which achieve the same performance as the original network. Algorithm 1 shows this procedure. The number of Bayesian optimization steps is a hyperparameter. In our experiments, we set it to 50, which is relatively modest. This parameter could be changed to support various compute budgets.\n\nIn this work we are not concerned with the computational cost of the complexity estimate: the networks and datasets are not very large. The central goal is to understand the complexity dynamics in depth. The complexity estimation budget will vary depending on the goal of the work. Here we want to get a good estimate of the complexity at every step. In many cases, practitioners will only want to know the complexity of the final model, and can perform this step only one time, after training.\n\n__\n\n> From what I understand, this complexity measure is somewhat dependent on the hyperparameters, in particular the per-layer truncation threshold \u03ba(\u03c4)\n\nThis is not correct: k(\\tau) is not a hyperparameter. The Bayesian optimization procedure searches for values of k(\\tau) at each step, to produce the tightest complexity estimate within its compute budget, as mentioned above. k(\\tau) is merely a way to allow for different degrees of rank decomposition per layer.\n\n> What is the exact definition of the novel complexity measure introduced in this paper?\n\nThe measure of complexity we use is given in Equation 5, the algorithmic rate\u2013distortion function. It returns the Kolmogorov complexity of the coarse model which satisfies the distortion bound.\n\n> For which models is this complexity measure well defined?\n\nThis complexity measure is well-defined for all possible models. Kolmogorov complexity is universal.\n\n> in line 400, can you clarify which subset of grokking experiments you used. And why you used this subset.\n\nWe have expanded the discussion in the experiments section. For simplicity, we chose the first 4 grokking experiments reported in the original grokking paper [3], and which are also studied in [1]. These tasks are the best studied, but we had no other particular reason for choosing them.\n\nWe view this simplicity as a virtue: complexity is a difficult and subtle concept, as this discussion shows. We choose to study complexity in the simplest possible setting to clarify its nature. The original grokking paper [3] studies 12 different simple algorithmic tasks made of binary operations. [1] reduce this to a subset of 9 of the original tasks, although they increase the size of the prime field from 97 to 113. We have expanded our discussion of these settings and added a hyperparameter table to the appendix to more clearly explain our experimental setup."
            }
        },
        {
            "summary": {
                "value": "The authors introduce a new complexity measure for neural networks and claim that this complexity measure can be used to explain 'grokking'. \"Grokking\" in machine learning is this idea that neural networks suddenly transition from memorization to generalization long after overfitting the training data. They show that their complexity measure correlates with this 'grokking' and then show how this complexity measure can be used to define a new regularization method which encourages low-rank representations. This regularizer is defined using the spectral entropy of the network weights."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Understanding the role of model complexity and how it should be measured is an important question in machine learning. This paper takes a good step in this direction and presents a compelling case for a complexity measure which is defined using the minimum description length and ideas from compression and information theory. The paper contributes to a deeper understanding of this 'grokking' phenomenon, which has gotten significant attention in recent years. \n\nThe paper has good theoretical motivation and makes an interesting connection with the concept of grokking in machine learning. Their intrinsic complexity measure and regularization technique are well-grounded in theoretical concepts from information theory. The authors provide clear explanations and justifications for their design choices.\n\nThe paper is logically structured and well-written and supports their theoretical claims with experiments on synthetic tasks, like modular arithmetic, for decoder transformer models."
            },
            "weaknesses": {
                "value": "The complexity measure defined and explored in this paper is positioned as a way to 'explain grokking'. \n\nComparison with other complexity measures. The empirical results in the paper are nice. But it would be good to have a fair comparison of how other complexity measures look when measure in the same scenarios. It's unfair to say that this new complexity measure \"explains\" grokking without uncovering a scenario where this complexity measure is able to capture this behavior where others are not. Otherwise, it's unclear if this is just a correlation relationship with the perceived behavior of 'grokking'. \n\nLacking discussion of the cost for computing this complexity measure. If I understand correctly, the proposed complexity measure involves a Bayesian optimization procedure for finding the optimal compression parameters, which could be computationally expensive. It would be nice to address or (ideally) investigating how difficult this measure is. This would  enhance the practicality of the approach.\n\nFrom what I understand, this complexity measure is somewhat dependent on the hyperparameters, in particular the per-layer truncation threshold $\\kappa(\\tau)$. It would be nice ot have a detailed analysis even experimentally of the sensitivity to this threshold.\n\nThis paper has some very nice ideas and is worth exploring but it would be good to have a section on Limitations of their approach with an honest assessment in terms of other complexity measures and the degree to which the results are not just correlational with this 'grokking' behavior. \n\nThe paper is carefully written and has a nice re-cap of the relevant ideas from information theory and compression in ML. However, the main message of the paper was at times hard to find. For example, what is the exact definition of this new complexity? I understand it relies on coarse-graning of the network and compression using bzip2 and I think the size of the compressed network is the proxy for the complexity. Is that the definition? This paper would benefit from more clear exposition in this respect."
            },
            "questions": {
                "value": "- What is the exact definition of the novel complexity measure introduced in this paper? And for which models is this measure well-dfined. The related conversation about compression and motivation from information theory and Kolmogorov complexity is very nice but it's unclear to me exactly how this measure is defined. Is this the content of Algorithm 2? Does the output of Algorithm 2 define the complexity measure? \n - in line 400, can you clarify which subset of grokking experiments you used. And why you used this subset. \n - in line 358 you state \"..we show that regularizing the spectral entropy leads to grokking..\" Is this an overstatement? How exactly is grokking defined quantitatively?\n - In Figure 3, you compare your regularization technique with weight decay. What is the dependence of the proposed spectral entropy regularization on the regularization weight? What behavior do you notice as you apply more or less spectral regularization? It would be nice to see the effect as the regularizaiton of the spectral entropy gradually increases.\n - Does Figure 4 include multiple seeds? Why are error bars not visible in this plot?\n\ntypos/nits\n - in Figure 2. Why include the \"ours\" distinction when all plots are \"ours\". \n - line 372, \"ideas\" to \"ideal\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the phenomenon of grokking through the lens of complexity theory and rate distortion theory. It proposes ways to compress model weights: \n-- Via a parameter quantization operation, as a twist on ideas of Hinton and Van Camp\n-- Via a low-rank approximation operation.\nThe idea is compress the models up to certain rate distortion thresholds, quantified by the loss. \nThey find that this compression is substantially more powerful than traditional compression methods (bzip) and argue that this is a better approximation of the Kolmogorov's complexity of the model.\nUsing this metric, the authors perform experiments on arithmetic operations and find that the grokking phase is associated with a drop from the complexity peak. Following this idea, they propose a new regularizer that apparently increases the grokking effect.\n\nOverall, this is a very well-written paper that lays out super interesting ideas and presents a compelling thesis and nice experiments. I am not sold on the idea that this is an explanation of grokking, but the observations and the conclusions are overall very interesting and I think this is a valuable contribution to understanding better what happens with grokking and is quite promising to improve learning performance of models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Excellent writing, compelling ideas, nice experiments, convincing thesis, possible follow-ups."
            },
            "weaknesses": {
                "value": "Is it really an explanation of grokking or more some interesting and attractive observations?\nThe experiments with the regularizer are not many."
            },
            "questions": {
                "value": "Have you tried applying these ideas to more complex datasets, does it compare favorably vs weight decay techniques ?\n\nBzip is not ideal to compress weights... are there other points of comparisons available?\n\nWhat is the efficiency of your compression method? How long does it take to compress?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce a measure of neural networks\u2019 complexity, and show that grokking could be explained by the rise and fall of the model\u2019s complexity. The authors also propose methods for compressing neural networks via quantization and spectral entropy-based regularization, and empirically demonstrate their performances with modular arithmetic tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is generally clear, and easy to read and interpret.\n- The paper provides nice intuitions on building generalizable neural networks, especially from the model complexity perspective.\n- The paper considers an interesting set of techniques for model compression with minimal performance loss, and tests them with experiments."
            },
            "weaknesses": {
                "value": "While the paper considers several promising ideas for model compression, there are a few limitations:\n- While the complexity explanation of grokking is interesting, it seems to overlap with the circuit efficiency explanation proposed by Varma et al. (2023). Although the authors acknowledge that model complexity is not exactly identical to efficiency or parameter norms, the added insights in this area feel somewhat limited.\n- The proposed model compression methods are quite similar to existing techniques on quantization and low-rank approximations, which raises questions about the novelty of the approach. Spectral entropy-based regularization is an interesting idea, but concerns about potential computational overhead and their applicability in more complex settings remain.\n- Lastly, the applicability of entropy regularization techniques in more complex problems beyond the modular arithmetic task raises some concerns. Additional evidence or analysis demonstrating how this technique can advance the complexity-performance Pareto frontier in more difficult tasks will strengthen the paper."
            },
            "questions": {
                "value": "1. How did you set the learning rates for experiments? Does the performance of entropy regularization vary with different learning rates?\n2. While entropy regularization surely helps in compressing the model, I expect that both the usual L2 regularization and the entropy regularization will achieve perfect test accuracy. Could you think of a scenario where the proposed regularization technique offers a clear performance advantage over L2 regularization?\n3. Will entropy regularization also help in training larger models with more complicated datasets, where they often do not have simple representations as one-dimensional numbers?\n4. Could the computational overhead of low-rank optimization become significant, especially when applied to large models? If so, how could we mitigate them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to study the grokking dynamics via the lens of information theory (minimum description length). In particular, they proposed: (1) a new compression algorithm to compress the neural network; (2) a new regularizer based on spectral entropy. They show that the spectral entropy regularizer outperforms the standard weight decay to the extent that a model with lower complexity is obtained. They claimed a factor of 30-40x improvement of the compression ratio over bzip2, which is impressive (although I can't find the file size data). However, none of the compression methods achieve a non-vacuous bound, since models are vastly over-parametrized."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper is well-written and very readable\n* The paper presents \"new\" theoretical tools to analyze neural networks\n* The analysis is a new angle to understand grokking"
            },
            "weaknesses": {
                "value": "* This paper deals with too many things simultaneously, which makes me a bit lost. What's *the* motivation of this paper? Otherwise, the paper reads like a collection of ok-ish results but none of them is impressive enough. For example, the idea of grokking as compression has been explored by [Liu et al.], [Humayun et al.] and [Deletang et al.]. The idea of using spectral entropy as a measure is explored in [Liu2 et al.], although it is novel to regularize the network with spectral entropy (which is unfortunately expensive).\n* The papers claim a 30-40x improvement in compression ratio, but I did not find and details or data. \n* Although this is a more theoretical paper than an experimental paper, I am not sure about its practical implications.\n\n**References**\n\n[Liu et al] Grokking as Compression: A Nonlinear Complexity Perspective, arXiv: 2310.05918\n\n[Del\u00e9tang et al.] Language Modeling Is Compression, ICLR 2024\n\n[Humayun et al] Deep Networks Always Grok and Here is Why, arXiv: 2402.15555\n\n[Liu2 et al] Towards Understanding Grokking: An Effective Theory of Representation Learning, NeurIPS 2022"
            },
            "questions": {
                "value": "* What's the key motivation of this paper?\n* Could you elaborate on the comparison with bzip2? What is being compressed, problem setup, compressed file size, etc.?\n* What practical implications does this paper have? I would consider a method practically useful if: (1) it can speed up grokking and/or (2) it can compress real-world datasets better than baselines."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the grokking phenomenon through compression-based approaches. Inspired by recent work on the intrinsic complexity of neural networks, and combining it with ideas from rate-distortion, quantization and low-rank approximation, the authors propose a new measure of neural networks complexity, which consists essentially of a coarse-graining procedure. They conduct experiments on simple arithmetic tasks which demonstrate that the rise and fall of this complexity might be predictive of the network starting to generalize. Moreover, this leads them to propose a new regularization scheme, based on spectral entropy, whose effect seems to reduce the total description length and the generalization bound, compared to other methods. This might lead to non-vacuous generalization bounds."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Grokking is an important topic for the community\n\n- The experiments suggest that the proposed regularization technique based on spectral entropy may induce grokking, which may be of practical interest.\n\n- The experiments suggest that the rise and fall of the proposed complexity seems to be predictive of when the model starts to generalize.\n\n- The proposed regularization techniques lead to better generalization bounds than classical weight decay or no regularization."
            },
            "weaknesses": {
                "value": "- Several notions are mentioned repeatedly but without being formally defined, such as capacity, distortion or $(\\lambda,\\delta)$ (Equation (9)). It would improve the paper to include additional theoretical background and more formal definitions. \n\n - It should be made clearer how the quantities introduced in Sections 3.1 and 4 are related to generalization. For instance, is it possible to write down a theorem with explicit dependence on these quantities, or are their consideration partially based on intuitions? Can the link of these quantities with Kolmogorov complexity be made more formal?\n\n - Despite the lack of formal theorems and proofs, the experiments are done on very simple arithmetic tasks. Therefore, it is not clear (neither theoretically nor empirically) whether the results may be generalized to more complex settings. I think that at least one experiment on a small dataset like MNIST or CIFAR10 could improve the paper.\n\n - It would be useful to include an experiment comparing the performance (in terms of accuracy) with and without the proposed regularization scheme. Indeed, we see that it reduces the MDL and the generalization bound, but, if I am correct, it is not clear whether it achieves better performance overall.\n\n - We see in Figure 4 that the proposed regularization scheme achieves the lowest complexity. However, the complexity is computed by Algorithm 2 and the proposed regularization is precisely penalizing the quantity computed by algorithm 2. Therefore it does not seem surprising that it is the lowest. As an ablation study, it would be interesting to make the comparison using other complexity notions. For instance, using the actual test accuracy would be very informative, to see whether the proposed regularization leads to better performance."
            },
            "questions": {
                "value": "- Is it possible to perform the same experiments on more complex but still relatively simple datasets like MNIST or CIFAR10?\n\n - Does the generalization bound of Equation (4) only hold for finite hypothesis spaces? If yes is that a realistic assumption in practical learning settings? Moreover, could you be more precise as to why the choice of Solomonoff prior should lead to tighter bounds than other priors, such as the uniform prior over $\\mathcal{H}$?\n\n - Line 181: Why can the empirical risk be understood as the entropy of the data under the model? Is there a way to formalize this fact?\n\n - Is it possible to obtain a formal statement relating the information capacity (Equation (9)) to generalization?\n\n - To what size and precision do the parameters $\\lambda$ and $\\delta$ (Section 4) refer to in practice?\n\n - How would the training accuracy be affected by the addition of Gaussian noise in practical deep learning settings?\n\n - Can you define more precisely the notations used in Algorithm 2, such as BO.SUGGESTPARAMETERS()? More generally, can you provide more details on the Bayesian optimization procedure?\n\n - Does your regularization technique always lead to lower test accuracy compared to weight decay?\n\n - Figures 3 and 5 are not analyzed in the text, can you add some insights on the result they present?\n\n**Remarks/questions regarding lines 152 - 155 and Equation (4)**  \nEven though it is not central to the paper, I have some questions about this part:\nAs I understand it, the bounds in terms of Kolmogorov complexity are obtained by choosing a good prior distribution in the bound of Langford and Seeger. It is not clear to me that such a choice of prior provides the most useful bound. More precisely, let $\\mathcal{H}$ be a finite set of hypothesis and $\\sigma : \\mathcal{H} \\to \\mathcal{H}$ be any bijection of $H$. Then $h \\mapsto 2^{K(\\sigma(h))}$ may be used as a prior instead of the usual Solomonoff prior, hence leading to a generalization bound in terms of $K(\\sigma (h))$. Yet another possibility would be to use the uniform prior over $\\mathcal{H}$. Therefore, choice of prior, and therefore the choice of Kolmogorov complexity as a generalization measure, seems to be arbitrary (please correct me if I am mistaken). Can you provide more insights as to why this leads to the most informative bound? \n\nI would be happy to discuss this further, please correct me if I misunderstood something.\n\n\n**Other minor remarks and typos**\n\n - In the introduction, the terms capacity and complexity are used before being defined, which may render the introduction hard to read. In general, more formal definitiosn of these concepts might enhance the readability of the paper. It could also help to define the notion of distortion function.\n\n - Line 122: regulariztion $\\to$ regularization\n\n - Equation (4): there is a missing parenthesis in $\\log(1/\\delta)$\n\n - There might be a clash of notation between the parameter $\\delta$ in Equations (4), (9) and (10). It would be clearer to use a different letter in each of these equations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}