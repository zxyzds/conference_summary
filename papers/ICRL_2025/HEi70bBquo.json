{
    "id": "HEi70bBquo",
    "title": "Signal Dynamics in Diffusion Models: Enhancing Text-to-Image Alignment through Step Selection",
    "abstract": "Visual generative AI models often encounter challenges related to text-image alignment and reasoning limitations. This paper presents a novel method for selectively enhancing the signal at critical diffusion steps, optimizing image generation based on input semantics. Our approach addresses the shortcomings of early-stage signal modifications, demonstrating that adjustments made at later stages yield superior results. We conduct extensive experiments to validate the effectiveness of our method in producing semantically aligned images, achieving state-of-the-art performance. Our results highlight the importance of a judicious choice of sampling stage to improve diffusion performance and overall image alignment.",
    "keywords": [
        "Visual Generative AI",
        "Diffusion Model",
        "Text-to-Image Alignment"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=HEi70bBquo",
    "pdf_link": "https://openreview.net/pdf?id=HEi70bBquo",
    "comments": [
        {
            "summary": {
                "value": "The paper titled \"Signal Dynamics in Diffusion Models: Enhancing Text-to-Image Alignment through Step Selection\" discusses a method to improve text-to-image alignment in generative AI models by enhancing signals at critical diffusion steps based on input semantics.  The study also highlights the importance of selecting the right diffusion steps for signal enhancement."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The assessment is comprehensive, indicating a thorough evaluation.\n2. The performance of the proposed method surpasses that of previous work, as demonstrated by the comparative results presented in the paper's table."
            },
            "weaknesses": {
                "value": "1. The source code is inaccessible via the anonymous link provided.\n2. The paper introduces numerous concepts without adequate explanation, which complicates comprehension:\n* The term \"GSN guidance\" is not clearly defined. Is it a concept coined by the authors? If \"GSN guidance\" refers to the optimization of latents, why isn't the proposal method considered a form of \"GSN guidance\"? What is the distinction between \"Ours\" and \"Ours+\"?\n* The phrase \"lacks inherent semantic meaning\" is used but not further elaborated upon.\n* It is unclear what the \"Similarity Score\" is intended to measure.\n3. The paper's novelty is questionable. The optimal iteration appears to be simply a result of the author's experimentation with various step iterations. But it has been widely accepted in research that detailed results are typically generated after the 8th step."
            },
            "questions": {
                "value": "Refer to the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduce Signal Dynamics in Diffusion Models, which explore the key stage of signal modifications that could help to yield superior results."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The method selectively enhance the signal at a key diffusion step, optimizing image generation.\n2. Explore and find the better stage to apply signal modification that leads to better results."
            },
            "weaknesses": {
                "value": "1. The paper lacks novelty. The main concept of the paper is to discover the best step to perform IterRef. It is not new and there are similar discoveries in FreeDoM[1].\n\n2. The paper lacks the comparison between different methods, there are many other methods that could also achive higher text-alignment like RPG[2], SLD[3], the results in the paper are not competitive enough to support the claim of sota.\n\n3. The current experimental analysis also appears insufficient. More evaluation metrics like FID,IS,T2I-CompBench[4] should be used to provide a more comprehensive results.\n\n[1] Yu, Jiwen, et al. \"Freedom: Training-free energy-guided conditional diffusion model.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[2] Yang, Ling, et al. \"Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms.\"\n\n[3] Wu, Tsung-Han, et al. \"Self-correcting llm-controlled diffusion models.\"\n\n[4] Huang, Kaiyi, et al. \"T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation.\" Advances in Neural Information Processing Systems 36 (2023): 78723-78747."
            },
            "questions": {
                "value": "1. Can this method applied to different architectures like current sota models SD3 or FLUX ?\n\n2. More evaluation results are needed between similar methods.\n\n3. What is the fundemental difference between your findings and the results showed in Fig.3 of FreeDoM ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an approach to improving text-image alignment that optimizes only one specific step during the generation process. Based on the analysis of experimental results, the authors show that later optimization brings better results. Extensive experiments are conducted to demonstrate the effectiveness of the methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is well written.\n2. The method is simple, clear, and easy to follow.\n3. The experiments are extensive and reproducible."
            },
            "weaknesses": {
                "value": "1. The qualitative results are insufficient to support the stated improvement: \ne.g., in Fig. 1 case \"a photo of a car and a blue cat\", the attribute blue is still leaked to entity car; \n        in Fig. 1 case \"a photo of a giraffe and a bear\", the bear is still not well generated; \n        in Fig. 1 case \"a photo of a giraffe and a banana\", the number of bananas is wrong;\n        in Fig. 6 case \"a photo of a giraffe next to a car and a carrot\", the carrots and the giraffe are mixed;\n        in Fig. 6 case \"a photo of a refrigerator next to a horse and a car\", the location of each object is not reasonable compared with other methods.\n2. The proposed method is incremental and lacks novelty, compared with Attend-and-Excite[1] and A-star[2].\n3. The quantitative improvement is marginal in most widely used metrics. The method outperforms others only in terms of TIAM[3], which is not a well-recognized evaluation metric yet.\n4. The experiments are conducted on SD 1.4, which falls behind current advanced models like SDXL, SD 3, and Pixart. Perhaps, the phenomenon in SD 1.4 and SDXL are different. The outdated base model makes this research less practical.\n\n[1]Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Trans. Graph., 42(4), jul 2023. ISSN 0730-0301. doi: 10.1145/3592116. URL https://doi.org/10.1145/3592116.\n[2] Aishwarya Agarwal, Srikrishna Karanam, K J Joseph, Apoorv Saxena, Koustava Goswami, and Balaji Vasan Srinivasan. A-star: Test-time attention segregation and retention for text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2283\u20132293, October 2023.\n[3]Paul Grimal, Herv\u00e9 Le Borgne, Olivier Ferret, and Julien Tourille. Tiam - a metric for evaluating alignment in text-to-image generation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 2890\u20132899, January 2024."
            },
            "questions": {
                "value": "1. See Weakness 4. It will be better if some experimental results on more recent models are provided.\n2. I would like to see the comparison of each method in terms of computational efficiency.\n3. The paper is not well organized enough and the authors might consider modifying the figures and tables to better present their work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}