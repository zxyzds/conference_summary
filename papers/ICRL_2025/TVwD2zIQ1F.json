{
    "id": "TVwD2zIQ1F",
    "title": "Provable Robustness of (Graph) Neural Networks Against Data Poisoning and Backdoors",
    "abstract": "Generalization of machine learning models can be severely compromised by data poisoning, where adversarial changes are applied to the training data. This vulnerability has led to interest in certifying (i.e., proving) that such changes up to a certain magnitude do not affect test predictions. We, for the first time, certify Graph Neural Networks (GNNs) against poisoning attacks, including backdoors, targeting the node features of a given graph. Our certificates are white-box and based upon (i) the neural tangent kernel, which characterizes the training dynamics of sufficiently wide networks; and (ii) a novel reformulation of the bilevel optimization problem describing poisoning as a mixed-integer linear program. Consequently, we leverage our framework to provide fundamental insights into the role of graph structure and its connectivity on the worst-case robustness behavior of convolution-based and PageRank-based GNNs. We note that our framework is more general and constitutes the first approach to derive white-box poisoning certificates for NNs, which can be of independent interest beyond graph-related tasks.",
    "keywords": [
        "graph neural networks",
        "provable robustness",
        "certificates",
        "poisoning",
        "data poisoning",
        "backdoor attacks",
        "neural tangent kernel",
        "adversarial robustness",
        "mixed-integer linear programming",
        "support vector machines"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "First deterministic & white-box certificates for (graph) neural networks against data poisoning & backdoors based on (i) the neural tangent kernel, and (ii) a novel reformulation of the bilevel poisoning problem as a mixed-integer linear program.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TVwD2zIQ1F",
    "pdf_link": "https://openreview.net/pdf?id=TVwD2zIQ1F",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a novel framework, QPCert, for certifying the robustness of GNNs, against data poisoning and backdoor attacks. This work leverages the NTK for white-box certification and reformulates the poisoning problem as a MILP to provide formal guarantees for GNNs. The authors explore this certification's effectiveness across various GNN architectures and benchmark datasets, and also analyze the effect of graph structure on robustness, presenting insights for architectural choices in GNNs with robustness considerations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is generally well-organized and thorough, with sections on methodological background, detailed steps in QPCert\u2019s derivation, and clear experimental results. Extensive experiments are conducted across multiple GNN architectures and datasets. The analysis includes diverse attack scenarios and perturbation models, highlighting the framework\u2019s adaptability and utility."
            },
            "weaknesses": {
                "value": "Although the focus is on feature perturbations, robustness against graph structure modifications is a critical issue for GNNs. Future work on structural robustness would greatly enhance the scope and impact of this framework.\n\nOn the assumption on large Width: The reliance on the NTK\u2019s emight limit applicability to smaller, practical network sizes. \n\nHow your work can be extended to graph classification?\n\nGiven the MILP reformulation, computational costs may scale poorly with large datasets,"
            },
            "questions": {
                "value": "No questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a white-box certification framework, QPCert, for evaluating the robustness of neural networks, specifically Graph Neural Networks (GNNs), against data poisoning and backdoor attacks. This framework, grounded in Neural Tangent Kernel (NTK) theory, reformulates poisoning detection as a mixed-integer linear program (MILP), enabling robustness certificates against node feature perturbations in GNNs. Thexperimental results to validate QPCert's effectiveness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper studies an interesting problem: the certified robustness against data poisoning and backdoor attacks.\n2. The extensive theoretical analysis is provided.\n3. The paper is well written."
            },
            "weaknesses": {
                "value": "1. While the theoretical analysis is promising, the empirical evaluation could be strengthened to more comprehensively demonstrate the effectiveness of the proposed certification method. Currently, only one attack, APGD [1], is considered in the experiments. Since certification methods aim to provide provable guarantees against all attacks within a specific threat model, it would be beneficial for the authors to clarify how their certification framework addresses the threat models of other commonly studied attacks, such as graph poisoning and backdoor attacks [2, 3, 4]. This would provide a more insightful discussion of the method\u2019s robustness and scope of applicability, beyond specific attack evaluations\n2. It appears that the Neural Tangent Kernel (NTK) approach is currently limited to specific GNN architectures such as GCN, SGC, and GraphSage, which could restrict the broader applicability of the proposed method. It would be helpful if the authors could discuss any challenges or limitations in extending their method to other GNN architectures or clarify why these specific GNNs were chosen as representative examples. Additionally, outlining potential directions for adapting the approach to other types of GNNs could strengthen the paper's discussion and broaden its scope.\n3. The proposed method addresses node feature perturbation attacks, yet structural perturbations are often more prevalent and impactful in real-world scenarios. It would strengthen the paper if the authors could discuss the challenges in adapting their method to structural perturbations or clarify their focus on feature perturbations. Additionally, a comparison of the practical implications of feature versus structural perturbations in real-world applications would provide valuable insights into the method's scope, limitations, and potential future directions.\n\n\n[1] Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks.\n\n[2] Adversarial Attacks on Graph Neural Networks via Meta Learning.\n\n[3] Graph Backdoor.\n\n[4] Unnoticeable Backdoor Attacks on Graph Neural Networks."
            },
            "questions": {
                "value": "1. Please add more experiments against some realistic graph poisoning and backdoor attacks.\n2. Please discuss applying the proposed method to more advanced GNNs.\n3. There are some works that study clean-label (graph) backdoor attacks [1, 2], can the proposed method be applied to this attack?\n\n[1] Clean-Label Backdoor Attacks.\n\n[2] A clean-label graph backdoor attack method in node classification task."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "I'm rating this paper as \"8: accept, good paper\".\n\nSummary: the paper proposes a certification against white-box and backdoor attacks, by making use of Graph NTK and the fact that for sufficiently wide networks the network is throughly understood given the input data and the neural tangent kernel."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Clear writing\n- Elegant ideas to make the bi-level optimisation feasible and tracktable"
            },
            "weaknesses": {
                "value": "-"
            },
            "questions": {
                "value": "- In line 369 it is mentioned that: \"... We note as we are the first work to study white-box certificates for clean-label attacks on node features in graphs in general, there is no baseline prior work. ...\". Even if there are no baselines for white-box attacks, aren't the certificates for black-box baselines still applicable? Specially because these methods are different in, e.g., their relaxation which may differently affect their performance and one cannot presume superiority of certificate in black-box versus white-box setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors leverage the Neural Tangent Kernel (NTK) to capture the complex training dynamics of GNNs and reformulate the bilevel optimization problem describing poisoning as a mixed-integer linear program. This allows them to provide white-box certificates for GNNs and neural networks in general, which was previously an unsolved problem."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- Provide a comprehensive theoretical analysis to support the methodology \n- Very inspiring to leverage NTK to defend the adversarial and backdoor attacks\n- Reformulated the bilevel optimization problem describing poisoning as a mixed-integer linear program"
            },
            "weaknesses": {
                "value": "- this paper only considers the scenario where the adversaries manipulate the node features, but there are plenty of graph attack works that manipulate the graph structure and the node features at the same time. Could the method in this paper be leveraged for poisoned data with both graph structures and node features manipulated?\n- The experiments only compare the GNNs with MLP. Could authors provide some adversarial/backdoor defense baselines? While the comparison with MLPs demonstrates the positive impact of QPCert, it does not adequately illustrate the extent of QPCert's effectiveness.\n- In Figure 2, MLP consistently has the lowest accuracy when evaluated in the PL task, but MLP can have a better performance than some GNNs in Figure 3 (PU and BU tasks) when perturbation budges are large. Could the author explain what factors lead to these differences?"
            },
            "questions": {
                "value": "the same as weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}