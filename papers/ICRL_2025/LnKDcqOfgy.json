{
    "id": "LnKDcqOfgy",
    "title": "Rate/Distortion Constrained Model Quantization for Efficient Storage and Inference",
    "abstract": "The proliferation of large pre-trained neural networks has recently revived research in both quantization of network weights (for faster inference), and in their\ncompression (to reduce file sizes). However, there has so far been little idea transfer between the two lines of research. In this paper, we combine techniques from\nquantization and compression to propose an efficient and highly effective post-training compression method for large neural networks. Our method extends the\nrecently published quantization method OPTQ (Frantar et al., 2023) with a tunable\nrate/distortion trade-off by introducing a cost per bit into OPTQ's rounding\noperation. Crucially, we estimate the bit rate based on the predictive model used\nin the state-of-the-art neural network compression method NNCodec (Becking\net al., 2023). In our experiments with several standard pre-trained networks from\nthe computer vision community, our method leads to significantly (up to 2.7x)\nsmaller file sizes than NNCodec at equal model performance, generally compressing to less than half a bit per network weight and implicitly pruning insignificant weights.\nAdditionally, and in contrast to NNcodec, our method offers the same opportunities for inference speed-ups as OPTQ. By proving that file size and inference\ncost can be reduced simultaneously, we hope that our contribution shows a path\ntowards deploying large neural networks on end-user devices, alleviating privacy\nconcerns, regulatory constraints, and dependency on large service providers.",
    "keywords": [
        "quantization",
        "model compression",
        "rate-distortion theory",
        "compression"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We propose a compression method for pre-trained neural networks that combines quantization and entropy based neural network compression.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=LnKDcqOfgy",
    "pdf_link": "https://openreview.net/pdf?id=LnKDcqOfgy",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a post-training compression technique that combines recent advancements in quantization and compression. It extends the OPTQ framework by incorporating a rate-distortion trade-off, achieved through the addition of a bit-cost function inspired by NNCodec\u2019s entropy model. The resulting OPTQ-RD method effectively balances compression strength and inference speed, achieving high compression ratios with minimal performance degradation across various CNNs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The method can be applied without modifying network architectures, broadening its utility for different models.\n\nThe method\u2019s ability to operate in a post-training setting with minimal calibration data makes it practical for real-world deployment.\n\nThe evaluation demonstrates that the proposed method consistently outperforms the baselines in terms of weight compression while preserving accuracy on par with them."
            },
            "weaknesses": {
                "value": "## Claims:\n\nThe paper does not reference previous methods that address the same problem [1,2].\n\nThe authors claim that OPTQ-RD achieves fast inference time; however, as I understand, only the weights are quantized, while activations remain in the floating point, typically resulting in a minimal reduction in inference time. This approach may reduce memory usage or storage, particularly in the context of compression and quantization for large language models (LLMs). However, it has a limited impact on convolutional neural networks (CNNs), especially those used in the experiments.\n\n\n## Experiments:\n\nThe experimental setting is quite limited. VGG, which is not commonly used in practice, is known to have sparse weights, making it difficult to generalize the findings to more widely adopted network architectures.\n\n\nAlthough bits per weight appears promising, Figure 1 and Table 1 provide little insight into its relevance to practical metrics, such as latency or energy consumption. Additionally, the plots suggest that, in intermediate cases, there is only a narrow range where the proposed method outperforms the vanilla baseline of OPTQ+DeepCABAC.\n\n\n## Writing\n\nThe authors begin the abstract with a statement on pre-trained large models and discuss recent work on LLM quantization, yet these topics appear unrelated to the core of this study. Conducting evaluations only on CNNs without addressing LLMs or even ViTs is entirely valid; however, if they are not central to the paper, it raises the question of why these topics are mentioned at all.\n\n### Refrences\n\n[1]CAT: Compression-Aware Training for bandwidth reduction, Baskin et al., JMLR 2021\n\n[2] Feature Map Transform Coding for Energy-Efficient CNN Inference, Chmiel et al. IJCNN 2020"
            },
            "questions": {
                "value": "How does the choice of entropy model impact compression outcomes, and could other entropy models be seamlessly integrated into the method?\n\nHow does the method handle outliers in model weights, especially in architectures with significant variations in weight distributions? While this may seem out of scope, it could provide insight into the method's effectiveness in quantizing activations.\n\nIf the method is compatible with various quantization schemes, could you evaluate the impact of different schemes on large language models (LLMs)? A model with 1 billion parameters should be sufficient for this validation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents OPTQ-RD, an extension of the OPTQ quantization method that introduces a rate-distortion trade-off. By adapting the quantization step to account for bit rate using an entropy model, the proposed framework compresses neural networks and maintains model accuracy. The method is experimentally evaluated on several computer vision models such as ResNets and VGG16 on CIFAR10 and ImageNet datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The paper proposes a combination of quantization and compression methods, which is bridging the gap between optimizing for storage and inference. The integration of an entropy model (DeepCABAC) into the quantization process distinguishes the proposal from the other SOTA methods.\n2) The proposal is evalauted on computer vision datasets to demonstrate its effectiveness.\n3) The paper is well written and structured."
            },
            "weaknesses": {
                "value": "1) The generalizabity of the proposal on other model architectues and datasets is not evaluated explored. Addressing this issue running some extra experiments would strengthen the paper.\n2) Table 1 presents the comparison with other compression techniques, however there is no comparison with the works reported in the related work section (e.g., those using quantization, pruning, and knowledge distillation). This could provide a more comprehensive performance context and help the reader understand better the benefits of the proposal.\n3) It would be beneficial to mention what is the novelty of the proposal. The way it is written currently, it is seems that the proposal is incremental in terms of novelty as it is a combination of various existing techniques."
            },
            "questions": {
                "value": "1) How does the proposed method work for other architectures apart from VGG16 on ImageNet and the ResNets on CIFAR10? \n2) What are the trade-offs in selecting the size of the calibration sets, and how does this affects models with significantly more parameters than the ones tested?\n3) How does this method compares agaainst other SOTA works? Currently,  the proposal is compared with very limited SOTA works (Table 1)\n4) Could you please mention the novelty of the proposal (please see the weaknesses above)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This submission proposed to combine neural network quantization (after training, Post-Training Quantization (PTQ)) and parameters compression (compression) for storage in an end-to-end procedure. The quantized parameters are guided to compression-friendly distribution. Specially, after network is trained, it used OPTQ for parameter quantization. The OPTQ used is reformulated to combine DeepCABAC (a compression method) by Lagrange, leading to a quantization results considering compression requirement. The Lagrange factor trading off quantization and compression is layer-wise by considering the Hessian in each layer.\n\nExperiments in Computer Vision tasks (ResNet series and ImageNet, CIFAR10) demonstrate its efficiency in performance and compression."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The idea of the proposed method is straight forward and easy to understanding.\n- It is interesting of combining quantization (for efficient inference) and compression (for storage)."
            },
            "weaknesses": {
                "value": "- The main methods used in this submission is published and well-known. The submission merely combine the methods with a Lagrange formulation."
            },
            "questions": {
                "value": "- In experiments, the submission use Bits-Per-Weight to represents the compression performance, how is it related to the actual storage?\n- As I understand, the compression take effects in disk (offline storage), instead of memory (online inference), is the proposed method benefit for inference speed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a method to combine compression with neural network quantization. The authors add a rate-distortion penalty to the rounding function and toggle its weighting to control the trade-off between compression rates and model quality. They conclude that their added compression penalty improves compression rates without severely impacting model quality."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors investigate combining ideas from the fields of neural network quantization and compression. They introduce a penalty to the rounding procedure that allows a practitioner to control the rate/distortion trade-off. The penalty works as they hypothesize."
            },
            "weaknesses": {
                "value": "While the experiments support their hypothesis, the baselines and models are insufficient. Since the authors are investigating post-training quantization techniques, it is expected to also test on language models. If model size is an issue for the authors, there are reasonably sized models to experiment with that are similar sizes to ResNet50 (e.g., Pythia-70M). Furthermore, if focusing only on convolutional architectures, it is important to test MobileNets, which are notoriously challenging and useful benchmarks. Finally, the design space for the Pareto frontier is unclear as the quantization levels for OPTQ are not disclosed."
            },
            "questions": {
                "value": "- What is the design space for the Pareto frontier?\n- Did you quantize each of the models to different bit widths?\n- If you are quantizing to 8-bit weights and activations before the penalty, it doesn't seem surprising that a compression penalty would cause significant savings. OPTQ (and other PTQ techniques for that matter) can push bit widths lower before significant degradation. Did you try quantizing weights and activations to 4 bits as the baseline?\n- Why not test on parameter-efficient architectures such as MobileNet, EfficientNet, etc.?\n- Why not test on small language models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}