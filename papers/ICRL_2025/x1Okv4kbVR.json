{
    "id": "x1Okv4kbVR",
    "title": "MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization",
    "abstract": "As large language models (LLMs) are rapidly advancing and achieving near-human capabilities, aligning them with human values is becoming more urgent. In scenarios where LLMs outperform humans, we face a weak-to-strong alignment problem where we need to effectively align strong student LLMs through weak supervision generated by weak teachers. Existing alignment methods mainly focus on strong-to-weak alignment and self-alignment settings, and it is impractical to adapt them to the much harder weak-to-strong alignment setting. To fill this gap, we propose a multi-agent contrastive preference optimization (MACPO) framework. MACPO facilitates weak teachers and strong students to learn from each other by iteratively reinforcing unfamiliar positive behaviors while penalizing familiar negative ones. To get this, we devise a mutual positive behavior augmentation strategy to encourage weak teachers and strong students to learn from each other\u2019s positive behavior and further provide higher quality positive behavior for the next iteration. Additionally, we propose a hard negative behavior construction strategy to induce weak teachers and strong students to generate familiar negative behavior by fine-tuning on negative behavioral data. Experimental results on the HH-RLHF and PKU-SafeRLHF datasets, evaluated using both automatic metrics and human judgments, demonstrate that MACPO simultaneously improves alignment performance of strong students and weak teachers. Moreover, as the number of weak teachers increases, MACPO achieves better weak-to-strong alignment performance through more iteration optimization rounds.",
    "keywords": [
        "weak-to-strong alignment",
        "preference optimization"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We propose a multi-agent contrastive preference optimization (MACPO) framework to facilitate weak teachers and strong students learn from each other to improve weak-to-strong alignment performance.",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=x1Okv4kbVR",
    "pdf_link": "https://openreview.net/pdf?id=x1Okv4kbVR",
    "comments": [
        {
            "summary": {
                "value": "The paper explores the alignment problem when the LLM outperforms humans and human supervision is therefore weak. The authors propose a multiagent contrastive preference optimization (MACPO) framework to for weak-to-strong alignment by iteratively reinforcing unfamiliar positive behaviors while penalizing familiar negative ones. The authors evaluate their method on HH-RLHF and PKU-SafeRLHF datasets and show that MACPO improves alignment performance of strong students and weak teachers."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The research problem is important and timely, as large language models are rapidly advancing and achieving near-human capabilities, and allowing them to surpass human performance with potentially only weak human supervision is a critical issue.\n2. The paper is well-written and clearly structured, with a good introduction and related work section, clearly described methodology, and well-organized experiments and results.\n3. The experimental results are promising. The scale of the experiments, diversity of evaluation, and the rich comparison are appreciated."
            },
            "weaknesses": {
                "value": "1. While the description of the MACPO framework in section 4.1, 4.2 is clear, the introduction of the framework in line 053 and line 196 is confusing. This is mainly due to the use of term 'behavior' and 'familiar' before they are defined. Moreover, the paragraph in line 196 is almost the same as the paragraph in line 061, which does not provide improved clarity.\n2. In Section 3.1, the formulation of the problem, the authors adopt an analogy setting where weak teachers are fine-tuned small models and strong students are big models 'initialized on weak labels generated by weak teachers for the held-out question set(line 261)'. In this case, it is not clear whether the strong student is truly strong, and it is not clear how this situation relates to the motivation problem in line 013. \n3. Computation efficiency discussion and comparison would be helpful as the proposed method requires multiple iterations of optimizing multiple models. Would the Strong-to-weak alignment and Self-alignment benefit from the longer training time (with the same effective computation as MACPO)?"
            },
            "questions": {
                "value": "Please see the weaknesses 2,3"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a multi-agent contrastive preference optimization (MACPO) approach for weak-to-strong alignment in LLMs. The authors utilize a multi-agent framework that iteratively improves both weak teachers and strong students by reinforcing unfamiliar positive behaviors and penalizing familiar negative ones. The experimental results show that MACPO achieves enhanced alignment performance over traditional methods, particularly as the number of weak teachers increases.\n\nThe main contributions of the paper include: 1) introducing the MACPO framework for weak-to-strong alignment; 2) incorporating mutual positive behavior augmentation and hard negative behavior construction to support iterative improvements; and 3) validating the proposed method's effectiveness through experiments on helpfulness and harmlessness alignment datasets\u200b."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The idea of using a multi-agent contrastive preference optimization approach to achieve weak-to-strong alignment is innovative. Most existing work focuses on direct fine-tuning or reinforcement learning methods to improve alignment, but these approaches cannot leverage the incremental learning power of weak teachers to iteratively refine alignment, limiting the model\u2019s ability to generalize across various behaviors. This paper introduces a method that enhances alignment by reinforcing positive unfamiliar behaviors and penalizing negative familiar ones, as well as significantly improves alignment performance as more weak teachers contribute to training.\n\n2. The workflow is well-structured, as it combines contrastive preference optimization with a multi-agent setup to make the alignment process adaptable and iterative. This approach encourages behavior diversity among agents and further enhances the robustness and effectiveness of the alignment process.\n\n3. The experiments are extensive, with detailed analysis of the results. These experiments validate the effectiveness of the MACPO framework and demonstrate the scalability and adaptability of the method across models with different sizes and complexities\u200b."
            },
            "weaknesses": {
                "value": "1. The experiments use only two alignment datasets, expanding the evaluation to include additional alignment tasks like toxicity detection or complex reasoning would provide a more comprehensive assessment of MACPO's generalizability.\n\n2. In Section 4.2 HARD NEGATIVE BEHAVIOR CONSTRUCTION, the paper does not clearly explain how agent interactions are managed or how behaviors are tracked across weak and strong agents throughout the alignment process."
            },
            "questions": {
                "value": "In Section 3.1, the paper assumes that using weak teachers can incrementally improve alignment by reinforcing positive behaviors. Could the authors clarify what criteria define a \"weak teacher\" and how its effectiveness in improving alignment is measured compared to using strong agents from the beginning?\n\nThe paper describes using multiple agents for reinforcement but lacks details on whether adjustments are made dynamically based on each agent's performance. Are there mechanisms in place to adjust reinforcement strategies depending on agent success rates, and if so, how are these adjustments implemented to ensure optimal alignment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces MACPO, a framework for weak-to-strong alignment that enables strong language models to learn from weaker teachers. The authors claim that MACPO enhances positive behavior exchange and alignment refinement through iterative strategies, improving both teacher and student alignment, and results on benchmark datasets show MACPO\u2019s effectiveness, with stronger alignment as the number of weak teachers grows."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors clearly define the problem, making it easy to understand the motivations behind their approach. This clarity in the problem statement helps to set a strong foundation for the rest of the work. And this paper opens up several avenues for future research, providing a solid foundation for follow-up studies. The authors discuss limitations and possible extensions, showing an awareness of the field's current needs and future directions."
            },
            "weaknesses": {
                "value": "The most concerning aspect is the statement in line 224: *\u201cSince a high perplexity ppl of the positive strong student indicates weak labels may contain negative noises,\u201d* which appears to form the foundational assumption of the entire framework. I have not encountered a statement like this before. In my opinion, alignment and preference learning should prioritize evaluation metrics as the basis for setting up metrics. Providing substantial evidence, such as a reference paper or experiments to demonstrate this metric\u2019s validity and necessity, would be essential for this work. However, I could not find corresponding evidence, which may represent a fundamental weakness. At least the author needs to show a positive correlation between perplexity and answer quality (although this positive correlation is also very weak evidence to me, considering that correlation does not mean causation). However, the author did not even show the most basic evidence, which is very limited.\n\nI outline other concerns:\n\n1. I find the pipeline somewhat confusing. According to Algorithm 1, the proposed method first derives the \u201cStrong Student\u201d before proceeding with \u201cWeak Teacher\u201d training. Typically, in teacher-student learning paradigms, the teacher model is trained first and then provides supervision for the student model. If my understanding of the pipeline is correct, could the authors clarify the rationale behind this approach?\n\n2. It appears that the model sizes listed in Table 1 are inconsistent. For example, MACPO results are reported using both the 8B and 70B models, while other approaches only rely on the 8B model. This discrepancy may undermine the fairness of the comparisons.\n\n3. In Table 1, how are the final evaluation results derived based on a third-party reward model? With multiple gold models available in RewardBench, it would be helpful for the authors to explain their choice of this particular model over others.\n\n4. For HH-RLHF training, it\u2019s unclear why the authors opted to use only 10K samples, which represent less than 10% of the original dataset. Do the final results depend heavily on this specific subset? This choice should be validated, as it raises concerns about the generalizability of the experimental results.\n\n5. Are the experiments restricted solely to alignment for helpfulness and harmlessness? It would be beneficial to extend these experiments to other tasks, such as Reddit TL; DR. Even if the approach does not perform well on such tasks, presenting these results could offer valuable insights."
            },
            "questions": {
                "value": "Please refer to the weakness, I'm happy to modify my rate based on the response of the authors and refer to other reviewers' comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes Multi-Agent Contrastive Preference Optimization (MACPO), which aims at letting strong students and weak teachers to learn from each other by encouraging unfamiliar positive behaviors and penalizing familiar negative behaviors. The proposed algorithm achieves better performance compared with other weak-to-strong alignment methods on helpfulness and harmlessness benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed approach is intuitive and partially solves the problem of collapsing by learning through self-generated data. \n\n2. The ablation study is comprehensive, validating the claims. The authors clearly illustrate the benefits brought up by unfamiliar positive behavior and familiar negative behavior."
            },
            "weaknesses": {
                "value": "1. The writing can be significantly improved, especially for section 4. The notation often comes with 4 to 5 super and subscripts, which is very difficult to follow. I highly recommend the authors clean this part up.\n\n2. The proposal is intuitive, but the concept of \u201cfamiliar\u201d and \u201cunfamiliar\u201d is not well defined or discussed. Does familiar mean self-generated content and unfamiliar mean content generated by other models? What are the measures you can use for determining familiarity?"
            },
            "questions": {
                "value": "If I understand correctly, when constructing positive behaviors, one selection criteria is that the weak labels should have low perplexity when evaluated on the strong student. However, doesn\u2019t this low perplexity mean that this weak label is familiar to the strong student? Because low perplexity indicates that this weak label is likely to be generated by the strong student as well. This seems to contradict the goal of generating unfamiliar positive behavior."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}