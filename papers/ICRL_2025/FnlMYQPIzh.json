{
    "id": "FnlMYQPIzh",
    "title": "ConvINT: A Semi-Structured Intention Framework for Conversational Understanding",
    "abstract": "Understanding user intentions is critical for conversational AI, especially with the rise of large language models (LLMs) that demand a more nuanced comprehension of dialogue. Existing approaches, relying on rigid slot-value structures or unstructured representations, often miss the complexity of human intentions. In this work, we propose ConvINT, a novel semi-structured intention framework that offers a more holistic and fine-grained understanding of user intentions by organizing them into four key aspects: situation, emotion, action, and knowledge. Grounded in psychological and cognitive intention theories, ConvINT provides LLMs with a richer context for understanding user inputs while offering a semi-structured format that seamlessly integrates with prompt-based intention learning. To enable the efficient adoption of this framework, we introduce a Weakly-supervised Reinforced Generation (WeRG) method that scales ConvINT annotations across large datasets with high quality. By combining a small set of human-annotated instances with coarsely labeled data as weak supervision signals, WeRG effectively learns to generate ConvINT annotations, ensuring both scalability and precision. Experimental results demonstrate that integrating ConvINT with WeRG markedly improves LLMs\u2019 ability to comprehend user intentions, yielding significant gains in downstream tasks such as response generation and task completion, as validated by both automatic metrics and human evaluations. These findings highlight ConvINT's potential as a comprehensive and adaptable framework for advancing intention understanding in conversational AI.",
    "keywords": [
        "Conversational Understanding",
        "Weakly-supervised Generation",
        "Large Language Models",
        "Fine-tuning"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=FnlMYQPIzh",
    "pdf_link": "https://openreview.net/pdf?id=FnlMYQPIzh",
    "comments": [
        {
            "withdrawal_confirmation": {
                "value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."
            }
        },
        {
            "summary": {
                "value": "This paper introduces ConvINT (Conversational INTention), a semi-structured framework for enhancing intention recognition in conversational AI. ConvINT employs a fine-grained, aspect-aware analysis of user input across four key dimensions: situation, emotion, action, and knowledge. To efficiently expand ConvINT annotations in large datasets, the authors develop the WeRG (Weakly Supervised Reinforcement Generation) approach, which synergizes multiple annotation sources\u2014including hard mapping, LLMs, and human annotations\u2014for fine-tuning the model. Experimental results demonstrate that the ConvINT framework improves conversational understanding and confirms the effectiveness of the WeRG method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The ConvINT framework effectively incorporates cognitive and psychological intention theories, making it suitable for handling emotional and nuanced interactions in real-world conversational AI. \n\n- The WeRG method for ConvINT annotations, which combines weak supervision from multiple sources with hierarchical rewards, ensures high-quality annotations while reducing dependency on costly human resources."
            },
            "weaknesses": {
                "value": "- Comparison Ambiguity: It seems that Direct Prompt and CoT Prompt were applied with llama2-7b (please clarify if this is incorrect). The comparative value of these methods against the WeRG training method remains unclear. Additionally, as GPT-3.5-turbo was used for annotation, the lack of detail on the quality of this data raises questions about the reliability of D_{mid}.\n\n- Limited Analysis: The ablation study on data sources and the experiments for downstream applications are conducted exclusively on DuRecDial, while the ablation study on aspects is performed only on ESConv. However, the noticeable drop when \u2018[Emotion]\u2019 is removed from ESConv seems expected and does not provide meaningful insight. Furthermore, there is no comparison with traditional interpretation methods in the downstream experiments, nor is there any analysis of key failure cases, such as poorly generated ConvINTs or unsatisfactory responses based on accurate ConvINTs, which could yield valuable insights.\n\n- Lack of Experimental Detail:  Specific details of the ConvINT labels for the test set and the evaluation by the three student annotators are missing. Additionally, it is unclear how the Success Rate (SR) metric is calculated."
            },
            "questions": {
                "value": "1. When constructing the ConvINT data, what is the quality of the content generated by GPT-3.5 as an annotator for each aspect? Additionally, how was the human annotation process conducted?\n\n2. Why was the ablation study on data sources and downstream applications limited to DuRecDial, and why was the aspect-based ablation study conducted solely on ESConv? Could expanding these studies across both datasets provide more comprehensive insights?\n\n3. In the downstream experiments, is there any comparison with traditional interpretation methods or analysis of key failure cases, such as poorly generated ConvINTs or responses that fall short despite accurate ConvINTs? If not, would these additions offer further insights?\n\n4. What are the specific details of ConvINT labels for the test set and the evaluation conducted with the three student annotators? Moreover, how is the Success Rate (SR) calculated?\n\n5. Line 293: KL regularised RL -> KL-regularized RL\n\n6. Line 350: one bracket"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes ConvINT, a semi-structure intention framework that offers fine-grained understanding of user intentions. Concretely, it proposes to categorize user intention into four categories: situation, emotion, action and knowledge. Building upon this rich representation, this work introduces a weakly supervised reinforced generation method that scales the ConvINT annotation. Experimental results show that the proposed method improved LLMs\u2019 ability to understand user intentions on two datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* This work is grounded in psychological and cognitive intention theories and produce a fine-grained ConvINT to represent user intentions. \n* The proposed WeRG method takes into account coarse-to-fine labels and introduced three different levels to capture fine-level, mid-level and coarse-level labels. Such design holds the potential to balance different types of data quality and annotation cost. \n* Detailed ablation studies are provided to demonstrate the effectiveness of each component, such as the use of prompting and the necessity of different levels of labels in ConvINT generation, as well as the amount of fine-annotated data."
            },
            "weaknesses": {
                "value": "* WeRG relies on three levels of labels and their relative quality difference to learn this hierarchical information with regard to rewards. However, it is unclear how such a mixture of three different levels of labels should be, and how model training is sensitive to such data mixture. \n* One of the justifications for using coarse and mid level labels is for cost-effective considerations. However, current experimental design didn\u2019t provide much comparison of cost and effectiveness with regard to different design choices here. \n* The current baselines only include vanilla prompting and chain of thought prompting. I wonder why no other training-based methods are not used as baselines. It seems WeRG can also be viewed as synthetic data generated from weaker models; would any prior synthetic data generation methods or distant supervision be good alternatives here?"
            },
            "questions": {
                "value": "* Many components of the current paper require further justification or motivation. For instance, it is a bit hard to follow the training details around Figure 2. Similarly,  ConvINT framework is described as \u201cAccording to the intention theories in psychological and cognitive sciences\u201d, but it is unclear how such grounding or design works that lead to the current triplets of situation, emotion, action and knowledge. Why is this detailed breakdown essential and can provide add-on value for conversational understanding? \n\n* How coarse-, mid-, and fine- level labels are derived and evaluated is missing key details. What\u2019s the quality of these mid-level labels? Are they noisy and to what extent? For fine-level labels, who are the annotators, how are annotators compensated, and what are the annotation agreements among these annotators?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies conversational understanding (CU).\n\nUnlike previous works that always use specified intent classes or slot-value pairs, this work uses LLMs to produce more flexible natural language CU understanding texts.\n\nNext, this work is inspired by psychological and cognitive intention theories (Schroder et al., 2014) and proposes four CU aspects. Then, this work proposes a weakly supervised RL fine-tuning method to use three sources of training data: (coarse mapping, LLM annotation, and human annotation ).\n\nFinally, experiments are conducted on two datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This work is well-organized and easy to follow.\n2. This work draws from interdisciplinary intention theories to formulate its task.  Subsequently, four CU aspects are identified.\n3. ConvINT/WeRG can simultaneously use three sources of training data.\n4. Many analyses about the sources of training data"
            },
            "weaknesses": {
                "value": "1. The novelty is limited. The contribution can be summarized as 1) a new 4-aspect CU template and 2) combining three sources of training data.\n2. In the experiments, it seems like the COT baseline prompt lacks the process of COT. It is more like an In-Context Learning prompt.\n3. The experimental setting is unfair. Two baselines are non-trained, prompt-based methods, but the proposed method has included a training process with supervised data. This means this work lacks settings of other (weakly-)supervised baselines.\n4. In the Table 3 Ablation Study, we can find the LLM-annotated $D_{mid}$ can heavily impact the result. This can also indirectly prove that LLMs perform well in CU tasks with a well-defined prompt. However, the baseline setting does not include such a well-defined prompt design. This means this work lacks settings of other necessary baseline prompts.\n5. The experiment lacks enough discussion about their training methodology. Most analyses are related to the sources o"
            },
            "questions": {
                "value": "minor issues:\n\n1.  Table 1: BARTScore is also higher is better."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a semi-structured framework, ConvINT, for intent recognition, introducing four key aspects inspired by psychological and cognitive intention theories. Additionally, the authors introduce WeRG, which scales ConvINT annotation while maintaining high quality."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper proposes ConvINT, a more fine-grained framework for intent recognition\n- The paper presents WeRG, which allows data synthesis from different source for model fine-tuning\n- The paper is easy to follow"
            },
            "weaknesses": {
                "value": "- Typo error l.350: \"((Appendix B)\", redundant \"(\"\n- l.367-l.369: from Table 1, I don't see that you differentiate the zero-shot CoT and few-shot CoT. (p.s. For evaluation, the deployed automated metrics are similarity-based or matching-based. Considering LLM-as-judge might be sensible here as well.)"
            },
            "questions": {
                "value": "- I'm confused about the first formulation, especially the subscript. I assume each data point contain multiple dialogues, consisting of user utterances and responses. Then $x_{i, t}$ should be the t-th utterance from the i-th instance, while $y_{i, t}$ is the corresponding response?\n- Is there a specific reason for choosing the DuRecDial dataset? Given that it is a dataset for conversational recommendations, I would imagine it might lack sufficient emotional or knowledge-based content, which are key aspects of your approach. Could you clarify why this dataset was selected? In addition, ESConv focuses on emotional support conversations, which should naturally contain more emotional information. Could you clarify why the informativeness score is lower compared to DuRecDial (Table 2)?\n- l.321: How large is your test set? How many human annotators participated in the annotation process, and what was the level of inter-annotator agreement? Additionally, could you provide details about the annotation instructions?\n- You specify the number of annotations in l.334-335. Could you clarify the background of the annotators? For the user study (Table 2), how many annotators evaluated each instance?\n- In l. 445\u2013446, you mention that the BARTScore of $w/o$ $D_{mid}$ and $w/o$ $r_c$ decreases (Table 3), which suggests improved performance. Could you provide clarification on this observation?\n- In l. 448, I'm uncertain if it's fair to draw that conclusion, as the training data is dominated by $D_{mid}$, which should be the most influential factor affecting performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}