{
    "id": "UvYrFbKj8j",
    "title": "Stutter makes large language models smarter",
    "abstract": "Large language models (LLMs) have achieved remarkable success in generating coherent and contextually relevant text. However, their large parameters and high memory requirements limit their efficiency and adoption in industry and academia. Recent studies have shown that dynamically adjusting inference operations can improve model performance without significantly increasing size. In this paper, we introduce the stutter mechanism, a novel method that enhances transformer models by selectively applying additional layers to more challenging tokens. This approach mimics a human speaker\u2019s stutter, allocating more computational effort where needed, thus improving\nlanguage capabilities without generating excessive tokens. Our experiments with various Pythia models demonstrate that the stutter mechanism consistently enhances performance across benchmark datasets. Specifically, the Pythia-410M model, enhanced by our method, outperforms the larger Pythia-1B model on WinoGrande and WSC. Additionally, our method is data-efficient, requiring only less than 1% of the pretraining data for the additional training. These results highlight the stutter mechanism\u2019s potential to enhance LLMs\u2019 efficiency and performance in real-world applications.",
    "keywords": [
        "Self-improvement for LLM"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=UvYrFbKj8j",
    "pdf_link": "https://openreview.net/pdf?id=UvYrFbKj8j",
    "comments": [
        {
            "summary": {
                "value": "The paper proposed a new mechanism called \"stutter\" to allow the model to use more compute during the inference time. The mechanism works in the following way: for every token, the model encodes it twice, and the second time, each layer has a module that attends to the last step's last layer representation. This allows the model to \"think twice\". The design is very simple and minimalistic and requires as few as 1B tokens to train. \n\nThe proposed method has huge extensibility: future work can explore when to \"stutter\" (for example, at more difficult prediction steps) and can stutter for multiple steps. This is similar to the recently popular \"inference scaling\" scheme. With almost no increase in the model parameters, the method allows the model to use more compute in the inference time, hence better results.\n\nThe authors conducted experiments on several small-scale Pythia models, which show some improvement on standard benchmarks in both few-shot/zero-shot settings. There are also ablations on the effect of which layer that stutter modules attend to and how many times to stutter. While this is still a very initial exploration (no experiments on potential stutter location selection; no experiments on truly generative tasks), the idea is very interesting and holds a lot of potential."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(1) The proposed method is clear, intuitive, and simple. The method holds a lot of potential in the inference-scaling scheme. \n\n(2) The empirical gain on small models are significant on certain tasks."
            },
            "weaknesses": {
                "value": "Though I really like the idea and think this can have a huge impact in future research, I have several concerns in terms of effectiveness, speed, and baseline.\n\n(1) The selected tasks mostly don't require complex reasoning (unlike tasks like GSM8K). It is unclear how this method will perform on tasks that truly require reasoning. I understand that tasks like coding/GSM8K will have only trivial results at this scale (1B), but maybe the authors can explore some more synthetic tasks that require multi-hop reasoning. \n\n(2) The gain is not very consistent or significant across different tasks. In this case, the authors should also report variance to show the significance of the results.\n\n(3) **My biggest concern** is that the method is extremely inefficient in inference. The current setting is that the model stutters at every token; since each stutter step needs to look at the last step's last layer, this essentially turns the parallel prefix filling (encoding the context) into an autoregressive procedure, which will be extremely slow, especially when the prefix is long. To the best of my knowledge, the authors did not discuss this. One remedy I can think of is to only stutter at the last token before the model outputs the answer. \n\n(4) The authors did not include discussion/comparison to a very relevant method: pause tokens (Goyal et al.). In fact, pause tokens are more efficient because they can still encode the prefix in parallel like standard transformers instead of the autoregressive style.\n\nGoyal et al. Think before you speak: Training Language Models With Pause Tokens"
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a parameter-sharing method to achieve deeper Transformer-based language models (LMs) without significantly increasing the number of total model parameters.\n\nTo achieve that, the authors aim to continually upcycle existing pretrained LMs (base LMs)  by introducing a light-weight adapter module, termed as token-retrospect map (linear attention).\nFor efficiency purposes, the base LM is kept frozen during the adaptation, which provides the initial hidden states of the input tokens.\nBased on those initial hidden states, a second adapted LM is applied atop with the adapter network to re-aggregate context information with deeper layer representations.\nThis technique can also be reviewed as another means of recurrency mechanism.\nThe particular implementation considered in this paper achieves an adapted LM with twice depth as that of the base LM with roughly additional 10% more parameters.\n\nThe paper applies the proposed technique to the Pythia LM family with model sizes ranging from 160M to 1B. A collection of language understanding datasets are used for evaluation.\nCompared with the corresponding base LMs, the adapted LMs based on the proposed method are observed to have at-odds performance improvements."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Enhancing/upcycling existing pretrained LMs with parameter-efficient methods to achieve new capabilities is an important research topic.\nThe proposed method is a reasonable technique."
            },
            "weaknesses": {
                "value": "The exposition of the paper requires substantial improvements:\n\n*The cross layer parameter sharing is a widely studied technique in previous work (e.g., albert [1] inter alia). It is necessary to cite and discuss properly.\n\n*Please update the citation based on ICLR recommendations, e.g., using \\citep.\n\n*Please provide proper citations for models/methods/datasets used in the paper, e..g, line 066, line 287. Without proper citations, it is hard to evaluate whether the experiment set up is properly and the comparison is meaningful.\n\n*As the entire input token sequence is used for the second pass, it is good to reflect that in Fig 1.\n\nThe experiment setup is problematic without enough convincing evidences:\n\n*Across all considered LMs with varying sizes and datasets, the improvements of the proposed method over base LM are at odds. Even for cases where there are certain improvements (e.g., WSC), the paper fails to include any insights, e.g., what are those improved cases and are those results statistically significant?\n\n*As the proposed techniques trade-off the computation complexity for parameter efficiency, it is good to picture the performance vs computation costs between base LMs and the proposed method. Without, it is hard to justify whether the extra costs are truly worthy.\n\n*It is good to test the robustness of those chosen hyperparameters. For example, It is unclear how those few-shot examples are chosen and how sensitive those decisions are. How the 1B token training dataset is selected from Pile and what domains are included?\n\n*Although it is good to consider LMs of varying sizes, it is better to include LMs from other families. This could provide more insights on the generalizability of the proposed method, e.g., Transformer architecture variants, pretraining corpus and tokenizers.\n\n\n[1] Lan et al., ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS"
            },
            "questions": {
                "value": "For the second pass, is the token retrospect allowed to attend over previously generated token hiddens in the second pass? Why or why not?\n\nCan you provide more info on the continual pretraining? What hardware is used? For the 1B token training dataset, how many epochs are used? Do you see any benefit with more tokens or longer training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a stutter mechanism on transformer models by applying additional token-retrospect layers ( newly trained additional attention layer) and repeating the token twice. Experiments are conducted on Pythia models with an additional training of 1 billion token, and are evaluated on several LLM benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The motivation of stutter mechanism is intriguing, by correcting the possible wrong tokens with a new token-retrospect map layer to do correction. The presentation and writing are clear and easy to follow. The authors conducted comprehensive benchmarks across multiple datasets, including LAMBDA, PIQA, WinoGrande, WSC, ARC, SciQ, and LogiQA, using Pythia models of three different parameter sizes\u2014160M, 410M, and 1B.  Additionally, it analyzes the effectiveness of chosen layers in sec4.4.2, showing that attending to specific layers could improve the performance."
            },
            "weaknesses": {
                "value": "1. **The reported performance improvement is not convincing**. Specifically, for example, we can see from Table 2 that base model vs stutter methods are 0.230 vs 0.215 on LogiQA, 0.892 vs 0.894 on SciQ. Similar results can be found at Table 3. \n\n2. **The experimental design is not quite fair**. As stutter models are pretrained 1 billion tokens more than base models, it is unknown whether such fluctuation of performance is due to the continual training, or the inclusion of extra 10% token-retrospect layers. Is is required to continual-train the base model for the similar token compute. \n\n3. **The details of implementation are missing**. As in Line#252, each token is stutter once, doubling the sequence length of the language model. It lacks discussion about the sequence length of this point.\n\n4. **Extra time cost**. The stutter methods require the forward process twice in both training and evaluation process. It is required to report the time and complexity cost versus base models."
            },
            "questions": {
                "value": "1. How does the stutter handle with the sequence length problem?\n\n2. As the paper mentioned, each token stutters once, and the strategy of stutter token selection is out of the scope of the paper, why stuttering/repeating each token once can work? \n\n3. How does the stutter methods impact the time complexity in both training and inference?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel method called the \"stutter mechanism\" that enhances transformer models by selectively applying additional layers to more challenging tokens. This approach mimics a human speaker's stutter, allocating more computational effort where needed, thus improving language capabilities without generating excessive tokens. Experiments with various Pythia models demonstrate that the stutter mechanism consistently enhances performance across benchmark datasets. Specifically, the Pythia-410M model, enhanced by this method, outperforms the larger Pythia-1B model on WinoGrande and WSC. Additionally, the method is data-efficient, requiring less than 1% of the pretraining data for additional training."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The motivation is strong, the hypothesis that not all tokens are equally easy to generate and for at least some of them, a transformer can do better by \u201dgiving more thought\u201d to an in-flight token by \u201dtransforming\u201d it with more operations. The stutter mechanism you propose is a creative and novel method to enhance the capabilities of large language models (LLMs) without significantly increasing their size. This addresses a critical need in the field for more efficient and adaptable models.\n\n2. The paper presents a well-structured approach to implementing the stutter mechanism, with clear explanations of the architecture and the token-retrospect map. The mathematical formulations are sound and the integration with existing transformer architectures is well-justified.\n\n3.  The experimental results are compelling, demonstrating significant performance improvements across various benchmarks. The fact that a smaller model (Pythia-410M) can outperform a larger one (Pythia-1B) with the stutter mechanism is particularly noteworthy."
            },
            "weaknesses": {
                "value": "1. Comparison with State-of-the-Art LLMs: The paper could be strengthened by comparing the stutter mechanism against other recent methods aimed at improving LLM efficiency or performance. Meanwhile, the used LLMs (Pythia) are limited, LLAMA is the important series of LLMs to conduct the experiments . This would provide a clearer picture of how your approach stands out in the current research landscape.\n\n2. Scalability Analysis: Although the paper mentions the potential for the stutter mechanism to be applied to larger models, an analysis of how the mechanism scales with model size would be valuable. How does the performance and efficiency change as model size increases?\n\n3. Long-Term Training Stability: The paper focuses on the training with 1 billion tokens, but it would be beneficial to understand the long-term training dynamics and stability of the stutter mechanism, especially when applied to larger datasets or over more training epochs."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}