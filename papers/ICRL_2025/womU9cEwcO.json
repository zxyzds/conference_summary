{
    "id": "womU9cEwcO",
    "title": "Autonomous agents from automatic reward modeling and planning",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a range of text-generation tasks. However, LLMs still struggle with problems requiring multi-step decision-making and environmental feedback, such as online shopping, scientific reasoning, and mathematical problem-solving. Unlike pure text data, collecting large-scale decision-making data is challenging. Moreover, many powerful LLMs are only accessible through APIs, which hinders their fine-tuning for agent tasks due to cost and complexity. To address LLM agents' limitations, we propose a framework that can automatically learn a reward model from the environment without human annotations. This model can be used to evaluate the action trajectories of LLM agents and provide heuristics for task planning. Specifically, our approach involves employing one LLM-based agent to navigate an environment randomly, generating diverse action trajectories. Subsequently, a separate LLM is leveraged to assign a task intent and synthesize a negative response alongside the correct response for each trajectory. These triplets (task intent, positive response, and negative response) are then utilized as training data to optimize a reward model capable of scoring action trajectories. This reward model can be integrated with LLM-based agents and various planning algorithms to enhance task-solving performance. The effectiveness and generalizability of our framework are demonstrated through evaluations conducted on different agent benchmarks. In conclusion, our proposed framework represents a significant advancement in enhancing LLM agents' decision-making capabilities. By automating the learning of reward models, we overcome the challenges of data scarcity and API limitations, potentially revolutionizing the application of LLMs in complex and interactive environments. This research paves the way for more sophisticated AI agents capable of tackling a wide range of real-world problems requiring multi-step decision-making.",
    "keywords": [
        "agents",
        "large language models",
        "planning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=womU9cEwcO",
    "pdf_link": "https://openreview.net/pdf?id=womU9cEwcO",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes ARMAP, a novel framework that enhances the task-solving abilities of large language model (LLM)-based agents in interactive, multi-step environments. The authors tackle key challenges associated with data scarcity and API restrictions, presenting a method that automates reward model learning from LLM agents\u2019 interactions within an environment, thus eliminating the need for human annotations or commercial LLM-based evaluation. The reward model can then guide planning algorithms (e.g., Monte Carlo Tree Search and Reflexion) to improve LLM agents\u2019 performance in tasks requiring iterative decision-making, such as e-commerce navigation and simple scientific experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Innovative Reward Modeling Approach: The ARMAP framework leverages LLMs to generate diverse action trajectories, then synthesizes task goals and feedback to train a reward model. This automation of reward modeling is a strong innovation, addressing critical limitations in agent-based tasks by reducing reliance on costly and often proprietary data.\n\nFramework Flexibility: The framework\u2019s compatibility with multiple planning algorithms (MCTS, Reflexion, Best-of-N) demonstrates flexibility and potential for broader application. The performance boost across different LLMs (Llama, Phi, and Mistral) also underscores the generalizability of the ARMAP model.\n\nEffectiveness in Customization: ARMAP\u2019s ability to modify reward targets for controllable behavior generation (e.g., minimizing action length or cost) is a valuable capability for task-specific tuning, as demonstrated in the Webshop experiments."
            },
            "weaknesses": {
                "value": "Limited Scope of Tested Environments: Although the ARMAP framework was evaluated in multiple environments, these remain relatively constrained in task diversity (e.g., online shopping, elementary science tasks). Further exploration into environments with more complex multi-modal interactions or requiring intricate goal alignment would provide stronger evidence of the framework\u2019s versatility.\n\nPotential Overhead in Data Synthesis: While the automated reward modeling is valuable, the reliance on in-context LLMs for both task generation and trajectory synthesis could introduce computational overhead. It would be useful to discuss the cost-benefit analysis of this approach, particularly in environments requiring higher levels of interaction fidelity.\n\nDependence on LLM Quality: ARMAP\u2019s effectiveness is inherently tied to the quality of the LLMs generating the synthetic data. While the framework was evaluated on open-source models, a more explicit discussion of performance across varying LLM qualities or limitations when using smaller LLMs would provide more insight into its applicability in resource-constrained scenarios."
            },
            "questions": {
                "value": "Some suggestions for improvement:\n\nWhy do we need pairwise comparisons - this works in foundation model post-training, but why not use success/failure reward model training and using that as areward or value function?\n\nCan you extend the experimental scope to include more diverse or high-stakes decision-making environments, such as ALFRED, BEHAVIOUR or HABITAT to illustrate ARMAP\u2019s performance on tasks requiring more advanced capability.\n\nComputational Efficiency Analysis: Including an analysis of the framework's data demands and comparisons with reward learning approaches would be beneficial, especially if extending the applicability of ARMAP to realistic low-resource settings.\n\nDetailed Error Analysis: A more granular analysis of failure cases in each environment, particularly for tasks that involve complex dependencies or decision making, would provide deeper insights into the limitations of the current approach and inform possible improvements in reward modeling."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a framework named ARMAP, aimed at enhancing the task-solving capabilities of LLM-based agents in challenging environments that necessitate multi-step decision-making. While traditional LLMs perform well in text-based tasks, they face challenges with interactive, goal-oriented tasks due to limited access to large-scale decision-making data. ARMAP tackles these issues by developing an automated reward model that assesses action trajectories without requiring human annotations.\n\nThe framework comprises three main components:\n1. Data Generation: An LLM agent interacts with the environment, producing diverse action trajectories that include both successful and unsuccessful task completion attempts. These trajectories, encompassing task intents, positive outcomes, and negative outcomes, are utilized to train the reward model.\n2. Reward Model: A specialized model evaluates the effectiveness of each trajectory in fulfilling a task, thereby guiding the LLM agents in their planning.\n3. Planning Algorithms: By integrating the reward model with planning methods like Monte Carlo Tree Search (MCTS) and Reflexion, the agent can optimize its actions to follow high-reward paths.\n\nExperiments depict ARMAP\u2019s efficacy across various benchmarks, demonstrating improved planning performance for different LLM agents. The approach offers advantages in flexibility and practicality, as it reduces reliance on human labels and expensive, closed LLMs, thereby facilitating the development of more autonomous and efficient AI agents capable of managing real-world tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Automated Reward Modeling: It presents an innovative method for autonomously learning reward models without the need for human-annotated data, addressing issues related to data scarcity and dependence on costly closed-source LLMs. This makes the framework scalable and practical for real-world applications.\n\nEnhanced Decision-Making for LLM Agents: By offering a reward-based evaluation system, ARMAP significantly boosts the ability of LLM agents to perform complex, multi-step tasks that require sequential planning, an area where standard LLMs often struggle.\n\nEfficiency and Cost-Effectiveness: By eliminating the need to fine-tune LLMs and avoiding reliance on proprietary LLM APIs, ARMAP provides a cost-effective solution that could make high-performing AI agents more accessible for widespread use."
            },
            "weaknesses": {
                "value": "Limited Applicability in Highly Dynamic Environments: While the framework performs well in simulated environments with fixed rules, such as online shopping simulations and controlled benchmarks, its effectiveness in rapidly changing, unpredictable real-world environments is uncertain. The model may struggle with scenarios that require quick adaptation to new patterns not present in the training data.\n\nComputational Overhead with Complex Planning: The integration of planning algorithms like MCTS, while effective, can introduce significant computational costs, especially when exploring multiple trajectories. This may limit ARMAP\u2019s efficiency in resource-constrained settings or for tasks requiring real-time responses."
            },
            "questions": {
                "value": "Synthetic Data Quality: How do you ensure the quality and diversity of the synthetic trajectories generated by LLMs? Have you observed any limitations when these synthetic trajectories don\u2019t align closely with real-world decision-making patterns?\n\nComputational Cost in Real-Time Applications: Given the computational demands of planning algorithms like MCTS, how would ARMAP perform in applications requiring real-time decision-making? Are there strategies for reducing overhead while retaining performance?\n\nReward Model Generalization: How well does the reward model generalize to tasks and environments different from those it was trained on? Have you tested ARMAP in domains requiring more complex, domain-specific knowledge, such as legal or medical contexts?\n\nScalability and Practical Deployment: What are the main challenges you foresee in scaling ARMAP for broader deployment in real-world applications? Are there specific areas (e.g., hardware requirements, integration with other models) that need further development?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "ARMAP presents a novel framework for autonomous agents by leveraging reward modeling and planning. It trains a reward model on contrastive trajectories, enabling effective decision-making in complex environments through LLM-as-agents. Unlike input-optimized prompting-based approaches, ARMAP scores steps within task trajectories, focusing on task completion. The ablation study supports the framework\u2019s effectiveness and adaptability."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality: The automatic reward model and data generation approach presented is novel, allowing the framework to guide task completion within complex decision-making environments effectively.\n\nQuality: ARMAP stands out by using a reward model to evaluate and guide navigation steps in agentic environments, enhancing decision-making processes and setting a solid foundation for handling intricate tasks autonomously.\n\nClarity: The paper is well-written, with a clear flow that effectively communicates the core concepts and approach. While a few notational details could be clarified, the overall presentation is strong and accessible.\n\nSignificance: The framework's value is demonstrated through LLM-agent task performance, highlighting flexibility in controllable task generation and practical application via a reward model, which reduces reliance on large LLMs or human labeling."
            },
            "weaknesses": {
                "value": "Specificity in Reward Model Design: The paper lacks detailed information on the size and neural architecture of the reward model. Additionally, challenges in reward model development are not clearly defined. More depth and specific examples are needed to clarify these choices and support the framework's claims.\n\nLimited Dataset Scope: The study could benefit from evaluating on a broader set of complex, long-trajectory decision-making agent datasets. Including established datasets such as AlfWorld or BabyAGI, which could strengthen the empirical evaluation and demonstrate robustness across diverse environments.\n\nInsufficient Detail on Multimodal and Visual Input Integration: While the paper mentions multimodal feedback and visual inputs, it lacks clarity of their impact on reward model training. An ablation study that isolates the effect of visual inputs compared to text-based inputs could better illustrate their importance and further validate the framework\u2019s design."
            },
            "questions": {
                "value": "Although the automatic reward model training is a good idea, there are few concerns after going through the paper and demand clarity of choice:\n1. Writing and Formatting:\n    * In Figure 1, the title \"Tree Planning\" should use lowercase \"(c)\" instead of capital \"(C).\"\n2. Reward Model Specifics:\n    * Could authors clarify the size of the reward model used in this study?\n    * In Line 100, authors mention challenges in developing a reward model (RM). Could they provide a few specific examples of these challenges for clarity?\n    * What neural architecture was selected for the reward model in this framework? Is this inspired from any previous works?\n3. Dataset Selection:\n    * Some established decision-making agent datasets, such as AlfWorld, BabyAGI, or PDDL, are not included. These embodied agent datasets offer complex, long trajectories that could be valuable to the study. Could authors comment on their absence or suitability?\n4. Multimodal Feedback:\n    * Line 150 refers to multimodal feedback. Could you specify which modalities other than text were used in predicting the next action?\n5. Reward Model Type:\n    * In Line 161, you state a focus on developing the reward model. Is this a classification model with a defined set of output classes, or is it a regression model?\n6. Observation Clarification:\n    * In Line 225, the phrase \u201c...corresponding environment observations...\u201d could benefit from refinement, as there\u2019s typically one extra observation at the start. Could this section be adjusted to clarify the distinction?\n7. Trajectory Generation and Instruction Use:\n    * In Figure 2, authors mention using \u201cinitial language instructions in the environment\u201d to generate trajectories, but it\u2019s unclear if any LLM was employed to identify keywords. For instance, in \u201cI am looking for jeans with 40w x 34l size, and price lower than 200 dollars,\u201d did the framework use LLM predictions to determine \"Jeans\" as the keyword for search?\n8. Impact of Visual Inputs:\n    * What role do visual inputs play in the reward model\u2019s training? Have authors conducted any ablation studies that use only text from trajectories to measure their impact? It would be helpful to know if the visual inputs significantly influence the final model performance. I find this missing.\n\nThese points would enhance the clarity and depth of the paper, particularly around architectural choices and empirical coverage. I am looking forward to the rebuttal during the discussion phase."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}