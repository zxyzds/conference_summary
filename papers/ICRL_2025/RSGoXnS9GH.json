{
    "id": "RSGoXnS9GH",
    "title": "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs",
    "abstract": "The growing use of large language model (LLM)-based chatbots has raised concerns about fairness. Fairness issues in LLMs can lead to severe consequences, such as bias amplification, discrimination, and harm to marginalized communities. While existing fairness benchmarks mainly focus on single-turn dialogues, multi-turn scenarios, which in fact better reflect real-world conversations, present greater challenges due to conversational complexity and potential bias accumulation. In this paper, we propose a comprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios, FairMT-Bench. Specifically, we formulate a task taxonomy targeting LLM fairness capabilities across three stages: context understanding, user interaction, and instruction trade-offs, with each stage comprising two tasks. To ensure coverage of diverse bias types and attributes, we draw from existing fairness datasets and employ our template to construct a multi-turn dialogue dataset, FairMT 10K. For evaluation, GPT-4 is applied, alongside bias classifiers including Llama-Guard-3 and human validation to ensure robustness. Experiments and analyses on FairMT 10K reveal that in multi-turn dialogue scenarios, current LLMs are more likely to generate biased responses, and there is significant variation in performance across different tasks and models. Based on this, we curate a challenging dataset, FairMT 1K, and test 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show the current state of fairness in LLMs and showcase the utility of this novel approach for assessing fairness in more realistic multi-turn dialogue contexts, calling for future work to focus on LLM fairness improvement and the adoption of FairMT 1K in such efforts.",
    "keywords": [
        "Fairness",
        "Benchmark",
        "Large language model"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=RSGoXnS9GH",
    "pdf_link": "https://openreview.net/pdf?id=RSGoXnS9GH",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a fairness benchmark designed for multi-turn dialogues, called FairMT-Bench. Then, it detailed the experiments and analysis with multiple SOTA LLMs across dimensions like tasks, dialogue turns, bias types and attributes. It carefully analyzed the results and pointed out the areas where LLMs fail to maintain a stable fairness. For example, LLMs tend to perform worse on fairness in multi-turn dialogues than single turn. Moreover, the paper proposes a more challenging fairness evaluation dataset, FairMT-1K, that contains examples LLMs perform worst on."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper contributes a novel fairness benchmark specifically for multi-turn dialogues, while current benchmarks primarily focus on single-turn dialogues.\n\n2. The paper extensively benchmarks on most popular LLMs, and provides detailed results and analysis across many dimensions, like tasks, dialogue turns, bias types and attributes. The paper comprehensively demonstrates each LLM's performance, and pinpoints the areas where fairness is challenging to LLMs. The results show that fairness, especially in multi-turn dialogues, is still a challenging task for LLMs.\n\n3. The paper has great presentation. The figures are very illustrative and insightful."
            },
            "weaknesses": {
                "value": "1. A few factors can make the evaluation computationally expensive: (1) using GPT-4 as the evaluator (2) the multi-turn nature of the data and the evaluation process (3) the data size. It would be great if the paper can include some discussion on evaluation cost.\n\n2. While the paper discusses diverse sources and dimensions of bias, it does not discuss potential mitigation strategies. Offering even preliminary solutions or suggestions for future research directions would be valuable.\n\n3. As fairness and quality can be contradictory metrics, models that perform well on fairness might sacrifice its quality. Therefore, it'd be better to add a model quality dimension in the analysis, for a more comprehensive and insightful comparison of the models.\n\n4. The distribution of different tasks needs more clarification. I couldn't find the numbers of each task in the paper. Could the authors point me to the numbers if they are present in the paper, or add this information if they are absent?"
            },
            "questions": {
                "value": "1. When constructing the more challenging subset (FairMT 1K), the six models may diverge in scores. So how did you use the model results to select the examples?\n\n2. Typos:\n  1) Line 368, should be \"multi-turn dialogues generally show higher bias rates\"?\n  2) Figure 4, (a) and (b) look the same, which I assume is not intended?\n\n3. As mentioned above, could the authors clarify the number of examples allocated to each task in the FairMT-10K dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces \"FairMT-Bench,\" a comprehensive benchmark to evaluate fairness in LLMs, specifically in multi-turn dialogue scenarios. Addressing a gap in fairness assessments, which have primarily focused on single-turn interactions, the paper provides a dataset (FairMT-10K) and tasks that target fairness across three stages of dialogue complexity: context understanding, user interaction, and instruction trade-offs. The authors also present a distilled, challenging subset, FairMT-1K, and use both GPT-4 and Llama-Guard-3 classifiers, alongside human validation, to benchmark fairness in 15 LLMs, revealing substantial limitations in current model fairness across multi-turn interactions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Novel Focus on Multi-Turn Fairness Evaluation: The paper addresses the crucial gap of multi-turn dialogue fairness, reflecting real-world complexities in conversational AI use cases.\n2. FairMT-Bench and its datasets (FairMT-10K and FairMT-1K) cover a wide array of bias types and attributes, providing a rich resource for fairness research.\n3. By evaluating 15 prominent LLMs, the paper provides a robust, comparative analysis of model fairness, offering valuable insights for future LLM alignment improvements.\n3. Employing both GPT-4 and Llama-Guard-3 as fairness evaluators, along with detailed experimental setups, enhances reproducibility and contributes essential tools for future fairness research."
            },
            "weaknesses": {
                "value": "1. While the multi-turn focus is novel, the evaluation method largely depends on established LLM tools (e.g., GPT-4 as a judge), which may limit innovation in developing new fairness detection methodologies.\n2. The paper does not thoroughly explore why certain attributes (like gender and race) showed consistently poor performance across models, missing an opportunity to deepen the community\u2019s understanding of these biases.\n3. Relying heavily on GPT-4 for generating synthetic dialogue data could introduce bias from the generative model itself, which might impact the generalizability of FairMT-10K and FairMT-1K to real-world scenarios."
            },
            "questions": {
                "value": "1. How do you ensure that biases from GPT-4, which is used to generate and evaluate responses, do not affect the fairness outcomes, particularly given GPT-4's role as both data generator and evaluator?\n2. Can you clarify if the performance discrepancies across bias attributes are due to data distribution differences or model-specific limitations?\n3. Lack of comparison with existing evaluation, such as S-Eval or more recent fair-related work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work highlights the motivation of studying fairness in multi-turn dialogue scenarios and pinpoint the current scarcity of relevant research and resources in this domain. To address these limitations, the authors construct a comprehensive multi-turn dialogue benchmark to evaluate LLM fairness capabilities across two bias types and six bias attributes. Through detailed experiments and analysis, the paper reveal fairness shortcomings in current LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. **Valuable Resources**\nThis paper first presents a fairness benchmark in multi-turn dialogue scenarios, covering diverse bias types and attributes.\n2. **Extensive Experiments**\nConduct comprehensive experiments on current SOTA LLMs across six designed tasks.\n3. **Reliable Evaluation**\nUse GPT4 as a Judge, alongside bias classifiers including Llama3-Guard-3 and human validation.\n4. **Comprehensive Analysis**\nAnalyze evaluation results of single-turn and multi-turn dialogue across different models, tasks, and groups.\n5. **Valuable Insights** \nReveal two distinct bias defense mechanisms for current LLMs, which target defense implicit biases and explicit biases respectively."
            },
            "weaknesses": {
                "value": "**Ambiguous Task Taxonomy**\nIn Section 3.2, two taxonomies about fairness tasks are primarily discussed: comprehension-focused tasks VS. bias-resistance tasks (Line 318-320) and implicit biases VS. explicit biases (Line 322-323). These taxonomies are clear and reasonable. However, the taxonomy outlined in section 2.1 lacks clarity and mention. The naming of \"interaction fairness\" class is somewhat confusing, and the boundaries between this class and the other two are not clearly defined."
            },
            "questions": {
                "value": "**Consider Maintaining Consistency Between the Taxonomy Definition Section and the Analysis Section**\nIn my opinion, the taxonomy outlined in Section 2.1 is less clear than the two taxonomies presented in Section 3.2. I suggest adopting a more straightforward and understandable classification for tasks definitions. If modifying the definitions is not feasible, at the very least, please ensure consistency between the classifications used in the definition and analysis sections."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents the first fairness benchmark Fair-MT  for multi-turn dialogues because prior works can only consider single-turn scenarios.\n\nThis benchmark includes two bias types (Stereotype and Toxicity) and six bias attributes (gender, race, religion, race, etc.) and up to 10K items.  The authors first collected source data from RedditBias, SCIB, and HateXplain.  Then, the authors use GPT-4 to construct the dataset for the identified six tasks ( 2 tasks per LLM capacity). Finally, the authors mainly use GPT-4 to evaluate the fairness of LLM candidates."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This work has a well-defined task taxonomy, which is clear and comprehensive.\n2. Unlike previous single-turn works, this work presents the first multi-turn fairness benchmark. Multi-turn has more practical significance and research value.\n3. Six multi-turn dialogue tasks are proposed to detect the fine-grained fairness of LLMs.\n4. LLMs can automatically accomplish most tasks related to dataset construction and evaluations. \n5. Very detailed evaluation using the proposed FairMT-10K.\n6. A more efficient FairMT-1K is also proposed."
            },
            "weaknesses": {
                "value": "1. This work has carefully designed two tasks for each LLM capability.   Although each task has a considerable amount of space to introduce, there is a lack of specific formal definitions for each task in the main paper. \n\n2. Continuing from the first point, the main paper of this work has ignored many necessary details in the main paper in the remaining parts. \n\n3.  This work does not involve enough human annotation in both the dataset construction stage and the evaluation stage.  There is only a small-scale human-annotation test in the Appendix.\n\n4. LLMs play the role of both the dataset constructor and the evaluation referee. Is this configuration really convincing?\n\n5. This work only focuses on the evaluation.  The author needs a more detailed discussion on what improvement directions can be brought to the subsequent researches through experimental findings."
            },
            "questions": {
                "value": "Questions:\n\n1. See my weaknesses 4.\n\nMinor Issues:\n\n1. Line 130:  three stages of interaction with users`: :`\n2. Line 732:  The prompt given to GPT-4 is shown in Figure `??`"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This work studies the fairness in LLMs.  The corresponding methodology and findings may require an ethics review."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}