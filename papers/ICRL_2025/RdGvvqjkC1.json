{
    "id": "RdGvvqjkC1",
    "title": "How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation",
    "abstract": "Jailbreak attacks, where malicious prompts bypass generative models\u2019 built-in safety, have raised significant concerns about model vulnerability. While diverse defense methods have been proposed, the underlying mechanisms governing the trade-offs between model safety and helpfulness, and their application to Large Vision-Language Models (LVLMs) remain insufficiently explored. This paper systematically investigates jailbreak defense mechanisms by reformulating the standard generation task as a binary classification problem to probe model refusal tendencies across both harmful and benign queries. Our analysis identifies two key defense mechanisms: safety shift, which generally increases refusal probabilities for all queries, and harmfulness discrimination, which enhances the model\u2019s ability to distinguish between benign and harmful queries. Leveraging these mechanisms, we design two ensemble defense strategies\u2014inter-mechanism and intra-mechanism ensembles\u2014to explore the safety-helpfulness balance. Empirical evaluations on the MM-SafetyBench and MOSSBench datasets on top of LLaVA-1.5 models demonstrate the effectiveness of these ensemble approaches in either enhancing model safety or achieving an improved safety-utility balance. These findings offer valuable insights into jailbreak defense strategies and contribute to the development of more resilient LVLM safety systems.",
    "keywords": [
        "Jailbreak",
        "defense",
        "analysis",
        "LVLM"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=RdGvvqjkC1",
    "pdf_link": "https://openreview.net/pdf?id=RdGvvqjkC1",
    "comments": [
        {
            "summary": {
                "value": "This paper attempts to characterize internal jailbreak defenses into two categories: Safety Shift and Harmfulness Discrimination. The authors do this by prompting the model with a classification question and analyzing the output distribution for benign and harmful queries. Using this formulation, the paper propose intra and inter mechanism ensembling techniques that help balance the safety and usefulness trade-off offered by the model. The evaluation is performed on LLaVA-1.5."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The author tackle an important problem of characterizing how LLM jailbreak defenses work. The paper is well written and motivated. I appreciate the author's effort in evaluating a variety of defenses. Moreover, attributing defenses to safety shift and harmfulness discrimination is an interesting idea."
            },
            "weaknesses": {
                "value": "1. The analysis in this paper can also be applied to text-only LLMs. Since, text-only LLMs are more widely used, the authors should consider expanding the analysis.\n2. The whole analysis focuses on affirmative response on benign queries and refusal on harmful queries. However, it does not take into account the quality of the generated responses (specially since the evaluation uses a pattern matching based judge). Combining multiple defenses could severely harm the quality of the returned responses. For instance, the query refactoring method Caption w/o image first captions the image and then adds it as text to the model prompt. While helping with safety, this would lead to decrease in quality of responses on benign queries.\n3. The mechanistic analysis is done on a classification setting, and the insights might not transfer to the generative setting (as also hinted by the authors)."
            },
            "questions": {
                "value": "1. The paper performs the entire evaluation on a single LVLM - LLaVA-1.5. It would be interesting to see if the findings generalize to other LVLMs like CogVLM and InternLM-XC.\n2. The mechanistic analysis characterizes defenses as either safety shift or harmfulness discrimination. However, Figure 3 (b) shows that combination of SR + MO i.e. two safety shift techniques lead to an increase in distance from 0.5 to 0.63. This suggests that defenses might be providing different degrees of both safety shift and harmfulness discrimination."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the trade-off between safety and helpfulness in jailbreak defenses, highlighting two fundamental mechanisms: safety shift and harmfulness discrimination. The authors analyze various ensemble strategies to enhance model safety and improve the safety-helpfulness balance, demonstrating their effectiveness across multimodal contexts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Introduces novel defense mechanisms (safety shift and harmfulness discrimination) for LVLMs, providing fresh insights into model security. \n2. Includes a comprehensive analysis supported by rigorous empirical validation across various datasets and models, utilizing a robust methodology by reformulating generation tasks into classification problems.\n3. Evaluates two ensemble defense strategies (inter-mechanism and intra-mechanism integration), examining the balance between enhancing model safety and preserving usability."
            },
            "weaknesses": {
                "value": "1. The captions for the figures and tables in the paper are overly simplistic.\n2. The experiments primarily rely on the MM-SafetyBench and MOSSBench datasets, which may not fully reflect the diversity and complexity of real-world scenarios.\n3. While the paper proposes various defense strategies, it may not adequately discuss their feasibility and cost-effectiveness in practical deployment."
            },
            "questions": {
                "value": "1. While the paper focuses on LLaVA-1.5-7B and LLaVA-1.5-13B, how do the authors ensure that the findings apply to other models within the LLaVA series?\n2. Can the captions for figures and tables be more detailed?\n3. What is the optimal combination of the 27 defense methods, and can you analyze why this combination yields better results? Additionally, do these findings have universality for other models, such as language models?\n4. Will these defense strategies affect the model's real-time response speed? Will they introduce additional overhead?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a novel and straightforward way to reformulate the\nLVLM's generation problem as a binary classification problem, in order to\ninvestigate the the mechanism of Jailbreak Defenses."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. A new angle to investigate jailbreak defenses is proposed. It is \n   interesting.\n2. The reformulation practice is interesting and valuable, providing a\n   effective way to investigate the mechanism of jailbreak defenses.\n3. Extensive experiments across various jailbreak defenses are conducted."
            },
            "weaknesses": {
                "value": "1. The motivation behind focusing on LVLMs is not clear.\n2. Further analysis on the results (especially the ensemble part) is needed.\n3. The selection of models under evaluation is not convincing."
            },
            "questions": {
                "value": "1. I believe the reformulation practice is valuable and straightforward.\n   However, it seems that it can also be applied to other types of models,\n   like LLMs. What is the motivation behind focusing on LVLMs?\n2. The ensemble part is not well analyzed. Could you provide more insights\n   on the ensemble mechanism? For example, provide some specific suggestions\n   for ensemble strategies.\n3. The selection of models under evaluation is not convincing; both models\n   are LLaVA. Could you add some more models with different architectures\n   to make the evaluation more convincing?\n4. Given the results presented in Figure 4, does there exist any way to\n   further investigate the representative power of the binary classifier?\n   Furthermore, can this representative power of the binary classifier\n   be improved?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper systematically investigates jailbreak defense mechanisms by reformulating the generative task as a binary classification problem to probe model refusal tendencies across both harmful and benign queries. Focusing on internal strategies, this study identifies two key defense mechanisms: safety shift and harmfulness discrimination. They increase refusal probabilities for all queries and enhance the model\u2019s ability to distinguish benign and harmful queries, respectively. Experiments demonstrate the effectiveness of the ensemble defense mechanism."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow.\n\n2. Two metrics, Mean Shift and Distance Change, are reasonable and can help to visualize the differences between different types of defenses.\n\n3. The experiment part is comprehensive and some insights in Section 4.4 and 4.5 are interesting."
            },
            "weaknesses": {
                "value": "1. Although the authors mention that they do not assess the actual usefulness of model\u2019s responses, but rather the willingness from a safety perspective, a core concern of this reviewer is that \u201cCan some seemingly effective defense methods, like refactoring, truly keep a good performance on benign queries?\u201d.\n\n2. Lack of the experiments of defenses against stronger and various attackers. This reviewer is worried about the limit of attack methods, as well as the datasets. MM-Safety-Bench is a comprehensive benchmark, but the malicious query is quite direct and lacks of stealthiness. \u201cHide the text queries at the bottom of associated images\u201d can only represent a few types of attack methods. More experiments on stronger or various attackers are recommended.\n\n3. Although the experiment part is comprehensive, it lacks of some mainstream model optimization defenses, such as PPO and DPO."
            },
            "questions": {
                "value": "1. See weakness 1. Can some seemingly effective defense methods, like refactoring, truly keep a good performance on benign queries?\n\n2. See weakness 2. Can MM-SafetyBench and MOSSBench represent most attack vectors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}