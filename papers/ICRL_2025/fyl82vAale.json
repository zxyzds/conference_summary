{
    "id": "fyl82vAale",
    "title": "Sequential Order-Robust Mamba for Time Series Forecasting",
    "abstract": "Mamba has recently emerged as a promising alternative to Transformers, offering near-linear complexity in processing sequential data.\nHowever, while channels in time series (TS) data have no specific order in general, recent studies have adopted Mamba to capture channel dependencies (CD) in TS, introducing a sequential order bias. To address this issue, we propose SOR-Mamba, a TS forecasting method that 1) incorporates a regularization strategy to minimize the discrepancy between two embedding vectors generated from data with reversed channel orders, thereby enhancing robustness to channel order, and 2) eliminates the 1D-convolution originally designed to capture local information in sequential data. Furthermore, we introduce channel correlation modeling (CCM), a pretraining task aimed at preserving correlations between channels from the data space to the latent space in order to enhance the ability to capture CD.\nExtensive experiments demonstrate the efficacy of the proposed method across standard and transfer learning scenarios.",
    "keywords": [
        "Mamba",
        "State-space Model",
        "Time Series"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "We propose SOR-Mamba, a time series forecasting method that uses Mamba with regularization to address the sequential order bias in capturing channel dependencies.",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=fyl82vAale",
    "pdf_link": "https://openreview.net/pdf?id=fyl82vAale",
    "comments": [
        {
            "comment": {
                "value": "Thank you for raising your score!\n\nIf you have any further questions or suggestions, please feel free to share them with us."
            }
        },
        {
            "title": {
                "value": "Feedback for authors"
            },
            "comment": {
                "value": "Dear Authors:\n\nThank you for your rebuttal!\n\nA1: The author also claims that this is the property of stability of model after some factors are affected, so according to the description of the property of the neural network in the statistical learning book, this should be called stability. This is mainly because robustness is too easily understood to mean that the model performance does not degrade after being disturbed by input noise.\n\nA2: If the data set size is small (i.e., ETT dataset), then the proposed method seems to have under-fitting on small-scale tasks, which will be an obstacle for the model to solve small-scale time series forecasting tasks.\n\nI was generally satisfied with the rest of the responses. Thus, I'm willing to increase the score to 6."
            }
        },
        {
            "comment": {
                "value": "###  **Q1.  Channel order shuffling method (fixed shuffling method vs. shuffled randomly each time)**\n\n\nWe use a **fixed shuffling approach** for channel order, with results averaged over five different random seeds. Using a **random shuffling each time** would impair the model\u2019s ability to distinguish between channels, leading to poor performance, as discussed in **Table 12**, **Figure 9**, and **L466\u2013470**.\n\n\n&nbsp;\n\n---\n###  **Q2. Incorrect calculation formula (Last row of Table 1)**\n\nThank you for pointing this out. We initially reversed the sign of the last row of **Table 1** to indicate the degree of **improvement**, as a lower MSE indicates better performance. However, following the reviewer\u2019s feedback, we corrected the sign to avoid any confusion."
            }
        },
        {
            "comment": {
                "value": "###  **W1-a) Reverse modeling is widely used**\n\nWe acknowledge that reverse modeling (bidirectional Mamba) is widely used, as noted in **L46--51**. However, we highlight its inherent limitations in **Table 1**, where the bidirectional Mamba does not consistently yield the best performance and, in some cases, underperforms compared to a single unidirectional Mamba. Additionally, the bidirectional Mamba uses two Mambas for tuning, doubling the computational burden compared to a single Mamba. In light of these limitations, we propose an ***alternative to reverse modeling (bidirectional Mamba)***: a single unidirectional Mamba with regularization, which demonstrates improved performance and efficiency, as shown in **Table 2,5** and **Table 13**, respectively.\n\n&nbsp;\n\n\n---\n###  **W1-b) Removal of 1D-conv is trivial**\n\nWhile removing the 1D-conv might seem minor, many studies in the TS domain often adopt architectures directly from NLP or vision tasks **without accounting for TS-specific properties**. Since the 1D-conv in Mamba [A] was originally designed to capture sequential patterns (**L66--68**), applying it to non-sequential TS channels is an **unnatural choice**, yet it has been commonly used without adaptation.  ***We believe our approach is non-trivial, as it specifically accounts for the distinctive characteristics of TS datasets that have been overlooked in other Mamba-based methods. Furthermore, the proposed method has the potential for application in other domains involving non-sequential data (e.g., tabular data)***.  \n\nFurther details on the removal of the 1D-conv are provided in **Appendix D**.\n\n\n&nbsp;\n\n[A] Gu, Albert, and Tri Dao. \"Mamba: Linear-time sequence modeling with selective state spaces.\" arXiv (2024)\n\n&nbsp;\n\n---\n###  **W2.  Performance improvement is not significant.**\nIt is important to clarify that our primary goal is not to achieve substantial performance gains over the baseline, but rather to demonstrate that a ***single unidirectional Mamba with a regularization strategy can effectively replace the widely used bidirectional Mamba***, yielding comparable or even better performance with greater efficiency, as shown in **Table 5** and **Table 13**. Additionally, applying the regularization strategy to the bidirectional Mamba also results in performance gains, as shown in **Table 6**. \n\n\nWhile the optimal model structure (and hyperparameters) may vary between the unidirectional and bidirectional Mamba setups, we used the original hyperparameters from the bidirectional Mamba for the unidirectional Mamba (see **L265--266**), allowing us to evaluate results on the same basis. Optimizing these hyperparameters specifically for the unidirectional Mamba configuration could likely yield additional performance gains.\n\n\n&nbsp;\n\n---\n###  **W3.  Necessity of removing 1D-conv**\n\n\nIt is important to note that we remove 1D-conv primarily ***for general TS datasets, where channels do not inherently have a sequential order***. We believe it is reasonable to design a model architecture suited to these general cases.\n\n\nAs shown in **Table 7** and **Figure 5**, removing the 1D-conv of Mamba in datasets where channel order has inherent structure (e.g., traffic data) does not always lead to performance benefits. However, this finding further supports our approach, indicating that the 1D-conv captures sequential relationships between tokens (channels). When channels have an inherent order, which is identifiable based on the dataset domain or type, removing the 1D convolution may not be desirable.\n\n\nFurther details on the rationale and process for removing the 1D-conv are provided in **Appendix D**.\n\n\n&nbsp;\n\n---\n### **W4.  Comparison with other Mamba models**.\n\nAs mentioned in **Section 2. Related Works**, several concurrent studies have utilized Mamba to capture CD [A,B,C,D], and these methods generally address the sequential order bias by employing the **bidirectional Mamba [A,B,C,D]**. However, aside from our baseline method [A], most of these works have not released code for reproducibility, making direct comparisons with our approach currently infeasible. We look forward to conducting comparisons when these implementations become available.\n\n&nbsp;\n\n[A] Wang, Zihan, et al. \"Is mamba effective for time series forecasting?.\" arXiv (2024)\n\n[B] Liang, Aobo, et al. \"Bi-Mamba4TS: Bidirectional Mamba for Time Series Forecasting.\" arXiv (2024) \n\n[C] Weng, Zixuan, et al. \"Simplified Mamba with Disentangled Dependency Encoding for Long-Term Time Series Forecasting.\" arXiv (2024)\n\n[D] Behrouz, A., Santacatterina, M., & Zabih, R. \u201cMambamixer: Efficient selective state space models with dual token and channel selection\" arXiv (2024)"
            }
        },
        {
            "comment": {
                "value": "###  **W1.  The regularization strategy is overly simplistic**\n\nWe believe that the regularization strategy ***should be simple and efficient*** to effectively replace the bidirectional Mamba for capturing CD. Our proposed regularization approach is intentionally straightforward and efficient, leveraging embeddings directly from the existing architecture ***without requiring additional modules or embeddings***. This design minimizes computational overhead while providing effective regularization within the model.\n\n&nbsp;\n\n---\n###  **W2. Justification of removal of 1D-conv**\n\nIt is important to note that we remove 1D-conv primarily ***for general TS datasets, where channels do not inherently have a sequential order***. We believe it is reasonable to design a model architecture suited to these general cases.\n\n\nAs shown in **Table 7** and **Figure 5**, removing the 1D-conv of Mamba in datasets where channel order has inherent structure (e.g., traffic data) does not always lead to performance benefits. However, this finding further supports our approach, indicating that the 1D-conv captures sequential relationships between tokens (channels). When channels have an inherent order, which is identifiable based on the dataset domain or type, removing the 1D convolution may not be desirable.\n\n\nFurther details on the rationale and process for removing the 1D-conv are provided in **Appendix D**.\n\n\n&nbsp;\n\n---\n###  **Q1.  Comparison with other (Order-invariant) Mamba architectures**\nAs mentioned in **Section 2. Related Works**, several concurrent studies have utilized Mamba to capture CD [A,B,C,D], and these methods generally address the sequential order bias by employing the **bidirectional Mamba [A,B,C,D]**. However, aside from our baseline method [A], most of these works have not released code for reproducibility, making direct comparisons with our approach currently infeasible. We look forward to conducting comparisons when these implementations become available.\n\n&nbsp;\n\n\n[A] Wang, Zihan, et al. \"Is mamba effective for time series forecasting?.\" arXiv (2024)\n\n[B] Liang, Aobo, et al. \"Bi-Mamba4TS: Bidirectional Mamba for Time Series Forecasting.\" arXiv (2024) \n\n[C] Weng, Zixuan, et al. \"Simplified Mamba with Disentangled Dependency Encoding for Long-Term Time Series Forecasting.\" arXiv (2024)\n\n[D] Behrouz, A., Santacatterina, M., & Zabih, R. \u201cMambamixer: Efficient selective state space models with dual token and channel selection\" arXiv (2024)\n\n&nbsp;\n\n---\n###  **Q2.  Definition of sequential order bias**\n\nAs mentioned in **L322--333**, the sequential order bias is quantified by ***measuring the performance difference (average MSE across four horizons) when the channel order is reversed***, using SOR-Mamba without regularization."
            }
        },
        {
            "comment": {
                "value": "###  **W1.  Complexity of proposed architecture**\nThank you for your feedback regarding the complexity of our approach. We believe the proposed method is both simple and effective, as noted by reviewer 9Yaz. \n\n&nbsp;\n\n> **(1) Simple architecture**\n\nThe removal of the 1D-conv is straightforward and does not require any additional changes to the remaining architecture, as detailed in **Appendix D**. \nFor the proposed regularization, we utilize embeddings from the existing architecture without introducing any additional modules or embeddings, as shown in **Appendix C**. This approach preserves simplicity and effectiveness while minimizing computational overhead. \n\n&nbsp;\n\n> **(2) Simple pretraining task**\n\nFor the CCM pretraining task, only the correlation between output vectors needs to be computed, with no need for extra modules. To clarify the process, we have included pseudocode for CCM in **Appendix J** and further guidance in **L244**, highlighting the task\u2019s simplicity.\n\n&nbsp;\n\nTo facilitate replication and ensure accessibility, we have also provided the code in the supplementary material.\n\n\n&nbsp;\n\n---\n###  **W2.  Trade-off in removing 1D-conv**\n\n\nIt is important to clarify that ***our primary motivation for removing the 1D-conv is not efficiency***, but rather to address ***general TS datasets***, where channels typically do not have a sequential order. We believe it is reasonable to design a model architecture suited to these general cases.\n\n\nAs shown in **Table 7** and **Figure 5**, removing the 1D-conv of Mamba in datasets where channel order has inherent structure (e.g., traffic data) does not always lead to performance benefits. However, this finding further supports our approach, indicating that the 1D-conv captures sequential relationships between tokens (channels). When channels have an inherent order, which is identifiable based on the dataset domain or type, removing the 1D convolution may not be desirable.\n\n\nFurther details on the rationale and process for removing the 1D-conv are provided in **Appendix D**.\n\n\n&nbsp;\n\n\n\n---\n###  **W3.  Hyperparameter sensitivity**\nThank you for pointing this out. We have, however, ***already*** discussed the robustness of our proposed method to hyperparameters across various datasets, as follows:\n- Robustness to **hyperparameter $\\lambda$**: **Appendix I.1**\n- Robustness to **distance metric (for regularization)**: **Appendix K.1** (as guided in **L207**) \n- Robustness to **distance metric (for CCM)**: **Appendix K.2** (as guided in **L224--255**)\n\n\n&nbsp;\n\n---\n###  **Q1.  Robustness to distance metric (for regularization)**\n\n\n\n\n **Q1-a.  Relationship between the distance metric $d$ and the performance**\n\n\n\nAs discussed in **Appendix K.1**, the performance gains from regularization loss are **robust across different choices of the distance metric $d$**. However, we did not observe a consistent relationship between the choice of metric and the specific characteristics of the TS datasets.\n\n&nbsp;\n\n\n**Q1-b.  Relationship between the effectiveness of regularization and the characteristic of dataset**\n\n\nWhile **Figure 4** shows that the degree of bias varies with the number of channels and channel correlation, we believe that the relationship between performance gains (i.e., the effectiveness of the regularization) and bias reduction is influenced by various inherent factors.\n\n\n&nbsp;\n\n\n---\n###  **Q2.  Effect of pretraining tasks by dataset characteristics**\n\nWe agree on the importance of analyzing the performance gains of different pretraining tasks and dataset characteristics, particularly in terms of the number of channels. For this reason, we have ***already*** discussed this in **Figure 6**, which demonstrates that the proposed pretraining tasks outperform both MM and reconstruction tasks across datasets with both large and small numbers of channels. Additionally, as shown in the right panel of **Figure 4**, the correlation between channels varies across datasets, with the ETT datasets exhibiting low correlation. Despite this, our method performs well on these datasets, demonstrating its robustness and effectiveness even in cases with weak channel correlations."
            }
        },
        {
            "comment": {
                "value": "###  **Q1.  Hyperparameter lambda and the effect of regularization**\n\n\nAs the reviewer mentioned **Table I.1** illustrates that the inclusion of the regularization loss significantly improves the performance **regardless of the value of lambda**.\n\n\nThis phenomenon is reasonable, as shown in **Figure 9** and **Figure G.1**, where the regularization loss converges very quickly early in the training process for our proposed methods. This rapid convergence suggests that the regularization effectively aligns the two reversed-order vectors with ease, yielding consistent performance improvements regardless of the value of lambda.\n\n&nbsp;\n\n\n---\n###  **Q2.  Motivation of (1) single unidirectional Mamba and (2) removal of 1D-conv.**\n\n\n> (1) **Motivation of single unidirectional Mamba**\n\n\nPrevious research used two separate Mambas to capture channel dependencies from both directions [A, B] in an effort to address the channel order bias. However, we found that using bidirectional Mamba does not always yield the best performance (**Table 1**) and can be inefficient (**L51--52**). By using a single Mamba with regularization loss, we effectively address this bias, achieving superior performance (**Table 2,4,5**)  and improved efficiency (**Table 13**) compared to using two Mambas.\"\n\n&nbsp;\n\n> **(2) Motivation of removal of 1D-conv**\n\n\nThe 1D-conv in Mamba [C] was originally designed to capture local information in sequential data (**L66--68**), making it less suitable for non-sequential data (i.e., channels in TS). However, previous works have overlooked the need to remove the 1D-conv when using Mamba to capture channel dependencies (CD) [D,E,F], where channels do not have sequential order. This has motivated us to remove it from the Mamba block. Further details on the removal of the 1D-conv are provided in **Appendix D**.\n\n\n&nbsp;\n\n\n[A] Wang, Zihan, et al. \"Is mamba effective for time series forecasting?.\" arXiv (2024)\n\n\n[B] Liang, Aobo, et al. \"Bi-Mamba4TS: Bidirectional Mamba for Time Series Forecasting.\" arXiv (2024) \n\n\n[C] Gu, Albert, and Tri Dao. \"Mamba: Linear-time sequence modeling with selective state spaces.\" arXiv (2024)\n\n\n[D] Weng, Zixuan, et al. \"Simplified Mamba with Disentangled Dependency Encoding for Long-Term Time Series Forecasting.\" arXiv (2024)\n\n\n[E] Cai, Xiuding, et al. \"MambaTS: Improved Selective State Space Models for Long-term Time Series Forecasting.\" arXiv (2024)\n\n\n[F] Ma, Shusen, et al. \"Fmamba: Mamba based on fast-attention for multivariate time-series forecasting.\" arXiv (2024)\n\n&nbsp;\n\n\n---\n###  **Q3. Lack of details regarding CCM**\n\n\nWe apologize for the lack of detail regarding the proposed pretraining task, CCM. To address this, we have included pseudocode for the task in **Appendix J** and provided additional guidance for readers in **L244**."
            }
        },
        {
            "comment": {
                "value": "###  **W1.  Motivation for regularization**\n\n\nIt is important to clarify that the proposed regularization was ***not specifically intended to enhance the capture of channel correlation***, but to ***address the sequential order bias***.\n\n\nAs shown in **Table 1** and **Figure 7**, previous methods employing bidirectional Mamba to handle sequential order bias still exhibited sensitivity to channel order, both in terms of performance and qualitative results.\nThis motivated us to develop a more effective approach to address the bias: the regularization strategy. This approach yields a more consistent representation that is robust to changes in channel order, as demonstrated in **Table 10**, **Appendix H.1** and **Figure 7**.\n\n\n\n\n&nbsp;\n\n---\n###  **W2.  Longer lookback window**\n\n\nWe set $L=96$ to align with the experimental protocols used in previous works (iTransformer [A], S-Mamba [B]). However, as the reviewer suggested, we also conducted an ablation study on input length across four datasets (Traffic, Electricity, PEMS04, ETTm1), testing various lookback lengths $L \\in \\\\{48, 96, 192, 336, 720\\\\}$, with forecast horizon of $H= 12$ for PEMS04 and $H = 96$ for the other datasets, in accordance with previous works [A,B]. The results, shown in **Appendix L**, indicate that performance remains robust to the choice of $L$ for some datasets and even improves with a larger $L$ for others.\n\n\n&nbsp;\n\n\n[A] Liu, Yong, et al. \"iTransformer: Inverted transformers are effective for time series forecasting.\" ICLR (2024)\n\n[B] Wang, Zihan, et al. \"Is mamba effective for time series forecasting?.\" arXiv (2024)\n\n\n\n\n&nbsp;\n\n---\n###  **W3.  Clarity of sentences**\n\n\nThank you for your feedback. \n\n\nWe understand that the excessive use of abbreviations without clear explanations can make the manuscript and experimental result tables difficult to follow. \n\n\nBelow is a list of the abbreviations used:\n- CI/CD: channel independence/channel dependence\n- TD: temporal dependencies\n- LTSF: long-term TS forecasting\n- 1D-conv: 1D-convolution\n- SSL/SL: self-supervised learning / supervised learning \n- FT/LP: fine-tuning / linear probing,\n- MSE/MAE: mean squared error / mean absolute error\n\n\nWe believe these abbreviations are commonly used in previous works. However, we will replace the abbreviations (TD, FT, LP) with full expressions, as they are not widely recognized, and expand (LTSF), as it appears infrequently in our manuscript. We will revise the text to provide clearer definitions and reduce abbreviation use to improve readability."
            }
        },
        {},
        {
            "comment": {
                "value": "### **W1. Why Mamba for capturing CD?**\n\n\nAs noted in **Line 39\u201341**, ***our focus is on Mamba\u2019s role in capturing CD, in line with recent work [A] that advocates for using complex attention mechanisms for CD while employing simple multi-layer perceptrons (MLPs) for TD.*** Consequently, recent works [B,C] have also explored using Mamba as an alternative to attention for capturing CD, aligning with our direction. In addition, we present further experiments on using Mamba to capture temporal dependency (TD) in **Table 11**. These results are consistent with previous studies, suggesting that complex architectures like attention or Mamba are more effective for capturing CD than TD.\n\n&nbsp;\n\n[A] Liu, Yong, et al. \"iTransformer: Inverted transformers are effective for time series forecasting.\" ICLR (2024)\n\n[B] Wang, Zihan, et al. \"Is mamba effective for time series forecasting?.\" arXiv (2024)\n\n[C] Ma, Shusen, et al. \"Fmamba: Mamba based on fast-attention for multivariate time-series forecasting.\" arXiv (2024)\n\n&nbsp;\n\n\n---\n###  **W2.  Longer lookback window**\n\n\nWe set $L=96$ to align with the experimental protocols used in previous works (iTransformer [A], S-Mamba [B]). However, as the reviewer suggested, we also conducted an ablation study on input length across four datasets (Traffic, Electricity, PEMS04, ETTm1), testing various lookback lengths $L \\in \\\\{48, 96, 192, 336, 720\\\\}$, with forecast horizon of $H= 12$ for PEMS04 and $H = 96$ for the other datasets, in accordance with previous works [A,B]. The results, shown in **Appendix L**, indicate that performance remains robust to the choice of $L$ for some datasets and even improves with a larger $L$ for others.\n\n&nbsp;\n\n[A] Liu, Yong, et al. \"iTransformer: Inverted transformers are effective for time series forecasting.\" ICLR (2024)\n\n[B] Wang, Zihan, et al. \"Is mamba effective for time series forecasting?.\" arXiv (2024)\n\n\n&nbsp;\n\n\n---\n###  **W3.  Calculation of (global) correlation**\n\nThank you for pointing this out. \nWe agree that the description of calculating the (global) correlation could be clearer. The global correlation, as mentioned in **L369--370**, refers to the correlation between channels across the entire \"training\" TS dataset, excluding the validation and test datasets. Details on the train-validation-test split protocol are described in **Appendix A**."
            }
        },
        {
            "comment": {
                "value": "###  **W1.  Definition of robustness**\n\n\nWe understand that robustness refers to the **stability of results or performance despite variations in certain factors**, and is not solely related to the context of adversarial attacks, as the reviewer mentioned. As noted in the **L17**, the robustness we discuss specifically pertains to **'robustness to channel order'** in TS, particularly regarding the channel order that Mamba receives as input\n\n\n\n\n&nbsp;\n\n----\n### **W2.  Performance of ETT datasets**\n\n\nIt is worth noting that \n(1) CD models, which capture dependencies between channels, typically have high capacity but lower robustness, whereas CI models exhibit the opposite [A,B]; and\n(2) the ETT datasets have the relatively small size among the benchmark datasets, as shown in **Appendix A.1**.\n\n\nDue to these factors, CI models (e.g., PatchTST [C], RLinear [D]) generally outperform CD models (e.g., iTransformer [E], S-Mamba [F], SOR-Mamba [ours]) on smaller datasets, including the ETT datasets. We believe that the relatively lower performance on the ETT datasets is not due to regularization strategy, but rather to the smaller dataset size, where CI models tend to be more suitable. \n\n&nbsp;\n\n[A] \u201cThe Capacity and Robustness Trade-off: Revisiting the Channel Independence Strategy for Multivariate Time Series.\u201d TKDE (2024)\n\n[B] Nie, Tong, et al. \"Channel-aware low-rank adaptation in time series forecasting.\" CIKM (2024)\n\n\n[C] Nie, Yuqi, et al. \"A time series is worth 64 words: Long-term forecasting with transformers.\" ICLR (2023)\n\n\n[D] Li, Zhe, et al. \"Revisiting long-term time series forecasting: An investigation on linear mapping.\" arXiv (2023)\n\n\n[E] Liu, Yong, et al. \"iTransformer: Inverted transformers are effective for time series forecasting.\" ICLR (2024)\n\n\n[F] Wang, Zihan, et al. \"Is mamba effective for time series forecasting?.\" arXiv (2024)\n\n\n&nbsp;\n\n----\n### **W3.  Training and inference time**\nWe appreciate the reviewer's interest in the practical efficiency of our approach. However, we have ***already*** conducted an analysis regarding the efficiency in terms of (1) the number of parameters, (2) complexity and memory, and (3) computational time (for both training and inference) using the Traffic dataset, which has a large number of channels (C=862), as shown in **Table 13**.\n\n\n\n&nbsp;\n\n----\n### **W4.  Misunderstanding of the term \u201csequence order\u201d**\nThank you for pointing this out. The varying sampling frequencies and periodic change rules for different channels in **Figure 1** were intended solely to illustrate our motivation in representing distinct channels. We agree that this may be confusing, so we will add further explanations to clarify this point."
            }
        },
        {
            "summary": {
                "value": "This paper proposes SOR-Mamba to solve the sequential order bias introduced by mamba capturing channel dependencies. Model SOR-Mamba incorporates a regularization strategy to minimize the distance between embeddings from data and reversed data and eliminates the 1D-convolution in the original Mamba block. And a pretraining task channel correlation modeling is introduced to preserve the channel correlation from the data space to the latent space. The effectiveness of the proposed method is demonstrated by the extensive experiment results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* This paper uses a single unidirectional Mamba with regularization and non-1D convolutions to capture channel correlations. By modifying the traditional Mamba module, it can better address the issue of sequential order bias. The research problem is clearly defined, and the solution is reasonable. And the pretraining task channel correlation modeling also enhances the model's generalization and performance.\n\n* The details of the model are mostly clear, with each module explained and supported by equations. The experiments  include time series forecasting, transfer learning and many ablation studies to validate the effectiveness of each module."
            },
            "weaknesses": {
                "value": "* The proposed method is mostly constructed on existing models -- reverse modeling has been widely used in literature and removing the 1d-conv is trival.\n* The experimental results show that the model's performance improvement across various datasets is not significant, mostly in the thousandth.\n* The removal of 1D-conv negatively impacts PEMS dataset in tale 7 and figure 5. The necessity to remove 1D-conv is uncertain.\n* The model uses the Mamba backbone, but the baseline only includes a single Mamba model: S-Mamba. Providing more models from the Mamba framework for comparison would be better."
            },
            "questions": {
                "value": "* When randomly shuffling the channel order, how is it done? Is a fixed shuffling method used for all sequences in a dataset, or is it shuffled randomly each time?\n* In the last row of Table 1, is the calculation formula written incorrectly?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces SOR-Mamba, a time series forecasting method that addresses the sequential order bias  while capture channel dependencies in Mamba, through a regularization strategy to minimize the discrepancy between two embedding vectors generated from data with reversed channel orders, and 1D-convolution removal, The authors also propose Channel Correlation Modeling (CCM) as a pretraining task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Clear problem identification regarding Mamba's limitations in handling unordered channels\n2. Comprehensive experiments\n3. Improved efficiency compared to other approaches"
            },
            "weaknesses": {
                "value": "**Limited Technical Novelty**:\n- The regularization strategy is overly simplistic that minimizing the distance between embeddings from different channel orders\n- The removal of 1D-conv lacks theoretical justification and appears to be an ad-hoc solution, As shown in Figure 5 of the paper, its removal may negatively impact datasets with ordered channels such as PEMS datasets"
            },
            "questions": {
                "value": "1. In the Manba architecture, are there other advances in order-invariant architectures that can be compared?\n2. The definition of sequential order bias in Figure 4 and how it is calculated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses a limitation in applying Mamba to time series forecasting: the sequential order bias when modelling channel dependencies, as channels typically lack inherent order. The authors propose SOR-Mamba, which introduces two main innovations: a regularization strategy that minimizes discrepancy between embedding vectors from reversed channel orders, and the removal of the unnecessary 1D-convolution layer. They also introduce Channel Correlation Modelling (CCM), a pretraining task that preserves channel correlations from data space to latent space. The method reported promising performance across different scenarios, including cases with missing data and varying channel orders, while maintaining computational efficiency in terms of memory and runtime."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed method addresses the sequential order bias in channel dependencies (CD). This bias can degrade performance when channels lack inherent order in multi-channel time series data. The SOR-Mamba method mitigates this bias by introducing a regularization technique that minimizes the discrepancy between embeddings generated from reversed channel orders, thus improving robustness. \n\nAdditionally, the paper proposes Channel Correlation Modeling (CCM) as a pretraining task, which preserves channel correlations from data to latent space, enhancing CD capture. The architecture modifications, including removing the 1D-convolution and focusing on unidirectional Mamba, can reduce computational overhead, achieving efficiency gains in memory and parameter usage compared to other models like S-Mamba. This should be of interests to the group of readers working on multi channel time series data forecasting."
            },
            "weaknesses": {
                "value": "The paper\u2019s approach is relatively complex, involving several architectural changes (e.g., removing 1D-convolutions, applying specific regularization, and adding CCM pretraining) that may complicate replication and limit accessibility. \n\nWhile it emphasizes efficiency improvements, the paper could provide more detailed explanations regarding the trade-offs involved in removing the 1D-convolution, especially on datasets where channel order may have some inherent structure (e.g., traffic data). The study also lacks a comprehensive discussion of potential limitations in real-world deployment, such as sensitivity to hyperparameters (e.g., \u03bb in regularization) and its impact across different datasets."
            },
            "questions": {
                "value": "1. How does the choice of the distance metric d in the regularization term impact the model's performance across datasets with varying levels of channel correlation? - is the regularization term\u2019s effectiveness dependent on the number of channels or dataset-specific factors etc.?\n\n2. The paper uses CCM to preserve channel correlations in the latent space during pretraining. How does CCM compare to masked modelling and reconstruction pretraining across different dataset sizes and channel configurations? - and is there any limitation of CCM if there's only exist weak channel correlations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This manuscript proposes an algorithm to regularize bidirectional Mamba channel vectors in order to solve the problem that there is no intrinsic order relationship between channels in time series data. The authors also design a new pre-training method (CCM) to help the model better establish the association between channels of multi-dimensional time series. Extensive experiments and ablation experiments are conducted, and the experimental results show the effectiveness of the relevant contributions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This study achieves a good improvement in results with only a few changes to the method. The proposed method is simple, novel and effective.\n2. The experiments carried out by the authors are very detailed and the relevant parameters are well discussed. This is an experimentally rigorous and solid study.\n3. The manuscript is well-written and the figures are clear and intuitive."
            },
            "weaknesses": {
                "value": "1. The authors lack a complete description of the motivation for regularization. Why regularizing the bidirectional mamba channel vectors and minimizing the distance between them can help the model better capture the correlation between channels? This requires the author to give a more intuitive and reasonable motivation.\n2. Although the authors have done a lot of experiments, they lack experiments on longer input lengths. Some widely used baselines in this field, such as PatchTST (arXiv preprint arXiv:2211.14730, 2022.), all use longer input sequences.\n3. The authors use too many abbreviations in their manuscripts and experimental result tables, and lack clear and complete explanations of the abbreviations. This makes the relevant content very difficult to read and easily confusing."
            },
            "questions": {
                "value": "1. Table I.1 seems to show that the hyperparameter lambda does not affect the results. Is this a reasonable phenomenon for the key contribution?\n2. What are the principles and motivations for using parameter-sharing CD-Mamba block and removing 1D-conv? Can the authors provide additional details?\n3. The description of the pre-training method used in this study is somewhat vague. Can the authors supplement the algorithm flow or provide detailed explanation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduce a new Mamba-based model for long term time series forecasting named SOR-Mamba which utilizes modified Mamba structure to capture channel dependencies. Furthermore, the authors also introduces channel correlation modeling as a pretraining task aimed at enhancing the ability to capture channel dependencies."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed SOR-Mamba uses a modified Mamba structure (also with a regularization strategy) to eliminate the sequential order bias of Mamba which is more useful in capture channel dependencies.\n2. The authors also introduce a pretraining task named channel correlation modeling (CCM) to preserve correlations between channels from the data space to the latent space in order to enhance the ability to capture CD."
            },
            "weaknesses": {
                "value": "1. Since Mamba is a sequential-based model, and the channels lake sequential order, why we need Mamba to extract channel dependencies? It is important to clarify the importance of capturing channel dependencies with Mamba.\n2. The experimental comparisons appear unfair, as models like PatchTST exhibit better performance with longer lookback window. I strongly suggest the authors conduct additional experiments, such as setting $L=336$."
            },
            "questions": {
                "value": "See Weaknesses above.\n3. In line 369-370, I\u2019m curious about how to calculate the global correlation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I think these if no ethics concerns."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this manuscript, the authors claim that channels in time series (TS) data have no specific order in general, recent studies have adopted\nMamba to capture channel dependencies (CD) in TS, introducing a sequential order bias. To address this issue, the authors propose SOR-Mamba, a TS forecasting method that 1) incorporates a regularization strategy to minimize the discrepancy between\ntwo embedding vectors generated from data with reversed channel orders, thereby enhancing robustness to channel order."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors propose SOR-Mamba, a TS forecasting method that handles the sequential order bias by\n1) regularizing the unidirectional Mamba to minimize the distance between two embedding vectors\ngenerated from data with reversed channel orders for robustness to channel order.\n2. The authors introduce CCM, a novel pretraining task that preserves the correlation between channels from\nthe data space to the latent space, thereby enhancing the model\u2019s ability to capture CD.\n3. With the exception of the power-related datasets (ETTm1 & m2, etc.), the experimental performance exceeds the current SOTA."
            },
            "weaknesses": {
                "value": "1. It is puzzling why the authors mention 'robust' but does not give experiments to resist noise or PGD attacks. Or it may be necessary to modify the terminology in the manuscript to be more precise, after all, 13 different domains of time series prediction tasks have been tried.\n2. With the exception of the power-related datasets (ETTm1 & m2, etc.), the experimental performance exceeds the current SOTA. Why is the performance of the author's scheme inferior to SOTA (i.e., PatchTST) on power-related datasets (ETTm1 & m2, etc.) with more significant global regularity?\n3. Despite the computational complexity comparison, the reviewer is concerned about the actual training and inference time.\n4. The description in Figure 1 seems to be the difference between different exogenous time series due to sampling frequency and periodic change rule, and the description of sequence order is prone to misunderstanding (i.e., covariables have orders)."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}