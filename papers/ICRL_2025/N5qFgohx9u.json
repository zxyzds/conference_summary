{
    "id": "N5qFgohx9u",
    "title": "Expanding Expressivity in Transformer Models with M\u00f6biusAttention",
    "abstract": "Attention mechanisms and Transformer architectures have revolutionized Natural Language Processing (NLP) by enabling exceptional modeling of long-range dependencies and capturing intricate linguistic patterns. However, their inherent reliance on linear operations in the form of matrix multiplications limits their ability to fully capture inter-token relationships on their own. We propose M\u00f6biusAttention, a novel approach that integrates M\u00f6bius transformations within the attention mechanism of Transformer-based models. M\u00f6bius transformations are non-linear operations in spaces over complex numbers with the ability to map between various geometries. By incorporating these properties, M\u00f6biusAttention empowers models to learn more intricate geometric relationships between tokens and capture a wider range of information through complex-valued weight vectors. We build and pre-train a BERT and a RoFormer version enhanced with M\u00f6biusAttention, which we then finetune on the GLUE benchmark. We evaluate empirically our approach against the baseline BERT and RoFormer models on a range of downstream tasks. Our approach compares favorably against the baseline models, even with smaller number of parameters suggesting the enhanced expressivity of M\u00f6biusAttention. This research paves the way for exploring the potential of M\u00f6bius transformations in the complex projective space to enhance the expressivity and performance of foundation models.",
    "keywords": [
        "Transformer",
        "Attention",
        "BERT",
        "M\u00f6bius Transformation",
        "NLP",
        "RoPe"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=N5qFgohx9u",
    "pdf_link": "https://openreview.net/pdf?id=N5qFgohx9u",
    "comments": [
        {
            "withdrawal_confirmation": {
                "value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."
            }
        },
        {
            "summary": {
                "value": "The paper replaces the self-attention mechanism in BERT with M\u00f6bius Attention, which is motivated by projective geometry.\n\nThe authors hypothesize and the linear projections of the query, keys, and values limit the representation ability of transformers. Moreover, they have a more targeted treatment of the position embeddings.\n\nThey implement a new attention mechanism and run ablations using the GLUE benchmark."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The method is pretty simple to understand and has interesting motivation from projective geometry."
            },
            "weaknesses": {
                "value": "The results just don't seem to be particularly strong. The level of improvement seen could just be from doing lots of ablations and model retrains with different seeds."
            },
            "questions": {
                "value": "The new attention mechanism doesn't seem to improve the GLUE benchmark very much. Is there a task where the authors think there might be a larger improvement? Why?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on introducing non-linearity into attention mechanism, specifically exploring transformations capable of operating across diverse geometric spaces. The authors propose M\u00d6BIUSATTENTION where the non-linearity is introduced through M\u00f6bius transformations, which can map points between different geometries, such as from a line to a circle, a circle to a line, and similarly among lines and circles, allowing for the model to capture more complex inter-token dependencies than traditional linear methods. The authors states that integrating M\u00f6biusAttention can lead to improved performance across various NLP tasks without necessarily increasing the model size."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The idea that introducing non-linearity into attention through M\u00f6bius transformations is interesting.\n- The writing in first half part of paper is good and easy to follow. It gives enough backgrounds and a lot of details about the method."
            },
            "weaknesses": {
                "value": "- **Motivation is not clear.** I understand the intuition that M\u00f6bius transformations can capture more complex patterns and therefore improve the attention. However, I am not sure why we need to capture these patterns and what kind of tasks require these patterns. \n- Some statements are inaccurate. For example:\n  -  line 91: \"integrating M\u00f6biusAttention can lead to improved performance across various NLP tasks without necessarily increasing the model size.\"\n  - line 1285: \"However, our results (Section 5) show that even with fewer layers, M\u00f6biusAttention leads to performance improvements on many tasks.\"\n  - line 460: \"As seen in the attention heatmaps in Fig. 8 in the Appendix, vanilla attention almost never assigns zero attention score to a token pair. In contrast, M\u00f6biusAttention gives most of the pairs zero score and only a few a non-zero one.\" \n  - These statements can not be drawn from experimental results.\n- The experiments also have several limitations as follows:\n  - **Performance is weak**: The performance improvements brought by M\u00d6BIUSATTENTION are marginal.  For example, from Table 2, we can see MobRoFormer H & T can only achieve 0.27 performance improvement compared to RoFormer but consuming more parameters. Given this improvement is marginal, It is not clear that whether the performance improvement is significant. Therefore, I recommend the authors to test this. \n  - **Lack of Efficiency Analysis**: Intuitively, enabling attention to capture more complex patterns will lead to an increase in FLOPs and inference latency. However, the authors do not provide analysis in these aspects.\n  - **Lack of Large Scale Experiments**: Currently, the authors only mainly focused on small models and GLEU tasks. It is necessary to verify the effectiveness of M\u00f6biusAttention in models with larger sizes and broader tasks."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce M\u00f6biusAttention, an attention mechanism that incorporates M\u00f6bius transformations within the query computation of Transformer models. M\u00f6bius transformations, known for their ability to map complex geometries, are utilized to enable Transformers to capture more intricate inter-token relationships beyond the capabilities of linear operations. The paper outlines the mathematical underpinnings of projective geometry and M\u00f6bius transformations, detailing how these concepts are integrated into the attention mechanism.  Experimental results demonstrate that M\u00f6biusAttention-enhanced BERT achieves similar performance to the original BERT."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The integration of M\u00f6bius transformations into the attention mechanism is a novel approach that leverages complex geometric mappings."
            },
            "weaknesses": {
                "value": "1. The paper's motivation lacks rigorous justification and empirical support. Specifically, in the second paragraph of the introduction, the authors make several unsupported claims about the limitations of linear transformations and softmax in Transformer attention mechanisms: \n   \n    a) The authors assert that *\"predominantly linear operations restrict the ability of models to capture complex linguistic dependencies,\"* but provide no theoretical analysis or experimental evidence demonstrating these supposed limitations.\n   \n    b) The authors claim that linear operations lead to *\"potential information loss within each attention layer\"*. However, they don't elaborate on what type of information is lost or why this loss occurs. \n\n2.  The experimental results raise significant concerns about the practical value of the proposed M\u00f6biusAttention mechanism. While the authors introduce a mathematically sophisticated approach using M\u00f6bius transformations, the empirical results show that M\u00f6biusAttention-based BERT performs comparably to the original BERT model, with no significant improvements in performance."
            },
            "questions": {
                "value": "Please address the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new novel class of attention mechanism, which is M\u00f6biusAttention that is based on geometric backgrounds. M\u00f6biusAttention develops the standard linear attention mechanism with M\u00f6bius transformation and complex domain extension. In addition to provide the theoretic understanding of M\u00f6biusAttention, this paper applies it to BERT and RoFormer, then conducts the benchmark GLUE task."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Relationship between geometry and attention mechanism is an underexplored research field. This paper explores this field based on projective geometry. \n\n- Strong mathematical background: The proposed methodology, M\u00f6biusAttention, is motivated by rigorous mathematical background. It brings reliability for the claim of geometric characteristics embedded in M\u00f6biusAttention mechanism different from the standard attention mechanism. In addition, the extension to complex domain of token representation is smoothly related to the proposed methodology.\n\n- Well-written texts: Even though several claims seem questionable, at least they were easy to understand because of the clear writing."
            },
            "weaknesses": {
                "value": "- The paper\u2019s main argument is not persuasive. This paper argues M\u00f6biusAttention can enhance expressivity based on its non-linear operation, M\u00f6bius transformation. However, I guess M\u00f6bius transformation would not bring significant expressivity enhancement because it is constrained to be projective linear group (PGL) which is a scaled general linear group.\n- Experiment results do not support the expressivity argument. First, despite of the enhanced expressivity (in hypothesis), the performance improvement in the GLUE task is not superior (around 0.2 point improvement on average). Second, the argument of overfitting problem due to the enhanced expressivity (in the paragraph, lines 360~370) needs more clarification.\n\n- \u201cLearning to Forget\u201d: the argument \u201cFig. 8 in the Appendix, vanilla attention almost never assigns zero attention score to a token pair. In contrast, M\u00f6biusAttention gives most of the pairs zero score and only a few a non-zero one\u201d seems not supported by the results. In Figure 8, the average number of zero elements in Vanilla head (a~f) is 11.17, while M\u00f6bius head (g~l) is 9.67. More importantly, the attention matrix of M\u00f6biusAttention seems almost uniform.\n- \u201cMemory and Time Complexity\u201d: the comparison of parameters (104M vs 110M) seems negligible.\n\n- No experimental comparison between other comparable works. The previous works, such as RoPE and NeuralAttention are not compared with M\u00f6biusAttention.\n\nMinor: \n- In \u2018Geometric Interpretation\u2019 section, the orientation could be changed in \u2018det M_{qj} = 1\u2019 case, since orientation can be changed without changing the volume. \n- In lines 314~317 \u201cM\u00f6bius transformations offer a robust framework for analyzing and interpreting text\u201d, this argument seems not verified by experiments. The authors might want to view the geometric shapes of some semantically similar text tokens.\n- Even though this paper argues that M\u00f6biusAttention does not increase computational complexity significantly, it is not experimentally compared. Due to the separated operations (layernorm, attention) for real and complex parts, it might increase computational cost.\n- In Table 2, the \u2018Overall\u2019 scores seem not in fair comparison because the number of candidates that contribute to the maximum case selection are different, 2 candidates for baselines and 4 candidates for M\u00f6bius."
            },
            "questions": {
                "value": "- In Equations 7 & 8, I understood \u2018w_{kj}\u2019 and \u2018w_{vj}\u2019 are learnable weight parameters. However, they use the same notation with word embedding vector \u2018w_i\u2019 in the paragraph (lines 255~259).\n- In Equation 9, I think \u2018Q_ij=T_{qj}(rho_{ij})\u2019 should be \u2018Q_ij=M_{qj}(rho_{ij})\u2019 because the output of T_{qj}(rho_{ij}) is 2-dim, so that it is unable to matrix-multiply Q (2xd dimension) and K (d dimension). Also, if \u2018T_{qj}(rho_{ij})\u2019 is correct, then it should be T_{qj}(rho^{h}_{ij}) because this map utilizes M which 2x2 matrix. Please clarify if I am wrong. \n- Some table captions include weird newlines."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}