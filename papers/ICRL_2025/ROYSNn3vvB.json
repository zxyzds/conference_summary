{
    "id": "ROYSNn3vvB",
    "title": "Local convergence of simultaneous min-max algorithms  to differential equilibrium on Riemannian manifold",
    "abstract": "We study min-max algorithms to solve zero-sum differential games on\nRiemannian manifold.\nBased on the notions of\ndifferential Stackelberg equilibrium\nand differential Nash equilibrium on Riemannian manifold,\nwe analyze the local convergence of \ntwo representative deterministic simultaneous algorithms $\\tau$-GDA and $\\tau$-SGA\nto such equilibrium.\nSufficient conditions are obtained to establish their linear convergence rates \nby Ostrowski theorem on manifold and spectral analysis. \nThe $\\tau$-SGA algorithm is extended from\nthe symplectic gradient-adjustment method in Euclidean space\nto avoid strong rotational dynamics in $\\tau$-GDA.\nIn some cases, we obtain a faster convergence rate of $\\tau$-SGA \nthrough an asymptotic analysis which is valid when \nthe learning rate ratio $\\tau$ is big.\nWe show numerically how the insights obtained from the\nconvergence analysis may improve\nthe training of orthogonal Wasserstein GANs using \nstochastic $\\tau$-GDA and $\\tau$-SGA on simple benchmarks.",
    "keywords": [
        "min-max algorithm",
        "differential game",
        "Riemannian manifold",
        "Wasserstein GAN"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ROYSNn3vvB",
    "pdf_link": "https://openreview.net/pdf?id=ROYSNn3vvB",
    "comments": [
        {
            "summary": {
                "value": "This paper extends the analysis of the local convergence of two-time-scale $\\tau$-GDA and $\\tau$-SGA methods to the differential Stakelberg equilibrium (DSE) of Euclidean min-max problem to the Riemannian min-max problem. This paper first generalizes the notion of DSE in the Euclidean space to the Riemannian manifold (in Section 2.1), using a local coordinate chart. Then, the authors provide a sufficient condition (in Theorem 3.1) for the fixed point method to have a local linear convergence. Based on Theorem 3.1, this paper shows a specific sufficient condition for the $\\tau$-GDA to locally linearly converge to DSE in Theorem 3.2. In addition, the authors generalize $\\tau$-SGA for Riemannian min-max problem, which is originally developed to mitigate undesirable rotational dynamics encountered by $\\tau$-GDA in the Euclidean space. In the Euclidean space, extragradient (or optimistic gradient) method is widely used to relieve such rotational behavior, but its Riemannian version, named RCEG, is computationally expensive. This paper claims that their $\\tau$-SGA equipped with auto-differentiation may have computational benefit over RCEG. Experiments on a toy example and a more realistic Orthogonal WGAN problem empirically confirm this paper's theoretical findings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Existing results on local convergence to DSE and DNE in nonconvex-nonconcave min-max problems in Euclidean space are crucial. Thus, while its extension to Riemannian manifolds may seem straightforward, it is a significant and non-trivial advancement. Although I was not able to check all the details of the proof, the parts I checked looked correct."
            },
            "weaknesses": {
                "value": "Missing preliminaries for non-experts on Riemannian manifold: For example, adding the definition of Riemannian gradient/Hessian/cross-gradient, the role of local coordinate chart and the relation between $f$ and $\\bar{f}$ would help readers to better understand the context."
            },
            "questions": {
                "value": "- lines 45-47 could be improved to better explain this paper's focus on differential equilibriums (DNE and DSE). This could have been local Nash and local minimax points. Then why differential equilibrium? \n- Table 1 is confusing: What is your focus here? The title should be more specific to make a point. Isn't Nash equilibrium found by Zhang et al. (2023) DNE? This is not clear in the table. Also, your result is local (both in optimality and convergence) while others might be global. I am not sure whether local/global should be discussed in the table, but I am sure that this table needs to be reorganized and rewritten.\n- I suggest rewriting Definition 3.1 to make it more explicit that it defines local linear convergence.\n- line 276: Were you trying to say that $M$ with eigenvalues with only negative real parts?\n- lines 524-525: I don't follow what do you mean here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper analyzes the local convergence of the deterministic \u03c4-GDA and \u03c4-SGA algorithms to differentiable equilibria in min-max games on Riemannian manifolds. Using the Ostrowski theorem, sufficient conditions on the algorithm hyperparameters (in particular, on the learning ratio \u03c4) are given for linear-rate local convergence to Differentiable Stackelberg Equilibria (DSE) for both \u03c4-GDA and Asymptotic \u03c4-SGA (an approximation to \u03c4-SGA that is amenable to easier analysis). Local convergence to Differentiable Nash Equilibria (DNE) is also given for \u03c4-GDA.\n\nThis paper marks the first time that \u03c4-SGA has been written for Riemannian manifolds, and the authors put it forth as an algorithm that can solve the problem of \u03c4-GDA\u2019s slow convergence rate in certain settings. Both theory and example are given to show that Asymptotic \u03c4-SGA can be locally convergent at a smaller \u03c4 compared to \u03c4-GDA, and with a faster rate; moreover, in Euclidean space the rate guarantee outperforms that of the extra-gradient method, which has been used to overcome the slow convergence rate of \u03c4-GDA but is costly in Riemannian space. Experiments also show that the gap between Asymptotic \u03c4-SGA and the true \u03c4-SGA is small, so the theoretical results should also hold for the more natural \u03c4-SGA.\n\nFinally, the paper applies stochastic \u03c4-GDA and \u03c4-SGA to train orthogonal Wasserstein GANs with a discriminator parametrized in the Stiefel manifold. It is shown that even with good initialization a small \u03c4 may cause \u03c4-GDA to oscillate with high amplitude, but that \u03c4-SGA can converge even at those small values of \u03c4."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall, this paper seems like a significant algorithmic contribution with sound theoretical grounding. The authors extend known, tight local convergence results for \u03c4-GDA from Euclidean space to Riemannian manifolds and introduce \u03c4-SGA to the context of Riemannian manifolds. Both in theory and practice \u03c4-SGA is shown convincingly to be able to achieve (local convergence) in settings where \u03c4-GDA cannot, highlighting its usefulness in this setting of Riemannian min-max problems with minimal assumptions (just f twice continuously differentiable).\n\nThe paper is generally well written. The sections on differentiable equilibria, algorithms, and local convergence read especially well, with much background and explanation given to readers. Motivations and the context of prior work and clearly written."
            },
            "weaknesses": {
                "value": "While it is clearly stated in the introduction that global convergence is difficult to achieve in this setting and local convergence (to differentiable equilibria) may be all we can get theoretically, it is not clear what these limitations mean, or are expected to mean, in practice. The experiments are set up to show local convergence after good initialization, but it is not clear what the good initialization entails or if \u03c4-SGA can be used when you are not near a DSE. Also maybe some comment could be made about the existence and significance of DSE in the settings provided (e.g., the Wasserstein GANs), and about other methods for doing min-max Riemannian optimization in these setting (e.g., over Steifel manifolds).\n\nI also find the writing of the GAN experiments and their setups/parameters/initialization a bit terse and less clearly written than what comes before, with a few things in particular lacking explanation. I ask some questions about this below."
            },
            "questions": {
                "value": "From the experiments, it seems like \u03c4-SGA is used as a sort of \u201clast mile\u201d algorithm to guarantee fast convergence after good initialization/pretraining (either via ansatz in the analytical examples of Figure 1 or via \u03c4-GDA for the GANs). Is this a reasonable interpretation of the authors\u2019 proposed use of \u03c4-SGA, and is there any reason that \u03c4-GDA\u2014in particular, alternating \u03c4-GDA\u2014is used to pretrain the GANs?\n\nIn Theoerems 3.2 and 3.4, after the main statements there are additional (\u201cfurthermore\u2026\u201d) statements for when \u03b3 is fixed exactly depending on L_g/L_s. Is this reasonable given that L_g depends on the DSE, which is not known in advance? (Maybe this is not so important, but as it is written I am not sure what the significance of these \u201cfurthermore\u201d statements is.)\n\n\u03c4-SGA has an additional hyperparameter \u03b8 (or \u03bc) to tune compared to \u03c4-GDA, and in the GAN example of Table 2 a \u201csuitable choice\u201d of \u03b8 seems to have been made. Does the value of \u03b8 matter much, and how is it chosen?\n\nI am not so familiar with the way \u201cretraction\u201d is used in Section 3.1, though it does not affect much as the algorithms are easily understood. Perhaps some quick definition or source could be given?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Using the concepts of DSE and DNE on Riemannian manifolds, the author derives conditions on the range of \u03c4 and learning rate of x to ensure linear convergence of \u03c4-GDA to DSE and DNE.\n\nIt introduces a novel algorithm \u03c4-SGA to enhance the convergence of \u03c4-GDA, allowing a broader range of \u03c4 for convergence when \u03c4 is large, and achieving faster local convergence than \u03c4-GDA.\n\nApplying these insights, the author improves the training of orthogonal Wasserstein GANs. Numerical results show that enhanced convergence of stochastic \u03c4-GDA and \u03c4-SGA can improve the learned generator on simple benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This article provides sufficient conditions for the local convergence of \u03c4-GDA and \u03c4-SGA to differential equilibrium on a Riemannian manifold, where the author proves a linear convergence rate of \u03c4-GDA to DSE and DNE, depending on the spectral radius of the Jacobian at equilibrium. THe author introduces \u03c4-SGA on Riemannian manifolds to further improve this rate."
            },
            "weaknesses": {
                "value": "- Some key terms could benefit from further explanation or intuitive insights. For example, what are the underlying intuitions behind the linear transformations in (9) and (10)?\n\n- It appears that several theorems are extensions of those from the Euclidean case. What are the main differences or key insights that allowed these results to be successfully extended to the Riemannian manifold?\n\n- Given the author's claim of linear convergence rates for the proposed methods, are there any experimental results to support this?"
            },
            "questions": {
                "value": "Question for Theorem 3.1. (Ostrowski Theorem on manifold)\n\n- The author indicates that the theorem is analogous to the theorem in the Euclidean case. Is the Euclidean version of the theorem also based on the spectral radius of a linear transformation similar to (9)? \n\n- What is the size of the local convergence radius around ( \ud835\udc65*, \ud835\udc66*)? \n\n- How can one ensure that the initialization lies within this local neighborhood? Does the initialization used in experiments satisfy this requirement? \n\n- What is the reasoning behind assuming a small spectral radius for \ud835\udc37\ud835\udc47* ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies min-max algorithms to solve zero-sum differential games on Riemannian\nmanifold"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "No"
            },
            "weaknesses": {
                "value": "This paper does not have any interesting parts. Firstly, the Riemannian optimization is not practical. Furthermore, could you show some practical utilization of your algorithms?"
            },
            "questions": {
                "value": "I think that this manuscript contributes nothing in science expect some hard understanding of mathematical symbols."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the local convergence of simultaneous minimax algorithms to differential Stackelberg equilibrium (DSE) and differential Nash equilibrium (DNE) on Riemannian manifolds. The authors study two representative deterministic algorithms, $\\tau$-GDA and $\\tau$-SGA. For $\\tau$-GDA, they derive sufficient conditions for convergence to DSE and DNE. For $\\tau$-SGA, they analyze an asymptotic variant and demonstrate a faster convergence rate to DSE under specific conditions. Furthermore, the authors conduct numerical experiments to explore the behavior of stochastic versions of the two algorithms."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **Significance**: This paper addresses the more general nonconvex-nonconcave setting, extending previous results to Riemannian manifolds.\n2. **Comprehensiveness**: It provides a range of results by examining two types of equilibrium points, two algorithms, and extensive numerical results."
            },
            "weaknesses": {
                "value": "1. **Limited Comparative Analysis and Interpretation**: The relationship and comparative insights between the theoretical results could be strengthened. Here are some examples.\n* (i) The paper examines two equilibrium points, DSE and DNE, where DNE is a subclass of DSE with stricter definitions. Convergence to DNE would be expected to require fewer restrictions or achieve a faster rate. Theorems 3.2 and 3.3 support the former expectation, as the range of $\\tau$ in Theorem 3.3 is broader than in Theorem 3.2, yet no convergence rate is provided for Theorem 3.3.\n* (ii) The authors seem to aim to show the superiority of $\\tau$-SGA over  $\\tau$-GDA. However, by comparing Theorems 3.2 and 3.4, the range of $\\tau$ ensuring convergence for asymptotic $\\tau$-SGA exceeds that of $\\tau$-GDA only if $\\| C + \\theta BB^\\top \\| < \\| C \\|$. Further discussion on practical ways to ensure this condition or examples where it holds would be helpful. Additionally, there is no theoretical counterpart to Theorem 3.3 for $\\tau$-SGA. While $\\tau$-SGA is proposed to avoid rotational dynamics, this advantage is demonstrated only in numerical results (e.g., Figure 1), not theoretically, leaving theoretical support insufficient.\n\n2. **Unclear Analysis for (Asymptotic) $\\tau$-SGA**: \n* (i) In line 327, the asymptotic analysis requires $\\theta$ to be of constant order. Why, then, are parameters $\\mu$ and $\\tau+1$ introduced in Eqs. 12 and 13, and what is their significance?\n* (ii) If the authors aim to show the superiority of $\\tau$-SGA, Theorem 3.4 alone is insufficient, and Figure 1's simple example is not enough to establish the similarity between $\\tau$-SGA and its asymptotic variant. The claim \"Theorem 3.4 is valid for $\\tau$-SGA\" in line 363 lacks rigor. If deriving a theoretical result for $\\tau$-SGA is challenging, additional numerical experiments for the asymptotic $\\tau$-SGA$ should at least be provided."
            },
            "questions": {
                "value": "1. In line 277, what is the order of $\\gamma^{\\cdot}(M)$? Could the authors provide an intuitive explanation of this function?\n2. The computation of $\\tau$-SGA involves a matrix-vector product. Are the benefits sufficient to justify the additional computational cost?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}