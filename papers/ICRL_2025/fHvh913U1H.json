{
    "id": "fHvh913U1H",
    "title": "Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning",
    "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks. These models are typically pretrained on extensive corpora and subsequently fine-tuned on task-specific datasets. However, during the fine-tuning process, LLMs often suffer from Catastrophic Forgetting (CF), wherein previously acquired general knowledge is lost. Traditional approaches to mitigating CF often rely on data replay, which may not be viable when the original training data is inaccessible. Additionally, methods that alter the training process or the model architecture can increase complexity and detract from the accuracy of downstream tasks, thus limiting their generalizability. In this paper, we propose Forgetting-Aware Pruning Metric (FAPM), a novel pruning-based approach to balance CF and downstream task performance. Our investigation reveals that the degree to which task vectors (i.e., the subtraction of pre-trained weights from the weights fine-tuned on downstream tasks) overlap with pre-trained model parameters is a critical factor for CF. Motivated by this insight, FAPM employs the ratio of the task vector to pre-trained model parameters as a metric to quantify CF, integrating this measure into the pruning criteria. Importantly, FAPM does not necessitate modifications to the training process or model architecture, nor does it require any auxiliary data. We conducted extensive experiments across six datasets encompassing natural language inference, question answering, reading comprehension, and cloze tests. The results demonstrate that FAPM limits CF to just 1% while maintaining 99% accuracy on downstream tasks, rendering FAPM highly competitive relative to the state-of-the-art methods that involve modifications to the training process.",
    "keywords": [
        "Large Language Models",
        "Catastrophic Forgetting",
        "Neural Network Pruning"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "We propose Forgetting-Aware Pruning Metric, a novel pruning-based approach to balance Catastrophic Forgetting and downstream task performance for LLMs.",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=fHvh913U1H",
    "pdf_link": "https://openreview.net/pdf?id=fHvh913U1H",
    "comments": [
        {
            "title": {
                "value": "To Reviewer HmBx (Continue)"
            },
            "comment": {
                "value": "# Q3. While you have elucidated how to control FAPM to achieve a better balance between accuracy and forgetting levels, is there a more effective approach to directly estimate an optimal balance point rather than adjusting parameters through multiple rounds of performance feedback?\n\nFAPM does not require multiple rounds of performance feedback to achieve a better balance between accuracy and forgetting levels. Instead, it directly controls the balance by calculating the mean of the absolute values of the pre-trained parameter matrix, as shown in Equation 1 by $| W_{pre}^{i} |$. For more details, refer to the textual description of Equation 1 (lines 257 - 272). If the \"multiple rounds of performance feedback\" you mentioned refer to the *for* loop in our pseudocode or the $i$ in Equation 1, this actually represents the application of our method to each module in the parameter matrix separately (or can be understood as layer-wise operations), rather than adjusting parameters through multiple rounds of performance feedback."
            }
        },
        {
            "title": {
                "value": "To Reviewer HmBx (Continue)"
            },
            "comment": {
                "value": "Thank you for Reviewer HmBx's review.\n\n# Q2. This appears to be a unique model merging method; however, why is there no comparison made with other methods of a similar nature?\n\nReviewer aVtG also mentioned this issue, stating, \"This paper overlooks an important baseline\u2014Wise-FT [1], a standard model parameter merging method,\" and provided a baseline for model merging for comparison. The original Wise-FT paper notes: \"WiSE-FT using a fixed mixing coefficient \u03b1=0.5 with the fixed optimal mixing coefficient.\" Therefore, we followed the hyperparameter settings from the original paper. The experimental results are presented in the following two tables.\n\n| Tasks      | Methods            | C-Eval  | GSM8K  | MMLU   | HumanEval | Avg.   | Performance |\n|:----------:|:------------------:|:-------:|:------:|:------:|:---------:|:------:|:-----------:|\n| RTE        | Pre-trained model  | 0.4386  | 0.7922 | 0.6594 | 0.5914    | 0.6204 | 0.819       |\n| RTE        | Full SFT           | 0.2311  | 0.075  | 0.2554 | 0.0       | 0.1403 | 0.890       |\n| RTE        | WiSE-FT            | 0.3046  | 0.4420 | 0.4255 | 0.5609    | 0.4332 | 0.889       |\n| RTE        | FAPM(Ours)         | 0.4623  | 0.7915 | 0.6454 | 0.5975    | 0.6242 | 0.897       |\n|||||||||\n| WikiQA     | Pre-trained model  | 0.4386  | 0.7922 | 0.6594 | 0.5914    | 0.6204 | 0.913       |\n| WikiQA     | Full SFT           | 0.2547  | 0.0    | 0.2422 | 0.0       | 0.1242 | 0.966       |\n| WikiQA     | WiSE-FT            | 0.2581  | 0.0    | 0.2458 | 0.0       | 0.1259 | 0.958       |\n| WikiQA     | FAPM(Ours)         | 0.4749  | 0.7975 | 0.6563 | 0.5853    | 0.6285 | 0.964       |\n|||||||||\n| Winogrande | Pre-trained model  | 0.4386  | 0.7922 | 0.6594 | 0.5914    | 0.6204 | 0.519       |\n| Winogrande | Full SFT           | 0.2792  | 0.0606 | 0.3438 | 0.0       | 0.1709 | 0.820       |\n| Winogrande | WiSE-FT            | 0.4741  | 0.4412 | 0.5967 | 0.5060    | 0.5045 | 0.830       |\n| Winogrande | FAPM(Ours)         | 0.4829  | 0.7680 | 0.6472 | 0.5731    | 0.6178 | 0.824       |\n|||||||||\n| SQuAD      | Pre-trained model  | 0.4386  | 0.7922 | 0.6594 | 0.5914    | 0.6204 | 0.371       |\n| SQuAD      | Full SFT           | 0.2806  | 0.0212 | 0.3206 | 0.0       | 0.1556 | 0.646       |\n| SQuAD      | WiSE-FT            | 0.4309  | 0.4009 | 0.5484 | 0.5102    | 0.4725 | 0.639       |\n| SQuAD      | FAPM(Ours)         | 0.4738  | 0.7310 | 0.6455 | 0.5748    | 0.6063 | 0.637       |\n\nTable 1: Comparison of FAPM and WiSE-FT on the Llama3-8B model. The 'Avg.' column represents the average performance on general tasks, while the 'Performance' column indicates the performance on downstream tasks. \n\n| Tasks      | Methods            | C-Eval  | GSM8K   | MMLU   | HumanEval | Avg.   | Performance |\n|:----------:|:------------------:|:-------:|:-------:|:------:|:---------:|:------:|:-----------:|\n| RTE        | Pre-trained model  | 0.7478  | 0.8180  | 0.6884 | 0.7682    | 0.7556 | 0.574       |\n| RTE        | Full SFT           | 0.2602  | 0.075   | 0.2423 | 0.0       | 0.1443 | 0.890       |\n| RTE        | WiSE-FT            | 0.7488  | 0.7664  | 0.4638 | 0.7682    | 0.6868 | 0.888       |\n| RTE        | FAPM(Ours)         | 0.7568  | 0.8104  | 0.6857 | 0.7500    | 0.7507 | 0.903       |\n|||||||||\n| WikiQA     | Pre-trained model  | 0.7478  | 0.8180  | 0.6884 | 0.7682    | 0.7556 | 0.896       |\n| WikiQA     | Full SFT           | 0.2510  | 0.076   | 0.2434 | 0.0       | 0.1426 | 0.965       |\n| WikiQA     | WiSE-FT            | 0.2476  | 0.0     | 0.2414 | 0.0       | 0.1222 | 0.963       |\n| WikiQA     | FAPM(Ours)         | 0.7555  | 0.8036  | 0.6902 | 0.7621    | 0.7529 | 0.962       |\n|||||||||\n| Winogrande | Pre-trained model  | 0.7478  | 0.8180  | 0.6884 | 0.7682    | 0.7556 | 0.558       |\n| Winogrande | Full SFT           | 0.4090  | 0.0303  | 0.2996 | 0.0609    | 0.1999 | 0.790       |\n| Winogrande | WiSE-FT            | 0.6747  | 0.5343  | 0.5821 | 0.5914    | 0.5956 | 0.780       |\n| Winogrande | FAPM(Ours)         | 0.7618  | 0.8068  | 0.6845 | 0.7395    | 0.7482 | 0.785       |\n|||||||||\n| SQuAD      | Pre-trained model  | 0.7478  | 0.8180  | 0.6884 | 0.7682    | 0.7556 | 0.451       |\n| SQuAD      | Full SFT           | 0.3531  | 0.02122 | 0.3183 | 0.0       | 0.1731 | 0.624       |\n| SQuAD      | WiSE-FT            | 0.6784  | 0.5743  | 0.5868 | 0.5524    | 0.5979 | 0.622       |\n| SQuAD      | FAPM(Ours)         | 0.7410  | 0.8006  | 0.6752 | 0.7500    | 0.7417 | 0.615       |\n\nTable 2: Comparison of FAPM and WiSE-FT on the Qwen2-7B.\n\nFrom the two tables above, we can conclude that WiSE-FT achieves comparable downstream task accuracy to FAPM. However, WiSE-FT still suffers from significant catastrophic forgetting."
            }
        },
        {
            "title": {
                "value": "To Reviewer HmBx"
            },
            "comment": {
                "value": "Thank you for Reviewer HmBx's review.\n\n# Response to Weaknesses\n\nThe four issues raised by reviewer HmBx in the weakness section primarily concern the generalizability of our proposed method or its potential limitations when applied to more specialized scenarios. To better demonstrate the generalizability of our method, we introduced two more specialized scenarios: a medical scenario and a mathematical scenario. For medical data, we selected the MedMCQA dataset, and for mathematical data, we selected the MetaMathQA dataset. We validated the effectiveness of our method in these two scenarios using Llama3-8B and Qwen2-7B. The experimental results are presented in the following two tables. Notably, even when catastrophic forgetting occurs during LoRA fine-tuning, our method remains effective. The detailed results can be found in our response to reviewer YFJm.\n\n| Tasks      | Methods            | C-Eval  | GSM8K  | MMLU   | HumanEval | Avg.   | Performance |\n|:----------:|:------------------:|:-------:|:------:|:------:|:---------:|:------:|:-----------:|\n| MedMCQA    | Pre-trained model  | 0.4386  | 0.7922 | 0.6594 | 0.5914    | 0.6204 | 0.5169      |\n| MedMCQA    | Full SFT           | 0.3315  | 0.4897 | 0.5365 | 0.4434    | 0.4502 | 0.5862      |\n| MedMCQA    | FAPM(Ours)         | 0.4586  | 0.7733 | 0.6638 | 0.5731    | 0.6172 | 0.5935      |\n|||||||||\n| MetaMathQA | Pre-trained model  | 0.4386  |      - | 0.6594 | 0.5914    | 0.5631 | 0.7922      |\n| MetaMathQA | Full SFT           | 0.3101  |      - | 0.4411 | 0.1890    | 0.3133 | 0.8178      |\n| MetaMathQA | FAPM(Ours)         | 0.4697  |      _ | 0.6447 | 0.5729    | 0.5624 | 0.8121      |\n\nTable 1: The results of FAPM on various datasets using Llama3-8B. The 'Avg.' column represents the average performance on general tasks, while the 'Performance' column indicates the performance on downstream tasks. Since the downstream task is a mathematical task, we used gsm8k for testing. Therefore, when calculating the performance of general tasks, we did not include gsm8k, which is indicated by a '-'.\n\n\n| Tasks      | Methods            | C-Eval  | GSM8K  | MMLU   | HumanEval | Avg.   | Performance |\n|:----------:|:------------------:|:-------:|:------:|:------:|:---------:|:------:|:-----------:|\n| MedMCQA    | Pre-trained model  | 0.7478  | 0.8180 | 0.6884 | 0.7682    | 0.7556 | 0.5208      |\n| MedMCQA    | Full SFT           | 0.5662  | 0.3731 | 0.5001 | 0.6443    | 0.5209 | 0.5811      |\n| MedMCQA    | FAPM(Ours)         | 0.7613  | 0.7968 | 0.6732 | 0.7445    | 0.7439 | 0.5713      |\n|||||||||\n| MetaMathQA | Pre-trained model  | 0.7478  |      - | 0.6884 | 0.7682    | 0.7347 | 0.8180      |\n| MetaMathQA | Full SFT           | 0.5582  |      - | 0.4319 | 0.3242    | 0.4381 | 0.8499      |\n| MetaMathQA | FAPM(Ours)         | 0.7468  |      - | 0.6857 | 0.7511    | 0.7278 | 0.8484      |\n\nTable 2: The results of FAPM on various datasets using Qwen2-7B.\n\nFrom the table above, we can see that our method still effectively alleviates forgetting to a mere 1% while maintaining an impressive 99% accuracy on downstream tasks in both the mathematical and medical task scenarios."
            }
        },
        {
            "title": {
                "value": "To Reviewer YFJm"
            },
            "comment": {
                "value": "Thank you for Reviewer YFJm's review.\n\n# Q1. A small nitpick. It would be really great if the captions of the images and tables could be a bit longer and more informative.\n\nWe have revised the paper according to the reviewers' comments, with the following changes: \n1. We have provided an explanation of the experimental setup for Figures 2 and 3. \n 2. We have improved the caption for Figure 4 by adding an example explanation. \n\nThe revised paper has been re-uploaded.\n\n# Q2. Nowadays most people prefer to use LoRA not because it reduces CF, but instead its pretty cheap. In my experience even LoRA suffers from CF if the finetuning domain is pretty niche. So I was wondering how informative are task vectors from LoRA merged models and how will this method perform if we use LoRA to fine-tune.\n\nIn our paper, we define task vectors as the subtraction of pre-trained weights from the weights fine-tuned on downstream tasks ($W_{ft} - W_{pre}$). In LoRA fine-tuning, the initialization of LoRA weights is not based on the pre-trained model weights. Therefore, the concept of task vectors as we defined cannot be applied when using LoRA for fine-tuning.\n\nHowever, through our consideration, we believe that with a simple modification, LoRA fine-tuning can still utilize our method. After training, we can merge the LoRA parameters with the pre-trained model parameters. The merging formula is $W_{new} = W_{pre} + W_{loraB}W_{loraA}$, where $W_{loraB}$ and $W_{loraA}$ are the LoRA matrices corresponding to a particular parameter matrix of the pre-trained model. Through the above formula, we observe that when merging the LoRA parameters with the pre-trained model parameters, it involves the dot product of the $W_{loraB}$ and $W_{loraA}$ matrices and then addition to the pre-trained model weights. Therefore, we can treat $W_{loraB}W_{loraA}$ as the task vectors in our algorithm for pruning. That is, $\\Delta W$ in Equation 1 of the paper is $W_{loraB}W_{loraA}$.\n\nTo validate the effectiveness of our approach, we conducted experiments on the SQuAD and MedQA datasets. MedQA is a newly added medical dataset. The model used is Llama3-8B. The experimental results are shown in the table below.\n\n| dataset | model    | ceval         | gsm8k         | mmlu          | humaneval     | Avg. | Performance |\n|---------------|---------------|---------------|---------------|---------------|------| ----- | ---- |\n| SQuAD| Pre-trained model | 0.4386        | 0.7922        | 0.6594        | 0.5914        | 0.6204 | 0.371|\n| SQuAD| LoRA          | 0.4795        | 0.7255        | 0.5914        | 0.5853        | 0.5954 | 0.648 |\n| SQuAD| FAPM (LoRA/0.9) | 0.4435        | 0.8013        | 0.6594        | 0.5853        | 0.6223 | 0.456 |\n| SQuAD| FAPM (LoRA/0.1) | 0.4502        | 0.7862        | 0.6552        | 0.5914        | 0.6207 | 0.616 |\n|SQuAD | FAPM (LoRA/0.05) | 0.4579        | 0.7793        | 0.6544        | 0.5886        | 0.6200 | 0.636 |\n| SQuAD| FAPM (LoRA/0.03) | 0.4591        | 0.7801        | 0.6511        | 0.5914        | 0.6204 | 0.644 |\n|||||||||\n| MedQA| Pre-trained model   | 0.4386        | 0.7922        | 0.6594        | 0.5914        | 0.6204 | 0.552 |\n| MedQA| LoRA                | 0.4216        | 0.7089        | 0.5889        | 0.5409        | 0.5650 | 0.640 |\n| MedQA| FAPM (LoRA/0.9)     | 0.4544        | 0.7899        | 0.6590        | 0.5914        | 0.6236 | 0.588 |\n| MedQA| FAPM (LoRA/0.1)     | 0.4540        | 0.7807        | 0.6439        | 0.5875        | 0.6165 | 0.623 |\n| MedQA| FAPM (LoRA/0.05)    | 0.4531        | 0.7845        | 0.6475        | 0.5914        | 0.6191 | 0.629 |\n| MedQA| FAPM (LoRA/0.03)    | 0.4592        | 0.7807        | 0.6484        | 0.5731        | 0.6154 | 0.632 |\n\nTable 1: Results of applying our method FAPM to LoRA fine-tuning on the SQuAD and MedQA dataset. The numbers after '/' indicate the pruning ratio. The 'Avg.' column represents the performance on general tasks, while 'Performance' indicates the performance on downstream tasks.\n\nFrom the table, we can see that LoRA fine-tuning exhibits some forgetting phenomena on both datasets, with the forgetting being more severe on the MedQA dataset. FAPM effectively alleviates forgetting to a mere 1% while maintaining an impressive 99% accuracy on downstream tasks in both datasets. Further experiments revealed that the pruning ratio in LoRA fine-tuning needs to be relatively small to avoid significant performance degradation in downstream tasks. We speculate that this is mainly because the LoRA matrices $W_{loraB}W_{loraA}$ are low-rank, and thus contain relatively few redundant parameters."
            }
        },
        {
            "title": {
                "value": "To Reviewer 53VB (Continue)"
            },
            "comment": {
                "value": "Thank you for Reviewer 53VB's review.\n\n#  Q3: Why not use the relative portion of the pruning metric alone? It\u2019s a complicated scoring metric, please ablate it.\n\nIn the paper, we presented comparative experiments with the pruning method that uses only \u201cabsolute change magnitude\u201d (refer to Tables 3 and 4 in the paper). Here, we supplement the results of ablation experiments that use only \u201crelative change magnitude\u201d (only RCM) for pruning. We conducted the experiments on Llama3-8B, with the pruning ratio consistent with that in the paper, which is 90%.\n\n\n| Tasks| Methods         | ceval   | gsm8k  | mmlu   | humaneval | Avg.   | Performance |\n|-----|---------|---------|--------|--------|-----------|--------|-------------|\n|RTE | Pre-trained model | 0.4386  | 0.7922 | 0.6594 | 0.5914    | 0.6204 | 0.819       |\n|RTE | Full SFT          | 0.2311  | 0.075  | 0.2554 | 0.0       | 0.1403 | 0.890       |\n|RTE |only RCM          | 0.4391  | 0.7915 | 0.6620 | 0.6097    | 0.6255 | 0.823       |\n|RTE |FAPM              | 0.4623  | 0.7915 | 0.6454 | 0.5975    | 0.6242 | 0.897       |\n|WikiQA | Pre-trained model | 0.4386  | 0.7922 | 0.6594 | 0.5914    | 0.6204 | 0.913       |\n|WikiQA |Full SFT          | 0.2547  | 0.0    | 0.2422 | 0.0       | 0.1242 | 0.966       |\n|WikiQA |only RCM          | 0.4385  | 0.7892 | 0.6589 | 0.6097    | 0.6240 | 0.947       |\n|WikiQA |FAPM              | 0.4749  | 0.7975 | 0.6563 | 0.5853    | 0.6285 | 0.964       |\n|Winogrande | Pre-trained model | 0.4386  | 0.7922 | 0.6594 | 0.5914    | 0.6204 | 0.519       |\n|Winogrande |Full SFT          | 0.2792  | 0.0606 | 0.3438 | 0.0       | 0.1709 | 0.820       |\n|Winogrande |only RCM          | 0.4369  | 0.7899 | 0.6597 | 0.5975    | 0.6209 | 0.551       |\n|Winogrande |FAPM              | 0.4829  | 0.7680 | 0.6472 | 0.5731    | 0.6178 | 0.824       |\n|SQuAD | Pre-trained model | 0.4386  | 0.7922 | 0.6594 | 0.5914    | 0.6204 | 0.371       |\n|SQuAD |Full SFT          | 0.2806  | 0.0212 | 0.3206 | 0.0       | 0.1556 | 0.646       |\n|SQuAD |only RCM          | 0.4449  | 0.7915 | 0.6582 | 0.5975    | 0.6230 | 0.374       |\n|SQuAD |FAPM              | 0.4738  | 0.7310 | 0.6455 | 0.5748    | 0.6063 | 0.637       |\n\nFrom the table, it can be seen that using only the relative change magnitude cannot ensure the performance on downstream tasks; the model performance is comparable to that of the pre-trained model. The reason is that pruning using the \u201crelative change magnitude\u201d is designed solely to address catastrophic forgetting. Using only the relative change magnitude results in retaining parameter values that are relatively small in the vector during pruning, which cannot guarantee the performance on downstream tasks."
            }
        },
        {
            "title": {
                "value": "To Reviewer 53VB"
            },
            "comment": {
                "value": "Thank you for Reviewer 53VB's review.\n\n\n# Q1: This paper proposes a complicated pruning metrics that only shows marginal improvement over prior art.\n\n1. We cannot agree with your comment of \"a complicated pruning metrics.\" This is because our method is sufficiently simple. FAPM only requires pruning the incremental model after fine-tuning, without necessitating any changes to the model training process, and our pruning procedure is also simple enough. To visually demonstrate the simplicity of FAPM, we have provided the PyTorch implementation code for the FAPM pruning metrics below. The formula (1) in the paper can be implemented in just a few lines of code.\n\n```\ndef FAPM(W_pre, W_tv, key):\n    \"\"\"\n    Computes the FAPM for a specific parameter in a model.\n\n    Parameters:\n    -----------\n    W_pre : OrderedDict\n        The model state_dict of the pre-trained model.\n    \n    W_tv : OrderedDict\n        The model state_dict of the task vectors,\n    \n    key : str\n        The identifier to specify which parameter matrix to use from both `W_pre` and `W_tv`.\n\n    Returns:\n    --------\n    score : torch.Tensor\n        The importance score (FAPM) for the specified parameter. \n    \"\"\"\n\n    # Compute the absolute value of the task vector for the specified parameter\n    abs_W_tv = W_tv[key].abs()\n\n    # Compute the mean of the absolute values of the pre-trained weights for the specified parameter\n    mean_abs_W_pre = W_pre[key].abs().mean()\n\n    # Compute the normalized absolute task vector\n    normalized_abs_W_tv = abs_W_tv / W_pre[key].abs()\n\n    # Compute the importance score (FAPM)\n    score = abs_W_tv - mean_abs_W_pre * normalized_abs_W_tv\n\n    return score\n```\n\n2. The improvements brought by our method are significant. FAPM effectively alleviates forgetting to a mere 1% while maintaining an impressive 99% accuracy on downstream tasks. Most importantly, compared to previous state-of-the-art methods, our greatest advantage lies in the simplicity of our approach. It does not require any changes to the model training process, does not necessitate adding extra model structures, and does not introduce additional data. Moreover, our method achieves results that are better than or comparable to the previous state-of-the-art methods (refer to Table 1 and Table 2 in the paper).\n\n3. The simplicity and effectiveness of our method have been fully recognized by other reviewers, e.g., *\"The simplicity of the technique introduced by the authors is really great\"* commented  by Reviewer YFJm and *\"FAPM doesn\u2019t complicate the model architecture or training process, making it easier to implement\"* commented by Reviewer HmBx. Therefore, we believe that Reviewer 53VB's statement regarding our method as \"a complicated pruning metric that only shows marginal improvement over prior art\" is not appropriate. We cannot agree with this assessment.\n\n# Q2: The quality of write-up is generally poor. For example, section 3.1 is entirely unnecessary and can be in the appendix without affecting the flow of the paper.\n\nWe do not believe that Section 3.1 is unnecessary because this section presents our analysis of the causes of the CF problem, elucidates our perspective on the issue, and introduces the starting point of our method. We believe these analyses are necessary and insightful. As mentioned by Reviewer YFJm, *\"I loved reading section 3.1 wherein they provide analysis and a clear thinking process of how they arrived at the method\"*, this section helps readers to better understand our motivation."
            }
        },
        {
            "summary": {
                "value": "This paper tries to tackle the problem of whether catastrophic forgetting (CF) due to finetuning be avoided without changing training process, without any additional data, and without altering model structure. To this end the authors come up with Forgetting-Aware Pruning Metric (FAPM) wherein the task vectors are not only pruned based on magnitude but also uses the ratio of task vector to pretrained parameters to avoid CF. The authors provide extensive experiments across various tasks and latest LLMs like Qwen and Llama3 models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The simplicity of the technique introduced by the authors is really great. Also I loved reading section 3.1 wherein they provide analysis and a clear thinking process of how they arrived at the method.\n\n2. There are extensive experiments in the paper across various tasks and two models. The models chose are the latest ones which makes this paper a bit more relevant. Section 5 in general is pretty enjoyable to read.\n\n3. Although the method is not pareto optimal but the numbers are consistently high on all the tasks across both Llama3 and Qwen2."
            },
            "weaknesses": {
                "value": "1. A small nitpick. It would be really great if the captions of the images and tables could be a bit longer and more informative."
            },
            "questions": {
                "value": "1. Nowadays most people prefer to use LoRA not because it reduces CF, but instead its pretty cheap. In my experience even LoRA suffers from CF if the finetuning domain is pretty niche. So I was wondering how informative are task vectors from LoRA merged models and how will this method perform if we use LoRA to finetune."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors introduce Forgetting-Aware Pruning Metric (FAPM), a new pruning-based method aimed at striking a balance between reducing forgetting and maintaining downstream task performance. They find that the overlap between task vectors \u2013 which represent the difference between pre-trained weights and those fine-tuned on new tasks \u2013 and pre-trained model parameters is a major factor in forgetting. FAPM uses this overlap ratio as a metric for assessing forgetting and incorporates it into pruning decisions. A key advantage is that FAPM doesn't require changes to the training process or the model architecture, nor does it need any additional data. The authors conducted experiments across six datasets covering various tasks, and the results show that FAPM keeps forgetting to just 1% while achieving 99% accuracy on downstream tasks, making it very competitive against other state-of-the-art methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. FAPM effectively limits forgetting to a mere 1%, which is a significant improvement.\n2. The method maintains an impressive 99% accuracy in downstream tasks, showing that it preserves performance.\n3. Unlike some existing methods, FAPM doesn\u2019t complicate the model architecture or training process, making it easier to implement.\n4. This approach doesn\u2019t rely on replaying original training data or using additional datasets, making it more practical in various scenarios."
            },
            "weaknesses": {
                "value": "1. The reliance on the overlap between task vectors and pre-trained weights might not generalize across all types of tasks or models.\n2. While pruning is effective, it may not address all aspects of learning or forgetting, so there could be other methods worth exploring along with FAPM.\n3. While the experiments are comprehensive, testing on even more diverse datasets could strengthen the generalizability of the findings.\n4. The paper doesn\u2019t discuss how FAPM performs in highly specialized or unusual task scenarios, which might reveal limitations."
            },
            "questions": {
                "value": "1. This appears to be a unique model merging method; however, why is there no comparison made with other methods of a similar nature?\n2. While you have elucidated how to control FAPM to achieve a better balance between accuracy and forgetting levels, is there a more effective approach to directly estimate an optimal balance point rather than adjusting parameters through multiple rounds of performance feedback?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors identified pruning as a way to mitigate catastrophic forgetting. Specifically, the authors propose to prune weight updates due to fine-tuning, using a combination of pruning metrics including absolute magnitude of change in weights and relative magnitude of change in weights relative to the magnitude of pre-trained weights."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Using pruning to mitigate catastrophic forgetting during fine-tuning is novel.\n- This paper contains some original insights about the generality/specificity trade-off when fine-tuning LLMs."
            },
            "weaknesses": {
                "value": "- This paper proposes a complicated pruning metrics that only shows marginal improvement over prior art.\n- The quality of write-up is generally poor. For example, section 3.1 is entirely unnecessary and can be in the appendix without affecting the flow of the paper.\n-  No ablation study. Why not use the relative portion of the pruning metric alone? It\u2019s a complicated scoring metric, please ablate it."
            },
            "questions": {
                "value": "- See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on addressing the issue of catastrophic forgetting (CF) in large language models (LLMs) during fine-tuning, i.e., striking a balance between speciality and generality.\nIt proposes Forgetting-Aware Pruning Metric (FAPM): A novel method that integrates the ratio of task vector magnitude to pre-trained model parameters into the pruning criteria, allowing for the mitigation of CF without modifying the model architecture or training process.\nFAPM successfully mitigates forgetting while retaining 99% of downstream task accuracy across various natural language tasks.\nThe experiments show that FAPM performs competitively against state-of-the-art methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.This paper introduces two key concepts through preliminary experiments\u2014\u201cabsolute change magnitude\u201d and \u201crelative change magnitude\u201d\u2014which are designed to balance the trade-off between task specialization and generality.\n\n2.Building on these concepts, the paper proposes the Forgetting-Aware Pruning Metric (FAPM), aimed at deriving an optimal task vector to obtain the final model weights.\n\n3.Extensive experiments demonstrate the effectiveness of FAPM, showing significant improvements over baseline methods."
            },
            "weaknesses": {
                "value": "1.This paper overlooks an important baseline\u2014Wise-FT [1], a standard model parameter merging method.\n\n2.The experimental setup for the preliminary experiments, particularly those in Sections 2 and 3, is not clearly detailed, e.g., the experimental model, raising concerns about whether similar observations would hold for other models.\n\n3.While instruction-following is a critical aspect of generality, this paper focuses solely on world knowledge and generic reasoning, omitting this aspect.\n\n4.The rationale and motivation for striking a balance between speciality and generality is not clearly articulated in the paper.\n\n[1] Mitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. 2021. Robust fine-tuning of zero-shot models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7949\u20137961."
            },
            "questions": {
                "value": "1.The limitation mentioned in lines 81-83, stating that \"methods that alter the training process or model architecture not only make the training process more difficult to control but also degrade the accuracy of downstream tasks,\" lacks references to prior work supporting this claim.\n\n2.The figures in this paper appear not to be in vector graphic format, which may affect the clarity and quality of the visualizations.\n\n3.The quotation marks in line 212 are not properly formatted according to standard usage."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}