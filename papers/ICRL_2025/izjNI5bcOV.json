{
    "id": "izjNI5bcOV",
    "title": "WeatherGFM: Learning a Weather Generalist Foundation Model via In-context Learning",
    "abstract": "The Earth's weather system involves intricate weather data modalities and diverse weather understanding tasks, which hold significant value to human life. \nExisting data-driven models focus on single weather understanding tasks (e.g., weather forecasting). \nWhile these models have achieved promising results, they fail to tackle various complex tasks within a single and unified model. \nMoreover, the paradigm that relies on limited real observations for a single scenario hinders the model's performance upper bound.\nInspired by the in-context learning paradigm from visual foundation models and large language models, in this paper, we introduce the first generalist weather generalist foundation model (WeatherGFM) to address weather understanding tasks in a unified manner. \nSpecifically, we first unify the representation and definition for diverse weather understanding tasks.\nSubsequently, we design weather prompt formats to handle different weather data modalities, including single, multiple, and temporal modalities. \nFinally, we adopt a visual prompting question-answering paradigm for the training of unified weather understanding tasks. \nExtensive experiments indicate that our WeatherGFM can effectively handle up to ten weather understanding tasks, including weather forecasting, super-resolution, weather image translation, and post-processing. Our method also showcases generalization ability on unseen tasks.",
    "keywords": [
        "AI for Science",
        "Weather foundation model",
        "in-context learning"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=izjNI5bcOV",
    "pdf_link": "https://openreview.net/pdf?id=izjNI5bcOV",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces WeatherGFM, a weather foundation model inspired by in-context learning approaches used in visual and language models. WeatherGFM aims to address multiple weather-related tasks within a unified framework by converting diverse tasks with various data types into general weather prompts. The model employs a masked modeling approach for training, while fully masking the targets during inference. Experimental results on SEVIR dataset show that WeatherGFM can handle up to ten different weather tasks, including forecasting, super-resolution, image translation, and post-processing. The model also demonstrates its generalization capabilities on unseen tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The proposed weather prompt design effectively handles a wide range of weather-related tasks.\n- By utilizing an in-context learning approach, WeatherGFM can generalize to unseen tasks without requiring fine-tuning.\n- WeatherGFM demonstrates its scalability across different model sizes.\n- The model successfully handles ten various tasks on the SEVIR dataset using a single, unified framework."
            },
            "weaknesses": {
                "value": "- In Section 3.2, it appears that WeatherGFM uses task-specific patch embedding layers, which may limit the framework's generalization ability. For instance, with the current design, WeatherGFM may struggle to generalize to new, unseen tasks that require novel sensor channels.\n- The impact of increasing dataset size is unclear in Figure 7. Here, the authors compare ViT-ST with WeatherGFM-Base to illustrate the scalability of WeatherGFM. However, since ViT-ST is trained on a single task while WeatherGFM is trained on multiple tasks with more data and parameters, it\u2019s unclear whether the performance gains are due to dataset scalability or multi-task training or larger number of parameters. Additionally, on radar spatial super-resolution (SR), scaling the dataset size or using multi-task training appears to degrade performance. A more detailed analysis and discussion on the effects of dataset scalability would be valuable.\n- It would be beneficial to compare WeatherGFM\u2019s performance against other climate foundation models (e.g., Aurora, ClimaX) using the ERA5 dataset. One of WeatherGFM\u2019s key strengths is its ability to unify multi-task, multi-modal datasets. Thus, similar to Aurora, pretraining WeatherGFM on a combination of ERA5 and other heterogeneous datasets could potentially enhance its performance. It would be impressive if WeatherGFM could achieve comparable results to ClimaX or Aurora without fine-tuning, especially by leveraging a larger dataset and multi-task training."
            },
            "questions": {
                "value": "- Why does WeatherGFM show weaker performance in weather image translation compared to other tasks?\n- In Table 1, there is a misuse of double lines for \"GOES2Radar, CSI/219.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presented a weather generalist foundation model based on in-context learning, which unified a wide range of weather understanding tasks, like weather forecasting, wealth image translation, wealth super-resolution, deblur, etc.\nIt first defines the visual prompt template and then does in-context learning to achieve such a unified model.\nThe experiments demonstrate that this unified model achieved comparable results with specialist models, e.g., ViT and Unet."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. A good contribution of this work is to apply in-context learning to weather-understanding tasks for building a unified model and confirming its effectiveness.\n2. The proposed model demonstrates comparable results with specialist models on ten weather understanding tasks,\n3. Mixed-modal mask modeling is interesting."
            },
            "weaknesses": {
                "value": "1. The motivation of the training objective in Figure 3 is unclear. The eq.5 is mismatched with Figure 3. There should be more details about the training objective. X_T in eq.5 lacks a definition for your universal representation.\n2. Details missing. For example, the patch embedding layer is task-specific. Does this mean that there is a hard code to select which patch embedding layer is used to handle different tasks or data modalities? If so, can this model be claimed as a unified model?\n3. The motivation of such a unified model is actually unclear to real-world applications. There is no obvious improvement over the conventional specialist model. In addition, there are not many combinations of tasks here. What is the essential advance of the proposed model compared with per-task specialists?"
            },
            "questions": {
                "value": "What is the motivation for designing Mixed-modal mask modeling? It seems like there should be some references that motivate this design."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a generalist foundation model for weather tasks that includes in-context learning that can handle multiple weather tasks."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**(S1)**: The problem of creating a more general weather foundation model is important, and it is valuable that the authors are investigating this area. \n\n**(S2)**: Demonstrating in-context learning is also useful for weather foundation models, although I admit I haven\u2019t looked too closely into whether there exists prior work that tackles this."
            },
            "weaknesses": {
                "value": "**(W1)**: Experimental comparisons are missing. The authors don\u2019t seem to compare WeatherGFM on the same tasks used in the Climax or Aurora papers, and it is unclear why this comparison isn\u2019t made. The main claim is that WeatherGFM is a more general foundation model, and yet the only comparison is made with simple task-specific baselines such as a ViT or UNet. There needs to be compelling evidence that WeatherGFM can outperform/compete with Climax or Aurora. Moreover, only 3 qualitative examples on out-of-distribution (OOD) examples are given in Figure 6. Quantitative evaluations on OOD tasks are quite important for a foundation model.\n\n**(W2)**: Experimental results are weak. For the image translation tasks in Table 2, it is unclear that WeatherGFM can outperform baselines on these datasets. The authors do mention that their goal is not to achieve SoTA performance on each task\u2014 this is fine, but since comparison with other SoTA methods is missing, the results on the tasks described are not compelling enough. Moreover, in weather forecasting and super-resolution, the performance of the ViT baseline is nearly the same as WeatherGFM, which suggests only a minute (if any) improvement with the author\u2019s designed modifications and pre-training strategy. Moreover, details on how these baseline ViT or UNets were trained are lacking, which are very crucial to determine if the experiments are fair.\n\n**(W3)**: Lack of significant novelty. While the authors do propose changes to the ViT pre-training setup used in other Weather foundation model setups, these are not significant. The authors\u2019 claims of supporting multi-modality isn\u2019t well supported considering other foundation models already do support inputs from different weather sensors and earth observation (EO) systems. The only difference here seems to be satellite imagery which I don\u2019t think is too different from the setup of the other foundation models, and the authors don\u2019t provide any evidence that the techniques of Climax or Aurora wouldn\u2019t also extend to this additional input. Masked image modelling for EO data and designing patch embeddings for different sensory inputs is an established technique, eg: [1].\n\n**(W4)**: Lack of presentation clarity. I think this paper is tackling an important problem, but does not clearly motivate its solution. \n* For example, figure 3 is quite unclear and it is very hard to understand what exactly is going on (in terms of the pre-training strategy or the architectural modifications). The masked/predicted/ground-truth patch disambiguation is not clear to me. It seems that this figure contains multiple subfigures that should clearly be disambiguated and explained.\n* The actual pre-training method is not described extensively. Specifically, lines 236-253 need to be clearly explained and expanded, since this seems to be the crux of the paper\u2019s modifications. Instead, too much space is given to explaining the architecture of a ViT (lines 254-280), which is not required in the main text of the paper.\n* Qualitative outputs from the model are not compared with qualitative outputs from other baselines. The authors demonstrate the outputs of WeatherGFM but don\u2019t compare these on tasks with other weather foundation models. This is related to (W1).\n* Critical details on training and inference are missing. What is the patch size? What kind of compute was used for training? How much compute? Was the dataset combined? Any dataset augmentation? etc. etc. etc. these details are important for reproducibility but are missing from the paper and appendix.\n\nOverall, given the weaknesses describe above, I don't think this paper is ready for acceptance in this conference. \n\nReferences:  \n[1] SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery, NeurIPS 2022."
            },
            "questions": {
                "value": "See Weaknesses. \n\nAre there ablations on the length of the input prompt sequence and how that affects performance?\nKeeping text bold in the introduction paragraphs hampers readability.\nLine 159-180: Define what t is and what its range is"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents WeatherGFM, a pioneering general-purpose weather foundation model that unifies diverse weather-related tasks through contextual learning. By integrating different weather data formats, WeatherGFM is trained with visual prompts to handle a wide range of tasks, including forecasting, super-resolution, image translation, and post-processing. The model is designed to overcome the limitations of task-specific models by leveraging a single architecture capable of understanding and adapting to varying weather data patterns. Extensive experiments show that WeatherGFM achieves state-of-the-art performance across multiple tasks and exhibits impressive generalization on unseen tasks, demonstrating its potential as a robust, adaptable solution for complex weather modeling challenges."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe proposed WeatherGFM can handle different weather data modalities and task objectives in a unified manner\n2.\tIn the unseen understanding task, the WeatherGFM still shows a certain generalization.\n3.\tThe introduction of the model input format is very clear, and the classification and identification of various weather image tasks are fully explained"
            },
            "weaknesses": {
                "value": "1.\tA mixed-modal masked image modeling (MMIM) pipeline, which is the core innovation part of the proposed framework of WeatherGFM, has not been introduced in this paper. Other parts of  WeatherGFM architecture are actually closer to the comparative ClimaX model framework. It will lack the innovative elaboration of the model framework.\n2.\tCSI is the main evaluation index in this paper, but ACC is also a very important index to evaluate the correlation of weather. Should this index be taken into account? Moreover, the setting of CSI threshold value needs some explanation in this paper to explain its important reference for this kind of weather situation classification.\n3.\tAccording to the comparative results of Table 2 experiments in this paper, in fact, the basic Vision Transformer can also realize a variety of understanding tasks of various data sets in the form of input format proposed by the author, and the effect of some tasks is better than that of WeatherGFM, so the advantages of this model in multi-task are specifically highlighted.\n4.\tThere are two clerical errors: (a) \"the\" should be capitalized. (b) The basic learning rate in line 366 should be 1e-4."
            },
            "questions": {
                "value": "1. Can you explain in detail which part of the module WeatherGFM shows its advantages in various tasks?\n2. Can you simply explain the significance of CSI threshold setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}