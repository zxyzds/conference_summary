{
    "id": "IcPkW3QNW2",
    "title": "DepthSplat: Connecting Gaussian Splatting and Depth",
    "abstract": "Gaussian splatting and single/multi-view depth estimation are typically studied in isolation. In this paper, we present DepthSplat to connect Gaussian splatting and depth estimation and study their interactions. More specifically, we first contribute a robust multi-view depth model by leveraging pre-trained monocular depth features, leading to high-quality feed-forward 3D Gaussian splatting reconstructions. We also show that Gaussian splatting can serve as an unsupervised pre-training objective for learning powerful depth models from large-scale unlabelled datasets. We validate the synergy between Gaussian splatting and depth estimation through extensive ablation and cross-task transfer experiments. Our DepthSplat achieves state-of-the-art performance on ScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and novel view synthesis, demonstrating the mutual benefits of connecting both tasks. We invite the readers to view our supplementary video for feed-forward reconstruction results of large-scale or 360 scenes. Our code and models will be publicly available.",
    "keywords": [
        "Gaussian Splatting",
        "Multi-View Depth",
        "Monocular Depth"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "Enjoy the mutual benefits by connecting Gaussian splatting and depth",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=IcPkW3QNW2",
    "pdf_link": "https://openreview.net/pdf?id=IcPkW3QNW2",
    "comments": [
        {
            "withdrawal_confirmation": {
                "value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."
            }
        },
        {
            "summary": {
                "value": "This paper introduces DepthSplat, a framework that bridges Gaussian splatting and depth estimation to improve both tasks. By incorporating pre-trained monocular depth features, the proposed method enhances 3D Gaussian splatting for novel view synthesis and enables unsupervised pre-training of depth models on unlabeled datasets. Extensive experiments demonstrate that DepthSplat achieves state-of-the-art performance in both depth estimation and view synthesis."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Unified Architecture: The architecture of DepthSplat seamlessly integrates depth estimation and 3D Gaussian splatting, allowing for efficient cross-task knowledge transfer. This integration addresses the limitations of each technique while enhancing their complementary strengths.\n\n2. Comprehensive Experiments: This paper validates its approach through extensive experiments, showcasing the model's robustness and generalizability."
            },
            "weaknesses": {
                "value": "Although\u00a0the\u00a0proposed\u00a0method\u00a0achieves\u00a0impressive\u00a0performance,\u00a0its\u00a0technical\u00a0contribution\u00a0is\u00a0relatively\u00a0weak.\u00a0\nThe\u00a0authors\u00a0claim\u00a0two\u00a0main\u00a0contributions:\u00a0integrating\u00a0pretrained\u00a0monocular\u00a0depth\u00a0features\u00a0into\u00a0the\u00a0multiview\u00a0depth\u00a0model\u00a0and\u00a0demonstrating\u00a0the\u00a0capability\u00a0of\u00a0Gaussian\u00a0Splatting\u00a0to\u00a0serve\u00a0as\u00a0an\u00a0objective\u00a0for\u00a0unsupervised\u00a0depth\u00a0learning.\u00a0\n\nFirst,\u00a0the\u00a0approach\u00a0to\u00a0fuse\u00a0multiview\u00a0features\u00a0and\u00a0monocular\u00a0features\u00a0is\u00a0straightforward,\u00a0and\u00a0the\u00a0idea\u00a0has\u00a0been\u00a0explored\u00a0in\u00a0previous\u00a0works,\u00a0such\u00a0as\u00a0[1,\u00a02].\u00a0\n\nSecond,\u00a0while\u00a0using\u00a0Gaussian\u00a0Splatting\u00a0for\u00a0unsupervised\u00a0depth\u00a0learning\u00a0is\u00a0interesting,\u00a0it\u00a0is\u00a0not\u00a0particularly\u00a0innovative,\u00a0as\u00a0view-synthesis\u00a0based\u00a0approaches\u00a0have\u00a0been\u00a0the\u00a0mainstream\u00a0for\u00a0unsupervised\u00a0depth\u00a0estimation.\u00a0Neural\u00a0rendering\u00a0methods,\u00a0such\u00a0as\u00a0NeRF,\u00a0have\u00a0already\u00a0been\u00a0integrated\u00a0into\u00a0unsupervised\u00a0depth\u00a0estimation\u00a0in\u00a0previous\u00a0works\u00a0[3].\u00a0\n\nFinally,\u00a0the\u00a0feedforward\u00a0framework\u00a0proposed\u00a0in\u00a0this\u00a0paper\u00a0is\u00a0largely\u00a0similar\u00a0to\u00a0previous\u00a0works,\u00a0such\u00a0as\u00a0Pixelsplat,\u00a0with\u00a0the\u00a0main\u00a0difference\u00a0being\u00a0the\u00a0architecture\u00a0of\u00a0the\u00a0depth\u00a0estimation\u00a0network.\n\n[1]\u00a0MultiView\u00a0Depth\u00a0Estimation\u00a0by\u00a0Fusing\u00a0Single-View\u00a0Depth\u00a0Probability\u00a0With\u00a0MultiView\u00a0Geometry.\u00a0CVPR\u00a02022\u00a0\n\n[2]\u00a0Learning\u00a0To\u00a0Fuse\u00a0Monocular\u00a0and\u00a0MultiView\u00a0Cues\u00a0for\u00a0MultiFrame\u00a0Depth\u00a0Estimation\u00a0in\u00a0Dynamic\u00a0Scenes.\u00a0CVPR\u00a02023\u00a0\n\n[3]\u00a0DeLiRa:\u00a0Self-Supervised\u00a0Depth,\u00a0Light,\u00a0and\u00a0Radiance\u00a0Fields.\u00a0ICCV\u00a02023"
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents DepthSplat, a novel approach that connects 3D Gaussian Splatting (3DGS) with multi-view and monocular depth estimation to enhance both depth prediction and generalized novel view synthesis. Leveraging pre-trained monocular depth features from the Monocular Depth Estimation(MDE) model, DepthSplat improves the robustness of depth estimation, especially in challenging scenes with occlusions, texture-less regions, and reflective surfaces, which are typically problematic for multi-view methods alone. By integrating monocular features into a dual-branch model architecture DepthSplat achieves both multi-view consistency and high-quality reconstruction. Ablation studies show that monocular features are crucial for best performance, especially in difficult areas, and that DepthSplat's dual-branch design outperforms single-branch alternatives by simplifying the learning process. As a result, the presented DepthSplat archives state-of-the-art performance in multiple datasets."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The overall paper is well-written and easy to understand, where most of the architectural designs and training details are also specifically mentioned, making the re-implementation of the paper easy.\n- From the perspective of generalized feed-forward 3DGS, I believe that existing works still struggle to extend to multiple views whereas DepthSplat enables up to novel view synthesis with 6 views.\n- Building up from a pre-trained Monocular Depth Estimation (MDE) Module, DepthSplat improves the performance of both Novel View Synthesis and Depth Estimation.\n- The choice of the backbone MDE network and the components of the architecture is discussed through well-conducted ablation studies."
            },
            "weaknesses": {
                "value": "Although the proposed contributions of DepthSplat are valuable, I believe that not all of the mentioned contributions are shown in the paper. Specifically,\n\n- Some of the architectures are not mentioned explicitly in the paper, such as how the Gaussian parameters are estimated.\n- Although the authors mention that the pre-training of the network only with the Gaussian Splatting rendering loss is a way to pre-train the depth model in a fully unsupervised manner, there is no explanation or ablation of how this pre-training improves the depth prediction performance."
            },
            "questions": {
                "value": "From the initial manuscript, I have some concerns and questions about the proposed method. With the questions properly addressed, I am eager to raise my ratings.\n\n1. About the pre-trained Monocular Depth Estimation Module the authors are leveraging to initialize the weights of the monocular backbone, I am a bit confused if the authors are utilizing DepthAnythingv1[1] or DepthAnythingv2[2]. In most of the parts, the model is only specified as DepthAnything which is not clear which version of the model the authors are utilizing. Specifically, in Section 4.2, it is mentioned that DepthAnythingv2 is utilized but in Table 2, v1 is best in depth prediction and v2 is best in novel view synthesis and the authors simply mention that DepthAnything shows the best performance. I would suggest the authors to explicitly mention the version of the Depth Anything network they are utilizing as it is a bit confusing.\n\n2. What is the architectural design for the Gaussian parameter prediction? It is only mentioned as \u201cWe append an additional lightweight network to\npredict other Gaussian parameters\u201d but I can\u2019t not find any additional information about the lightweight network in the main paper or the appendix.\n\n3. How does the pre-training stage only with the Gaussian Splatting rendering loss improve the performance of depth estimation? Specifically, I would like to see the comparison between the Depth prediction branch fine-tuned directly with the depth labels and with the pre-training stage.\n\n4. As the authors claim that DepthSplat can achieve multi-view consistent depths, are there any evaluations that can show the superiority of DepthSplat on multi-view consistency? For example, a recent work called FreeSplat[3] mentions that when using per-pixel Gaussians, due to the slight inconsistent position of the Gaussians of corresponding pixels, this leads to quality degradation and proposes a Gaussian fusion method. I believe that with multi-view consistent depth, this type of additional method is not strictly required. I would appreciate if the authors could explain how the multi-view consistency can be understood or evaluated.\n\n---\n\n[1] Yang, Lihe, et al. \"Depth anything: Unleashing the power of large-scale unlabeled data.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[2] Yang, Lihe, et al. \"Depth Anything V2.\" arXiv preprint arXiv:2406.09414 (2024).\n\n[3] Wang, Yunsong, et al. \"FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes.\"\u00a0*arXiv preprint arXiv:2405.17958*\u00a0(2024)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There are no concerns."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces DepthSplat, a novel model that integrates 3D Gaussian splatting (3DGS) with single- and multi-view depth estimation, thereby enhancing the performance of both tasks. By leveraging pre-trained monocular depth features, the model produces robust multi-view depth predictions that improve 3DGS outputs. This framework enables pre-training depth model from large scale unlabelled dataset without the ground truth depths. Through this approach, the model achieves state-of-the-art performance in both multi-view depth estimation and novel view synthesis tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper leverages pre-trained monocular depth models effectively to achieve strong and superior performance in both depth estimation and novel view rendering.\n- DepthSplat achieves efficient memory usage and computational scalability, allowing it to handle large-scale scenes and even produce 360\u00b0 views. This is a substantial improvement for feed-forward 3DGS models."
            },
            "weaknesses": {
                "value": "1. Can the authors analyze how DepthSplat\u2019s depth estimation performance varies with different numbers of input views? This would provide a better understanding of its robustness in sparse-view settings. Moreover, I would like to see how it performs with varying number of inputs in terms of PSNR, SSIM and LPIPS, when evaluated on RealEstate10K.\n2.  I would like to see the efficiency comparison (memory consumption) to MVSplat [1], with varying the number of input views. Also, I am quite confused by the speed comparison in Table 6. If I understood correctly, the proposed method shares similar architecture to MVSplat, besides it adds monocular feature extraction part, which should increase the computation. Why does it infer faster than MVSplat?\n3. For N-view evaluation, It seems to show different observation from [2, 3]. In [2, 3], MVSplat's synthesized image qualities drastically drop when the number of views increase. However, in the Table 6, it seems to increase. What is the difference that led to such differing results? If the number of gaussians increase, unless a module to control the number of Gaussians is proposed [2, 3] I was assuming that the results should not improve significantly. Is it because the authors use top-2 nearest views only for cost computation? However, even so, this does not explain the apparent improvements despite redundant 3D Gaussians depicting the same object surface.\n4. The paper does not provide a detailed description of the architecture for the 3D Gaussian prediction module. This component is central to the view synthesis pipeline, so more information on its design would improve clarity and reproducibility.\n5. The paper lacks results on the depth estimation performance after unsupervised pre-training alone, without additional fine-tuning. Having these results would help evaluate the standalone effectiveness of the unsupervised pre-training step for depth tasks.\n6. As noted in the related works section, DepthSplat effectively augments multi-view cost volumes with pre-trained monocular depth estimation features. However, its approach closely resembles existing models that fuse monocular and multi-view depth estimations, such as those presented in [4, 5], particularly in how they handle the fusion of these estimations, aside from DepthSplat's use of pre-trained models. The biggest differences may be either concatenation or cross-attention between monocular branch and multi-view branch, and the use of pre-trained large model weights. To me, the novelty and effectiveness are questionable, since without pretrained weights, the performance would drop significantly, and the fusing part is highly similar to previous works. Providing more justification on how the integration of pre-trained models constitutes a novel contribution would help strengthen the paper.\n\n[1] Chen, Yuedong, et al. \"Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images.\" arXiv preprint arXiv:2403.14627 (2024).\n\n[2] Fei, Xin, et al. \"PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary Views.\" arXiv preprint arXiv:2410.18979 (2024).\n\n[3] Wang, Yunsong, et al. \"FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes.\" arXiv preprint arXiv:2405.17958 (2024).\n\n[4] Li, Rui, et al. \"Learning to fuse monocular and multi-view cues for multi-frame depth estimation in dynamic scenes.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[5] Cheng, Junda, et al. \"Adaptive fusion of single-view and multi-view depth for autonomous driving.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."
            },
            "questions": {
                "value": "1. Pre-trained models like DUSt3R[1] and MASt3R[2], which perform well on both monocular and multi-view tasks, might serve as viable alternatives to DepthSplat's dedicated monocular branch. Have the authors considered using pre-trained models like DUSt3R or MASt3R, which could potentially simplify the architecture? What benefits does DepthSplat\u2019s separate monocular branch provide in comparison? \n2. How well does DepthSplat generalize across datasets, such as when a model trained on RealEstate10K is tested on DL3DV? Would it maintain performance in depth estimation and novel view synthesis on previously unseen datasets?\n3. Would it be possible if DepthSplat's depth estimation result is compared with 3DGS works like MVSplat or PixelSplat? This comparison would be very interesting to see.\n\n[1] Wang, Shuzhe, et al. \"Dust3r: Geometric 3d vision made easy.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[2] Leroy, Vincent, Yohann Cabon, and J\u00e9r\u00f4me Revaud. \"Grounding Image Matching in 3D with MASt3R.\" arXiv preprint arXiv:2406.09756 (2024)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There are no concerns."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents DepthSplat, which connects Gaussian splatting and depth estimation. Specifically, DepthSplat exploits the complementary nature of sparse view feed-forward 3DGS and robust single/multi-view depth estimation to enhance the performance in both tasks.\nFor 3DGS: DepthSplat leverages pre-trained monocular depth features (DINOv2) to improve the ability of generalizable of 3D Gaussian reconstructions.\nFor depth estimation: This paper provides an unsupervised method for pre-training depth prediction models, achieving superior results compared to training from scratch."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. DepthSplat combines the advantages of 3DGS reconstruction and depth estimation to enhance the performance in both tasks. I think this topic is innovative.\n2. DepthSplat enhance the depth quality of low-textured areas and reflective surfaces by employing a pre-trained monocular depth model.\n3. DepthSplat achieves state-of-the-art results on both tasks."
            },
            "weaknesses": {
                "value": "1. Please answer the difference between Transplat[1] and DepthSplat. it seems that Transplat also improves the quality of reconstruction by using DINO features.\n2. It seems similar to MVSFormer[2] when training the depth model, may I ask the difference between the two methods?\n\n[1] Zhang C, Zou Y, Li Z, et al. Transplat: Generalizable 3d gaussian splatting from sparse multi-view images with transformers[J]. arXiv preprint arXiv:2408.13770, 2024.\n[2] Cao C, Ren X, Fu Y. MVSFormer: Multi-view stereo by learning robust image features and temperature-based depth[J]. arXiv preprint arXiv:2208.02541, 2022."
            },
            "questions": {
                "value": "1. I have some doubts about the training process of DepthSplat. Is the model for predicting gaussian parameters and the model for predicting depth trained separately?\n2. Are the depth and 3DGS results in Table 1 evaluated using the same model?\n3. Can you provide metrics for DepthSplat pre-trained models using only photimetric loss in Table 3?\n\n\nIf the author can answer my question, I am willing to change my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a model named DepthSplat, which integrates Gaussian splatting with depth estimation. \nThe core of the pipeline is a 2D U-Net module that fuses multi-view depth information (from the cost volume) with monocular depth (derived from a pre-trained model, such as Depth Anything). \nExperiments conducted on both depth estimation and novel view synthesis tasks demonstrate that the proposed method achieves competitive results in both areas."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1 - The task addressed in this paper is meaningful, and its insights are interesting.\n\nS2 - This paper contributes a robust multi-view depth model by leveraging pre-trained monocular depth features, resulting in high-quality feed-forward 3D Gaussian splatting reconstructions.\n\nS3 - The study demonstrates that Gaussian splatting can serve as an unsupervised pre-training objective for learning powerful depth models from large-scale unlabeled datasets, which is valuable for large-scale pre-trained models.\n\nS4 - Experimental results show that both depth estimation (Table 4) and reconstruction outcomes (Table 5) are significantly improved compared to the tested baselines."
            },
            "weaknesses": {
                "value": "W1 - The experiment in Section 4.3 appears inconclusive, as it does not convincingly support the claim that \"Gaussian splatting can serve as an unsupervised pre-training objective for learning robust depth models from large-scale unlabeled datasets.\"\n\nW2 - The paper bears similarities to Transplat[1], which also uses monocular depth priors to enhance Gaussian splatting reconstruction. What are the main differences between these two approaches?\n\nW3 - Additional details about DepthSplat, such as its inference speed and memory consumption compared to Mvsplat[2], would be valuable.\n\n[1]Zhang C, Zou Y, Li Z, et al. Transplat: Generalizable 3d gaussian splatting from sparse multi-view images with transformers[J]. arXiv preprint arXiv:2408.13770, 2024.\n\n[2]Chen Y, Xu H, Zheng C, et al. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images[J]. arXiv preprint arXiv:2403.14627, 2024."
            },
            "questions": {
                "value": "Q1 - In the experiment in Section 4.3, it is unclear whether DepthAnything and UniMatch were fine-tuned on the test datasets, as this information is not clearly indicated in Lines 425-431.\n\nQ2 - Table 3 appears inconclusive since these three methods utilize different architectures, making it difficult to directly assess the advantage of Gaussian splatting for pre-training. An additional comparison showing DepthSplat with and without pre-training on RE10k could be informative.\n\nQ3 - More details on inference speed and memory cost across different configurations (e.g., depth model, hierarchical architecture) would enhance understanding.\n\nQ4 - What are the key differences between DepthSplat and Transplat?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}