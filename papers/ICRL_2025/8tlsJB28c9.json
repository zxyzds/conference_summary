{
    "id": "8tlsJB28c9",
    "title": "M2Edit: Locate and Edit Multi-Granularity Knowledge in Multimodal Large Language Model",
    "abstract": "Multimodal knowledge editing is an important method for modifying outdated or incorrect knowledge in Multimodal Large Language Models (MLLMs). However, existing datasets for multimodal knowledge editing lack multi-granularity knowledge. In this paper, we present a more realistic dataset called M2Edit, which includes three distinct types of knowledge: entity, relation, and action. Additionally, existing knowledge editing methods for MLLMs lack the ability to handle multi-granularity knowledge and generalize to multimodal data. To address these limitations, we propose the multimodal knowledge editing method MLE. This approach identifies key knowledge layers within different components and collaboratively edits the various components of MLLMs. As a result, we observe significant improvements in visual generality performance, ranging from 4.8 to 10.8, and achieve the best overall performance on knowledge data of different granularities.",
    "keywords": [
        "Multimodal knowledge editing; Multi-Granularity Knowledge; M2Edit; Multimodal Large Language Model;"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8tlsJB28c9",
    "pdf_link": "https://openreview.net/pdf?id=8tlsJB28c9",
    "comments": [
        {
            "title": {
                "value": "Response to Reviewer #ddNb"
            },
            "comment": {
                "value": "We appreciate very much your constructive comments on our paper. Please kindly find our response to your comments below. We hope that our response satisfactorily addresses the issues you raised. Please feel free to let us know if you have any additional concerns or questions.\n\n#### **Q1**:  The proposed method is evaluated on a limited range of multimodal models.\n\n> - In line with the methodologies employed in previous studies [1], which focused on Mini-GPT4 and BLIP-2, we have maintained their experimental framework to uphold the integrity of our research. To substantiate the efficacy of our approach, we have extended our investigation with further experiments:\n\n> - Evaluation of the Edit Score for LLaVA-1.5 7B [2]:\n\n>| Method | Entity Score | Relation Score | Action Score |\n>| ------ | ------------ | ------------- | ----------- |\n>| FT     | 33.5        | 27.0         | 30.2        |\n>| MEND   | 82.3        | 70.6         | 77.8        |\n>| ROME   | 71.2        | 52.5         | 78.3        |\n>| **MLE** | **86.4**    | **75.9**     | **86.0**    |\n\n>The results clearly demonstrate the superior performance of our method when applied to LLaVA-1.5 [2].\n\n#### References\n\n**[1] Exploring Edits in Multimodal Large Language Models. EMNLP 2023**\n\n**[2] Enhancing Visual Instruction Tuning. NeurIPS 2023**"
            }
        },
        {
            "summary": {
                "value": "The paper works on multimodal knowledge editing.\nIt introduces a new dataset for this task, M2Edit (Multi-Granularity Multimodal knowledge Editing dataset). This dataset incorporates multi-granularity knowledge (relation, entity, and action) to address the limitations of existing multimodal knowledge editing datasets.\nMoreover, the paper proposes a multimodal knowledge editing method, MLE (Multimodal Location-based Editing).\nIt identifies key knowledge layers within different components of MLLMs and collaboratively edits them to improve the model's performance on multimodal data. The method demonstrates significant improvements in visual generality performance and performs well in terms of different knowledge granularities."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is overall clear and well-organized.\n2. The paper proposes a useful dataset M2Edit including multi-granularity knowledge. This addresses the limitations of previous multimodal knowledge editing datasets.\n3. The proposed method MLE can edit multi-granularity knowledge within MLLMs.\n4. The report experiments verify the performance of MLE."
            },
            "weaknesses": {
                "value": "1. The paper ignores the discussion on the complexity of the MLE method.\n2. The paper currently has limited analysis of error cases. Adding this could inspire further research work.\n3. The uploaded material doesn't include the code, only the used dataset.\n4. The paper doesn't mention how many samples the method edits at once, so it seems the paper does not report the results of batch editing.\n5. The introduced dataset M2Edit seems to include counterfactual knowledge. How does the MLE perform with real-world knowledge as in [1,2]?\n\n[1] MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions  \n[2] Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing"
            },
            "questions": {
                "value": "1. Line 483, To address --> to address\n2. It is hard to recognize the sentences with the striped background in Figure 2.\n3. Can you provide some analysis of error cases?\n4. The supplementary material only contains the M2Edit dataset. What about the code of MLE?\n5. The paper should explain how the various metrics are computed.\n6. How many samples do you edit at once? What is the performance of MLE when editing with several samples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces M2Edit, a dataset with entity, relation, and action knowledge types, designed for multimodal large language models (MLLMs). It highlights the challenge of knowledge editing across different granularities within MLLMs and proposes the MLE (Multimodal Location-based Editing) method to tackle this. MLE sequentially identifies and edits key knowledge layers within MLLMs, enhancing generality and effectiveness in multimodal contexts. The model demonstrates improved accuracy and generalization over previous methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Innovative multi-granularity approach in knowledge editing for MLLMs, addressing a gap in existing datasets.\n- The MLE method shows significant performance improvements, particularly in visual generality and model adaptability.\n- Offers detailed methodology for locating and editing specific knowledge layers within MLLMs, aiding model interpretability."
            },
            "weaknesses": {
                "value": "The proposed method is evaluated on a limited range of multimodal models, which restricts the generalizability of the findings across other MLLMs with different architectures or training objectives. Specifically, the recent VL models like QwenVL2, Llava should be evaluated."
            },
            "questions": {
                "value": "No"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores advanced techniques in knowledge editing for multimodal large language models (MLLMs). \nThe authors introduce M2Edit, a dataset to enhance multimodal knowledge editing by incorporating multi-granularity knowledge types\u2014entities, relations, and actions. \nThey highlight the limitations of existing methods and datasets in addressing multi-granularity and multimodal challenges. \nThe paper proposes a method, MLE (Multimodal Location-based Editing), which improves knowledge editing by identifying and modifying key knowledge layers across the various components of MLLMs. \nExperiments show that the method improves visual generality performance and achieves superior results on multi-granularity knowledge compared to existing benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The reviewer finds the authors' motivation for considering multi-granularity in knowledge editing to be interesting, which aligns with intuitive understanding of the subject.\n- The dataset contributed by the authors (if executed flawlessly) could potentially be a significant asset to the field of multimodal knowledge editing.\n- The method introduced by the authors outperforms the baseline."
            },
            "weaknesses": {
                "value": "The paper's primary issue appears to be the unclear expression and presentation of content, with so many details lost, making it difficult for the reviewer to understand the whole story fully. Furthermore, significant potential issues have been identified within the dataset section.\n\n\n## Major Concerns:\n- The reviewer questions the setting of multimodal knowledge editing as posited by the authors, perceiving that it remains limited to textual LLM thinking. Notably, the M2Edit only edits the textual part of image-text pairs, which implies no equivalent editing for visual knowledge, thereby not affecting the image component. If the entire multimodal knowledge editing topic is defined in this manner, the reviewer questions the scientific validity of this definition and suggests that a fundamental improvement is necessary. The reviewer suggests that the authors further clarify this point in their discussion.\n- While the paper mentions relational type knowledge in a triplet format, the examples shown in Figures 1 and 2 do not represent triplets but rather entity-level knowledge. The manifestation of relation-level knowledge editing remains unclear. The reviewer recommends that the authors revise and clarify this point clearly in the text.\n- The dataset claims the importance of three levels of knowledge but does not integrate these levels within a single scope; different levels of annotations cannot coexist within the same instance, which likely limits the dataset's utility. Therefore, the reviewer hopes that the authors can further explain and clarify this matter.\n- The data annotation process is not clearly articulated, raising concerns about the control over data quality, especially as it relies entirely on an automated process via ChatGPT, which is prone to introducing noise. Please provide a detailed description of this step in the manuscript.\n- Figure 2 is really challenging to understand; it is unclear what the multiple lines of text within circles represent. Please provide further details.\n- Similarly, Figure 3 is also difficult to decipher; the meanings of various arrows and shapes within the figure are not explained, and the significance of the different rectangles in the bottom-left box and what r, s, t represent are not clarified. Please provide additional information.\n- In the methods section, the authors claim that to address the limitations of existing knowledge editing methods\u2014which cannot handle multi-granularity knowledge and lack generalization on multimodal data\u2014they propose a method called MLE (Multimodal Location-based Editing). However, the reviewer does not understand the causal relationship between the existing methods' inability to handle multi-granularity knowledge and the proposed \"Locate Then Edit\" approach. Is it necessary for multi-granularity knowledge editing to be implemented specifically through a \"Locate Then Edit\" method? \n- The methods were only validated using older MLLMs like BLIP2-OPT and MiniGPT4, which may not represent the most advanced MLLMs, thus not sufficiently proving the effectiveness and generality of the proposed multimodal knowledge editing methods. The reviewer suggests adding more MLLMs for experimental comparison.\n- The experimental analysis conducted by the authors lacks sufficient depth and breadth. The reviewer strongly recommends enhancing the content of the experimental analysis.\n- The absence of any anonymous links for accessing model code and data examples impedes the reviewer's ability to further investigate and address the issues raised, casting doubts on the reproducibility of the research. Will the authors consider open-sourcing the code and resources?\n\n\n\n## Typos & Expression\n\nOverall, the writing and expression in the paper are overly casual and lack the refinement expected in scholarly communication.\n\n- There is a grammatical error on page three, line 112.\n- All images in the paper are non-vectorial.\n- The citation format throughout the paper does not adhere to standard academic norms.\n- There are numerous detail-oriented issues, such as inconsistent punctuation in equations\u2014some equations end with a comma or period while others do not, creating a disorganized appearance.\n\n\nOverall, the reviewer is open-minded. If the authors can actively and effectively address these concerns, the reviewer would consider raising the rating."
            },
            "questions": {
                "value": "The paper contains quite many aspects that are not clearly explained, making it challenging for the reviewer to understand. Below are some questions that need addressing:\n\n\n- The term \"in-scope\" mentioned in Figure 2 and its caption is ambiguous; does it refer to \"in-domain\"?\n- The caption in Figure 2 states, \"After editing the MLLMs, the in-scope samples need to be generalizable, and the out-of-scope samples should not be unchanged\", but this statement is confusing and lacks clarity.\n- The head entity \"Arcadia\" mentioned on page four, line 177, is not visible in the middle part of Figure 2, making the reviewer confused about its inclusion and relevance.\n- Beyond BLIP2-OPT and MiniGPT4, how does the proposed method perform on other state-of-the-art MLLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}