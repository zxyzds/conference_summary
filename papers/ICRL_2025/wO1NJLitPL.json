{
    "id": "wO1NJLitPL",
    "title": "A Bregman Proximal Viewpoint on Neural Operators",
    "abstract": "We present several advances on neural operators by viewing the action of operator layers as the minimizers of Bregman regularized optimization problems over Banach function spaces. The proposed framework allows interpreting the activation operators as Bregman proximity operators from dual to primal space. This novel viewpoint is general enough to recover classical neural operators as well as a new variant, coined Bregman neural operators, which includes the inverse activatio and features the same expressivity of standard neural operators. Numerical experiments support the added benefits of the Bregman variant of Fourier neural operators for training deeper and more accurate models.",
    "keywords": [
        "neural operators",
        "proximal optimization",
        "bregman divergence",
        "fourier neural operator"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=wO1NJLitPL",
    "pdf_link": "https://openreview.net/pdf?id=wO1NJLitPL",
    "comments": [
        {
            "summary": {
                "value": "This article constructs neural operators from the point \nof view of Bregman optimization problems. The proposed idea uses the dual space of Banach functional theory, \nand it allows to recover classical neural operators \nand define new ones. Numerical results \non the newly constructed operators improve the accuracy of \nstate-of-the-art results by using deeper networks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Although technical, the article is well-written and easy to follow. \n\nThe contribution raises an important question on the choice \nof a metric/divergence in the functional space of the solution u."
            },
            "weaknesses": {
                "value": "It seems that there is still a gap between the universal approximation result\nand the numerical results in the article as \nthe theoretical assumption about the non-linearity sigma (sigmoid type)\ndoes not hold in the numerical models (sigma=softplus is not sigmoid type). \nTherefore it would be good to mention this gap in the conclusion."
            },
            "questions": {
                "value": "- There is some notation inconsistency in the definition of the kernel K_t^ac in eq. 3\nand the K_t in Section 3.1. Are you talking about the same type of kernel in these 2 places? \nWhy do you use k^(t) as the kernel density, rather than the k_t as before (below eq .3)? \n\n- Section 3.1, it is unclear what the sigma_1 and sigma_2 after Remark 6 comes from, \ndo they depend on g_t?\n\n- Is bar{D} the closure of the set D in the definition of A in Section 4? \nWhy do you consider the space C with \\bar{D} rather than with D?\n\n- some type in remark 9: no in?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel expressive framework called BFNO to improve FNO by understanding the action of operator layers via the minimization of Bregman regularized optimization problems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The Bregman-based perspective on neural operators is intriguing, and the paper presents a variety of strong theoretical results."
            },
            "weaknesses": {
                "value": "1. The writing is poor, and I recommend that the authors carefully revise the paper from start to finish, especially regarding newly defined matrices or functions. For example, in Eq. 4, the definitions of $M_t$ and $K_t$ are not clearly stated when they first appear in the paper.\n2. The experiments are too simplistic. The authors only compared BFNO with FNO. I suggest including other FNO improvements as baselines.\n\ntypo: the operators in Line 370"
            },
            "questions": {
                "value": "Q: Which $\\psi$ in Table 1 do you use in the experiments? Do you compare the BFNOs obtained from different $\\psi$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies neural operators from the perspective of Bregman proximal optimization. The nonlinear activation layers such as (sigmoid, tanh, SoftPlus) are interpreted as the Bregman proximity operators. Based on the above optimization viewpoint, a new neural operator named BFNO is proposed as an extension of FNO, where an additional term sigma^{-1}(.) introduced in the expression of FNO before the function sigma(). A few experiments show that BFNO performs better than FNO especially for a large number of neural layers."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(1) The main contribution of the paper is that the authors interpret the neural operators as Bregman proximal optimization. This opens up the possibility of bringing knowledge or theory of Bregman proximal optimization into the field of neural operators for many possible future work.  \n\n(2) As I mentioned earlier,  BFNO is proposed as an extension of FNO by introducing an additional term sigma^{-1}(.) in the expression of FNO before the function sigma(), which I think has a similar effect as the skip connection in F-FNO or ResNet."
            },
            "weaknesses": {
                "value": "(1) I think the authors should compare the performance of BFNO to that of F-FNO equipped with skip connection. This is because from a high level point of view, the introduction of an additional term sigma^{-1}(.) in the expression of FNO before the function sigma() is very similar to the skip connection in F-FNO. It is very interesting to find out which one performs better.  Personally I think F-FNO might perform better because the skip connection also has a strong motivation from the ODE point of view.  BatchNormalization can also be included in  F-FNO smoothly. If the authors are able to show that BFNO performs better instead with a good explanation, I would be happy to change my score of the paper. \n\n(2) The paper conducted theoretical analysis but were not able to show in theory why BFNO performs better than FNO, which I think is very critical. Instead, they conduct experimental results to argue the superiority of BFNO. This is also partly the reason for me to suggest the comparison between the performance of BFNO and that of F-FNO.\n\n(3) It is not clear to me if BatchNormalization or Layer normalization can also be covered by the framework of Bregman proximal optimization. The reason I have this concern is that the FNO paper used BatchNormalization in their experiment. I would think doing so improve the training stability. If BatchNormalization cannot be covered by  the framework of Bregman proximal optimization, it suggests the limitations of the framework. \n\n(4) Another weakness is that the activation function needs to be monotonic for it to be invertible. This excludes a few functions such as ReLU and Swish.  I understand that SoftPlus is similar to ReLU but still it suggests the framework has some limitations."
            },
            "questions": {
                "value": "(1) I wonder if the authors use BatchNormalization when implementing FNO in their experiments because the original FNO paper used it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This is a primarily theoretical paper which proposes to address a lack of formalism for characterizing architecture in previous methods.  The authors propose to replace the basic layer of a Fourier Neural Operator, equation (3), with a more general layer, equation first written down in equation (4).  This architecture was actually originally proposed in Frecon et al. (2022) \"Bregman neural networks\".  A large part of the papers introduces technical theory from convex analysis.  University approximation results are presented for the architecture.  Numerical experiments are presented comparing FNO with BFNO.  Seven equations are solved. It is demonstrated Fig 2a, that on the Burgers equation, the accuracy of FNO degrades as the number of layers are increased while BFNO is more accurate."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The authors bring the formalism of Bregman operators, which is normally used in convex analysis and optimization to bear on the analysis of neural network architectures."
            },
            "weaknesses": {
                "value": "The paper introduced a heavy mathematical formalism without justification.  The ideas of the paper are not clearly presented and the theoretical contribution is not substantial. \n\nOverly technical:\n\n- General conference audience will not understand the technical papers\n- Neural operator specialists: will not understand the paper.\n- Only experts in convex analysis will be able to follow much of the paper.\n- Section 2 begins with a dense paragraph of convex analysis, unreadable to anyone who doesn't already know the area.  It's also unconnected to later sections, so not immediatly clear what is needed from this paragraph.\n- Section 2.3 is an overview of bregman operators, bregman distance, textbook material, again not clear how much is needed.  \"For additional details, the reader canrefer to Bauschke & Combettes (2017)\"\n\nThe architecture is not clearly defined in the paper. \n- The architecture was originally proposed in  Frecon et al. (2022) \"Bregman neural networks\" which was not an influential paper.  In that paper, it was made clear that the architecture involves bilevel optimization.  This is not explained or made clear in the current paper. \n- In section 2.2. equation (4), which is a modification of (3), names the architecture, but it is not defined. \nLater, it is defined as a special case of (6), which takes half a page to write down. \"In (6), the proximity operator plays the role of an activation function operator, which in general will have the form of a nonlinear Nemytskii operator. ... Moreover, differently from usual architectures (Kovachki et al., 2021), ...\"\n- Nemytskii operator is never defined. However, \"This relationship is crucial for further establishing connections with neural operator layers.\"\n\nExperiments: \nThe numerical experiments are quite limited and show minimal improvement, or improvement in very particular situations, compared to FNO. \n\"To this end, we conducted an experiment using the Burgers\u2019 dataset with viscosity \u03bd = 10\u22123, with results presented in Figure 2a. First, we observe that BFNO systematically yields lower prediction error, irrespectively of T. Second, the performanceof FNO degrades starting from T = 16, while BFNO demonstrates better performance as T increases until it reaches a plateau at T = 64. \"\n\n- Was this also the case for the other 6 equations solved, or just for this one?  Why did you just present the one equation"
            },
            "questions": {
                "value": "Questions: \n\n1. Explain the architecture in more direct way.  Is there a bilevel optimization? This was much easier to understand in Frecon et al. (2022) \"Bregman neural networks\"\n2. \"Previous methods often lack of a general formalism for characterizing their architecture.\"  But there is a well known paper that does:  \"Neural Operator: Learning Maps Between Function Spaces.  How does the contribution of this paper related to that one?\n3. \"In this work, we propose a novel expressive framework for neural operators by conceptualizing the action of operator layers as the minimizers of Bregman regularized optimization problems over Banach function spaces.\"  This sentence does not make sense, please clarify.\n\n4. \"We prove universal approximation results\". This is true of almost any reasonable neural network, including MLP.  How does this argument show anything special about this particular architecture?\n\n- \"the proposed framework allows applying the extensive body of literature on proximal numerical optimization, of which Bregman proximity operators belong to, in order to study neural operators.\"\n  - convince me why this is useful.  \n- \"This opens the way to extend the analysis done on neural networks to (Bregman) neural operators in the same spirit of Combettes & Pesquet (2020a;b).\"\n  - Explain what would be achieved by this. \n5. In Fig 2a, did the same results hold the other 6 equations solved, or just for this one?  Why did you just present the results for Burgers equation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}