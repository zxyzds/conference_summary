{
    "id": "mHx8JFURtn",
    "title": "Rethinking logic in AI: A novel benchmark inspired by polynomial analogue of Gandy's fixed point theorem",
    "abstract": "This paper introduces a novel benchmark for evaluating the logical reasoning capabilities of Large Language Models (LLMs), grounded in the polynomial analogue of Gandy's classical fixed point theorem. Drawing on concepts from mathematical logic, we design a parameterized set of recursive problems where the objective is for LLMs to predict the outcome of a Boolean function, achievable within a polynomial number of steps. By varying the parameters, we generate problem instances of differing complexity. Our experiments reveal that current state-of-the-art LLMs fail to reliably solve even the simplest cases, despite an effective deterministic algorithm existing. Notably, even advanced models like GPT-4 exhibit significant biases in solving benchmark problems. These findings highlight the limitations of modern LLMs as code interpreters, even in basic scenarios, and underscore the necessity for hybrid LLM/interpreter systems. Furthermore, they emphasize the importance of developing quantitative tests for reasoning, given the increasing reliance on LLM-based systems in decision-making applications.",
    "keywords": [
        "llm",
        "logic",
        "benchmark",
        "Gandy's fixed point theorem"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We introduce a benchmark revealing that even advanced LLMs like GPT-4 struggle with basic logical reasoning tasks",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mHx8JFURtn",
    "pdf_link": "https://openreview.net/pdf?id=mHx8JFURtn",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a method to generate functions that will be evaluated by a LLM in order to study how LLLMs perform as interpreters\\"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper addresses a pertinent ptoblem\n\nThe proposal seems sound and allows for a number of parameters\n\nThe evaluation is interesting and includes both open and closed source systems."
            },
            "weaknesses": {
                "value": "Reading this work, I got the feeling that it may be too early for such in-depth analysis, we should first improve LLMs :). \n\nThe paper does provide some insight on problems with bias, but the results seem quite independent of parameter variation. Again, raises the question of whether we are pushing the LLMs too hard?\n\nFinally, do you consider the parameters representative of what the LLMs will find?\n\npg 6: being easy to verify in one\u2019s mind -> that's an interesting point, the paper does not discuss this basic question much,\n\nThere are some typos, namely Eq?? in 3"
            },
            "questions": {
                "value": "The title starts very strongly: RETHINKING LOGIC IN AI. Are you actually doing that?\n\nAbstract: The abstract seems to have redudant text\neg, \nEven advanced models like GPT-4 exexhibit significant biases in solving benchmark problems\n\nand then \neven the most advanced GPT-4 models exhibit biased behavior while\nsolving recursive problems.\n\n5 on the results of mathematical logic -> it is not very clear what you refer to here"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors study the reasoning capabilities of LLMs by studying their capabilities to decide recursively defined properties of nested lists. They formulate the properties to be tested so that they are polynomial time testable. They implement a generator that generates these problem instances with varying complexity; pairs consisting of a nested list and recursively defined property. They consider two kinds of encodings of their problems to LLMs; inputs to conversational LMs and inputs to code completion LMs. They then compare the reasoning capabilities of different LLMs.\n\nThey found out that the LLMs they tested were not capable of deciding the recursive properties."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "It is interesting to study the capabilities of LLMs using mathematically defined objects."
            },
            "weaknesses": {
                "value": "In summary, the authors study the capabilities of LLMs by studying how well they can decide recursively defined properties of nested lists. While to have understanding on the capabilities of LLMs is important, the paper itself seems to me more like a good student project to showcase the limits of the current capabilities of LLMs, than a research paper ready to be published. In particular, the main contribution of the paper is the idea to study capabilities of LLMs by inputing pairs consisting of nested lists and recursively defined properties of those lists, the task being to decide whether the list satisfied the properties given. The authors then test various LLMs on the inputs they generate.\n\nThe theoretical contribution of the submission is to consider recursively defined properties and inputs to decide, and the technical contribution is to generate these inputs and tabulate the results with respect to different LLMs. I do not think that either contribution of the paper suffices for a publication in a top general conference in machine learning."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel benchmark for evaluating the logical reasoning capabilities of Large Language Models (LLMs). \n\nThe proposed approach involves generating [object, condition] pairs, where objects are nested lists of elements (for example, numbers) and conditions are expressed with recursive functions. The task of the LLM is to determine whether the object satisfies all conditions.\nThe source code of the benchmark is publicly available.\n\nThe main finding of the paper is that current state-of-the-art LLMs fail to reliably solve even the most straightforward cases, thus pointing to significant limitation in their logical reasoning capabilities."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The challenge of assessing precisely the reasoning capabilities of LLMs is extremely important to the development of LLMs themselves. This paper makes a contribution in this sense by introducing what looks like an interesting benchmark.\nHowever, why the proposed benchmark is actually important is not developed enough in the submission.\n\n2) The experimental evaluation is rather in-depth, with several important LLMs considered, both proprietary and publicly available."
            },
            "weaknesses": {
                "value": "1) The paper is only 8 pages long, two pages shorter than the page limit. \nIt is not clear why the authors didn't include some of the material appearing in the appendix, for instance.\n\n2) The authors define their class of problems through the Polynomial Analogue of Gandy\u2019s Fixed Point Theorem (which I am not familiar with). However, it is not clear why this specific class is so important as to represent a benchmark for LLMs.\n\nAlso, the authors might consider to clarify the relation with standard propositional Boolean logic (which I assume is the non-nested version).\n\n3) The related literature is not discussed at any length. It almost looks like there has not been any other effort to assess the reasoning capabilities of LLMs, including logic-based reasoning."
            },
            "questions": {
                "value": "1) Why is the class of problems considered in this submission so relevant as to represent a benchmark for LLMs?\n\n2) How does this contribution compare to the current state of the art?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new benchmark dataset for evaluating the reasoning ability of LLMs, based on the PAG theorem. The problems in the dataset contain a theorem that returns boolean output based on a given nested list. The dataset is created using a condition generator and a probe generator to generate the theorems and the nested list instances respectively. The theorem and the assignment are represented in text as Python functions and lists. The paper then prompts LLMs to predict whether a theorem is satisfied or not by the given nested list, and finds that state of the art LLMs are unable to when prompted in a zero shot setting."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The proposed dataset can be generated parametrically, which varies its difficulty level. \n\nThe proposed dataset is difficult for existing LLMs. \n\nThe dataset generators are well-defined."
            },
            "weaknesses": {
                "value": "To me, the dataset is not strongly motivated. I agree that new reasoning benchmark datasets are important for LLMs, but why this specific dataset is needed is unclear. What is the advantage of this dataset over, for example, a set of problems that include SAT formulas in CNF and the set of assignments to the variables, where the LLM is asked to verify if the solution is valid?\n\nIt seems that the LLMs are only evaluated on this dataset using zero-shot prompting. Extensive literature has shown that LLM reasoning abilities can be significantly enhanced by chain-of-thought [1], tool-using [2], self-consistency [3] etc. Therefore, it is hard to draw a definitive conclusion on LLM reasoning abilities from zero-shot prompting results alone.\n\nBy representing the problem in Python code, the dataset is limited to evaluating LLM\u2019s reasoning ability in the specific context of executing Python, which may not represent LLM\u2019s reasoning ability in the general case. For example, by asking the conversational LLM to output the result of print(is_member_0(x)), the LLM needs to understand the Python syntax on top of the logic of whether x satisfies f or not.\n\n[1] Wei, Jason, et al. \"Chain-of-thought prompting elicits reasoning in large language models.\" \n\n[2] Schick, Timo, et al. \"Toolformer: Language models can teach themselves to use tools.\" \n\n[3] Wang, Xuezhi, et al. \"Self-consistency improves chain of thought reasoning in language models.\""
            },
            "questions": {
                "value": "The problem is prompted to the LLMs in the format of python code. Why not represent the problems using text? Were other representations considered to represent the problems?\n\nWere there any further analysis performed on the results shown in Figure 1? Would simply negating the solution from GPT-4-turbo achieve 90%+ accuracy? Does a model getting the wrong answer 90% of the time mean it is consistently doing (wrong) reasoning internally?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}