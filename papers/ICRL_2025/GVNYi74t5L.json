{
    "id": "GVNYi74t5L",
    "title": "M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models",
    "abstract": "Multilingual capability is an essential aspect for large multimodal models, since they are usually deployed across various countries and languages. However, most existing benchmarks for multilingual multimodal reasoning struggle to differentiate between models of varying performance; even language models without visual capabilities can easily achieve high scores. This leaves a comprehensive evaluation of leading multilingual multimodal models largely unexplored. In this work, we introduce M4U, a novel and challenging benchmark for assessing the capability of multi-discipline multilingual multimodal understanding and reasoning. M4U contains 8,931 samples covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare in Chinese, English, and German. Using M4U, we conduct extensive evaluations of 21 leading Large Multimodal Models (LMMs) and Large Language Models (LLMs) with external tools. The evaluation results show that the state-of-the-art model, GPT-4o, achieves only 47.6\\% average accuracy on M4U. Additionally, we observe that the leading LMMs exhibit significant language preferences. Our in-depth analysis indicates that leading LMMs, including GPT-4o, suffer performance degradation when prompted with cross-lingual multimodal questions, such as images with key textual information in Chinese while the question is in German. We believe that M4U can serve as a crucial tool for systematically evaluating LMMs based on their multilingual multimodal reasoning capabilities and monitoring their development.",
    "keywords": [
        "Multilingual",
        "Multimodal Reasoning",
        "Large Multimodal Models"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "In this work, we introduce M4U, a novel and challenging benchmark for assessing the capability of multi-discipline multilingual multimodal understanding and reasoning.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=GVNYi74t5L",
    "pdf_link": "https://openreview.net/pdf?id=GVNYi74t5L",
    "comments": [
        {
            "summary": {
                "value": "This paper curates an evaluation benchmark (called M4U) for multilingual multimodal models. It consists of 8,931 samples covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare in Chinese, English, and German. The evaluation shows that it's a challenging benchmark for most LMMs, where GPT-4o achieves only 47.6% average accuracy on M4U."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* **Writing**: The paper is well-written and easy to follow.\n* **Well-designed Curation Process**: A large portion of the dataset is curated to avoid data contamination.\n* **Contribution**: It would be a useful resource for the community to assess the multilingual performance of LMMs."
            },
            "weaknesses": {
                "value": "* **Language Coverage**: Only three languages (Chinese, English, and German) are covered in this benchmark.\n\n* **Language-specific characteristics**: The multilingual samples are directly translated from one language therefore the language-specific characteristics are not considered in the curation process. Especially for STEM, it's very much evaluating the translation ability or the multilingual ability of the model. It would be interesting to cover samples where the interesting characteristics (e.g., culture) behind multilingualism really matter. For example, for healthcare, the guideline in different countries may differ so having those knowledge is important and will make the benchmark more interesting."
            },
            "questions": {
                "value": "* Is there any language-specific samples covered in the current dataset?\n\n* Is it possible to evaluate only the OCR ability of different models to probe if these models fail because of the multilingual OCR ability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper begins by collecting examination questions with relatively low leakage risk from Chinese universities, resulting in a Chinese multimodal assessment dataset comprising 8,931 items. The authors then constructed English and German translation assessment datasets by translating the text only. They claim that this dataset is a particularly challenging one for multimodal large models and explore the cross-linguistic performance of these models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Through extensive labor in collection and processing, the authors have created a relatively large multimodal reasoning assessment dataset that reflects university-level knowledge. This effort addresses the issues of high data leakage risk in existing datasets and the inability to differentiate performance among various models."
            },
            "weaknesses": {
                "value": "> 1. Exaggeration of Contributions:\n\nThe article's scientific contributions focus on \"challenging visual reasoning tasks\" and \"multilinguality.\" However, it lacks sufficient experimentation and effort to substantiate and showcase these contributions.\n\n- Regarding the contribution of \"challenging visual reasoning tasks,\" the authors attempt to demonstrate this by collecting university-level questions with low leakage risk and arguing that these tasks are \"difficult\" from the perspectives of \"answering based solely on text\" and \"low model accuracy.\" However, they do not provide data on human expert performance for these questions, which undermines their definition of \"difficulty\" due to the lack of reference to human accuracy rates.\n\n- Concerning the \"multilingual\" contribution, merely translating text into two other languages does not convincingly support the claim of \"multilinguality,\" especially in light of the absence of experimental data to ensure the accuracy of the translation, casting doubt on the quality of the multilingual aspect of the dataset.\n\n- Most importantly, as described in the paper, if the images contain Chinese text, retaining the Chinese in the assessments of other languages is clearly unreasonable. Furthermore, considering that most multimodal models' visual encoders are not trained on Chinese text-image pairs, their ability to encode Chinese text is limited, significantly constraining the models' reasoning capabilities and potentially leading to inaccurate assessments of the dataset.\n\n> 2. Lack of Rigor and Missing Information in Data Processing:\n\nFor the critical data quality verification aspect of the dataset, the authors use vague language. For instance, they repeatedly refer to \"over 10 college and graduate students\" when discussing the number of annotators, which I believe is an inadequate expression for a scientific paper. Additionally, there are other missing data statistics, as detailed in the Questions section below."
            },
            "questions": {
                "value": "Q1: How is the accuracy of the multilingual data translation ensured? Were native speakers involved in verifying the accuracy?\n\nQ2: Why was the translation of text in the images not considered?\n\nQ3: The paper mentions that the dataset questions come from online videos, PDFs, and textbooks. What are the respective proportions of these sources?\n\nQ4: How is the accuracy of the questions generated ensured? Was there a cross-checking process, and what is the consistency rate of the statistics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new challenging benchmark for assessing the capabilities of LMMs in multi-discipline, multilingual, and multimodal understanding and reasoning. The benchmark covers various disciplines and languages, and the authors have meticulously collected and constructed this benchmark. The authors conducted extensive evaluations of leading LMMs and LLMs, demonstrating that there is still significant room for improvement in existing models' capabilities in multilingual multimodal reasoning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "-  The motivation behind this benchmark is well-founded, the benchmark provides support for subsequent evaluations of the capabilities of LMMs/LLMs in multidisciplinary, multilingual, and multimodal understanding and reasoning.\n- The construction of the benchmark is thorough, the paper offers a complete assessment and analysis of LMMs and LLMs.\n- The paper is well-written and easy to understand."
            },
            "weaknesses": {
                "value": "- The data sources for the benchmark are mainly focused on science exam questions, and there is still room for improvement in terms of comprehensiveness.\n- Compared to other benchmarks, there does not seem to be a significant difference.\n- The evaluation method is relatively simple and may not fully reflect the model's multilingual understanding and reasoning abilities."
            },
            "questions": {
                "value": "- Were more evaluation methods attempted, such as translating questions into English or incorporating OCR results of images into the questions, to compare the models' understanding and reasoning abilities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces M4U, a novel and challenging benchmark for evaluating multilingual multimodal understanding and reasoning capabilities across 16 subfields in Science, Engineering, and Healthcare in three languages. The experiments show that even the state-of-the-art model, GPT-4, still has significant room for improvement, especially in expert-level multilingual multimodal reasoning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper introduces a carefully annotated multimodal, multilingual reasoning dataset, consisting of 8,931 multiple-choice questions covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare.\n2. The paper tests various MLMMs on this dataset to evaluate their performance on subject-specific multimodal reasoning and analyzes the causes of their limitations."
            },
            "weaknesses": {
                "value": "1. Necessity of the benchmark. It is suggested to provide specific examples of real-world applications or use cases where this benchmark would be valuable for evaluating MLLMs. Emphasizing its importance in areas like educational robots or medical scenarios might help clarify its necessity.\n2. General applicability of the benchmark. This benchmark requires extensive subject-specific knowledge, making reasoning skills secondary. The emphasis on subject knowledge could limit its general applicability. It is suggested to reorganize the title and the Introduction section to highlight specific scenarios and necessary requirements for the benchmark. For example, in MMMU(MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI), the title emphasizes the scenario is \"expert AGI\". \n3. Limited Contribution. The main focus of the paper is on introducing the benchmark and evaluating various MLLMs on it. However, there is no novel approach in terms of the design, construction, or evaluation methods of the benchmark, nor is there an attempt to improve upon methods that performed poorly. This makes the contribution seem limited."
            },
            "questions": {
                "value": "1. The benchmark content is quite challenging. Given the difficulty level, it\u2019s hard to envision which specific models or fields would absolutely need this benchmark.\n\n2. While M4U offers a more refined, discipline-specific benchmark, it may not serve as a true innovation in assessing multimodal understanding and reasoning, as it primarily tests subject-specific knowledge. The need for such specialized knowledge might be limited outside of subject-focused educational contexts. Narrowing the scope to specific application areas\u2014such as healthcare, education, or scientific research\u2014would help make the benchmark\u2019s significance and applicability more compelling.\n\nTherefore, I suggest you provide evidence or examples demonstrating how M4U tests multimodal understanding and reasoning capabilities beyond just subject knowledge. These examples would help to clarify the benchmark's value and innovation.\n\n3. As shown in the analysis in Figure 4, the top performance barriers are \u201cPerceptual Error\u201d, \u201cLack of Knowledge\u201d and \u201cReasoning Error\u201d with the first two accounting for over 69%. This suggests that the main performance limitations stem from the MLLM\u2019s image encoder and LLM lacking subject-relevant knowledge, rather than reasoning capability. Therefore, describing this dataset as a measure of reasoning and understanding might be somewhat misleading.\n\nIt is suggested to provide additional analyses or examples that demonstrate how the dataset tests reasoning capabilities despite the prevalence of perceptual and knowledge-based errors. You can show examples of those failures due to \u201cReasoning Error\u201d. These examples are the characteristics of your benchmark.\n\n4. In Line 112-113, \"Unlike MMMU and CMMMU, our dataset focuses on the evaluation of multilingual multimodal reasoning\". As far as I know, MMMU and CMMMU are also multi-science and multi-modal benchmarks. The difference is that your benchmark can evaluate problems in multiple languages, and the problems are harder. \n\nI wonder if the multilingual aspect truly necessary? While it is essential in certain contexts, this point isn\u2019t clearly presented. Additionally, Chinese and German are not particularly low-resource languages. It\u2019s unclear why multilingualism is a primary focus in this work. \n\n5. It is suggested to detail the process of the data collection and annotation processes. A high-quality dataset requires not only accurate labels but also well-defined annotation standards and quality control procedures. This information is essential for assessing the dataset\u2019s reliability and usefulness, as well as aiding other researchers in replicating and leveraging the data. The paper should include details about annotator selection, annotation processes, quality control measures, and how label consistency and accuracy were ensured. For example, the following could be expanded: \u201cTo minimize the risk of data contamination, samples are collected from college exams and quizzes from online video lectures. Additionally, a significant portion (35%) of the questions in M4U are written by our team based on textbooks\u201d (line 82-84). For the first part, what type of crowdsourcing platform was used? For the latter, how did you extract questions from textbooks, formulate questions, and choose answer options? Highlighting the technical aspects of crowdsourcing and data engineering might make this work more engaging."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}