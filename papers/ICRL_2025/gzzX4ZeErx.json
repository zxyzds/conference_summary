{
    "id": "gzzX4ZeErx",
    "title": "MIRAI: Evaluating LLM Agents for International Event Forecasting",
    "abstract": "We present MIRAI, a benchmark designed to systematically evaluate LLM agents as temporal forecasters to predict international events. Our benchmark features an agentic environment with tools for accessing an extensive database of historical, structured events and textual news articles. We refine the GDELT event database with careful cleaning and parsing to curate a series of relational prediction tasks with varying forecasting horizons, assessing LLM agents\u2019 abilities from short-term to long-term forecasting. We further implement APIs to enable LLM agents to utilize different tools via a code-based interface. Notably, MIRAI features a dynamic data construction pipeline that supports periodically downloading recent news and events, and automatically generates the most recent test split. This allows us to evaluate any newly released model in a contamination-free manner as we can always construct a test split later than its knowledge cutoff date. MIRAI comprehensively evaluates the agents\u2019 capabilities in three dimensions: 1) autonomously source and integrate critical information from large global databases; 2) write codes with both domain-specific APIs and libraries for tool-use; and 3) jointly reason over historical knowledge from diverse formats and timespan to accurately predict future events. Through comprehensive evaluation, we establish a reliable benchmark for assessing the capabilities of LLM agents in forecasting international events and contribute to the development of more accurate and trustworthy models for international relation analysis.",
    "keywords": [
        "LLM Agents",
        "Temporal Forecasting",
        "Tool Use"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We introduce MIRAI, a novel benchmark designed to systematically evaluate LLM agents as temporal forecasters to predict international events.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=gzzX4ZeErx",
    "pdf_link": "https://openreview.net/pdf?id=gzzX4ZeErx",
    "comments": [
        {
            "summary": {
                "value": "The paper presents MIRAI, a benchmark designed to assess the performance of LLM agents in predicting international event relations. It emphasizes APIs to enable LLM agents to utilize different tools, a refined GDELT event database, and a dynamic data construction pipeline to ensure contamination-free test sets, aiming to provide a reliable standard for assessing the capabilities of LLM agents in forecasting international events."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "\u2022\tClarity and Structure: The paper is clearly written and easy to follow, which facilitates understanding and replicability.\n\u2022\tContamination-Free Test Sets: The focus on providing contamination-free test sets is a significant strength, ensuring unbiased performance evaluation. Regular updates to the test sets also add to its robustness and relevance over time, when newer models should be evaluated.\n\u2022\tInsightful Visuals: Figure 2 offers a valuable overview of the distribution of relation types within the dataset.\n\u2022\tDetailed Analysis: The authors provide an in-depth result analysis across diverse categories, such as action order and accuracy across different relation types"
            },
            "weaknesses": {
                "value": "1.\tAbsence of Baseline Comparisons: As a benchmark paper, a comparison with simple (heuristic) baselines is critical to contextualize the agents performance in the MIRAI benchmark. The lack of such comparisons with simple baselines is a notable drawback, reducing the ability to assess the evaluated agents true effectiveness.  See suggestion 3).\n2.\tFocus Solely on Relation Prediction: The exclusive focus on relation forecasting seems narrow. Incorporating link prediction tasks (i.e., predicting specific subjects and objects for given queries) would align it better with existing TKG forecasting approaches and provide a more comprehensive benchmark. See question 1)\n3.\tDoubts in Experimental Setting when comparing to \"traditional\" models: \no\tThe \"traditional\" models that you compare to are a bit old, which may not reflect current advancements in temporal knowledge graph forecasting. Inclusion of more recent models, such as TiRGN (Time-Guided Recurrent Graph Network with Local-Global Historical Patterns for Temporal Knowledge Graph Reasoning), would enhance the relevance of comparisons. \no\tI have some doubts about the evaluation on these models, see question 5. For example, why finetune it only until 2023-06, even though some agents have more recent cutoff-dates. \n\n1.\tIntegrate Human Performance in Main Paper: Including the presented human forecasting performance in the main paper (currently in Appendix G.4) would strengthen the benchmark's validation, given the strong performance of human evaluators on most metrics.  I strongly suggest to include the human study in main paper.\n2.\tCondense the Appendix: I suggest to potentially not include the full API specification and implementation, and README of the github repo, but instead a link to the github repo containing these. This could streamline the paper, allowing readers to focus on MIRAI's core contributions.\n3.\tInclude a simple Baseline for Comparison: To provide a more meaningful comparison, I strongly suggest to include a simple/ heuristic baseline to compare the agents to. For example, a baseline that predicts the same relations as previously occurred, e.g. a modification from [1] to relation prediction. \n4.\tReferences: AutoGPT Documentation: this should at least contain a date and a link.\n [1] Gastinger, J., Meilicke, C., Errica, F., Sztyler, T., Schuelke, A., & Stuckenschmidt, H. (2024). History repeats itself: A Baseline for Temporal Knowledge Graph Forecasting. International Joint Conferences on Artificial Intelligence."
            },
            "questions": {
                "value": "1.\tFocus on Relation Forecasting: Why did the authors choose to focus solely on relation forecasting rather than including link prediction (predicting subjects and objects), which is more standard in TKG forecasting, and KG completion evaluation?\n2.\tMetric Selection: What motivated the choice of the current metrics, and why were more commonly reported metrics, like MRR and Hits@k, excluded?\n3.\tDataset Update Commitment: The authors commit to updating the dataset split monthly. How long is this commitment expected to last, and will it be sustained indefinitely?\n4.\tHigher Thresholds for Test Data: What is the rationale behind applying higher thresholds to the test data (100 daily mentions, 5 news articles) than to the training data? \n5.\tClarification on the \"traditional models\" experiments: Section 3.2 3): \no\tWhat do you mean with \"fine-tuned\" in this case? Did you train the models on the provided GDELT event data? \no\tAssuming that with \"fine-tune\" you mean train: Why call it fine-tuned? \no\tWhy do you fine-tune it only until 2023-6? considering that the query date is 2024-02, this is a very large gap. It seems also unfair, considering that e.g. for Llama-3.1-8B-Instruct the cutoff date is 2023-12. \no\tDo you apply RE-GCN in single-step or multi-step prediction mode, i.e. for predicting t, do you feed ground truth up until t-1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new LLM benchmark for short- and longterm forcasting international events, which is essentialy trying to predict relations between international parties. The general and fine-grained relations stem from an ontology, namely CAMEO. The authors define a think, act, oberserve loop, where the agent processes the query at hand and can give an answer right away, otherwise can act to get more information via coding and eventually observes the executed code. The benchmark equips agents to act by either single functions for data retrieval or code blocks for writing more complex code.\n\nThe results show that the task is challenging for LLMs by evaluating a set of baseline LLMs, where GPT-4o mini ends up performing the best. Next to the baseline LLMs, the authors also show the performance of fine-tuned methods from other benchmarks, underlining the benchmark is challenging. In addition, the authors run additional experiments and analyses, including observed generated code errors, boosting smaller architectures or investigating chosen action orders."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Novel relation prediction task formulation (wrt other benchmarks) \n- Sensibe LLM system design, enabling the to query for information or to write own code, and defining an agent-based behavioural loop\n- Sufficiently broad evaluation, including diverse analyses and also including baselines from other competing benchmarks\n- Empirical results are promising"
            },
            "weaknesses": {
                "value": "- Unclear if, beyond the integrated fine-tuned approaches from other works, the results for LLMs are superior if compared to other task formulations such as QA. Since LLMs have been used there as well, the question remains open if other types of system components or prompts significantly worked better in the past"
            },
            "questions": {
                "value": "- Did you also try a fine-tuned the baseline LLMs (given their are open-source)?\n- Which competitor in the related work overview would be strongest compared to your approach?\n- From the related work it becomes quite clear what the conceptual differences to prior LLM QA works are, but how much better does the new agent design actually work compared to more straightforward prior approaches?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces MIRAI, a benchmark to test LLMs for events forecasting. The article provides a detailed description of the process for generating predictions from LLMs which are based on recently extracted news and geopolitical events. This is based on a strategy called ReAct that includes the three steps of thinking, acting, and observing. An ontology called CAMEO is used for the geopolitical events analysis. Extensive experiments have been performed to compare MIRAI to other approaches and strategies. Different Large Language Models have been used to evaluate their performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-written.\n- Extensive experiments have been provided. These include comparing MIRAI process against other strategies and temporal reasoning \n  benchmarks and comparing base LLMs and smaller Language Models using MIRAI.\n- The results of these experiments seem promising."
            },
            "weaknesses": {
                "value": "- Only one concrete example is provided and discussed in the paper. More examples would facilitate a better understanding of the results.  For instance, examples that showcase different types of forecasting scenarios or that highlight the strengths of MIRAI could have been included.\n- The work should have been also compared with RAG-based (Retrieval-Augmented generation) approaches. It is unclear how MIRAI's approach differs from RAG-based methods.\n- Typos  line 088: \".. is able to better utilizes and benefits... \" ->  \".. is able to better utilize and benefit... \""
            },
            "questions": {
                "value": "How LLMs would perform if the news were used as a direct source instead of following the ReAct strategy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces MIRAI, a practical benchmark designed to predict future interactions between two countries. It constructs a dataset based on news articles and existing information extraction and representation tools, and then exposes an API that allows an LLM agent to interact with the different data sources. The authors' extensive experiments provide valuable insights into how LLMs perform on this task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1. The paper provides the code and data. Besides, it comes with an extensive description of the data and code, ensuring reproducibility and reuse.\n\nS2. The authors ran a lot of experiments testing different scenarios and configurations. They obtained very insightful and broad results about the task.\n\nS3. The paper is well illustrated, with many examples showcasing the performance of the systems. It is also well-written and easy to follow."
            },
            "weaknesses": {
                "value": "W1. The paper heavily relies on the appendix (the paper is 73 pages long!). As a reviewer, I am not supposed to read it, but in many cases, I had to. For example, the previous work is mostly in the appendix (two small paragraphs are in the main paper) and the human evaluation is in the appendix. \n\nW2. The technical contribution of the paper could be clearer. The dataset construction pipeline is standard and relies on many existing tools with additional cleaning. The baselines are also coming from previous works.\n\nW3. The scope of the task is very narrow: Predicting the future interaction between two countries.\n\nOthers/Typos\n\nO1. Some figures are hard to read, especially in black and white (2, 4, 5, 7, 8)\n\nO2. Human evaluation of the dataset is done in the appendix. Is it enough to only evaluate 51 events? Besides, 82% seems quite low to me.\n\nO3. The authors include the confidence intervals but do not discuss them. In many cases, there are overlaps between the baseline, making the conclusions unclear.\n\nO4. Adding the human evaluation in Table 2 is only one line, and it would be insightful.\n\nO5. Having the RAG baseline would be very insightful in knowing if one needs the API interface.\n\nT6. Line 420, problem citation (et al, 2024)\n\nO7. Figure 8.b: In general, red is used to show incorrect things. Here, it is counter-intuitive."
            },
            "questions": {
                "value": "Q1. Are the news articles freely available? Are there copyright issues?\n\nQ2. Line 260: Is balancing this way fair? Some months are naturally more active than others, and some relationships are more frequent than others.\n\nQ3. How does the number of parameters in the LLM impact the performance? In Table 2, GPT-3.5-turbo has more parameters than the other baselines but is competitive with GPT-4o-mini, which the oldness of GPT-3.5 can explain.\n\nQ4. For the Direct IO baseline, isn't a main issue outputting the output following the standard of the answer? Do you provide the possible relationships in any way?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}