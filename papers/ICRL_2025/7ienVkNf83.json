{
    "id": "7ienVkNf83",
    "title": "EReLELA: Exploration in Reinforcement Learning via Emergent Language Abstractions",
    "abstract": "The ability of AI agents to follow natural language (NL) instructions is important for Human-AI collaboration. \nTraining Embodied AI agents for instruction-following can be done with Reinforcement Learning (RL), yet it poses many challenges.\nAmong which is the exploitation versus exploration trade-off in RL. \nPrevious works have shown that NL-based state abstractions can help address this challenge. \nHowever, NLs descriptions have limitations in that they are not always readily available and are expensive to collect. \nIn order to address these limitations, we propose to use the Emergent Communication paradigm, where artificial agents learn an emergent language (EL) in an unsupervised fashion, via referential games. \nThus, ELs constitute cheap and readily-available abstractions. \nIn this paper, we investigate (i) how EL-based state abstractions compare to NL-based ones for RL in hard-exploration, procedurally-generated environments, and (ii) how properties of the referential games used to learn ELs impact the quality of the RL exploration and learning.\nWe provide insights about the kind of state abstractions performed by NLs and ELs over RL state spaces, using our proposed Compactness Ambiguity Metric.\nOur results indicate that our proposed EL-guided agent, entitled EReLELA, achieves similar performance as its NL-based counterparts without its limitations. \nOur work shows that RL agents can leverage unsupervised EL abstractions to greatly improve their exploration skills in sparse reward settings, thus opening new research avenues between Embodied AI and Emergent Communication.",
    "keywords": [
        "Emergent Communication",
        "Exploration",
        "Reinforcement Learning",
        "Abstraction",
        "Emergent Languages",
        "Natural Languages"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "This paper proposes to use abstractions guided by cheap Emergent Languages instead of expensive Natural ones to improve Exploration in Reinforcement Learning, showing similar performance despite differences in the kind of abstractions performed.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=7ienVkNf83",
    "pdf_link": "https://openreview.net/pdf?id=7ienVkNf83",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces an algorithm for augmenting reinforcement learning\nalgorithms with emergent communication-derived rewards that aid in tasks where\nexploration is a difficult part of the task.  This algorithm works by training\nagents to play a referential game with observations from the environment; the\nspeaker agent is then able to generate abstracted descriptions of the\nobservations for the RL agent which can encourage the agent to make new\nobservations that are not trivially different.  This algorithm is validated\nwith a handful of experiments and new metric \"Compactness Ambiguity Metric\" (CAM)\nwhich quantifies the way in which the speaker agent generates abstract\ndescriptions of the environment observations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The major strength of the paper is that the EReLELA algorithm itself is\npresented clearly and is well motivated by (1) the success of language-based\nabstraction methods for RL-based exploration and (2) the potential of emergent\ncommunication to produce learned, human language-like communication.  I think\nthis contribution is especially important on the emergent communication side of\nthings because the field lacks practical applications of emergent languages,\nand integrating it into an algorithm such as the one this paper introduces\ncould not only be effective in its own right but be an effective demonstration\nof the applicability of emergent communication methods."
            },
            "weaknesses": {
                "value": "The major weaknesses of this paper are two fold.  First, CAM is not clearly\ndefined and/or justified.  It seems like it is a key analytical tool in the\nempirical work of this paper, but its presentation did not give me a clear\npicture of what it was doing either in theory or in practice.\n\nThe second also relates to clarity, namely the lack of clarity of the\nexperiments themselves.  The graphics themselves are quite noisy and refer to\nsettings that are not described in detail (e.g., \"Agnostic STGS-LazImpa-10-1\nELA+AccThresh=90+Distr=256+UnifDSS\").  Since the experimental settings are not\nestablished at the beginning of the experiments section, I have very little\nidea as to how to interpret the empirical results.  Is there a baseline?  Which\none is the proposed method?  Which other settings am I supposed to compare it\nto?  Since I cannot easily answer these questions reading this section of the\npaper, I cannot determine what is learned about the proposed algorithm.\nI think it could be the case that the experiments themselves already contain\nthe requisite data for presenting an effective analysis of the algorithm, but\nthose things would need to be presented more simply and methodically."
            },
            "questions": {
                "value": "My main questions derive from the _Weaknesses_ section above: What are the\nexperiments showing?  Less is more when it comes to these graphics and\npresenting these results.  Regarding CAM: what exactly is the metric?  What are\nthe inputs and outputs, precisely?  Once this is clarified, is it the case the\nCAM is actually measuring the things we want to measure?  How do we validate\nthis?\n\nIt is possible I could be convinced to raise my review scores if the authors\nare able to streamline the presentation of the experiments (especially the\ngraphics) _and_ the results are still substantive enough for the paper's\nclaims.  While I appreciate the thoroughness of the introductory sections,\nI think they could be compact to make room for a more extensive explanation of\neach experiment.  If the CAM and experiments sections of the paper had as much\nclarity as the introduction, related work, and EReLELA sections, I easily\nrecommend acceptance.\n\n### Minor Comments\n\n- (Abstract) \"done ne\" -> \"done\"\n- (1 Introduction) Typo at the very beginning?\n- (1 Introduction) \"NLs oracle\" -> \"NL oracle\"\n- (1 Introduction) In a sentence or two, why is it necessary to use\n  language-based abstractions?  Wouldn't it be easier to represent things as,\n  say, an embedding or more formal structure?  (I have an inclination as to\n  what the answer to this question is, but I think it should be touched on in\n  the text for clarity.)\n- (Line 055) \"NLs, that are\" -> \"NLs, which are\"\n- (Line 058) \"hard-exploration\" -> \"hard exploration\"\n- (Line 065) What does \"aligned by not similar to\" mean?\n- (Line 067) \"advantages _over_ their NL\"?\n- (Line 090) The discussion of intrinsic versus extrinsic reward is a little\n  unclear (partially on the writing level).  I can see what is being\n  communicated, but someone with slightly less RL background might have a more\n  difficult time.\n- (Line 105) This is a good distinction to make.\n- (Line 114) Extra space before \";\"\n- (Line 122) \"entail to good exploration\": Not sure what this means.\n- (Line 138) \"constraint\" -> \"constrain\"\n- (Line 160) Space after end quote\n- (Line 161) Use `\\citep`\n- (Line 216) Extra space before \",\"\n- (Line 228) Does \"may not be passed\" mean \"is not passed with a certain\n  probability\"?  The phrasing \"may not\" is not clear here since it makes it\n  sound like it is \"not allowed to be passed\".\n- (Line 276) This paragraph is difficult for me to follow.\n  - $i\\in[0,N-1]$ suggests that $i$ is a real number, but I believe it is\n    discrete.  Using $i\\in{0, 1, \\dots, N-1}$ would be clearer.\n  - What is $\\lambda_i$?\n  - What is a \"time interval threshold\"?\n  - Using pseudocode might be clearer here (I don't think I follow it enough to\n    say this for sure, though).\n  - (Sec 3.2) What is the input and output of CAM?  I get that it is creating\n    a discrete distribution based on utterances used to describe observations,\n    but what is the metric itself?  Is the distribution the metric itself or is\n    it the entropy or the divergence from some baseline metric?\n- While I appreciate explicitly naming the hypotheses, they are not stated with\n  enough clarity and precision to be testable.  That is, how do we know\n  precisely when the hypothesis as been validated or not?\n- (Sec 4.1, Fig 2) These are difficult to follow, especially with the colors\n  and the names which have not been well specified.  For example, I do not know\n  what \"shared\" or \"agnostic\" refers to in the architecture.\n  - Unless it is necessary, it would be good to reduce the number of\n    referential game settings that you report so as to minimize confusion.\n- The \"natural language\" baseline should be called a \"synthetic language\" since\n  it is just programmatically generated and not gathered/derived from human\n  language in a meaningful way.\n- The different text colors for the experimental settings is a bit distracting.\n  I think it would be better to come up with simple, easy-to-remember names for\n  each setting and use those without worrying about colors (aside from the\n  lines/legend on the plots themselves."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors hypothesize that emergent languages can benefit RL agent exploration, to the same extent as expensive natural lanaguge descriptions. They propose a method to learn such emergent languages via reference games to induce intrinsic rewards jointly with the RL objective (EReLEA). They provide evidence that the learned emergent language is as useful, even more compact than natural language oracles."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Interesting method. The proposed CAM metric made a decent attempt at measurnig the quality of abstractions as far as I can understand its definition.\n2. Contextualization of the problem is clearly articulated in sec 1 and 2.\n3. The analysis about Zipf's Law of Abbreviation and the learned emergent langauge is insightful.\n4. The ablation studies seem thorough (if only I can interpreate how they precisely differ in context)."
            },
            "weaknesses": {
                "value": "My primary concern is presentation quality. Improved clarity can significantly benefit readability of this paper, as well as my understanding of the main method. \n1. Typo/abbreviation mistakes: line 3 of the abstract \"be done ne(?) with Reinforcement Learning\", start of the first paragraph of introduction, line 42 \"in effect\" (?), single quotes in line 43, \"it dynamics\" -> \"its dynamics\" in line 98, and so on.\n2. What's the superscript -1 on line 271?\n3. Figure readability is sadly discounted by low resolution, small font size, and the lack of in-figure legends. Personally I find it hard to parse the results without clear legend names and matching color coding, even with captions.\n4. Undefined H3.1 and H3.2 in lines 337-338\n5. $\\beta_1$ and $\\beta_2$ in Figure 2 captions seem out of blue. Are they defined anywhere in the main text? Why do they imply \"shared\" and \"anogostic\"? Perhaps a table comparing configurations of parallel runs? \n6. I appreciate the intuition, but I struggle to understand the formulation of the proposed CAM in sec 3.2. I think neither eq 4 nor the relative ambiguity of a language are CAM, but I cannot find exactly how CAM is computed in the main paper."
            },
            "questions": {
                "value": "1. What is the precise definition of CAM?\n2. How do parallel experiments (i.e., different curves in Figure 2) differ exactly?\n\nI am happy to raise the score if these questions are addressed with clarity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates to what extent referential games, and the resulting emergent language abstractions, can be used to derive intrinsic rewards in hard exploration problems of reinforcement learning agents."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The research question of the paper is creative: can emergent language abstractions be used to help RL agents in hard exploration problems."
            },
            "weaknesses": {
                "value": "A) I don\u2019t see empirical gains of using emergent language abstractions: Looking at Figure 2, it seems to me that the performance of the natural language abstraction agent (gray) and the emergent language abstraction agent (green) are within noise levels of each other. Thus, I find it difficult to conclude from the experiment that emergent language abstractions are important, in particular since both the natural language abstraction agent and the emergent language abstraction agent are using count based exploration terms as well. These should be ablated.\n\nB) Even if there were empirical gains, I would want to see a comparison to state-of-the-art intrinsic reward methods for hard exploration problems based on natural language abstractions to see the point empirically proven that emergent language abstractions are supposedly preferable. In particular, I would expect comparisons to \n- Mu et al. Improving Intrinsic Exploration with Language Abstractions. NeurIPS 2022. https://doi.org/10.48550/arXiv.2202.08938\n- Klissarov et al. (2023). Motif: Intrinsic Motivation from Artificial Intelligence Feedback. arXiv. https://doi.org/10.48550/arXiv.2310.00166\n- Zhang et al. OMNI: Open-endedness via Models of human Notions of Interestingness. ICLR 2024. https://arxiv.org/abs/2306.01711 \n\nC) Related to the above, I believe the authors need to evaluate on harder exploration problems, such as MiniGrid\u2019s KeyCorridor-S3-R3 and MultiRoom-N10-S10, or MiniHack (Samvelyan et al. MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research. NeurIPS 2021. https://doi.org/10.48550/arXiv.2109.13202). Moreover, I would like to see experiments beyond gridworlds, e.g., on Vizdoom (c.f. Henaff et al. Exploration via Elliptical Episodic Bonuses. NeurIPS 2022. https://doi.org/10.48550/arXiv.2210.05805).\n\nD) I believe it would be important to add a tabula-rasa RL agent, as well as only RND baseline to Figure 2.\n\nE) p8 Figure 3 looks to me like the experiments did not finish in time."
            },
            "questions": {
                "value": "- Abstract: It\u2019s not entirely clear to me what limitations of NL-based counterparts you are referring to here.\n- p1: \u201cRND \u2026 which can be difficult to deploy\u201d \u2014 Why are they difficult to deploy? RND is a very straightforward intrinsic reward method. \n- From Figure 1 it looks like the intrinsic reward is only generated from the speaker. Why shouldn\u2019t one also derive the intrinsic reward from the listener?\n\nComments\n- p4 Figure 1 is too small to read. Same goes for other figures in the paper (e.g. Figure 2)\n- p7 Figure 2 caption: explain the different methods variants in more detail.\n\nWhat the authors would have to demonstrate to see an improved rating from me:\nDemonstrate clearer gains of emergent language abstractions over natural language abstractions (A) on harder exploration problems (C) while also comparing to state of the art natural language abstraction methods (B) and adding tabula-rasa RL, as well as RND, baselines (D). Present results of finished experiments where each method is ran for the same number of steps (E)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}