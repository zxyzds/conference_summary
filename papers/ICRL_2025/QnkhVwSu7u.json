{
    "id": "QnkhVwSu7u",
    "title": "ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language Models for Interpretable Reward Design in Robotics",
    "abstract": "Reinforcement learning (RL) has demonstrated compelling performance in robotic tasks, but its success often hinges on the design of complex, ad hoc reward functions. Researchers have explored how Large Language Models (LLMs) could enable non-expert users to specify reward functions more easily. However, LLMs struggle to balance the importance of different features, generalize poorly to out-of-distribution robotic tasks, and cannot represent the problem properly with only text-based descriptions. To address these challenges, we propose ELEMENTAL (intEractive LEarning froM dEmoNstraTion And Language), a novel framework that combines natural language guidance with visual user demonstrations to align robot behavior with user intentions better. By incorporating visual inputs, ELEMENTAL overcomes the limitations of text-only task specifications, while leveraging inverse reinforcement learning (IRL) to balance feature weights and match the demonstrated behaviors optimally. ELEMENTAL also introduces an iterative feedback-loop through self-reflection to improve feature, reward, and policy learning. Further, ELEMENTAL reward functions are interpretable. Our experiment results demonstrate that ELEMENTAL outperforms prior work by 22.7\\% on task success, and achieves 45.0\\% better generalization in out-of-distribution tasks, highlighting its robustness in LfD.",
    "keywords": [
        "Interactive Robot Learning",
        "Inverse Reinforcement Learning",
        "Feature Abstraction",
        "Vision-Language Models"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=QnkhVwSu7u",
    "pdf_link": "https://openreview.net/pdf?id=QnkhVwSu7u",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes \"ELEMENTAL\", an approach for integrating user demonstrations in language-model-based reward specifications such as in EUREKA. The paper argues that such demonstrations could alleviate the ambiguity of language task specifications. The approach has 3 phases. The first one involves prompting the VLM with text and demonstrations to obtain an executable feature function. In this stage, the demonstration is represented as either a superimposed image or four keyframes. In the second stage, the aim is to learn a reward function that is linear in the features and tries to match the demonstrations, and in an inner loop updates the policy via PPO and using the obtained reward. In the last stage, the agent reflects on its feature function using the discrepancy of the feature counts in the dataset and policy-generated trajectories. The paper includes several simulation-based evaluations of the method and mainly compares it to EUREKA as a baseline. The results show a consistently significant improvement in performance in comparison to EUREKA."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- the paper is very well written and self-contained. It's also quite a smooth read.\n- the proposed approach is quite interesting and using demonstrations to alleviate language ambiguity is a needed step for automatic reward design.\n- the experiments in simulation are quite extensive and the results support the main claims of the paper.\n- the experiments include important ablations of some aspects of the method."
            },
            "weaknesses": {
                "value": "- the paper lacks motivation for the choice of an IRL-based approach (with reward linear in the features) to include visual inputs as opposed to following the EUREKA-style approach.\n- the paper is only validated in simulation. It would be interesting to see whether these approaches could alleviate the reward engineering usually required to handle real-world problems such as jerky motions and unsafe behaviors. Just one real-world experiment with a robot would suffice. For instance, looking at your simulation environments, an experiment with either the Franka, ANYmal, or ShawdowHand would be a great addition to the evaluation. This could also be a sim-to-real transfer experiment.\n- the paper lacks ablations of the various normalization steps (equations 6 and 7).\n- the paper does not include any information on the amount of time needed for a full run of the algorithm on the various tasks. I think this aspect is very important for readers to decide whether the approach is feasible for their applications and for future methods to improve upon this. I would suggest comparing the wallclock time of running your method and EUREKA.\n\nI am willing to raise my score if these points are properly addressed."
            },
            "questions": {
                "value": "- how is keyframe selection performed for the manipulation tasks? This aspect is important to understand the assumptions you make (do you assume to have access to such keyframes from an oracle/user for each demonstration?) or do you have some method to get them (very important for reproducibility)?\n- In Table 2, is this the reward or success rate or what? why is it missing a standard deviation? why is there such a difference between ant original and ant with reversed observation? this difference might indicate that the statistical significance of the claims based on this table is questionable. Please update the paper (text and table caption) to make the metric clearer.\n- how important/necessary are the normalization steps?\n- ELEMENTAL without visual inputs is consistently worse than EUREKA, which brings the question of whether a different approach to reward design (not IRL-based) is more suitable for this kind of problem. Can you please discuss this?\n- intuitively, what would be the advantage of this method versus using a VLM itself as a success detector? success detectors like the ones presented in [1] could in theory also receive textual instructions and demonstrations. I agree that they would give out a sparse reward signal, or if they are modified to produce denser rewards this value would be quite uncalibrated, but I am interested in the authors' opinion on this matter.\n\n[1] Du, Yuqing, et al. \"Vision-language models as success detectors.\" arXiv preprint arXiv:2303.07280 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel approach to reward tuning that combines language goals and visual user demonstration inputs with Vision-Language Models (VLMs) to address ambiguities inherent in language-only reward tuning. The method consists of three stages. In the first stage, simulator code, language-based goal descriptions, and images of user demonstrations are provided to a VLM, which then generates code to calculate features relevant to the task. In the second stage, a reward function and policy are learned online using maximum-entropy Inverse Reinforcement Learning (IRL). Finally, in the third stage, rollouts from the policy are used to compute the discrepancy between the feature counts in the user demonstrations and those in the actual rollouts. This discrepancy is fed back into the VLM for iterative refinement, alternating between the second and third stages to optimize performance. Experiments conducted on Isaac Gym tasks demonstrate that the proposed method achieves superior performance compared to state-of-the-art (SOTA) language-only reward tuning and IRL methods that do not leverage VLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed method leverages VLMs to propose features relevant to the task rather than generating the entire reward function. This approach minimizes the risk of overfitting to environments encountered during VLM/LLM training and reduces issues with low code execution rates, as confirmed by the paper\u2019s experimental results.\n- The proposed framework takes in both a language goal and visual demonstrations, effectively addressing the ambiguities associated with using only one type of input."
            },
            "weaknesses": {
                "value": "- The requirement for MDP environment code as an input limits this method to simulated environments. In real-world applications, this would require explicitly specifying all relevant objects and dynamics, which could be impractical or infeasible.\n- To what extent does the assumption that the reward is a weighted sum of the feature vectors limit the expressiveness of the reward function? This limitation excludes more complex functional forms, such as exponentials, logarithmic functions, or features in the denominator, potentially limiting the method's ability to capture nuanced task-specific details.\n- When using a superimposed image as a visual demonstration in tasks like navigation, there is an inherent ambiguity in capturing the temporal direction of actions. For tasks where superimposed images are unsuitable, the method selects approximately four keyframes from the demonstration, introducing an additional need for keyframe identification.\n- The method may struggle with highly complex simulation environments, as it requires the entire MDP environment code as input."
            },
            "questions": {
                "value": "- How does the allowance of up to 3 attempts impact the method's effectiveness? For tasks where the method fails after 3 attempts, would increasing this threshold lead to successful code executions, or would these tasks likely fail at this stage regardless of additional attempts?\n- How does the proposed reward in the experiments compare with the ground truth reward? Would be good to see comparisons and analysis on this\n- In Table 1, over how many iterations is the proposed method trained? Does each iteration consistently lead to performance improvements? It would be helpful to see reward curves plotted against iteration count to better understand the effectiveness of the reflection stage."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Using large language models (LLMs) for specifying reward functions has shown success in reinforcement learning. However, existing methods like EUREKA describe tasks and rewards only through language. To remedy this, ELEMENTAL combines learning from demonstration (LfD) and LLMs for reward engineering to extract features, learn a policy and reward, and iteratively refine features. ELEMENTAL leads to better generalization and performance than existing methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper makes a clear extension from EUREKA, by incorporating visual inputs. In addition, it proposes a novel framework of feature extraction, learning, and reflecting. In particular, I find the self-reflection loop to be compelling\n\nThe diagram and writing are clear."
            },
            "weaknesses": {
                "value": "The authors mention that ELEMENTAL helps \u201calign robot behavior with user intentions better\u201d and that EUREKA allows humans to \u201cinterpret and interactively refine the robot\u2019s behavior\u201d and is more \u201cuser-aligned\u201d (line 144). There are no experiments or further discussion of this, and it is not explored in this work.\n\nFurther discussion on the effect of self-reflection, such as the types of features that are discovered through self-reflection, would be interesting."
            },
            "questions": {
                "value": "1. What is the training time needed for these tasks?\n2. Why is Peng et al. (2024b) not included as a baseline?\n3. How does this work compare with reinforcement learning from VLM rewards?\n4. A more thorough appendix would be useful. For instance, what dimension are the features? What are example features?\n5. What is the environment state space for the tasks (line 248)?\n6. In Table 2, what are the differences in the EUREKA and ELEMENTAL implementations? For example, are the VLMs / LLMs different, and might that account for the reduced generalization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper framework that can utilize a combination of fast simulation, demonstration, and VLMs. The main idea is to (i) use VLMs to useful state features from environment code and image from demonstration, (ii) run IRL algorithm to learn reward and the policy from state features , and (iii) use the state feature matching counts as a reflection metric to compare how the learned policy is close to demonstration. The main contribution of this paper lies in investigating a way to utilize demonstrations and VLMs for improving IRL, which seems a promising direction to pursue."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Idea is intuitive and direction seems promising\n- Improved performance from incorporating VLMs into IRL pipeline"
            },
            "weaknesses": {
                "value": "Overall, I like the idea of this paper but it's missing too many results/analysis/discussion to support why & how the method works and what happens during the training.\n\nThe main weakness of this paper is that it's only reporting the numbers in main table and does not provide results that can help readers understand how the proposed idea works and that can support the claims made in the paper. For instance,\n- Despite the claim, the paper is missing any result or discussion/analysis on the interpretability of rewards, and how it is helpful\n- Despite the proposed framework has multiple iterations of training, it's not clear how the performance changes across the iteration, how the *interpretability* of reward improves.\n- Is VLM really understanding what demonstration is? What would happen if the VLMs receive sub-optimal data from random exploration in the same tasks? What would happen if you give demonstrations in a different way?\n- How crucial it is to give demonstrations in visual observation? what would happen if you give demonstrations as a sequence of states to LLMs instead of using VLMs?\n- It's missing analysis/experiments that investigate the effect of VLM choices on the effectiveness of the framework. It could be also nice to include results that show how the method is sensitive to the choice of VLMs. For instance, how good the VLMs should be good to enable this framework to work? Would open-source models be okay? \n\nAlso, experiments are missing some details and baselines:\n- Details on experimental setup is not clear. Are all the methods using the same resources for training? It's not clear if all the models are trained until convergence.\n- It seems to me that BC performance is a bit weak but it's difficult to understand why as there are not that many experimental details. What would be the performance if we use a powerful BC algorithm such as DiffusionPolicy? \n\nChi, Cheng, et al. \"Diffusion policy: Visuomotor policy learning via action diffusion.\" The International Journal of Robotics Research (2023): 02783649241273668."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}