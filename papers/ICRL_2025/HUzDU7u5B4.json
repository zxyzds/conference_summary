{
    "id": "HUzDU7u5B4",
    "title": "On-Policy Fine-grained Knowledge Feedback for Hallucination Mitigation",
    "abstract": "Hallucination occurs when large language models (LLMs) exhibit behavior that deviates from the boundaries of their knowledge during the response generation process.\nPrevious learning-based methods focus on detecting knowledge boundaries and finetuning models with instance-level feedback, but they suffer from inaccurate signals due to off-policy data sampling and coarse-grained feedback.\nIn this paper, we introduce \\textit{\\b{R}einforcement \\b{L}earning \\b{f}or \\b{H}allucination} (RLFH), a fine-grained feedback-based online reinforcement learning method for hallucination mitigation.\nUnlike previous learning-based methods, RLFH enables LLMs to explore the boundaries of their internal knowledge and provide on-policy, fine-grained feedback on these explorations.\nTo construct fine-grained feedback for learning reliable generation behavior, RLFH decomposes the outcomes of large models into atomic facts, provides statement-level evaluation signals, and traces back the signals to the tokens of the original responses.\nFinally, RLFH adopts the online reinforcement algorithm with these token-level rewards to adjust model behavior for hallucination mitigation.\nFor effective on-policy optimization, RLFH also introduces an LLM-based fact assessment framework to verify the truthfulness and helpfulness of atomic facts without human intervention.\nExperiments on HotpotQA, SQuADv2, and Biography benchmarks demonstrate that RLFH can balance their usage of internal knowledge during the generation process to eliminate the hallucination behavior of LLMs.",
    "keywords": [
        "Hallucination;Large Language Model;"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=HUzDU7u5B4",
    "pdf_link": "https://openreview.net/pdf?id=HUzDU7u5B4",
    "comments": [
        {
            "summary": {
                "value": "The paper, proposes a novel approach called RLFH (Reinforcement Learning for Hallucination) to reduce hallucinations in Large Language Models (LLMs) by fine-grained, token-level  reward. RLFH leverages an online reinforcement learning framework and was tested across multiple datasets and showed improved fact-based accuracy compared to existing hallucination mitigation strategies."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **Fine-Grained Feedback:** RLFH's use of atomic fact decomposition and token-level rewards enables precise correction of hallucinated responses, surpassing the efficacy of previous, more coarse-grained approaches.\n\n2. **Automated Fact Assessment:** The model employs an LLM-driven feedback mechanism to evaluate truthfulness and informativeness, potentially eliminating human intervention and thus increasing scalability."
            },
            "weaknesses": {
                "value": "1. **Unsuitable Baselines:** The paper selects \"advanced aligned models\" such as Zephyr, Orca, Llama2 Chat, and Vicuna 1.5 as baselines (line 311). However, this comparison is unsuitable, as these models differ not only in alignment methods but also in their underlying architectures and training parameters. For a fair comparison, the authors should have evaluated different alignment and hallucination mitigation methods on a consistent model architecture rather than comparing across varied pre-trained models. \n\n2. **Unsignificant effect:** Despite the methodological novelty, the empirical results indicate only marginal improvements in factuality compared to some baseline methods. The reported gains, although statistically significant, may not justify the increased computational complexity and reinforcement learning setup of RLFH, especially given the modest FactScore improvements (e.g., +2.0% on some benchmarks). This raises questions about the practical advantages of RLFH over simpler, less resource-intensive approaches.\n\n3. **Dependence on External Knowledge:** The model\u2019s reliance on external datasets and existing knowledge boundaries raises concerns about its adaptability in real-world scenarios where such data may not be readily available or up-to-date."
            },
            "questions": {
                "value": "Could the authors replace the current baseline models with a consistent model architecture and compare only different alignment and hallucination mitigation methods?\n This would provide a clearer evaluation of RLFH\u2019s effectiveness by removing confounding factors related to model architecture and pre-training differences."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Reinforcement Learning for Hallucination (RLFH), an innovative approach aimed at reducing hallucination in large language models (LLMs). It focuses on instance-level feedback and relies on off-policy data sampling, RLFH provides fine-grained, on-policy feedback at the token level. This method decomposes responses into atomic facts and applies statement-level evaluations to assign feedback to specific tokens in the output, guiding the model to reduce hallucinations. To facilitate on-policy optimization, RLFH includes an automated fact assessment framework to evaluate the truthfulness and helpfulness of these facts. Experimental results on HotpotQA, SQuADv2, and Biography datasets show that RLFH effectively mitigates hallucination in LLMs by balancing their use of internal knowledge during response generation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.RLFH provides a novel approach to feedback by breaking down outputs into atomic facts and assigning token-level rewards, which allows for a more precise correction of hallucination errors.\n\n2.By using on-policy data sampling, RLFH ensures that the feedback provided is more aligned with the model\u2019s real-time behavior, improving the relevance and effectiveness of the adjustments made to the model.\n\n3.the experimental results have shown the effectiveness of the proposed approach."
            },
            "weaknesses": {
                "value": "1.the proposed method is not a fully reliable on-policy method, as the feedback is provide by the LLM itself, not trustworthy feedbacks. Thus, it is hard to say after multi-turn iteration, if the model will be better, and maybe the wrong knowledge will accumulate.\n\n2.the compared baselines should involve RLHF-based methods, i.e, DPO and RLAIF. Although these methods are not specially designed for on-policy optimization, it is necessary to show how they will perform under this occasion, to verify the motivation in the introduction part.\n\n3.the writing of this paper needs to be polished. There are several typos: e.g., \"Turning\" -> \"Turing\""
            },
            "questions": {
                "value": "Please refer to the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents RLFH, a method that decomposes the output of large language models into atomic facts and validates them with reference documents. Then the sentence-level evaluation feedback is provided. After that, RLFH traces the feedback back to the original response and builds a token-level reward signal. Finally, an online reinforcement algorithm with these signals adjusts the model behavior to alleviate the illusion. The main contribution is to propose an online reinforcement algorithm based on fine-grained feedback signals and a non-manual, LLM-driven reward model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.The motivation of the paper is reasonable, and the problems to be solved are relatively important.\n\n2.The model adjusts the model itself based on the evaluation feedback of the results, rather than just modifying the results. The method is interesting.\n\n3.The analysis of the experimental results is detailed. Both the overall summary analysis and some detailed research are given."
            },
            "weaknesses": {
                "value": "1.The paper fails to fully solve the problem it raises. Evasive ignorance, for example, is a type of illusion that can't be detected or changed.\n\n2.The model simply gives feedback on the correctness of each sentence based on the original answer. If the original answer itself has information missing, the model will not be able to correct and give feedback.\n\n3. Please discuss the potential challenges or benefits of extending their method to other LLMs."
            },
            "questions": {
                "value": "For page 5 line 267: \u201cTable ??\u201d , please specify which table you are referring to."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}