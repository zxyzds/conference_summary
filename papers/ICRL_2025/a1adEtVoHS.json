{
    "id": "a1adEtVoHS",
    "title": "TextSquare: Scaling up Text-Centric Visual Instruction Tuning",
    "abstract": "Text-centric visual question answering (VQA) has made great strides with the development of Multimodal Large Language Models (MLLMs), yet open-source models still fall short of leading models like GPT4V and Gemini. A key contributing factor to this disparity is the absence of extensive, high-quality instruction tuning data. To this end, we introduce a new approach for creating a massive, high-quality instruction-tuning dataset, Square-10M, generated by leveraging the versatile multimodal capabilities of closed-source MLLMs. The data construction process, termed Square, consists of four steps: Self-Questioning, Answering, Reasoning, and Evaluation. Our experiments with Square-10M led to three key findings: 1) Our model, TextSquare, considerably surpasses open-source previous state-of-the-art text-centric MLLMs and sets a new standard on OCRBench (62.2%). It even outperforms top-tier models like GPT4V and Gemini on six out of ten text-centric benchmarks. 2) We demonstrate the importance of VQA reasoning data in offering comprehensive contextual insights for specific questions, which not only improves accuracy but also substantially mitigates hallucinations. Specifically, TextSquare scores an average of 75.1% across four general VQA and hallucination evaluation datasets, outperforming previous state-of-the-art models. 3) Notably, the phenomenon observed in scaling text-centric VQA datasets reveals a vivid pattern: an exponential increase of instruction tuning data volume is directly proportional to the improvement in model performance, thereby validating the necessity of the dataset scale and the high quality of Square-10M.",
    "keywords": [
        "Text-Centric Multimodal Large Language Model",
        "Visual Instruction Tuning",
        "Scaling Relationship"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=a1adEtVoHS",
    "pdf_link": "https://openreview.net/pdf?id=a1adEtVoHS",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes Square-10M, a large-scale text-centric visual instruction tuning dataset constructed using closed-source MLLMs, and demonstrates its effectiveness through TextSquare, an 8.6B parameter model achieving competitive performance on various benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Square-10M represents a significant scale for text-centric VQA instruction tuning. \n\nThe data collection strategy provides a clear pipeline for generating high-quality instruction tuning data through self-questioning, answering, reasoning, and evaluation.\n\nThe paper offers valuable insights into scaling laws for text-centric VQA instruction tuning, showing logarithmic relationships between data scale and model performance."
            },
            "weaknesses": {
                "value": "Limited Technical Novelty:\n\nThe core methodology primarily combines existing techniques and reliance on closed-source model. The main finding that more high-quality instruction data improves performance is expected. \n\nThere are lack of comparison with automatically generated OCR datasets such as webpages screenshots and pdf parsing library. \n\nThe evaluation benchmark. The author-reported numbers come from the December 2023 Gemini technical report. However, GPT-4V and Gemini Pro have gone through multiple iterations, with more recent performance numbers reported by Molmo (Deitke et al. 2024) showing significantly different results:\n\nDocVQA: Molmo reports 93.1% for Gemini vs paper's 88.1%\nAI2D: Molmo reports GPT-4V at 89.4% and Gemini Pro at 94.4%, compared to the paper's 78.2% and 73.9% respectively"
            },
            "questions": {
                "value": "This has several important implications:\n\nEvaluation:\n\nThe evaluation section needs updating to reflect current state-of-the-art performance. The paper's claims about beating closed-source models need to be reassessed, given the updated numbers\nGPT-4o results are notably absent from the comparison.\n\n\nInteresting Methodological Insight:\n\nGiven the timeline, the authors seem to have newer version of Gemini for data generation while comparing against older benchmark numbers creates an interesting dynamic\nThis might explain how TextSquare achieved superior results - it's effectively being trained on distilled knowledge from a more advanced version of Gemini than the one it's being compared against. One possibility is that Gemini had also went through data iteration and improved its OCR heavy data. \nThis observation raises important questions about the role of model iteration and knowledge distillation in advancing open-source models. I think paper also should hit the snapshot version of the model and explicitly mention their version to enhance reproducibility."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, a prompting strategy for generating text-centric visual instruction-tuning data is proposed. The authors used a commercial VLM (Gemini) to generate a large amount of data, which was subsequently used to fine-tune an open-source VLM, achieving considerably strong performance across a wide range of text-centric VLM benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The model performance is strong. The paper presents a feasible solution for achieving closed-source GPT-4V level performance on text-centric benchmarks with an open-source model.\n- The paper scaled the data to 10M and demonstrated its effectiveness by visualizing the scaling trend."
            },
            "weaknesses": {
                "value": "This is a solid paper if evaluated as an engineering report. However, as an ICLR submission, it falls short in terms of novelty and scientific contributions, and the ablations are insufficient.\n\n - Weak Novelty: The approach is essentially self-instruction and knowledge distillation. The proposed prompting methodology (Square) appears to be a straightforward implementation and lacks ablations to prove the effectiveness of each step.\n - Limited Scientific Findings: Although the abstract lists some \u201cfindings,\u201d they primarily summarize strong performance, and it's somehow expected that adding chain-of-thought data could improve performance, and the log-linear relationship between model performance and training data volume can be observed -- they have already been demonstrated in NLP. It\u2019s not particularly surprising to observe this in text-centric VQA.\n - Use of Gemini: The comparison between different VLMs in section 3.5 is unclear and seems insufficient.\n    - It\u2019s unclear what \u201c10 questionnaires\u201d means. Does it imply that 10 participants labeled the 4x1k QA sets generated by the 4 LLMs?\n    - The definition of \u201cmeaningful\u201d is not clearly stated in the questionaire.\n    - In \u201cconsidering the time cost, price, and quality of the data generated, Gemini-pro is our best choice\u2026,\u201d details on the time cost for each VLM and their respective prices should be included. Clarification on how the trade-off was made is needed.\n - API Cost and Dataset Availability: The paper used a commercial VLM (Gemini) to generate a substantial amount of data, but it does not detail the API cost, which is likely to be significant. Additionally, there are no plans to release the constructed dataset, limiting its contribution to the community.\n- Typo: \n    - Line 037: visual question-answering<missing space>(VQA) \n    - Line 103: training convergence loss? What is it?"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Recent research in multimodal large language models has greatly advanced text-centered VQA, with closed-source models such as GPT4V and Gemini outperforming open-source models. Challenges include insufficient instruction-adjusted data and domain inconsistencies. This paper describes the Square strategy for creating large, high-quality datasets (Square-10M) for instruction adjustment.Square includes the steps of self-questioning, answering, reasoning, and evaluating, which improves data quality and reduces illusions. TextSquare based on Square-10M outperforms existing models and demonstrates that inference data improves VQA performance. Experiments show that extending instruction tuning data reduces convergence loss and improves model performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) This work constructs a high-quality dataset, Square-10M, performs full open-source data collection, and generates it using innovative build links;\n2) The dataset makes current open source models better on a variety of benchmarks, some of which are comparable to closed-source multimodal macromodels\n3) The correlation between data size, loss of convergence, and model performance for text-centered VQA instruction tuning is demonstrated through thorough experiments"
            },
            "weaknesses": {
                "value": "1) The construction of the dataset in this work relied too much on the Gemini model and did not demonstrate the effectiveness of this construction logic on other models;\n2) The construction step of the dataset, the Square strategy, did not strike the reviewers as novel, preferring it to be a variant step similar to Chain of Thoughts;\n3) The magnitude of the dataset is relatively large, and the balance between the overhead of the construction costs and the benefits derived is open to discussion."
            },
            "questions": {
                "value": "1) Provide a link to a demo of this dataset, or partially open source it, and add the impact of different magnitudes of the dataset on the performance of the model;\n2) Add experiments comparing other dataset construction ideas to demonstrate the uniqueness and necessity of the Square method with examples;\n3) Demonstrate the same stability and superiority of the construction method on other callable commercial MLLMs (besides Gemini). Propose experiments with smaller subsets of the data to demonstrate the necessity of the full dataset size.\n4) Layout of the paper to be adjusted"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper builds a large-scale instruction-tuning dataset for text-centric VQA by distilling proprietary models (GPT-4V, Gemini). First, 3.8M text-rich images are manually collected, containing charts, documents, slides, tables, screenshots, street views, receipts, etc. Then, proprietary MLLMs are used to generate question-answer pairs for each image, and to provide a rationale for the answers. The MLLMs are further used to assess the quality of the annotations with self-evaluation and consistency methods and to filter the data. After filtering, 9.8M question-answer pairs are kept. The generated Square-10M dataset is paired with in-domain datasets and used to finetune an open-source MLLM: InternLM-XComposer-2. The finetuned MLLM, TextSquare, is evaluated on a suite of text-centric benchmarks, outperforming existing open-source MLLMs while being competitive (and sometimes slightly outperforming) proprietary MLLMs. In addition, TextSquare outperforms existing methods on (discriminative) hallucination benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Improving the OCR capabilities of MLLMs is a relevant research direction given the current real-world applications of such models.\n* The paper is clear and easy to follow.\n* The efficacy of the dataset in finetuning MLLMs for text-centric VQA is thoroughly evaluated on a comprehensive suite of benchmarks.\n* The paper includes a comprehensive set of ablations on different subsets of the collected/generated data."
            },
            "weaknesses": {
                "value": "* The novelty of the paper is limited. For instance, previous works have also distilled proprietary MLLMs into large-scale datasets to finetune open-source models (e.g. [1]).\n* Proprietary models are used to generate synthetic data to finetune open-source models. This means the performance of finetuned models is upper bounded by that of proprietary models. In cases where TextSquare's performance surpasses that of proprietary models might be just due to better prompting.\n* Eliciting a rationale after providing the answer (figure 4) is not helpful to improve answer quality due to left-to-right generation.\n* The design choices for the model architecture and finetuning recipe are not justified or ablated.\n* It is expected that a model trained with 10M more datapoints performs better, so it is not surprising that TextSquare outperforms InternLM-XComposer-2.\n\n[1] Liu, Haotian, et al. \"Visual instruction tuning.\" Advances in neural information processing systems 36 (2024)."
            },
            "questions": {
                "value": "* How can TextSquare achieve better performance than proprietary models if it was trained with synthetic data generated by them?\n* In line 268, what does the initial 82.6% accuracy value correspond to?\n* Missing relevant DocVQA datasets such as Docmatix [1].\n* Can authors provide any insights on why finetuning on text-centric VQA data helps reducing hallucinations?\n* Why does TextSquare's accuracy on InfoVQA differ between tables 1 and 3?\n\n[1] Lauren\u00e7on, Hugo, et al. \"Building and better understanding vision-language models: insights and future directions.\" arXiv preprint arXiv:2408.12637 (2024)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}