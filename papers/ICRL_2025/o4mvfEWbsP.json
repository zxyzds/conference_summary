{
    "id": "o4mvfEWbsP",
    "title": "Sparse Hyperspectral Band Selection Based on Expectation Maximization",
    "abstract": "Band selection is crucial in spectral imaging, as it involves choosing the most relevant bands from large hyperspectral datasets to retain essential information while reducing the burden of data transmission and analysis. Addressing this need, we introduce a novel method for band selection that utilizes an Expectation Maximization algorithm to facilitate selection through the sparsification of spectral band importance. Our method enhances sparsity effects and effectively delineates the relationships between spectral bands during the sparsification process. Supported by thorough theoretical analysis and experimental validation on public datasets, our approach has proven to be both robust and practical. Compared to other sparsification methods, it not only excels in achieving significant sparsity effects but also demonstrates marked advantages in illustrating inter-band relationships. Our method delivers outstanding performance in band selection tasks and holds potential for broader applications in other sparsity-oriented contexts in the future.",
    "keywords": [
        "Hyperspectral Band Selection",
        "Sparse Learning",
        "Feature Selection",
        "Hyperspectral Image Classification",
        "EM Algorithm"
    ],
    "primary_area": "learning theory",
    "TLDR": "This paper proposes a sparse learning method for spectral band selection based on the EM algorithm, which effectively characterizes the relationships between spectral bands to achieve state-of-the-art performance.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=o4mvfEWbsP",
    "pdf_link": "https://openreview.net/pdf?id=o4mvfEWbsP",
    "comments": [
        {
            "comment": {
                "value": "**Positioning within the literature, including the above cited work.**\n\n**Re:** Thank you for your valuable feedback. You are correct, and we will include the cited references in our revised paper. Due to time constraints, we were unable to thoroughly review these papers, but after a general review, we found that these methods have not been applied in the context of deep learning. Furthermore, based on our research, there doesn't seem to be any prior work using the EM algorithm for sparse learning in deep network parameters. Therefore, we believe our method still holds pioneering significance in this regard. \n\nRegarding the inclusion of these methods in our experiments, we are uncertain whether they are directly applicable to the band selection framework. Please allow us some time to further assess their relevance, and if they can be integrated into our comparisons, we will definitely include them in the experiments."
            }
        },
        {
            "comment": {
                "value": "**Weaknesses: EM algorithm is out of date in the area of hyperspectral image band selection.**\n\n**Re:** Please sir tell me what is NEW! Did you know that many of the very latest advances can now be interpreted as the EM algorithm? In the field of weakly supervised learning, the EM algorithm continues to dominate!\n\n**1) Band selection refers to selecting important bands from hyperspectral images rather than selecting bands with high correlation from hyperspectral datasets.**\n\n**Re:** \nWe aim to find the most important combination of bands. We are not selecting the most correlated bands, but rather focusing on the most important combination. Therefore, our sparsification  method is based on band combinations.\n\n**2) The author points out that existing methods are not always accurate in assigning band importance. How can we prove whether this problem exists? How can you prove that your method can effectively solve this problem?**\n\n**Re:** We provide a theoretical proof in Appendix H demonstrating that sparse learning in deep learning-based band selection can more effectively represent band importance. Additionally, the effectiveness of our method is validated through experiments on three public datasets and significance testing, which further support the claim that our method addresses the issue of accurately assigning band importance.\n\n**3) The loss of L1 and L2 cannot produce stable sparse effects, and this issue should be illustrated with a diagram to help readers better understand.**\n\n**Re:** Please refer to Figure 2 in section 4.5.\n\n**4) Why use the EM algorithm to design the band selection method? How does your method solve the problem of unstable sparsity of L1 and L2, as well as the difficulty in describing the band relationships during the sparsity process? All of these need to be logically explained in the introduction.**\n\n**Re:**\nThe choice of the EM algorithm stems from our understanding of the problem, where we consider band selection as a weakly supervised learning problem, with the latent variables being the selected bands. As for the issue of stable sparsity, please refer to Section 3.4 for a detailed explanation. Regarding the advantages of our method in capturing inter-band relationships, we provide further insights in Section 3.6. \n\n**5) The method lacks a framework diagram, which should allow readers to clearly understand the steps of the method without reading the entire text.**\n\n**Re:** Our method primarily focuses on the derivation and proof of the loss function. The process itself is not complex\u2014training is performed using Equation 1, and after convergence, the sparse selected bands are obtained. However, we accept the suggestion and will include a framework diagram to clearly illustrate the steps of the method.\n\n**6) Why is the name of the method used instead of the author in the experimental table? My suggestion is to modify it to the method name and year.**\n\n**Re:** We will follow the suggestion and update the table to use the method name and year. Thank you.\n\n**7) In the field of band selection, SVM is a commonly used evaluation method, and you should add relevant experiments on SVM.**\n\n**Re:** We understand that SVM and similar classifiers are commonly used for evaluation in band selection tasks. However, we do not agree with this approach. First, many current band selection methods use deep learning, so evaluating with relatively weaker classifiers like SVM may not truly reflect the capability of the selected bands. Second, the state-of-the-art classification and reconstruction tasks are increasingly relying on deep learning, and evaluation methods should evolve accordingly. In our experiments, we used two different deep learning classifiers, and we have also added two more in Appendix D.1. We believe that this experimental setup sufficiently demonstrates the effectiveness of our method."
            }
        },
        {
            "comment": {
                "value": "**Weakness 1: The literature review on band selection is limited in the section of related work. A number of new relevant methods are not mentioned.**\n\n**Re:** We appreciate the feedback and will update the related work section to include the recent methods on band selection.\n\n**Weakness 2: The problem formulation and motivation of this paper are not precise. Some statements lack evidence and are not convincing. For instance: in line 46, \u201c.These approaches are problematic because the assigned importance is not always precise and overlooks the interplay between bands\u201d. There are many band selection methods that leverage the band correlations. Moreover, in line 52: \u201cExisting methods of imposing sparsity, such as L1 and L2 losses, do not consistently yield table sparsity effects;\u201d This sentence is vague and is difficult to understand. The claimed second challenge in line 52 is not convincing.**\n\n**Re:** We acknowledge that this part of the formulation is not precise, and we will revise it accordingly. Based on our research, most deep learning-based ranking and clustering methods select representative bands by ranking importance weights. However, this approach struggles to capture inter-band relationships using only importance weights, and ranking can introduce errors. For example, while the combination of bands i and j may benefit classification, their individual importance scores may not rank highly.\n\nRegarding the L1 and L2 sparsity methods, we aimed to highlight that these common sparsity techniques in deep learning do not consistently achieve stable sparsity effects. For instance, when trying to select $k$ bands, it is difficult to control L1 or L2 regularization to ensure exactly $k$ bands are selected. This issue is demonstrated in the experimental results, particularly in Figure 2.\n\n**1) Since the compared methods are task-independent, the improvements of the proposed method can be mainly attributed to the task-related loss. The authors need to provide fair comparisons. (weakness 3)**\n\n**Re:** In fact, our comparisons are fair, as almost all the methods we compared are task-related. The task itself serves as the optimization objective\u2014whether it's reconstruction or classification. Without a clear optimization goal, band selection would have no meaningful context.\n\n**2) How to set the number of bands in real applications? How does the number of bands influence the performance?**\n\n**Re:** In real applications, the number of bands is primarily determined by the limitations of transmission and processing equipment. In Appendix D.2, we analyze the impact of different numbers of bands on classification performance. Our findings indicate that the performance tends to improve initially as more bands are selected, but after reaching a certain point, it starts to decline. \n\n**3) The authors claim \u201ca novel deep-learning band selection method\u201d in the conclusion. In which part, deep neural network is utilized.**\n\n**Re:** The sparse loss we proposed is designed specifically for sparse tasks in deep learning, enabling the simultaneous sparse learning of parameters while training the task. In our method, the deep neural network is used in the classification or reconstruction task, guiding the parameters to adapt to these specific applications.\n\n**4) In the abstract, the authors claim that the proposed method is robust. In which aspect, it is robust. How to support this claim?**\n\n**Re:** \nWe conducted experiments on three public datasets with various classifiers (for further details, see Appendix D.1). Additionally, we performed significance testing and demonstrated the stability of the sparsification process. These results collectively support our claim of robustness, as the method consistently outperforms others across different datasets and classifiers. Therefore, we believe these findings provide strong evidence that our method is robust in various scenarios."
            }
        },
        {
            "comment": {
                "value": "**1) Due to the hyperspectral imaging mechanism, spectral bands are not independent, particularly among adjacent bands. This is a significant distinction between band selection and traditional feature selection. However, the proposed method does not account for this issue, which diminishes its applicability to hyperspectral images.**\n\n**Re:** In Section 3.6, we introduced the advantages of our method in modeling the relationships between spectral bands. Furthermore, in Section 4.6, we provided experimental validation to demonstrate that our method can more effectively select spectral bands while considering the spectral band dependencies. Based on these discussions and results, we believe our approach does indeed leverage the inherent dependencies between spectral bands, enhancing its applicability to hyperspectral image analysis.\n\n**2) The proposed method combines Expectation-Maximization with a sparsification process, alternating the selected bands into a deep learning model. However, it is unclear why this approach is superior to inputting all bands directly into the deep learning model for feature learning. Furthermore, the distinction between band selection and channel attention is ambiguous; the latter can be trained in an end-to-end manner. What is the true advantage of the proposed method?**\n\n**Re:** In fact, our method is also trained in an end-to-end manner, and there is no process of alternating the selected bands into the deep learning model. The Expectation-Maximization algorithm is utilized in the loss computation, where the expectation is calculated and then incorporated into the loss function, which is optimized in an end-to-end manner during training. \n\nRegarding the distinction between band selection and channel attention, the key difference lies in the optimization result. In our approach, the final optimization leads to sparsity in the selected bands, which is a crucial feature of our method. This sparsity is what distinguishes our approach from channel attention mechanisms. For a more detailed explanation of the relationship between sparsification and band selection, we have provided a discussion in Appendix H.\n\n**3) Sparse representation techniques have been widely utilized for feature selection or band selection. How does the introduced sparsification process differ from classical sparse representation learning?**\n\n**Re:** As far as we know, sparse representation techniques commonly used in traditional feature or band selection tasks are not directly applicable to deep learning models due to the unique challenges involved in end-to-end training. In Section 4.5, we compared the performance of different sparse loss functions for the band selection task. Our method demonstrates more stable sparsity results and, crucially, has the ability to implicitly model the relationships between spectral bands. \n\n**4) The optimization of the importance weight $c$ during training is not clearly explained. Further clarification is needed.**\n\n**Re:** \nWe assign a learnable parameter $c_i$ for each spectral channel. The original spectral data is scaled by $c$, and the scaled data is then fed into the neural network as inputs. The parameter $c_i$ is also involved in the calculation of the sparsity loss through dynamic programming. During training, the model is optimized according to Equation 1, with $c_i$ being influenced by both the task loss and the sparsity loss.\n\n**5) In Table 3, the bands selected by the proposed method on the KSC dataset include adjacent bands, such as 24-26 and 30-32. Intuitively, this appears inappropriate for hyperspectral images. How do the authors justify this selection and the better classification accuracy?**\n\n**Re:** The effectiveness of the selected bands can be validated through the final classification results. Specifically, we re-trained the classification model using the selected bands and evaluated its performance on the test set. The improved classification accuracy provides indirect evidence supporting the effectiveness of these selected bands. It is also important to note that both deep learning models and the EM algorithm involve optimization processes that are not always intuitively interpretable. While we may not have a direct explanation for each specific band selection, the expermental results demonstrate that, when combined, the selected bands contribute to better classification performance."
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel hyperspectral band selection method that leverages sparse importance representation and Expectation Maximization. The method is applicable to both supervised and unsupervised tasks, achieving state-of-the-art performance on several real hyperspectral datasets. Overall, the paper offers valuable insights into feature selection; however, several issues need to be addressed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper presents a novel band selection method specifically designed for hyperspectral images. This method leverages the Expectation-Maximization algorithm and sparse representation, distinguishing it from existing band selection approaches.\n2. The paper offers a solid theoretical foundation and introduces an effective optimization strategy for the proposed method.\n3. The proposed method is versatile and can be applied to a variety of downstream tasks, including both supervised and unsupervised classification.\n4. The experimental results on three hyperspectral datasets demonstrate a promising improvement over existing methods."
            },
            "weaknesses": {
                "value": "1. Due to the hyperspectral imaging mechanism, spectral bands are not independent, particularly among adjacent bands. This is a significant distinction between band selection and traditional feature selection. However, the proposed method does not account for this issue, which diminishes its applicability to hyperspectral images. \n2. The proposed method combines Expectation-Maximization with a sparsification process, alternating the selected bands into a deep learning model. However, it is unclear why this approach is superior to inputting all bands directly into the deep learning model for feature learning. Furthermore, the distinction between band selection and channel attention is ambiguous; the latter can be trained in an end-to-end manner. What is the true advantage of the proposed method?\n3. Sparse representation techniques have been widely utilized for feature selection or band selection. How does the introduced sparsification process differ from classical sparse representation learning? \n4. The optimization of the importance weight $c$ during training is not clearly explained. Further clarification is needed.\n5. In Table 3, the bands selected by the proposed method on the KSC dataset include adjacent bands, such as 24-26 and 30-32. Intuitively, this appears inappropriate for hyperspectral images. How do the authors justify this selection and the better classification accuracy?"
            },
            "questions": {
                "value": "See the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel method for band selection based on expectation maximization algorithm, which facilitates the selection process via the sparsification of spectral band importance. The presentation is clear and extensive experiments demonstrate the effectiveness of the design."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe authors integrate band selection and post-tasks within a unified framework. For the first time, the EM algorithm is adopted for band selection, facilitating the sparsity of band importance.\n2.\tThe authors provide a theoretical analysis of the model. Extensive experiments are conducted to demonstrate the effectiveness of the design."
            },
            "weaknesses": {
                "value": "1.\tThe literature review on band selection is limited in the section of related work. A number of new relevant methods are not mentioned.\n2.\tThe problem formulation and motivation of this paper are not precise. Some statements lack evidence and are not convincing. For instance: in line 46, \u201c.These approaches are problematic because the assigned importance is not always precise and overlooks the interplay between bands\u201d. There are many band selection methods that leverage the band correlations. Moreover, in line 52: \u201cExisting methods of imposing sparsity, such as L1 and L2 losses, do not consistently yield table sparsity effects;\u201d This sentence is vague and is difficult to understand. The claimed second challenge in line 52 is not convincing.\n3.\tThe comparison is unfair. The proposed method is supervised, which introduced the post-task related loss. However, the compared methods are task-independent. For this reason, the results are not convincing."
            },
            "questions": {
                "value": "1.\tSince the compared methods are task-independent, the improvements of the proposed method can be mainly attributed to the task-related loss. The authors need to provide fair comparisons.\n2.\tHow to set the number of bands in real applications? How does the number of bands influence the performance?\n3.\tThe authors claim \u201ca novel deep-learning band selection method\u201d in the conclusion. In which part, deep neural network is utilized. \n4.\tIn the abstract, the authors claim that the proposed method is robust. In which aspect, it is robust. How to support this claim?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This method leverages an Expectation Maximization (EM) algorithm to sparsely represent the importance of spectral bands, thereby enhancing selection efficiency and reducing data processing complexity. The paper asserts that this approach not only achieves significant sparsity but also excels in illustrating the inter-band relationships, offering a potential solution for spectral band selection tasks. The authors provide a comprehensive theoretical analysis and experimental validation using public datasets, demonstrating the method's superiority over other sparsification techniques."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Neither the questions raised nor the methods used in the paper are new."
            },
            "weaknesses": {
                "value": "EM algorithm is out of date in the area of hyperspectral image band selection."
            },
            "questions": {
                "value": "1.Band selection refers to selecting important bands from hyperspectral images rather than selecting bands with high correlation from hyperspectral datasets.\n2.The author points out that existing methods are not always accurate in assigning band importance. How can we prove whether this problem exists? How can you prove that your method can effectively solve this problem?\n3.The loss of L1 and L2 cannot produce stable sparse effects, and this issue should be illustrated with a diagram to help readers better understand.\n4.Why use the EM algorithm to design the band selection method? How does your method solve the problem of unstable sparsity of L1 and L2, as well as the difficulty in describing the band relationships during the sparsity process? All of these need to be logically explained in the introduction.\n5.The method lacks a framework diagram, which should allow readers to clearly understand the steps of the method without reading the entire text.\n6.Why is the name of the method used instead of the author in the experimental table? My suggestion is to modify it to the method name and year.\n7.In the field of band selection, SVM is a commonly used evaluation method, and you should add relevant experiments on SVM."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel method for band selection in hyperspectral images, by integrating sparsity within an EM algorithm."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The method is interesting, with some nice theoretical properties. The paper is well written."
            },
            "weaknesses": {
                "value": "The authors present their method as being the first to implement a sparsity representation method based on the EM algorithm. However, this is not correct, as the authors are missing many related methods that integrate sparsity within an EM algorithm. Some of the missing related methods are:\n- Bouveyron, C., & Brunet-Saumard, C. (2014). Discriminative variable selection for clustering with the sparse Fisher-EM algorithm. Computational Statistics, 29, 489-513.\n- Ghosh, A. K., & Chakraborty, A. (2017). Use of EM algorithm for data reduction under sparsity assumption. Computational Statistics, 32, 387-407.\n- Wang, Z., Gu, Q., Ning, Y., & Liu, H. (2015). High dimensional em algorithm: Statistical optimization and asymptotic normality. Advances in neural information processing systems, 28.\n- Latouche, P., Mattei, P. A., Bouveyron, C., & Chiquet, J. (2016). Combining a relaxed EM algorithm with Occam\u2019s razor for Bayesian variable selection in high-dimensional regression. Journal of Multivariate Analysis, 146, 177-190.\n- Ro\u010dkov\u00e1, V. (2018). Particle EM for variable selection. Journal of the American Statistical Association, 113(524), 1684-1697.\n- Ro\u010dkov\u00e1, V., & George, E. I. (2014). EMVS: The EM approach to Bayesian variable selection. Journal of the American Statistical Association, 109(506), 828-846.\n- Wang, J., Liang, F., & Ji, Y. (2016). An ensemble EM algorithm for Bayesian variable selection. arXiv preprint arXiv:1603.04360.\n\nAs a result, the contributions of the submitted work are clearly positioned in such literature. Moreover, the comparative experiments are missing such related work."
            },
            "questions": {
                "value": "Positioning within the literature, including the above cited work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}