{
    "id": "8ZA7lrzw7O",
    "title": "Sharper Analysis of Data Echoing and New Communication-Efficient Algorithm for Data Parallelism",
    "abstract": "Over the past decade, breakthroughs in both general-purpose and specialized hardware have propelled the success of large-scale machine learning. However, the advancements in general-purpose hardware are not keeping pace with those in specialized hardware. Consequently, operations conducted on the general-purpose hardware have become the primary performance bottleneck. Notably, data loading significantly lags behind the gradient computation during training. To address this issue, the technique of data echoing has been introduced, whereby the current batch of samples is reused for gradient computation to minimize idle time while waiting for new data. However, this approach can lead to overfitting on the current batch, and it remains unclear whether convergence benefits from this practice. In this paper, we provide a sharper analysis on a stochastic variant of data echoing and show that it obtains linear speedup proportional to the number of reuse times. Additionally, we investigate the impact of the communication bottleneck in data parallelism of data echoing, and propose a new communication-efficient data echoing algorithm via reducing the frequency of model averaging. We then show that it is possible to perform data echoing without additional communication cost with data parallelism. Finally, we perform empirical experiments to verify our analysis on the data echoing and the proposed efficient algorithm for data parallelism.",
    "keywords": [
        "data echoing",
        "data loading bottleneck"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8ZA7lrzw7O",
    "pdf_link": "https://openreview.net/pdf?id=8ZA7lrzw7O",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a new analysis (ie extending and improving previous work) of data echoing, an important technique use in practice in the training of DNN.\nMoreover, the author also propose a new (communication efficient) technique tackling the problem of communication bottleneck.\n Numerical experiments support the proposed techniques."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well written, easy to follow and the contributions are stated clearly.\n- the paper proposes an improvement/extension over the work of  Agarwal et al. (2020) by extending their results to the non-convex setting and do not require the gradients to be bounded. Rather, they propose require an assumption present from in Even, 2023. I have not written the proof in details, but I am not surprised by this result.\n- to the best of the reviewer's knowledge the problem of finding  \"communication efficient data echoing algorithm\" has not been addressed in the litterature. The reviewer this is in an interesting direction worth tackling (which this paper does)"
            },
            "weaknesses": {
                "value": "- the reviewer finds the idea of the cosine scheduler for data loading probability particularly interesting. \nHowever, the reviewer wishes to see some theoretical/formal arguments on the soundness of this technique/how this impacts convergence/the results developed in the paper.\nPerhaps this is trivial (admittedly the reviewer is not an expert on this topic). In any case, the reviewer does believe this should be stated/clarified.\n- The numerics should be more extensive/have more results. MobileNet-V2 are not SOTA anymore (see MobileNet-V3). Moreover, the application is for the training of neural nets. Hence the reviewer expects that results for more modern/SOTA architectures  (transformers....) should be present, regardless of the speed of these architectures at inference time. \n- please edit the graphs in figure 4/5 to include the name of the dataset, model and lr (as title for instance....)\n\nI am for now putting a 5/ marginally below acceptance threshold in order to have those few comments addressed.\nI would be open to increase my score provided those points are properly addressed (especially about the numerics)"
            },
            "questions": {
                "value": "- please see my comment above on cosine LR schedulers and the comment on the architectures used for the numerical experiments\n- the reviewer is curious how definition 3.1 differs from the definition of a \"standard\" markov chain.\nNote that i am not raising this in the \"weaknesses\" section but I do believe this could be clarified/highlighted."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides a convergence analysis for SGD with reused data samples (i.e., dada echoing). The analysis is standard for non-convex optimization; however, it uses an unusual assumption (line 076) that the gradient is still unbiased with the reused data samples."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The work is well-motivated: As data movement is expensive in modern computer systems, data reusing can significantly improve the performance of neural network training. Previous work has shown practical benefits and preliminary theoretical results for reusing data samples in SGD. This work aims to provide a sharper analysis for the data echoing algorithm. \n\n2. Presentation is good: The paper is well written. The author clearly described the data echoing algorithms and their theoretical results."
            },
            "weaknesses": {
                "value": "My main concern about the paper is the unusual assumption it uses (line 076). The key difference between standard SGD and data echoing SGD lies in gradient computation. For standard SGD, we can assume unbiased gradients due to i.i.d. sampling of data. However, for data echoing SGD, I strongly suspect this assumption doesn't hold. If we could make the same assumption for data echoing, I don't see how the analysis would differ from standard SGD."
            },
            "questions": {
                "value": "You assume the gradient is unbiased for large enough $\\tau$; however, in the actual algorithm, I guess $\\tau$ is limited by M. Can you give more explanation on the assumption?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper suggests a markovian perspective when analysing the data echoing algorithm, the technique that allows to mitigate the data loading overhead. Data echoing combined with Stochastic Gradient Descent uses the biased gradient in the parameter update. The authors show the boundedness of such bias under mild assumptions and run a few experimental runs to show its efficiency in practice. In addition to that, they introduce a novel data echoing algorithm that is adjusted to data parallelism setting."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors tackle more general setting than the prior works by considering non-convex optimization problems. They take an interesting approach to model data echoing in SGD through Markov chain processes. They provide an analysis of the problem, proving the comparable convergence to SGD while having a linear speedup in terms of number of reuse steps. \n\nThe authors also provide an adaptation of the data echoing algorithm to a data parallelism setup. They noticed that the communication that happens when synchronizing the weights negates the benefits that data echoing introduces for a single node setup. Thus, they combine a data echoing algorithm with the delayed weight synchronization to keep idle time low, while proving the convergence of the novel optimization scheme. \n\nOverall, the work extends on the previous line of work with an interesting algorithmic and theoretical contributions."
            },
            "weaknesses": {
                "value": "As a person who has not been familiar with data echoing before, I find the experimental results counter-intuitive and misleading. In particular, they do not answer the question \"if your hardware has a particular data loading speed what level of data echoing you need to use?\".\n\nI see the data echoing technique as a way to reduce the idle time that is due to the long waiting for the next loaded samples. So I expect data echoing may decrease the total training time, but as a method that uses a biased gradient estimation, it should require more optimization steps to converge to a minimum compared to a vanilla SGD. In the experimental section we only see experiments with respect to \u00abnumber of example loads\u00bb, which creates an impression that data echoing is in general always better than SGD. This paper lacks more figures that are plotted against the number of gradient steps or epochs to see a bigger picture and compare convergences. More thorough experimental section would provide a better intuition whether you need to use data echoing or not if your hardware support a certain data transfer speed.\n\nMoreover, to have a fair comparison of the data echoing algorithm and SGD, we need to choose the best learning rate for each algorithm independently, right now the learning rate is the same for both algorithms. Indeed, if roughly speaking data echoing performs as an SGD that reuses the same batch several times, then the similar effect can be sometimes obtained by increasing the learning rate for SGD, which can be partially observed on some figures.\n\nConcerning the theoretical analysis, the convergence results look reasonable, but the proofs are not easy to follow as some derivation steps are either skipped or not explained, which makes it harder to read and thus verify the correctness of the proofs for a person not familiar with this line of work. Overall, the appendix seems to be written hastly, I suggest the authors to pay more attention to how they explain their derivations and add missing details in the proofs.\n\nMinor remarks:\n\nline 038: parallelism -> parallel\n\nline 237: slow -> slows\n\nline 265: it is stated that $\\nabla f(x_t, B_t)$ approximates well $\\nabla f(x_{t-\\tau})$ and that it follows from Lemma 3.6. Please comment more on this as it is not a straightforward induction\n\nline 307: explain how do you get the minimum burn-in time (a lower bound for T)?\n\nline 363: effect to -> effect on\n\nline 370: $c_{\\nu}$ was never introduced before\n\nUse $\\times$ or $\\cdot$ for multiplication, instead of *\n\nFigure 1 from the main paper contradicts the description in the appendix, as in one place higher i means higher data loading speed, in other place the opposite.\n\nI suggest using separate numbering for Definition, Lemmas and Theorem independent of section number, so that there are Theorem 1 and Theorem 2, instead of Theorem 3.8 and Theorem 4.2\n\nIn equation 4, I would use $p_t$ instead of $p$ directly to simplify Algorithm 1 and 2 descriptions (e.g. line 3 of Algorithm 1)\n\nThe order of Figures doesn\u2019t follow their order of mentioning in the text"
            },
            "questions": {
                "value": "Can you do an experiment where each algorithm is compared with their best corresponding learning rate? (see above)\nHow to set your algorithm for a given data loading speed for it to do the best performance?\n\nPlease provide the modern GPU performance numbers for a standard SGD algorithm and what it corresponds to in figure 1?\n\nThe equation 8 in the appendix is explained with independent sampling property, which seems to contradict the whole Markov Chain formulation of the optimization problem where $d_l$ depends on $d_{l-1}$ due to data echoing. Can you please explain these transitions more in detail?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides a sharper analysis of previous data echoing work. The paper concludes that data echoing can get linear speedup proportional to sample reuse times. Then it proposes reducing gradient averaging frequency based on data echoing frequency to reduce communication cost. For evaluation, this paper adopts a cosine diminishing schedule for data echo probability and valid its effectiveness."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Theoretical proof of data echoing achieving linear speedup proportional to num of reus times. The authors formulate stochastic data echoing as Markov chain gradient descent problem and provide a sharper analysis.\n\ndata echo is a good direction for reducing data loading overhead in distributed environments.\n\nThe proposed cosine diminishing schedule on data echoing achieves better model test accuracy."
            },
            "weaknesses": {
                "value": "1 ) the paper lacks of systematic understanding of how distributed training works and what could be bottleneck. For example, one major contribution in this paper is to reducing cross GPU communication frequency for gradient averaging with probability p^{c}_{t} (detailed in Algorithm2 and section 4). It does not consider how it can be grounded in real world training. \n\n1.1) If every GPU's triggering gradient averaging probability is i.i.d, then it is almost impossible to pre-allocate and pre-form the GPU communication group for each gradient averaging collective. If forming communication groups ad hoc, it means every time before starting communication, we need to initialize a new communication group and ping every involved GPU to build connection, which will be much bigger overhead compared with reduced communication frequency gain. \n\n1.2) If all GPUs communicate at same time but with lower frequency, this kind of techniques already exists as gradient accumulation steps. Further more, comparing with data echoing's reducing data communication frequency with probability and may have model training accuracy loss, gradient accumulation step can mimic identical model training loss curve of pure distributed data parallel (DDP) but communicate gradient at a much lower frequency. \n\n2 ) the paper lacks major results. LLM is a good example for distributed model training. As mentioned in Sec.5, the paper also use wiki text and gpt-2 model training for evaluation. However, I could not find any results in the paper. The only results on cifar-10/100 + small CNN like resnet/mobilenet usually do not need distributed training. Therefore cifar10/100+ small CNN results are not very convincing. \n\n3 ) The paper is lack of novelty. Two major contributions in this paper. First it provides a tighter convergence analysis of previous work of data echoing via formulating stochastic data echoing to markov chain gradient descent. This first contribution is theoretical contribution but does not proposing any new idea. The second contribution is reducing cross-GPU gradient averaging frequency based on data echoing frequency. The idea seems novel in data echoing setting, but there is a much widely adopted and existed approach call gradient accumulation step, which does not hurt model training accuracy at all while reducing gradient averaging communication frequency. One minor novelty is adding cosine diminishing schedule on data echoing, but this novelty contribution is limited, since any diminishing schedule may work in data echoing setting.\n\nminor issues:\n\nall the figures from fig3 to fig7 (especially fig7) in both x and y axis, the texts are too small to see even enlarge to 200%."
            },
            "questions": {
                "value": "How does communication with some probability compared with widely used gradient accumulation step approach? To me, gradient accumulation step approach does not hurt any model training accuracy loss and much easier to be used in real world applications (i.e. reuse the same communication groups all the time with NCCL/RCCL).\n\nHow would this paper's approach works in real distributed training environment? (either larger dataset like imagenet, or larger models like gpt-2/3, llama-2/3)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}