{
    "id": "kauK28525D",
    "title": "Large Language Models Can Be More Robust Multiple Choice Selectors Through Attention Intervention",
    "abstract": "Multiple-choice question (MCQ) is a common task for evaluating large language models (LLMs). LLMs' performance on MCQ is often affected by various biases. Previous research has extensively examined the impact of inherent option bias on MCQ predictions, where this bias refers to a preference for a specific option ID token introduced during the model's training. However, in an in-context learning scenario, few-shot prompting can also introduce a form of bias, known as context option bias. This occurs, for instance, in extreme cases where all demonstration answers are consistently option A, in which case LLMs may predict A for the given question whatever the question is. Context option bias can significantly degrade LLMs' performance. To better observe the LLMs' behavior when affected by the context option bias, we deliberately use demonstrations with obvious context option bias for MCQ to amplify the effect. The results indicate that certain attention heads in LLMs are particularly sensitive to context option bias. Motivated by this observation, we propose our approach, CoLo, to address this issue. First, using samples with ordinary and biased demonstrations as input, CoLo compares the outputs of two types of inputs and localizes attention heads sensitive to context option bias through sequential interventions. Then, we propose an attention scaling-based method to intervene in the identified attention heads during the inference stage, thereby mitigating the impact of context option bias on the LLMs\u2019 predictions. Experimental results demonstrate that CoLo effectively alleviates the impact of context option bias and improves the LLM's robustness on MCQ tasks.",
    "keywords": [
        "Large Language Models; Reasoning; In-context learning"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "A method for localizing and mitigating option bias in large language models.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kauK28525D",
    "pdf_link": "https://openreview.net/pdf?id=kauK28525D",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a method to interpret which attention cells are responsible for \u201ccontext option bias\u201d \u2014 a form of bias that occurs when an LLM inclines to generating one option more than others in an in-context learning scenario. The authors propose a difference metric that measures the change in attention values when biased or ordinary examples are used with in-context learning. This metric is also used to detect top-layers that are the most important for context option bias. They also propose a solution to alleviate the impact of bias: scaling attention values of the detected cells using a custom temperature. Experimental results suggest that the proposed method improves MMLU performance and detected attention heads transfer to other domains such as CEVAL. The detected heads are robust to different number of choice or demonstrations but sensitive to the particular biased option."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The custom temperature per attention head is a simple but affective approach for detecting context option bias.\n\n2. Experimental results indicate success with better detection and improved performance.\n\n3. Transferability to other tasks indicate that these heads are feature of the model and pre-training rather than downstream domain. This is an interesting finding."
            },
            "weaknesses": {
                "value": "There are a few concerns that I detailed below.\n\n1. I think a better motivation on why context option bias for MCQ is a practical problem is needed. Given that options in in-context examples can be shuffled freely, this needs a better motivation.\n\n2. What is the minimal number of examples for successful detection of bias? More specifically, how does the performance scale with more examples?\n\n3. What if you add a custom prompt message that explains that \u201cthere are multiple options, the answer can be any one of them, and no particular option such as A or B is more likely.\u201d Would this be an affective strategy to steer the attention heads?\n\n4. How does the problem change with increasing model size? Are larger models more or less susceptible?\n\n5. Equation 2 should be $y=argmax$.\n\n6. How did you choose $K_l$ and $K_h$? How does the performance change as $K_l$ and $K_h$ increases?\n\n7. Previous work has used different temperatures for different domains (see [1] for one such work) that are not necessarily 1.0. Can you run a search over different temperatures for the baseline and report the best performing model?\n\n8. Can you explain why the performance of Llama-3 and Gemma models are very different from official numbers from respective papers?\n\n9. Can you explain what do you mean by confidence in Section 4.5?\n\n10. Caption of the Figure-5 is not grammatically sound. Please rewrite.\n\n[1] Large Language Monkeys: Scaling Inference Compute with Repeated Sampling. Bradley Brown and Jordan Juravsky and Ryan Ehrlich and Ronald Clark and Quoc V. Le and Christopher R\u00e9 and Azalia Mirhoseini."
            },
            "questions": {
                "value": "1. Why does context option bias for MCQ a practical problem?\n\n2.  How does the performance scale with more examples?\n\n3. What happens if you use a custom prompt message?\n\n4. What is the impact of model size on the problem?\n\n5. How does the performance change w.r.t. different $K_l$ and $K_h$? How did you choose these parameters?\n\n6. Can you improve the baseline by finding the best temperature?\n\n7. Why does the performance of Llama-3 and Gemma different from official numbers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes  a method  aiming at mitigating context option bias in multiple-choice questions taks, by comparing the LLMs\u2019 outputs for biased and ordinary prompts in order to localize attention heads associated with the context option bias. Attention scaling interventions is then employed. Experiments on several dataset are described in order to validate the method, although the standard deviations are not reported."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "-Paper addresses the weaknesses in large language models related with content bias in multiple-choice questions task (MCQ).\n\n-The paper claims that the proposed method is demonstrated empirically to reduce the content bias in MCQ task on MMLU, although the standard deviations in the experiments are not  reported."
            },
            "weaknesses": {
                "value": "1)The paper provides no theoretical explanation for why only specific attention heads contribute to bias or why scaling the specific attention heads mitigates the bias effectively. This lack of theoretical grounding may affect the reproducibility and understanding of the method, making it difficult to predict its behavior across other models.\n\n2)The standard deviations are not reported in the experiments. Especially in the view of the small differences in several important experiments, this makes impossible to evaluate the method empirically.\n\n2)Narrow focus on attention scaling as the intervention technique. The paper uses exclusively the attention scaling to address bias without exploring other possible intervention methods, such as layer-based adjustments or targeted fine-tuning. This narrow approach may miss alternative solutions that could yield more robust or efficient debiasing effects.\n\n3)Recent large language models, such as  for example Llama 3.2, Mistral, Qwen 2 were not used in experiments, do they suffer from similar biases? Is the proposed method effective in these cases?\n\n4)Reliance on artificially biased prompts. The study constructs heavily biased demonstrations to amplify context option bias and identify attention heads associated with this bias. This contrived setup may not accurately reflect the subtler biases encountered in real-world applications, potentially limiting the generalizability of the findings. It is not clear why reducing the bias in the artificially biased scenario will lead to increased performance in real-world applications. \n\n5)Sensitivity to hyperparameter choice. The method has hyperparameters, such as the scaling factor T for attention adjustments, but the paper  analysis of how sensitive the model's performance is to these choices is limited. Without detailed guidance for these hyperparameters, reproducing the results and applying the method to new datasets or models would require extensive tuning, which can be  resource-intensive.\n\n6)Limited evaluation of cross-domain generalizability.  The evaluation of cross-dataset robustness is limited to certain datasets and tasks, where the reported small change is impossible to validate in the absence of  standard deviations reported. A more comprehensive assessment across a broader range of domains would be necessary to confirm the method\u2019s generalizability, especially given the diverse biases in data sources.\n\n7)The readability and clarity of the paper can be improved. The paper often employs long sentences which can obscure the main points. Sentences like 'To better observe the LLMs\u2019 behavior when affected by the context option bias, we deliberately use demonstrations with obvious context option bias for MCQ to amplify the effect' are long and somewhat repetitive. Table 2 is discussed briefly without highlighting the most important findings or explaining how they support the claims. Redundant phrases and wordiness occasionally affect readability. For example, phrases like 'To better excavate the reasons behind this problem and resolve it...' could be made more concise. 'One of effective methods' requires 'the' article here; '...significantly degrade LLMs\u2019 performance' as  'context option bias' is singular, so 'degrade' should be 'degrades.'  Vague pronoun references, in a few instances, pronouns (like 'it' or 'this') refer ambiguously to previous sentences, making the meaning less clear. 'The experiments find that... This inspires us to test...' \u2014 here, \"This\" could refer to several prior ideas, so specifying 'This result' or 'This observation' would improve clarity.\n\n8)The captions under Figure 8 are somewhat misleading, how to read the frequency of the localized field from picture (c)?"
            },
            "questions": {
                "value": "Did you experiment with other recent llms, such as Llama 3.2, Mistral, Qwen 2, do they suffer from similar biases? Is the proposed method effective in these cases? \n\nWhy reducing the bias in such artificially biased scenario will lead to better performance in real-world applications?\n\nDid you try other options of reducing the bias, like targeted fine-tuning?\n\nWhy the standard deviations in the experiments were not reported?\n\nHow to tune the hyperparameters of the method such as T? the performance seems to depend heavily on its value (see table 1)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Paper proposes a new approach to identify and intervene with attention heads in order to minimize biases from in-context examples"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "_ Proposed approach is novel and the problem of biased in-context examples in important\n- Paper is clearly written and easy to understand\n- Experiments are thorough, including different models and tasks\n- Results show that the approach leads to improved performance and is robust to the style of the input and the dataset"
            },
            "weaknesses": {
                "value": "- Results mostly show 1-3% improvement, which is nice, but not a huge improvement\n- Experiments are focused on multiple choice. It would be great to show that the proposed method is able to generalize to more open ended question answering tasks, which are more representative of realistic tasks that LLMs are used for\n- The proposed method requires both original and biased versions of in-context examples. However, in more open ended question, LLMs may still suffer from biased in-context examples, but it may not be clear what the bias is. Is there a way to not required both biased and unbiased versions of in-context examples in the method?"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an approach, termed CoLo, to address the bias in multiple-choice question (MCQ) selection that arises from context option bias in large language models (LLMs). CoLo mitigates context option bias by intervening on specific attention heads that are highly sensitive to biased in-context demonstrations. By selectively scaling attention in these heads, CoLo reduces the model's predisposition to biased options, showing improvements in MCQ accuracy across several datasets. The authors also conduct extensive experiments to validate the cross-domain robustness of their method, indicating that CoLo's effect generalizes well across various MCQ scenarios."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* I appreciate the focus on context option bias, a previously-overlooked form of bias that can impact LLM performance in MCQ tasks. The design of CoLo is also well-motivated.\n* The paper provides extensive empirical evidence that shows improvements in MCQ accuracy and recall standard deviation. The authors also consider both zero-shot and few-shot scenarios and the cross-dataset setting, enhancing the generalizability of their findings."
            },
            "weaknesses": {
                "value": "I would like to see evaluation with more larger models, as the current experiments are conducted with at most 7B/8B-level models. So I am not fully convinced about CoLo's scalability. For example, how CoLo would perform with 30B/70B-level, or MoE models (these experiments may be conducted with Qwen2/Qwen2.5 models, as LLaMA-3 does not have too many variants)? Also, what the specific GPU hardware and hours will be needed for various-sized models? The lack of experiments and analysis on larger models may limit the broader impact and implications of the proposed method."
            },
            "questions": {
                "value": "1. Identifying the attention heads most associated with context option bias requires multiple rounds of sampling, which could add computational complexity in real-world applications (and higher than the PriDe baseline). While this overhead is justified within the paper\u2019s scope, discussing potential optimizations or alternative approaches might help broaden CoLo\u2019s applicability, especially for larger-scale implementations.\n2. While the concept of CoLo is sound, the process for selecting and intervening on attention heads may seem somewhat convoluted to readers not already familiar with the technical nuances of attention heads and scaling interventions. Additional clarifications or illustrative examples could enhance the accessibility of the proposed approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}