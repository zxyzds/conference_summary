{
    "id": "br4H61LOoI",
    "title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
    "abstract": "In this work, we introduce a novel evaluation paradigm for Large Language Models (LLMs) that compels them to transition from a traditional question-answering role, akin to a student, to a solution-scoring role, akin to a teacher. This paradigm, focusing on \"reasoning about reasoning,\" termed meta-reasoning, shifts the emphasis from result-oriented assessments, which often neglect the reasoning process, to a more comprehensive evaluation that effectively distinguishes between the cognitive capabilities of different models. Our meta-reasoning process mirrors \"system-2\" slow thinking, requiring careful examination of assumptions, conditions, calculations, and logic to identify mistakes. This deliberate approach encourages deeper cognitive engagement, as models must scrutinize each step of the reasoning process to detect errors, making this paradigm especially valuable for tasks that involve complex problem-solving. By applying this paradigm in the GSM8K dataset, we have developed the MR-GSM8K benchmark. Our extensive analysis includes several state-of-the-art models from both open-source and commercial domains, uncovering fundamental deficiencies in their training and evaluation methodologies. Notably, while models like Deepseek-v2 and Claude3-Sonnet closely competed with GPT-4 in GSM8K, their performance disparities expanded dramatically in MR-GSM8K, with differences widening to over 20 absolute points, underscoring the significant challenge posed by our meta-reasoning approach.",
    "keywords": [
        "Benchmark",
        "LLM",
        "Math",
        "Evaluation"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We transformed the GSM8K benchmark under our novel meta-reasoning paradigm and conducted extensive experiments on series of LLMs",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=br4H61LOoI",
    "pdf_link": "https://openreview.net/pdf?id=br4H61LOoI",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes MR-GSM8k a new evaluation dataset adopted from GSM8k to test the depths of cognitive abilities in LLMs. This dataset consists of samples from GSM8k, code based solutions to the problems, and reverse-engineered versions asking the model to find the value of an input variable given the answer. Rather than relying on surface-level final answer match, they aim to score the model for its reasoning ability by proposing a new metric MR score - which given a <task, solution> pair, expects the model to evaluate its correctness, identify the first point of error, and the reason for the error. Evaluating a range of models, they demonstrate that these models perform similarly on the original GSM8k but show wide variance on MR-GSM8k as it tests their core ability to reason, and investigate possible reasons for this, urging for benchmarks that go deeper than surface-level evaluations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The construction of this new dataset is quite novel and opens doors to more nuanced evaluation methods to test the reasoning of language models.\n- With increasing interest in the interpretability of LMs, this work transforms an existing dataset to measure which parts models fail.\n- They highlight the memorization issue prevalent in LMs \u2014 it has imitation skills but lacks a deep understanding of the underlying logical as they can solve the task but not infer with the same accuracy if a given solution is correct or not."
            },
            "weaknesses": {
                "value": "- The dataset construction procedure is of limited novelty with only the reversal mode being significantly different.\n- A more detailed error analysis with some discussion on actionable insights on how to combat them would be beneficial to the community\nSeveral works have already probed the memorization effect and lack of genuine reasoning in LLMs on different tasks (Embers of Autoregression, McCoy et. al; Deciphering the Factors Influencing the Efficacy of Chain-of-Thought, Prabhakar et. al; GSM-Symbolic, Mirzadeh et al.) providing concrete insights into the error patterns.\n- Given these weaknesses, I feel the dataset could be of interest to a section of the community."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new benchmark category, Meta-Reasoning (reasoning about reasoning), and a new corresponding meta-reasoning benchmark dataset, MR-GSM8K, derived from the well-known GSM8K. In this benchmark, models are shown question-solution pairs and are tasked with identifying correct solutions, incorrect steps, and explaining incorrect steps. All three of these subtasks are captured in their proposed novel metric, the MR-score. They show that their benchmark indeed tests for a new capability and is quite challenging - models that perform well on the reasoning dataset GSM8K surprisingly perform substantially worse on MR-GSM8K."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Originality\n-The benchmark task and evaluation metric appears to be novel. It is testing for a new capability, reasoning about reasoning.\n\nQuality\n-Their benchmark has led to strong insights that may influence future research. For example, they show that bigger models are not necessarily better than smaller models on their challenging meta-reasoning benchmark, with Phi-3 beating models many times larger.\n\n-The creation of the benchmark dataset underwent several rounds of reviews from LLMs and human annotators. \n\nClarity\n-The paper is logically organized and easy to follow. \n\nSignificance\n-It seems that multi-step reasoning datasets don\u2019t fully assess the steps taken to arrive at a solution; instead, they score a model based on its final answer. So there is a need for more nuanced evaluation, such as this work. This work challenges the surface-level assessments of previous reasoning benchmarks and will inspire deeper research into enhancing model reasoning."
            },
            "weaknesses": {
                "value": "-Table 2 could be easier to read, see suggestions below."
            },
            "questions": {
                "value": "-Table 2 needs more clarity: consider adding a table description defining your abbreviations \u201cTask1-TPR\u201d, etc. Also the values that are bolded were not intuitive - in the table description also explain what bold means. Also explain what k=0, k=3 means."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors to propose a new evaluation paradigm that shifts the role of models from \u201cQA student\u201d to a \u201csolution-scoring teacher.\u201d In this approach, instead of generating solutions to questions, models are presented with question-solution pairs and are have to determine the correctness of each solution, identifying the first error step if any, and providing explanations for errors. This paradigm encourages models to engage in meta-reasoning, assessing different reasoning processes rather than simply arriving at correct answers. The authors developed a meta-reasoning version of the GSM8K benchmark, called MR-GSM8K, and introduced a new metric, the MR-Score, which is a weighted combination of three submetrics. Instances in MR-GSM8K are manually labeled by experts to ensure accuracy in evaluation. Their findings indicate that specialized math models struggle to generalize to this new benchmark, and, contrary to expectations, larger models do not consistently perform better in tasks on MR-GSM8K. The authors argue that models frequently exhibit superficial reasoning in math tasks and link these to their possible limitations in \u201cSystem 2\u201d thinking\u2014i.e., that models fail to engage in slow, deliberate reasoning that examines assumptions, conditions, and logic for thorough error detection."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I commend authors' general aims in evaluation of LLMs' math reasoning steps rather than just final solutions as well as their efforts to ensure minimise the annotation errors and quality control the benchmark instances. I also like the metric MR-Score they introduced to evaluate model performance. Introducing a meta math reasoning benchmark is also important in advancing our understanding of LLMs abilities."
            },
            "weaknesses": {
                "value": "I find it difficult to understand why, given their goal of assessing LLMs\u2019 step-by-step math reasoning abilities, the authors chose to create a meta-reasoning benchmark where the task is to evaluate the correctness of provided step-by-step math solutions rather than to generate correct step-by-step reasoning. Evaluating solutions is a distinctly different skill from producing them. Just as we wouldn\u2019t expect students who excel at solving math problems to also excel at verifying others' solutions, we shouldn\u2019t assume these skills are interchangeable for models.\n\nFurthermore, the authors' results might even suggest that their benchmark tests different capabilities than those measured by GSM8K. For instance, despite the claims in lines 375-379, the results in Table 2 indicate that none of the models specialized in solving math problems performed well on their benchmark. In lines 377-378, the authors claim that MAmmoTH-70B and DeepSeek-Math-7B-RL achieve decent performance. However, Table 2 shows that their MR-Scores are near zero in multi-shot learning and only slightly higher in zero-shot learning. If Table 2 is accurate, this could indicate that their benchmark does not truly measure the same abilities as GSM8K, MATH, or other similar benchmarks.\n\nGiven this, it\u2019s unclear how the authors can argue that fine-tuning on math problems like those in GSM8K leads to overfitting or only a superficial mastery of mathematical reasoning, especially if their benchmark assesses a different skill than GSM8K and similar tests. Why should we expect high performance on GSM8K to correlate with high performance on MR-GSM8K?\n\nThe authors need to clarify their definition of \"mathematical reasoning.\" How does their concept of math reasoning differ (or not) from the concept tested by benchmarks like GSM8K? Are these two concepts comparable, or do they address entirely separate abilities?"
            },
            "questions": {
                "value": "If the authors are interested in evaluating LLMs' math reasoning rather than just their ability to produce a correct answer, why not have the models generate final answers using Chain-of-Thought (CoT) or Program-of-Thought (PoT) reasoning and evaluate those directly, rather than assessing the models' meta-reasoning abilities?\n\nWhat do the authors mean by \"mathematical reasoning\"? Are they evaluating the same abilities as those tested by GSM8K and similar benchmarks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a novel paradigm for evaluating LLMs. Rather than assessing their ability to correctly produce a final answer, the paradigm assesses their ability to produce a more fine-grained analysis of given question-solutions paris. They focus on mathematical questions, specifically GSM8k. They generate a rather detailed dataset derived from GSM8k and include 3 types of base metrics for assessing the LLM in the evaluation process."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The authors suggested paradigm shift to solution-driven evaluation (i.e., the correctness of the entire solution process) is indeed important, and not heavily studied\n* The annotation process of the dataset with the three fields seems to be very useful in assessing fine-grained performance of LLMs\n* Most claims and the steps taken by the authors in their research are fairly clear\n* The process of training and selecting annotators\n* They provide useful insights into LLM performance\n  * Regarding \u201cspecialized\u201d-trained models, specifically math models, they highlight their inability to generalize to related tasks\n  * Larger models are not necessarily better than smaller ones\n  * Fine-tuning LLMs on specific benchmarking tasks (such as GSM8k) may result in overfitting to the data in a detrimental way. Thus, fine-tuning should be handled with care\n  * The impact of the number of correct solutions that is shown to the models during in-context-learning (i.e., \u201csusceptibility of language models to the distribution of few-shot examples\u201d)"
            },
            "weaknesses": {
                "value": "* The authors explain the reasoning behind the 3 types of questions in their MR-GSM8k. Each type was previously suggested and studied: (1) sampling GSM8k, (2) modifying GSM8k to require code in the solution, (3) \u201creversed reasoning\u201d from GSM8k. If these types were already studied, what is the novelty or added value in the suggested benchmark? This is unclear. Providing more information can help understand the novelty of their data generation process\n* Discussion section\n  * Subsection 6.1 is based on Appendix-D, and thus it is difficult to grasp the insights provided here. I suggest making this subsection clearer by providing some examples\n* The authors claim that their suggested paradigm enables them to transform any benchmark. However, if I understand correctly, this is not really the case. First, they studied a math benchmark. In addition, the novelty here seems only marginal since their suggested benchmarking-transformation techniques (POT + reverse reasoning) was already suggested before. If there is indeed a claim for some novel method for transforming any benchmark, then it is not clear what the novel method is."
            },
            "questions": {
                "value": "* Table 1 - unclear what the values under the last column represent (First Error Steps). Is this the step where the first error occurred? Make it clearer\n* Table 2\n  * I suggest using explicit terms for task 1/2/3, or alternatively use explicit metrics where possible, such as ACC_{reason}, ACC_{step}. Currently, it is unclear. For example, what is the difference between \u201cTask2-Accy\u201d and \u201cACCstep\u201d ?\n  * please describe all abbreviation (TPR, TNR), including \u201ck\u201d in the table (they are not defined before the table is referenced)\n* Lines 381 - 393: Point the reader to the few-shot examples that were given to the models (or did I miss it?)\n* Conclusions section - the first paragraph is a summary of the paper, but does not contain conclusions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}