{
    "id": "rcmhydaEJp",
    "title": "Flow-based imputation of small data",
    "abstract": "Many challenges in the physical sciences can be framed as small data problems, where theoretical progress is hindered by the sparsity, low-dimensionality, and/or limited sample size of available empirical data compared to a physical system\u2019s numerous dynamical degrees of freedom. Developing trustworthy imputation methods for these datasets holds immense scientific importance. Normalizing flows are a promising model choice for imputation due to their ability to explicitly estimate sample likelihoods. However, research has shown that normalizing flows are often unreliable for out-of-distribution (OOD) detection in high-dimensional settings, which undermines their trustworthiness for imputation tasks. In contrast, low-dimensional settings provide opportunities to tractably evaluate and mitigate likelihood estimation errors, revealing strategies to reduce or eliminate specific error modes. We focus on the most stringent assumption in normalizing flows: diffeomorphism between the target and base distributions. This assumption introduces two distinct error modes, which we identify and address through a simple and effective strategy. Our approach significantly enhances the trustworthiness of normalizing flows for imputation in small data problems.",
    "keywords": [
        "Normalizing flows",
        "imputation",
        "diffeomorphism",
        "out of distribution detection"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "Likelihood estimation errors resulting from the diffeomorphism constraint in normalizing flows are explicitly quantified, and a tailored mitigation strategy is introduced for low-dimensional scientific applications.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=rcmhydaEJp",
    "pdf_link": "https://openreview.net/pdf?id=rcmhydaEJp",
    "comments": [
        {
            "summary": {
                "value": "This paper explores the use of normalizing flows for imputing incomplete small\ndatasets. It focuses on the diffeomorphism assumption and identifies two key\ntypes of errors: target mode error and base mode error. Overall, the\npresentation is convoluted, making the core contributions difficult to follow."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper explores an underexplored technique of data imputation using\nnormalizing flows, making it a novel application."
            },
            "weaknesses": {
                "value": "1. The paper lacks a theoretical analysis of its proposed approach.\n\n2. The presentation needs significant improvements. The following points are\n   indicative rather than exhaustive: \n   - Key terms and notations should be defined and explained. For example,\n     \"out-of-distribution\" is a key term for the paper and can be explained in\n     one or two sentences, but this is not done in the paper.  \n   - The introduction to normalizing flows needs improvements. Equation (1) is\n     simply a change of variables. Its connection to flows needs to be\n     clarified. What do you mean by \"approximately satisfy\"? If $p_x()$ is a\n     density function, is the value of (1) one? Is the main goal of normalizing\n     flows to learn a transform $f$ such that $f^{-1}(x)$ follows a Gaussian\n     distribution? What is the purpose of that? Also, what do you mean by\n     \"target sample\"?  \n   - Do the \"mode(s)\" discussed before equation (6) have the same meaning as the\n     \"mode\" defined in line 139? In any case, \"mode\" has a specific definition\n     in probability distributions, so it is better to use another term.\n   - Some equations, such as (8) and (9), do not contribute meaningfully to the\n     presentation. What do you mean by \"finitely bounded\"?\n   - Does ReLU satisfy the three conditions for diffeomorphisms?"
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The article investigates potential pathologies when using normalising flow methods for data imputation. Two chief sources of errors are explained and identified concerning the mismatch between the underlying number of assumed modes in the normalised space and the data generating distribution. These errors are demonstrated on a toy model, and methods are proposed to identify these issues and practice and avoid them. The integrated OOD residual is proposed as a metric for evaluating model success in an imputation context."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The explanation of the methods is clear and informative:\n\nThe examples listed in lines 31/32 are quite specific and niche, but not irrelevant.\nThe basic explanation of normalising flows is really clear and helpful.\nThe explanation of the two types of error from these models makes sense and sets up the rest of the article well.\nAnalysis of the toy target distribution is nice. The diagrams are very informative for the problems that you are trying to highlight and solve.\nIntegrated OOD residual is a reasonable metric to use for predictive accuracy."
            },
            "weaknesses": {
                "value": "Some notation could be changed, specifically the use of \u201cm_{b->}\u201d, which is a little too similar to m_b. Also m_t* vs m_t. \nDo you really not have any applications beyond the four-squares toy example? This article is unconvincing without any real data or even vaguely realistic simulated data.\nThere is very little discussion or implementation specifically relevant to imputation as set up in the introduction. If the point of the article is about types of error when training normalising flows then the framing around imputation is unnecessary and confusing."
            },
            "questions": {
                "value": "Article framing:\n\nIs low-dimensional imputation that difficult? What is the comparison with methodologically? Does this model just solve an easy problem with a greater (and not strictly necessary) degree of accuracy?\nThis article makes a couple of points well concerning potential pathologies of normalising flows and the underlying number of modes, but really doesn\u2019t fulfil any initial promises concerning imputation questions. Does this method make a substantial difference when used in the context of a larger analysis pipeline of which imputation is a single component? What about when compared to other imputation methods, which are pretty effectively developed for low-dimensional problems already? The article gives no indication of this, while implying from the title onwards that this is going to be the point.\nDoes this article have much to say about predictive uncertainty of the normalising flow methods? Can something like multiple imputation be done effectively here, where the uncertainty of the imputation method can effectively be marginalised out? Do we expect these uncertainties to be well-calibrated against reality? Neural networks do not in general offer well-calibrated probabilistic predictions: this might be considered a major drawback in an imputation context.\n\nExperiment design/rigour:\n\nWhat is the computational setup? Would this work on a laptop? Do you need a server? TPUs?\nDo you not have any applications beyond the four-squares toy example? Are you expecting people to apply this as an imputation method on real data without any more convincing experiments from your study?\n\nCompute questions:\n\nWhat\u2019s the differing compute load between different sizes of training data or dimensionality of problem? Is the issue of scaling to higher dimensions simply a case of computational cost or more fundamental?\n\u201cThe following architectural details and hyperparameters were optimised for 2D toy distributions by trial and error\u201d - more details here would be good. How much work did this really comprise? How much extra computation? Could this be done systematically or was it done by hand? Similarly for batch sizes/epochs/learning rates/regularisation coefficients. What was your metric for success when tuning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Normalizing flows assume a diffeomorphism exists from a base distribution to a target distribution. The authors note that this assumption implies that base and target distributions must have the same number of modes. Here, a mode is defined as a subset of the support of the distribution that is disconnected from the rest of the support. \nThe authors note that when using a standard normal distribution as a base distribution, this implies that spurious \"filaments\" between the true modes must exist to match the number of base modes (here, one) and transformed modes. They use a standard normalizing flow from Tensorflow on a 2D toy problem to investigate the effect of using a multimodal base distribution and how to select the number of base modes when the number of target nodes is unknown. They also introduce metrics, such as the Out-Of-Distribution (OOD) residual and ratios, to measure the performance of their method. The ultimate goal is to estimate the target distribution better to detect OOD samples."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The initial idea to notice that multi-modal distributions will inherently be ill-approximated by diffeomorphism is interesting and potentially solves a problem of the method. Trying to tackle this problem to perform data imputation, specifically for scientific issues, is an essential contribution. \nThe toy example is thoroughly explored with appealing figures."
            },
            "weaknesses": {
                "value": "The paper, and in particular section 2, lacks mathematical rigor. A mode is vaguely defined, while the Supp (support) is not defined. Moreover, since the paper decides to tackle approximate modes, the definition in equation 6 should have been modified to feature a tolerance epsilon to define approximate modes. And since approximate modes are considered, nothing stops a diffeomorphic map from mapping one mode to two approximately disjointed modes, provided that the filaments are sufficiently low probability in practice.  Thus, the conjecture (10) is more of a best-case scenario. Equation 21 is also not correctly mathematically defined and could be simplified using indicator functions.\n\nThe paper shows that some modes may be left unused, even when the total number of modes between base and target are equal. No solution is provided to correct this problem. \n\nI find the experiment lacking on several fronts. \nFirst, with a uniform target distribution and support made of squares, the diffeomorphism assumption is already violated, even accounting for the modes. This can be seen in the relatively poor recovery, which seems full of artifacts, even with the right number of base modes. It also seems to dominate the error, as the best OOD residual is with one base mode. \nThe introduction mentions applications in physical sciences, but the actual example is very toy-like. There is also no proof that it captures any of the characteristics of a real-world example. \nThe experiments seem to show that while having multiple modes for the base distribution would be better, this is a very risky endeavor. It risks leaving base modes unassigned, which results in high OOD errors. \nFinally, the presentation lacks color bars to indicate the magnitude of the errors, making it difficult to evaluate the results visually. \n\n\nThe title, introduction, and discussion are somewhat misleading. The toy example does not reflect physical science applications very well. The high dimensionality needs to be tackled. Despite being in the title, the small data part is only mentioned in the conclusion and only as a consequence of the computational complexity. At the same time, none of the difficulties of working with a few samples have been considered."
            },
            "questions": {
                "value": "It also seems that the OOD ratio is simply a Monte-Carlo estimate of an integral similar to the OOD residual, so why not use an integral here as well? \n\nWhy would a real-world example feature multiple modes? How do we know it is an important source of error for OOD classification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}