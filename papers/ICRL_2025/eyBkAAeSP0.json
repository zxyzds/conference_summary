{
    "id": "eyBkAAeSP0",
    "title": "Adversarial Suffixes May Be Features Too!",
    "abstract": "Despite significant ongoing efforts in safety alignment, large language models (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks that can induce harmful behaviors, including those triggered by adversarial suffixes. Building on prior research, we hypothesize that these adversarial suffixes are not mere bugs but may represent features that can dominate the LLM's behavior. To evaluate this hypothesis, we conduct several experiments. First, we demonstrate that benign features can be effectively made to function as adversarial suffixes, i.e., we develop a feature extraction method to extract sample-agnostic features from benign dataset in the form of suffixes and show that these suffixes may effectively compromise safety alignment. Second, we show that adversarial suffixes generated from jailbreak attacks may contain meaningful features, i.e., appending the same suffix to different prompts results in responses exhibiting specific characteristics. Third, we show that such benign-yet-safety-compromising features can be easily introduced through fine-tuning using only benign datasets, i.e., even in the absence of harmful content. This highlights the critical risk posed by dominating benign features in the training data and calls for further research to reinforce LLM safety alignment. Our code and data is available at \\url{https://github.com/anonymous}.",
    "keywords": [
        "LLM",
        "adversarial attacks",
        "safety alignment",
        "robustness"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "Using benign data to jailbreak LLM.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=eyBkAAeSP0",
    "pdf_link": "https://openreview.net/pdf?id=eyBkAAeSP0",
    "comments": [
        {
            "summary": {
                "value": "The paper aims to draw connections to the famous Ilyas et al. paper on adversarial examples being the result of unusual, non-robust features in the image domain. The authors aim to do so by running experiments that PGD-style optimize prompts that aim to produce a specific pattern, and then viewing transferability of these patterns between benign prompts (where they produce the pattern and a benign response) and harmful prompts (where they produce the pattern and a jailbreak response). So the feature is essentially something that elicits the pattern."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The choice of problem itself is interesting, there is little effort on any interpretability of adversarial suffixes in the literature.\n- Extensive experiments to tackle each section. I don't think the experimental setups themselves have issues (although you could use more SoTA models)"
            },
            "weaknesses": {
                "value": "- Maybe say soft prompt optimization when describing your embedding optimization strategy, that's the common term I believe.\n- Your loss in Equation 3 may be related to a sparsity constraint from prior work. Take a look: https://arxiv.org/pdf/2405.09113\n- The correlation analysis is a nice idea. But there may be problems - who is to say that PCC of 0 means little to no impact? It says there is no linear correlation, there may still be correlation. Any reason why you don't use existing token attribution techniques instead - I'm sure there must be some? \n- Section 3.1 - This is rather poorly written, and took me several reads to understand. Correct me if I am wrong - you are constructing synthetic responses that enumerate/speak in poems to give an answer, and then optimize a suffix against some victim model so that it responds in this enumerated/poetic fashion. The reason it reads poorly is because its not clear what the target string is as per your soft prompt optimization strategy. You should probably explicitly state that you are optimizing the suffix to produce this unusual output, and that by default the victim model doesn't respond in this fashion. Would recommend being clear about this in the beginning of the section.\n- Again Section 3.1 - In general, I am not sure why it is surprising that suffixes for benign prompts transfer to other prompts unless I see an example of the suffix. The prompt optimization objective seeks to maximize probability of specific token(s) in the output distribution, it could very well be that the suffix (in the token space) says something like \"noBeGIN w#(with1234\" - this is likely to cause an independence from the original prompt. I have seen suffixes like this when running GCG.\n- Section 4.2 - Again, I am having a hard time understanding why this feature is interesting when there is (to me) a high likelihood of the story telling/repeat response/basic program directive being present in the tokens itself (when considering the suffix projected to the token space). In this case, the feature is simply some suggestive input-level tokens, no? It is really only insightful if the tokens don't obviously suggest these directives on their own. If it helps, the analogy in my head is, say you have a a binary dog vs. cat image classifier - if an adversarial perturbation applied to a dog image (to make it look like a cat) already resembles the cat, then why is it meaningful or interesting to study? There needs to be some way of characterizing the actual content of the suffixes. \n- Also \"wearisomely\" is a typo."
            },
            "questions": {
                "value": "- Please address my 3 comments on the PCC, section 3.1, and 4.2. Perhaps I am misunderstanding something. I think the work has potential."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a modification to the usual adversarial suffix generation methods in the form of a regularizing term loss in the loss that increases based on the distance between the embedding of the adversarial suffix and the k nearest embedded datapoints. They find that if they fit a suffix from a dataset of, ex, poems, then conditioning the model on this suffix makes the model both output something poem-formatted (thus satisfying the criterion that the suffix has a \"benign\" property) but also that it induces a harmful output (so it has a \"malign\" property)."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "See below"
            },
            "weaknesses": {
                "value": "I learned nothing new after reading the paper so I consider the contribution very weak and the methodology unsound. The writing and lack of reference to prior work indicates to me that the authors are not familiar with the latest papers in the field. This is to be expected because of the overwhelming quantity of jailbreak papers accepted at conferences. But it does mean that this work will not have much impact.\n\nThe paper evaluates the suffix based on a) whether it produces the desired style and b) whether it is harmful. However, it's well known that roleplay-style jailbreaks will already work to induce harmful outputs. This is because the model is not used to knowing that it should map {harmful question} -> {refusal} in the role-played setting. So I don't think the paper has any contribution beyond that of the many papers that analyze the impact of roleplaying on inducing harmful outputs. At least if the authors could have showed that this suffix is somehow \"better\" than a roleplay prompt, that could have been something, but I didn't see that anywhere.\n\nI don't like the evaluation template that's used in Appendix A.1 and no comparison is done between this newly introduced evaluator and previously proposed evaluators or human judges. \n\nSome of the evaluation is done on models like Vicuna and Mistral which are not aligned and can always be trivially jailbroken. \n\nThe analysis that employs the Pearson correlation coefficient is incorrectly used to establish causation in the paragraph starting on Line 296. When introducing this, the authors note that they are computing correlations between the hidden state representations. The hidden state representation is the model's representation of the first token that will be decoded conditioned on the prompt, where the prompt consists of either a prefix (H_p), a suffix appended to a prefix (H_p+s), or just the suffix (H_s). And they state if the hidden state representation of the prefix+suffix (H_p+s) is more correlated with the representation of just the suffix (H_s) than the representation of just the prefix (H_s), then this means the generated adversarial suffix had more of a role in shaping the model's output than the prefix, where the prefix is the harmful instruction. But I don't agree with this. After all, the hidden state representation that's being captured here is just the first token that's being decoded. That first token might be a newline character if the suffix was fitted on Structure. But this doesn't tell us that the entire model's output is mostly dependent on the suffix. We'd need to do this analysis based on the entire model output.\n\nThe evaluation of the effectiveness of the adversarial suffix uses GCG and a variant of GCG. A number of suffixes are generated and then the \"best\" one is chosen. But it's not clear that there is a fair comparison. To be clear I don't doubt that this method is better than GCG, which is the literal first UAR ever proposed. I just want to see some evidence that the comparison is fair.\n\nThe authors note that the more important insight from the evaluation in their view is that the method is able to generate very successful embedding suffixes. This is pretty much trivial since the embedding space is continuous and the entire difficulty of the suffix optimization is that it's discrete, but this also enables the suffix to be used on APIs where embeddings can't be provided directly to the model. I don't really follow the conclusion of the paragraph starting on Line 343 (first paragraph in Section 4) where they state that their goal is to show that the embedding suffixes generated by the attack are effective in inducing harmful outputs, but it's not to conduct an adversarial suffix attack. \n\nThe section on finetuning LLMs on benign datasets compromising safety is well established by Qi et al 2023 and He et al 2024 [1]. I found nothing new by reading this section. The authors should include a comparison to these works and probably there are more that cite them; there have been many many many papers written on the compromise of safety due to finetuning such as Qi et al 2024 [2].\n\nThere are some minor typos (Line 339) (Lines 859-862).\n\nPutting screenshots of the OpenAI dashboard in the paper is pretty off-putting.\n\nIf I look at examples from the datasets I'm not convinced that the optimized suffix isn't learning something else. Ex, consider the example on line 986. It could just be that the suffix has distilled that the response should be very long. \n\nAttack papers should generally provide some commentary on defenses, and I saw no such commentary.\n\n[1] https://arxiv.org/abs/2404.01099 (COLM 2024)\n[2] https://arxiv.org/abs/2406.05946"
            },
            "questions": {
                "value": "Please provide the corrected code link.\n\nDid you evaluate the marginal benefit of the suffix over just instructing the model to respond in poetic verse?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work attempts to empirically demonstrate some correlation between adversarial suffixes (generated by an automated jailbreak attack) and benign instructions (e.g., ones that instruct the model to respond in some specific format). The authors conduct three main sets of experiments to demonstrate this phenomenon from different perspectives."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. I believe that the first experiment has the most surprising and scientifically interesting result. This suggests that adversarial suffixes do not simply work by directly \u201creverting\u201d the alignment process but also by taking the model out of the usual distribution (I suspect that the model is not safety-tuned / aligned under a very broad set of instructions and prompts).\n2. It is also interesting to see that different datasets, though all seemingly benign, can have a very different effect on the model on which they were fine-tuned (Experiment 3).\n3. The experiments are mostly well-designed and do support the main hypothesis of the paper.\n4. I\u2019m quite surprised that the nearest token project works at all in Algorithm 1. From personal experience and reading a few related papers [[1](https://arxiv.org/abs/2104.13733), [2](https://arxiv.org/abs/2405.09113), [3](https://arxiv.org/abs/2410.04234)] in this domain, I believe that naively projecting embeddings to tokens has not found much success. Although this is slightly out of scope for this paper, it would be interesting to deep dive into why this algorithm works and the others have failed."
            },
            "weaknesses": {
                "value": "1. **Additional experiments.** I believe that this paper contains interesting observations, but I would really want to see more extensive experiments and a deeper dive into the underlying reasons.\n    1. I hope to see more response formats/styles in Experiment 1. What about response as a song, with some persona, in a different languages, etc.?\n    2. Is there any correlation between these formats and the datasets used for aligning the model? Perhaps, these formats are never seen by the model during safety tuning or RLHF, which makes them OOD and can easily avoid the refusal?\n    3. In Experiment 1, Llama-2 is the only model that underwent safety tuning (I believe that Vicuna and Mistral are only instruction-tuned). I\u2019d like to see whether this observation applies broadly to aligned models.\n    4. Experiment 1 only covers the embedding attack. I\u2019m curious whether the result also extends to the token-level attack (hard prompt)?\n2. **Other adversarial suffix generation algorithms**\n    1. I don\u2019t think I understand the point of Algorithm 1. There are many existing prompt optimization methods for both hard and soft prompts. So why do the authors also propose a new algorithm?\n    2. I\u2019m also wondering whether the adversarial suffixes generated from different algorithms also exhibit a similar phenomenon. Is there anything special with Algorithm 1? (This is related to Question 5 below).\n3. **Novelty**\n    1. Experiment 3 is not entirely novel as it is a well-known phenomenon today.\n    2. If we know that fine-tuning can undo the alignment of LLMs, it is perhaps not very surprising that prompt tuning could also undo the alignment as well. Experiment 1 is essentially equivalent to soft prompt tuning.\n4. **PCC metric.** Since X, Y are vectors, cov(X, Y) should be a covariance matrix? Based on the caption of Figure 2, it seems like each dot is one PCC value computed on a pair of prompt and suffix. If my understanding is correct, this is treating each dimension of the embedding as a \"sample\" which seems wrong to me. The right interpretation would be to compute PCC of one embedding dimension across all the samples (pairs of prompt and suffix). That said, it seems difficult to interpret this number so my suggestion is to just compute a distance (Euclidean or cosine) between the two vectors and just average over all the pairs/samples.\n\n### Nitpicks\n\n1. Figure 1: This example is not that clear to me.\n2. Section 3.1 (Dataset Construction): It is unclear to me how this experiment setup has to do with \"generate suffixes that capture benign features.\" I'd suggest at least describing the purpose of the two constructed datasets, what they represent, what the desired outcomes are, etc.\n3. Section 3.1 and the term \"feature extraction\": This terminology is also quite misleading to me; When seeing the term, I expect that we will take some intermediate representation of a neural \nnetwork given some inputs. However, I think \"feature extraction\" here refers to the conceptual \"feature\" of the suffix (like \"cat features\", \"dog features\" in the computer vision domain) -- There's no \"extraction\" here I believe."
            },
            "questions": {
                "value": "1. L243 (\u201dWe apply the method presented in Section 2 to generate suffix embeddings\u2026\u201d): What is the target response y? I assume that it is the one generated by Llama2-7B-chat-hf above. I just want to confirm that this is correct.\n2. L248 (\u201dFor each dataset, we generate one suffix in the form of embedding\u2026\u201d): Is this optimized with all samples at once (like universal attack) or just take the best embedding out of N where N is the dataset size? How is the \"best performance\" exactly determined here?\n3. L312 (\u201dWe first apply our method presented in Section 2 to construct multiple suffixes\u2026\u201d): How many suffixes are generated using Algorithm 1? Why are the numbers different between GCG (1k) and AmpleGCG (5k)? \n4. L323: GCG generates only one for each prompt-response pair? Not universal, i.e., optimizing for multiple pairs at the same time?\n5. Section 4.2: Does suffixes from GCG and AmpleGCG also exhibit any response style/format pattern as observed in suffixes from Algorithm 1? If not, what makes it different? \n6. L430 (\u201dthree constructed based on the embedding universal adversarial suffixes generated in Experiment 2.\u201d): What are the reasons for using these datasets from Experiment 2? Is the intention to say that some response formats/styles are better correlated with harmfulness or are just better at undoing the alignment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper reports on a finding that suffixes\nthat encourage certain benign behavior, can also\ncause a model to respond in unsafe ways.  It\nalso reports that fine-tuning a model on certain\nbenign datasets, can cause the fine-tuned model\nto become less safe."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper describes some interesting phenomena\nrelated to safety alignment, and reports on the\nresults of several experiments.  The results are\nsurprising.\n\nThese results have implication for safety\nalignment and limitation of current alignment\nmethods."
            },
            "weaknesses": {
                "value": "I'm not sure how significant the results are.\nI'm struggling a bit to synthesize what their\nimplications are and what I've learned.  This\nmight be my own shortcomings.\n\nI find the framing challenging to understand.\nI don't understand in what sense the suffix is\na \"feature\", or what it means to call a suffix\na \"feature not a bug\".\n\nI find the use of suffixes that are embedding\nvectors (instead of tokens) fairly unconvincing.\n\nI am concerned the \"LLM as judge\" may be inaccurate\nand may be classifying harmless responses as safety\nviolations.  For the 4 examples shown in this paper,\nin my opinion half of them involve such erroneous\nclassifications.  This might throw off the paper's\nresults.\n\nDetailed comments\n\nFig.1: Are these real responses, or did you make them up?\nIf real, have you copied them accurately?  e.g., \"converving\"\nRather than <suffix representing...>, can you show the actual\nsuffix?  What is the rightmost column representing, how\nshould I interpret it, and why is it there?\n\nFig.1: The caption claims that the middle column violates\nsafety alignment, but I don't see any violation of alignment.\nThere is no racist joke.  I don't think it violates safety\nor any safety policy to respond in the way shown here.\n\nSec 2.1: Why is it reasonable to suffixes to be embedding\nvectors that don't correspond to any input token?  GCG\ngenerates adversarial suffixes that are input tokens.\nYou might be finding embedding vectors that don't correspond\nto any input token, and if so, I don't see how that is\nvery relevant or provides strong enough evidence in\nsupport of your hypothesis.\n\nRegularization (2) does not suffice to ensure that the\nembedding corresponds to any input token.  The paper\nclaims it \"encourages the optimized suffix embeddings\"\nto correspond to valid token sequences (and \"encouraging\nthe generation of valid token embeddings\"), but that is\nan overstatement; it doesn't do that.  It encourages\nproximity to embeddings of valid tokens, which is not\nthe same thing.\n\nSimilarly, when you compute a suffix (embedding vectors)\nusing regularization (2), it is misleading to call it a\n\"token sequence\" when it is actually an embedding vector\n(that cannot be exactly produced by any sequence of\ntokens) rather than a sequence of input tokens.\n\nSec 3.1: Is there any overlap between the set of 1000\nbenign prompts used for suffix generation and the 500\nprompts used for evaluating transferability?\n\nSec 4.2: I'm not clear on what suffixes you used\nfor these experiments.  Are they suffixes generated\nto produce poetry outputs?  to produce structured\noutputs?  suffixes generated by GCG?  etc.?\n\nI'm confused.  Don't you generate the suffix\nto induce responses that are poems or lists?  It seems\nweird that a suffix optimized to produce lists would\nin practice produce BASIC programs (for example).  What\nis going on here?\n\nThe name of the programming language is BASIC, not\n\"Basic\" or \"basic\".  It is confusing to use \"basic\"\nwhen you mean BASIC.\n\nYou say 3 suffixes trigger a distinct response style.\nWhat is the denominator?  In other words, how many\nsuffixes did you try, to find 3 that have this\nproperty?\n\nSection 5.1: I don't understand how using Llama3guard\nmakes sense.  Llamaguard is not designed to respond to\narbitrary prompts.  Instead, it is designed as a\nclassifier, to classify input as toxic or not.\n\nFig.14,16: This does not violate OpenAI's policies\nand is not a safety violation."
            },
            "questions": {
                "value": "Can you pick a random sample of responses judged\nharmful, and manually rate whether they are harmful\n(e.g., violate OpenAI policies), to validate the\naccuracy of your judge?  I'm concerned it might\nnot be very accurate.\n\nCan you randomly sample 10 responses judged harmful,\nand show them to us, so we can form our own opinion?\n\nAre you able to demonstrate similar results with\nsuffixes that are tokens, not embedding vectors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}