{
    "id": "xEZiEhjTeq",
    "title": "Stagewise Development in Transformers and the Geometry of the Loss Landscape",
    "abstract": "We show that internal structure emerges in discrete developmental stages during transformer training, for language modeling and in-context linear regression. We introduce a method for detecting boundaries between these stages by probing the degeneracy of the geometry of the population loss. The measure of degeneracy we use is the Local Learning Coefficient (LLC), which is derived from singular learning theory. We establish the validity of the stages revealed by the LLC using a range of behavioral and structural metrics. In a majority of the stages the loss decreases and the geometry becomes less degenerate, which is consistent with an emerging literature on stagewise learning and saddle-to-saddle dynamics in neural network training. However, we also discover several stages in which the geometry becomes more degenerate, and in one example link this to a phenomenon that we call layer normalization collapse. These findings provide new insights into the intricate process of transformer training and underscore the importance of loss landscape geometry in understanding model development.",
    "keywords": [
        "Science of deep learning",
        "loss landscape geometry",
        "training dynamics",
        "singular learning theory"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "Transformers learn in discrete developmental stages that can be discovered by studying the local geometry of the loss landscape.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=xEZiEhjTeq",
    "pdf_link": "https://openreview.net/pdf?id=xEZiEhjTeq",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates the emergence in stagewise development of internal structures in the training of transformer models, specifically for tasks like language modeling and in-context linear regression. The authors utilize the Local Learning Coefficient (LLC) to measure geometric degeneracy in the loss landscape and identify distinct training stages of the model\u2019s behavior and internal structures. Beyond general loss decreases in model training, the authors discover several stages in which the geometry becomes more degenerate linking to a phenomenon named \u201clayer normalization collapse\u201d. These findings provide valuable insights into the complex processes of transformer training and underscore the importance of loss landscape geometry in understanding model development."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces a novel method Local Learning Coefficient (LLC) to identify the stage boundaries\n2. The evaluation methods proposed in this article have been verified for two different types of tasks providing an enhanced understanding of transformer model training.\n3. Bring a new insight to the model stability and robustness by showing a phenomenon of \u201clayer normalization collapse\u201d."
            },
            "weaknesses": {
                "value": "1. This method has certain limitations in model selection. The language model only has 3 million parameters. Perhaps these methods cannot be directly generalized to large models such as GPT and BERT. Therefore, the universality of this model needs to be verified. \n\n2. The LLC method has certain limitations. Because it's estimation will be affected by the training parameters. When the parameters are not the local minimum of the loss, the estimation of LLC might have some bias. Though the article attempts to estimate through the SGLD method, since LLC is sensitive to hyperparameter selection, this instability will affect the credibility of experimental results.\n\n3. The author proposed the concept of \"layer normalization collapse\", but lacked some more in-depth discussions such as the causes and some quantitative analysis. These analyzes will add to the value of this study."
            },
            "questions": {
                "value": "1. Can this method be extended to a larger range of tasks such as image processing? Can similar patterns also appear?\n\n2. Do you have theoretical reasons to believe your method would or would not scale to larger models?\n\n3. Could you provide metrics on the rate or extent of collapse across different model sizes or training regimes.\n\nFor other questions, please refer to weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper employs \"Local Learning Coefficient\" (LLC), a recently proposed metric, to measure the geometry of loss landscape during training transformers.\n\nBy conducting experiments on two-layer attention-only transformers on some simple tasks, the authors find that the training could be divided into several discrete development stages according the LLC features.  And then different behavioral and structural features are found among different stages.\n\nThis work could bring meanings to reveal the learning mechanistic (and decrete development) for transformers."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This work finds there are some discrete development stages for transformers via analyzing the geometry of loss landscape. It could bring insights for related works about mechenistic interpretability to transformers.\n\n2. I like the analyses in Stage validation section, which tries to connect LCC trend and some visible important features, though I also have some questions about this section.\n\nOn the whole it is an interesting work and I expect to have more discussion in rebuttal period. I am open to improve the score if my following concerns are well addressed."
            },
            "weaknesses": {
                "value": "1. I am concerned about the theory adopted in this paper, LLC. It seems that the theory of LLC is not widely accepted in the research community and may subject to potential pitfalls and limitations. The lack of peer-reviewed studies on LLC means that we have a limited understanding of its applicability, reliability, and overall validity.\n\n2. In Section 4.3, the authors state that First-layer previous-token heads start to form, and the evidence is that Figure 2(b) top. However, I think it is more like a confuse cause and effect. After the authors discover that two specific heads start to have high previous-token score in LM3 stage, the previous-token heads, 1:2 and 1:5, are then indicated. Whilest, In LM2 stage, there are already many heads that get high scores, why aren't they the previous-token heads? Furthermore, LM3 seems less meaningful compared to other stages.\n\n3. I think this paper might neglect the writing of some specific experimental implementation methods, which needs to be clarified more.\n- LCC is based on the measure of loss. How do you measure loss? Specifically, what dataset or distribution (and how many samples) do you use to measure loss? Would different dataset lead to totally different results? (that is, different loss landscapes for different validation datasets).\n\n[1] Lau, Edmund, Daniel Murfet, and Susan Wei. \"Quantifying degeneracy in singular models via the learning coefficient.\" arXiv preprint arXiv:2308.12108 (2023)."
            },
            "questions": {
                "value": "1. In Figure 1, would the training process contain more stages (i.e. d(LCC)/dt = 0 points) if you lengthen the training? If there are more stages, what is the corresponding features (behavior and structure) and why do you think the first 5 stages are the most important?\n\n2. What is the mathematical basis for d(LCC)/dt = 0 points? Why are these points critical and able to become the boundary of development stages (from mathematical perspective)? Otherwise, is this a target-guided results (that is, we have discrete stages first and we dig the math features of the boundary)?\n\n3. I notice that this paper did not employ a learning rate scheduler throughout the training process. Though to some degree I acknowledge the setup for controlling variables, learning rate is a very important factor to determine the loss landscape. Many previous works point out that lower learning rate helps models to converge to a local minimum, and decayed learning rate can help models generalize better due to more flattened local minimum. What do you think about the impact of learning rate schedule on your work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the Local Learning Coefficient (LLC) as a novel approach to explain phase transitions in transformer training dynamics. LLC is calculated by measuring the expected difference between the posterior and local optimal solutions. The authors establish connections between LLC and the geometry of model likelihood, offering a new perspective on transformer learning behavior."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Introduces a new metric (LLC) for analyzing transformer training dynamics and phase transitions, providing an alternative to traditional per-token loss measures\n\n- Presents comprehensive experimental analysis with thorough ablation studies\n\n- Provides detailed methodology and experimental setup documentation"
            },
            "weaknesses": {
                "value": "- The relationship between loss changes and LLC lacks clear description. While Table 1 presents changes in both ICL loss and LLC, it \n does not establish a definitive connection to phase transitions. In Figure 1 the dynamics of ICL loss and LLC do not match or describe the phrase transitions.  The paper can benefit from quantitative correlation metrics between ICL and LLC to strengthen the qualitative observations.\n\n- Despite extensive analysis, the paper primarily reinforces existing findings from Olsson et al. (2022) rather than presenting novel insights into transformer learning dynamics"
            },
            "questions": {
                "value": "1. What are the specific advantages of LLC over per-token loss that justify its adoption as a preferred metric for analyzing transformer training dynamics?\n\n2. The LLC computation relies on finding $w*$ (local minimum) through stochastic Langevin dynamics. How can LLC be reliably estimated with respect to weights $w_t$ during the actual training process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper leverages the LLC as metrics to establish the developmental stages for language modeling of transformer and in-context linear regression. These stages reveal the developmental changes of models in language modeling."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe authors analysis the behavioral and structural changes of each stages to confirm the meaning of LLC, and each stage actually has the valuable behavioral changes to reveal the learning process in the language modeling.\n2.\tThe paper uses the realistic data to train analysis the language modeling of transformer.\n3.\tThe experiment is adequate and reasonable and the paper is well written."
            },
            "weaknesses": {
                "value": "1.\tThe used transformers are one and two layer attention-only. Whether the same behavioral and structural changes can be seen in more larger transformer. The scaling may affect the loss landscape."
            },
            "questions": {
                "value": "The same as above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}