{
    "id": "FHsaa6lZMp",
    "title": "Fine-Grained Machine-Generated Text Detection",
    "abstract": "Machine-Generated Text (MGT) detection identifies whether a given text is human-written or machine-generated. However, this can result in detectors that would flag paraphrased or translated text as machine-generated. Fine-grained classification that separates the different types of machine text is valuable in real-world applications, as different types of MGT convey distinct implications. For example, machine-generated articles are more likely to contain misinformation, whereas paraphrased and translated texts may improve understanding of human-written text. Despite this benefit, existing studies consider this a binary classification task, either overlooking machine-paraphrased and machine-translated text entirely or simply grouping all machine-processed text into one category.  To address this shortcoming, this paper provides an in-depth study of fine-grained MGT detection, categorizing input text into four classes: human-written, machine-generated, machine-paraphrased, and machine-translated. A key challenge is the performance drop on out-of-domain texts due to the variability in text generators, especially for translated or paraphrased text. We introduce a RoBERTa-based Mixture of Detectors (RoBERTa-MoD), which leverages multiple domain-optimized detectors for more robust and generalized performance. We offer theoretical proof that our method outperforms a single detector, and experimental findings demonstrate a 5--9\\% improvement in mean Average Precision (mAP) over prior work on six diverse datasets: GoodNews, VisualNews, WikiText, Essay, WP, and Reuters. Our code and data will be publicly released upon acceptance.",
    "keywords": [
        "Machine-Generated Text Detection",
        "Fine-grained Classification",
        "Mixture of Experts"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=FHsaa6lZMp",
    "pdf_link": "https://openreview.net/pdf?id=FHsaa6lZMp",
    "comments": [
        {
            "summary": {
                "value": "The paper presents novel task of fine-grained MTD detection, where the detector should be able to predict 4 labels: humn-generated, machine-generated, machine-translated and machine-paraphrased. Novel architecture for this task is roposed, refered as MoD (Mixture of Detectors), consisting several detectors for individual domains, and the trainable router. \n\nTheoretical results are presented demonstrating the theoretical benefits of the proposed architecture over the individual detector.\n\nThe evaluation is done on several popular MTD datasets for various generator models. Besides, the OOD evaluation is presented. The method outperformes  the most of the considered baselines by a large margin"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper proposes novel important tasks of fine-grained MTD. The presented analysis demonstrates that indeed Machine-Generated, Machine-Translated and Machine-Paraphrased texts have unique features and different usage scenarios, and it is important to distinguish them\n- Novel architecture of MTD is proposed\n- The evaluation is done of large amount of data domains and generator models, and includes OOD setup. The proposed method outperforms most of the considered baselines by a large margin."
            },
            "weaknesses": {
                "value": "- The theoretical framework in Sec 3.3 is not quite clear (see Questions)\n\n- The benefit of the proposed method over standard Mixture-of-Expert is not clear\n \n- In section 3.1, the statistics of the generated data is absent (see Q4)\n\n- The most of the baseline models are Roberta-based classifiers. Comparing to the proposed method, they have k times less parameters, where k is the number of individual detectors. Only two baselines do not belong to this class: RoBERTa-MoS and Binoculars; comparing to them, the proposed method have marginal to no improvement\n\n- No comparison to Roberta-MoS in OOD setup    \n\n- In Sec. 4.3, the Qualitative result paragraph describes the properties of the dataset rather then the classifier results. There is no information about the typical errors, or any other qualitative description of the results of the proposed detector."
            },
            "questions": {
                "value": "Q1. Please explain the notation used in the Definition and Theorems in Sec. 3.3\n- What is Patch? Does it correspond to a set of tokens, or a subset of features (e.g. coordinates) in the text embeddings, or smth else?\n- What is feature vector $v_k$? The index indicates the whole cluster, but it is used for the description of the individual data point \n- What does the notation $y\\alpha v_k$ mean? Is it a vector multiplied by 2 scalars $y$ and $\\alpha$ ?\n\nIn general, could you please provide the example of the considered setup in the Machine-Generated Text Detector domain, defining patches, clusters, data features, distraction features and the noise.\n\nQ2. How many detectors were used for each dataset? Does this number correspond to the theoretical bound from Theorem 2?\n\nQ3. How are ChatGPT-D and Roberta-MPU atapted to fine-grained MTD setup? Are they fine-tuned on the same dataset as MoD? \n\nQ4. Please describe the statistics of the train/test dataset used in Tables 1 - 3 (its fine-grained part). For Generator models, please specify which version of each model is used (i.e. number of parameters)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents an in-depth study on fine-grained Machine-Generated Text (MGT) detection, which can classify text into four categories: human-written, machine-generated, machine-paraphrased, and machine-translated. The authors note that existing detectors struggle with out-of-domain text, particularly for translated or paraphrased text. To address this, they propose a RoBERTa-based Mixture of Detectors (RoBERTa-MoD) which uses multiple detectors optimized for different domains to improve performance. The authors provide a theoretical proof that their method outperforms a single detector and experiments show a 5-9% improvement in mean Average Precision (mAP) over previous work on six diverse datasets. The authors also introduce a data preparation process to generate articles across different fine-grained categories, enabling automatic creation of training and evaluation data for the task."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) It presents an in-depth study on fine-grained Machine-Generated Text (MGT) detection, a topic that has been overlooked in previous studies. By classifying text into four categories (human-written, machine-generated, machine-paraphrased, and machine-translated), the research contributes significantly to the field. \n2) The paper introduces the RoBERTa-based Mixture of Detectors (RoBERTa-MoD), a novel method that uses multiple domain-optimized detectors for more robust and generalized performance. This method addresses the performance drop on out-of-domain texts, a key challenge in MGT detection. \n3) The paper's quality is evident in the theoretical proof provided for the method's superiority over a single detector. Additionally, the research is significant as it achieved a 5-9% improvement in mean Average Precision (mAP) over prior work on six diverse datasets."
            },
            "weaknesses": {
                "value": "1) The concept of employing a mixture of experts (MoE) for a task is not unusual, considering its extensive application in various tasks such as general LLM, summarization, and machine translation. While the application of MoE in text detection is relatively new, it is not a groundbreaking concept in terms of its fundamental idea. \n2) The study lacks an ablation analysis on MoD. Questions such as the influence of the number of detectors on the results, the effect of different corpus (domains) on detection, and the likelihood of the router specializing to specific detectors given an input article, remain unanswered. \n3) The paper doesn't delve deep into the confusion between classes, for instance, between translate and human/paraphrase. As the paper is centered on the fine-grained detection of machine-generated text, such analyses about the challenges of fine-grained detection are anticipated and would offer valuable insights to the readers. \n4) The experimental results may not have been compared in a fair manner. However, due to the lack of clear descriptions of the settings for the baselines, I will reserve my judgment until the rebuttal period, during which I expect a response."
            },
            "questions": {
                "value": "LN053: what is the special of your method compared to Abassy et al., 2024 given that they both do fine-grained MGT detection?\n\nLN091: have you considered GPT models for the detection? What is the underlying reason for choosing RoBERTa? \n\nLN320: how do you calculate the AUROC for the 4-class classification problem?\n\nTable 1: the experiments are limited to generations from open-source models. Have you considered latest closed-source models like GPT4, Gemini, and Claude3?\n\nTable 1: In my view, a RoBERTa-single model trained on the same data is a demanded baseline to demonstrate the advantage of the MoE design.\n\nTable 1: Are models like ChatGPT-D, RoBERTa-MPU, and LLM-DetectAIve trained on the same data as the proposed method?\n\nLN427: How are your zero-shot experiments designed? When we say zero-shot, we refer to a method without any task specific training. How could your MoD method do the zero-shot given that it requires a joint training of the router and the detectors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addressing the detecting of machine generated text proposed to extend the traditional binary classification (machine or human generated) to a four-classes problem: human-authored, machine-generated, machine-translated or machine-paraphrased. The authors propose a multi-class classification model uses 4 Roberta-models and a gating mechanism. The whole network is trained in two stages"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* An interesting twist to an important problem\n\n* An analysis that combines both theoretical insights and empirical ablations"
            },
            "weaknesses": {
                "value": "* Motivation: the idea behind a 4-class problem (instead of the more traditional 2-class) is motivated by the fact that machine-modified text (translated or corrected) might not be as harmful as text that is generated by a model from scratch. While this is a somehow appealing explanation, it would be interesting to see if that difference actually has an empirical impact. Sect 4.5 makes this attempts, by applying the proposed method on existing benchmarks, obtaining good scores (although not always outperforming the current state-of-the-art). More interesting to me would be an analysis on how `Binoculars` performs on the new datasets (considered as binary problem). Another interesting aspect would be to verify if the more fine-grained classification actually results in a better binary classification."
            },
            "questions": {
                "value": "* Could you address the weakness mentioned above?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates fine-grained MGT detection; the authors propose to categorize an input text into four classes: human-written, machine-generated, machine-paraphrased, and machine-translated. They introduce a data preparation process to generate articles across different fine-grained categories, enabling the automatic creation of training and evaluation data for the task. The paper introduces a RoBERTa-based Mixture of Detectors (RoBERTa-MoD) for fine-grained MGT detection, which leverages multiple domain-optimized detectors for more robust and generalized performance. The paper presents theoretical proof that the method outperforms a single detector, and experimental findings show an improvement in mAP over prior work on six various datasets: GoodNews, VisualNews, WikiText, Essay, WP, and Reuters."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "\u2014 The idea of separating into three categories is intuitive and transferred from the actual use cases. However, it makes the task more complicated as the generation style is similar for paraphrasing/translating/generation using the same models.\n\n\u2014 It is beneficial for nature to theoretically assert if some models and approaches are worse or better without extensive model training.\n\n\u2014 Reproducibility statements, limitations, and ethical considerations are included. Clear contribution. The code and data will be publicly released upon acceptance."
            },
            "weaknesses": {
                "value": "Some questions for reproducibility of the research.\n\n\u2014 Please include information about the strategy of round-trip translation and paraphrasing. The rationale behind round-trip translation is not discussed. Was the translation performed for each language using all the models? \nAre the authors certain that the prompt \"Paraphrase/Translate the following article: x.\" was used consistently across all models? If so, how does LLAMA determine which language to translate? What are the specific languages used? \nHow do authors gather paraphrases and translations? For each article, there should be a minimum of four different translations or paraphrases. Have the authors confirmed that removing the first sentence from the text does not alter the meaning of the translation? \n\n\u2014 For the training data, have the authors saved the model's proportion distribution? No statistics for the training corpora. How many examples were used for each class? \nSection 3.1 needs more details.\n\n\u2014 Line 372  `All methods were fine-tuned on data from Llama-3 (Touvron et al., 2023) and Qwen-1.5 (Bai et al., 2023) and then evaluated on all LLMs.`\nWhich data? How much data in which format? Maybe the authors mean: \"evaluated on the data from all the LLMs\"? I guess the methodology was to check in out-of-domain evaluation so that the data formed with different models and not evaluated by the same models in an LM-as_judge manner. The formulations are confusing. It is not clear from the texts what data you trained detectors, fine-tuned which exact models, and with what models you evaluated what.\n\n`Llama-3 and Qwen-1.5 are in-domain generators for training the detector, and StableLM-2, ChatGLM-3, and Qwen-2.5 are out-of-domain generators to evaluate the model\u2019s generalization ability.`\nAm I right that, based on the data from models StableLM-2, ChatGLM-3, and Qwen-2.5, were the detectors not trained?\n\n`However, in the same dataset, human-written articles in the training and testing sets may follow similar data distributions`\nThere is no information on whether they may or may not.\n\n\u2014 Line 325: `LLM-DetectAIve is directly trained on fine-grained MGT data, which can be considered as a fine-tuned RoBERTa.`\nWhy should it be considered? No explanations/justifications\n\n\u2014 Figure 5. It seems like the Qualitative Results are based on one example. Paraphrasing can change factual information as well as translation. The paper needs some quantitative metrics to catch it."
            },
            "questions": {
                "value": "\u2014 The translated and paraphrased texts, if created using LLMs, can also contain misinformation and factual errors. It depends on the LLM; the percentage of errors is much rarer than that of machine generation, but it still needs to be checked.\u00a0\n\n\u2014 \"As discussed in the Introduction,\" add the link to the Introduction section.\n\n\u2014 Line 307: Set the spaces in cite like in:\u00a0 `GoodNews(B`\n\n\u2014 The paper would improve from the footnotes to direct links for the open datasets and a clear explanation of the steps with data processing.\n\n\u2014 Table 1 would improve if the information about the model's size was added.\n\n\u2014 It's a bit strange not to see the results section.\n\n\u2014 The results would be interesting to check on the different lengths of the output. We could see the correlation with length.\n\n\u2014 Factual error: LLM-DetectAIve distinguishes four categories: (i) human-written, (ii) machine-generated, (iii) machine-written, then machine-humanized, and (iv) human-written, then machine-polished. The idea is different from paraphrasing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}