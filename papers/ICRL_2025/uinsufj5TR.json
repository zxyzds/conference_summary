{
    "id": "uinsufj5TR",
    "title": "Enabling Sparse Autoencoders for Topic Alignment in Large Language Models",
    "abstract": "Applications that generate topic-aligned output from large language models (LLMs) are frequently limited by the computational intensity and lack of interpretability of existing approaches, like fine-tuning. Recent work shows that Sparse Autoencoders (SAE) applied to LLM layers have neurons corresponding to interpretable concepts. Consequently, these SAE neurons can be modified to align generated outputs, but only towards pre-identified topics and with some parameter tuning. Our approach leverages the interpretability properties of SAEs to enable alignment for any topic. This method 1) scores each SAE neuron by its semantic similarity to an alignment text and uses them to 2) modify SAE-layer-level outputs by emphasizing topic-aligned neurons. We assess the alignment capabilities of this approach on diverse public topics datasets, including Amazon reviews, Medicine, and Sycophancy, across open-source LLMs, GPT2, and Gemma with multiple SAEs configurations. Experiments aligning to medical prompts reveal several benefits over fine-tuning, including increased average language acceptability (0.25 vs 0.5), reduced training time across multiple alignment topics (333.6s vs. 62s), and acceptable inference time for many applications (+0.00092s/token). Our anonymized open-source code is available at https://anonymous.4open.science/r/sae-steering-8513/README.md.",
    "keywords": [
        "Alignment",
        "SAEs",
        "Mechanistic Interpretability",
        "Large Language Models"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "Proposing and evaluating LLM-independent layer-level steering with open-source sparse autoencoders offers a promising boost in accuracy and compute efficiency over current methods.",
    "creation_date": "2024-09-17",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-13",
    "forum_link": "https://openreview.net/forum?id=uinsufj5TR",
    "pdf_link": "https://openreview.net/pdf?id=uinsufj5TR",
    "comments": [
        {
            "comment": {
                "value": "For anonymization, we used the standard anonymization platform (anonymous.4open.science) and included our names/affiliations in the regex to ensure it functioned correctly. Between submission and now, it seems the tool\u2019s real-time performance may have changed unexpectedly. We've since removed our reliance on it.\n\nIf possible, we'd greatly appreciate any feedback you might be able to offer, even from a general scientific perspective. We think it\u2019s a promising idea and would value your insights."
            }
        },
        {
            "summary": {
                "value": "This paper introduces an approach for achieving topic alignment in LLMs using Sparse Autoencoders (SAEs), offering an alternative to computationally intensive fine-tuning methods. The key contribution is a technique that scores SAE neurons based on their semantic similarity to desired alignment topics and then modifies layer outputs by emphasizing high-scoring neurons.\n\nThe methodology consists of two main components: 1) A scoring mechanism that evaluates SAE neurons' relevance to target topics using semantic similarity and a reference prompt set, and 2) A \"swap\" approach that modifies SAE outputs by emphasizing aligned neurons in a context-sensitive way. This enables topic alignment without extensive parameter tuning while maintaining interpretability through the SAE structure."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper presents a new application of SAEs for topic alignment that addresses key limitations of existing approaches like fine-tuning in the topic alignment research area. The scoring and modification methods are generally well-motivated.\n- The experimental methodology covers multiple LLMs, topics, and SAE configurations. The evaluation metrics cover both performance (perplexity, linguistic acceptability) and efficiency aspects."
            },
            "weaknesses": {
                "value": "My main concern is the lack of qualitative performance comparisons with conventional topic alignment methods, as indicated in the first paragraph of your introduction -- the proposed SAE-based method has advantages in computational efficiency, interpretability, etc. You should compare them. Additionally, the current organization of the paper is not self-contained; for instance, ablation studies should be rearranged to the main text. Regarding the motivation, justifications for evaluation metrics and methods in lines 200-206 (e.g., why these three aspects (aligned, polysemantic, unaligned) of neurons) should be clearly presented."
            },
            "questions": {
                "value": "See weaknesses. I suggest including more baseline results in the rebuttal phase."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes using Sparse Autoencoders (SAEs) for topic alignment in LLMs to achieve efficient and interpretable topic alignment by scoring SAE neurons based on their semantic similarity to specific alignment topics. Authors demonstrate the effectiveness of the proposed method across datasets (e.g., medical and Amazon reviews) and models like GPT-2 and Gemma to show the advantage of SAEs in reducing training times and improving interpretability. The authors also introduce contamination metrics to quantify topic alignment uncertainty."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper introduces an interesting approach by applying SAEs to align LLMs with specific topics, which is relatively unexplored. Compared to FT, this approach is more interpretable and efficient.\n\n2. This paper evaluates the proposed method across several datasets and models, showing the effectiveness and generalizability of the results. Further analysis, including the alignment score, SAE neurons distribution, and model generation output, covers various aspects of alignment performance.\n\n3. The contamination metric is simple to evaluate the uncertainty in alignment."
            },
            "weaknesses": {
                "value": "1. The experiments are conducted on GPT2 and Gemma. It is unclear if the method is easy to use on other model families or larger models e.g. LLAMA 405B. It seems that the configuration would be hard to obtain on larger models.\n2.  It seems that the configuration of SAE is varied on different topics.  Further investigating the impact of different configurations may be valuable."
            },
            "questions": {
                "value": "1. Does the proposed score/method evaluate the impact on the polysemantic neurons?\n2. Is this method easy to scale with much larger LLMs with billions of parameters?\n3. The contamination metric is used to quantify alignment uncertainty. Is there any human evaluation other judgment to show the reliably of this metric?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel approach for using Sparse Autoencoders (SAEs) to enable topic alignment in Large Language Models (LLMs) without requiring computationally intensive fine-tuning. The authors propose two main methodological contributions: (1) a scoring mechanism to identify SAE neurons relevant to alignment topics, and (2) a modification approach that uses these scores to alter SAE layer outputs in a context-sensitive way. The paper demonstrates the effectiveness of their approach across multiple datasets (Amazon reviews, Medicine, Sycophancy) and models (GPT2, Gemma), showing improvements in language acceptability and training efficiency compared to fine-tuning approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. **Novel Approach**: The paper presents an innovative method for topic alignment using SAEs, addressing a significant gap in the literature. The approach is well-motivated by the limitations of existing methods like fine-tuning.\n\n2. **Technical Depth**: The methodology is thoroughly developed with clear mathematical formulations for both the scoring mechanism and modification approach. The authors provide detailed justification for their design choices.\n\n3. **Comprehensive Evaluation**: The experimental evaluation is extensive, covering:\n   - Multiple datasets and topics\n   - Different model architectures\n   - Various SAE configurations\n   - Both quantitative and qualitative metrics\n\n4. **Practical Utility**: The approach shows promising results with:\n   - Reduced training time (333.6s vs 62s)\n   - Acceptable inference time overhead (+0.00092s/token)\n   - Improved language acceptability in some configurations"
            },
            "weaknesses": {
                "value": "1. **Limited Scale**: While the approach is tested on GPT2 and Gemma, there's no evaluation on larger, more current models. This raises questions about scalability.\n\n2. **Parameter Sensitivity**: Though the authors claim their approach doesn't require parameter tuning, the results show significant variation across different SAE configurations and layers. More analysis of these dependencies would be valuable.\n\n3. **Baseline Comparisons**: While fine-tuning is used as a baseline, comparison with other lightweight adaptation methods (like prompt tuning or LoRA) would strengthen the evaluation.\n\n4. **Theoretical Foundation**: The paper could benefit from stronger theoretical justification for why the proposed scoring mechanism effectively identifies relevant neurons."
            },
            "questions": {
                "value": "1. How does the approach scale to larger models? Have you tested or analyzed computational requirements for models like LLaMA or GPT-3?\n\n2. The results show significant variation in performance across different layers. Could you provide more insight into how to select the optimal layer for applying the SAE modifications?\n\n3. How robust is the approach to different types of alignment tasks? While medical domain alignment shows promising results, are there certain types of alignment that are particularly challenging?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a method for topic alignment in LLMs using Sparse Autoencoders (SAEs) to emphasize sparse layer activation relevant to specific topics without fine-tuning the parameters. By scoring sparse token activation results, the method is able to perform topic alignment without fine-tuning the parameters, reducing training time and enhancing interpretability. Experiments across diverse datasets demonstrate the method\u2019s adaptability."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Novel approach for using sparse token encoding to do the topic alignment.\n2. This work is highly relevant for applications requiring frequent topic shifts in generated text, offering a scalable and interpretable alternative to fine-tuning."
            },
            "weaknesses": {
                "value": "1. Poor performance. The method's performance remains suboptimal. While the authors present a novel approach to topic-alignment fine-tuning, the results showcased in Table 2 still lack clarity, with the distance score favoring traditional fine-tuning as the more effective option. In addition, providing a ground-truth case will make it easier to know what kind of output is desired. From the current cases, I believe all responses are not helpful for the users. Meanwhile, could the authors clarify what specific benefits their method might offer in comparison, such as interpretability or flexibility, that justify its use over standard fine-tuning except for the efficiency? Efficiency considerations are addressed further in Weak Point 4.\n2. Lack of proper comparison. The paper lacks a comprehensive comparison with other common methods for topic-based fine-tuning, such as prompt hints, LoRA, and token-based fine-tuning. Given that a simple prompt hint typically provides a strong baseline, it is unusual that this comparison is missing. It would be helpful if the authors could add this baseline, alongside relevant metrics like computational efficiency, interpretability, and overall performance, to provide a fuller assessment of their method\u2019s value relative to established alternatives.\n3. Lack of proper target models and experiment details. The experimental setup lacks sufficient detail regarding the target models used. Although GPT-2 and Gemma are mentioned briefly, there is no information on model size, version, or specific configurations. To support a more robust conclusion, I recommend that the authors apply their method across a variety of large language models (LLMs) and provide clear details on each model's configurations including framework, version, and size. This would mitigate any potential bias introduced by the training data of a single LLM and enhance the generalizability of the findings\n4. The cost analysis in the paper is incomplete, particularly regarding the training time associated with adding a sparse encoding layer. Since a sparse encoding layer is not a standard module for most LLMs, the additional time and computational resources required for training should be included in the analysis. A more detailed breakdown of these costs would give readers a more accurate understanding of the method\u2019s overall efficiency and practical feasibility"
            },
            "questions": {
                "value": "1. What LLM did you use for the experiment? In detail, what is the version and model size? \n2. Have you tried using simple prompt hints? For example, please respond as a professional doctor or just use GPT-4 to rephrase the prompt to the desired domain."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper's content is interesting, but unfortunately, the code contains the name of a person presumed to be the author. Therefore, I think this paper should be rejected."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper's content is interesting, but unfortunately, the code contains the name of a person presumed to be the author. Therefore, I think this paper should be rejected."
            },
            "weaknesses": {
                "value": "The paper's content is interesting, but unfortunately, the code contains the name of a person presumed to be the author. Therefore, I think this paper should be rejected."
            },
            "questions": {
                "value": "The paper's content is interesting, but unfortunately, the code contains the name of a person presumed to be the author. Therefore, I think this paper should be rejected."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}