{
    "id": "mV6cO4mGjH",
    "title": "Dynamics Based Neural Encoding with Inter-Intra Region Connectivity",
    "abstract": "Extensive literature has drawn comparisons between recordings of biological neurons in the brain and deep neural networks. This comparative analysis aims to advance and interpret deep neural networks and enhance our understanding of biological neural systems. However, previous works did not consider the time aspect and how the encoding of video and dynamics in deep networks relate to the biological neural systems within a large-scale comparison. Towards this end, we propose the first large-scale study focused on comparing video understanding models with respect to the visual cortex recordings using video stimuli. The study encompasses more than two million regression fits, examining image vs. video understanding, convolutional vs. transformer-based and fully vs. self-supervised models. Our study resulted in both, insights to help better understand deep video understanding models and a novel neural encoding scheme to better encode biological neural systems. We provide key insights on how video understanding models predict visual cortex responses; showing video understanding better than image understanding models, convolutional models are better in the early-mid visual cortical regions than transformer based ones except for multiscale transformers and that two-stream models are better than single stream. Furthermore, we propose a novel neural encoding scheme that is built on top of the best performing video understanding models, while incorporating inter-intra region connectivity across the visual cortex. Our neural encoding leverages the encoded dynamics from video stimuli, through utilizing two-stream networks and multiscale transformers, while taking connectivity priors into consideration. Our results show that merging both intra and inter-region connectivity priors increases the encoding performance over each one of them standalone or no connectivity priors. It also shows the necessity for encoding dynamics to fully benefit from such connectivity priors.",
    "keywords": [
        "Neuroscience",
        "Neural Encoding",
        "Video Understanding"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mV6cO4mGjH",
    "pdf_link": "https://openreview.net/pdf?id=mV6cO4mGjH",
    "comments": [
        {
            "summary": {
                "value": "This work provides a comprehensive study of predicting fMRI signals in visual cortical areas while humans view short video clips, by mapping from the activations within a wide range of artificial neural networks that process the same videos or still images from them. The study compares models that differ along several interesting dimensions:  image-computable and video computable models, CNNs and transformers, and fully-supervised vs self-supervised training regimes. The results show that video-computable, convolution-based, fully-supervised models are generally the best predictors of visual cortical activity. The relevance of these results is supported by additional encoding studies where the encoding \u201ctarget\u201d is an artificial neural network of known architecture, showing that it is possible to distinguish between image-computable and video-computable models using a system identification approach. A novel strategy for incorporating inter and intra-region functional connectivity information is proposed, and this approach improves the encoding performance of the already best-performing models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tNeural encoding is an important area of research that can yield insights to both improve artificial neural networks and better understand biological neural systems. \n\n2.\tThere is a thorough review of recent related work that makes it clear how the current paper is situated given existing literature. \n\n3.\tA wide variety of appropriately chosen \u201csource\u201d candidate models are employed for comparative encoding experiments, and they vary in several interesting ways as outlined by the authors (single-image vs video computable, CNN vs transformer, fully supervised vs self-supervised). The breadth of the study in this regard is unprecedented for encoding of visual cortical responses to video inputs. \n\n4.\tThe work demonstrates in a convincing way that video computable, fully supervised CNN models can generally enable more accurate predictions of visual cortical activity than models that are image computable, are based on transformer architectures, and/or are self-supervised. \n\n5.\tThe work also presents evidence that system identification of image-computable vs video-computable systems is plausible (figure 2). This provides additional support for conclusions drawn regarding biological targets. \n\n6.\tA novel approach for incorporating inter and intra-region connectivity is presented, which significantly improves the voxel prediction performance."
            },
            "weaknesses": {
                "value": "This paper is primarily limited by a frequent dearth of clarity in the writing, as well as a methodological concern about the way inter and intra-region connectivity priors are incorporated into the encoding models. This reader\u2019s score could improve considerably with clarification of these issues. \n\n1.\tAdditional proofreading/editing is recommended to clarify and improve sentence structure in many parts of the paper. For example, the description of the \u201cReal environment\u201d (last part of section 3.1) is ambiguous \u2013 it states that two different datasets are used, but then says \u201cwe mainly use the training set, and perform cross-validation\u2026\u201d, \u201cThe dataset provides fMRI recordings of ten subjects\u2026\u201d, \u201ceach video and voxel in the brain was represented by a single activation value\u201d without indicating which dataset these statements refer to (or is it both?). Also in this part of the paper, what does it mean that each video and voxel in the brain was represented by a single activation value? Aren\u2019t there different values across time and between subjects? In several parts of the paper, there are similar ambiguities such as it being unclear which dataset is being referred to at what time. \n\n2.\tIt is not clearly specified how the inter/intra region connectivity module is combined with the rest of the encoding models to make voxel activity predictions. The current version of the paper claims that intra-region connectivity priors improve predictive performance \u2013 but seemingly by directly providing the voxel values that are being predicted as inputs to the overall model. The improvements in performance might be due not to a meaningful incorporation of connectivity priors, but rather undesirable leakage of label information into the input. \n\n3.\tOne of the claims is that inter/intra-region connectivity information improves performance to a greater extent in video-computable models compared with image-computable models \u2013 the evidence for this is presented in  figure 6b. However, there is a confound because connectivity-related performance improvements are compared between image-computable ResNet-50 and video-computable MViT: how can it be determined whether the apparent interaction effect is related to image computable vs video computable models rather than CNNs vs transformers?\n\nMinor comments: \n\n4.\tIn the abstract and several other places, saying that \u201cthe study encompasses more than two million regression fits\u201d does not seem especially relevant \u2013 the number of regression fits in itself does not seem to be a particularly informative way to quantify the scale of the study. \n\n5.\tIn describing the contributions of the paper, the authors seem to emphasize that prior works did not compare different neural network architectures - but in section 3.1, there is a statement that \u201cwhile previous works focused on the architecture aspect, we argue it is even more important to look into whether the model is learning dynamics (e.g., motion) or simply using static information from a single image.\u201d Yet, the comparison between single image vs video processing would seem to be an important emphasis of Lahner et al 2024. These claims about the relationship between prior works and the current paper should be clarified. \n\n6.\tThe terms \u201creal environment\u201d and \u201csimulated environment\u201d are not especially informative \u2013 perhaps something like \u201cbiological target\u201d and \u201cartificial neural network target\u201d would be an improvement\n\n7.\tIn first paragraph of section 4.1, \u201cAs for image understanding models we use the default sampling rate of eight.\u201d Does this refer to 8 frames per second? This whole paragraph seems quite ambiguous \u2013 why are video understanding vs image understanding models being discussed here in relation to sampling rates?  \n\n8.\tThe text is some of the figures is too small to be easily readable, especially figure 2, figure 3, figure 5, and supplementary figures 7, 8, 10, and 11.\n\n9.\tMost of the in-text citations would be best formatted with both author and year inside the parentheses. In LaTex, you can use \\citep{} instead of \\cite{}\n\n10.\tThe goal of the paper seems to be stated in an overly general way in the abstract (\u201cThis comparative analysis aims to advance and interpret deep neural networks and enhance our understanding of biological neural systems\u201d). \n\n11.\tTypos: \n\nA.\tIn the introduction (end of page 1), \u201cwhile the later showed that these fMRI recordings\u201d (should be latter instead of later)\n\nB.\tGrammar issue in start of 2nd paragraph of section 3.1: \u201cHow do deep video understanding models families compare to biological neural systems\u201d \n\nC.\tEnd of section 3.1 \u201cThe datasets we use is provided at TR one second\u201d (should be are, not is, and the TR acronym should be defined)"
            },
            "questions": {
                "value": "1.\tThe following reflects the evolution of my understanding while reading the paper: the authors study the ability of artificial neural network activations to predict biological signals in the visual cortex (the \u201ctarget\u201d) \u2013 they also include an ANN-based target \u2013 what does the artificial target add to the study? Section 3.1 seemingly attempts to explain this, but it may need to be edited for clarity. Is the idea that we know the structure of the target ANNs in the artificial setting, so we can test our ability to uncover mechanistic insights through encoding experiments, and therefore get a sense for how informative the system identification experiments with biological targets are likely to be? This becomes much clearer upon review of Han et al 2023, but it should also be explained more explicitly in the introductory sections of the present work. \n\n2.\tIn equation 1, in the first term, is a 1/L normalization needed for the summation $\\sum_{l=1}^{L} \\omega_l \\hat{Y}_l$ ? Without the normalization, it seems mildly inconsistent with the statement above equation 1 that \u201cwe learn the weights of one fully connected layer to provide the predictions of the voxels of one region of interest in the visual cortex as, $\\hat{Y}_l = W_l X_l$.\u201d Or perhaps this normalization is absorbed by $\\omega_l$?\n\n3.\tIn section 3.4, the description of the connectivity module indicates that all voxels from all regions are used to predict voxels from a single region. Doesn\u2019t this mean that the desired output of the model (voxel values from a single target region) are contained in the input of the model, making this a trivial mapping? It makes sense that dropout layers could partially alleviate this problem, but why does it make sense to set up the architecture in this way in the first place where at least some of the outputs are trivially accessible in the input? \n\n4.\tSection 4.1: for four-fold cross validation where each fold has 90% of the videos in the training set and 10% in the test set \u2013 are the 10% test sets disjoint between the folds, or randomly selected? (Not a major issue, but 75:25 might have been a slightly more natural choice for four-fold cross validation) \n\n5.\tIn section 4.2, there is a statement that \u201cwe include video understanding models that are trained on three different datasets which are Kinetics, Charades, and Something-Something v2\u201d. These datasets are not cited, and it is not clear how the results from the three datasets were combined to produce figure 2 \u2013 were the results pooled somehow?\n\n6.\tMight it be useful to discuss why the observed trends seem to hold for some visual cortical regions and not others? For example, video-computable models are no better than image-computable models for predicting FFA and PPA activity - it is interesting to speculate that perhaps recognition of faces (associated with FFA) or environmental scenes (associated with PPA) in the brain might be less dependent on temporal dynamics.\n\n7.  For plots like those in figures 2 and 3, what does each dot represent? For example, is it one regression score from one type of \u201csource\u201d model averaged among cross-validation folds, or is it an accuracy from one of the folds only? Are results from different source models pooled together in these plots? Relatedly, it is not clearly specified how the Welch\u2019s t-tests are performed \u2013 which sets of values specifically are used to compute the t-test? \n\n8.\tWhy are system identification results presented in the main text specifically for image vs video computable models, rather than other comparisons of interest like CNNs vs transformers or self-supervised vs fully-supervised?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper compares a variety of image and video models to fMRI data, evaluating these models for representational alignment to human responses. The paper reports this analysis for multiple types of networks, comparing image/video, convolutional/transformer, single/multi stream, and self-supervised/unsupervised networks. The authors report the best predictors of human responses are convolutional, multi-stream, unsupervised video networks. Additionally, the paper reports experiments with a intra- and inter- region connectivity priors which improve encoding performance."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Originality: To my knowledge, this paper uses an established approach, but is novel in the scale and types of comparisons between types of models compared. \n\nQuality: The paper is of good quality, with good statistical analysis and control conditions.\n \nClarity: The majority of the paper is written very clearly, especially the sections comparing image/video, convolutional/transformer, single/multi stream, and self-supervised/unsupervised networks to BOLD signal.\n\nThe scale of experiments in terms of number of models is commendable, and the authors were clever to choose models that enabled comparisons across many different dimensions.\n\nSignificance: This is an important emerging area in understanding alignment of models with human data, especially for new types of models, which show improved performance on visual tasks, but as the authors show, have worse representational alignment with human responses. This paper is significant in its contributions to understanding the quality of these alignments over multiple different axes of comparison."
            },
            "weaknesses": {
                "value": "There is little explanation of the intra- and inter- connectivity priors, especially their structure, and interpretation of their results beyond their contribution to improved representational alignment, and the value of their learned weights in figure 5c."
            },
            "questions": {
                "value": "I am a bit confused about the intra- inter- connectivity priors, what their structure is, and if the results demonstrate anything beyond that complex connectivity as seen in human visual cortex increases predictivity?\n\nRelatedly, the paper would benefit from even a minimal interpretation of the learned connectivity weights. What is their structure and how does this change with training? How do they contribute to improvement of the representational alignment? What do these learned weights imply biologically? Such an analysis would likely also address the previous point.\n\nI challenge the authors in L435-437 on leaving the reasons behind the better alignment seen in fully supervised networks to future work. This is a major result of the paper and I challenge the authors to at least make some hypotheses or have a few sentences of discussion as to why this could be the case. The validation of such a hypothesis can be left to future work.\n\nMinor syntax issues:\nL151 \u201c: e?\u201d.\u201d\nL363: \u201cThis difference decrease as\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work is developed in two parts, aiming to (1) compare the neural predictivity of video deep neural networks on video fMRI data, and (2) propose a method to improve prediction scores via utilizing brain region connectivity.\nIn the first part, it tests the brain predictivity of models with different architectural design and learning paradigm and contrasts to image models (in total, 26 video action recognition models and 11 image object recognition models are tested). The results indicate that video models are better predictors than image models, in early regions convolutional models are better than transformers, and supervised models are better than self-supervised. Additionally a system identification check is performed which shows that video models can predict other video models better than image models, and vice versa, based on four target models. In the second part, this work proposes training a module to consider all regions in the prediction of any single region and shows improvements on the prediction scores of two selected video models and one image model; the image model has a less pronounced improvement which the authors claim is due to the lack of dynamics."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is original in the aspect of application (model-to-brain encoding) to a new domain (human video fMRI aside from image fMRI), and the writing style has good quality. Previous work is also sufficiently covered. The questions asked are important and in general this work is a step in the right direction."
            },
            "weaknesses": {
                "value": "However, there are several soundness and presentation related issues in its execution that negatively impact this reader\u2019s opinion of the paper.\n\nFirst of all, the paper reads as two rather disjoint parts that fail to come together as one clear proposition (see summary for the two parts). As such, it looks like this could be two unrelated papers, that each would need further work to be able to stand on its own. This is the case in many points throughout the paper, for example in Figure 1, the reader is lead to think that this is the main figure / main method proposed, but the extent of the results with this method are underwhelming, as are the technical details provided.\n\nSecond, there are several issues with the soundness of the methods.\n* The reader is not convinced that the improvements observed in brain predictivity are caused by the learned connectivity, and not just by obtaining a more robust or less noisy embedding by considering other regions. There are also inconsistencies in the results regarding this aspect; in figure 5c it is observed that LOC does not connect with any other areas apart from itself - then how is it explained that in figure 5a performance is significantly improved with the connectivity module? Could it be that connectivity is not the reason at all for the improvement? For the learned connectivity itself, extensive comparison with findings in neuroscience literature on region connectivity is lacking.\n* For the analysis in section 4.5 as a whole, it is very unconvincing that only two models are compared, instead of the 37 models initially examined in the first part. When comparing to the image models, only one image model is tested and from that the conclusion \u201cshows the necessity for encoding dynamics to fully benefit from such connectivity priors.\u201d is inferred, which is too strong.  Additionally in figure 6b, it is inconsistent that not even Slowfast is shown but only MViT. In the same figure, scores \u201cin a different scale after multiplying by 100\u201d are shown instead of percentage of improvement, not allowing to observe the improvement regardless of the models\u2019 performance scale itself. \n* The motivation to use the Algonauts challenge\u2019s training set for the main results without evaluating on the official test set, is unclear. An even better, more obvious choice for the main results would be to use the full Bold Moments Dataset which is more refined, higher quality, and also includes the test set.\n* The supervised vs. self-supervised comparison is not clean, as all the self-supervised models are transformers - thus the relationship between convolutional models and transformers is also partly transferred in that result. The comparison is also between very different numbers of models, with only 5 SSL and more than 30 SL. Additionally, throughout the paper results are shown with models mixed from different training datasets, which makes the effects of each examined comparison (e.g. architecture) and the training dataset, entangled. See Conwell et al. (2022, 2024) for an example of clean comparisons.\n* The reader finds that the fine-grained analyses of figure 4 are too fine-grained to be scientifically valid. What does evaluating a single design choice for two-stream processing tells us about either the networks or the brain? Why is the specific TimesFormer architecture chosen to compare against two other (specific) architectures as representatives of supervised, self-supervised, and fine-tuned self-supervised? Furthermore, quite a lot of emphasis is put on the multiscale component of MViT, however, without an explicit ablation it is not possible to know which factor of this model contributes to its good brain predictivity. Additionally, the authors reach the conclusion that early regions relate better to convolutional models \u201csince they better capture high-frequency components\u201d but this is not evidence drawn from their results and rather from other publications. It is not shown that this is the reason behind their results. Further, the model MViT does not conform to this so it seems that this conclusion is too strong.\n\nThird, there are some major problems in the paper\u2019s presentation.\n* Figure 1 has several issues. (1) It does not show any details of the Inter-intra Region Connectivity Model (layers, loss function), which is the main component proposed. (2) It shows the pipeline only at the inference mode and does not mention neither which mode is shown or their (substantial) differences. (3) In the first stage of voxel prediction (encoding) it is not clear that the predicted voxels are the outputs of the encoding procedure (they either look like the video models\u2019 input or at best as if they are their direct output which is also not the case). (4) Finally, a major point of this figure was to show that different degrees of connectivity between regions are predicted, but from the size of the visualization on the left it is near impossible to tell differences between the thickness of the arrows.\n* In section 3.4, where the Inter-intra Region Connectivity Model is introduced, there are no equations to show the model layers, and more importantly the loss function to train this module is not described at all, in equation or text form.\n* In section 4.1, it is not clear whether the hyper-parameter tuning is conducted on the Algonauts training set or the authors\u2019 training set (90% of the Algonauts training set, different for each of the 4 folds). This is an important clarification to make because in the first case the encoding test set is seen during hyper-parameter tuning.\n* Phrases like \u201cwe mainly use the training set\u201d in section 3.1 are too vague - when exactly do the authors not use it? From section 4.1 it seems that only the training test is used in all cases.\n* Video models used are described as \u201cvideo understanding\u201d models throughout the paper, which is inaccurate because they are all video action recognition models - video understanding is a vague umbrella task that could potentially include segmentation, reasoning, and many other more complex tasks than classification. By also describing the image models as \u201cimage understanding\u201d models, authors obscure the fact that they are trained on a different task than the video models, which is object recognition rather than action recognition.\n\nReferences:\n\nConwell, Colin, et al. \"What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?.\" BioRxiv (2022): 2022-03.\n\nConwell, Colin, et al. \"A large-scale examination of inductive biases shaping high-level visual representation in brains and machines.\" Nature Communications 15.1 (2024): 9383."
            },
            "questions": {
                "value": "Overall the reviewer feels that although some of the presentation problems could be corrected, there are overarching problems with the structure, i.e. disjoint two parts, and the soundness of this paper, i.e. correlational evidence, unclean comparisons, and rushed conclusions, that make it hard to change in a way sufficient for acceptance. \nThe final rating is 5 in absence of the option for 4 (because 3 would be too low)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors use multiple neural network models to attempt to fit signals from functional magnetic resonance imaging (fMRI) obtained while participants watched short video clips from two existing datasets (Cichy et al 2021, Lahner et al 2024)."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The authors have done extensive calculations with lots of computational models.\nUnderstanding how the brain processes dynamic signals is an important question."
            },
            "weaknesses": {
                "value": "- The manuscript is poorly written and it is very hard to decipher what the authors did. There is almost no methodological information whatsoever. I list an initial list of questions that would be essential to answer when the authors submit the work to some other venue. \n- It is recommended that a native speaker goes over the paper to fix grammar and clarity. \n- It is also recommended that someone reads the paper with reproducibility in mind. This can help the authors understand all the information they need to provide. \n- In this type of data fitting work, it is always useful to define a lower bound given by chance and an upper bound sometimes defined by computing split-half regressions across multiple repetitions of identical stimuli\n- The temporal resolution of fMRI is very poor. The field has struggled with getting any meaningful temporal information about of videos. Given that the authors seem to be interested in timing, it would be useful to consider data with better temporal resolution. If the authors cannot use better data, then they should first document that there is any temporal information that can be obtained from the fMRI signals during videos. \n- The authors do not say anything about eye movements. Participants typically make 2 to 4 saccades per second while watching images or video. Presumably these saccades would radically change the activity in visual cortex. It is unclear how one can make any meaningful claims about visual cortex without documenting the neural responses to each saccade.  \n- Before jumping into large neural networks with large numbers of free parameters, it would be useful to quantify how well the data can be explained by simple models like pixels, contrast, optic flow, etc. This is assuming that the authors first convince themselves and show that there is actual reproducible data that warrants explanation."
            },
            "questions": {
                "value": "There is almost no explanation of what the authors did here. The authors are using existing datasets and they do not need to copy and paste the entire methods section of the original papers. But they need to explicitly indicate what they have done here.\n\t(1) In terms of the experimental data: \n\tHow many participants?\n\tHow many video clips? \n\tWhat was the length of the video clips?\n\tHow many repetitions of each video clip?\n\tWhat was the size of the image, the frame rate, and the volume of the audio? \n\tWhat was the resolution of the eye tracker used?\n\tWhat kind of video clips? What was the content? Were there cuts in the video? \n\t(2) In terms of the analyses,\n\tSaying \u201c\u2026 we use the brain responses\u2026\u201d has little meaning. \n\tWhat are the units of the measurements?\n\tHow reliable are the measurements (e.g., reproducibility across repetitions of identical stimuli)?\n\tWhich time interval was used for the analyses? There is a mention of one second but the temporal resolution is much slower than 1 second, it would be important to document that there is indeed information at 1 second time scales. \n\tWhich voxels were used for each region?\n\tWhich hemispheres? \n\tHow were the 9 regions of interest defined? \n\t(3) Data fitting\n\tWhat was the dimensionality of the predictors in each case?\n\tHow was cross-validation performed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Among the many things that are absent in the description is the notion that participants consented to the experiments, which is in the original data description."
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}