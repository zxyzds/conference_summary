{
    "id": "NxyfSW6mLK",
    "title": "REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments",
    "abstract": "Do generalist agents require large models pre-trained on massive amounts of data to rapidly adapt to new environments? We propose a novel approach to pre-train relatively small models and adapt them to unseen environments via in-context learning, without any finetuning. Our key idea is that retrieval offers a powerful bias for fast adaptation. Indeed, we demonstrate that even a simple retrieval-based 1-nearest neighbor agent offers a surprisingly strong baseline for today's state-of-the-art generalist agents. From this starting point, we construct a semi-parametric agent, REGENT, that trains a transformer-based policy on sequences of queries and retrieved neighbors. REGENT can generalize to unseen robotics and game-playing environments via retrieval augmentation and in-context learning, achieving this with up to 3x fewer parameters and up to an order-of-magnitude fewer pre-training datapoints, significantly outperforming today's state-of-the-art generalist agents.",
    "keywords": [
        "Generalist Agent",
        "Retrieval",
        "In-Context Learning",
        "Imitation Learning",
        "Reinforcement Learning"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We propose a retrieval-augmented generalist agent that can adapt to new environments via in-context learning",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=NxyfSW6mLK",
    "pdf_link": "https://openreview.net/pdf?id=NxyfSW6mLK",
    "comments": [
        {
            "summary": {
                "value": "This work introduces REGENT, a retrieval-augmented generalist agent designed to adapt to unseen environments without finetuning. It leverages in-context learning and achieves significant generalization across different environments, including robotics and game-playing. It proposed a retrieve-and-play method and incorporated a transformer-based policy to train on sequences of queries and retrieved neighbors."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The work is well-organized and easy to follow.\n2. The observation that the simple retrieve-and-play method can match or surpass the performance of state-of-the-art generalist agents in unseen environments is interesting."
            },
            "weaknesses": {
                "value": "1. This paper uses nearest neighbor retrieval, claiming it is effective for small training datasets. Yet, they use transformer architecture, known as data hungry. These contradict each other. To prove that nearest neighbor retrieval is indeed effective, it should also be compared to other retrieval methods. \n2. There are some places that the author overclaims:\n\n- The author claims multiple times that Gato \"struggles to achieve transfer to an unseen Atari game even after finetuning, irrespective of the pretraining data\". Note that in Section 5.2 and Figure 9, Gato did a comprehensive study to show that they can transfer to new tasks with few demonstrations. \n\n- The author claims that their model has the ability to adapt to new tasks via in-context learning. However, the ICL refers to a model's capacity to learn and perform tasks by observing examples \"without requiring additional training.\" The LLM can do ICL **without training via RAG**. This is conceptually different from what the author claimed ICL in their paper. \n\n- The author claims that methods like decision transformers cannot generalize to new goals caused by changes in visual observation/available control/game dynamics. Nevertheless, the experimental setting in this paper is simple: the simulation benchmark, like Metaworld, does not involve many visual changes between different tasks. To verify that the REGENT can indeed handle these environmental changes, it is recommended that a real robot experiment be conducted, as Gato does in their paper. \n\n3. Why is JAT/Gato finetuned on new demonstrations, as it is already trained by all data, such as 50 tasks in MetaWorld? This seems to make JAT/Gato overfitting on the target dataset, thus performing extremely badly in experiments in Figure 4. The comparisons are not fair, and the author should provide proper reasons and compare them with the vanilla JAT/Gato. \n\n4. Missing highly-related reference [1,2].\n\n[1] Retrieval-Augmented Embodied Agents, CVPR24\n[2] Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning, EMNLP"
            },
            "questions": {
                "value": "Please see weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a method that pre-trained on diverse demonstrations across different environments and tasks that can be adapted to unseen environments via in-context learning, with very few expert demonstrations, and without any fine-tuning. The method is a semi-parametric model that leverages a simple retrieval-based 1-nearest neighbor agent and in-context learning based on other retrieved neighbors to generalize to unseen robotics and game-playing environments. The authors show that their approach is more sample-efficient and parameter-efficient than strong baselines such as Gato."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ This paper proposes a novel and interesting idea of combining a 1-nearest neighbor agent with a Transformer pre-trained to perform in-context learning from retrieved N-nearest neighbor demos. This approach seems to work well for both the Gato setup and the ProcGen setup regarding generalization over unseen environments for robotics or game-playing tasks.\n+ The results of the simple 1-nearest neighbor agent show that it serves as a surprisingly strong baseline, which provides insights for solving tasks in the unseen environment."
            },
            "weaknesses": {
                "value": "+ The paper will be stronger if its experiments are also performed on a larger set of manipulation tasks (ManiSkill2, RLBench, etc.)"
            },
            "questions": {
                "value": "+ I am curious what are the results of REGENT without the help of the 1-nearest neighbor agent."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes REGENT, a retrieval-augmented generalist agent that can adapt to new environments without fine-tuning. REGENT can outperform other RL  agents with fewer parameters in two settings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The methodology is well-explained and easy to understand.\n\n2. REGENT demonstrates better performance than larger models (JAT/Gato) with fewer parameters across multiple environment types."
            },
            "weaknesses": {
                "value": "1. Limited Comparison Scope:\n\nWhile the paper draws inspiration from retrieval-augmented generation (RAG) in language models, it lacks comparisons with recent LM embodied agents that use retrieval-augmented methods. Some existing work:\n- https://arxiv.org/abs/2308.10144\n- https://arxiv.org/abs/2402.03610\n\n2. Insufficient Ablation Studies, some valuable ablations could have included:\n- Analyzing performance w/wo image information in the context\n- Testing various context sizes\n - Analyzing the REGENT's inference latency per state is crucial for real-world deployment. Given the retrieval mechanism and context processing at each step, runtime performance analysis would be valuable for assessing practical applicability\n\n\n3. Training Data Requirements:\n\nWhile REGENT shows improved data efficiency compared to baselines like JAT/Gato, it still requires substantial pretraining data across different embodied settings (14.5M transitions in JAT/Gato setting, 12M in ProcGen). This raises questions about its true few-shot learning capabilities and practical applicability in scenarios where large-scale demonstration data is unavailable across task families."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes to leverage retrieval augmentation to reduce the amount of training data required for training generalist agents.\nThe authors introduce $\\texttt{REGENT}$, a retrieval-augmented architecture that incorporates a non-parametric retrieval component (R&P) with learning of a parametric policy.\nFurther, they demonstrate theoretical guarantees for performance with respect to the number of expert demonstrations.\nThey pre-train $\\texttt{REGENT}$ on 4 different domains and evaluate on unseen levels and holdout tasks, where it exhibits strong performance compared to baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors provide theoretical guarantees for performance with respect to the number of demonstrations.\n\n$\\texttt{REGENT}$ is a novel architecture that effectively incorporates retrieval augmentation for imitation learning.\n\nRetrieval augmentation enables training on much less data than other generalist agents.\n\nThe authors demonstrate the promise of retrieval augmentation for unseen levels and tasks."
            },
            "weaknesses": {
                "value": "**Significance of results:**\n\nThe authors only provide averages in all figures (except for Figure 7), but no variance estimates. How do those look? Are the results on the unseen tasks statistically significant?\n\nThe authors claim that $\\texttt{REGENT}$ outperforms JAT even after fine-tuning, but there is little information about the fine-tuning protocol, i.e. what fine-tuning technique is used, etc. Therefore it is difficult to determine the significance of the results. Can the authors elaborate a on this?\nAlso there are fine-tuning techniques specifically designed for few-shot learning, which may be more suitable in this setting [1,2].\n\nOn unseen levels of ProcGen the authors only compare $\\texttt{REGENT}$ to R&P, but no results for MTT, why is this the case?\nBC is a very weak baseline for training from scratch. The authors should compare to other more recent imitation learning techniques [3,4,5] that use the same amount of expert demonstrations as $\\texttt{REGENT}$. This would shed light on the significance of the reported results.\n\nAnother interesting experiment would be zero-shot evaluation without having expert demonstrations on the unseen tasks. If $\\texttt{REGENT}$ performs well in this scenario, this would greatly strengthen the paper.\n\nThere is a lack of ablation studies, for example performance on varying the number of examples in the context and ablation on different distance metrics.\n\nFinally, it is not really clear why the authors differentiate between the two different settings. Why not include ProcGen in the first setting?\nWhy are different baselines used for the different settings? MTT could also be applied to the first setting, right?\nAt least having a fixed set of all baselines across both settings would strengthen the paper.\n\n**Limitations and claims**\n\n- The authors claim that deployment on unseen tasks is instantaneously, this is not true though, as deployment in an unseen environment requires collection of expert demonstrations which in turn requires training of an expert policy. This should be made more explicit as a limitation of the proposed method.\n- Moreover, claims on \"handful of examples\" should be caveated as thousands of samples are not a handful.\n- Authors claim apples-to-apples comparison between JAT and $\\texttt{REGENT}$ when being trained on the same amount of data. This is not apples-to-apples though as $\\texttt{REGENT}$ uses expert demonstrations of the unseen tasks. The more fair comparison is to fine-tuning JAT.\n- Further, the work would benefit from a bit more accurate positioning on the landscape of methods that do or do not rely on demonstrations. This is an important aspect, as there are approaches that aim at leveraging in-context learning in the context of learning-to-learn as in meta-learning [6]. Examples are [7,8], where the latter also relies on retrieval augmentation. \n\n**Presentation:**\n\nPresentation can be improved, in particular:\n\n* Eq. 1 & 2: $c_t$ is provided as input to the policy, but it is never used, it is only used in Eq. 3, as R&P does not rely on context information\n* Figure 3: enumeration in caption, but not in figure, either remove them in caption or add in figure\n* Paragraph on $\\texttt{REGENT}$ architecture is not really clear as the reader does not have an overview what modalities are represented in the training data, therefore the choice of encodings seem arbitrary.\n* Theorem 5.2: $\\boldsymbol{H}$ represents the horizon, right? In section 3 it is not boldface, this should be consistent.\n* Figure 8: hard to follow what actions are \"good\" or \"bad\" as there is no explanation for the action index\n\n**References:**\n\n[1] Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning, Liu et al., NeurIPS 2022\n\n[2] Scaling & Shifting Your Features: A New Baseline for Efficient Model Tuning, Lian et al., NeurIPS 2022\n\n[3] Generative Adversarial Imitation Learning, Ho et al., NeurIPS 2016\n\n[4] Disagreement-Regularized Imitation Learning, Brantley et  al., ICLR 2020\n\n[5] Of Moments and Matching: A Game-Theoretic Framework for Closing the Imitation Gap, Swamy et al., ICML 2021\n\n[6]   Evolutionary principles in self-referential learning, or on learning how to learn, Schmidhuber et al., Diploma Thesis, 1987\n\n[7] Towards General-Purpose In-Context Learning Agents, Kirsch et al., NeurIPS 2023 FMDM Workshop\n\n[8] Retrieval-Augmented Decision Transformer: External Memory for In-context RL, Schmied et al., arXiv 2024"
            },
            "questions": {
                "value": "- Why is the scaling factor for MixedReLU set to 10? This seems like an arbitrary choice, why not remove it entirely?\n- Does the number of demonstrations for MTT on ProcGen match the number of demonstrations for $\\texttt{REGENT}$?\n- Is it always the case that $\\texttt{REGENT}$ only receives the current query state in its context? Have you tried providing more sequential information?\n- What ProcGen mode is being used? Easy or hard?\n- How important is the ordering in the context of $\\texttt{REGENT}$? Did you try permuting the order?\n- In Figure 15 in Appendix D not all curves show improvement for using more demonstrations, do the authors have an intuition why this is the case?\n- From Figures 11, 12, 13, 14 it can be seen that JAT is particularly good on BabyAI, but not good on other domains, while $\\texttt{REGENT}$ is very good at Metaworld. Do the authors have an intuition why this is the case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}