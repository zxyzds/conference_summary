{
    "id": "yitH9xAHQs",
    "title": "Forewarned is Forearmed:  Harnessing LLMs for Data Synthesis via Failure-induced Exploration",
    "abstract": "Large language models (LLMs) have significantly benefited from training on diverse, high-quality task-specific data, leading to impressive performance across a range of downstream applications. Current methods often rely on human-annotated data or predefined task templates to direct powerful LLMs in synthesizing task-relevant data for effective model training. However, this dependence on manually designed components may constrain the scope of generated data, potentially overlooking critical edge cases or novel scenarios that could challenge the model. In this paper, we present a novel approach, \\name, designed to automatically generate effective training samples that expose the weaknesses of LLMs. Specifically, we introduce a dedicated proposer trained to produce queries that lead target models to generate unsatisfactory responses. These failure-inducing queries are then used to construct training data, helping to address the models' shortcomings and improve overall performance. Our approach is flexible and can be applied to models of various scales (3B, 7B, and 8B). We evaluate \\name on three key applications\u2014safety, honesty, and math\u2014demonstrating that our generated data is both highly effective and diverse. Models fine-tuned with \\name-generated data consistently outperform those trained on human-annotated or general model-generated data, offering a new perspective on data synthesis for task-specific LLM enhancement.",
    "keywords": [
        "data synthesis",
        "preference learning",
        "LLM alignment"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We present a novel approach for automatically generating effective training samples from the target model's failure cases by optimizing another model to create samples via iterative preference learning.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=yitH9xAHQs",
    "pdf_link": "https://openreview.net/pdf?id=yitH9xAHQs",
    "comments": [
        {
            "summary": {
                "value": "The authors propose an iterative finetuning method for finetuning a target langague model by using synthetic data generation from a proposer language model which proposes harder and harder questions to a target language model. This is in effect a curriculum learning approach which trains a target model on harder and harder samples. The proposer model is also trained to propose harder and harder questions by using errors in the target model\u2019s answers.\n\nA proposer language model generates few-shot candidate questions. Then the target model predicts answers to these questions. The answers are then compared to gpt-4o-mini's answers, which is used as a gold-standard. If the answers agree then this question is placed into the negative set {x^-}. If the answers from the target model does not agree with the gpt-4o-mini then the question is placed in the positive set {x^+}. These sets are then used by DPO is used to finetune the proposer model to produce harder and harder samples by leveraging the positive and negative sets. Finally, the proposer model generates synthetic questions which are deemed hard for the target model, labels are generated by the proposer model or gpt-4o-mini. The target model is then trained with SFT."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The idea of using the language model prediction errors to create a set of easy and hard examples to train a proposer model with DPO is novel and a nice idea.\n\n- Interesting results that show that harder to predict data points aka using a curriculum harder and harder questions is beneficial for some red teaming and honesty benchmarks. This is interesting since, in comparison other papers such as [1], show that hard samples actually hurts target model performance albeit in a different dataset domain.\n\n- Wide range of experiments: red teaming, honesty and mathematical reasoning to demonstrate that the method can generalize to multiple domains.\n\n[1] Evans, Talfan, et al. \"Bad students make great teachers: Active learning accelerates large-scale visual understanding.\" arXiv preprint arXiv:2312.05328 (2023)."
            },
            "weaknesses": {
                "value": "- Weak results on mathematical reasoning which do not demonstrate considerable improvement in performance. Nor are many other similar iterative methods which have some sort of synthetic question-answer prioritization compared to [2, 3].\n\n- No ablation experiments, what is the performance with 1 iteration versus 5? What if you generate 1k or 10k samples to populate the positive and negative sets for DPO training?\n\n[2] Lee, Nicholas, et al. \"Llm2llm: Boosting llms with novel iterative data enhancement.\" arXiv preprint arXiv:2403.15042 (2024).\n[3] Jiang, Yuxin, et al. \"Lion: Adversarial distillation of proprietary large language models.\" arXiv preprint arXiv:2305.12870 (2023)."
            },
            "questions": {
                "value": "- What is $R(\\cdot)$ in line 204?\n\n- Performance for ReverseGen in Tables 1 and 2 has not saturated with increasing iterations and it seems like you are under reporting results. What would happen if you ran this for 4 or 5 iterations? At what point would performance saturate? What if you generated 10000k instruction candidates and performed t=1 iteration versus 2000k instruction candidates with t=5 iterations?\n\n- When using harder and harder samples to train a model for example in active learning, or data selection, where data points are prioritized with a larger loss then this can cause a negative feedback loop with a catastrophic drop in performance from a target model. Did you observe similar artifacts in your experiments?\n\n- In Tables 1, 2, 3 you have a rows \u2018without failure induction\u2019 but you do not describe what this ablation is?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a data synthesis approach by training a data generator and leverage the performance of the target model as a training signal. Specifically, the predictions of the target model are used to construct a preference dataset (target model's failure cases are preferred) for the training of the data generator which performs DPO on top of those preference data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper is overall well written and easy to understand.\n* The proposed approach is novel. It performs RLHF to train the data generator, with the target model works as a preference provider."
            },
            "weaknesses": {
                "value": "The experiments miss 2 significant baselines. \n* To verify the effect of the proposed RLHF approach, there should a baseline finetuning the data generator (proposer LLM) with a collection of failed samples, and generate a dataset.\n* A strong LLM (i.e., gpt-4o) plays an important role in the proposed method when obtaining the oracle label, so there should another baseline directly prompting the gpt-4o multiple times to generate a synthetic dataset."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a technique called ReverseGen for generating failure-inducing examples for an LLM. The technique uses a proposer model that is fine-tuned using pairs of positive and negative examples. The evaluation shows that the generated data can be used to fine-tune and improve models in safety, honesty, and mathematical reasoning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper introduces a new technique for synthesizing failure-inducing data for a target LLM \n\n- The technique is effective in 3 distinct domains of safety, honesty, and mathematical reasoning and shows improvement on SOTA on each. I appreciate the inclusion of Table 5 and Table 7 with examples in the evaluation section"
            },
            "weaknesses": {
                "value": "The writing of the paper, especially the separation of the technical section and implementation details needs to be improved. Many of the evaluation choices seem a bit arbitrary and need to be organized better to understand how each of the applications of the proposed technique fits into a single framework (if they do). \n\n- Line 126: \u201c these studies tend to focus on particular tasks or inputs, and overlook the task generalization and the data salience on model enhancement. Our work differs from these prior works in that we focus on generating failure-guided data for target models\u201d\nCan you be more specific in comparison with each of these prior works? It is unclear to me how ReverseGen differs from many of these works mentioned in the related works\n\n\n- Line 176: The term \u201csolvable\u201d and \u201cunsolvable\u201d is defined here and never mentioned here. The term \u201csolvable\u201d for positive examples is quite unclear. Use a more appropriate name for this.\n\n\n- Line 202: Section 3.2 is the technical section and it starts talking about \u201cgpt-40-mini\u201d. I would recommend authors to separate the implementation details from the technical section. \u201cGpt-40-mini\u201d is a specific model and used to label the responses, define the model used for labeling as a parameter of the technique that\u2019s instantiated as a specific model. \n\n## Minor:\n- Line 189: \u201cWe begin by warming up the proposer model $M_{prop}$ with the initial task-specific instruction set\u201d - A bit unclear wording. Can be more technically precise, especially in the technical section of the paper\n\n- Line 190: \u201c3-shot prompts\u201d - this seems like implementation detail as well which would be more appropriate if it was in the evaluation section\n\n- Line 204: What\u2019s R(.)? I don\u2019t see it defined anywhere before\n\n- Line 220: \u201c$M_{ref}$ is the reference model that remains unchanged\u201d: this doesn\u2019t really define what is $M_{ref}$\n\n- Line 227: typo - two dots \u201c..\u201d\n\n- Line 275: why were these hyperparameters chosen?\n\n- Line 284: \u201cResponses from the target model are generated using greedy decoding for deterministic quality measurement, with the temperature parameter set to 0\u201d - Doesn\u2019t greedy already mean temperature doesn\u2019t matter?\n\n- Line 287: \u201cInstructions for the SFT data are produced by proposer models, while responses are derived from the target models using tailored prompts for safety tasks, generated by gpt-4o-mini for knowledge-intensive tasks.\u201d -> the sentence is too long and hard to understand. What are \u201cknowledge-intensive tasks\u201d in this context?\n\n- Line 319: \u201cwell-safe\u201d?\n\n- Line 429: \u201cReverseGen solely relies on synthetic data\u201d - Doesn\u2019t it use MMLU as the initial instruction seed?\n\n- Line 523: \u201cThis may result from inadequate assessment of REVERSEGEN\u2019s proposed questions by the current benchmark or the need for more effective fine-tuning algorithms for difficult questions.\u201d - a bit unclear"
            },
            "questions": {
                "value": "Since the technical section and implementation details are somehow mixed, it is really hard to understand some of the details of each of the experiments. I have the following questions about the evaluation:\n\n- In what experiments, GPT-40-mini used? How was this decided? Is there an ablation study on its use?\n- For the honesty calibration and mathematical reasoning experiment, what number of \"proposer fine-tuning\" iterations are used in ReverseGen? \n- Why is Llama3 used in 4.4 but not in the previous experiments?\n\nCan you add a table that maps each of the variables in the technical section to the specific choice made in each experiment? This would make it easier for the reader to understand each experiment. For instance, the columns could be $M_{prop}, M_{tgt}$, number of examples, seed data, number of iterations, use of GPT-4o-mini, and other implementation details for each experiment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces REVERSEGEN, a method for generating training data by identifying model weaknesses through failure-inducing exploration."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Novel approach to data synthesis through failure exploration\n* Comprehensive evaluation across three important tasks (safety, honesty, math reasoning)"
            },
            "weaknesses": {
                "value": "* The paper lacks clear justification and motivation for why generating failure-guided data would improve model performance\n* No theoretical framework explaining why failure cases would be more valuable than standard training data\n* Table 6 shows similar result w and wo failure induction in math reasoning task, does this mean failure induction does not benefit math reasoning tasks?\n* No analysis of computational costs or token/API budget comparisons with baseline methods\n* Reward mechanism not clearly explained\n* Insufficient baseline comparisons, especially for mathematical reasoning task"
            },
            "questions": {
                "value": "* Could you provide theoretical or empirical justification for why failure-inducing data would be more valuable than standard training data for model improvement?\n* How does REVERSEGEN's performance evolve with increasing iterations? What determines convergence?\n* Could you analyze why failure induction appears less effective for mathematical reasoning (Table 6) compared to other tasks?\n* What are the computational requirements (tokens, API calls, training time) compared to baseline methods?\n* Could you provide more details about the reward mechanism design and validation process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}