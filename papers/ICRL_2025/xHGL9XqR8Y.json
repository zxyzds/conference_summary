{
    "id": "xHGL9XqR8Y",
    "title": "The Wisdom of a Crowd of Brains: A Universal Brain Encoder",
    "abstract": "Image-to-fMRI encoding is important for both neuroscience research and practical applications. However, such \u201cBrain-Encoders\u201d have been typically trained per-subject and per fMRI-dataset, thus restricted to very limited training data. In this paper we propose a Universal Brain-Encoder, which can be trained jointly on data from many different subjects/datasets/machines. What makes this possible is our new voxel-centric Encoder architecture, which learns a unique \u201cvoxel-embedding\u201d per brain-voxel. Our Encoder trains to predict the response of each brain-voxel on every image, by directly computing the cross-attention between the brain-voxel embedding and multi-level deep image features. This voxel-centric architecture allows the functional role of each brain-voxel to naturally emerge from\nthe voxel-image cross-attention. We show the power of this approach to: (i) combine data from multiple different subjects (a \u201cCrowd of Brains\u201d) to improve each individual brain-encoding, (ii) quick & effective Transfer-Learning across sub- jects, datasets, and machines (e.g., 3-Tesla, 7-Tesla), with few training examples, and (iii) we show the potential power of the learned voxel-embeddings to explore brain functionality (e.g., what is encoded where in the brain).",
    "keywords": [
        "Image-to-fMRI encoding",
        "Explore the brain using ML",
        "Brain-Image cross-attention",
        "Visual Perception",
        "Brain Mapping",
        "Neuroscience",
        "Computer Vision"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "A Universal Brain-Encoder, which combines data from multiple brains (from many different subjects/datasets/machines, without any shared data).",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=xHGL9XqR8Y",
    "pdf_link": "https://openreview.net/pdf?id=xHGL9XqR8Y",
    "comments": [
        {
            "title": {
                "value": "Mapping Voxel Embeddings to Brain Regions."
            },
            "comment": {
                "value": "Dear Authors,\n\nI have a question regarding the mapping of voxel embeddings to brain regions. Given that each voxel embedding is a 256-dimensional vector, could you please clarify how these embeddings are associated with specific brain regions, particularly in Section 5, \"Exploring the Brain Using Voxel-Embeddings\"?\n\nThank you."
            }
        },
        {
            "summary": {
                "value": "This paper introduces a Universal fMRI Encoder for the prediction of brain responses to image stimuli. Unlike traditional subject-specific brain encoding models, the proposed work is trained and validated across multiple subjects and datasets. The model learns voxel embedding through cross-attention with multi-level deep image features, allowing the model to capture functional roles of different brain regions while sharing other network weights across subjects. The model is evaluated on 3 datasets on two measurements: a comparison of the estimated fMRI signal vs. ground truth and the image retrieval accuracy using top-k accuracy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed Universal Brain-Encoder can effectively handling sequences from different subjects, datasets, and machines, which enhances its applicability for both neuroscience research and practical applications\n\n2. The paper presents comprehensive experimental results, and the proposed Universal Brain-Encoder achieves satisfied performance across multiple datasets. Notably, it achieves substantial performance improvements when trained on multi-dataset inputs, supporting the authors' argument regarding the \"Crowd of Brains\" concept"
            },
            "weaknesses": {
                "value": "1. The idea appears to closely resemble existing works such as [1], MindFormer [2], MindEye2 [3], MindBridge [4], and BDI [5]. These studies also learn a set of independent parameters for each subject while sharing most parameters across subjects. The novelty of the proposed idea needs further clarification.\n\n2. Some brain decoding methods employ symmetric architectures, so they have both Image-to-fMRI and fMRI-to-Image networks, such as [6] and [7]. A discussion about these approaches should be included in the comparative experiments.\n\n3. The quality of the generated fMRI data requires more validation. The authors should use additional metrics or evaluation methods to assess whether the generated data can still be used to analyze brain activity. For instance, the authors can use existing brain decoding models to prove they can reconstruct images from the generated fMRI sequences.\n\n4. An ablation study on different Voxel Embedding dimensions should be included.\n\n5. Is it better to utilize all voxels in fMRI sequences? The proposed voxel-based approach has the potential to capture latent semantic relationships between brain activities and input signals, whereas manually selected ROIs may lead to information loss. If the method can effectively model all voxels and provide visualize results as demonstrated in Fig. 7, it would yield interesting results.\n\n[1] Functional Brain-to-Brain Transformation with No Shared Data\n[2] MindFormer: A Transformer Architecture for Multi-Subject Brain Decoding via fMRI\n[3] MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data\n[4] MindBridge: A Cross-Subject Brain Decoding Framework\n[5] Brain Dialogue Interface (BDI): A User-Friendly  fMRI Model for Interactive Brain Decoding\n[6] From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI\n[7] Rethinking Visual Reconstruction: Experience-Based Content Completion Guided by Visual Cues"
            },
            "questions": {
                "value": "What is the spatial resolution of the pre-processed fMRI datasets and the corresponding dimensionality of the 4D volumetric data? It is curious whether the spatial resolution can support the fine-grained analysis of brain response as shown in Fig. S13."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns are found in this work, and all datasets used are from the public domain."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work trains a universal brain encoder that can predict the brain responses from multiple participants and datasets. The input to the encoder is a stimulus image and a learned 256-dimensional voxel-wise embedding. The voxel-wise embeddings are randomly initialized, and contain no information about the participant or the spatial location of the voxel. All other parameters in the encoder are shared between voxels, participants, and datasets.\n\n- The predictions are evaluated with per-voxel pearson correlation and a per-image retrieval metric.\n- Their universal encoder significantly outperforms baseline single-subject encoders (figure 4)\n- Inclusion of a higher quality 7T data improves performance on older 3T and 4T datasets (figure 5)\n- A pre-trained encoder can transfer to new subjects and datasets just by learning new voxel embeddings. Performance is much higher and learning is faster than a single-subject encoder.\n- K-means clustering is applied to the voxel embeddings to identify regions for food, words, faces, sports, indoor scenes, outdoor scenes."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- This is a very strong and well written paper. The methods are easy to understand and well motivated. I could see this encoder being used a lot when working with smaller vision datasets. \n- Retrieval accuracy is impressively high. It looks close to 95% top-1 accuracy for subjects 1 and 2 across 1000 test images (chance is 0.1%).\n- Statistical tests are performed for all experiments."
            },
            "weaknesses": {
                "value": "I think the paper is lacking some exploration and visualization of the voxel embeddings. Here are some ideas:\n- Apply the clustering to more than 2 participants. \n- Other clustering methods besides k-means (i.e. some that can deal with outliers)\n- A flatmap visualization with outlines of previously identified category selective regions for faces, bodies, places, and words. This would be helpful for comparing to the clusters identified with k-means.\n- A UMAP or tsne applied to the combined embeddings for the 8 participants, and then visualized on the cortical surface with a color mapping."
            },
            "questions": {
                "value": "In other papers that use NSD, subject 5 is typically the best performing subject. However in this work the retrieval accuracy is quite a bit lower than subjects 1 and 2. Any ideas why this might be the case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposed a Universal Brain Encoder that can train on multiple subjects from different datasets. The methods applied cross-attention between features of the image and fMRI voxel. Experiments on three datasets have shown that performance can be improved after fine-tuning with new subjects."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The problem of the universal brain-image generation is important.\n\n2. Experiments successfully demonstrated the proposed method can train on multiple subjects from different datasets and achieved a better performance.\n\n3. The presentation of motivations, methods, and experiments is clear and easy to follow."
            },
            "weaknesses": {
                "value": "1. The announcement of the *first-ever Universal Brain-Encoder* is too aggressive. The idea of the model is to be able to train on multiple subjects and datasets instead of **universally** applying to any unseen subjects or datasets. The performance of the proposed model on a new subject is tested via few-shot transfer learning instead of zero-shot learning. \n\n2. The method of cross-attention is not novel and exists in the field of brain-image generation [1,2,3].\n\n[1] Sun, Jingyuan, et al. \"Contrast, attend and diffuse to decode high-resolution images from brain activities.\" Advances in Neural Information Processing Systems 36 (2024).\n\n[2] Wang, Shizun, et al. \"Mindbridge: A cross-subject brain decoding framework.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[3] Scotti, Paul S., et al. \"MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data.\" arXiv preprint arXiv:2403.11207 (2024).\n\n3. Lack of ablation studies. For example, in Figure 7, functional embedding is not evaluated to show the functional roles of voxels but the k-means results of voxel embeddings were used. Also, other components, which were claimed essential in the main text, are not evaluated.\n\n4. The potential power of finding subject-specific brain parcellation is interesting, but the demonstration in Figure 7 shows this can only proceed on visual networks instead of the whole brain. Brain parcellation is for the whole brain."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a multi-subject fMRI encoding model. The authors set learnable voxel-wise embedding for each subject and optimize the subject-specific voxel-wise embedding and the subject-shared encoding model through the fMRI prediction task. Through the evaluation on multiple fMRI datasets, the authors validate the effectiveness of the proposed method and further validate that few-shot cross-subject transfer can be achieved. Finally, this paper utilizes learned voxel-wise embedding to initially explore concept-selective in the brain cortex, showing its value in neuroscience applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ This paper focuses on the intersting but few-studied issue of multi-subject fMRI encoding.\n+ The motivation for this paper is clear and the proposed method has yielded promising results.\n+ The proposed method achieves cross-device and cross-subject few-shot transfer learning, making it highly applicable.\n+ Neuroscience exploration using the voxel-wise embedding proposed in the paper is promising.\n+ The paper gives detailed implementation details of the model model which helps in understanding and also ensures reproducibility."
            },
            "weaknesses": {
                "value": "#### 1. More related works should be discussed:\nThe authors should discuss additional related works or acknowledge that these related works have inspired them, including but not limited to:\n+ At the level of model design, the proposed method seems to be a revised version of [1], which employs ROI-wise embedding rather than voxel-wise embedding. \n+ At the level of research ideas, some works also train encoding models and then use them for neuroscience explorations, such as [2][3].\n+ at the level of fMRI representation learning, [4][5] already show the use of multi-subject data can enhance each subject's representation, and [4][6][7] already show the use of other subject's fMRI can achieve few-shot transfer learning.\n\n#### 2. Limited evaluation metrics:\n\nIn this paper, only voxel-wise Pearson coefficients and retrieval results are used as evaluation metrics, and the inclusion of more metrics such as $R^2$, MSE, etc. can further indicate the fMRI encoding accuracy.\n\n\n#### 3. On fMRI replicable\n\nThe method proposed by the authors fails to address the issue of fMRI replicability, which is a common problem with regression-based fMRI encoding models. The authors already discuss this in their limitation and assume that the fMRI captured by subjects viewing the same image multiple times is the same. However, this assumption may greatly limit the training of fMRI encoding models.\n\n\n\n[1] Hossein Adeli et al. Predicting brain activity using Transformers. bioRxiv, 2023: 2023.08. 02.551743. \n\n[2] Andrew Luo et al. Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models. NeurIPS 2023.\n\n[3] Andrew Luo et al. BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity. ICLR 2024.\n\n[4] Shizun Wang et al. A cross-subject brain decoding framework. CVPR 2024.\n\n[5] Guangyin Bao et al. Wills Aligner: A Robust Multi-Subject Brain Representation Learner. arXiv:2404.13282.\n\n[6] Paul S. Scotti et al. MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data. ICML 2024.\n\n[7] Zixuan Gong et al. MindTuner: Cross-Subject Visual Decoding with Visual Fingerprint and Semantic Correction. arXiv:2404.12630."
            },
            "questions": {
                "value": "+ In Figure 2, the dimension of Voxel Embedding is \"1xE\", why 1 and not the number of voxels? If it's 1, does it mean that we need to train a model for each voxel? If it is the number of voxels, then the number of voxel embeddings should be much larger than the image tokens (i.e. P), and at this point does the attention module make the randomly initialized voxel embeddings overly self-concerned?\n+ In Figure 6(a), why is the performance of coding saturated when the few-shot samples exceed 3000?\n\nI'm willing to further raise my rating according to the author's rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}