{
    "id": "vjbIer5R2H",
    "title": "Improved Risk Bounds with Unbounded Losses for Transductive Learning",
    "abstract": "In the transductive learning setting, we are provided with a labeled training set and an unlabeled test set, with the objective of predicting the labels of the test points. This framework differs from the standard problem of fitting an unknown distribution with a training set drawn independently from this distribution. In this paper, we primarily improve the generalization bounds in transductive learning. Specifically, we develop two novel concentration inequalities for the suprema of empirical processes sampled without replacement for unbounded functions, marking the first discussion of the generalization performance of unbounded functions in the context of sampling without replacement. We further provide two valuable applications of our new inequalities: on one hand, we firstly derive fast excess risk bounds for empirical risk minimization in transductive learning under unbounded losses. On the other hand, we establish high-probability bounds on the generalization error for graph neural networks when using stochastic gradient descent which improve the current state-of-the-art results.",
    "keywords": [
        "concentration inequality",
        "generalization bounds",
        "graph neural networks",
        "transductive learning",
        "unbounded losses"
    ],
    "primary_area": "learning theory",
    "TLDR": "We derive two novel concentration inequalities for suprema of empirical processes when sampled without replacement for unbounded functions, which take the variance of the functions into consideration and apply our new inequalities to GNNs.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vjbIer5R2H",
    "pdf_link": "https://openreview.net/pdf?id=vjbIer5R2H",
    "comments": [
        {
            "summary": {
                "value": "This paper derives risk bounds for transductive learning scenarios with specific applications to Graph Neural Networks (GNNs) under unbounded loss functions. The work focuses on theoretical guarantees for both sub-Gaussian and sub-exponential loss functions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Novel analysis of unbounded loss functions in the transductive learning setting\n- Mathematical rigour in deriving the theoretical bounds\n- Practical applications to GNN scenarios"
            },
            "weaknesses": {
                "value": "**Weaknesses:**\n\n- Limited scope of unbounded loss functions:\n\n  - Analysis is restricted to sub-Gaussian and sub-exponential functions (and sub-Weibull in appendix)\n  - Other important classes of unbounded loss functions are not addressed\n\n\n- Insufficient comparison with prior work:\n\n  - The paper overlooks crucial related work, particularly [1] (Maurer & Pontil, 2021). While [1] focuses on inductive settings, their theoretical foundations appear relevant. A comparative analysis between Theorems 1 and 2 and the results in [1] is needed.\n\n- Limited Contribution:\n  - The current contribution of theoretical analysis in the GNN framework is limited. The current results are general and independent of the Graph properties. \n\n**Minor Comments:**\n\n-  In Assumption 3, \"\u03b1-H\u00f6lder\" is misspelled\n- Add the explanation of Hoeffding's reduction method to the appendix\n- Use \"Boundedness\" instead of \"Boundness\"\n- Use \"techniques\" instead of \"technologies\"\n- Line 221 \"We mainly follows the traditional technique...\" --> \"We mainly follow the traditional technique\"\n\n---\n\n**References:**\n\n- [1] Maurer, A., & Pontil, M. (2021). Concentration inequalities under sub-gaussian and sub-exponential conditions. Advances in Neural Information Processing Systems, 34, 7588-7597."
            },
            "questions": {
                "value": "- Graph Properties:\n\n  - Which specific graph properties are leveraged in your analysis?\n  - The current bounds seem applicable to general transductive learning scenarios - how are they specialized for graph-based problems?\n\n\n- Asymptotic Behavior:\n\n  - Please elaborate on the behavior of your bounds as $u \\rightarrow \\infty$ and $m\\ll u$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the transductive learning problem, where the learner receives a subset of labeled samples drawn without replacement from a dataset, alongside unlabeled samples for which the goal is to predict the labels. As the samples are not independent and the loss function may be unbounded, the authors develop concentration inequalities for the supremum of empirical processes sampled without replacement for unbounded functions. They use these inequalities to derive tighter risk bounds for transductive learning problems and graph neural networks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper is the first to derive concentration inequalities for the supremum of empirical processes sampled without replacement for unbounded functions, presenting a novel result. Furthermore, these concentration inequalities are utilized to refine the risk bounds for transductive learning and graph neural networks found in the literature."
            },
            "weaknesses": {
                "value": "The paper lacks numerical results to support the derived risk bounds. Additionally, as it does not provide lower bounds for the risk, it remains unclear whether the resulting bounds could be further improved.\n\nThere are some typos in the paper:\n\nLine 221: we mainly \"follows\"\nLine 222: we \"introduced\"\nLine 405: w_1^{T+1}   ->   w^{T+1}"
            },
            "questions": {
                "value": "In Theorem 1, for a fixed $c$ from the set $C$, both $f(c)$ and supremum of $f(c)$ are not random. So how is the Orlicz defined when there is no randomness here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this work, the authors establish generalization bounds for the transductive setting, applicable even in cases with unbounded loss. The core technical contribution is a novel tail bound for the relevant empirical process. Using these results, the authors then derive generalization bounds for graph neural networks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The transductive setting has gained renewed attention recently, as many practical problems are better suited to transductive learning than the traditional iid statistical framework. In this context, the work is particularly relevant.\n\nThe concentration bounds presented are non-trivial, and their proof involves sophisticated mathematical tools."
            },
            "weaknesses": {
                "value": "1. The authors do not offer any motivation for addressing unbounded loss. A discussion on why this is an important and relevant problem to study would be beneficial.\n\n2. It is unclear what the significance of Theorems 3 and 4 is. Let\u2019s consider Theorem 3 as an example. Given other terms, the upper bound can at best be\n$$ \\frac{N^2 \\log\\left( \\frac{1}{\\delta}\\right)}{m^2 u}.$$\nThe authors claim this bound is state-of-the-art for $m = o(N^{2/5})$. If we set $m = N^{1/5} = o(N^{2/5})$, then $u = N - N^{1/5} \\leq N $, and the upper bound is at least\n$$ \\geq \\frac{N^2 \\log\\left( \\frac{1}{\\delta}\\right)}{N^{2/5} \\cdot N} \\geq N^{3/5} \\log\\left( \\frac{1}{\\delta}\\right).$$\n\nGiven that this is the highest the proven upperbound can be, it is difficult to see why such a bound would be of interest as $N$ can be quite large, making the bound potentially vacuous. I may likely be missing something here, and I would be happy to engage with the authors during the discussion session to gain further clarity and adjust my score."
            },
            "questions": {
                "value": "1. Given the claim that the authors prove new results for unbounded loss, it would be helpful to discuss what is already known in the context of bounded loss. Are similar concentration inequalities established for the bounded loss setting? The paper references [1,12] as proving some tail bounds\u2014could the authors provide a brief summary of the results from those works?\n\n\n\n2. See the weakness mentioned above. Could the authors offer a clearer explanation of how to interpret Theorems 3 and 4? Specifically, how should the generalization bound behave as the size of the training set $m$ increases? Does the bound vanish in any meaningful way as $m$ increases, and if so, how?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies potentially improved risk bound for transductive learning following the conventional localized method. The main difference the author claims is that the risk bounds are for unbounded functions. However, such claim, together with the technical results for  unbounded functions, are very questionable. Furthermore, there are no detailed comparison to the current state-of-the-art risk bounds for the main results in Theorem 3 and Theorem 4."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper studies potentially improved risk bound for transductive learning following the conventional localized method. The main difference the author claims is that the risk bounds are for unbounded functions. However, such claim, together with the technical results for  unbounded functions, are very questionable."
            },
            "weaknesses": {
                "value": "There are several major technical drawbacks.\n\n\n\n1. While this paper claims that the risk bounds for transductive learning are for unbounded loss functions, the assumptions required for the results are essentially designed for bounded loss functions. For example, the main results Theorem 3 and Theorem 4 need the assumption that $E[f^2] \\le B E[f]$. It is well known that such assumption, $E[f^2] \\le B E[f]$, holds mainly for bounded loss functions, such as that in the classical local Rademacher complexity work (Bartlett Local Rademacher Complexities, AOS 2005). It turns out that while the paper claims risk bounds for \"unbounded loss\", but the results rely on the assumption which mainly hold for bounded loss functions.\n\n2. It is well known that Rademacher complexity or local Rademacher complexity based methods derive distribution-free risk bounds that do not need distributional assumptions. In contrast, the risk bounds in the main results Theorem 3 and Theorem 4 require sub-Gaussian and sub-exponential loss functions. It is not clear which loss functions are  sub-Gaussian or sub-exponential, and such restriction on the loss functions can significantly limit the application scope of the derived bounds.\n\n3. There are no detailed comparison to the current state-of-the-art risk bounds for the main results in Theorem 3 and Theorem 4, such as the existing transductive bounds in (Tolstikhin et al. 2014, Localized Complexities for Transductive Learning. COLT 2014). Without comparison to prior art, the significance of these results is not clear and questionable.\n\n4. The risk bounds in the main results, Theorem 3 and Theorem 4, do not convergence to 0 under the case that $m = N^{\\alpha}$ or \n$m = N^{\\alpha}$ with $\\alpha \\in (0,1/2]$, and they even diverge to $\\infty$ if $\\alpha \\in (0,1/2)$.  This is in a strong contrast to existing risk bounds for excess risk bounds where such bounds should always at least converge to $0$, and it is really misleading to claim such risk bounds are improved ones."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}