{
    "id": "XcAJ0qsMgh",
    "title": "Annealing Flow Generative Model Towards Sampling High-Dimensional and Multi-Modal Distributions",
    "abstract": "Sampling from high-dimensional, multi-modal distributions remains a fundamental challenge across domains such as statistical Bayesian inference and physics-based machine learning. In this paper, we propose Annealing Flow (AF), a continuous normalizing flow-based approach designed to sample from high-dimensional and multi-modal distributions. The key idea is to learn a continuous normalizing flow-based transport map, guided by annealing, to transition samples from an easy-to-sample distribution to the target distribution, facilitating effective exploration of modes in high-dimensional spaces. Unlike many existing methods, AF training does not rely on samples from the target distribution. AF ensures effective and balanced mode exploration, achieves linear complexity in sample size and dimensions, and circumvents inefficient mixing times. We demonstrate the superior performance of AF compared to state-of-the-art methods through extensive experiments on various challenging distributions and real-world datasets, particularly in high-dimensional and multi-modal settings. We also highlight AF\u2019s potential for sampling the least favorable distributions.",
    "keywords": [
        "Continuous Normalizing Flow",
        "Generative Model",
        "Optimal Transport",
        "High-Dimensional Sampling",
        "Multi-Modal Sampling"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=XcAJ0qsMgh",
    "pdf_link": "https://openreview.net/pdf?id=XcAJ0qsMgh",
    "comments": [
        {
            "comment": {
                "value": "We appreciate your comments and suggestions. We would like to clarify some weaknesses and questions you are concerned about:\n\n* The paper lacks a related works section. Namely, the connections to NN-assisted MCMCs, which are in my opinion the most related methods, are not discussed and the performances of these approaches are not reported. In my experience, these methods have better performance than what the paper seems to indicate. In particular, they typically do correct for the mode imbalance that a traditional MCMC based on local updates could not do.\n\nIn the second-to-last paragraph, we discussed recent NN-assisted MCMC methods, including those that use neural networks as transition kernels and MCMCs based on normalizing flows. Notably, the \"AI-Sampler\" compared in our experiments is an NN-assisted MCMC method, which leverages NN to construct the transition kernels. We will include additional experimental comparisons with other NN-assisted methods, including \"Neural Importance Sampling\" you mentioned and the papers mentioned by Reviewer 7EWK and HDGZ.\n\nA unique advantage of our algorithm is that, unlike most NN-assisted sampling methods, it does not involve MCMC as part of the training process. This approach offers several benefits, including avoiding the long mixing times of MCMC (which can increase exponentially with dimensionality) and producing independent samples. NN-methods that lack Annealing-guidance and $W_{2}$ regularization often suffer from mode-seeking behaviors and often fail with densities that have widely-separated modes (like in the settings of our experiments).\n\n* In this regard, the novelty of the paper is lesser than what it appears. Annealed Flow Transport (Arbel et al 2021 in the paper) proposes a very related strategy.\n\nThank you for pointing out this work. We noted that it focuses on discrete normalizing flows (NFs), which inherently exhibit mode-seeking behavior, rather than continuous normalizing flows (CNFs). To address such behavior, the paper uses SMC and importance weights to adjust the loss. Additionally, the experiments presented are primarily focused on scenarios with a low number of modes. However, the mode-seeking tendencies of NFs may become more pronounced as the number of target modes increases, as in our experiment with 1024 widely separated modes in 50D space.\n\nBesides, through experiments, we observed that without $W_{2}$-regularization (as in their NFs), the transport paths become less organized and more dispersed, which means the flow often misses the optimal path. WWe are currently working on developing theoretical insights into the convergence properties of Annealing guidance and $W_{2}$ regularization. Thanks for bringing this work to us, and we will include it in the revised draft.\n\n* The proposed sampling methods lacks theoretical guarantees of the accuracy of the sampling conversely to NN-assisted (Markov Chain) Monte Carlo samplers, such as Neural IS [3], Flow MC (Gabrie et al 2021, 2022 in the paper) or Annealed Flow Transport (Arbel et al 2021 in the paper).\n\nOur algorithm does not rely on an MCMC process, so fewer theoretical properties, such as the law of large numbers, CLT, and variance properties, can be proved. Nonetheless, after submission, we provided an official proof of the infinitesimal optimal velocity field, demonstrating it as the score difference between two consecutive annealing densities. We will further expand on the convergence properties of annealing guidance and $W_{2}$ regularization, which are key to our algorithm\u2019s ability to explore the entire space effectively. Thanks for bringing us attention to develop more theories!\n\n* The cost of training only one velocity field at the time is not discussed, nor how to choose the number of these intermediary steps.\n\nThank you for your detailed review. We will discuss these in the revised draft.\n\n* I am surprised by PT\u2019s result in Figure 2. I would expect that this exact sampler designed to sample from multimodal distribution works perfectly in this 1d example. Can the author justify underwhich circumstances the imbalance result was obtained?\n\nFor PT to balance samples between the modes, it must allow transitions between different modes frequently enough at different temperatures. When the two modes are far-separated, particles can have difficulty moving between them even at a high-temperature.\n\n * Table 3, what are the expected ground truth values?\n\nThis is shown in the first line of the table.\n\n* For the importance flow method as well, the paper claims \u201cThe estimator is unbiased and can achieve zero variance theoretically\u201d but this is only true if the density estimation ratio is predicted exactly by the neural network.\n\nThank you again for all these comments! We will revise our draft accordingly."
            },
            "title": {
                "value": "Reply to Reviewer JJiD"
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer HDGZ"
            },
            "comment": {
                "value": "We appreciate your detailed comments and suggestions and will revise our draft accordingly. We would also like to clarify some of your questions related to our draft:\n\n* Could you compare your work against [1] and [2] which have solved the same problem with very similar tools?\n\nWe have read the two papers carefully, and noted that their methods are based on gradient flows, instead of Annealing-guided flow with Wasserstein regularization. Through experiments, we observed that without $W_{2}$-regularization, the paths become less organized and more dispersed, which means the flow often misses the optimal path; and if the density modes are widely separated or in high-dimensional spaces, without Annealing-guidance, the gradient flow often fails. These two key features help ensure success in high-dimensional multi-modal cases (an example is in Figure 6, a density with 1024 modes in 50D). We will develop theories on the guiding effects of $W_{2}$ and Annealing. Thank you for identifying these two papers; we will include comparison experiments with theirs.\n\n* Could you rewrite sections 2 and 3 but defining the normalizing constants from the beginning? I think it will make the paper much clearer. Moreover, it would avoid writing KL divergences between unnormalised distributions as done extensively in section 3 and appendix 1.\n\nThank you for your suggestions. We will revise accordingly.\n\n* In common litterature, the Kullback-Leiber divergence is defined between normalized distributions as $D(p \\parallel q) = \\mathbb{E}_{X \\sim p} \\left[ \\log \\frac{p(X)}{q(X)} \\right]$. However, L760 shows a completely different definition.\n* According to the beginning of section 3.2, the quantity $\\rho_k$ is the density of the flow following $v_k$, which means that $\\log \\rho_k(x(t_{k-1}))$ is not independent of $v_k(x(s), s)$, which would contradict L800.\n\nWe're sorry for the confusion; there is a typing error in the proof. We should have written the first step as $KL(T f_{k-1} \\parallel f_k) = E_{X\\sim \\rho(t_{k})}\\left[ \\log T f_{k-1}(X) / f_{k}(X) \\right]$, where $\\rho(t)$ represents the density evolution governed by equation (2).\n\nHere, $\\rho(t_{k}) = T f_{k-1}$, and from the constraints in (3), we see that the density evolution is subject to the equations (1) and (2). Then subject to (1) and (2), we can write the objective $E_{X\\sim \\rho(t_{k})}\\left[ \\log T f_{k-1}(X) / f_{k}(X) \\right] = E_{X\\sim \\rho(t_{k-1})}\\left[ \\log T f_{k-1}(X(t_{k})) / f_{k}(X(t_{k})) \\right]$, where $\\log T f_{k-1}(X(t_{k})) =\\log \\rho(X(t_{k})) = \\log \\rho(X(t_{k-1})) - \\int_{t_{k-1}}^{t_k} \\nabla \\cdot v_k(X(s), s) ds$. \n\nFor the second question: $v_{k}$ flows density from $\\rho(X(t_{k-1}))$ to $\\rho(X(t_{k}))$, where $X(t_{k}) = X(t_{k-1}) +\\int_{t_{k-1}}^{t_{k}} v_k(X(s), s) ds$. Therefore, $E_{X\\sim \\rho(t_{k-1})} [ \\log \\rho(X(t_{k-1})) ]$ is a constant that is independent of $v_{k}$. (It depends on $v_{1},...,v_{k-1}$.) We are sorry again for the typing typo that leads to the misunderstanding.\n\n* At L260, you claim that your training procedure only requires a single neural network. Doesn't your sampling procedure require to keep all the intermediates\n\nOur algorithm adopts a block-wise training procedure instead of end-to-end training. Our algorithm keeps the intermediate $v_{k}$. However, during training, only one NN (one $v_{k}$) is trained at each time, thus greatly enhancing training efficiency.\n\n* Could you perform the experiment on Fig. 1 with differently weighted modes ? I feel like recovering the weights of the modes is the true challenge.\n\nThank you for the suggestion. After submission, we implemented modes with different weights, and the results are also convincing. We will include them in our future submission.\n\n* Could you compute the metrics given in [6] which seem computable in your case ? Those metrics really focus on multimodal distributions.\n* Could you provide metrics's mean and standard deviation (as well as the number of samples used) for each experiment ?\n* Could you display true samples from the Funnel distribution alongside yours ? It would clarify the pros and cons of each algorithm.\n\nThank you! We will.\n\nWe wish to conclude by highlighting that we are the first to use Annealing-guided CNFs with $W_{2}$ regularization, which are the key to efficient explorations of all modes in high-dimensional spaces. Our experiments focus more on extreme distributions (high-dimensional densities with a large number of widely separated modes), which are distinct from many other CNFs studies. In our recent draft, we proved that the infinitesimal optimal velocity field represents the score difference between two consecutive densities. We will continue refining the theories, and thank you for these useful suggestions."
            }
        },
        {
            "summary": {
                "value": "The paper proposed a so-called Annealing flow (AF) sampling method for  high-dimensional multi-modal distributions.  The key idea is to learn a continuous normalizing flow (CNF)  based transport map, guided by annealing process, to transition samples from an easy-to-sample distribution to the target distribution. The feature of  AF training does not rely on samples from the target distribution. The results in dimensional up to 50  show that AF ensures effective and balanced mode exploration, especially for multi-model distribution by comparing  AF to other SOTA methods though various challenging distributions, particularly in high-dimensional and multi-modal settings, including  AF\u2019s potential for sampling the least favorable distributions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper present a continuous Normalizing flow  based method for sampling from challenging high dimensional multi-modal distributions.  There are some theoretically insights on the proposed models using the  predefined intermediate and the numerical results show the superiority of the proposed sampling method under variety of criteria."
            },
            "weaknesses": {
                "value": "1. The details of training process is missing. Especially, the cost of the training process of CNF flow is not mentioned,  and also how many intervals are needed in order to obtain a stable sampling process. It seems also crucial especially how the overall sampling results depend on the flow interval. \n\n2. It is unclear how the method can be generalized to real high dimensional problems arising from real applications and distribution free problems where only samples are available. \n\n3. The sample efficiency (both sampling speed and quality) is not commented and compared with different methods."
            },
            "questions": {
                "value": "1. The author should explain  the detail of training  including how the minimization problem is solved and how the parameters would affect the sampling performance. \n2. Comments or preliminary results  on how it can appplied to any real high dimensional applications (>>50), for example a real physical machine learning problem. \n3. Other comparisons results in terms of computational cost."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to fit continuous normalizing flows (CNFs) to multi-modal unnormalized target distributions using annealing and optimal transport techniques. The proposed method first defines a sequence of annealed distributions interpolating the target distribution and a simple base distribution. Leveraging the dynamic optimal transport objective, the authors propose to train a sequence of CNFs to model the transition between each pair of neighboring annealed distributions. This method encourages the model to explore modes without relying on any target samples or running MCMC chains. Various experiments show that the proposed method has comparable or better performance than some of the traditional MCMC, particle-based and NN-based methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper is generally well-written. The description of the proposed method is clear and easy to follow.\n2. The proposed sampler is able to explore modes in multi-modal target distributions, which is a desired property in many applications.\n3. Once the model is trained, the sampling cost is low since it does not require running MCMC chains and sampling scales linearly with the sample size and dimensionality.\n4. The proposed method is evaluated on various experimental settings and has comparable or better results than some of the tradition MCMC, particle-based and NN-based samplers, showing its good applicability."
            },
            "weaknesses": {
                "value": "1. The manuscript contains a GitHub link in Line 369-370, which reveals the authors' identity.\n\n2. The proposed method lacks technical novelty and is a bit incremental, since it directly applies an optimal transport loss with KL relaxation to train a sequence of CNFs to model the transitions between neighboring annealed distributions, which is a trivial combination of existing techniques. \n\n3. The scalability and reliability of the proposed training algorithm is questionable, since it requires training K seperate CNFs sequentially, each modeling the transition between one of the K neighboring pairs of annealed distributions. Furthermore, within each transition, the interval is further discretized into S grid points in order to estimate the integral.\n- If K is small, the transition will be difficult and the model can fail to capture some modes. If S is small, the discretization error in numerical integration will be large and the gradient for updating the model will be inaccurate. If K and S are large, the proposed training algorithm will be very expensive as it requires backpropagating through all these steps and hierarchies.\n- The approximation error in each transition will accumulate as the sampler gets closer to the target distribution due to the bootstrap-style training method.\n\n4. It is also unclear how the proposed method should be positioned among other flows-based or NN-based samplers with similar techniques, since many important recent related works are not discussed in the paper, including \n- flow-based sampler trained with annealing: [1, 2, 3]\n- score/diffusion-based sampler: [4, 5, 6, 7, 9]\n- optimal control-based sampler: [8]\n\n5. Several closely related baselines are missing in the experiment section:\n- At the moment, most baselines are MCMC/particle-based samplers.\n- Since the proposed method is a flow-based model trained with the annealing technique, it should be empirically compared to at least the following models which use similar flow-annealing techniques: AFT [1], CRAFT [2], FAB [3]. \n- In addition, since the proposed method is broadly an NN-based sampler, ideally it should also be compared with some of other types of SOTA NN-based samplers, such as iDEM [6], DDS [7], PIS [8], PDIS [9].\n\n6. For the toy 1D experiment in Figure 2, it would be more convincing if \n- the two modes in the target distribution are unbalanced (i.e., one has a larger density than the other), as suggested by [5] to test whether the proposed method exhibits a similar blindeness issue as in some of the score-based samplers.\n- it also includes results of other flow and annealing-based baselines in this comparison to get a sense of how these similar methods compare to each other, since they are more related to the proposed method in this paper.\n\n7. [Optional] The proposed method is only tested on synthetic problems. Perhaps for future work, it would be nice if the authors could consider some real-world problems, such as sampling from Boltzmann distributions (i.e., Boltzmann generator, see e.g., [3, 6]) which share exactly the same problem setting as in this paper.\n\n**Reference**:\n\n[1] Arber et al., \u201cAnnealed flow transport Monte Carlo\u201d, ICML 2021\n\n[2] Matthews et al., \u201cContinual repeated annealed flow transport Monte Carlo\" ICML 2022.\n\n[3] Midgley et al., \u201cFlow annealed importance sampling bootstrap\u201d, ICLR 2023\n\n[4] Huang et al., \u201cReverse diffusion Monte Carlo\u201d, ICLR 2024\n\n[5] Chen et al., \u201cDiffusive Gibbs sampling\u201d, ICML 2024\n\n[6] Akhound-Sadegh et al., \u201cIterated denoising energy matching for sampling from Boltzmann densities\u201d, ICML 2024\n\n[7] Vargas et al., \u201cDenoising diffusion samplers\u201d, ICLR 2023\n\n[8] Zhang et al., \u201cPath integral sampler: a stochastic control approach for sampling\u201d, ICLR 2022\n\n[9] Phillips et al., \u201cParticle denoising diffusion sampler\u201d, ICML 2024"
            },
            "questions": {
                "value": "1. How sensitive is the weight hyperparameter $\\gamma$ that controls the balance between the two terms in the dynamic optimal transport loss? Does the value of $\\gamma$ vary a lot across different experiments? How the values of $\\gamma$ were chosen in the experiments?\n\n2. How does the hyperparameters $K$ (the number of intermediate distributions) and $S$ (the number of discretization grids within each transition between two intermediate distributions) scale with the dimensionality and multi-modaility of the target distribution?\n\n3. [Typo] What is \"MEID\" in table 2? Is it \"MIED\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "details_of_ethics_concerns": {
                "value": "The manuscript contains a GitHub link in Line 369-370, which reveals the authors' identity."
            }
        },
        {
            "summary": {
                "value": "The present paper proposes a sampler based on a continuous normalizing flow. The idea is to decompose the transport into sub-transport tasks following an annealing scheme between a simple base distribution and the target. Velocity fields are learned independently for each such transport task so as to minimize a Kullback-Leibler divergence with a transport cost penalization. Once the networks are trained, the sampling is operated by simply sampling from the continuous NF. \n\nThe paper also points that the method can be employed to learn a flow as a proposal distribution in a rare event  importance sampling (IS) scenario. It is proposed that the IS weights are estimate by training a neural network to estimate the density ratio between the flow and the target. \n\nNumerical experiments report a good mode exploration and qualitative good coverage for the proposed method compared to diverse samplers not relying on deep learning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed methods addresses the important point of mode coverage in the task of sampling multimodal distributions in high dimension, which is demonstrated in numerical experiments."
            },
            "weaknesses": {
                "value": "- The paper lacks a related works section. Namely, the connections to  NN-assisted MCMCs, which are in my opinion the most related methods, are not discussed and the performances of these approaches are not reported. In my experience, these methods have better performance than what the paper seems to indicate. In particular, they typically do correct for the mode imbalance that a traditional MCMC based on local updates could not do. \n\n- In this regard, the novelty of the paper is lesser than what it appears. Annealed Flow Transport  (Arbel et al 2021 in the paper) proposes a very related strategy. \n\n- Limitations are not properly discussed: \n\t- The proposed sampling methods lacks theoretical guarantees of the accuracy of the sampling conversely to NN-assisted (Markov Chain) Monte Carlo samplers, such as Neural IS [3], Flow MC (Gabrie et al 2021, 2022 in the paper) or Annealed Flow Transport (Arbel et al 2021 in the paper). \n\t- The cost of training only one velocity field at the time is not discussed, nor how to choose the number of these intermediary steps.\n\t- For the importance flow method as well, the paper claims \u201cThe estimator is unbiased and can achieve zero variance theoretically\u201d but this is only true if the density estimation ratio is predicted exactly by the neural network. \n\n\n\nMinor: \n- About \u201cscore based sampling that do not rely on neural networks\u201d (line 66), the paper should also cite RDMC [1] and SLIPS [2].\n\n\n\n[1] Xunpeng Huang and Hanze Dong and Yifan HAO and Yian Ma and Tong Zhang. Reverse Diffusion Monte Carlo. The Twelfth International Conference on Learning Representations, 2024, https://openreview.net/forum?id=kIPEyMSdFV\n\n[2] Grenioux, Louis, Maxence Noble, Marylou Gabri\u00e9, and Alain Oliviero Durmus. \u201cStochastic Localization via Iterative Posterior Sampling.\u201d In Proceedings of the 41st International Conference on Machine Learning, 16337\u201376. PMLR, 2024. https://proceedings.mlr.press/v235/grenioux24a.html.\n\n[3] M\u00fcller, Thomas, Brian Mcwilliams, Fabrice Rousselle, Markus Gross, and Jan Nov\u00e1k. \u201cNeural Importance Sampling.\u201d ACM Transactions on Graphics 38, no. 5 (October 31, 2019): 1\u201319. https://doi.org/10.1145/3341156."
            },
            "questions": {
                "value": "- Is the condition of optimality of the transport crucial to the success of the method? \n\n- I am surprised by PT\u2019s result in Figure 2. I would expect that this exact sampler designed to sample from multimodal distribution works perfectly in this 1d example. Can the author justify underwhich circumstances the imbalance result was obtained?\n\n- Table 3, what are the expected ground truth values?\n\n- The notations between $\\tilde f_k$ (the target annealing path of distributions)  and $f_k$ (the intermediary push forwards of the CNF) is maybe inconsistent in some places. For instance in (8) and (10), should the expectation be over $\\tilde f_{k-1}$ instead of $f_{k-1}$ ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "A continuous normalizing flow-based transport map guided by annealing is designed to sample from high-dimensional\nand multi-modal distributions with unknown normalization factor. The approach supports effective exploration\nof modes in high-dimensional spaces. The paper contains several comparison with state-of-the-art methods.\nThe method can be extended to a distribution-free model that allows \nto learn an importance flow from a dataset for sampling its least-favorable distribution with\nminimal variance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The basic idea is straightforward, but interesting and the numerical results are quite convincing."
            },
            "weaknesses": {
                "value": "The mathematical notation is in many parts cursorily or wrong as the proof of Proposition 1.\nThe numerical experiments which are restricted to symmetric multimodal targets must be improved. \nSee questions and experimental suggestions  below."
            },
            "questions": {
                "value": "- in the introduction in connection with normalizing flows I am missing stochastic normalizing flows, see\n\\\\\nH. Wu, J. K\u00f6hler, and F. Noe, Stochastic normalizing flows, in Advances in Neural Information\nProcessing Systems 33, 5933\u20135944, 2020.\n\\\\ \nP. Hagemann, J. Hertrich, G. Steidl,\n   Stochastic normalizing flows for inverse problems: a Markov Chains viewpoint\nSIAM Journal on Uncertainty Quantification 10 (3), 1162-1190,2022\n\t\n- in (3): don't call the \\textbf{value} of the integral $\\mathcal T$; later this is your OT transport map.\n- line 126: hint to Fig. 1 is too early here since $t_1,t_2$  is not explained so far\n- line 152: ,,pushes density from $f_{k-1}(x)$ to $f_{k}(x)$, since these are function values, please skip the $x$;\nthis appears also at other places (but is a minor remark)\n- line 159: I would not say that (3) is equivalent to (6)\n- in (6) and in the following: what is the pushforward of $\\tilde{f}_{k-1}$ by $\\mathcal{T}$  for an unnormalized density? Definition of KL for \nunnormalized densities ?; see Prop 1 \nThe authors should write the definitions down, then they would see that this notation does not work.\n- In general the authors switch between $f_k$ and $\\rho_k$\n\n- Prop 1 (Appendix A): Despite that the tilde notation does not work  and that $T$ is indeed $\\mathcal T$ the authors have to correct\n  line 770: first equality is wrong; you need here and in the following $\\mathbb E_{x \\sim \\mathcal T _\\#  \\tilde f_{k-1} }$ (unfortunately the system does not translates this latex formula correctly, but the authors hopefully see what I mean);\n - from 774 to 776 is wrong and becomes correct with my correction above.\n- First equality in 794 is wrong and in the final equality it should be $\\log \\rho_{k-1} (x(t_{k-1})) - \\int ...$\n\n- Prop. 2 is folklore; in formula (9) in the expectation value $x(t) \\sim ??$ is missing\n\n- Prop 3: Do not start ,,By Taylor expansion''. This is already the proof.\nWrite the correct assumptions on x and $\\tilde E$ for this Taylor expansion.\nIndeed 1. and 2. are folklore and there is nothing to prove. \nHowever, the authors  wrongly replaced $x$ by $X$ in part 2 of the appendix.\n- from formula (19) to (20) there is nothing to prove; in general the heuristic App. B appears superfluous to me,\nbut maybe it could be of interest for people not directly working in the field.\n- The results of the experiments seem to depend heavily on the symmetry of the target density, Please show a modified experiment of Fig. 6 where you shift the target density in the first dimension e.g. by 5 to the left (and keep the variance of the latent distribution).\nI guess that the reconstruction misses modes.\n- the comparison with HMC is a little unfair. Please redo the HMC experiment with a different chain for each sample starting in the same latent distribution as your model. I would expect that for your symmetric target distribution this woks fine."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of sampling from unnormalized probability distributions by introducing a novel approach called Annealed Flow (AF). The authors define a sequence of annealed distributions and propose learning continuous normalizing flows between each distribution in the sequence. Furthermore, the paper presents an additional method, termed Importance Flow (IF), which allows for computing expectations by first sampling the optimal importance distribution using AF and then learning the associated importance ratio. Experimental results validate the effectiveness of AF and IF across a diverse set of target distributions."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The importance flow technique is original"
            },
            "weaknesses": {
                "value": "- The paper is poorly written (and follows unusual notations for instance with the normalizing constants)\n- The paper suffers from theoretical issues/typos (KL between unnormalised distributions, proof of proposition 1, ...)\n- The authors don't mention or compare to direct competitors [1, 2] who solve the same problem with the same tools\n- Poor experimental part with very few comments"
            },
            "questions": {
                "value": "- Could you compare your work against [1] and [2] which have solved the same problem with very similar tools ?\n- Could you rewrite sections 2 and 3 but defining the normalizing constants from the beginning ? I think it will make the paper much clearer. Moreover, it would avoid writing KL divergences between unnormalised distributions as done extensively in section 3 and appendix 1.\n- I suspects mistakes have been made in the proof of proposition 1. Could you explain the following points ?\n1. In common litterature, the Kullback-Leiber divergence is defined between normalized distributions as $D(p || q) = \\mathbb{E}_{X \\sim p}[\\log p(X) / q(X)]$. However, L760 shows a completely different definition (different order and unormalized densities);\n2. According to the beginning of section 3.2, the quantity $\\rho_k$ is the density of the flow following $v_k$ which means that $\\log \\rho_k(x(t_{k-1}))$ is not independent of $v_k(x(s),s)$  which would contradict L800.\n- The block-wise training procedure (Sec. 4.1) uses AF samples from step $k-1$ to train for step $k$. However, those samples are approximate and have no reasons to be calibrated according to $f_{k-1}$. Can we trust those samples to compute expectations with respect the $f_{k-1}$ ? (Note that the same remark goes for the Importance Flow procedure)\n- At L260, you claim that your training procedure only requires a single neural network. Doesn't your sampling procedure require to keep all the intermediates $\\{v_k\\}_{k = 1}^K$ ?\n- Could you perform the experiment on Fig. 1 with differently weighted modes ? I feel like recovering the weights of the modes is the true challenge.\n- Could you provide numerical comparisons with recent VI approaches [3,4,5] and with similar approaches [1,2] ? Those methods are important competitors.\n- Could you compute the metrics given in [6] which seem computable in your case ? Those metrics really focus on multimodal distributions.\n- Could you provide metrics's mean and standard deviation (as well as the number of samples used) for each experiment ?\n- Could you display true samples from the Funnel distribution alongside yours ? It would clarify the pros and cons of each algorithm.\n\n\n[1] Tian, Y., Panda, N., & Lin, Y. (2024). Liouville Flow Importance Sampler. In Proceedings of the 41st International Conference on Machine Learning (pp. 48186\u201348210). PMLR.\n\n[2] Fan, M., Zhou, R., Tian, C., & Qian, X. (2024). Path-Guided Particle-based Sampling. In Proceedings of the 41st International Conference on Machine Learning (pp. 12916\u201312934). PMLR.\n\n\n[3] Francisco Vargas, Will Sussman Grathwohl, & Arnaud Doucet (2023). Denoising Diffusion Samplers. In The Eleventh International Conference on Learning Representations .\n\n[4] Qinsheng Zhang, & Yongxin Chen (2022). Path Integral Sampler: A Stochastic Control Approach For Sampling. In International Conference on Learning Representations.\n\n[5] Richter, L., & Berner, J. (2024). Improved sampling via learned diffusions. In International Conference on Learning Representations.\n\n[6] Blessing, D., Jia, X., Esslinger, J., Vargas, F., & Neumann, G. (2024). Beyond ELBOs: A Large-Scale Evaluation of Variational Methods for Sampling. In Proceedings of the 41st International Conference on Machine Learning (pp. 4205\u20134229). PMLR."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}