{
    "id": "KUf2iyin77",
    "title": "Q-Adapt: Adapting LMM for  Visual Quality Perceiver with Progressive Instruction Tuning",
    "abstract": "The rapid advancement of Large Multi-modal Foundation Models (LMM) has paved the way for the possible Explainable Image Quality Assessment (EIQA) with instruction tuning from two perspectives: overall quality explanation, and attribute-wise perception answering. However, existing works usually overlooked the conflicts between these two types of perception explanations during joint instruction tuning, leading to insufficient perception understanding. To mitigate this, we propose a new paradigm for perception-oriented instruction tuning, i.e., Q-Adapt, which aims to eliminate the conflicts and achieve the synergy between these two EIQA tasks when adapting LMM, resulting in enhanced multi-faceted explanations of IQA. Particularly, we propose a progressive instruction tuning strategy by dividing the adaption process of LMM for EIQA into two stages, where the first stage empowers the LMM with universal perception knowledge tailored for two tasks using an efficient transfer learning strategy, i.e., LoRA, and the second stage introduces the instruction-adaptive visual prompt tuning to dynamically adapt visual features for the different instructions from two tasks. In this way, our proposed Q-Adapt can achieve a lightweight visual quality perceiver, demonstrating comparable performance and, in some instances, superior results across perceptual-related benchmarks and commonly-used IQA databases.",
    "keywords": [
        "Explainable Image Quality Assessment",
        "LMM"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=KUf2iyin77",
    "pdf_link": "https://openreview.net/pdf?id=KUf2iyin77",
    "comments": [
        {
            "summary": {
                "value": "This work proposed a paradigm for perception-oriented instruction tuning named Q-Adapt, aiming to alleviate the conflict between overall quality explanation task and attribute-wise perception answering task. The authors designed a two-stage training strategy. In stage one, a LMM is finetuned to obtain a powerful base for two tasks; in stage two, some parameters in the LMM are frozen while the other parameters are finetuned to drive LMM more focus on the task instruction. In the second stage, a V-T generator and a T-V prompter are designed to achieve the bi-directional multimodal interactions."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "In terms of originality, this paper proposes a new method; In terms of quality, based on the experimental results provided by authors, the proposed method can only improve the performance in a small number of experiments, thus its superiority is worth reconsidering. In terms of clarity, the authors describe the implementation details of the proposed method in detail, but the description of the experimental settings is not clear enough, which is easy to mislead the reader. In terms of importance, if the proposed method could actually improve the performance of LMM on two tasks, it would be a promising method which worthes further researching."
            },
            "weaknesses": {
                "value": "1.\tThe symbols in some equations are not explained clearly, such as n and m near line 196 representing the dimension of feature space.\n2.\tThe legend is completely consistent with the abscissa label in the Figure 3, so the authors are advised to remove the legend.\n3.\tIt is suggested that the authors add an experiment in Sec 3.2, using the same method as in Figure 3 to perform finetune on another task, attribute-wise perception answering task, to prove that joint finetune will affect the performance which is mentioned in the conclusion 1 given by the authors in Sec 3.2. At present, it can only be proved that adding perception negatively affects the overall quality task.\n4.\tThe left values are exactly the same for Equ.3 and Equ.7. Although the author declares \"update\" above Equ.7, it is also recommended to use different symbols for the left values of Equ.3 and Equ.7 to avoid confusion.\n5.\tIt seems that the authors forgot to mark the better results as bold in the top right grid of Table 6.\n6.\tThe sentence starting with \u201cWe analyze the effect of progressive...\u201d at line 468 does not seem to belong to the previous section titled \u201cMultimodal Interaction\u201d; it appears that a title is missing here.\n7.\tAuthors are suggested to add experiments similar to Figure 3 in the experiment section, using Quality, Perception, Joint and proposed methods to finetune the model separately, and test it on two EIQA tasks, in order to verify that the proposed method can \"improve synergy between the two EIQA tasks and enhances the multifaceted explanations of IQA\"."
            },
            "questions": {
                "value": "1.\tWhy does the author only analyze the experimental results of Q-bench-A1 in Table 1 and not those of Q-bench2-A1?\n2.\tWhat does a score of 0, 1, and 2 mean in Table 2, and how is the GPT score calculated?\n3.\tIn Table 1 and Table 3, the proposed method is even worse than vanilla LMM in many experiments. Can the author explain the reasons for this phenomenon?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes to adapt MLLM for visual quality assessment (as textural output) with a two-stage instruction tuning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The use of MLLM for visual quality assessment is a rising research direction that is worth deep investigation.\n2. The related work section, for example, the categorization of MLLM-based visual quality assessment, is very well written.\n3. The experimental results look good."
            },
            "weaknesses": {
                "value": "1. The term \"progressive\" is a little misleading; it is simply a two-stage training method.\n\n2. The motivation for this work is unclear. The authors claim that the two examined tasks are conflicting, which is not well justified. From the reviewer's perspective, the two tasks could complement each other: a holistic understanding of visual quality aspects of the image can enhance more detailed quality assessment tasks involving local image analysis, and vice versa. This indeed motivates the authors to propose the two-stage training method.\n\n3. The use of LoRA in the first stage of training is not clearly explained. Why is parameter-efficient fine-tuning (such as LoRA) necessary? Which subset of parameters is subject to LoRA fine-tuning, and how are these parameters identified?\n\n4. The information flow in stage two (Figure 2) appears somewhat redundant. For example, both raw visual features and processed visual features (by the V-T generator) are sent to the T-V prompter, and then combined with the raw features to feed into the language model. Is such a complex design necessary? Further justification would be appreciated.\n\n5. The generated quality-relevant textual descriptions should be evaluated for both correctness and diversity. Template-like textual outputs are unlikely to be perceived as explainable.\n\n6. The experimental setups need clearer descriptions. For instance, how are the competing methods implemented?\n\n7. How were the visualizations in Figure 4 generated, and how should these results be interpreted and compared?"
            },
            "questions": {
                "value": "The authors should work on Points 2, 3, 4, and 5 to raise the reviewer's rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Q-Adapt, a framework designed to adapt Large Multi-modal Models (LMMs) for Explainable Image Quality Assessment (EIQA). It tackles the challenge of handling two conflicting EIQA tasks: overall quality explanation and attribute-wise perception answering. To resolve these conflicts, the authors propose a progressive instruction tuning approach that consists of two stages. In the first stage, universal perception knowledge is acquired using a parameter-efficient method like LoRA to fine-tune the model for both tasks. In the second stage, instruction-adaptive visual prompting is introduced to allow the model to dynamically adapt to task-specific instructions, improving the synergy between vision and language features. The experimental results show that Q-Adapt significantly improves performance across benchmarks, efficiently balancing both EIQA tasks and leading to better visual perception and reasoning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "[1] The paper provides extensive experiments, which convincingly demonstrate the effectiveness of the proposed method.\n\n[2] The approach achieves performance on par with 8B models while only using a 3B model, which is highly encouraging.\n\n[3] The method is simple and straightforward, making it easy to implement and understand.\n\n[4] The method offers valuable insight into the task conflicts within EIQA, shedding light on how to address these challenges effectively."
            },
            "weaknesses": {
                "value": "[1]The motivation section needs improvement. In Line 51, the conflict between the two EIQA tasks is mentioned without explaining why this conflict arises or giving examples. This should be clarified in the introduction rather than in Line 220.\n\n[2]It\u2019s unclear what the proposed method gains from the perception answering task in the second phase. The paper stresses the importance of the explanation task and highlights the negative impact of the perception answering task during joint training. If this is the case, why include the perception answering task at all? The authors should provide a detailed analysis of the knowledge required for both tasks, as well as how they benefit from and conflict with each other.\n\n[3]The proposed V-T Generator and T-V Prompter are central to the paper, but their approach seems similar to [1]. While this might be new for the IQA field, the contribution to the broader CV and MLLM communities appears limited.\n\n[4]The results in Figure 3 are confusing. The caption refers to the quality explanation task, but Line 216 and the y-axis suggest these are results for the attribute-wise perception answering task. This needs clarification.\n\n[5]In the experiments presented in Figure 3, was the data volume kept consistent across the two tuning settings? Assuming that Figure 3 indeed represents results for the attribute-wise perception answering task, why does the model tuned specifically for this task perform the worst? Could this be attributed to differences in data volume?\n\n[6]The reviewer suggests merging the paragraph starting at Line 244 in Section 3.3 with Section 3.4. The current organization may mislead readers into thinking that the connector is responsible for adaptively selecting the required perceptual knowledge based on task instructions\n\n[7]How significant is the impact of using MLP versus Q-Former as the connector on the final results? \n\n[8]Table 7 lacks an ablation study that focuses solely on fine-tuning with the quality explanation task in the second stage. This would help evaluate whether including the attribute-wise perception answering task is beneficial to model training.\n\n[9]Based on the results reported in Table 2, the sum score for Qwen-VL-Max should be 5.18, not the current value. The reviewer recommends carefully verifying the reported data throughout the paper.\n\n[10]Including more comparisons with other MLLM performance benchmarks would further strengthen the paper. For instance, comparisons with models such as LLaVA-OneVision [2], LLaVA-NeXT-Interleave-7B [3], and Qwen2-vl [4] would be highly beneficial.\n\n[11]Minor Weaknesses:\n- The use of \"(i)\" and \"(ii)\" in the second paragraph of the Introduction appears multiple times, which disrupts the flow and readability. The reviewer suggests varying the symbols when listing multiple points.\n- There is a typo in Table 6, second row: \"$Q$-$Former^{Co}$\" should be \"$Q$-$Former^Q$.\"\n\n[1] Instruction Tuning-free Visual Token Complement for Multimodal LLMs.\n\n[2] Llava-OneVision: Easy Visual Task Transfer.\n\n[3] Llava-Next-Interleave: Tackling Multi-Image, Video, and 3D in Large Multimodal Models.\n\n[4] Qwen2-VL: Enhancing Vision-Language Model\u2019s Perception of the World at Any Resolution."
            },
            "questions": {
                "value": "[1] What are the computational costs of using the V-T generator and T-V prompter modules, respectively?\n\n[2] Are there some examples of failed feature visualizations?\n\n[3] The overall quality of IQA images is related to both global and local information. The Visual Prompt (Task 2) in Figure 4 seems to rely too much on global information, which seems unusual. Could you explain why this might be the case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This article proposes progressive instruction tuning to address the Explainable Image Quality Assessment problem. This method can solve the conflict problem when two types of datasets are jointly trained and achieve better multi-task training. The article also proposes bidirectional interaction between instruction and visual feature to further improve model performance. The proposed method has achieved excellent performance on multiple EIQA tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This article is well written.\n2. The proposed method performs well on some datasets."
            },
            "weaknesses": {
                "value": "1. In Table 1, Q-Adapt-3B is worse than Co-Instruct-8B with Co-Instruct dataset for training. Can you explain this phenomenon? Considering that Co-Instruct have more data than Q-Instruct. Does this mean that your method will fail (i.e., worse than Co-Instruct-8B) when the amount of data is large enough? If it is due to the amount of model parameters, is it possible to conduct a comparative experiment using the same LLM as Co-Instruct-8B?\n2. Lack of ablations. From Table 7, i notice that joint training (69.73) is better that Perception training (68.89) but worse than Quality training (73.41) in Stage 1. However, there is a lack of only quality training experiments in Stage 2. So, i wonder if joint training is the best method in stage 2? What about joint training v.s. quality training in stage 2?"
            },
            "questions": {
                "value": "Please comment the two points in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}