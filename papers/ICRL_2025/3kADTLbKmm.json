{
    "id": "3kADTLbKmm",
    "title": "SparseDM: Toward Sparse Efficient Diffusion Models",
    "abstract": "Diffusion models have been extensively used in data generation tasks and are recognized as one of the best generative models. However, their time-consuming deployment, long inference time, and requirements on large memory limit their application. In this paper, we propose a method based on the improved Straight-Through Estimator to improve the deployment efficiency of diffusion models. Specifically, we add sparse masks to the Convolution and Linear layers in a pre-trained diffusion model, then transfer learn the sparse model during the fine-tuning stage and turn on the sparse masks during inference. Experimental results on a Transformer and UNet-based diffusion models demonstrate that our method reduces MACs by 50% while increasing FID by only 0.44 on average.  Sparse models are accelerated by approximately 1.2x on the GPU. Under other MACs conditions, the FID is also lower than 1 compared to other methods.",
    "keywords": [
        "Diffusion models",
        "sparse pruning",
        "2:4 sparsity"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=3kADTLbKmm",
    "pdf_link": "https://openreview.net/pdf?id=3kADTLbKmm",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a pruning strategy for Diffusion models, using mask pruning to achieve progressive multi-step pruning. Ultimately, it realizes 1:2 pruning according to the Ampere architecture. During training, knowledge distillation is used to transfer knowledge from the full model to the pruned model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The writing is very clear, and the main idea is highlighted effectively."
            },
            "weaknesses": {
                "value": "1. The pruning strategy is based on existing structures, with a relatively simple motivation. There are already other methods that achieve similar results, such as using linear attention or directly training a smaller model with distillation. \n2. Compared to directly using STE-based pruning, it does not further reduce the computational load.\n3. In Section 3.2, \"Transfer learn sparse diffusion models\" strategy is mentioned, but it does not explain the significant differences between this strategy and the progressive sparse training strategy discussed in Section 2.2. If the focus is solely on testing with perturbed datasets, it may not constitute a significant contribution.\n4. A generalized pruning strategy suitable for Transformer networks has not been proposed; simply relying on data perturbations is insufficient to demonstrate applicability to other datasets. Further testing on additional datasets, such as CelebA-HQ, LSUN Church, would be beneficial.\n5. Many of the latest comparative algorithms from 2024 are not mentioned, such as \"Pruning for Robust Concept Erasing in Diffusion Models\" and \"LD-Pruner: Efficient Pruning of Latent Diffusion Models using Task-Agnostic Insights.\"\n6. There is no comparison of the parameter counts for each layer of the SD model before and after sparse pruning. It is recommended to include a chart in the appendix to illustrate this.\n7. While Section 2.3 mentions applying perturbations to the dataset, it does not provide specific details on how the perturbations were implemented.\n8. The experiments only validate the FID score as a single metric; it is advisable to explore additional metrics, such as SSIM."
            },
            "questions": {
                "value": "Could the authors provide the parameter counts for each layer of the SD model before and after sparse pruning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to improve the efficiency of DM by sparse matrix for 2:4 sparse acceleration GPU. The authors improve the STE method and propose to gradually transfer knowledge from dense models to sparse models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThis paper is well-written.\n2.\tThe motivation is clear enough.\n3.\tThe organization of this paper is great."
            },
            "weaknesses": {
                "value": "1.\tThere is a typo in Eq5. Please also check all equations. Moreover, not all symbols have been explained.\n2.\tThe experiments are relatively limited. Specifically, only two U-ViT and DDPM are tested on the proposed pruning, which are proposed in 2022 and 2020 respectively. More recently proposed DiT or other methods should also be included.\n3.\tThe limitation and discussion are missing in this paper."
            },
            "questions": {
                "value": "1.\tThe authors mentioned that \u201cit does not mean that the greater the sparsity, the better the FID\u201d. Please discuss the reason and why you choose 2:4 sparse.\n2.\tPlease discuss the reason ASP performs so worse in all experiments.\n3.\tPlease also clarify why your method and STE-based pruning fulfill the same MACs.\n4.\tPlease explain the reason that the FID of the proposed method in Fig. 3a obtain a lower FID in the first several steps.\n5.\tWhy the initial FID of 2:4 sparse in Fig.3b and Fig.3d is different?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work aims to reduce the computation of Diffusion Models during inference. The authors suggest a method of straight-through estimation, which applies sparse masks to layers of a pretrained diffusion model and then employs transfer learning for training. Then, they use the same sparse mask during inference to improve compute efficiency."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The 2:4 sparse model calculation offers practical values for practitioners using NVIDIA Ampere architecture GPUs."
            },
            "weaknesses": {
                "value": "- While it may have some practical value for practitioners using NVIDIA Ampere architecture, the same technique may not benefit other practitioners or general researchers without access to Ampere architecture. \n\n- Besides, the straightforward idea of using masked training is neither interesting nor technically new. \n\n- More disappointingly, the speed acceleration due to this customized training for a particular architecture increases by x1.2 only. Studies related to reducing time steps for Diffusion inference or diffusion quantization/pruning methods may be more effective in achieving the same purpose."
            },
            "questions": {
                "value": "Please address the weakness stated above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces SparseDM, which converts existing diffusion models into sparse models that fit in a 2:4 sparse operator on the GPU. Specifically, the authors propose a Straight-Through Estimator (STE)-based fine-tuning framework that learns sparse masks. These sparse masks accelerate GPU inference speed up to 1.2. Comprehensive experiments validate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The paper introduces a simple fine-tuning method that converts existing diffusion models into sparse models, enabling them to be used in scenarios with limited computing power, such as on mobile devices.\n\n* The observations about fixed sparse training are interesting.\n\n* Experiments on various generation scenarios verify the effectiveness of SparseDM compared to baselines."
            },
            "weaknesses": {
                "value": "**Weakness 1: More clarifications on Section 2.3.**\n\nIn Section 2.3, the authors claim that diffusion models only consider the distribution shift of the noisy data while sparse pruning methods only consider the model's weight change. Then, referring to RFR, the authors convert the model's weight changes resulting from sparse pruning methods into data changes for the diffusion model's training process. However, typical diffusion models have indicators for perturbed data (such as the noise schedule and timestep embedding), and it is unclear how these relate to perturbations caused by sparse training.\n\n**Weakness 2: Lack of analysis of fixed sparse training**\n\nI am not sure why fixed sparse training would be more effective than traditional progressive sparse training. Based on the experimental results, it seems that fixed sparsity applies a consistent distribution shift across all noise levels in diffusion training, whereas progressive sparse training gradually shifts the predefined noise levels, which may hinder the diffusion training process. However, this claim has not been theoretically verified, so the authors should provide theoretical proof to demonstrate the relationship between diffusion training and sparse training."
            },
            "questions": {
                "value": "* In Table 3, some variants (e.g., patch size = 2 and mlp_ratio = 2) are slower than the dense model, why do you think this is?\n* I think it would strengthen the effectiveness of SparseDM if the author show that it can also be applied to models like Stable Diffusion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}