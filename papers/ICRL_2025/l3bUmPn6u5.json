{
    "id": "l3bUmPn6u5",
    "title": "PFT: Enhancing Prompt Injection Robustness via Position-Enhanced Finetuning",
    "abstract": "Large Language Models (LLMs) are widely adopted in closed-domain applications, where differentiating between system instructions and user input is crucial to prevent unintended malicious actions. However, instruction-following LLMs often blindly follow instructions in user inputs, opening up the risk of prompt injection attacks. This paper investigates whether Supervised Fine-Tuning (SFT) can teach LLMs to strictly distinguish system instructions from user input. Our study reveals a key weakness: SFT-tuned models follow system instructions reliably only when the key instruction is placed immediately after the initial tokens. We find that the proximity of the key instruction to the initial tokens significantly influences the model's ability to execute the intended task, and consequently, its susceptibility to prompt injection attacks.To address this issue, we propose PFT, a novel position-enhanced fine-tuning approach that leverages position IDs to more effectively distinguish between system and user tokens. The experimental results demonstrate that PFT improves the robustness of SFT-tuned models against prompt injection attacks, even when the key instruction is placed arbitrarily in the system prompt, without compromising performance. Our work sheds light on the importance of prompt format in enhancing the security of LLMs and offers a practical solution to improve their robustness.",
    "keywords": [
        "Large Language Models (LLMs)",
        "Safety",
        "Instruction Following",
        "Adversarial Attacks",
        "Position Encoding"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=l3bUmPn6u5",
    "pdf_link": "https://openreview.net/pdf?id=l3bUmPn6u5",
    "comments": [
        {
            "summary": {
                "value": "By adapting the position-dependent encoding,\nit is possible to strengthen LLM's ability to\nfollow system prompts and reject prompt\ninjection style attacks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper identifies shortcomings of existing\nmodels, presents experiments to demonstrate these\nshortcomings, and then presents a simple and\nelegant solution.\n\nFrom the paper's results, it appears that the\nproposed mechanism does improve a certain form\nof robustness.\n\nI think the approach might be of interest to\nresearchers."
            },
            "weaknesses": {
                "value": "Overall, I struggled to evaluate this paper.\nThe paper has some interesting results, and I'm\nnot sure what to make of them.  So I'm not sure\nwhether to recommend acceptance or not for this\npaper.\n\nThe paper's notion of robustness is: the LLM\nshould be able to tolerate irrelevant instructions\nappearing before the key instruction, in the\nsystem prompt.  Is this important?  Do real\napplications use system prompts that contain many\nirrelevant instructions before the key instruction?\nI'm not sure.  Therefore, I'm unsure whether the\nproblem this paper tackles is important.\nI encourage the authors to provide examples\nor evidence of real-world applications where\nthis property is important.\n\nThe notion of robustness that I'm used to is a\nbit different: can the LLM resist all attacks?\nIf we pick some class of attacks, what is the\nattack success rate of the strongest attack in\nthat class?  In other words, increasing robustness\nmeans reducing the attack success rate of some\nattack -- and this is evaluated for average-case\nprompts (i.e., ones that will appear in real\napplications), rather than worst-case prompts\n(e.g., where we add extraneous instructions at\nthe start).  That's not the notion this paper\ntakes on, though.\n\nThe paper starts from a premise about how LLMs\nwill/should be used (put the instruction in system\nmessage, the data in user message), a premise that\nI am skeptical about.  Then it draws some conclusions\nabout that usage.  I'm not sure whether those\nconclusions generalize to ways of using LLMs that\nI think are more appropriate and more common.\nAlso, I'm not sure whether the paper's results\ngeneralize to multiple models (different LLMs).\n\nI also struggle to tell whether this paper's\nresults are primarily telling us something about\nprompt injection (inserting malicious instructions\ninto a field that is only supposed to contain\ndata) or system-message-following (supplying user\nmessages that contradict/violate rules/guardrails\nestablished in the system message).  Details on\nexperiments are vague and so it's hard for me to\ntell what is being tested.\n\n\nDetailed feedback:\n\nAbstract: I had a hard time understanding the abstract.\nI don't understand what is meant by \"the key instruction\",\nor what system instructions have to do with prompt\ninjection.\n\nSec 1, paragraph 2: I don't agree with the claim here\nabout how engineers typically build systems with LLMs.\nI don't think the zero-shot prompt is typically put in\nthe system instructions.  Instead, I think the prompt\nor instruction is typically put in the user message,\nand the system message typically contains guardrails\nthat constrain what types of prompts/instructions will\nbe followed.  If you disagree, I encourage you to look\nfor quantitative evidence (perhaps surveying some\ncollection of systems built by others).\n\nSec 1: the paper seems to conflate two issues that I\nconsider separate: (a) prioritizing system instructions\nover user instructions when they conflict; (b) ignoring\nall instructions in the data part of the user message.\nI consider prompt injection to be problem (b), and\nproblem (a) to be a separate problem.  The introduction\ndoes not distinguish between these two, and that makes\nit harder for me to understand what problem the paper\nis and isn't solving.\n\nOr, to put it another way, we can distinguish between\nsystem instructions, user instructions, and user data.\nThe paper does not seem to clearly distinguish between\nthese.  It seems to assume the system message contains\ninstructions and the user message contains data.  In\nmy experience, that is not representative of how LLMs\nare used in real systems and not representative of how\nproduction LLMs are trained in practice.  My experience\nis that the system message (if present) contains\ninstructions (such as guardrails or restrictions or a\ndefinition of the domain/scope), and the user message\noften contains a combination of both instructions and\ndata.\n\nTable 1: It's not clear to me what this is showing.\nWhat does a number like 10% mean?  What is \"Gandalf\nSummarization\"?  I think the table caption should\nprovide a self-contained explanation, or the table\nshould be moved to later in the paper after all key\nconcepts have been defined.  I think the table\ncaption should specify what the number/percentages\nmean (what are they measuring?  attack success rate?).\n\nSec 2.1: So many details are missing.  What base\nmodel do you use?  How large is your dataset?  What\nare the attacks you evaluate against?\n\nI am quite skeptical of the claim that the model\nis secure.  I don't believe you've evaluated against\nenough attacks to draw such a conclusion.  There\nare some quite strong attacks, such as GCG and TAP\n(modified to create prompt injection attacks), which\nare not considered here.  Therefore, it is not\nwarranted to conclude that the model is secure, as\nyou don't know whether it will be secure against\nthese more sophisticated attacks.\n\nSec 2.2: Does not match my experience.  In my\nexperience, general instructions (\"You are an\nAI assistant\") go in system messages, the\n\"key instruction\" (e.g., \"Translate this to\nFrench\") goes in the user message, and background\nknowledge and context and few-shot examples and\nRAG-retrieved excerpts typically go in the\nuser message.\n\nMany details are missing.  What model did you\nuse?  What attack did you use?  What was the\ndataset and tasks for evaluation?  What is meant\nby an \"attack dataset\"?\n\nFig 2: How is accuracy measured?  How do you tell\nwhether the model's respons is accurate?\n\nSec 3.1: It might help if you stated what\nprompt injection attack techniques you used.\nFrom Fig. 4, it sounds like maybe you used\nthe most naive attack, just directly asking\nthe model to do something different.\n\nFig 4: I'm not convinced the model is trained\nto know what \"user's input\" means, so I'm not\nsure whether this is a fair test of LLM capabilities\nor if this is the right way to use LLMs for this\nkind of task.\n\nSec 4: Do you have any explanation whether\nit is better to have a fixed-length gap between\nsystem vs user message (i.e., user message\nstarts at token $k+1+d$) or have the user\nmessage start at a fixed position (i.e.,\nuser message starts at token $d+1$)?  Have\nyou tried both?\n\nSec 4: It seems this assumes that messages\nwill always appear in the order: system\nmessage first, then user message second.\nIn contrast, existing LLMs allow them to be\ninterspersed arbitrarily.  Does this restriction\ncause any loss in flexibility?  Does it matter?\n\nAlso, how do you propose to handle multi-turn\ninteractions?  How will they be encoded, and\nwhere will gaps appear or not appear?\n\nSec 5.1: Are you doing SFT on a model that\nhas already been instruction-tuned, or on a\nbase model that has not?  It sounds like you\nare doing SFT on a model that was already\ninstruction-tuned?\n\nSec 5.1: How do you generate the desired\nresponses to pairs where the system prompt\nand user input have been swapped?\n\nSec 5.1: Will you make your dataset and\ncode available, to support reproducibility?\n\nSec 5.2: I don't understand what the attacks\nare.  Please provide more detail.  For\nGandalf Summarization, the paper cites\nLakera AI, 2023b, but that reference is\nbogus: the URL is for the Lakera main web\nsite, which clearly does not have the claimed\ninformation, nor could I find any other\nwebpage on the Lakera web site with the\nlisted title.  The same comments apply\nto Lakera 2023a.  Please provide direct\nlinks or references for each attack, and\npreferably describe the attack in a\nself-contained way in the paper.\n\nI suspect you might not be testing against\nthe strongest prompt injection attacks,\nsuch as found in other papers on the\nsubject.  For instance, I would be more\nconvinced if you had evaluated against\ncompletion attacks, TAP attacks, and GCG\nattacks.\n\nSec 5.3: I disagree with using \"most\nrobust\" here.  I believe what you actually\nmeasure is the property \"isn't distracted\nby extraneous instructions at the start\".\nThat isn't the same thing.  What you study\nis one narrow, specific aspect of robustness.\nThis is relevant if real systems use\nsystem prompts that start with a lot of\ngeneric, extraneous instructions.  It is\nless clear how relevant it might be if\nreal systems don't do that.  And it does\nnot necessarily imply that PFT can defend\nagainst stronger attacks.\n\nFig. 6: This is missing a key metric: you\nshould also show the accuracy and\nlog-likelihood for the undefended base model\nwith none of these defenses applied.  That's\nwhat users will really care about: does the\ndefense harm the utility of the model,\ncompared to existing models with no defense\napplied; not whether your defense is about\nas good as other plausible defenses.\n\nConsequently, I don't think the paper can\nreasonably claim that PFT doesn't hurt model\nperformance.  It might be true, but I don't\nthink the paper has evaluated that.\n\nFig. 6(b) shows that there is some deviation\nfrom the base model.  Is 0.5 KL divergence\na large deviation, or a small one?  It's hard\nto know.  Perhaps looking at a small random\nsample of responses from both the base\nmodel and PFT model would help.\n\nSec 6: The paper seems to claim that OpenAI's\ninstruction hierarchy method has fragility\nwhen the key instruction appears later in\nthe input.  But it's not clear that this\nhas actually been tested.  I would find this\nanalysis of the instruction hierarchy method\nmore compelling if the paper empirically\nmeasured this, e.g., on GPT 4o-mini.\n\nThe paper appears to claim that StruQ has\nfragility in this case as well, and that PFT\nis more robust.  However, I don't think this\nhas been demonstrated, as the paper does not\nevaluate StruQ.  StruQ uses some techniques\nthat were not tried in any of the models\nevaluated in this paper.  I think the comparison\nto StruQ would be more convincing if the paper\ncompared to a StruQ-trained model.  I don't\nthink we should view StruQ as designed just\nto defend against Completion attacks; it seems\nlike it is trying to handle all prompt\ninjection attacks, as much as possible.\n\nThe paper seems to be missing one piece\nof related work, BIPIA (Yi et al,\narXiv:2312.14197)."
            },
            "questions": {
                "value": "How does the robustness against prompt injection\nattack of your model compare to prior work, such\nas StruQ's and BIPIA's models?  How robust is\nit, when evaluating on the strongest attacks,\ne.g., Completion attacks, TAP, and GCG?\n\nHow do your results change if you put both the\ninstruction and the data in the user message?\n\nWhat is the accuracy and log-likelihood\n(Fig. 6(a)) for the undefended base model?\nHow do I interpret a KL divergence of 0.5?\n\nHow common is it for real prompts to contain\nextraneous instructions before the key instruction?\n\nHow should I interpret the paper's results\n(see discussion above)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes PFT, a novel position-enhanced fine-tuning approach that leverages position IDs to more effectively distinguish between system and user tokens. The experimental results demonstrate that PFT improves the robustness of SFT-tuned models against prompt injection attacks, even when the key instruction is placed arbitrarily in the system prompt, without compromising performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper explores the security and robustness of models under different conditions of prompt structure. It finds that SFT-tuned models, which are secure when key instructions are positioned at the beginning of the prompt, become vulnerable when these instructions are placed later. The study demonstrates that the proximity of the key instruction to the start of the input significantly impacts the model's adherence to the designated task. To address this vulnerability, the paper introduces Position-Enhanced Fine-Tuning (PFT), a method designed to protect models from adversarial inputs by ensuring robustness and maintaining performance, regardless of where instructions are positioned within the prompt."
            },
            "weaknesses": {
                "value": "1. The definition of the problem lacks clarity. Specifically, the formal definition of a \"key instruction\" in system prompts is ambiguous. How and why does it differ from other system prompts? It is challenging to distinguish key instructions from other contextual prompts,  especially since other prompts can sometimes serve as the context for the key instructions. This will lead to complex scenarios that needs a detailed discussion.\n2. Following the first point, the generalizability of PFT needs to be clarified, as key instructions vary across different contexts. The validation dataset used in the paper mirrors the examples provided in the introduction, which does not suffice to demonstrate PFT's applicability in more complex and practical scenarios.\n3. Stronger attacks are necessary. In the realm of prompt injection, several existing studies employ learning-based methods to launch attacks, as referenced in [1]. I suggest that the authors include experiments to test the resilience of PFT against these types of attacks.\n4. The paper lacks a discussion on adaptive attacks. If attackers know the PFT method and can fine-tune the model accordingly \u2013 for instance, tuning it to adhere strictly to user instructions?\n\n[1] Pasquini, Dario, Martin Strohmeier, and Carmela Troncoso. \"Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks.\" arXiv preprint arXiv:2403.03792 (2024)."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper has two main contributions: (1) They demonstrate that LLMs are more likely to follow instructions *closer* to the beginning of the input. This is quite a surprising and interesting phenomenon. (2) Based on this observation, the authors suggest a modified SFT procedure to combat prompt injection attacks. This defense works by shifting the user input by a constant offset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The observation regarding how position of an instruction affects the model\u2019s LLM to follow it is scientifically interesting. The experiments are convincing for this point. It is very interesting to see that the attack token logit shoots up with only a small number of inserted sentences and then plateau.\n2. The proposed defense is simple and seems to work at least in a limited setting."
            },
            "weaknesses": {
                "value": "1. **Choice of the model to be fine-tuned.**\n    1. Why do you fine-tune instructed models instead of the base pre-trained model? The instructed models can already solve these tasks so it is perhaps expected that the utility is not hurt via PFT. These models are also already aligned (via RLHF or safety tuning) so they should already be somewhat resilient to such attacks.\n    2. Or, the authors intend to propose PFT as an additional step after RLHF? However, I believe that \n    the authors are suggesting to replace SFT with PFT, and if so, the base model for this experiment should be the base Llama-3-8B, not instruct.\n2. **Weak defense baselines.**\n    1. I believe that this experiment is missing two important baselines: Instruction Hierarchy and StruQ. My understanding is that the authors intend to use \u201cDelimiter-enhanced SFT\u201d to represent StruQ and \u201cData-augmented SFT\u201d to Instruction Hierarchy. However, I would suggest reproducing these baselines exactly or as close as possible and compare to them in the same setup (same dataset). Without this direct comparison, it is impossible to conclude whether PFT is better.\n    2. Data-augmented SFT: What happens if the augmentation is done to the user prompt instead of the \n    system prompt? This would be more like StruQ.\n    3. Data augmentation is used in both Instruction Hierarchy and StruQ, and it is different from what\u2019s done in this paper. What about training against a subset of the attack? If we do that and combine with PFT, is there any improvement?\n3. **Weak attack baselines.** The authors should consider evaluating against stronger attacks, e.g., a set of handcrafted attacks from StruQ, jailbreaks, or even automated attacks such as [PAL](https://arxiv.org/abs/2310.08419), [TAP](https://arxiv.org/abs/2312.02119), [AutoDAN](https://arxiv.org/abs/2310.04451), or [GCG](https://arxiv.org/pdf/2307.15043). This would help with comparison to the prior works.\n4. **KL divergence metric.** It is mentioned that the KL divergence is computed between p_model(output text|prompt) of the model before and after fine-tuning. As far as I know, there is no way to efficiently compute this because the set of \u201coutput texts\u201d is just too large. My guess is that the authors compute the KL divergence at *each* token conditioned on the prior tokens, which is a different quantity from what is stated.\n5. **Not applicable to real-world / more diverse use cases.** This defense does not hurt the utility because all of the Alpaca samples put the instruction close to the beginning of the input. This defense would fail immediately when legitimate instructions are part of the user input and placed anywhere, e.g., in chatbots.\n\n### Nitpicky Stuff\n\n- In Table 1, calling the model before fine-tuning a \u201cbase\u201d model is slightly confusing. In my understanding, this is an aligned and instructed model, but \"base model\" often refers to a pre-trained model on the next-token prediction task. I might suggest simply calling it \"before fine-tuned,\" or at least make it very in the text or caption (not delay until Section 5).\n- L193 (\u201dthis is not a strong enough signal for LLMs\u2026\u201d): I believe that this statement is plausible but still not very convincing. Could you explain more why the delimiters would be a weaker signal than the begin-of-text token? Is it because there is only one begin-of-text token in every sample, and it is always at the beginning? -- I see that this is discussed later on in Section 3.2. It might be a good idea to refer to it here.\n- L211 (\u201dAlso the first insertions have a dramatic effect\u2026\u201d): I can understand the message from looking at Figure 3, but this statement is hard to follow and is pretty confusing to me.\n- L285 (\u201dassigning a unique signature to each token based on its role (system or user), we create a continuous, fine-grained distinction throughout the entire input.\u201d): I believe that this sentence is not an accurate description of PFT. Each token is pushed back by the same offset $d$ which does not sound like a unique signature to me. I\u2019m also not sure what \u201ccontinuous, fine-grained distinction\u201d refers to."
            },
            "questions": {
                "value": "1. Table 1: I'm also quite surprised that the attacks are very effective on these \"base\" safety-aligned models. Is there any detail I can see regarding the implementation? For example, what is the system prompt for these models? Did you place the main instruction from these datasets in the system prompt or the first user message? Llama-3-Instruct has no system prompt by default and likely expect instruction to be in the first user message. Violating this format might hurt the model's performance.\n2. Which model is used for this experiment? Is it the same model from Section 2.1, or a different public model? I think this information bit is rather important, i.e., the result is very much expected if it is the model from Section 2.1.\n3. L358 (\u201dWe ask GPT-4 to filter out cases where the model still misinterprets the user input as a separate task.\u201d): Could you explain more why this step is necessary?\n4. Figure 6(a): Is the log-likelihood computed on generations or on the correct answer? The paper says \n\u201dgenerations\u201d but this does not make sense (e.g., confident wrong answer)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the vulnerability of Large Language Models (LLMs) in closed-domain applications, where distinguishing system instructions from user input is essential to prevent prompt injection attacks. The study reveals a limitation in Supervised Fine-Tuning (SFT), where models reliably follow system instructions only when these instructions appear immediately after the initial tokens. To address this, the authors introduce Position-Enhanced Fine-Tuning (PFT), which uses position IDs to better differentiate between system and user tokens."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The ideas are clearly presented, with contributions precisely defined.\n\n2. Key terms and concepts are thoroughly defined, such as key instructions.\n\n3. The writing is fluent, well-organized, and easy to read, with appropriate examples that enhance readability and understanding."
            },
            "weaknesses": {
                "value": "1. The contribution appears limited to \"pure\" closed-domain tasks, where users are restricted to providing only input data without additional task-specific instructions. In real-world scenarios (e.g., translation tools or platforms like ChatGPT and Copilot), users often add specific instructions, such as style preferences or formatting guidelines, which PFT does not address. Figure 2 suggests that PFT might not perform well in these more dynamic contexts.\n\n2. The paper states that PFT imposes no performance penalty (Section 5.3), yet assumes that users will not add prompts, focusing solely on task input data. It would be helpful to analyze the trade-off between robustness and performance in a practical context where users provide detailed instructions. Examining the parameter \\( d \\) and its relationship with performance in such scenarios would clarify PFT\u2019s effectiveness.\n\n3. Including a comparison between PFT and a simple baseline prompt designed to direct LLMs toward system tasks would add value. Observers might find a \"SOTA vs. SOTA+PFT\" comparison more informative than \"vanilla vs. vanilla+PFT.\"\n\n4. In the robustness experiments (Figure 5), only data points with fewer than ten inserted sentences (\\#ofSentences < 10) seem relevant for real-world usage. It would be helpful to highlight these cases more explicitly.\n\n5. No code is provided, which limits the reproducibility of the findings.\n\n6. Table 1 is difficult to interpret due to the lack of a legend or clear explanation of its components. Adding this information would improve clarity."
            },
            "questions": {
                "value": "1. In the performance experiments shown in Figure 3, was each model fine-tuned with its respective number of input sentences?\n\n2. After the insertion process, do the system prompts remain fluent at the sentence level?\n\n3. Including \"prefixes\" or \"suffixes\" in instructions for LLMs typically does not impact performance significantly. However, as shown in Figures 2 and 3, your experiments reveal a substantial drop in accuracy with just five sentences, causing a 50-80% decrease. Could you clarify the factors behind this observation?\n\n4. Can I infer that \"if the system prompt is sufficiently long, the model will remain robust and perform well even without PFT\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}