{
    "id": "TCgcEQjaUQ",
    "title": "Scalable Message Passing Neural Networks: No Need for Attention in Large Graph Representation Learning",
    "abstract": "We propose Scalable Message Passing Neural Networks (SMPNNs) and demonstrate that, by integrating standard convolutional message passing into a Pre-Layer Normalization Transformer-style block instead of attention, we can produce high-performing deep message-passing-based Graph Neural Networks (GNNs). This modification yields state-of-the-art results in large graph transductive learning, outperforming the best Graph Transformers in the literature without requiring the otherwise computationally and memory-expensive attention. Our architecture not only scales to large graphs but also makes it possible to construct deep message-passing networks, unlike simple GNNs, which have traditionally been constrained to shallow architectures due to oversmoothing. Moreover, we provide a new theoretical analysis of oversmoothing based on universal approximation which we use to motivate SMPNNs.",
    "keywords": [
        "Message Passing Neural Networks",
        "Graph Representation Learning",
        "Large Graphs"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TCgcEQjaUQ",
    "pdf_link": "https://openreview.net/pdf?id=TCgcEQjaUQ",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes to substitute the multi-head attention in graph transformers with pure message passing. The authors provide theoretical analysis to prove that combined with residual connection, graph transformers with pure message passing can achieve universal approximation and alleviate over-smoothing."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The proposed method is simple.\n\n- The paper focuses on over-smoothing, an important problem in graph learning.\n\n- The paper provides both empirical and theoretical analyses."
            },
            "weaknesses": {
                "value": "- The contribution of this paper is weak. The main focus is replacing the attention module in the transformer with a message-passing module and using the residual connections to alleviate the over-smoothing problem. However, the use of residual connections to address over-smoothing has already been explored in DeepGCN[1], which this paper does not mention or compare. Additionally, the implementation of deep GNNs has been studied in several other works[1]-[4].\n\n- The paper contains a substantial amount of repetitive descriptions of existing work, such as message-passing neural networks and GCN (Eq. 2/3 vs. Eq. 6). Additionally, the theorem introduced in Section 4.1 is neither utilized later in the paper nor a contribution of this work.\n\n- The contributions of the paper also involve repetitive work, such as a) using residual connections to mitigate over-smoothing; b) based on reference [5], Theorem 4.4 is evidently valid.\n\n- Empirical analysis lacks heterophilic benchmarks to validate the effectiveness of SMPNN on over-smoothing.\n\n- In tab. 7, SMPNN achieves the best performance on ogbn-proteins with 12 layers. Does SMPNN require significantly more computational resources and parameters to achieve comparable or better performance to baseline models?\n\n\n\n[1] DeepGCNs: Can GCNs Go as Deep as CNNs? ICCV'19\n\n[2] Graph Convolutional Networks via Initial residual and Identity Mapping, ICML'20\n\n[3] Training Graph Neural Networks with 1000 Layers, ICML'21\n\n[4] Revisiting Heterophily For Graph Neural Networks, NeurIPS'22\n\n[5] How Powerful are Graph Neural Networks? ICLR'19"
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes Scalable Message Passing Neural Networks (SMPNNs), a framework designed to scale traditional message-passing GNNs. By incorporating residual connections, it avoids the issue of oversmoothing. The method claims to work without the need for computationally and memory-intensive attention mechanisms."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-written with a clear motivation. The methodology is easy to follow, and the experiment section is well-structured.The paper is well-written with a clear motivation. The methodology is easy to follow, and the experiment section is well-structured."
            },
            "weaknesses": {
                "value": "The experiments are not strong enough. For instance, all SMPNN variants should be included consistently across tables and figures in the section. The same applies to baselines, unless there are reasonable explanations for exclusions. Additionally, the distinctions among SMPNN variants and the strengths of SMPNN are not clear. For example, SMPNN uses significantly more GPU memory than SGFormer, yet the paper still claims that it does not use more memory than the baselines. Some presentation needs to modified, for example: FF notation needs to be written consistently with the rest of the paper."
            },
            "questions": {
                "value": "- See weaknesses.\n- If SMPNN w/o FeedForward uses significantly less GPU memory and performs better in accuracy than SGFormer, why is it not considered the main variant?  \n- The difference between Figure 2 and Figure 3 is unclear.  \n- For the SMPNN variant with attention (detailed in the Appendix), how does it differ from GAT (Veli\u010dkovi\u0107 et al., 2017)?\n- A real-time per-epoch runtime analysis should be included alongside the big-O analysis in the paper.\n- Could you clarify why SMPNN, which uses message passing, primarily compares against an attention-based baseline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Scalable Message Passing Neural Networks (SMPNNs), a deep, scalable GNN framework that omits attention in favor of local message-passing, achieving efficient performance on large graphs. It claims to outperform transformer-based models while avoiding issues like over-smoothing."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper provides solid theoretical support, particularly on residual connections and universal approximation, which strengthens the SMPNN design and its claims.\n\n2. By replacing attention with scalable message-passing, SMPNN achieves efficient performance and good experiment results on large graphs, offering a notable advancement for scalable GNN applications."
            },
            "weaknesses": {
                "value": "1. In Section 3.2, the authors label their proposed block as a \"transformer block.\" However, the SMPNN framework lacks any attention mechanism, which is a core component of transformers. Consequently, SMPNN functions more like a deep GCN with residual connections rather than a genuine transformer model. This categorization is misleading, as attention mechanisms fundamentally distinguish transformers by enhancing scalability and representation capacity in large graph models. Existing models such as GCNII [1], EGNN [2], and DeeperGCN [3] have already explored architectures with enhanced depth. Although these models improve scalability, they are still linear and inherently limited compared to transformers.\n\n2. Table 1 mentions that the SMPNN has training time complexities comparable to other transformer-like models, such as GraphGPS and Exphormer. However, the experiments do not include comparisons with these transformer-based models, especially models with higher computational complexity, like Graphormer, that might showcase different trade-offs in performance versus scalability. While SMPNN is intended to scale to larger graphs, demonstrating performance across various data scales, especially small to medium datasets, would test its broader applicability.\n\n3. SMPNN relies on local message-passing operations without incorporating global attention mechanisms, which may limit its ability to capture long-range dependencies effectively. For tasks where global context is essential, SMPNN's performance could be suboptimal compared to transformer-based models that use global attention mechanisms. A discussion on this limitation and potential strategies would strengthen the paper.\n\n[1] Chen, Ming, et al. \"Simple and deep graph convolutional networks.\" International conference on machine learning. PMLR, 2020.\n[2] Zhou, Kaixiong, et al. \"Dirichlet energy constrained learning for deep graph neural networks.\" Advances in Neural Information Processing Systems 34 (2021): 21834-21846.\n[3] Li, Guohao, et al. \"Deepergcn: All you need to train deeper gcns.\" arXiv preprint arXiv:2006.07739 (2020)."
            },
            "questions": {
                "value": "same to weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose a new Scalable Graph Convolution Network. The author develops a scalable message passing block which involves a residual connection that connects two parts. The first part is a GCN block, and the second part is a point-wise feed forward layer as in transformer. This architecture retains computation efficiency. Meanwhile, the author illustrates how their method solves the over smoothing problem. The original graph convolution is not a universal approximator. However, with a residual connection, the graph convolution can turn to a universal approximator. And the authors provide extensive experiment results to prove the efficiency of their method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Pros: \n\n1: The motivation of this paper is clear. The author adapts the transformer architecture to address the scalability issue in Graph Neural Networks. \n\n2: The author presents both theoretical analysis and experimental results for their methods, offering a comprehensive approach. \n\n3: The authors provide extensive experiments on different dataset and relevant ablation study to prove the effectiveness of their method."
            },
            "weaknesses": {
                "value": "Cons: \n\n1: The SMPNN can maintain its performance but does not gain any advantages from a deeper network. What causes this? \n\n2: For larger graph datasets, the improvement from SMPNN is less pronounced. For example, on the ogbn-papers-100M dataset, the improvement is only 0.2%. Could this suggest that the model size is still inadequate? If we use a larger network for both SMPNN and Graph Transformer, SMPNN should experience less oversmoothing and demonstrate a more significant accuracy improvement."
            },
            "questions": {
                "value": "Please see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}