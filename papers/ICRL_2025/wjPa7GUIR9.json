{
    "id": "wjPa7GUIR9",
    "title": "Fragile Giants: Understanding Susceptibility of Models to Subpopulation Attacks",
    "abstract": "As machine learning models become increasingly complex, concerns about their robustness and trustworthiness have become more pressing. A critical vulnerability to these models is data poisoning attacks, where adversaries deliberately alter training data to degrade model performance. One particularly stealthy form of these attacks is subpopulation poisoning, which targets distinct subgroups within a dataset while leaving overall performance largely intact. The ability of these attacks to generalize within subpopulations poses a significant risk in real-world settings, as they can be exploited to harm marginalized or underrepresented groups within the dataset. In this work, we investigate how model complexity influences susceptibility to subpopulation poisoning attacks. We introduce a theoretical framework that explains how overparameterized models, due to their large capacity, can inadvertently memorize and misclassify targeted subpopulations. To validate our theory, we conduct extensive experiments on large-scale image and text datasets using popular model architectures. Our results show a clear trend: models with more parameters are significantly more vulnerable to subpopulation poisoning. Moreover, we find that attacks on smaller, human-interpretable subgroups often go undetected by these models. These results highlight the need for developing defenses that specifically address subpopulation vulnerabilities.",
    "keywords": [
        "poisoning",
        "robustness",
        "subpopulations"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We investigate the connection between model complexity and vulnerability to subpopulation-targeted poisoning attacks for real-world overparameterized models and datasets",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wjPa7GUIR9",
    "pdf_link": "https://openreview.net/pdf?id=wjPa7GUIR9",
    "comments": [
        {
            "summary": {
                "value": "This study highlights that overparameterized models are particularly vulnerable, often failing to detect issues in smaller, interpretable subpopulations. The analysis reveals a strong relationship between model complexity and susceptibility to such attacks, exacerbated by the long-tailed nature of modern datasets. These findings stress the need for subpopulation-specific defenses, as traditional approaches may be insufficient for increasingly complex systems."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe study provides a robust theoretical framework that highlights the vulnerability of locally-dependent mixture learners to subpopulation poisoning attacks. This builds on existing knowledge of how long-tailed data distributions are memorized, offering a deeper understanding of the challenges in defending against these attacks.\n2.\tThe research empirically demonstrates that complex models exhibit significant shifts in their decision boundaries when exposed to subpopulation poisoning. This finding highlights the vulnerability associated with increased model complexity.\n3.\tThe study conducts an extensive empirical analysis of realistic, overparameterized models across diverse real-world image and text datasets. By executing 1,626 individual poisoning attacks on various combinations of dataset, subpopulation, model, and parameters, it robustly establishes that larger models are more susceptible to subpopulation poisoning attacks."
            },
            "weaknesses": {
                "value": "1. The paper's novelty is unclear, as prior research, such as that by Jagielski et al., has proposed two methods for defining subpopulations: one based on data annotations and the other using clustering techniques. The authors then use this foundation to establish Theorem 1, which seems unchallenging.\n\n2. The authors assert that \"In this model, subpopulations have distinct supports, meaning that each data point is associated with only one subpopulation.\" However, in real-world datasets, some data points may belong to multiple subpopulations."
            },
            "questions": {
                "value": "1. What is the attack scenario that the adversary knows the specific subpopulation within the mixture distribution to which the validation samples belong?\n\n2. The authors focus on binary classification. How would this approach apply to a more complex task? Would it still be effective?\n\n3. In Figure 5b, ResNet 50 appears to be more vulnerable to subpopulation attacks than ResNet 101. Could you elaborate on the reasons for this difference?\n\n4. The authors conduct 1,626 individual poisoning attacks across various combinations of dataset, subpopulation, model, and alpha. I\u2019m curious if different poisoning attack methods yield varying effects on the models. Could you elaborate on that?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors explore the relationship between subpopulation attacks and the complexity/overparameterization of machine learning models. Subpopulation attacks are a form of model poisoning attack in which an adversary targets a specific distribution instead of isolated samples and aims to degrade model performance on that specific distribution without significantly impacting the overall performance of the model. The authors theoretically and experimentally proved that ML models that exhibit local dependence (including larger and overparameterized models) are more susceptible to subpopulation attacks. Although the paper focuses on subpopulation attacks, I believe that this work could be helpful in improving fairness-aware ML."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. Theoretical explanations for local dependence vs. susceptibility to subpopulation attacks are supported by experimental results.\n2. The experimental setup contains different types of dataset (tabular, image, and text), which strengthens the authors' claim."
            },
            "weaknesses": {
                "value": "1. The authors consider only the binary classification case, which limits the full exploration."
            },
            "questions": {
                "value": "1. Do you think subpopulation attacks could be used to measure fairness in ML? Or are ML models that incorporate bias removal less susceptible to subpopulation attacks? \n2. Is it possible to detect and/or mitigate local dependence?\n3. Could one subgroup be affected by the subpopulation attack target another subgroup due to their \"closeness\"?\n4. Page 1, line 053: repetition of \"more\". Page 10, line 535: missing full stop."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a subpopulation poisoning attack targeting specific subgroups within data, exploiting the complexity of overparameterized machine learning models, which can inadvertently memorize and misclassify these subgroups. The paper reveals the relations of the attack success and model complexity and subgroup size."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- S1: The paper explores some key parameters of learning including model complexity and learning of similar inputs.\n- S2: the paper is easy to read."
            },
            "weaknesses": {
                "value": "- W1: It is unclear how the subgroups are identified.\n- W2: The significance of the work over the existing work is not clear. Also, these observations look consistent with the understanding of general supervised training.\n- W3: The approach is limited to discriminative models."
            },
            "questions": {
                "value": "- Q1: Are subgroups manually identified (Line 306) or automatically clustered (Line 205)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examines the vulnerability of machine learning models of various sizes to subpopulation poisoning attacks. The authors develop a theoretical framework to explain why overparameterized models are particularly susceptible to these attacks. They then conduct extensive experiments across multiple models and datasets, showing that more complex models are indeed more vulnerable to subpopulation poisoning. Additionally, the paper highlights the challenges in developing effective defenses to mitigate these specific vulnerabilities."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The topic of understanding subpopulation attacks is interesting.\n- The authors provide valuable insights, such as the finding that larger models are more susceptible to subpopulation attacks.\n- Extensive experiments strengthen the credibility of the conclusions."
            },
            "weaknesses": {
                "value": "- The definition needs more illustration.\n- The generalizability of the conclusions is not entirely clear."
            },
            "questions": {
                "value": "Definition 2 needs more detailed explanation. Specifically, what does f_p represent here? Also, what does S_p signify? The authors should clarify these terms when they are introduced to improve readability.\n\nThe paper mentions two types of subpopulation definitions: one based on clustering of samples\u2019 latent space representations, and another based on predefined semantic annotations in the dataset. In their evaluation, the authors use subpopulations defined by manual annotations that provide semantic information about the samples. I would like to know if this selection might influence the conclusions\u2014specifically, can the findings in this paper generalize to the first type of subpopulation definition? Additional insights on this point would be helpful.\n\nAnother concern is related to generalization. I noticed that the authors adopt a straightforward implementation of subpopulation attacks by flipping labels within the subpopulation. Have the authors tried other, potentially stronger, attack methods? If so, would these different attack types affect the conclusions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores how model complexity influences susceptibility to subpopulation poisoning attacks. The authors first prove that a learning algorithm is naturally susceptible to subpopulation poisoning attacks if it exhibits local dependence (Theorem 1). After that, the authors speculate that modern overparameterized deep learning models (e.g., MLP) also have this vulnerability since most of the existing learning algorithms have local dependency (in some settings) proved in existing works. Besides, the authors empirically verify this understanding through experiments. In particular, the authors also show that this vulnerability varies across different subgroup sizes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors attempt to give a deeper understanding and theoretical analysis of existing attacks. It should be encouraged.\n2. This is a well written paper. The definitions of symbols and the overall flow are clear.\n3. The experiments are sufficient to support author\u2019s statements to a large extent."
            },
            "weaknesses": {
                "value": "1. The scope of this paper is limited. \n- In this paper, the authors focus only on the subpopulation poisoning attacks. To the best of my knowledge, this particular attack type (rather than the general data poisoning) is still not yet a widely recognized threat. \n- In particular, this paper only focuses on the label flipping subpopulation poisoning attack. It further limits the generalizability of the ideas in this paper.\n- The main finding (i.e., that more complex models are more vulnerable to such attacks) seems to be expected. More importantly, the authors do not provide insights on how to exploit some of the understandings found in this paper.\n2. There are some potential over-claims.\n- Line 19-21: To the best of my knowledge, Theorem 1 is only related to locally dependent learners instead of overparameterized models, not to mention model capacity.\n- Line 41-44: missing the type of backdoor attacks [1].\n- Line 130-131: please provide references or experiments to show that the previous findings are not necessarily true in subpopulation poisoning attacks.\n3. Theorem 1 seems to be a straightforward extension of the one proposed in [2].\n4. The authors should discuss potential applications of their findings, instead of simply highlighting the need for more attention for defenses.\n5. The authors should also conduct experiments on other types of poisoning attacks, instead of just the label flipping subpopulation poisoning attack. \n\n\nMinor Comments\n1. There are still many typos (e.g., Line 183, Line 204).\n\n\nReferences\n1. Backdoor Learning: A Survey.\n2. Subpopulation data poisoning attacks.\n\n\nPS: I am not very familiar with subpopulation poisoning attacks, although I did a lot of work on data poisoning and its defenses. Please feel free to correct me if I have any misunderstanding. I am willing to increase my scores if the authors can address (parts of) my concerns."
            },
            "questions": {
                "value": "Please kindly refer to 'Weakness' for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper primarily investigates the relationship between model complexity and vulnerability to subpopulation poisoning attacks. Through theoretical analysis and experimental research, the authors examine how machine learning models of varying complexity (such as neural networks of different sizes) respond to data poisoning attacks targeting on specific subgroups. They discover that as model complexity increases, so does the model's sensitivity to these attacks, particularly for medium-sized subgroups. The research also reveals that very small subgroups are often resistant to effective poisoning attempts."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Integrates theoretical framework with empirical analysis, enhancing result validity."
            },
            "weaknesses": {
                "value": "1.\tWhile the paper identifies vulnerabilities, it does not provide possible defense strategies to mitigate the risks associated with subpopulation poisoning attacks, which could be improved.\n2.\tThis paper touches on the impact on marginalized groups, and it could benefit from a more in-depth discussion of the ethical implications of subpopulation poisoning attacks and the responsibilities of researchers and practitioners"
            },
            "questions": {
                "value": "-\tSection 5.1 notes that \"Small Subgroups are Difficult to Poison,\" which seems at odds with the conclusion suggesting defenses for subgroup vulnerabilities. The authors should clarify the relationship between subgroup size and vulnerability. The paper concludes that \"Larger Subgroups Are Less Affected by Model Size\" and \"Model Complexity Affects Medium-Sized Subgroups Disproportionately.\" The authors should further analyze how these findings for medium and large subgroups differ from model performance under traditional data poisoning attacks. This would align the paper's observations with its recommendations and highlight the uniqueness of subgroup poisoning attacks.\n-\tI am wondering how model interpretability might influence the detection and understanding of subpopulation poisoning attacks, which could be a critical aspect of building trustworthy ML models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper should discuss the ethical concerns raised from this work."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This manuscript inspects models' robustness against data poisoning attacks and empirically finds that models with more parameters are significantly more vulnerable to subpopulation poisoning. Fine-grained analysis suggests that attack intensity and subgroup size may also influence attack damages."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Clearly, a lot of effort has been put into this work. I believe the contribution is sufficient for a publication. The topic of data poisoning attacks is popular, especially in the era of generative AI. \n\n2. The layout of the paper is clear. Most of the concepts are formally defined or introduced. \n\n3.  The claims made are grounded by sufficient empirical evidence, as well as fine-grained analysis."
            },
            "weaknesses": {
                "value": "No efforts were made to build a defense against the attacks explored, and thus not maximizing social benevolence. \n\nIntensity of an attack remains undefined."
            },
            "questions": {
                "value": "1. What is the intensity of a data poisoning attack? It was used in the paper but not defined/introduced. \n\n2. It would be nice if the manuscript could talk about how the findings made in this paper can help build a defense even if the defense might not work for all subgroups."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}