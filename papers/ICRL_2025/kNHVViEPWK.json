{
    "id": "kNHVViEPWK",
    "title": "Unearthing Skill-level Insights for Understanding Trade-offs of Foundation Models",
    "abstract": "With models getting stronger, evaluations have grown more complex, testing multiple skills in one benchmark and even in the same instance at once. However, skill-wise performance is obscured when inspecting aggregate accuracy, under-utilizing the rich signal modern benchmarks contain. We propose an automatic approach to recover the underlying skills relevant for any evaluation instance, by way of inspecting model-generated {\\em rationales}. After validating the relevance of rationale-parsed skills and inferring skills for $46$k instances over $12$ benchmarks, we observe many skills to be common across benchmarks, resulting in the curation of hundreds of \\emph{skill-slices} (i.e. sets of instances testing a common skill). Inspecting accuracy over these slices yields novel insights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet, on average, Gemini 1.5 Pro is $18\\%$ more accurate in \\emph{computing molar mass}, but $19\\%$ less accurate in \\emph{applying constitutional law}, despite the overall accuracies of the three models differing by a mere $0.4\\%$. Furthermore, we demonstrate the practical utility of our approach by showing that insights derived from skill slice analysis can generalize to held-out instances: when routing each instance to the model strongest on the relevant skills, we see a $3\\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices and framework open a new avenue in model evaluation, leveraging skill-specific analyses to unlock a more granular and actionable understanding of model capabilities.",
    "keywords": [
        "evaluation",
        "skills",
        "tradeoffs",
        "frontier models",
        "llm",
        "large multimodal model",
        "rationale",
        "error analysis",
        "interpretability"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "Automatic skill-based evaluation of LLMs and LMMs by parsing skills from rationales for existing benchmarks",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kNHVViEPWK",
    "pdf_link": "https://openreview.net/pdf?id=kNHVViEPWK",
    "comments": [
        {
            "summary": {
                "value": "This paper uses LLMs to annotate benchmark instances with skills required to solve them. The authors validate their annotation in multiple different (mostly automated) ways. They also analyze how different models' capabilities vary when restricted to so-called skill slices, i.e. sets of benchmark instances that all require a specific skill to solve (according to the automated annotation)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The writing is generally quite engaging \n- Decomposing broad benchmarks into more specific ones seems quite useful for things like model selection, whenever models are to be deployed in a specific setting. \n- Despite the caveats listed in the weaknesses, the results on differences in model capabilities are quite interesting. The additional validation experiments, like the one on model routing, make me believe that there is indeed meaningful signal in the results."
            },
            "weaknesses": {
                "value": "- All results on comparing pairs of models would strongly benefit from adding error bars to show that the differences are indeed exceeding what would be expected from noise when comparing equally performant models. \n- Additional statistical analysis to show that the most stark model differences cannot be explained by random noise + testing many hypotheses (i.e. comparing models on many different skills) would also strenghten the results. \n- Releasing the annotations would be helpful for the reviewers to get a better intuitive picture of the validity of the proposed methods. \n\n- Nitpicks:\n   - The writing sometimes seems a bit too grandiose:\n       - For example, whether the paper is \"providing a valuable resource for the research community\" seems like something the community will decide on.\n  - The formatting in Appendix E is off (lots of empty space)\n  - Appendix G is incomplete (it ends mid-sentence)\n  - Some kind of face-value evaluation (such as a small scale human evaluation) beyond GPT-4 as judge would be useful for section 5."
            },
            "questions": {
                "value": "- What does figure 8 show? Is the last row's annotation a typo?\n- What happens when combining skill and text/image embeddings in figure 8?\n- What are the pros and cons of the verification approach based on negative samples?\n- Which skills and benchmarks does the routing approach improve most on (let's say compared to an ensemble, as well as \"perfect\" routing that has access to the ground truth)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new method to evaluate LLMs by analyzing their skills rather than just relying on overall accuracy scores. It extracts detailed reasoning steps (rationales) from models like GPT-4 to identify the skills used, grouping these into \"skill-slices\". By doing this, the researchers can reveal strengths and weaknesses that are not revealed through traditional evaluations. They also introduce probing questions to validate the identified skills, ensuring that the analysis reflects the model's true abilities. This approach aims to provide a deeper understanding of model capabilities and guide improvements for future models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper provides a new insight\u00a0to evaluate LLMs by evaluating on fine-grained skill analysis rather than traditional scores. This method of extracting \"skill-slices\" from model-generated rationales allows for a deeper understanding of models\u2019 capabilities. The insight is reasonable, and the meaningful for current LLM studies.\n- The design of probing questions allows for a robust validation for the extracted skills, which could reduce the risk of hallucination.\n- The paper provides extensive experimentations across multiple benchmarks and models. It clearly provides solid analysis and findings which might benefit for future studies.\n- The paper provides skill annotations, which would guide future model and dataset developments. \n- The skill-slices design could benefit for the explainability and transparency of large language models.\n\nIn general, it is a good paper."
            },
            "weaknesses": {
                "value": "- Real-world tasks often require\u00a0a combination of multiple skills. The current method of skill extraction assumes that skills can be categorized and analyzed in isolation. This overlooks the fact that the interaction between multiple skills may be\u00a0non-linear\u00a0and context-dependent, meaning that combining skills does not always produce the same outcome as using those skills independently.\n- Some LLMs may operate as \"black boxes,\" where the true internal reasoning process is not fully transparent. This means that the skills inferred from the generated explanations may not align with the model's real decision-making processes.\n\nThe paper has already discussed some limitations."
            },
            "questions": {
                "value": "Given that models may hide internal process and not explicitly demonstrate all the skills they use, how would we evaluate those models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper mainly studies the evaluation challenge from testing multiple skills in modern LLM evaluations. Given a specific benchmark dataset, the skill-wise performance is obscured and the possible hidden rich signal is usually underutilized. To alleviate this issue, this paper propose a automatic approach for recovering the underlying skills given any evaluation instance. In specific, this is implemented by inspecting model generated rationales. By examining the relevance of rationale-parsed skills and inferring skills for 46k instances over 12 benchmarks, it is shown that many skills to be common across benchmarks, resulting in the curation of hundreds of skill-slices. These skill slices are finally further shown to deliver novel insights on model trade-offs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- First of all, the LLM evaluation problem is very important, especially under the case where multiple or many skills are evaluated together. This has been shown by Figure 1 as well to clearly clarify the motivation of this work. \n\n- The paper is well written and easy to follow\n\n- Automatic discovering the skills related to any evaluation instance is an important topic.\n\n- Some findings are quite interesting. For instance, Gemini 1.5 Pro to be much stronger in math and science skills, like \u2018computing molar mass\u2019, while falling far behind for legal skills like \u2018applying constitutional law\u2019."
            },
            "weaknesses": {
                "value": "- The automatic verification protocol should be further explained. This is the single most important part of this method pipeline. From my understanding, querying LLM models or APIs with specific prompt engineerings can always get some sort of skills, rationales or chain of thoughts. The more important thing is how can you guarantee these generated content are properly validated. The author claimed that they will \"release all our rationales, skill annotations, and skill-slices, which we term the Skill-Index, to the public.\", but more explanation should be provided. The verified trustworthy accuracy here will be quite important.\n\n- The technique contribution looks a bit limited. All the work done here are PE related work."
            },
            "questions": {
                "value": "- Assume in the industry, you are not allowed to access those strong models (e.g., GPT-4o). Given a specific LLM, the skill discovering ability is actually constrained by the skill ability of this LLM. How could you explain the effectiveness of automatic method, if let's say the model itself is weak in one category of skill while you expect this LLM can identify this skill and generate the corresponding rationales automatically?\n\n- Given the generated rationales will be treated as a golden standard for the future use in other model evaluations, how to guarantee the trustworthiness of such rationales? This is actually the same question listed in weakness part.\n\n\n- I am curious: How is the cost of conducting such kind of research?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an automatic approach to infer underlying skills for evaluation instances by inspecting model-generated rationales. This helps in achieving a finer-grained understanding of model capabilities from existing benchmarks, increasing the interpretability of evaluating LLMs with benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The way of analyzing the model evaluation benchmark from the aspect of inferred skills is interesting and novel to me.\n\n- This work utilizes a post-hoc verification of skills and validates its validity by comparing the results with human validation.\n\n- The authors conduct experiments to show some interesting findings on skill generation and model evaluation."
            },
            "weaknesses": {
                "value": "Overall, the presentation needs improvement such as:\n- A simple prompt example used to generate a detailed rationale shown in Figure 2 would be better.\n- A clear subsection of used benchmarks and LLMs in the main paper.\n\nI am confused about the description of the experimental section such as:\n\n- Figure 8 is not clear enough at first sight. Which method uses skill annotations? And does the higher precision @ k = 20 indicate better performance or worse? Could you please explain the reason behind the evaluation?\n- Figure 9 only depicts the number of skills per instance obtained by direct prompting vs. our rationale parsing. While the larger number cannot show the generation effectiveness. Reliability or relevance to the question are more important. Is there any other ablation study showing the comparisons concerning the reliability of annotated skills?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}