{
    "id": "Ke2BEL4csm",
    "title": "A Non-Contrastive Learning Framework for Sequential Recommendation with Preference-Preserving Profile Generation",
    "abstract": "Contrastive Learning (CL) proves to be effective for learning generalizable user representations in Sequential Recommendation (SR), but it suffers from high computational costs due to its reliance on negative samples. To overcome this limitation, we propose the first Non-Contrastive Learning (NCL) framework for SR, which eliminates computational overhead of identifying and generating negative\nsamples. However, without negative samples, it is challenging to learn uniform representations from only positive samples, which is prone to representation collapse. Furthermore, the alignment of the learned representations may be substantially compromised because existing ad-hoc augmentations can produce positive samples that have inconsistent user preferences. To tackle these challenges, we design a novel preference-preserving profile generation method to produce high-quality positive samples for non-contrastive training. Inspired by differential privacy, our approach creates augmented user profiles that exhibit high diversity while provably retaining consistent user preferences. With larger diversity and consistency of the positive samples, our NCL framework significantly enhances the alignment and uniformity of the learned representations, which contributes to better generalization. The experimental results on various benchmark datasets and model architectures demonstrate the effectiveness of the proposed method. Finally, our investigations reveal that both uniformity and alignment play a vital role in improving generalization for SR. Interestingly, in our data-sparse setting, alignment is usually more important than uniformity.",
    "keywords": [
        "sequential recommendation",
        "non-contrastive learning"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We present a non-contrastive learning framework for sequential recommendation",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Ke2BEL4csm",
    "pdf_link": "https://openreview.net/pdf?id=Ke2BEL4csm",
    "comments": [
        {
            "summary": {
                "value": "Summary:\nThis paper addresses the issue of high computational cost in contrastive learning (CL)-based sequential recommendation. Innovatively, it introduces non-contrastive learning into sequential recommendation. To tackle the representation collapse and alignment problems caused by solely relying on positive samples, a novel user preference-preserving profile generation method is proposed for the Non-Contrastive Learning framework for sequential recommendation (NCL-SR). NCL-SR is capable of learning generalized and robust user representations for sequential recommendation. Experimental results demonstrate that NCL-SR outperforms traditional SR models and CL-based SR models in terms of performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Strength\uff1a\n1.This paper innovatively proposes a non-contrastive learning framework for sequential recommendation.\n2.Inspired by Differential Privacy (DP), a theoretically guaranteed user preference-preserving data augmentation method is proposed to address representation collapse and preference inconsistency issues.\n3.Two novel loss calculation methods are introduced.\n4.Extensive experiments are conducted to verify the effectiveness, and the roles of feature alignment and uniformity in sequential recommendation are analyzed."
            },
            "weaknesses": {
                "value": "Limitation\uff1a\n1.What does \"s()\" represent in Eq. 1? What does \"f()\" represent in Eq. 4?\n2.What do the three polygons in the upper part of Fig. 1 represent? Why is there no corresponding polygon for the third red text?\n3.During the construction of candidate user profiles, a item-level approach is adopted. How is the number of perturbations determined? Why weren't further experiments conducted to verify the effect of perturbation quantity on recommendation performance?\n4.What is the split ratio of the training, validation, and test sets? Why wasn't it explicitly stated?\n5.The motivation of this paper is to address the high computational cost and memory consumption in negative sample extraction of CL in SR. Why weren't further experiments conducted to verify the effect of NCL-SR on computational cost and memory consumption?\n6.Further refinement of the text is needed. For instance, what is the relationship between \u201cNCL SR\u201d and \u201cNCL-SR\u201d in line 178? The term \"differential privacy\" in line 210 is in lowercase, while \"Differential Privacy\" in line 215 is in uppercase. Consistency should be maintained."
            },
            "questions": {
                "value": "Limitation\uff1a\n1.What does \"s()\" represent in Eq. 1? What does \"f()\" represent in Eq. 4?\n2.What do the three polygons in the upper part of Fig. 1 represent? Why is there no corresponding polygon for the third red text?\n3.During the construction of candidate user profiles, a item-level approach is adopted. How is the number of perturbations determined? Why weren't further experiments conducted to verify the effect of perturbation quantity on recommendation performance?\n4.What is the split ratio of the training, validation, and test sets? Why wasn't it explicitly stated?\n5.The motivation of this paper is to address the high computational cost and memory consumption in negative sample extraction of CL in SR. Why weren't further experiments conducted to verify the effect of NCL-SR on computational cost and memory consumption?\n6.Further refinement of the text is needed. For instance, what is the relationship between \u201cNCL SR\u201d and \u201cNCL-SR\u201d in line 178? The term \"differential privacy\" in line 210 is in lowercase, while \"Differential Privacy\" in line 215 is in uppercase. Consistency should be maintained."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a Non-Contrastive Learning framework for Sequential Recommendation powered by preference preserving user profile generation: NCL-SR. The NCL-SR framework eliminates the computational overhead of identifying and generating negative samples in CL. The experimental results on various benchmark datasets and model architectures demonstrate the effectiveness of the NCL-SR method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ The methodology of this paper is technically sound. The method itself is somewhat novel to me.\n\n+ To my best of knowledge, it is the first attempt to utilize matrix cross entropy in the recommendation system.\n\n+ The proposed NCL-SR achieves much better performance against other baseline models."
            },
            "weaknesses": {
                "value": "+ Fig.1. needs to be polished. For instance, it is unclear for me what is the plot meaning of the exponential mechanism with polygons in Fig.1? \n\n+ Why do you set the utility score as $\\Delta_u = e - 1/e$? It is unclear for me. Could you please kindly provide some insights on that? \n\n+ Could different values of $\\gamma$ affect the model performance? \n\n+ Some similar papers should be cited or discussed, e.g., [1] and [2].\n\nRef:\n\n[1] DDGHM: Dual dynamic graph with hybrid metric training for cross-domain sequential recommendation\n\n[2] Enhancing hierarchy-aware graph networks with deep dual clustering for session-based recommendation"
            },
            "questions": {
                "value": "Please refer to the Weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose the first Non-Contrastive Learning (NCL) framework for Sequential Recommendation (SR). The authors design a novel preference-preserving profile generation method to produce high-quality positive samples for non-contrastive training. The paper is easy to follow and well-organized. The authors conduct extensive experiments to validate the efficacy of the proposed NCL-SR."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1: The paper is easy to follow and well-organized.\n\nS2: The authors provide theorem analysis for the proposed method which is novel and interesting to me.\n\nS3: The authors conduct extensive experiments to validate the efficacy of the proposed NCL-SR."
            },
            "weaknesses": {
                "value": "W1: Some illustrations should be improved. For example, the authors should motivate why you choose Matrix Cross Entropy (MCE) against some other methods (e.g., Barlow Twin, MEC and DirectAU) for contrastive learning? \n\nW2: Some method details are missing. For example, the calculation of $C(Z,Z\u2019)$ in Eq.(9) should be provided to make the paper more readable.\n\nW3: Since you need to figure out the eigenvalue of $\\bm{V}$ in Eq.(9), how about the time complexity of your proposed method?"
            },
            "questions": {
                "value": "Please refer to the Weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces the NCL-SR, which is designed to reduce the computational burden found in traditional Contrastive Learning apporaches by eliminating the need for negative sampling. CL methods generally require negative samples to avoid representation collapse, but this comes with high computational costs. The main contribution of NCL-SR is its preference-preserving profile generation technique, which uses differential privacy to create diverse yet consistent positive user profile samples that retain the user\u2019s preferences. This helps to overcome challenges such as representation collapse and preference inconsistency."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- This paper solves a important question in traditional SR field, that CL approaches cost quite a lot computational resources. By adopting the profile augmentation method proposed in this paper, the performance of SR can even higher than those traditional CL methods.\n\n- The authors provides theoritial guarantees for the uniformity and alignment."
            },
            "weaknesses": {
                "value": "- Since the main motivation of NCL is to reduce the cost of traditional CL methods, I believe the efficiency study is needed.\n- When comparing with other SR models (besides CL methods), I believe they should also be implemented with e5 for a fairer comparison."
            },
            "questions": {
                "value": "- The authors claim that the user profile generation is inspired by differential privacy, I wonder how it inspired the proposed framework in details? Maybe some addtional literature review might help to explain."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}