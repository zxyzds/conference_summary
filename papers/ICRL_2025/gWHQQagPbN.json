{
    "id": "gWHQQagPbN",
    "title": "Beyond 2:4: Exploring V:N:M Sparsity for Efficient Transformer Inference on GPUs",
    "abstract": "To date, 2:4 sparsity has stood as the only sparse pattern that can be accelerated\nusing sparse tensor cores on GPUs. In practice, 2:4 sparsity often possesses low\nactual speedups (\u2264 1.3) and requires fixed sparse ratios, meaning that other ratios,\nsuch as 4:8, 8:16, or those exceeding 50% sparsity, do not incur any speedups on\nGPUs. Recent studies suggest that V:N:M sparsity is promising in addressing\nthese limitations of 2:4 sparsity. This sparsity divides a weight matrix into mul-\ntiple V\u00d7M blocks, pruning (M-4) columns within each block and applying 2:4\nsparsity to the remaining columns. V:N:M sparsity inherently encompasses 2:4\nsparsity but allows for higher and more flexible pruning ratios, typically resulting\nin greater practical speedups. However, regarding accuracy, the effects of V:N:M\nsparsity on broader Transformer models, such as vision Transformers and large\nlanguage models (LLMs), are largely unexamined. Moreover, Some specific is-\nsues related to V:N:M sparsity, such as how to select appropriate V and M values,\nremain unresolved. In this study, we thoroughly investigate the application of\nV:N:M sparsity in vision models and LLMs across multiple tasks, from pretaining\nto downstream tasks. We propose three key approaches to enhance the applica-\nbility and accuracy of V:N:M-sparse Transformers, including heuristic V and M\nselection, V:N:M-specific channel permutation and three-staged LoRA training\ntechniques. Experimental results show that, with our methods, the DeiT-small\nachieves lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains ac-\ncuracy even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5\nsparsity performs comparably or better than training-free 2:4 sparse alternatives on\ndownstream tasks. More importantly, V:N:M-sparse Transformers offer a wider\nrange of speedup-accuracy trade-offs compared to 2:4 sparsity. Overall, our explo-\nration largely facilitates the V:N:M sparsity to act as a truly effective acceleration\nsolution for Transformers in cost-sensitive inference scenarios.",
    "keywords": [
        "V:N:M sparisity",
        "Transformer inference acceleration",
        "accuracy-speedup tradeoffs",
        "channel permutation"
    ],
    "primary_area": "infrastructure, software libraries, hardware, systems, etc.",
    "TLDR": "We propose methods to facilitate the V:N:M sparsity to act as a truly effective acceleration solution for Transformers in cost-sensitive inference scenarios.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=gWHQQagPbN",
    "pdf_link": "https://openreview.net/pdf?id=gWHQQagPbN",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates the use of V:N:M sparsity in Transformer models and proposes three key methods to enhance its performance: heuristic V and M selection, V:N:M channel permutation, and a three-stage LoRA training technique. The experimental results demonstrate that V:N:M sparsity offers a broader range of speedup-accuracy trade-offs compared to traditional 2:4 sparsity."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper introduces a comprehensive framework that generates highly accurate V:N:M-sparse Transformers under different constraints, allowing users to balance the system performance and model accuracy.\n\n2. The paper addresses key challenges in applying V:N:M sparsity by introducing three innovative techniques: heuristic V and M selection, V:N:M channel permutation, and a three-stage LoRA training technique. \n\n3. Experiments show superior performance of proposed methods."
            },
            "weaknesses": {
                "value": "1. The paper lacks a comparison with the latest related works, which limits its ability to contextualize the advantages and innovations within the current research landscape, such as [1]. Moreover, please also report the model performance without sparsity in Figure 4,5,7,8,9 to show the influence of introducing model sparsity. \n\n2. The experiments in this paper are limited to small-scale models, such as ViT, DeiT, and Llama-7B. It does not extensively explore the performance on larger models, like ViT-Huge or LLaMA models with higher parameter counts, where the effectiveness of the sparsity method might vary.\n\n3. The work would be stronger if it provided performance benchmarks for Llama2-7B on pretraining benchmarks, such as MMLU or GSM-8k, to give a more comprehensive evaluation of the model's capabilities across a broader set of important benchmarks. If computational resources are limited, consider performing post-training on Llama2-7B to demonstrate the applicability of this paper's methods.\n\n[1] Yang H, Yin H, Shen M, et al. Global vision transformer pruning with hessian-aware saliency[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023: 18547-18557."
            },
            "questions": {
                "value": "This paper propose three techniques, including heuristic V and M selection, V:N:M channel permutation, and a three-stage LoRA training technique. Please further explain the relationship between these techniques, are they complementary, sequential, or something else?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores applying V:N:M sparsity to the weight parameters of transformer-based models, which can well retain accuracy while harvest inference speedup from sparse tensor cores in advanced GPUs. To achieve this, the authors propose three techniques to convert a dense transformer into a V:N:M sparse model, including a heuristic method for selecting V and M values, V:N:M-specific channel permutation, and a three-stage LoRA training approach for LLMs. They conduct extensive experiments on mainstream transformers, including DeiT, Swin Transformer, and Llama2-7B, across multiple tasks, demonstrating that their method outperforms naive 2:4 sparsity versions. Overall, this paper could be a good empirical contributions, paving the way for V:N:M sparsity compression for Transformers."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper is well-written and easy to follow. It integrates multiple techniques to accelerate V:N:M-sparse transformers. While the novelty of each technique is somewhat limited, the resulting performance improvements are substantial.\nFor theory strengths, the paper formulates the selection of V and M value as an optimization problem, balancing accuracy with speedup constraints. To solve this efficiently, they propose a two-phase sifting process to identify the optimal (V, M) combinations; Moreover, it adapts channel permutation to V:N:M sparsity, applying it to both the input and output of weight matrices to enhance accuracy for low training budgets. Finally, to ensure stable training of V:N sparse transformers, the authors integrate a three-stage LoRA training approach, consisting of Dense LoRA, Sparse LoRA with a dynamic mask, and Sparse LoRA with a fixed mask."
            },
            "weaknesses": {
                "value": "1. While it's intuitive that a smaller M results in lower sparsity in the sparse transformers, the process for excluding certain (V, M) combinations is unclear. Additionally, there is insufficient evidence in the paper demonstrating that mask diversity (MD) improves transformer accuracy.\n2. In Definition 1, the objective is to maximize accuracy under speedup constraints s. However, there should be experimental evidence demonstrating that the chosen V and M values meet these constraints.\n3. In Section 5.4, although V:N:M sparse Llama2 outperforms RIA and Wanda, it still suffers significant accuracy loss compared to the dense counterpart. For instance, HellaSwag accuracy drops from 57.23 to 42.88, and ARC-C from 43.3 to 34.3.\n4. Equation (3) lacks the description of $\\overline{M}_{t-1}$\n5. In section 4.3, the definitions of $Mv$, $Wv$, $Bv$ and $ Av$ are unclear. Also, it would better to disclose detailed LoRA configurations in the three-staged LoRA training experiments (section 5.4) , such as dynamic sparse mask initialization, the interval of updating sparse masks, the actual training iteration assignment two different stages etc.\n6. Figure 3 does not intuitively illustrate \"in the absence of regularization, frequent updates to the masks can negatively impact the gradient flow of V:N:M sparse Transformers during fine-tuning\". Additionally, adding a comprehensive empirical study would make it more convincing to draw the conclusion that \"the iterations for the first two stages should not exceed 10% of the total iterations.\""
            },
            "questions": {
                "value": "1. For large language models (LLMs), latency is not solely determined by FLOPs; it is typically constrained by memory bandwidth and access patterns. In Definition 1, is the measured speedup based on a batch size of 1? Furthermore, when varying the batch size, will this lead to different V and M sparsity values? \n2. Can channel permutation enhance the performance of SR-STE training or LoRA training?\n3. How is the regularization coefficient $\\lambda$ in equation (3) tuned to improve training stability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses the limitations of 2:4 sparsity, by thoroughly investigating the application of V:N:M sparsity in vision models and LLMs across multiple tasks. The authors propose methods for selecting optimal V and M values and introduce techniques like V:N channel permutation and a three-stage LoRA training method, designed to enhance the accuracy and applicability of V:N Transformers, particularly in vision and language models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper goes beyond theoretical contributions by proposing practical methods to improve the deployment of V:N:M\nsparsity. Techniques like heuristic selection of V and M values, channel permutation, and three-staged LoRA training are thoughtfully designed to optimize accuracy-speedup trade-offs, even with limited training resources.\n2. The authors validate their approach across multiple benchmarks, including vision Transformers and large language models (LLMs), demonstrating that V:N:M sparsity can achieve high speedups with minimal accuracy loss."
            },
            "weaknesses": {
                "value": "1. The heuristic method for selecting V and M values may not work for extensive Transformers models. My concerns are that even in the paper the proposed methods can achieve lossless performance on specific model DeiT-base and LLama2-7B. But what are the expected results on other models?\n2. The novelty and originality may not be enough. This paper is more like an engineering implementation technique. Authors may need to explain originality more."
            },
            "questions": {
                "value": "1. The heuristic method for selecting V and M values may not work for extensive Transformers models. My concerns are that even in the paper the proposed methods can achieve lossless performance on specific model DeiT-base and LLama2-7B. But what are the expected results on other models?\n2. The novelty and originality may not be enough. This paper is more like an engineering implementation technique. Authors may need to explain originality more. \n3. \"V:N:M-specific channel permutation, which improves the accuracy of V:N:M-sparse Transformers\". Why channel permutation can improve accuracy? \n4. \"a higher MD leads to better Transformer accuracy\" Is this claim from extensive experiments or other places?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The conventional 2:4 sparsity has been the only sparse pattern capable of accelerating GPU sparse tensor cores, and its performance was well-established. \nHowever, the paper claims that 2:4 sparsity has limitations, such as low actual speedups and support for only fixed sparsity ratios. \nTo address this, the paper adopts the V:N:M approach. \nV:N:M sparsity combines n:m pruning with structured pruning and claims to overcome the fixed 50% pruning ratio limitation of traditional 2:4 sparsity."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Figure 1 is well-drawn and easy to understand.\n2. The experiments conducted on the ViT model and LLM model are excellent."
            },
            "weaknesses": {
                "value": "1. [Novelty] I am not sure about the differences between this paper and the Venom paper [1]. Wasn't V:N:M proposed in the Venom paper [1]? How does it differ from Venom [1]?\n2. The Venom paper proposed a GPU kernel for speedup sparse model on GPUs. how does this paper handle processing on the GPU? Does it propose a dedicated GPU kernel? If so, what are the differences compared to the Venom [1] GPU kernel?\n3. The title of the paper includes \"Transformer\"\u2014what specific characteristics of Transformers does it take into account?\n4. Looking at Table 5, the caption lacks sufficient explanation. For instance, a clear description of \"$\\Delta$\" is necessary in the caption.\n\n\n[1] Venom: A vectorized N:M format for unleashing the power of sparse tensor cores. SC 2023"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies accuracy and speedup tradeoff of V:N:M sparse Transformers. It introduces a heuristic method for parameter selection, a channel permutation approach for low budget training and a three-stage LoRA training procedure applicable to various scenarios. Experimental results show that the proposed methods significantly accelerate the V:N:M sparse Transformers while maintaining lossless accuracy compared to 2:4 sparsity."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors propose three training approaches to enhance the applicability and accuracy of V:N:M sparse Transformers. They present a heuristic V and M selection method to find the tradeoff between accuracy and speedup for V:N:M sparse Transformers. To improve the accuracy within limited training budgets, they introduce a channel permutation approach. Finally, they introduce a Dense-Dynamic-Fixed mask training procedure to maintain the model accuracy.\n\n2. The paper is well-written and easy to follow up. The introduction and motivation are clear and well-organized. The experiments are comprehensive."
            },
            "weaknesses": {
                "value": "1. Why is the paper claimed that only a 2:4 sparse pattern can be accelerated by sparse tensor cores? I have some doubts about it. One could write a custom CUDA kernel and accelerate based on the indices of the weights.\n2. How did you implement the kernel using sparse tensor core? For higher sparsity (like 2:12, 2:24), the observed speedup falls short of the theoretical speedup, possibly due to inefficiencies in the implementation. Moreover, would you consider using cusparse/cublas or other CUDA kernels to compare speedup with the proposed method at highly sparsity?\n3. In Fig.4(a), the authors compare accuracy improvement under similar speedup conditions. In my view, more parameters generally lead to higher accuracy, so it would be fairer to compare accuracy at the same sparsity. Fig.4(a) shows how V and M selection impacts the accuracy and speedup of V:N:M sparse Transformers. However, the V:N:M setting in Fig.8 differs. Does this imply that V and M selection is not applied in Fig.8?"
            },
            "questions": {
                "value": "Please see the above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}