{
    "id": "cfe2zDg1G8",
    "title": "Scenario-Wise Rec: A Multi-Scenario Recommendation Benchmark",
    "abstract": "Multi Scenario Recommendation (MSR) tasks, referring to building a unified model to enhance performance across all recommendation scenarios, have recently gained much attention. However, current research in MSR faces two significant challenges that hinder the field's development: the absence of uniform procedures for multi-scenario dataset processing, thus hindering fair comparisons, and most models being closed-sourced, which complicates comparisons with current SOTA models. Consequently, we introduce our benchmark, Scenario-Wise Rec, which comprises six public datasets and twelve benchmark models, along with a training and evaluation pipeline. We have also validated our benchmark using an industrial advertising dataset, further enhancing its real-world reliability. We aim for this benchmark to provide researchers with valuable insights from prior works, enabling the development of novel models based on our benchmark and thereby fostering a collaborative research ecosystem in MSR. Our source code is also available.",
    "keywords": [
        "Recommender System",
        "Multi-scenario Recommendation",
        "Click-Through Rate Prediction"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "The Benchmark for Mutli-scenario Recommendation.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cfe2zDg1G8",
    "pdf_link": "https://openreview.net/pdf?id=cfe2zDg1G8",
    "comments": [
        {
            "summary": {
                "value": "The paper provides a much-needed benchmark for the emerging multi-scenario recommender systems (MSR) field. This benchmark contains a comprehensive framework (including training and evaluation pipelines) and inspects multiple public and industrial data sets. The detailed comparison of various MSR models provides valuable insights into their advantages and disadvantages. In addition, the source code is available to the public."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1: The paper presents the first dedicated benchmark for multi-scenario recommendation tasks, which may become a valuable resource in the field. It offers a comprehensive pipeline that includes data processing, model training, and evaluation, setting a new standard for transparency and reproducibility in MSR research. \n\nS2: Including public and industrial datasets strengthens the benchmark's reliability and applicability, covering many real-world scenarios. The publicly available source code and detailed tutorials will greatly facilitate researchers in conducting experiments and building upon the benchmark.\n\nS3: The paper provides a fair and detailed comparison of twelve state-of-the-art MSR models, which is valuable for researchers looking to understand the current landscape of MSR."
            },
            "weaknesses": {
                "value": "W1: Although the advantages and disadvantages of different baselines are provided, it can further enhance insights by solving the theoretical basis of the model and providing mitigation strategies for the \"seesaw effect.\"\n\nW2: The focus of standard MSR is understandable, but it does not explore the benchmark testing of more scenario-related topics (e.g., multi-scenario multi-task) and additional information (e.g., user's interactive history sequence), limiting the scope of its practical procedures.\n\nW3: The paper could benefit from a more detailed analysis of the models' computational efficiency, particularly how they scale with dataset size and complexity, which is crucial for practical deployment. In addition, moral considerations in the MSR system are also beneficial, especially data privacy and potential prejudice."
            },
            "questions": {
                "value": "Q1: Can the cause of different baselines on different data sets provide some additional insights? Are there any strategies that can be summarized for the seesaw effects?\n\nQ2: Can the authors provide more detailed analyses or benchmarks on the computational efficiency and scalability of the MSR models, especially in resource-constrained environments? \n\nQ3: Can the moral considerations in the MSR system provide some corresponding insights?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presented benchmark results on 6 existing public datasets and one newly collected industrial dataset with 12 recommendations models. There is an emphasis on multi-scenario recommendation, where different datasets are treated as multi-scenario by using some features for differentiating scenarios, such as user age, ad position, context, item category, platform, and news genres. Results are reported for how the number of \u201cscenarios\u201d affect model performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "comprehensive experiments are done with a wide range of algorithms and datasets, including a newly collected industrial dataset, with code provided and clear instructions."
            },
            "weaknesses": {
                "value": "The contribution and novelty is limited since the authors merely present existing public benchmark datasets with some features for differentiating \u201cscenarios\u201d, whereas some, if not all, of these features could very well be just normal features, and no justification is provided on why it\u2019s a reasonable choice to make them \u201cscenario\u201d features, and it seems the results are not benchmarked with treating the dataset as \u201csingle-scenario\u201d as is, and treat the \u201cscenario-feature\u201d as normal feature. How the \"scenario\" features are handled are also not clearly described. \n\nMany experiments are done, but limited insights are drawn from these experiments except some observations. Especially how the \u201cscenarios\u201d are modeled, and why certain models should be better than others. \n\nThe introduction of the industrial dataset seems new, but the description seems quite plain, and it\u2019s not convincing why this newly collected dataset is a good dataset for benchmarking."
            },
            "questions": {
                "value": "Why does it make sense to artificially make certain features differentiate scenarios? \n\nWhat is the \u201c301\u201d context feature?\n\nWhat is \u201cdense scenario\u201d and \u201csparse scenario\u201d?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "this paper collected user data from an online advertisement platform. The paper claim to have annoymized user-identity the technical preprocessing of the data (e.g., hashing etc.), but probably need double check privacy concerns when it's published."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a new benchmark for Multi-Scenario Recommendation. To deal with the problems of lacking standard data process pipline and closed-source models in MSR research, this paper introduces Scenario-Wise Rec, which includes six public datasets and one industrial dataset, twelve benchmark models, and a standardized training and evaluation pipeline to achieve model comparisons.\n\nThe contributions are:\nIt is the first benchmark specifically designed for MSR, a standardized pipeline for data processing, training and evaluation.\nOpen-Source and Real-World Applicability: Scenario-Wise Rec is publicly available, and is validated by applying it to a complex industrial dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper developed a standardized benchmark specifically for Multi-Scenario Recommendation tasks. This idea is good and needed in this area. This platform is particularly novel. It provides a comprehensive framework that includes multiple datasets, implementations, and a full evaluation pipeline, aiming to give comparisons across scenarios. The idea of the standard platform and benchmark is quite important. It provides a valuable tool to address the growing need for reliable, standardized MSR evaluations. \n\nThe paper is also well-organized and clear in its presentation. It is easy to read and the idea is presented directly. The authors provide much context about the challenges in MSR and the motivation behind their benchmark. The diagrams and tables effectively illustrate the framework\u2019s components and results, which help the researcher who may want to leverage the benchmark."
            },
            "weaknesses": {
                "value": "The idea of \u200b\u200bthe article is very good, providing a benchmark. However, because it is a benchmark, it must adapt to different needs and also take into account the multi-scenario interaction of MSR, which is a very difficult task. This paper has an ambiguous definition of multi-scenario. The datasets used are segmented but lack true cross-scenario relationships, which limits the ability to share knowledge between scenarios as MSR ideally should. The experiments focus on isolated scenario performance without evaluating transferability across scenarios.\n\nAlso, the use of only AUC and Logloss limits insights into cross-scenario performance consistency. What if other researchers's model have different targets and need different evaluation metrics? \n\nLast, the pipeline may lack modularity, restricting researchers from customizing data processing, model structures, or metrics."
            },
            "questions": {
                "value": "1. The experimental setup appears focused on individual scenario performance. Could you consider expanding the experiments to include cross-scenario evaluations, such as training a model in one scenario and testing in another, to validate transferability? This would provide valuable insights into the models\u2019 adaptability across scenarios.\n\n2. Could you elaborate on how the current data segmentation aligns with a true multi-scenario framework? Specifically, how do you envision the potential for knowledge transfer between segmented scenarios in the current datasets, given that they do not share users or items across contexts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a benchmark that includes six existing datasets for multi-scenario ctr prediction. Besides, they implement twelve models in these datasets with established data and training pipelines. The datasets and codes are provided."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The datasets and codes are provided.\n2. The topic is meaningful in that researchers can implement baseline comparison via the provided benchmark.\n3. The experiments are comprehensive with 10 times running."
            },
            "weaknesses": {
                "value": "1. Some clarifications are confusing. The terminology requires revision, particularly the use of \"recommendation,\" which does not accurately reflect the concept being discussed.\n2. The comparative analysis presented in Table 1 does not sufficiently demonstrate novel contributions relative to existing work in the field.\n3. The methodology section would benefit from additional detail, particularly regarding the model parameter selection process and optimization criteria.\n4. The motivation and challenge are not convincing. Most of the works in the papers have been provided with runnable code responsity."
            },
            "questions": {
                "value": "1. Why do authors claim that this research is a cut-edge benchmark, while no related explanations are provided in the paper?\n2. It seems that most of the baselines and evaluation metrics are designed for CTR prediction. So why use recommendation expressions in the paper?\n3. What is the novel challenge for researchers to implement the multi-scenario models?\n4. Can you provide statistical analysis for supporting the second challenge, that many MSR models are closed-sourced."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}