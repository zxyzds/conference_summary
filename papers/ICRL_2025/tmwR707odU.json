{
    "id": "tmwR707odU",
    "title": "Curriculum GNN-LLM Alignment for Text-Attributed Graphs",
    "abstract": "Aligning Graph Neural Networks (GNNs) and Large Language Models (LLMs) benefits in leveraging both textual and structural knowledge for Text-attributed Graphs (TAGs) learning, which has attracted an increasing amount of attention in the research community. Most existing literature assumes a uniformly identical level of learning difficulties across texts and structures in TAGs, however, we discover the $\\textit{text-structure imbalance}$ problem in real-world TAGs, $\\textit{i.e.}$, nodes exhibit various levels of difficulties when learning different textual and structural information. Existing works ignoring these different difficulties may result in under-optimized GNNs and LLMs with over-reliance on either simplistic text or structure, thus failing to conduct node classifications that involve simultaneously learning complex text and structural information for nodes in TAGs. To address this problem, we propose a novel Curriculum GNN-LLM Alignment ($\\textbf{CurGL}$) method, which strategically balances the learning difficulties of textual and structural information on a node-by-node basis to enhance the alignment between GNNs and LLMs. Specifically, we first propose a text-structure difficulty measurer to estimate the learning difficulty of both text and structure in a node-wise manner. Then, we propose a class-based node selection strategy to balance the training process via gradually scheduling more nodes. Finally, we propose the curriculum co-play alignment by iteratively promoting useful information from GNNs and LLMs, to progressively enhance both components with balanced textual and structural information. Extensive experiments on real-world datasets demonstrate that our proposed $\\textbf{CurGL}$ method is able to outperform state-of-the-art GraphLLM, curriculum learning, as well as GNN baselines. To the best of our knowledge, this is the first study of curriculum alignment on TAGs.",
    "keywords": [
        "Graph neural networks",
        "Large language models",
        "Text-attributed graphs",
        "Curriculum learning"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "We propose a Curriculum GNN-LLM Alignment method for  TAGs to strategically balance the learning difficulties of textual and structural information on a node-by-node basis,  enhancing the alignment between GNNs and LLMs.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=tmwR707odU",
    "pdf_link": "https://openreview.net/pdf?id=tmwR707odU",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces curriculum GNN-LLM alignment to tackle the text-structure imbalance problem in learning on text-attributed graphs. It proposes a node-wise difficulty measurer, a class-based node selection strategy, and a curriculum co-play alignment to balance and enhance learning between GNNs and LLMs. Extensive experiments show CurGL outperforms selective existing models on the classic node classification task across five well-known datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The motivation to balance the role of LLMs and GNNs in a fine-grained sample-wise manner for TAG learning is straightforward and convincing.\n- Writing is clear and easy to follow. Pseudo code is present to assist better understanding of the mechanisms designed."
            },
            "weaknesses": {
                "value": "- Novelty. The E-M training between LLMs and GNNs has been explored by a handful of pioneer works, e.g., SimTEG and GELM. The proposed CurGL enhances such framework by adding additional strategies for balancing the LLM and the GNN.\n- Unclear and potentially unfair experimental setting for baseline llm-as-predictor models. Is is unclear that how are the llm-as-predictor models testified, especially in Table 1. Is is based on a zero-shot manner as in Glbench, or are they also trained with the same training set? In terms of training, is it training-from-scratch or fine-tuning?\n- Lack of discussion of a very relevant work, GraphGPT: Graph Instruction Tuning for Large Language Models.\n- The proposed CurGL relies heavily on the availability of training data, and shows poor potential in transferring learning and zero-shot generalisation. Although it leverages an LLM for textual-based learning, the design of node-wise difficulty measurer and class-based selection strategy requires high-quality labels for the model to learn and converge. While it is quite reasonable for a graph learning model, the trained CurGL cannot be adapted to unseen data (unlike LLaGA, OFA and others), making its potential application quite narrow.\n- Can CurGL generalise beyond node classification, to link prediction or graph classification?"
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a method called Curriculum GNN-LLM Alignment designed for Text-Attributed Graphs (TAGs). The goal is to address the text-structure imbalance problem in TAGs, where nodes have varying levels of difficulty in learning textual and structural information. The CurGL approach progressively balances these difficulties through three key modules."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe integration of node difficulty into node classification is clearly motivated and engaging.\n2.\tThe presentation is well-organized, and the ablation study clearly demonstrates the impact of each technique."
            },
            "weaknesses": {
                "value": "1.\tMy primary concern is whether this method is effective for heterophilic graphs, as the proposed difficulty measurer seems tailored to homophilous graphs.\n\n2.\tThe class-based node selection assumes balanced class sampling within each subgraph. However, this assumption may be problematic for highly imbalanced datasets, potentially leading to biased or suboptimal node selection.\n\n3.\tRelated to the above, it\u2019s unclear how the authors handle edges between selected nodes in the class-based node selection. In each subgraph, nodes from different classes are sampled proportionally, which does not guarantee connectivity among them.\n\n4.\tAnother major concern is computational overhead. This method involves additional steps beyond GLEM, which already has considerable running time[1]. Efficiency analysis is essential and appears missing from the experimental section.\n\n5.\tThe performance appears insufficiently strong; for instance, the results on Arxiv are weaker than those of methods like LEADING[2]. Larger datasets, such as ogbn-products and ogbn-papers100M should also be included.\n\n6.\tThe paper\u2019s claim of using LLMs seems overstated, as small language models are used. To solve this obvious mismatch and align claims with experiments , it would be more appropriate to use models like LLAMA or GPT.\n\n7.\tIt is unclear if this approach generalizes to tasks beyond node classification, such as link prediction and graph classification. Focusing solely on node classification may limit the overall contribution.\n\n8. I am curious why GLEM achieves strong performance on Cora and Pubmed, which have very few labels. The results seem inconsistent with findings from existing works[1].\n\n9.  Similar to GLEM, this method relies heavily on the quality of pseudo-labels. Although an ablation study is provided, it does not fully address potential issues that could arise with low labeling ratios. \n\n[1] Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs\n\n[2] Efficient End-to-end Language Model Fine-tuning on Graphs"
            },
            "questions": {
                "value": "1. How does the implementation achieve sampling of simpler nodes first?\n2. Are the pretrained language models in the third block in Table 1 fine-tuned with labels?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tackles the text-structure imbalance problem in representation learning on text-attributed graphs. The authors propose a curriculum GNN-LLM alignment method to balance the learning difficulties of textual and structural information."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is clearly written and easy to follow.\n\nThe proposed method demonstrates improved performance."
            },
            "weaknesses": {
                "value": "The novelty of this paper is incremental. It is evident that the ideas of difficulty measurement and curriculum learning are borrowed from existing works, such as [1,2]. Additionally, using loss as a measure of learning difficulty is a well-adopted approach.\n\nThe authors emphasize the text-structure imbalance problem, i.e., the learning difficulties of different nodes vary depending on textual attributes and topological structures. However, they provide only an intuitive example in figure 1 to illustrate this issue, without presenting real experimental results to substantiate the existence of this problem.\n\nIn Table 1, it is unusual that SOTA methods perform worse than the baseline GCN. An explanation for this unexpected phenomenon is needed.\n\nWhat is the computational efficiency of the proposed model? Calculating node difficulty involves computing shortest paths between nodes, which increases computational complexity and may limit the model\u2019s applicability to large-scale datasets.\n\nThe integration of curriculum learning into the training stage is unclear. According to Algorithm 2, the training process appears closely similar to that of the existing method [2].\n\nThe mathematical representation of equations and loss functions lacks clarity. For example:\n\n   a. In Eq. (1), what does \"MLP\" represent? Is it a large language model or a multi-layer perceptron?  \n   b. What is the relationship between $\\hat{y}$ and $f_\\theta(S)$? Are they equal?  \n   c. In Eq. (6), there is no $y_v$. What does $1$ represent? Is it a column vector or a scalar? Additionally, if $D_t$ is larger, does this imply that the loss is also larger?\n\n[1] Mitigating label noise on graph via topological sample selection. \n\n[2] Learning on large-scale text-attributed graphs via variational inference."
            },
            "questions": {
                "value": "Does selecting nodes with lower difficulty improve classification accuracy on nodes with higher difficulty?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents CurGL, a curriculum-based alignment method for Text-Attributed Graphs that introduces a text-structure difficulty measurer, a class-based node selection strategy, and curriculum co-play alignment to iteratively enhance Graph Neural Networks and Large Language Models. The experiments demonstrate the superiority of the method."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The writing is fluent and easily comprehensible.\n- It is one of the earliest works to apply Curriculum Learning in the field of TAGs.\n- The authors propose a class-based hard node selection strategy and a text-structure difficulty measurer, which serve as an inspiration for the community."
            },
            "weaknesses": {
                "value": "- There is a lack of explanation and mathematical proof as to why the EM algorithm is used instead of updating the model parameters of LLM and GNN simultaneously.\n- The experiments in this work are insufficient, as they only include node classification experiments without covering graph classification or link prediction. Additionally, I have some concerns about the current experimental results, see below."
            },
            "questions": {
                "value": "1. Could the authors provide more insights into the selection of the $\\lambda$ and $\\alpha$ values? For different datasets (Cora and Citeseer in Figure 4), they seem to require different values.\n2. In section 4.4, why does the accuracy of pseudo-labeled nodes selected by CurGL decrease as the number of steps increases?\n3. In Section 4.5, why do both of the two components, LLM and GNN, exhibit opposite trends across different datasets?\n4. In the main results, it seems that a simple GCN outperforms models like TAPE and ENGINE."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}