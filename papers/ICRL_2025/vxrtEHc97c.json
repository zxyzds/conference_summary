{
    "id": "vxrtEHc97c",
    "title": "LagEncoder: A Non-Parametric Method for Representation Learning",
    "abstract": "Non-parametric encoders offer advantages in interpretability and generalizability. However, they often perform significantly worse than deep neural networks on many challenging recognition tasks, and it remains unclear how to apply these techniques effectively to such tasks.\nIn this work, we introduce LagEncoder, a non-parametric, training-free feature extraction method based on Finite Element Basis Functions. Our encoder has a universal architecture that can be applied to various types of raw data and recognition tasks. We found that LagEncoder overcomes the limitations of neural networks in regression problems, where they struggle to fit multi-frequency functions. LagEncoder can be used independently to build models, similar to the principles of transfer learning, where only the head is trained\u2014this makes the model converge quickly and requires low training costs.\nAdditionally, LagEncoder serves as an efficient parameter-efficient fine-tuning (PEFT) approach. Our experiments on the ImageNet dataset show that pre-trained models using LagEncoder achieve performance improvements within just one training epoch. Moreover, it does not require adjustments to the original training recipe, and the model's total parameters remain nearly unchanged. Our evaluation of the scaling law for model performance shows that using LagEncoder is more cost-effective than simply increasing model size.",
    "keywords": [
        "Non-parametric encoder",
        "Finite element method",
        "Interpretable model",
        "Universal architecture",
        "Scaling law",
        "ImageNet",
        "ResNet",
        "ViT"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vxrtEHc97c",
    "pdf_link": "https://openreview.net/pdf?id=vxrtEHc97c",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces LagEncoder, a non-parametric, training-free feature extraction method based on Lagrange basis function."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The method have some empirical success in terms of regression and NLP and CV tasks."
            },
            "weaknesses": {
                "value": "1. The figure 4 is really hard to see.\n2. The Computer vision results imporvement is really minor. Considering the extra computation it needed, it doesn't supervise me there is some improvement\n3. The paper doesn't provide a good reason why I want to use this methods."
            },
            "questions": {
                "value": "1. If you still need a trainable backbone, how can you have strong mathematical explainability? Or do you believe your method have better explainability than a trainable linear layer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents LagEncoder, a nonparametric, training-free feature extraction method based on finite element basis\nfunctions. \nThe encoder can be combined with various model architecture withr reasonable performances. \nThe experiments on the ImageNet dataset demonstrate that pre-trained models using\nLagEncoder achieve performance improvements within just one training epoch."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper idea is novel and the results are encouraging."
            },
            "weaknesses": {
                "value": "The writing of the paper is super bad. It is very hard to track different symbols and the symbols sometimes are wrong.\n1. In Eq.(4), the paper introduces p but never defined before or at the equation.\n2. For matrix T, the meaning of the values in the matrix are never defined. In my understanding, each column of the matrix should describe the seven simplices relationship to the corresponding nodes. \n3. The relationship of i,j,k in Eq.(5) is not clearly defined. It takes me time to figure out the meaning of n, n_t and d.\n4. In Algorithm 1, you introduced completely new symbol definitions compared to previous versions, which further decreases the readability of the paper.\n5. In algorithm 2, what is v(i) in compute loss step? I would guess it is x(i)."
            },
            "questions": {
                "value": "1. For the benchmark in NLP, what is the performance if the freedom n of LagEncoder increased?\n2. The paper claims interpretability, which I completely did not see from experiments. I think some attributions to the input is the interpretability.  The experiments in 3.1.2 is unconvincing.\n3. The few epochs extra training is very impressive. However, can this method be combined with original architecture and directly train from scratch? Will that also improve the performance? \n4. For the experiments in table 1, what is the performance change if we increase d and n?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes using the Finite Element Method (FEM) for training-free feature extraction, particularly using the Lagrange basis function. Furthermore, the paper re-derives the Lagrange basis to exploit parallelism in a deep learning setting. The proposed LagEncoder is universal and is demonstrated on regression, image classification, text classification. Since LagEncoder can have high computational demands when dealing with high dimensional data, the paper demonstrates how it can be incorporated as a parameter efficient fine-tuning method. Scaling laws show that LagEncoder can outperform purely scaling model size."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* To the best of my knowledge, application of the Finite Element Method for representation learning is a novel contribution. \n* Other contributions of the paper include tricks to adapt the Lagrange basis to a deep learning setting, e.g., a re-derivation that allows parallelism, incorporating it into PEFT modules. \n* Scaling laws show that incorporating LagEncoder with a negligible amount of parameters can reach the same performance as a scaled-up version of the base model. \n* Experiments are conducted for multiple domains: regression, vision, text classification. One particularly compelling result is matching a 6.13 million parameter word2vec model using only 256 parameters."
            },
            "weaknesses": {
                "value": "* The biggest limitation is addressed by the paper itself. It is extremely expensive to use LagEncoder directly on high dimensional data, which \"restricts its direct application to large-scale datasets.\"\n* For the NLP task, LagEncoder is compared against word2vec which is more than ten years old, limiting the relevancy of this evaluation. Can LagEncoder be incorporated in modern language models?\n* For the vision tasks, the improvements from LagEncoder seem negligible with a fraction of a percent improvement. \n* The paper lacks comparison against other PEFT methods such as LoRA."
            },
            "questions": {
                "value": "* Can LagEncoder be incorporated in modern language models?\n* How does LagEncoder compare against the widely used PEFT method LoRA? The PCA and residual mentioned in Section 2.3 are highly reminiscent of LoRA."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}