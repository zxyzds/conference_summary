{
    "id": "wnT8bfJCDx",
    "title": "Explaining Modern Gated-Linear RNNs via a Unified Implicit Attention Formulation",
    "abstract": "Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art Transformer explainability methods. Our code is attached as a supplement.",
    "keywords": [
        "Explainability",
        "Interpretability",
        "Gated-Linear RNNs",
        "Attention-free",
        "Mamba"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "Explaining Modern Gated-Linear RNNs via a Unified Implicit Attention Formulation",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wnT8bfJCDx",
    "pdf_link": "https://openreview.net/pdf?id=wnT8bfJCDx",
    "comments": [
        {
            "summary": {
                "value": "The paper studies the problem of sequence modelling. The authors aim to provide a unified framework of the recent attention-free methods such as Mamba and RWKV. The paper presents empirical results to validate the proposed unified framework."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The unified framework makes it easier to study and compare the different sequence modelling algorithms.\n- It is important for the community to learn about such work.\n- The experimental results are interesting."
            },
            "weaknesses": {
                "value": "Although it is hard to evaluate such approaches empirically, interpretability-based metrics are not very conclusive in general."
            },
            "questions": {
                "value": "- What is the best method to evaluate the unified framework beside the interpretability analysis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a unified framework that reformulates various gated recurrent neural network (RNN) architectures, such as Mamba, RWKV, and Griffin, into implicit causal self-attention layers. This reinterpretation aims to make these models more interpretable by constructing attention-like matrices for use in visual and NLP explainability tasks. Experimental evaluations demonstrate that this approach achieves competitive performance in robustness and attribution metrics, though prior work has already suggested that certain gated models, including Mamba, are attention-based."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper offers a clear explanation of how implicit attention could be used to interpret gated RNNs, making it accessible to readers interested in explainability across model types.\n\nBy applying the framework to both NLP and vision tasks, the authors demonstrate its cross-domain relevance."
            },
            "weaknesses": {
                "value": "Previous work has already conceptualized models like Mamba as attention-like, meaning that simply reinterpreting these gated RNNs under an implicit attention framework may not be largely novel.\n\nThe paper does not thoroughly compare its implicit attention framework with existing interpretability tools for gated RNNs."
            },
            "questions": {
                "value": "A critical question is: given that previous work has noted the attention-like properties of models such as Mamba, what specific benefits does the implicit attention framework offer over these prior interpretations?\n\nWhat are the current explainability methods or metrics for modern gated-linear RNNs and how's the comparison between them and attention matrices?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper writes out gated recurrent architectures such as Mamba, RKWV, and Griffin as causal self-attention layers. The main objective of this is to increase interpretability, which is tested through perturbation and segmentation tests."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "In my view, this is a pretty complete paper. From my understanding, the authors present an extension of the paper Ali et al. (2024) to include additional architectural components in the linearization and not just the S6 mechanism considered before. The authors show that the model improves interpretability through visualizations and a set of both perturbation and segmentation tests. The ablation gives quite a lot of strength to their arguments, but I am not so familiar with these types of explainability results so I am not able to comment on the details."
            },
            "weaknesses": {
                "value": "While the authors have explored the interpretability side of things extensively, I was wondering if it would be worth comparing the performance of the linearized models compared to its recurrent counterparts when trained on some small datasets?"
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tries to provide an implicit self-attention formulation for most state-of-the art non-transformer sequence models (known as Gated Linear RNNs) such as Mamba, Griffin, RWKV, and ReNet. In this way, it can exploit techniques used in attention explainablity to explain these new models.\n\nCompared to the closest work (Ali et al, 2024), which only formulates the S6 layer in Mamba, the main contribution of paper is:\n- Formulating more layers with implicit self-attention and propose a unified and more accurate attention-based representation for all the SOTA gated linear RNNs.\n\nOther contributions include:\n- Introducing new explainablity technique for these models leveraging the self-attention formulation\n- Showing performance of their explanations and attributions by purturbation testing and segmentation.\n- Showing their proposed formulation can give attributions which can further used in some performance-enhancing techniques (based on in-context learning) for large language models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Covering multiple models including most popular modern non-transformer sequence models.\n- Evaluating the performance of the resulting attributions in multiple quantitate experiments and down-stream tasks (across both vision and NLP).\n- Showing the impact of various sub-layers in the ablations study"
            },
            "weaknesses": {
                "value": "- In their ablation study, the authors could discuss the trade-off between the time explainability and more accurate formulation/explainability.\n\n- The main baseline paper (Ali et al, 2024) has not been published yet. So, it is hard to evaluate this paper. Actually, the performance of the model in downstream tasks such as segmentation and attribution-based performance-enhancement helped me to have better evaluation of the proposed method."
            },
            "questions": {
                "value": "mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}