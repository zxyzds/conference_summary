{
    "id": "QdiMWcwU5w",
    "title": "Dynamic Noise Preference Optimization for LLM Self-Improvement via Synthetic Data",
    "abstract": "Although LLMs have achieved significant success, their reliance on large volumes of human-annotated data has limited their potential for further scaling. In this situation, utilizing self-generated synthetic data has become crucial for fine-tuning LLMs without extensive human annotation. However, current methods often fail to ensure consistent improvements across iterations, with performance stagnating after only minimal updates. To overcome these challenges, we introduce Dynamic Noise Preference Optimization (DNPO). DNPO employs a dynamic sample labeling mechanism to construct preference pairs for training and introduces controlled, trainable noise into the preference optimization process. Our approach effectively prevents stagnation and enables continuous improvement. In experiments with Zephyr-7B, DNPO consistently outperforms existing methods, showing an average performance boost of 2.6\\% across multiple benchmarks. \nAdditionally, DNPO shows a significant improvement in model-generated data quality, with a 29.4\\% win-loss rate gap compared to the baseline in GPT-4 evaluations. This highlights its effectiveness in enhancing model performance through iterative refinement.",
    "keywords": [
        "Preference Optimization",
        "Alignment of LLMs",
        "Self-improvement",
        "Synthetic Data",
        "Noise"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=QdiMWcwU5w",
    "pdf_link": "https://openreview.net/pdf?id=QdiMWcwU5w",
    "comments": [
        {
            "summary": {
                "value": "This paper explores an approach that uses synthetic data to help train a LLM.  The authors argue that human annotated data is both expensive and noisy, and sometimes nosier than generated data.  As such, they use a LLM to help score generated vs human samples.  In addition, to help promote continuous improvement, the authors add additional noise to further boost performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The topic addressed is both important and timely\n2. The solution proposed is intuitive, and makes sense\n3. The specific implementation seemingly differs from some prior work on this same task"
            },
            "weaknesses": {
                "value": "1. The authors only evaluate using a single architecture, and, thus, we don't know if the proposed improvements are only specific to this architecture.\n\n2. The DNPO approach is quite similar to those in LNL, e.g., UNICON [A].  Whether the human or generated sample is the best label can also be framed from an LNL perspective.  As such, the contribution here can be argued as a simple application of this known solution.\n\n3. Adding noise has some similarity to both masked language modeling and methods like excitation backprop [B] or self challenging [C].  While the proposed approach is seemingly different, it isn't clear if those differences are important\n\n4. The gains are seemingly small enough that I would be concerned about their statistical significance.  The authors should provide a statistical test or even just error bars to alleviate this issue\n\n5. The use of a LLM to evaluate the quality of the annotations is completely unconvincing.  One could argue that these language models are simply going to learn similar features, so all this test is doing is validating that this data is generated rather than that it is of a higher quality.\n\n6. The authors cite several prior works that address the same task, but the authors do not compare against most of them.  As such, it isn't clear that the gains reported are significant in comparison to related work.\n\n[A] UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning. CVPR 2022\n\n[B] Top-down Neural Attention by Excitation Backprop. ECCV 2016\n\n[C] Self-Challenging Improves Cross-Domain Generalization. ECCV 2020"
            },
            "questions": {
                "value": "Unfortunately I cannot shortlist my weaknesses to fewer questions.  Each of them would have to be addressed for me to significantly raise my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the author intends to leverage the generated data to improve the performance of LLM. \nSome assumptions are not well verified or have some conflicts."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The main idea is easy to follow. \nThe synthetic data is good to use. \nThe task on boosting LM without human annotations is good for the community to explore."
            },
            "weaknesses": {
                "value": "My main concerns are that some claims are not verified and ambiguous.\n\n1. Some claims contain conflicts.\n- \"Is human-annotated data truly better? \" \nThe author said that \" around 30% of the generated data is of equal or higher quality compared to the human-annotated data.\"\nMore fairly,  according to the number in Figure 1, we also could say that around 80% of the real data is of equal or higher quality compared to the human-annotated data.\nSo human-annotated data is truly better than generated data.\n\n- Only using GPT4o-mini as the data quality metric is questionable. \nI think it would be better to use a fused metric with SSIM, FID and manual check.\n\n- \u201cstagnation\u201d is ambigous. \nFigure 2, I only can see the model overfits the generated data. \nI think \"overfitting\" is better than using \"stagnation\". \nHow about more generated data, since the data generation is free from the human annotation?\nWill the model still overfit the generated data?\n\n2. Ablation studies.\nThe author claims that \"the lack of variation in generated data across iterations, leading to stagnation during model updates.\"\nOne simple method is using the generative model for online data generation.\n\n3. Contribution-1 Dynamic sample labeling (DSL)  is incremental. \nDSL seems to be a simple filtering process? \nI do not see any technical contribution. Similar filterring process has been widely used in the model trainning, such as BLIP.\n\n4. The motivation of  Contribution-2 Noise preference optimization (NPO) is not well-proved.\nFigure 10 is not supposed to be better than the existing methods in Figure 2. \nI think the data distribution only shows the style similarity but does not indicate the good data for training.\nIndeed, Figure 10 is simmilar to Figure 2, but the figure 10 shows more generated data."
            },
            "questions": {
                "value": "Please see the points in Weakness.\n\n1. Some claims contain conflicts. Please explain. \n\n2. Ablation studies are missing. \n\n3. Contribution-1 is incremental. \n\n4.The motivation of  Contribution-2 Noise preference optimization (NPO) is not well-proved.\nIndeed, figure 10 is simmilar to Figure 2, but the figure 10 shows more generated data."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper attmpts to improve the LLM using generated data, and propose a method named DNPO. The authors challenge the assumption that the human-annotated data is superior than the generated data, and devise a dynamic smaple labeling strategy to pick both the human-annotated data and the synthesic data for LLM tuning. Ana an noise preference optimization is proposed to utilize the picked data from DSL. Experiment results show that the proposed method can attain better performance than SPIN."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "using fake data to improve LLM is an interesting  and study-worhty problem.\na clear motivation is presented"
            },
            "weaknesses": {
                "value": "1, Dynamic sampling labeling requires an \"more powerful evaluation model\"  to evaluate the human-generated data and synthesic data,   does this evaluation nedd to be superiror than the LLM model to be improved?  If yes, why do you not directly tune the evaluation model? If no, how do you ensure the given evaluation is credible and indeed pick more valuable data to fine-tune the LLM? \n\n2, SPIN adopts 50K prompts from UltraChat for evaluation, while this paper only picks 20K,  why do not align the amount?\n\n3,  How the amount of the synthesic data effects the final results, I did not observe any discussion regarding this quesion."
            },
            "questions": {
                "value": "Figure 2 x and y-axis are both not clear for me"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}