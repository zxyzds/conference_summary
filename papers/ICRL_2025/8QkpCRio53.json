{
    "id": "8QkpCRio53",
    "title": "Preference Optimization for Combinatorial Optimization Problems",
    "abstract": "Reinforcement Learning (RL) has emerged as a powerful tool for neural combinatorial optimization, enabling models to learn heuristics that solve complex problems without requiring optimal solutions. Despite significant progress, existing RL approaches face challenges such as diminishing reward signals and inefficient exploration in vast combinatorial action spaces, leading to inefficient learning. In this paper, we propose $Preference \n\\ Optimization (PO)$, a novel framework that transforms quantitative reward signals into qualitative preference signals via statistical comparison modeling, emphasizing the superiority among generated solutions. Methodologically, by reparameterizing the reward function in terms of policy probabilities and utilizing preference models like Bradley-Terry and Thurstone, we formulate an entropy-regularized optimization objective that aligns the policy directly with preferences while avoiding intractable computations. Furthermore, we integrate heuristic local search techniques into the fine-tuning process to generate high-quality preference pairs, helping the policy escape local optima. Empirical results on standard combinatorial optimization benchmarks, such as the Traveling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP), demonstrate that our method outperforms traditional RL algorithms, achieving superior sample efficiency and solution quality. Our work offers a simple yet efficient algorithmic advancement in neural combinatorial optimization.",
    "keywords": [
        "Combinatorial Optimization",
        "Reinforcement Learning",
        "Preference-Based Reinforcement Learning"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8QkpCRio53",
    "pdf_link": "https://openreview.net/pdf?id=8QkpCRio53",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes Preference Optimization (PO) framework for neural combinatorial optimization, transforming quantitative reward signals into qualitative preferences. This approach is designed to address issues in RL for combinatorial optimization, such as diminishing reward signals and inefficient exploration. \bFurthermore, integrating local search in training loop improves the quality of generated solutions without additional inference time. Experiments on benchmarks like the Traveling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) show that PO yields better solution quality and sample efficiency than conventional RL algorithms."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1, The paper is well-written, with particularly clear and convincing motivation.\n\n2, The authors successfully adapt Preference Optimization (PO) to combinatorial optimization (CO), not only applying it but also providing a persuasive explanation of why PO is well-suited for CO tasks.\n\n3, The experiments and analyses strongly support the claimed advantages of PO, effectively demonstrating its benefits (better solution quality and sample efficiency)."
            },
            "weaknesses": {
                "value": "1, The experiments are limited to TSP-100 and CVRP-100. Given that recent studies often include tests up to TSP-10000, additional experiments on larger instances would strengthen the findings. Performance limited to TSP-100 seems less impactful.\n\n2, The comparison with baseline models is somewhat limited. Are there no models beyond Pointerformer? It would be valuable to see how PO performs against recent SOTA methods.\n\n3, It\u2019s unclear if integrating local search into the training loop is truly beneficial. Running local search for even 1\u20132 seconds (which seems trivial compared to inference time) could likely yield significant performance improvements. Could the authors provide results with an additional 1\u20132 seconds of local search (e.g., POMO (LS))?"
            },
            "questions": {
                "value": "1. How is  r_{\\theta}  in Equation (8) defined?\n\n2. Can you explain the last paragraph of Section 3.1 in detail? It\u2019s unclear how using PO helps mitigate the inference objective \u2260 training objective problem.\n\n3. Could PO be extended to heatmap-based approaches like DIMES?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a preference-based reinforcement learning method to address combinatorial optimization problems characterized by large action spaces, which are difficult to optimize using only reward signals. Unlike conventional reinforcement learning that aims to maximize expected reward, combinatorial optimization focuses on maximizing the expected maximum reward. This results in an inconsistency between the inference and training objectives, highlighting the need for preference-based optimization. The paper emphasizes the efficiency of optimization through pairwise preference comparisons of solution trajectories generated by local search methods such as 2-opt. Experiments on TSP-100 and CVRP-100 demonstrated that auto-regressive neural solvers based on the AM algorithm (e.g., POMO, Sym-NCO) exhibited smaller performance gaps with the preference optimization objective."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "An interesting aspect of this study is its approach of combining the strengths of local search methods, such as 2-opt, with RL-based neural solvers through preference-based RL. By integrating pairwise preference comparisons between solution trajectories with RL objectives in combinatorial optimization problems, this method demonstrated advantages. Experimental results on TSP-100 and CVRP-100 showed slight improvements in performance."
            },
            "weaknesses": {
                "value": "The main weakness of this paper is that experiments were limited to TSP-100 and CVRP-100, making the results insufficient for comprehensive validation. For problems with 100 nodes, many neural solvers already achieve small gaps. Including results for larger-scale problems, such as TSP-1000 or real-world settings like TSPLIB, would have strengthened the findings. Another limitation is the lack of comparison with state-of-the-art methods like DIMES [1] and DIFUSCO [2]. Demonstrating the effectiveness of the preference optimization algorithm at larger scales with significant performance gains is necessary.\n\n[1] Qiu et al., \"DIMES: A Differentiable Meta Solver for Combinatorial Optimization Problems\", NeurIPS 2022\n\n[2] Sun & Yang, \"DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization\", NeurIPS 2023"
            },
            "questions": {
                "value": "1. The addition of a pairwise preference loss function may alter the original RL objective and potentially compromise the model\u2019s optimality guarantee. Are there any side effects associated with incorporating preference?\n\n2. In CO problems where effective local search algorithms like 2-opt for TSP are not available, how are trajectories for preference comparisons obtained?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce a method called Preference Optimization (PO), which adapts a preference learning framework commonly applied in large language model (LLM) alignment to improve policy learning in deep neural networks for combinatorial optimization. They demonstrate its effectiveness through two types of vehicle routing problems: the Traveling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP).\n\nPO trains the policy neural network with pairs of solutions. Rather than focusing on the qualitative score difference between solutions, PO guides the network to consistently prefer the better option in each pair. This relative comparison enables PO to enhance learning efficiency by emphasizing the ranking of solutions rather than their specific metrics (conventional approach). The authors highlight that this method is especially advantageous in later training stages, where traditional approaches struggle to provide useful feedback as the quality differences between solutions diminish. By maintaining a strong training signal throughout, PO achieves better overall performance.\n\nExperimental results demonstrate that substituting traditional reinforcement learning algorithms, such as REINFORCE, with PO leads to substantial improvements in the solution quality of neural network solvers. Additionally, the authors propose an integrated approach that combines PO with a local search algorithm, yielding further enhancements in solution quality."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The introduction of innovative training algorithms like PO for policy neural networks is exciting and likely to stimulate further research in the field.\n\nThe theoretical foundation for policy optimization using PO is thorough and informative."
            },
            "weaknesses": {
                "value": "The authors hint that the pairwise comparison method does not fully outperform the \"multi-start\" strategy of POMO, with the optimality gap reported in Table 1 appearing to confirm this. Even if the proposed method does not surpass \"multi-start,\" the novelty and value of the preference learning approach are clear. It would, however, be helpful for the authors to include a complete comparison.\n\nThe authors reason that PO should bolster neural network training in the later stages, yet the empirical results appear to indicate otherwise. Specifically, PO seems to accelerate convergence during the early stages rather than providing a late-stage advantage, with limited evidence supporting its efficacy in the later phases. It is possible that pairwise learning introduces additional noise to the policy network compared to learning based on a large number of homogeneous solutions, as seen with the \"multi-start\" approach. Further investigation is necessary to substantiate the authors' claim that PO offers an advantage in the later stages of training."
            },
            "questions": {
                "value": "Given that routing problems like TSP and CVRP allow for efficient generation and evaluation of a large number of candidate solutions, are they the most suitable applications for PO? Might PO prove more effective in other (more realistic) tasks where evaluating candidate solutions is more computationally expensive, making sampling efficiency a more critical factor?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method for training an artificial neural network model to solve combinatorial optimization problems. The authors identify issues with the REINFORCE-based parameter update method, which has been widely used in various existing Neural Combinatorial Optimization studies, and propose Preference Optimization(PO) as a solution. Furthermore, the authors present a method that integrates the proposed PO method with existing local search techniques for training. They apply the proposed method to AM, POMO, Sym-NCO, and Pointerformer, demonstrating better results in TSP and CVRP compared to traditional REINFORCE-based optimization methods. The authors also report strong performance in generalization experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- The authors propose Preference Optimization (PO) as an optimization method for Neural Combinatorial Optimization (NCO) models. They demonstrate, through experiments on TSP100 and CVRP100, that applying the proposed PO to existing studies such as AM, POMO, Sym-NCO, and Pointerformer leads to improved model performance.\n\n- The proposed Preference Optimization method in this paper is particularly noteworthy in that it can be applied to various existing NCO studies to enhance their performance. It is expected to be applicable to a wide range of future NCO research.\n\n- In generalization experiments, the model trained with Preference Optimization also outperformed the model trained with the REINFORCE-based optimization method.\n\n- Through experiments such as Advantage Assignment, Consistency, and Trajectory Entropy, the paper effectively analyzes the superiority of the proposed PO method.\n\n- The paper provides a thorough theoretical derivation process for the proposed Preference Optimization method.\n\n- Overall, this paper is well-written."
            },
            "weaknesses": {
                "value": "- In the Table 1 experiments, PO was applied to four existing studies, but the experiments were conducted only on a single problem size for two routing problems, TSP and CVRP. The experimental conditions are restricted to routing problems and a problem size of 100. This paper does not provide experimental results to verify the effectiveness of Preference Optimization for larger problem sizes or for other types of problems beyond routing.\n\n- The y-axis scales in Figure 1 and Figure 4 are inconsistent, making it difficult to assess the extent to which PO outperforms RL at each epoch stage, particularly regarding improvements on the diminishing reward signal issue."
            },
            "questions": {
                "value": "- Considering the case of training POMO (with PO) following Algorithm 1, it seems that SAMPLINGSOLUTION() is performed according to POMO\u2019s multiple starting points, and the shared baseline from POMO (Kwon et al., 2020) is not expected to be used. Please confirm whether this understanding is correct.\n\n- According to Appendix D, it appears that the Finetune step in POMO (Finetune) was performed for 5% of the epochs. Please clarify the additional training time (or resources) incurred due to this finetuning process.\n\n- In the case of Local Search, even when considering the training time/resources required due to Local Search overhead, is it still beneficial to perform LS? Alternatively, if training with the same amount of resources, would it be more advantageous to train for more epochs without LS? What are the authors' views on this?\n\n- In Equation (3), $\\alpha$ is a parameter that determines the weight of entropy regularization. What does $\\alpha$ represent in Equation (8)? Is it the same as in Equation (3)?\n\n- The $\\alpha$ values used for TSP100 and CVRP100 experiments are different. Please explain why there are different and how they were set. Does the $\\alpha$ value affect the learning efficiency? If there is a change in learning efficiency, how does it vary with changes in the $\\alpha$ value? If applying Policy Optimization to problems other than TSP100 and CVRP100, what would be a good way to set the $\\alpha$ value? Is there a general range or any reference information that could be helpful for setting the value?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}