{
    "id": "2vHIHrJAcI",
    "title": "Revisit the open nature of open vocabulary segmentation",
    "abstract": "In Open-Vocabulary Segmentation (OVS), we observe a consistent drop in model\nperformance as the query vocabulary set expands, especially when it includes se-\nmantically similar and ambiguous vocabularies, such as \u2018sofa\u2019 and \u2018couch\u2019. The\nprevious OVS evaluation protocol, however, does not account for such ambiguity,\nas any mismatch between predicted and human-annotated pairs is simply treated\nas incorrect on a pixel-wise basis. This contradicts the open nature of OVS, where\nambiguous categories can both be correct from an open-world perspective. To\naddress this, in this work, we further study the open nature of OVS and pro-\npose a mask-wise evaluation protocol thatis based on matched and mismatched\nmask pairs between prediction and annotation respectively. Extensive experimen-\ntal evaluations show that OVS models consistently perform better under the pro-\nposed mask-wise protocol compared to the previous pixel-wise one. Moreover,\nanalysis of mismatched mask pair reveals that large amount of ambiguous cate-\ngories exist in commonly used OVS datasets. Interestingly, we find that reducing\nthese ambiguities during both training and inference enhances zero-shot inference\ncapabilities. These findings and the new evaluation protocol encourage further\nexploration of the open nature of OVS and broader open-world challenges.",
    "keywords": [
        "Open vocabulary segmentation",
        "Evaluation"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=2vHIHrJAcI",
    "pdf_link": "https://openreview.net/pdf?id=2vHIHrJAcI",
    "comments": [
        {
            "summary": {
                "value": "This study proposes new evaluation metrics for Open-Vocabulary Segmentation (OVS) tasks. A key limitation of evaluating OVS methods on fixed-category datasets is that traditional image segmentation metrics may misclassify visually similar objects as errors, even when they are semantically related but belong to different categories. This issue intensifies with an increasing number of category labels in the test dataset. This issue becomes more pronounced as the number of category labels in the test data increases. Previous research has addressed this challenge, resulting in improved metrics such as Open-mIoU and SG-IOU. The central premise of this work is to focus evaluation on mask similarity rather than textual similarity."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The primary contention of this manuscript is to shift the focus of evaluation from textual to mask similarity in assessing OVS models. The authors have identified a gap in the current assessment metrics, which are deemed inadequate for evaluating OVS models, and have proposed a novel metric to address this issue."
            },
            "weaknesses": {
                "value": "The manuscript exhibits a lack of clarity and organization in its writing."
            },
            "questions": {
                "value": "Q1: The analysis in Section 3 appears disconnected from subsequent sections.\n\nQ2: In Figure 2, $\\mathbb{A}$ represents a set of predicted binary masks. How are the predicted masks in $\\mathbb{B}$ and $\\mathbb{C}$ derived from $\\mathbb{A}$? If they are matched to GT masks based on IoU using bipartite matching, it seems Figure 2 suggests that the number of predicted masks by the model exceeds that of the ground truth, which is not realistic. Additionally, predicted masks in $\\mathbb{B}$ and $\\mathbb{C}$ should not overlap according to $\\mathbb{C} = \\mathbb{A} \\backslash \\mathbb{B}$.\n\nQ3: The correlation between Algorithm 1 and Section 4 is weak: For example, (1) The CM is not referenced outside the Algorthm 1. (2) The calculations for the core evaluation metrics -- front, back, and errors -- are not represented in Algorithm 1 or any other equations. (3) How is the best threshold $\\tau^*$ used in Algorithm 1? \n\nQ4: What constitutes a good evaluation metric? The last sentence of the introduction (line 83 on page 2) implies that the authors equate higher performance values with better evaluation metrics, which is unreasonable. \nIn Figure 3, the authors seem to suggest that more stable evaluation metrics are preferable; however, this should also be compared with other metrics like Open-mIoU and SG-IoU."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The performance of Open Vocabulary Segmentation (OVS) models will decrease as the query vocabulary size increases, especially when semantically similar category names are present, contradicting the original purpose of OVS. To address this, the authors proposed a mask-wise evaluation protocol based on match/mismatch between prediction and annotation mask pairs, avoiding forced category matching. Key innovations include reducing ambiguity and constructing an ambiguous vocabulary graph. Comprehensive experiments and analysis reveal numerous ambiguous categories in current OVS datasets. Utilizing the proposed protocols during the training and testing stages can help to improve the model\u2019s zero-shot inference capability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Good motivation, authors pointed out the current OVS evaluation sets have many semantic similar categories, which may influence the training&testing stages of model, which further influence the inference ability of current OVS methods. Based on this, authors proposed a new evaluation protocols to alleviate this issue.\n\n2. The whole paper is relatively clear and easy to follow.  \n\n3. Very comprehensive experiment results on multiple datasets and multiple OVS methods."
            },
            "weaknesses": {
                "value": "Writing Suggestions: \n\n1. In the Abstract, authors claim that OVS models perform better under the new mask-wise protocol needs further clarification. To make fair comparisons between the mask-wise and pixel-wise protocols, the authors should add more details about how they determine \"better\" performance. Providing such details would help readers understand the basis for this improvement claim.\n\n2. In the Abstract, the phrase \u201cenhances zero-shot inference capabilities\u201d likely refers to the capabilities of OVS models. Clarifying this would improve readability. \n\n3. Given the similarity between open-vocabulary segmentation and open-vocabulary semantic segmentation, the authors should add a brief section comparing these two concepts. Highlighting key differences in their applications or objectives would help avoid potential confusion and clarify the unique focus of their work.\n\n4. For Equation (5), the authors should provide more detailed motivation for choosing this to determine the best threshold, rather than simply listing the source. It would be helpful if they could explain why this method was selected over alternative approaches and how it specifically benefits their evaluation protocol.\n\n5. The equation at lines 324 to 327 is missing a number."
            },
            "questions": {
                "value": "1. A significant concern is that the proposed evaluation protocol relies on having sufficient data to identify semantically similar categories. In real-world applications, if the training data lacks adequate masks to differentiate similar categories (e.g., \"sofa\" and \"couch\"), the protocol may struggle during testing. To address this, it would be helpful if the authors could analyze the performance of their method with limited training data or provide insights into the minimum data requirements necessary for effective improvement. Additionally, experiments or discussions on the robustness of data scarcity and the impact of potentially misleading information would strengthen the evaluation.\n\n\n2. While the authors' approach to handling ambiguities through the visual modality is quite interesting, it may be more intuitive to identify similar categories based purely on semantic meaning. For instance, using the text modality to assess semantic similarities could potentially provide greater improvements than relying solely on visual information. To explore this, it would be valuable for the authors to compare their visual-based approach with a text-based semantic similarity approach. Or add more discussions about the potential advantages and disadvantages of incorporating textual semantic information into their method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper gives a deep observations on open-vocabulary semantic segmentation. To address the ambiguous category issue, the authors propose mask-wise evaluation protocol and a confusion vocabulary graph for open-vocabulary datasets. The experiments validate method defectiveness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper presents an interesting analysis on the openness of open-vocabulary semantic segmentation.\n\n2. The mask-wise evaluation protocol sounds reasonable.\n\n3. The experiments are conducted on multiple existing methods."
            },
            "weaknesses": {
                "value": "1. The quality of ambiguous vocabulary graph seems important for performance. Currently, the related experiments are not enough. I think it is better to provide more experiments to verify the quality of ambiguous vocabulary graph.\n\n2. The accuracy for front and back is not very clear. I suggest that the authors give an equation to explain it.\n\n3. The comparison of whether reducing ambiguities during training or not is necessary."
            },
            "questions": {
                "value": "Please refer to weakness.  It is important to give more experiments for ambiguous vocabulary graph and more comparsion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}