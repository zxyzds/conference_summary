{
    "id": "vWRwdmA3wU",
    "title": "Differentiable Optimization of Similarity Scores Between Models and Brains",
    "abstract": "What metrics should guide the development of more realistic models of the brain? One proposal is to quantify the similarity between models and brains using methods such as linear regression, Centered Kernel Alignment (CKA), Normalized Bures Similarity (NBS), and angular Procrustes distance. We find that a \"good\" value for a similarity score is not fixed but varies depending on the similarity measure and the dataset. To better understand the limitations of these similarity measures we analyze neural activity recorded in five experiments on nonhuman primates, and optimize synthetic datasets to become more similar to these neural recordings. How similar can these synthetic datasets be to neural activity while failing to encode task relevant variables? We find that CKA and some variations of cross-validated and regularized linear regression, differ from angular Procrustes, and yield high similarity scores even when task relevant variables cannot be linearly decoded from the synthetic datasets. Synthetic datasets optimized to maximize similarity scores initially learn the highest variance principal component of the target dataset, but angular Procrustes captures lower variance dimensions much earlier than methods like CKA. We show in both theory and simulations how these scores change when different principal components are perturbed. And finally, we jointly optimize multiple similarity scores to characterize their allowed ranges, and reveal that a high angular Procrustes similarity, for example, implies a high CKA score, but not the converse.",
    "keywords": [
        "similarity measures",
        "representational alignment",
        "procrustes distance",
        "centered kernel alignment",
        "linear regression"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "Not all metrics for representational alignment are created equal; we show limitations in similarity metrics between models and brains by maximizing similarity with gradient descent.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vWRwdmA3wU",
    "pdf_link": "https://openreview.net/pdf?id=vWRwdmA3wU",
    "comments": [
        {
            "summary": {
                "value": "This paper studies several popular methods to quantify the similarity between models and neural data by applying them to five neural data from several studies. The approach is to directly optimize synthetic datasets to maximize their similarity to neural recordings.  The work is of expository nature and there have been several reviews on similarity measures, but this work is model-agnostic and can shed light on how different metrics prioritize various aspects of the data, such as specific principal components or task-relevant information."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Similarity measures have played a pivotal role in guiding the development of more realistic models of the brain. This work provides new insights and challenges of such measures."
            },
            "weaknesses": {
                "value": "This work is of expository nature, so by this nature its advancement in methodology and theory is less significant."
            },
            "questions": {
                "value": "1. What guidance will you provide to scientists in choosing a suitable similarity measure? \n\n2. How will this work have impact in the way that similarity scores are applied in practice?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper aims to compare various similarity measures by optimizing randomly initialized datasets to various neural recording datasets. The authors find that some measures such as CKA can have high scores without sufficiently encoding task relevant information. The paper then investigates how much of a dataset needs to be captured before a certain score is achieved. It finds that some of the measures that have high scores without encoding task-relevant information also are most sensitive to high variance principal components. The authors complete theoretical and perturbation experiments to validate this hypothesis."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Very Clear writing, easy to see what analysis is being done and why.\n2. Evaluating in a model agnostic way puts the focus on the measures and leads to a better understanding of the relevant differences for completing model-brain comparisons.\n3. This analysis is fundamental to the field. Understanding what aspects lead to a high similarity score is extremely important to guide development of new models and to properly apply the modeling results to the brain."
            },
            "weaknesses": {
                "value": "1. Ridge-Regression seems to be the most widely-used measure in the field although most of the comparisons focus on CKA vs Angular Procrustes. Would be nice to see more commentary on this especially in Fig. 7 where it seems independent from angular Procrustes.\n2. From the start of the paper it seems like it will answer the question: \"What metrics should guide the development of more realistic models of the brain?\" The discussion seems to attempt to avoid this question: \"Our findings demonstrate that the interpretation of these scores is highly dependent on the specifics of both the metric and the dataset. We do not claim that one metric is superior to another, as indeed, they are sensitive to different aspects of the data and in some cases can be largely independent. Rather, we emphasize that the concept of a \"good\" score is nuanced and varies with context.\" I would like the authors to comment more directly on what should be done. Should this style of analysis be done for every new dataset which can provide a score range that encodes certain relevant variables? Is there some other guideline? It makes sense that there isn't one best choice but the question that starts off the paper doesn't seem to be addressed.\n3. The datasets are all electrophysiology datasets whereas comparisons are often also done with fMRI datasets, will these results still hold for these datasets? Especially with the difference in sampling between the methods."
            },
            "questions": {
                "value": "1. Do the authors have suggestions on how to use these measures? Or do they have a suggestion of what analysis is still needed before picking a measure?\n2. Why does ridge regression seem to be independent from Angular Procrustes (Fig 7)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to compare how different similarity measures such as CKA and linear regression behave, based on both prior theoretical work as well as by optimizing synthetic data through Adam to become more similar (under some similarity measure, e.g. CKA) to a reference neural dataset. The paper analyzes what properties a synthetic dataset can be expected to have (e.g. with respect to decodability of task relevant variables) at various levels of similarity to a reference dataset under a range of similarity measures."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Although optimizing a set of features to become more similar to neural data has been done (e.g. optimizing neural network models of the brain), specifically optimizing a synthetic dataset to in order to gain insight into how similarity measures behave, especially at various intermediate levels of similarity, is novel. \n- Most discussions of similarity measures have focused on the special case where the similarity score is 1 (for example, what happens when response profiles X and Y are equivalent under some similarity measure such as CKA), so discussion of how intermediate values behave for different measures is a good contribution, especially since we are often dealing with intermediate levels of similarity in practice, e.g. when comparing models to brain data.\n- CKA and linear regression are widely used methods of measuring similarity, so this paper can potentially be useful to many researchers comparing models to brain data."
            },
            "weaknesses": {
                "value": "- In this paper, the regularization level for Ridge Regression is fixed to some chosen level (and the authors do consider results for different fixed levels of lambda). However it seems to me that, because of the probability of overfitting in high dimensional data settings, it is generally preferable to tune the ridge penalty through some cross-validation method (searching over a range of possible alpha values) such as k-fold so as to select the lambda that will maximize generalization performance on the chosen data.\n- Linear regression is only done in one direction, from the model to the reference neural dataset. This is good to know about, but it also would be useful to see what happens when linear regression is done in both directions, i.e. if synthetic data is optimized so that it predicts the brain and the brain predicts the data as well. \n- RSA is also widely used as a similarity measure, but is not mentioned at all in the paper. It would be very useful if the paper included an analysis of RSA, especially since RSA is mathematically very closely related to linear CKA, but the formulas for RSA and linear CKA are not identical. It would be good to therefore have an analysis of the relationship between these two methods, as well as empirical simulations showing how the intermediate values compare for RSA and CKA (just as the authors did for other methods, like comparing CKA to NBS).\n- While many parts of the paper are clearly written, Figures 5 and 6 were hard for me to understand, and there was not much explanation in either the caption or main text. See questions below.\n- While I understand the intended application of these results is to help researchers better understand similarity scores when comparing models to brains, the paper title seems a bit misleading, since it gives the impression that the paper is optimizing the similarity between an actual ANN model and the brain, whereas what is actually done here is optimizing similarity scores between a randomly initialized matrix and the brain features. Perhaps the title doesn't need to be fixed, but initially the title gave me a different idea of what the paper was going to do."
            },
            "questions": {
                "value": "In Figure 5, what does PC explained variance mean? What is the PC threshold? (I'm also not sure which PC we are talking about here - is it the first, largest PC?) Why is it that the score to reach PC threshold is *larger* for a smaller PC explained variance? Shouldn't explaining less variance require a smaller score? \n\nIn Figure 6, what does it mean to perturb a single PC? How much do you perturb that PC? (it isn't stated, but I assume that how much you perturb it is very important for what the resulting similarity score should be)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the properties of several similarity metrics in various neural activity datasets. The goal of similarity metrics is to quantify how well models of brain align with neural data. However, there are inconsistencies across different metrics, i.e., some metrics score high while others score low. This paper aims to address this inconsistency problem and propose a model-agnostic synthetic dataset optimization to analyze the properties of similarity metrics. The optimization dynamics in numerical experiments reveal that there is no single metric that is universally applicable for all dataset since the concept of a good score is highly dependent on the dataset. Additionally, the authors provide a python package that includes various similarity metrics."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The problem is well-formulated in the introduction and clearly illustrated in Figure 2.\n* Numerical experiments are presented clearly for the reader.\n* The published code is well-structured, enhancing reproducibility.\n* The observations made in Figure 3 are interesting (that some scores are good for some datasets while they are bad for other datasets)."
            },
            "weaknesses": {
                "value": "The paper has some clarity and novelty issues in my opinion. Please see the points below and the questions section.\n\n* One premise stated in the abstract is that the paper offers a theoretical analysis to show how similarity metrics are dependent on the principal components of the dataset. However, this premise appears weak to me because: (i)  it does not seem to be a novel analysis but rather a predictable outcome of using Frobenius and nuclear norms in metrics CKA and NBS; (ii) the assumption $\\langle u_X^i, u_Y^i\\rangle \\approx 0$  is introduced without sufficient context and is unclear; and (iii) the assumption is said to hold with large sample sizes, validated in a numerical experiment, yet I did not see a clear mention of dataset sizes in the paper. Including more details on the datasets and clarifying the underlying assumptions would be helpful.\n\n* The introduction and related work section suggest that prior research lacks practical guidance on metric selection given a dataset. However, I am uncertain if this paper proposes such a guidance. Suppose I have a neural dataset $X$ and model representations $Y$ to compare. How should I choose the most suitable metric based on this paper? My understanding is that I can optimize a synthetic dataset $Z$ using various metrics, observe the optimization dynamics, and then choose a similarity metric for $X$ and $Y$. Is that correct? I am asking this since I am struggling to understand how this paper offers a method for selecting an appropriate metric for a given task, if indeed it is promising.\n\n* The claim between lines 267-270 is not detailed enough in the paper. The authors mention testing a hypothesis, but they simply state \"we tested this hypothesis ... but this did not change the results\" without further context. The results for that is not shared in the paper. Including these results in the appendix would enhance the paper's clarity.\n\n* The joint optimization method in Section 4.4 and Appendix C.3 is unclear, as the details on experiments in this section are sparse. I think the paper can benefit from more details.\n\n* The term \"Proof\" in Appendix C.2 and Section 4.3 seems a bit strong without an accompanying theorem or lemma, especially since the assumption is only noted in the appendix. Revising the word \"proof\" might be appropriate.\n\n**Minor Comments** \n\n* The sentence in line 417 is repeated; it was already mentioned that Williams et al. (2021) advocate taking the arccos of CKA to align with distance metric axioms.\n\n* Appendix B.1 could provide more dataset details rather than referring readers to other works. Including information such as dataset dimensions and data collection methods would be helpful."
            },
            "questions": {
                "value": "* What is the reason for using ridge regularization in the $R^2$ definition (in the numerator in line 208)? In that case, the numerator will not be the residual square and I do not see the rationale behind this. \n\n* Why is line 512 specifically bold-faced, but not the next one? According to Figure 7, the relation that high value of angular Procrustes implies a high score for linear regression appears more established compared to the relation of angular Procrustes and CKA scores.\n\n* Given the findings, the main takeaway seems to be that similarity metrics are highly sensitive to different data aspects and may be mutually independent. How, then, would the authors suggest selecting the best similarity metric for a given dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}