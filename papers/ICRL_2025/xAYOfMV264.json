{
    "id": "xAYOfMV264",
    "title": "A Dual-Agent Adversarial Framework for Generalizable Reinforcement Learning",
    "abstract": "Recently, empowered with the powerful capabilities of neural networks, reinforcement learning (RL) has successfully tackled numerous challenging tasks. However, while these models demonstrate enhanced decision-making abilities, they are increasingly prone to overfitting. For instance, a trained RL model often fails to generalize to even minor variations of the same task, such as a change in background color or other minor semantic differences. To address this issue, we propose a dual-agent adversarial policy learning framework, which allows agents to spontaneously learn the underlying semantics without introducing any human prior knowledge. Specifically, our framework involves a game process between two agents: each agent seeks to maximize the impact of perturbing on the opponent's policy by producing representation differences for the same state, while maintaining its own stability against such perturbations. This interaction encourages agents to learn generalizable policies, capable of handling irrelevant features from the high-dimensional observations. Extensive experimental results on the Procgen benchmark demonstrate that the adversarial process significantly improves the generalization performance of both agents, while also being applied to various RL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial framework, the RL agent outperforms the baseline methods by a significant margin, especially in hard-level tasks, marking a significant step forward in the generalization capabilities of deep reinforcement learning.",
    "keywords": [
        "Generalizable Reinforcement learning",
        "Adversarial Learning"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "The paper proposes a dual-agent adversarial framework for generalizable reinforcement learning.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=xAYOfMV264",
    "pdf_link": "https://openreview.net/pdf?id=xAYOfMV264",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a dual-agent adversarial policy learning framework to address generalization gaps in reinforcement learning (RL). The authors first derive a lower bound on generalization performance, showing that optimizing this bound corresponds to constrained optimization at each RL step. They then leverage some approximations, leading to the dual-agent adversarial framework proposed in this paper. It employs two identical policy networks that are updated alternately to minimize reliance on irrelevant features through a combined loss function, which includes both the primary task loss and a new adversarial loss that includes both adversarial attacks of the other and robust defences of itself. Experiments on the ProcGen benchmark show that this approach outperforms PPO and DAAC baselines in generalization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow, with an effective and engaging presentation. The paper begins with a theoretical analysis, systematically deriving the motivation and key designs, and ultimately showing good results."
            },
            "weaknesses": {
                "value": "My main concern is the evaluation. Although the performance is promising, the paper lacks in-depth analysis and extensive discussion on several aspects.\n\nCurrently, it seems that even without generalization, the proposed method is showing good performance. Thus it is unclear if it is due to better convergence or better generalization capability. We should be careful about this when drawing the conclusion that the proposed method has better generalization performance. And it would be helpful to add some experiments/discussion to compare only the generalization if the two baselines have similar in-distribution performance. Also, one needs to check if the comparison is fair or not. It would be helpful to report the wall clock time and number of gradient steps between the proposed method and the baseline.\n\nSecond, the approach is evaluated on only one generalization setup. However, it is unclear how challenging such a generalization setting is. It would be beneficial to conduct further analysis, assessing the degree to which the proposed algorithm enhances generalization across varying levels of difficulty, such as easy, moderate, and challenging generalization cases.\n\nThird, please also consider adding the training complexity compared to baseline methods, as well as a discussion on the impact of hyperparameter $\\alpha$.\n\nAdditionally, the authors may want to compare and discuss their method relative to other approaches aimed at enhancing RL generalization, such as [1][2]. Given that these methods may also align with the two characteristics described in Section 4.2, further elaboration on the distinctions or similarities would strengthen the paper.\n\n* [1] MaDi: Learning to Mask Distractions for Generalization in Visual Deep Reinforcement Learning\n* [2] Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning\n\nIn Section 2, the introduction of the concept of MDP state semantics and the subscript $\ud835\udc5a$ in the MDP notation is not well-motivated or clearly explained. Consider improving clarity by first defining the distribution $\ud835\udc5d\ud835\udc40$ explicitly and then introducing $m$. Furthermore, the limitations and future works should be discussed in the paper."
            },
            "questions": {
                "value": "* Will the theorem hold if $\ud835\udc40_{train}$ is unbounded? In RL, policies typically interact with the environment on the fly, allowing for the gathering of infinite samples.\n* Are the two characteristics discussed in Section 4.2 sufficient to ensure robust generalization performance? If not, what additional considerations could be relevant?\n* Are any weights needed in Equation 19? If not, why?\n* Would it be beneficial to consider heterogeneous encoders in the proposed method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel adverserial learning framework that involves a minimax game process between two homogeneous agents to  improve the generalization capability of these agents in RL. This framework integrates with existing RL algorithms such as PPO, leverages no additional human prior knowledge which can lead to poor robustness in generalization and has minimal hyperparameters allowing for effective applicability. The authors additionally derive lower bounds for the training and generalization performance of the agent and show that by minimizing the policy's robustness to irrelevant features, one can improve generalization performance. The authors evaluate their framework in the ProcGen environment, showing gains over algorithms such as PPO and DACC."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Empirically show a significant improvement over prior work in the ProcGen environment with their adverserial learning framwork\n- Provide theoretical insights about how a policy's robustness to irrelevant features improves generalization performance which is a novel contribution that can be generally applied to any algorithm."
            },
            "weaknesses": {
                "value": "- Several works such as DRAC and RARL consider a multi-agent/adversarial optimization process in RL. Would be good to include an extensive evaluation of these approaches as baselines and contextualize the novelty of your approach with respect to each baseline.\n- The method is primarily evaluated in the ProcGen environment and could benefit from additional empirical evaluation with a larger set of RL benchmarks to further evaluate the efficacy of the approach.\n- GANs and other adversarial optimization techniques commonly have issues with mode collapse, vanishing gradients and convergence issues, which all make optimization more difficult. Though this is controlled with the parameter $\\alpha$, would be good to consider the tradeoff of the robustness to adversarial threats and the performance of the agent."
            },
            "questions": {
                "value": "- One claim is that the method can be widely used with a variety of algorithms. Would you be able to share results on how this approach transfers with different Online RL algorithms in the ProcGen benchmark?\n- Would you be able to provide some qualitative analysis of the representations learned by your framework compared with those of PPO and DACC to further validate the claim that robust representations are being learned?\n- Could you share results of the sensitivity of $\\alpha$ and the selection criterion for it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a dual-agent adversarial framework to improve generalization in reinforcement learning (RL). In this setup, two agents interact adversarially, each attempting to disrupt the other's policy while maintaining stability in its own. This competition drives both agents to develop robust and generalizable strategies. The framework is efficient, adding only one hyperparameter, and shows strong performance improvements in challenging environments, especially when used with standard RL algorithms like PPO. This approach offers a promising solution for enhancing RL generalization without relying on complex data augmentations or human-designed biases."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. **Solid Theoretical Background**: The approach is backed by strong theory, clearly explaining how it supports RL generalization.\n2. **No Human Bias in Addressing Generalization**: The method achieves generalization without relying on human biases, such as hand-designed augmentations.\n3. **No Extra Network Parameters**: The framework achieves its goals without adding network parameters, relying on just one hyperparameter for flexibility.\n4. **Novel Idea**: The dual-agent adversarial setup is an innovative way to tackle RL generalization.\n5. **Strong Performance**: The approach performs well across tested environments, demonstrating robust generalization and effectiveness."
            },
            "weaknesses": {
                "value": "1. **Limited Environments and Baselines**: Testing is somewhat limited in environments and baseline comparisons. Adding diverse environments, such as DMC-GB[1], and competitive baselines like PIE-G[2], SVEA[3], and ARPO[4] would provide a more complete comparison and further demonstrate the model's capabilities.\n\n[1] Generalization in reinforcement learning by soft data augmentation., Hansen et al., ICRA 2021\n\n[2] Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning., Yuan et al., NeurIPS 2022\n\n[3] Stabilizing deep q-learning with convnets and vision transformers under data augmentation., Hansen et al., NeurIPS 2021\n\n[4] Adversarial Style Transfer for Robust Policy Optimization in Deep Reinforcement Learning., Rahman et al., ArXiv 2023"
            },
            "questions": {
                "value": "Please refer to the Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The submission proposes an adversarial framework that involves a game process between two agents: each agent seeks to maximize the impact of perturbing the opponent\u2019s policy by producing representation differences for the same state, while maintaining its own stability against such perturbations. The submission conducts experiments in the ProcGen environment with 3 random seeds in 8 different games and provides comparison against DAAC and PPO."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Generalization in deep reinforcement learning is a highly important research direction."
            },
            "weaknesses": {
                "value": "The theoretical claims of the submission follow almost immediately from previous work, and do not bring any additional new knowledge. \n\nTable 1 should include standard deviations. Three random seeds is relatively small to interpret the results reported. The results reported in Table 1 and Figure 5 are contradictory. Table 1 reports that DAAC in climber is 3.299 and PPO + Adv. (Agent 1) is 4.473. However, Figure 5 clearly reports that DAAC performance as the highest. How is this possible?\n\nWhy is it only compared to DAAC and original PPO? There are more studies on generalization in deep reinforcement learning. \n\nIn the DAAC paper there is another algorithm called IDAAC that performs better. Why is the algorithm IDAAC not included in the comparison?\n\nI would also recommend checking page 9 and page 10 of the ProcGen section of paper [1]. In particular, the paper [1] states for ProcGen that: \n\n*\u201cWe note that a number of improvements reported in the existing literature are only 50 \u2212 70% likely.\u201d*\n\nFurthermore the paper [1] states:\n\n*\u201cInstead, we recommend using normalization based on the estimated minimum and maximum scores on ProcGen and reporting aggregate metrics based on such score.\u201d*\n\nAs it has been reported in [1] and [3], the performance of PPG [2] is also quite high. It might be good to include PPG in the comparison baseline.\n\nProcGen seems to have 16 tasks. Both of these papers [1,2] test across the 16 games in the ProcGen environment. The submission tests their proposed algorithm in only 8 of them.\n\n[1] Deep Reinforcement Learning at the Edge of the Statistical Precipice, NeurIPS 2021.\n\n[2] Phasic Policy Gradient, ICML 2021.\n\n[3] Decoupling Value and Policy for Generalization in Reinforcement Learning, ICML 2021.\n\nMore recent techniques report substantially higher scores in the ProcGen environment [1,2].\n\n[1] DRIBO: Robust Deep Reinforcement Learning via Multi-View Information Bottleneck, ICML 2022.\n\n[2] Explore to Generalize in Zero-Shot RL, NeurIPS 2023.\n\n\nHow adversarial learning is mentioned in the introduction is incorrect. In the introduction it is stated that: \n\n*\u201cAdversarial framework facilitates the development of agents capable of adapting to new environments by emphasizing the distinction between relevant and irrelevant information.\u201d*\n\nby referring to these studies [1,2,3] as adversarial learning \n\n[1] State-Adversarial DQN for robust deep reinforcement learning, NeurIPS 2020.\n\n[2] Robust adversarial reinforcement learning, ICML 2017.\n\n[3] Robust Deep Reinforcement Learning through Adversarial Loss, NeurIPS 2021.\n\nHowever, recent studies demonstrated that in fact adversarially trained policies cannot generalize, and furthermore the generalization skills of standard reinforcement learning training is substantially higher [1]. \n\n[1] Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness, AAAI 2023.\n\n\nSince the submission proposes an adversarial training method it would have been good to test against adversarial examples as well. It does not have to be the most state-of-the-art adversarial attacks, but still it would have been good to include for reference.\n\n\nAnother thing I want to mention is that by employing the proposed adversarial learning framework the number of encoder parameters that needs to be trained is in fact doubled. This brings a new set of questions. Is it really a fair comparison to lower capacity models as previous ones? Would the prior methods perform also well if we simply just increased the parameters in the encoder?"
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a dual-agent adversarial framework aimed at improving the generalization capabilities of reinforcement learning (RL) models, which often struggle with overfitting and fail to adapt to minor variations in tasks. The proposed framework facilitates a game process between two agents that learn to perturb each other\u2019s policies while maintaining their own stability, enabling them to focus on relevant features in high-dimensional observations. Extensive experiments on the Procgen benchmark demonstrate that this adversarial approach significantly enhances the agents\u2019 performance, especially in challenging environments, outperforming traditional RL algorithms like Proximal Policy Optimization (PPO). Additionally, the authors theoretically prove that reducing an agent\u2019s robustness to irrelevant features can improve its generalization performance. Overall, the study marks a significant advancement in addressing generalization challenges in deep reinforcement learning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The introduction of a dual-agent adversarial framework is an innovative approach that addresses the pressing issue of overfitting and generalization in reinforcement learning, offering a new perspective on how agents can improve adaptability in varying environments.\n- The paper provides a strong theoretical foundation, proving that reducing an agent\u2019s robustness to irrelevant features can lead to better generalization, enhancing the depth of the contribution.\n- The experiments conducted on the Procgen benchmark show significant performance improvements over existing methods like PPO, demonstrating the effectiveness of the proposed framework in real-world, challenging tasks.\n- By focusing on reducing overfitting and enhancing generalization, the paper addresses a critical gap in reinforcement learning research, providing solutions applicable to broader, more complex environments.\n- The framework is well-designed to scale across different environments, making it applicable to a wide range of RL tasks with high-dimensional observations."
            },
            "weaknesses": {
                "value": "- While the framework performs well on the Procgen benchmark, its applicability to real-world tasks remains untested, leaving questions about how well it generalizes outside controlled environments.\n- The dual-agent adversarial framework introduces additional computational complexity, which may pose challenges in terms of scalability and efficiency for resource-constrained systems.\n- Although the framework is shown to improve generalization, more detailed ablation studies could have been included to clarify the contribution of individual components, such as the specific impact of the adversarial training mechanism.\n- The paper assumes that irrelevant features can be identified and suppressed, but it does not sufficiently address how to detect these features in environments where their classification is unclear or context-dependent.\n- The comparison with state-of-the-art methods is somewhat limited, with a stronger focus on performance gains rather than in-depth analysis of differences in behavior between approaches."
            },
            "questions": {
                "value": "- How does the adversarial training framework handle environments where the distinction between relevant and irrelevant features is not well-defined or context-dependent?\n- What strategies could be employed to reduce the computational overhead introduced by the dual-agent setup, especially in more complex or resource-constrained environments?\n- Could the method be extended or adapted to improve generalization in real-world tasks beyond the Procgen benchmark, and what modifications would be necessary to achieve this?\n- What impact would the framework have in environments with continuous action spaces or higher-dimensional state representations, where irrelevant features may be harder to isolate?\n- How would performance vary in scenarios where adversarial training results in catastrophic forgetting of useful features, and what mechanisms could prevent this?\n- Are there any considerations for applying this approach to tasks with dynamic or evolving feature relevance, where the set of relevant features may change over time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}