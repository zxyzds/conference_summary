{
    "id": "iv1TpRCJeK",
    "title": "$\\forall$uto$\\exists$$\\lor\\!\\land$L: Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks",
    "abstract": "This paper presents $\\forall$uto$\\exists$$\\lor\\\\!\\land$L, a novel benchmark for scaling Large Language Model (LLM) assessment in formal tasks with clear notions of correctness, such as truth maintenance in translation and logical reasoning.  $\\forall$uto$\\exists$$\\lor\\\\!\\land$L is the first benchmarking paradigm that offers several key advantages necessary for scaling objective evaluation of LLMs without human labeling: (a) ability to  evaluate LLMs of increasing sophistication by auto-generating tasks at different levels of difficulty; (b) auto-generation of ground truth that eliminates dependence on expensive and time consuming human annotation; (c) the use of automatically generated, randomized datasets that mitigate the ability of successive LLMs to overfit to static datasets used in many contemporary benchmarks. Empirical analysis shows that an LLM's performance on $\\forall$uto$\\exists$$\\lor\\\\!\\land$L is highly indicative of its performance on a diverse array of other benchmarks focusing on translation and reasoning tasks, making it a valuable autonomous evaluation paradigm in settings where hand-curated datasets can be hard to obtain and/or update.",
    "keywords": [
        "Large Language Models",
        "Logical Reasoning",
        "Autoformalization",
        "Informalization",
        "Formal Translation",
        "Truth Maintenance"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "An automatic and scalable benchmark for evaluating LLMs for truth maintenance w.r.t. formal syntax.",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=iv1TpRCJeK",
    "pdf_link": "https://openreview.net/pdf?id=iv1TpRCJeK",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a new approach to evaluate and benchmark LLMs on formal syntax. The authors suggest using CFGs to generate synthetic data for testing the LLMs, they utilize formal verifiers to verify the logic.\n\nContributions:\n1. A new method to evaluate LLMs on formal syntax without using static datasets by generating synthetic data using CFGs and adding vocabulary using another LLM\n2. Their method has better indication of accuracy than other metrics for the LLM performance on various FOL."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. LLMs can be now tested on dynamic data generated by this framework for Formal Syntax\n2. Their method has better indication of accuracy than other metrics for the LLM performance on various FOL. \n3. Very good experimentations , dataset size and presentation"
            },
            "weaknesses": {
                "value": "1. Explanation on how the generated dataset is having diverse data points or quality is missing."
            },
            "questions": {
                "value": "1. How diverse can the generated dataset be ?\n2. How do you ensure that duplicate data points won\u2019t be there ? I.e data points with very close similarity or minute changes or only change in vocabulary while the logic statement is same \n3. Would love to know how LLMs with tools work as verfiers"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The \u2200UTO\u2203\u2228\u2227L method evaluates the ability of LLMs to maintain truthfulness in formal tasks by dynamically generating datasets, utilizing the models' automatic formal and informal processing capabilities, and using formal verifiers. This approach allows for automated, human-free evaluation and can be extended to any formal grammar category that uses syntax and accepts equivalence checks. The authors demonstrate the effectiveness of \u2200UTO\u2203\u2228\u2227L as an evaluation framework through empirical studies, revealing the limitations of current state-of-the-art models in maintaining truthfulness in formal tasks. By comparing it with existing benchmarks, the article highlights the practicality and importance of \u2200UTO\u2203\u2228\u2227L in evaluating LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The article introduces the \u2200UTO\u2203\u2228\u2227L benchmark as an innovative evaluation framework that can automatically assess the ability of large language models (LLMs) to maintain truthfulness in formal tasks. Its practicality lies in dynamically generating datasets, reducing the need for manual annotation, and providing an evaluation method without human intervention.\n2. The \u2200UTO\u2203\u2228\u2227L framework is designed to be flexible, allowing it to be extended to any formal grammar category that uses syntax and accepts equivalence checks.\n3. The article demonstrates the correlation between the results of \u2200UTO\u2203\u2228\u2227L and other existing benchmarks, indicating that \u2200UTO\u2203\u2228\u2227L can serve as an effective alternative or complementary tool."
            },
            "weaknesses": {
                "value": "1. \u2200UTO\u2203\u2228\u2227L relies on existing formal verifiers and parsing libraries to ensure evaluation accuracy. This means its reliability depends on the performance and accuracy of these tools, and any errors in them could affect the evaluation results of \u2200UTO\u2203\u2228\u2227L.\n\n2. While \u2200UTO\u2203\u2228\u2227L reduces the need for manual annotation, generating dynamic datasets and performing formal verification may require significant computational resources, especially for large-scale datasets and complex logical structures."
            },
            "questions": {
                "value": "1. The article mentions using context-free grammars (CFGs) to generate datasets dynamically. How does this method ensure diversity and balance at different difficulty levels? Specifically in fields like logic and regular expressions, how does it ensure comprehensive coverage of various possible syntactic structures while maintaining a balanced sample size for each structure type?\n\n2. The article evaluates whether LLMs can act as verifiers to assess the equivalence of formal grammar expressions and finds that even the most advanced LLMs struggle with this role. In the evaluation process, in which types of logical structures or specific conditions do LLMs perform the worst? Are there specific logical structures or syntactic features that make it difficult for LLMs to accurately judge equivalence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces autoeval benchmark, designed for the autonomous evaluation of LLMs in formal tasks like truth maintenance and logical reasoning. The benchmark offers several features, such as dynamic dataset generation using CFGs and the use of formal verifiers to assess correctness. The authors argue that their approach mitigates common issues in existing benchmarks, like overfitting to static datasets, and provides a scalable method for evaluating LLMs without relying on human annotations. Empirical results show that performance on this benchmark correlates well with other established benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The use of CFGs to dynamically generate out-of-distribution datasets is a significant strength.\n- The use of formal verifiers to check the semantic equivalence of LLM outputs is a good method that avoids brittle syntactic checks. This allows for a more robust evaluation of truth maintenance across different formal syntaxes."
            },
            "weaknesses": {
                "value": "- While the paper focuses on formal tasks like truth maintenance and logical reasoning, it remains unclear how well the proposed benchmark generalizes to less structured or more complex natural language tasks. The reliance on formal verifiers may limit its applicability to broader LLM use cases, such as creative writing or conversational AI, where formal correctness is less critical.\n- Informalization, the process of converting formal syntax into natural language, is inherently ambiguous. The paper does not thoroughly address how it handles cases where multiple valid informalizations exist. This could lead to inconsistent evaluations when different LLMs produce semantically equivalent but syntactically different outputs.\n- Weak evidence for the robustness of this method. While the paper claims that performance on autoeval correlates well with other benchmarks, the empirical validation seems somewhat limited. I would suggest that the authors test their method on a larger set of models. The current correlation results, while promising, could be influenced by noise or outliers, especially given the relatively small number of models tested, they might not be robust enough to generalize across different architectures or configurations. By increasing the number of models tested (e.g., to 50+), the authors could reduce the impact of potential outliers and noise in the data."
            },
            "questions": {
                "value": "- Given that formal verifiers are central to your evaluation process, how do you handle cases where the verifier fails or produces ambiguous results? Are there fallback mechanisms in place?\n- Can your dynamic dataset generation approach be extended to more complex natural language tasks that do not rely on formal syntax? If so, how would this impact the scalability and performance of your evaluation framework?\n- What steps have you taken to ensure that your benchmark remains relevant as LLMs evolve? Specifically, how do you plan to update the benchmark as new models with different architectures or capabilities are developed?\n- In your empirical analysis, how many different LLMs were tested? It would be useful to know whether your results generalize across a broad range of models or are specific to a few."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}