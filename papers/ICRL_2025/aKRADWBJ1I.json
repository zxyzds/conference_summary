{
    "id": "aKRADWBJ1I",
    "title": "ActSafe: Active Exploration with Safety Constraints for Reinforcement Learning",
    "abstract": "Reinforcement learning (RL) is ubiquitous in the development of modern AI systems. However, state-of-the-art RL agents require extensive, and potentially\nunsafe, interactions with their environments to learn effectively. These limitations\nconfine RL agents to simulated environments, hindering their ability to learn\ndirectly in real-world settings. In this work, we present ActSafe, a novel\nmodel-based RL algorithm for safe and efficient exploration. ActSafe learns\na well-calibrated probabilistic model of the system and plans optimistically\nw.r.t. the epistemic uncertainty about the unknown dynamics, while enforcing\npessimism w.r.t. the safety constraints. Under regularity assumptions on the\nconstraints and dynamics, we show that ActSafe guarantees safety during\nlearning while also obtaining a near-optimal policy in finite time. In addition, we\npropose a practical variant of ActSafe that builds on latest model-based RL advancements and enables safe exploration even in high-dimensional settings such\nas visual control. We empirically show that ActSafe obtains state-of-the-art\nperformance in difficult exploration tasks on standard safe deep RL benchmarks\nwhile ensuring safety during learning.",
    "keywords": [
        "Safe Exploration",
        "Constrained Markov Decision Processes",
        "Safe Reinforcement Learning"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We propose a novel model-based algorithm for safe exploration in continuous state-action spaces and derive theoretical guarantees for it. We design a practical variant of our algorithm that scales well to vision control tasks.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=aKRADWBJ1I",
    "pdf_link": "https://openreview.net/pdf?id=aKRADWBJ1I",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces ACTSAFE, a model-based reinforcement learning (RL) algorithm designed for safe exploration within continuous state-action spaces. ACTSAFE utilizes epistemic uncertainty in the model as an intrinsic reward mechanism to encourage safe set expansion during exploration. The authors provide theoretical analysis that includes safety and sample-complexity guarantees, suggesting that ACTSAFE achieves near-optimal policy convergence within finite episodes. Additionally, a practical implementation of ACTSAFE is proposed, enabling the method to scale to high-dimensional tasks, including visual control. Empirical evaluations demonstrate ACTSAFE\u2019s performance across various benchmarks, supporting both safe exploration and strong task performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- Theoretical Rigor: The paper offers a thorough theoretical framework for ACTSAFE, deriving safety guarantees and sample-complexity bounds for safe exploration in continuous state-action spaces\u2014a notable contribution in the safe RL domain.\nScalability in Practical Settings: The authors extend ACTSAFE to visual control tasks, demonstrating scalability beyond low-dimensional models. This practical application is a significant step towards bridging the gap between theoretical safe RL algorithms and real-world, high-dimensional deep RL applications.\n\n- Comprehensive Experimental Analysis: The experimental analysis is extensive, covering both theoretical settings (under the Gaussian process assumptions) and more complex visual motor control tasks in the SAFETY-GYM environment, as well as sparse reward exploration. This breadth of evaluation provides valuable insights into the algorithm's performance under varied conditions."
            },
            "weaknesses": {
                "value": "- Reliance on Assumptions: The theoretical guarantees rely on idealized assumptions (e.g., well-calibrated Gaussian processes and specific Lipschitz conditions), which may limit generalizability. However, the authors provide strong empirical evidence that ACTSAFE performs well even when these assumptions are not strictly met, somewhat mitigating this concern."
            },
            "questions": {
                "value": "- The paper mentions the use of offline-collected data comprising 200K environment steps from a random policy. Can the authors clarify how this offline dataset is incorporated into ACTSAFE? Also, are all baseline algorithms utilizing this offline dataset consistently? If not, it may introduce unfairness in the comparisons."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors present a model-based algorithm, called ActSafe, that has strong theoretical safety guarantees."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is very well written. The high level idea of the algorithm is presented in an understandable way, without sacrificing mathematical rigour. By combining existing ideas, like intrinsic motivation or safe Expansion operators, the authors seem to have created a conceptually simple but very powerful algorithm for safe RL. \n\nBased on a strong theoretical foundation a practical implementation is provided, that crucially maintains the safety constraints. The experiments seem well designed, highlighting the clear strengths of the algorithm (its safety guarantees), while also discussing weaknesses. The authors especially discuss scalability weaknesses that may result from the choice of Gaussian Processes to approximate the system dynamics."
            },
            "weaknesses": {
                "value": "While the two-phase approach that ActSafe employs are the foundation for its safety guarantees, I would expect that this comes at a cost. A comparison of total environment steps in both loops required, wall clock or memory requirements would have been a nice addition."
            },
            "questions": {
                "value": "Safe RL is not my area of expertise, and I therefore ask the metareviewer to discount my (unfortunately brief) review accordingly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tackles the problem of safe exploration in reinforcement learning by proposing a new model-based constrained RL algorithm. Under the constrained MDP formulation, with regularity assumptions on the underlying dynamical system, the paper demonstrates theoretical safety guarantees and sample complexity guarantees. A practical version of the algorithm with weaker guarantees is shown to be applicable for visual control tasks in different simulated benchmarks. The results show less safety violations compared to prior constrained RL approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper targets a problem that is relevant to a broad community including controls, reinforcement learning, and robotics. Safe exploration in the context of model-based control is interesting because a reliable approach in this space has the potential for being deployed in safety critical scenarios as well as in scenarios that require sample efficiency and real-world exploration is challenging. \n\n- The proposed algorithm based on intrinsic exploration for reducing uncertainty of policies at the boundary of the current safe set and expanding the safe set based on reachability beyond it, is novel to the best of my knowledge. It is also intuitively sound and doesn't make any fundamentally limiting assumptions of the underlying system. \n\n- The theoretical guarantees of the base algorithm are strong, and the regularity assumptions seem reasonable to me. \n\n- The experiments on simulated environments are good, and it is nice to see results in challenging control tasks like a simulated humanoid walking, that go beyond the predominant evaluations of prior safe RL works on simple simulations like SafetyGym."
            },
            "weaknesses": {
                "value": "- A major weakness is that the paper's motivations are disconnected from the experiments. For example the intro states: \n\n\"In many real-world settings, environments are complex and rarely align exactly with the assumptions\nmade in simulators. Learning directly in the real world allows RL systems to close the sim-to-real\ngap and continuously adapt to evolving environments and distribution shifts. However, to unlock\nthese advantages, RL algorithms must be sample-efficient and ensure safety throughout the learning\nprocess to avoid costly failures or risks in high-stakes applications.\" \n\nHowever the experiments are all in simulated environments and there are no real-world experiments either in a controls setting or in a robotics setting.\n\n \n- It is unclear how the model-based version of ActSafe can be reliably be deployed in the real-world for safety-critical applications. Both theoretically and empirically in the simulated experiments, the constraint violations are lower than the baselines but nowhere close to 0. In addition learning a recurrent state-space model is a more complex choice than directly trying to learn a policy through model-free RL. It is unclear why there are no evaluations in a model-free setting, and what are the motivations for just considering model-based RL, after proposing a safe exploration algorithm that seems to be fairly generic and broadly applicable. It will be helpful to clarify these points. \n\n- In lines 233-235, it is mentioned that prior works do not have dimensional policies, but there is no explicit clarification about how high dimensional control tasks can the proposed approach tackle.  What are the relative differences? Are there any fundamental assumptions that prevent these prior works from being applicable to higher dimensional tasks?\n\n\n- The related works seem to miss a lot of prior approaches that also have similar ideas of conservative exploration  and actually demonstrate results on real-world settings where safety in important. For example, see [A-C] below. [A] in particular demonstrates results on real robotic manipulation tasks where safety is critical. \n\n[A] Thananjeyan, Brijen, Ashwin Balakrishna, Suraj Nair, Michael Luo, Krishnan Srinivasan, Minho Hwang, Joseph E. Gonzalez, Julian Ibarz, Chelsea Finn, and Ken Goldberg. \"Recovery rl: Safe reinforcement learning with learned recovery zones.\" IEEE Robotics and Automation Letters (RA-L)\n\n[B] Bharadhwaj, Homanga, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and Animesh Garg. \"Conservative safety critics for exploration.\" ICLR 2021\n\n[C] Srinivasan, Krishnan, Benjamin Eysenbach, Sehoon Ha, Jie Tan, and Chelsea Finn. \"Learning to be safe: Deep rl with a safety critic.\""
            },
            "questions": {
                "value": "Please refer to the list of weakness above, \n\n- Can the authors explain the disconnect between the motivations in the paper and the experiments? \n\n- In lines 233-235, it is mentioned that prior works do not have dimensional policies, but there is no explicit clarification about how high dimensional control tasks can the proposed approach tackle.  What are the relative differences? Are there any fundamental assumptions that prevent these prior works from being applicable to higher dimensional tasks?\n\n- It is unclear why there are no evaluations in a model-free setting, and what are the motivations for just considering model-based RL, after proposing a safe exploration algorithm that seems to be fairly generic and broadly applicable. It will be helpful to clarify these points. \n\n- In lines 225-226 it seems that in the general case the epistemic uncertainty will be uncalibrated / unreliable due to function approximation errors in the neural network model. Does the theory account for this?\n\n- Can the authors clarify relations to the missing related works, including very related safe exploration works that actually show results in real-world settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces ActSafe, a model-based algorithm for addressing the challenge of safe and efficient exploration in continuous state-action spaces, and provides a detailed theoretical analysis of it. This algorithm integrates OPAX and LBSGD with typical model-based RL methods to achieve safe and efficient exploration performance in vision control tasks. They conduct sufficient experiments in Safety-Gym domain, demonstrating that their approach outperforms prior works regarding safe exploration."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The problem is well-motivated.\n2. This paper is mostly well-written and has solid theoretical foundations. The code provided is very helpful for the reproducibility and review of this paper.\n3. The empirical results in sparse reward tasks are impressive."
            },
            "weaknesses": {
                "value": "1. The proposed algorithm ActSafe, is a combination of  OPAX, LBSGD, and Dreamer(or other model-based RL methods). The technical contributions are incremental.  \u00a0But considering the experiments are solid and sound, that should be fine.\n2. Inconsistency. This algorithm addresses the problem of safe exploration by maintaining a 'safe set' of policies. However, after checking the code provided, there is no such concept of a 'safe set', they just optimize the actor with the typical safe actor-critic method. Since the theoretical results might be unconvincing because of the inconsistency between the theory and practical implementation, it would be nice if the authors could explain this.\n3. Baselines.  In Figure 5, the author compares ActSafe with existing methods like OPTIMISTIC\u3001UNIFORM and GREEDY. Are there any other more competitive baselines?  Comparison with methods like GoSafeOpt\u3001OPAX might be necessary."
            },
            "questions": {
                "value": "1. It would be great if the author could address the main weaknesses I have outlined above. If they are properly addressed, I would be happy to raise my score, as I may have misunderstood the paper.\n2. Considering that there are no hard constraints on safety during learning in practical implementations, can ActSafe maintain safety during training after removing the initial offline data?\n3. I am very familiar with BSRP-Lag, but the algorithm's performance in the PUSHBOX task doesn't quite match my experience, can you disclose more experimental details?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}