{
    "id": "LtHy5y4Ep0",
    "title": "Dual Variance Reduction with Momentum for Imbalanced Black-Box Discrete Prompt Learning",
    "abstract": "Black-box prompt learning has proven to be an effective approach for customizing large language models (LLMs) offered as services to address various downstream tasks. \nWithin this domain, policy gradient-based methods have garnered substantial attention as a prominent approach for learning discrete prompts.\nHowever, the highly imbalanced data distribution in the real world limits the applicability of such approaches by influencing LLMs' tendency to favor certain categories.\nTo tackle the challenge posed by imbalanced data, this paper pioneers the integration of pairwise AUC loss into the policy gradient optimization of discrete text prompts and proposes learning discrete prompts with doubly policy gradient.\nUnfortunately, the doubly policy gradient estimation suffers from two variance components, resulting in unstable optimization.\nAs a further improvement, we propose (1) a novel unbiased variance-reduced doubly policy gradient estimator and (2) incorporating the STORM variance reduction technique. \nUltimately, we introduce a novel momentum-based discrete prompt learning method with doubly policy gradient (mDP-DPG).\nCrucially, we provide theoretical convergence guarantees for mDP-DPG within standard frameworks.\nThe experimental results show that mDP-DPG surpasses baseline approaches across diverse imbalanced text classification datasets, emphasizing the advantages of our proposed approach for tackling data imbalance.\nOur code is available at the following URL: https://anonymous.4open.science/r/DPDPG-1ECB.",
    "keywords": [
        "prompt learning; black-box optimization; imbalanced data"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=LtHy5y4Ep0",
    "pdf_link": "https://openreview.net/pdf?id=LtHy5y4Ep0",
    "comments": [
        {
            "summary": {
                "value": "This paper considers the (discrete) black-box prompt tuning for LLM for binary classification problems with imbalanced data. The main contributions are as follows:\n\n1. The authors propose the mDP-DPG (momentum-based Discrete Prompt learning with Doubly Policy Gradient) approach. This approach combines pairwise AUC loss and variance reduction techniques to deal with imbalanced data.\n\n2. A theoretical analysis is provided, showing that mDP-DPG achieves optimal convergence rates for imbalanced data scenarios, under some conditions such as bounded variance of the gradient.\n\n3. Experiments on benchmark datasets (like CoLA and MRPC) and real-world imbalanced datasets (Amazon reviews) demonstrate that mDP-DPG outperforms baseline methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. This is the first paper that studies how to address imbalanced data in NLP prompt learning.\n\n2. The theoretical results might be insightful in the realm of black-box policy optimization."
            },
            "weaknesses": {
                "value": "1. Scenario\n\nThe paper considers prompt tuning involving:\n- black-box LLM (e.g., API-only LLM)\n- classification tasks\n- imbalance data\n\nHowever, prompt tuning for black-box LLMs is generally more relevant to complex generative tasks rather than simpler classification tasks. It is uncommon for entities to use models like GPT-4 primarily for text classification, especially when manually designed prompts can already yield strong performance on tasks like GLUE. This positioning may reduce the appeal of the proposed method to the main audience of ICLR.\n\n2. Experiments\n\nThe paper does not include experiments on widely used API-only LLMs, such as GPT-3.5 or GPT-4. Additionally, it lacks comparisons with other existing black-box prompt tuning methods like APE [R1], EvoPrompt [R2], and ZOPO [R3]. \n\n[R1] Zhou, Y. et al. Large language models are human-level prompt engineers.\n\n[R2] Guo, Q. et al. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers.\n\n[R3] Hu, W. et al. Localized Zeroth-Order Prompt Optimization."
            },
            "questions": {
                "value": "Please see \"weakness\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a variance-reduced policy gradient descent method for black-box prompt learning. To deal with the class imbalance issue, the authors use AUC as the objective function. To accelerate the algorithm convergence, the authors propose to use two variance reduction techniques. The proposed algorithm has a convergence guarantee and good experimental performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. To deal with the class imbalance issue, the authors propose to use AUC as the objective function.\n\n2. To get fast convergence, the authors propose to reduce variance from both policy estimation and optimization."
            },
            "weaknesses": {
                "value": "1. The authors assume that the tokens in the prompt are sampled independently from their marginal distribution. However, it seems that tokens in the prompt should have some relation.\n\n2. The proposed method can only for binary classification. How to extend to the multi-class? Do we need to train multiple different prompts?"
            },
            "questions": {
                "value": "1. Can the authors describe why the probability they use assuming the tokens in the prompt sampled independently?\n\n2. Is there a simple way to extend from binary classification to multi-class classification, since binary classification is rare in practice?\n\n3. How concentrated the final solution P is? If the probability spread out to many different tokens, how can we generate a stable prompt to use?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper considers the problem of discrete prompt learning with black-box language models. The authors propose an unbiased variance-reduced doubly policy gradient estimator for optimizing prompts through policy gradient optimization. By incorporating the STORM variance reduction techniques, the algorithm obtains a stable prompt. Experimental results on several benchmark tasks show the effectiveness of the proposed algorithm."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This work studies the challenge of data imbalance in discrete prompt learning with black-box models, a significant issue in many practical applications.\n\n2. Discrete prompt learning is widely necessary for practical applications with black-box models. The proposed method has the potential to serve as a fine-tuning approach for many commercial black-box models."
            },
            "weaknesses": {
                "value": "1. The proposed method, along with its theoretical analysis, largely combines existing techniques, which limits its technical novelty.\n\n2. Several of the backbone models used in the experiments, such as RoBERTa-large and GPT2-XL from 2019, are outdated for current black-box applications. Additionally, the Llama 3 8B model is relatively small. In real-world applications, more commonly used models might include Llama 3 70B or GPT-4, accessed via API. How does the proposed algorithm perform on these state-of-the-art language models?"
            },
            "questions": {
                "value": "see weakness 2"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tackles the challenges of learning discrete prompts for large language models (LLMs) in the context of imbalanced data distributions. The authors integrate pairwise AUC loss into a policy gradient optimization framework and introduce a doubly policy gradient approach. To improve stability, they introduce the variance-reduced doubly policy gradient estimator (VR-DPGE) and the STORM technique into their method, momentum-based discrete prompt learning with doubly policy gradient (mDP-DPG). They demonstrate their method with theoretical proof and experimental results."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. the paper presents a novel approach for optimizing prompts in the context of imbalanced classification problems with LLMs.  The authors introduce VR-DPOGE and STROM strategy to  effectively address the dual variance involved in gradient calculation. \n2. The paper offers rigorous theoretical guarantees for the mDP-DPG method. Additionally, the experimental results validate the effectiveness of the approach, demonstrating a clear advancement over baselines across diverse imbalanced text classification datasets."
            },
            "weaknesses": {
                "value": "The paper lacks sufficient explanation on how it addresses the stated problem. \n\nFor instance, while the authors introduce the \"unbiased VR-DPGE estimator,\" they do not provide adequate references for VR-DPGE, nor do they explain how Equation (8) specifically reduces variance. Some notation such as $T^{(k)}$  used in Equation (8) is not clearly defined.\n\nA similar issue arises with the STORM strategy, where there is no explanation of the role of momentum. The equation appears only within the algorithm section. There is no definition on the  proj$_{C}$ function. \n\nThe theoretical results also require further discussion on what the theorem indicates about the resulting prompts."
            },
            "questions": {
                "value": "1. Please provide more intuitive discussion on how the VR-DPOGE and STORM strategy can reduce the variance of gradient estimation.\n\n2. The paper assumes  independent categorical distribution on each token distribution, and proves that this distribution will converge. However, will the distribution converge to a delta distribution centered on a single optimal prompt? Could a correlated distribution over prompt tokens lead to convergence on such a single optimal prompt and potentially improving performance?\n\n3. In Tables 1 and 2, several AUC values fall below 0.5. What could be the possible reasons for this performance being worse than a random guess?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}