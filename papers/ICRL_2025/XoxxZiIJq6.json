{
    "id": "XoxxZiIJq6",
    "title": "Comet: A Communication-efficient and Performant Approximation for Private Transformer Inference",
    "abstract": "The prevalent use of Transformer-like models, exemplified by ChatGPT in modern language processing applications, underscores the critical need for enabling private inference essential for many cloud-based services reliant on such models. However, current privacy-preserving frameworks impose significant communication burden, especially for non-linear computation in Transformer model. In this paper, we introduce a novel plug-in method Comet to effectively reduce the communication cost without compromising the inference performance. We second introduce an efficient approximation method to eliminate the heavy communication in finding good initial approximation. We evaluate our Comet on Bert and RoBERTa models with GLUE benchmark datasets, showing up to 3.9 less communication and 3.5 speedups while keep competitive model performance compared to the prior art.",
    "keywords": [
        "private inference",
        "secret sharing",
        "Transformer",
        "language model",
        "homomorphic encryption",
        "multi-party computation"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=XoxxZiIJq6",
    "pdf_link": "https://openreview.net/pdf?id=XoxxZiIJq6",
    "comments": [
        {
            "summary": {
                "value": "This paper tackles the communication (and latency) overhead introduced by nonlinear operations (GELU, LayerNorm, and Softmax) in cryptographically secure private inference of transformer-based models. The authors replace GELU and Softmax with a smoothed maximum unit (SMU) function, effectively transforming nonlinearities into inverse square root operations, and developed the methods such as double approximation and share flooding to efficiently compute them in privacy-preserving settings.  \n\nBy mitigating the communication overhead associated with these nonlinear operations, this work tackles a major bottleneck for the practical deployment of private LLM services."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "$\\bullet$ Comprehensive evaluation of post-training performance across a variety of downstream tasks, demonstrating robustness, applicability, and generalizability of their proposed approaches. \n\n$\\bullet$ The integration of SMU with trainable parameters as an alternative to standard nonlinearities like GELU and ReLU-approximated Softmax is an interesting approach to simplifying the conventional nonlinearities and reducing communication and latency overheads in private inference. \n\n$\\bullet$ A thorough comparison with previous methods for private inference in transformer-based models."
            },
            "weaknesses": {
                "value": "$\\bullet$ The contributions in this paper are largely engineering tweaks rather than substantive algorithmic advancements. While incorporating SMU (Biswas et al., CVPR'22) as a replacement for traditional nonlinearities like GELU and ReLU-approximated Softmax in a transformer-based model is an interesting approach, it lacks sufficient originality and novelty. Additionally, the use of Network Raphson methods for inverse square root calculations, as previously implemented in CryPTen but with an improved initialization, which reduces the number of iterations to converge, does not offer a particularly compelling approach. These techniques are often highly model- and task-dependent, requiring tuning when applied to other tasks or domains. \n\nFurthermore, the authors\u2019 proposed share flooding method, which is based on the observation  \"that the absolute magnitude of tensor values is closely surrounded around zero (Line#317),\" is rather intuitive and lacks deeper insight. Nonetheless, the authors will definitely get the points for the double approximation approach which effectively reduces the communication overhead to constant time complexity for lookup tables. \n\nOverall, this paper lacks significant research insights or novel observations addressing the challenges of efficient private inference for large language models.\n\n$\\bullet$ I also find the comparison with CrypTen unclear, as CrypTen is a 3PC method, whereas the authors use a 2PC method. At one point (Line#482-483), the authors mention excluding PUMA due to its 3PC computation, which raises questions about the consistency of the comparison.\n\n$\\bullet$ The authors should consider comparing their approximation methods with the polynomial approximations of LayerNorm, GELU, and Softmax used in *Zimerman et al., Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption* (**ICML'24**). It\u2019s essential to at least qualitatively contrast and position the merits (efficiency and predictive performance) of their approach with these recent polynomial approximations. \n\n$\\bullet$ I find it quite challenging to follow the draft because the readability suffers from overly long paragraphs. Breaking up the content into shorter, clearer paragraphs would really help make the information easier to digest.\n\n## Correction in the draft\n\n$\\bullet$ Figure 1 depicts the 2PC threat model for Post-LN configuration, however, in the caption the authors have mentioned CrypTen (which is 3PC). Also, in the FFN block diagram, one linear layer (after GELU layer) is missing. \n\n$\\bullet$ Line#180: GELU necessitates the Gaussian error function---> BERT and other recent transformer-based models adopt the $Tanh$ approximated GELU implementation, which is significantly faster, even in cryptographic settings (e.g., Bumblebee), than the original GELU   (`torch.nn.GELU()`). See [1] and their implementation in hugging face activation libraries [2]\n\n1. https://paperswithcode.com/method/gelu\n\n\n2. https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py#L49"
            },
            "questions": {
                "value": "$\\bullet$ Why the authors have compared their 2PC method to 3PC method CrypTen? Is that the comparison with Crypten's Newton Raphson's method and other approximations, and not exactly with their 3PC settings? \n\n$\\bullet$ Why do the client and server share, which is an integer in the field arithmetic, converted to floating point numbers (Eq 1 and Eq 2)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Comet to tackle to slow private inference for transformer based model. It is a plug-and-play method with an efficient approximation method to reduce the cost in non-linear computation. It achieves 3.9x less communication and 3.5x speed up compared to previous SOTA."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The methods are well motivated by observations.\n2. The approximations are quite novel and interesting."
            },
            "weaknesses": {
                "value": "1. Figures are (in my opinion, not a major weakness) not good enough: (1) Figure 1 seems like hand written, with small fonts and large empty spaces. (2) Figure 2 should be put in appendix or wrapfigure (since they are not the main point of the paper).\n2. The 2-relu approximations have been proposed in literature, e.g. MPCFormer. Also, since approximation has been heavily studied, can the author possible make a small table to present what is in literature and what is not (to improve presentation)?"
            },
            "questions": {
                "value": "Please see the weakness section.\n\nOne other comment I have is that the experiment testbed is quite outdated. The author should consider more benchmark and larger scale model (This is out-of scope of the paper I think, but would be useful for future research), e.g. Vicuna and MMLU. ChatGPT is not Bert-like models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Comet, a framework aimed at efficient, privacy-preserving inference for Transformer models. Comet achieves this by unifying complex non-linear functions, like GeLU and Softmax, under a single protocol based on inverse square root calculations. It introduces a \"double approximation\" method to eliminate the need for extensive communication when finding initial approximations, further improving communication efficiency with a technique called \"share flooding.\" Experimental results show that Comet reduces communication costs while maintaining good performance on models like BERT and RoBERTa on the GLUE benchmark"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Designing an improved approximation for non-linear functions in private Transformer inference is critical, and this paper's adoption of the \"smu\" function is both innovative and effective, demonstrating strong accuracy after fine-tuning.\n2. The paper presents an effective solution for obtaining a good initial approximation in the context of 2PC-based secret sharing, which is a key contribution to reducing communication overhead."
            },
            "weaknesses": {
                "value": "1. The properties of the \"smu\" function are not sufficiently explained. While experiments indicate that it achieves 1-2% higher accuracy than the original GeLU and Softmax functions, the underlying mechanism remains unclear. Including a graph of the approximated function would help clarify this.\n2. The explanation of the double approximation method is confusing. Since MPC typically operates on fixed-point numbers, it is unclear whether the double approximation method is designed for floating-point operations, which are costly in MPC.\n3. It is worth noting that other approximation methods like Bolt, Bumblebee, and Puma do not require fine-tuning, and this difference should be mentioned for context."
            },
            "questions": {
                "value": "Do you have code to reproduce your work since the \"smu\" function is rarely used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a secure Multi-Party Computing (MPC)-based private inference framework, called Comet. Comet aims to address the challenge of high communication overhead, particularly for non-linear computations in MPC-based Transformer model inferences. Existing solutions, such as Look-Up Table (LUT) methods and aggressive polynomial approximations, often result in either high communication costs or reduced model performance. To tackle this, Comet harmonizes non-linear functions like GeLU and Softmax into a single approximation function, eliminating the need for heavy communication in finding initial approximations for Newton iterations. A \u201cshare flooding\u201d technique is also proposed to further optimize secure two-party computations. Comet was evaluated on encoder-based Transformer models like BERT and RoBERTa (as well as on the evaluation benchmark GLUE) to demonstrate that it is both communication-efficient (i.e., fast in inference) and more accurate over the benchmark compared to existing MPC-based private Transformer inference frameworks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper is well-motivated. Studying methods that enable private inference over large language models has great practical potential for privacy-constrained use cases, such as in financial and medical institutions. Private inference methods (e.g., MPC-based approaches) can serve as potential alternatives to pure local computing.\n- The paper is well-written, and the techniques proposed in the Comet framework appear sound.\n- The experimental results look promising, especially regarding accuracy on the GLUE benchmark."
            },
            "weaknesses": {
                "value": "- The paper was motivated by using ChatGPT as an example, but all experiments were conducted on encoder-based Transformer models like BERT and RoBERTa. I understand that, architecturally, encoder models and decoder models differ mainly in language masking. However, models like GPT are autoregressive, which involves completely different computational intensity. For instance, autoregressive models generate tokens one by one, making it very difficult to batch inputs.\n- Although MPC-based frameworks ensure that neither the client nor the server discloses information to each other, in Comet\u2019s setting, the private information includes both the model weights and the users\u2019 data. However, both parties still see the same (non-encrypted) output in these frameworks. While this may be acceptable for classification tasks conducted by BERT, it may not be suitable for generative models like ChatGPT (e.g., generated content could contain private user information).\n- The absolute inference speed remains quite slow, e.g., around 30 seconds per batch, as shown in Table 3. Speeds like this make the proposed framework less practical."
            },
            "questions": {
                "value": "- As discussed in the \u201cWeaknesses\u201d section, there is concern about whether the proposed Comet framework can be applied to generative language models. Can the authors provide a feasible approach to extend the proposed framework to generative LLMs like ChatGPT and Llama?\n- What if, in a practical application, the server model is not open-source (e.g., users can only call their API, like GPT and Claude)? Is there a way to still conduct private inference?\n- The experiments in this paper are conducted on CPUs. However, Crypten supports GPU computing [1]. I wonder how the performance of Comet compares to Crypten on GPUs.\n- It seems the major weakness of MPCFormer [2] is that it requires fine-tuning using knowledge distillation (KD). However, the idea makes sense for pure inference. I wonder if Comet could also benefit from KD.\n- Encoder models are generally easy to batch. I wonder how different batch sizes affect the computation speed of Comet.\n\n[1] https://github.com/facebookresearch/CrypTen?tab=readme-ov-file#installing-crypten  \n[2] https://openreview.net/forum?id=CWmvjOEhgH-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}