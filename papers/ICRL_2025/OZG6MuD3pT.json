{
    "id": "OZG6MuD3pT",
    "title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
    "abstract": "Chain-of-Thought (CoT) prompting has become a key strategy for enhancing the inferential abilities of large language models (LLMs) in reasoning tasks. However, it often struggles with ensuring reasoning validity and maintaining informativeness. This paper presents the Logic Agent (LA), a novel framework designed to boost the validity of reasoning in LLMs through strategic logic function calls. Distinct from traditional methods, LA converts LLMs into dynamic agents that apply propositional logic rules, transforming natural language inputs into structured logical forms. The agent utilizes a robust suite of predefined functions to guide the reasoning process effectively. This approach can enhance the structured and coherent generation of reasoning outputs, improving their interpretability and logical consistency. Through detailed experiments, we showcase LA's ability to adapt across different LLM sizes, significantly enhancing the accuracy of complex reasoning tasks across various domains.",
    "keywords": [
        "reasoning",
        "large language models",
        "logic",
        "logic reasoning"
    ],
    "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)",
    "TLDR": "We present the Logic Agent (LA) framework to boost the validity of reasoning in LLMs through strategic logic function calls.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=OZG6MuD3pT",
    "pdf_link": "https://openreview.net/pdf?id=OZG6MuD3pT",
    "comments": [
        {
            "summary": {
                "value": "The authors present a Logic Agent (LA) framework for providing LLMs with a logical reasoning toolbox that can be used to assist in responding to queries requiring complex reasoning. They present a number of logical reasoning problems encountered by LLMs, and focus on contrapositive reasoning as an exemplar, something that LLMs have been shown to perform poorly on. This method can be applied generally to a wide variety of contexts and the authors present results on 7 different datasets from 3 task categories: Multiple choice reading comprehension, natural language inference and true of false questions, and on 5 different LLM backbones. The general approach requires the encoding of the context and query into propositional logic (with the addition of existential and universal qualifiers,) and the application of reasoning on these encodings using inference rules. Results show that their approach outperforms base LLMs (without LA) and chain of thought prompting (CoT) on the majority of tests with some cases being substantial improvement."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The overarching idea is a good one, to find a general way to encode context and query as propositions and to use common rules to help reason about these and avoid the LLMs making logic errors in their responses. The realisation of this into a workable scheme is pretty good too. This is evidenced by the general applicability of the approach and the fact that there are consistent improvements over other methods. There is a broad set of datasets and models being tested with a meaningful baseline and an ablation study."
            },
            "weaknesses": {
                "value": "The paper could be a little clearer in some parts. There is a little vagueness about how the method is applied. There are also some details that could be explained in a bit more detail. For instance, the authors state that the context is turned into propositional logic but that the forall and exists quantifiers are used (which suggests a first order-like encoding). It also wasn't clear how the logic generating step was developed to encode the context and what choices had been explored in achieving this. Finally, the writeup of the ablation study is very unclear and I am not sure what has been done here.\n\nOn another note, the analysis of the results felt somewhat oversold. There was a \"significant leap\" which was \"particularly evident in datasets requiring complex reasoning\" but this didn't really reflect the results. Some of the results were better by a margin for the LA over the other methods but this seemed less systematic than the authors were claiming. It would also help to identify where the method wasn't working so well and perhaps hypothesize about why that it. In short, it would benefit from a more objective analysis. On a related note, I also felt a bit like things were being oversold, particularly in the second half of the paper, with phrasing like: \"capability to significantly refine\", \"firmly establishes LA as a transformative approach\", \"elucidates its broad applicability and effectiveness\", \"enhance the versatility\" and so on. This is somewhat reminiscent of auto-generated text.\n\nThere  are some little niggling presentation errors that could do with clearing up too. For instance \\citet seems to have been used throughout rather than \\cite. And on page 8 there is a hanging reference ??"
            },
            "questions": {
                "value": "How are `Variable` and the `forall` and `exists` quantifiers used within an otherwise propositional framework?\n\nHow are the LLMs guided to produce the guided generation? Are they fed the rules as text and asked to apply them? Or do you generate them functionally and feed to the LLM as additional context?\n\nWhat happens if there is no clear encoding of the context into any usable atoms or implication rules? Is the LLM explicitly instructed in how to deal with that?\n\nSection 5.1 seems to suggest that there was some default parser used as logic parser throughout. Is that right? Or was the corresponding LLM model used as a logic parser for its own pipeline? If the latter, then why don't we see the logic parsing results for the other models shown, just GPT-4?\n\nHow the parsing results in table 3 calculated? Do you have a parsing ground-truth for the data? If so, how was that established and how large is it?\n\nIn the ablation study how is the Parsed Logic method applied? Why have you chosen GPT-3.5 in this case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a framework LA designed to enhance logical reasoning capabilities of LLMs by integrating propositional logic rules and deduction functions directly into the model's reasoning process. They use LLM as agents which performs the reasoning over the desired task. This work achieves this by converting natural language expression into the more concrete formal logic which helps in deciding chain of the reasoning to get the final answers. This work complement the chain of thought based model by introducing rule following and validate the logical inference.  Logic agent guide LLMs through predefined logical functions, which transform natural language inputs into logical forms that the model can interpret and apply with accuracy across various reasoning tasks. The work evaluates performance across different models, including GPT based and open-source LLMs (Mistral and LLAMA), showing that it significantly enhances accuracy in tasks that demand rigorous logical reasoning."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Verification of logical chain validity and use of symbolic reasoning model is compelling for the task requiring reasoning. Idea of making LLM as a agent for decision making makes sense as LLM are now widely used for tool selection and function calling. Presentation wise, I really liked the introduction part of the paper which makes compelling argument for the motivation of the work.\n1. Presents a unique approach to improving logical reasoning in LLMs by transforming them into rule-guided agents.\n\n2. Offers a promising alternative to existing Chain-of-Thought prompting techniques.\n\n3. Comprehensive evaluation over multiple models and datasets of varying difficulty level.\n\n4. The potential applications for Logic Agent in areas requiring logical rigor, such as legal reasoning and complex decision-making, make it a valuable development."
            },
            "weaknesses": {
                "value": "I really liked the introduction and related work section of the paper, which made very compelling premise for the logic agents. However section explaining the logic agents were not clear, apart from the Fig.2, section explaining them were not clear to me. Author(s) should put an effort so that readers who are not very deep and expert into this topic can benefit from this work. This requires through revision of the section 3 of the paper.  Some pointers, author say in 3.3 \"we prompt LLM to discern and decide upon the most appropriate rule to apply...\", but how this is done is missing from the papers, examples showing this will be helpful. I found paper to be bit rushed.\n1. As this work is based on domain of prompting, it would be great to show how this look like, either in the paper or appendix. E.g., CoT paper does it quite nicely (for inspiration).\n2. Appendix is cross referenced and it is written that code is available, but I found both to be missing.\n3. How LLM are used as automated decision makers can be explained in details as this is a good innovation point but not explained properly.\n4. Section 5.1 about parsers, how parsers prompting work is missing and makes hard to follow. It is written as a passing by argument but not explained. So nice opportunity is missed."
            },
            "questions": {
                "value": "1. Section 5.1 about parsers, details about default parsers is missing. Does it mean that LA agent is a default parsers?\n2. In sec. 5.1 Author(s) wrote \"To harness GPT-4\u2019s parsing capabilities, we crafted specific prompts aimed at guiding the model to\ntranslate natural language statements into propositional logic forms\", how did you achieved that?\n3. In results table 2, are there other SOTA model available, if yes why not compared against or can be shown in appendix?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper claims to propose a framework called Logic Agent which helps LLMs to choose from various propositional logic rules for reasoning. As the result,  LA  + LLM outperforms few shot chain-of-thought reasoning across various LLM models of different sizes."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Improving ability  for reasoning for LLMs is a very important field of research."
            },
            "weaknesses": {
                "value": "The paper promises a lot in introduction and in the introduction; however it unfortunately runs short in implementing them. Main weaknesses are as follows:\n\n- It is not clear what is the \"framework\", it would really help to have an illustrative figure. Figure 1 and Figure 2 are more like a snapshot and the examples there are not really helpful (for instance Option A when we don't know other options, also many rules are based on absolute contrapositive, so  its not clear how they are useful.) \n\n- Exposition lacks careful writing both on defining the formal reasoning tasks and the propositional logic component. (E.g., formal semantics and parsing side is purely verbal in terms of natural language.) Given all these paper is not technically substantiated. \n\n- Important models such as Claude  and o1 is missing (although this is on the relatively minor side, since maybe the latter was not available by then). \n\n- Besides, why the quantifiers if you are working with propositional logic. And if so, then it should be mentioned that these are QBF sentences. There are a few typos: liu et al.  and also the transitivity double arrow  technically does not  work. \n\n-It is not clear whether the presented results are statistically valid.  For instance, how many shot  CoT has been done? \n\n-Major related work  and comparison to other works as benchmarks are  missing e.g., works of Subbarao Kambhampati."
            },
            "questions": {
                "value": "Many of the rules are special case of contraposition (e.g., upper contrary law). How does including them additionally helps?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Logic Agent (LA), a framework designed to enhance logical reasoning capabilities in Large Language Models (LLMs) through strategic logic rule invocation. The framework converts natural language into formal logic representations and provides predefined functions for systematic reasoning. The authors evaluate LA across multiple models and tasks, demonstrating improvements in logical reasoning precision."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The framework introduces a systematic way to incorporate formal logic rules into LLM reasoning through well-defined functions (e.g., contrapositive law, transitive law, De Morgan's law), which provides a structured approach to logical deduction to improve the reasoning in CoT."
            },
            "weaknesses": {
                "value": "1: My main concern is that the analysis of the experiment is not inclusive; there is no clear comparison showing how LA improves reasoning other than vanilla CoT. Also missing ablation studies on individual components (e.g., impact of different logic rules, effectiveness of rule selection), and there is no detailed analysis of computational overhead or latency introduced by the framework and lack of error analysis showing standard failure modes\n\n2: Insufficient details about prompt design choices and development process;  Limited explanation of how the framework ensures consistent rule application; Lack of engineering details about reproducing the results as well as the sensitivity to the prompts, which is  especially concerning given the contribution of the work is heavily engineered based \n\n3: Novelty concern; heavily relying on GPT-4 for generation and evaluation creates potential circular reasoning which raises the questions on how can this be helpful in real world, especially given the improvement is not very significant.  Also, this seems to be limited to propositional logic without a clear path to handling more complex logic types, which raises questions about the generalizability of the method."
            },
            "questions": {
                "value": "1: How does the computational cost and efficiency scale?\n\n2: What specific improvements in reasoning quality can be attributed to the framework versus simply better prompting? how do you justify such improvement quantitatively?\n\n3: How were the prompting strategies developed, and what alternatives were considered?\n\n4: How does the framework perform on more complex logical reasoning (Multi-gop, open ended,etc) tasks not covered in the current evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}