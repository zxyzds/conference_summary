{
    "id": "Q3oAX9HoH2",
    "title": "Nested Gloss Makes Large Language Models Lost",
    "abstract": "Large language models (LLMs) have succeeded significantly in various applications but remain susceptible to adversarial jailbreaks that void their safety guardrails. \nPrevious attempts to exploit these vulnerabilities often rely on high-cost computational extrapolations, which may not be practical or efficient. \nIn this paper, inspired by the authority influence demonstrated in the Milgram experiment, we present a lightweight method to take advantage of the LLMs' personification capabilities to construct $\\textit{a virtual, nested scene}$, allowing it to realize an adaptive way to escape the usage control in a normal scenario.\nEmpirically, the contents induced by our approach can achieve leading harmfulness rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open-source and closed-source LLMs, $\\textit{e.g.}$, Llama-2, Llama-3, GPT-3.5, GPT-4, and GPT-4o.",
    "keywords": [
        "Trustworthy Machine Learning; Large Language Model"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We propose a jailbreak attack method to bypass LLMs' safety protocols by concealing the attack intention with nested instructions.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Q3oAX9HoH2",
    "pdf_link": "https://openreview.net/pdf?id=Q3oAX9HoH2",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes the \u201cDeepInception\u201d nested prompt injection method, which leverages the\npersonification capabilities of LLMs to carry out jailbreak attacks in a black-box environment. This\napproach constructs a multi-layered context, gradually guiding the model to generate harmful\ncontent through virtual multi-level instructions, bypassing its ethical restrictions. Experiments show\nthat DeepInception achieves a high jailbreak success rate across both open-source and closedsource LLMs and exhibits a sustained jailbreak effect."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper proposes an innovative nested jailbreak method that effectively bypasses the ethical\nrestrictions of LLMs in a black-box environment.\n2. It provides detailed experimental analysis, validating the effectiveness and transferability of the\nmethod across different models and attack scenarios.\n3. Inspired by the Milgram experiment, it offers a reasonable psychological theory to support the\ndesign of the method, adding a unique perspective."
            },
            "weaknesses": {
                "value": "1. The appendix is detailed but somewhat extensive. It is recommended to move the \u201cMulti-Modal Attack\u201d and \u201cChat Histories\u201d sections to the supplementary materials to enhance the clarity and readability of the main text.\n2. In the appendix of the paper, the section on EVALUATION METRIC AND EXAMPLES categorizes the harmfulness based on model responses into five ratings but does not specify how these rating statistics are converted into the harmfulness rates/scores used in the experiments. Could this be supplemented here?"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates leveraging the instruction-following and personification capabilities of LLMs to bypass internal safeguards and generate unexpected harmful content. The proposed prompting method, DeepInception, achieves a higher harmfulness rate than previous baselines and enables continuous jailbreak attacks. This approach is applicable to both open- and closed-source models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper draws an analogy between human psychological experiments and LLMs, and based on insights from psychology, proposes a novel prompting approach called DeepInception.\n- The experiments cover different open-source and closed-source LLMs and extend to LMMs and recent model o1. The popposed methods achieve decent jailbreaking performance.\n- The proposed prompting strategy remains effective when defense strategies are deployed.\n- It shows that the proposed method can hypnotize LLM to a self-loss state and can induce more harmful responses."
            },
            "weaknesses": {
                "value": "- The paper's core concept lacks technical novelty. LLMs are known to be vulnerable to various natural language prompting strategies, such as virtualization and role-playing. Although this new prompt strategy is somewhat more sophisticated, it offers limited insights into new types of vulnerabilities in LLMs and may not significantly contribute to developing more robust models.\n- GCG can be adapted for black-box settings when optimized on open-source models. Therefore, statements in the paper like \"GCG is not black-box LLM applicable\" are inaccurate.\n- Model-based evaluations are often unreliable and can result in false positives. An additional human study to validate these evaluations is necessary, either to assess if the output is genuinely harmful or to confirm if DeepInception's response is indeed more harmful than the baseline responses.\n- The paper\u2019s formatting needs improvement. For example, Figure 7 appears above Figure 6, which is unexpected and should be corrected."
            },
            "questions": {
                "value": "What conclusions can be drawn from the authors' findings that DeepInception generates responses with lower perplexity (PPL) values? From my perspective, as long as the response is harmful, then the model is jailbroken regardless of whether the model has higher confidence (high perplexity) in the response or not (low perplexity). Lower perplexity doesn't necessarily mean that one method is better than the other.\nBesides, perplexity is highly influenced by length, and there may be cases where responses from different methods exhibit the same level of harmfulness but differ in length."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The developed prompting strategy can be used to elicit objectionable content from the LLMs and cause society harm."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Within this work, the authors aim to \u2018jailbreak\u2019 Large Language Models (LLMs) by causing them to output harmful content. Their method attempts to leverage existing findings within human Psychology research, which then also provides some interpretability as to why their method works. They show that LLM guardrails are still vulnerable to harmful requests which are asked \u2018indirectly\u2019 and within \u2018nested\u2019 prompts. Their method is shown to be more effective than other jailbreaks, even against state of the art models that are using existing defence strategies."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The evaluation of the method is impressively done. Not only are state-of-the-art models considered (both open and closed sourced) but the work also tests against existing defence methods like \u201cself reminder\u201d and \u201cin context defense\u201d. I am convinced that this method is effective, in general and in comparison to other black-box methods.\n\n+ The authors source existing work to justify why human behaviour findings may transfer to LLM behaviour findings, which is a very interesting direction that is currently under-explored. \n\n+ Their specific theory, which combines the findings that \u2018self-loss states induce instruction-following behaviour\u2019 and that \u2018LLMs are good at impersonating/ simulating people\u2019 (and thereby losing their aligned selves), is valid. Hiding harmful requests inside of fiction-generating requests is not a novel idea, but the author\u2019s correctly claim that existing work does not explain why their methods work, which subtracts from their usefulness towards helping develop defences.\n\n+ Besides the common benchmark, the work also presents an interesting application of this method, getting multimodal models to extract identities and locations from a given photo. Follow up work could determine the significance of this kind of attack."
            },
            "weaknesses": {
                "value": "+ The largest problem with the paper is the disconnect between the supposed motivation of the method, the \u2018Milgram experiment\u2019, and the actual implementation of the method. To resolve this, the authors could state that they were \u2018merely inspired\u2019 by the experiment, opposed to drawing strict equivalences and stressing how much their method follows the original findings. This would then also improve the clarity of the introduction, which makes reference to details of the experiment when these details are only explained later in the preliminaries section. Otherwise, these disconnects should be addressed more clearly, so as to not mislead readers.\n\n+ Here are some specific \u2018disconnects\u2019 and contradictions that I\u2019m referring to: In section 3.1, the authors claim that the Milgram experiments \u201cdid not directly command the participants to [do harmful thing]. Instead, the experimenter provided a series of arguments and explanations to persuade the participants to proceed\u201d which thus motivates their indirect prompting of the LLM. They also claim that the experiments had a progressively escalating element to them, which motivated the author\u2019s use of nested guidance and progressive refinement of the answer generation. The latter claim is valid, the experiment directors start with saying \u201cPlease continue or Please go on\u201d and building up towards \u201cYou have no other choice; you must go on\u201d (taken directly from the study), but in either of these cases the command is certainly direct! The whole point of the experiment was that people follow the orders of authoritative figures, even if the actions are deemed immoral, which does not transfer to LLMs as the authors admits that LLMs quickly reject direct orders from the user to do immoral things, and make no effort towards incorporating this \u2018authority\u2019 idea into their method. If the false notion was correct that the experiment directors actually tried to \u201cpersuade\u201d the participants with a \u201cseries of arguments\u201d then that would still not support the author\u2019s method and instead be more similar to the PAP work (reference 74) that the authors benchmark against, as well as beat. Furthermore, the actual method the author's purpose seems more to do with tricking the LLM by asking it to do an action which the model does not realise to be harmful by its own standards, the authors referring to it as \u201chypnotising\u201d the model into a \u201crelaxed\u201d state. This contradicts the great efforts that the directors of the original experiment went through to make participants clearly aware of the harm of their actions, as well as the fact that the study is infamous for emotionally stressing the participants, opposed to relaxing them.\n\n+ Though the method works well, and does not require as much (1) compute, (2) access to data nor (3) access to the models as the methods it compares against, it provides much less of a technical contribution than those other works. Especially because fairly similar prompt templates have been proposed before and it is potentially easier to introduce a hard-coded filter for this type of attack than an adversarial optimisation-based one."
            },
            "questions": {
                "value": "Where did you get your information about the Milgram experiment? Checking it again, does the experiment agree with my criticisms?\n\nIs the \u201cjointly inducing\u201d property just trying to formalise what other work is doing, for example, when their objective is to have the model start their response with \u201cSure, here is how\u201d instead of directly optimising for the harmful output?\n\nIn table 3, why does self-reminder for GPT3.5 actually help the score?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper describes DeepInception, a jailbreak method for LLMs. The technique uses nested prompts to create multiple layers of fictional scenarios, tricking the model into giving harmful answers as part of the nested scenarios. Experiments demonstrate DeepInception's high success rates across various LLMs, including GPT-3.5, GPT-4, and Llama models, as well as multimodal models like GPT-4V. The method can induce a \"continuous jailbreak\" state, where LLMs remain compromised in subsequent interactions. The authors analyze the method's effectiveness, comparing it to other effective jailbreaks such as GCG and PAIR. The work aims to expose vulnerabilities in LLMs to promote better safety measures."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors present a successful jailbreak, uncovering vulnerabilities in LLM safety training.\n- The jailbreak is tested on a number of different LLMs from the Llama and OpenAI GPT families, amongst others.\n- The jailbreak is compared to other known effective jailbreaks such as GCG and Pair.\n- Efforts are made to ensure reproducibility such as sharing source code and providing the prompt template."
            },
            "weaknesses": {
                "value": "- The link to the Milgram experiment is tenuous and seems like an unnecessary aspect of the framing. In particular, the key innovation in the jailbreak appears to be the user of nested scenes, which doesn't have a direct analogue in the Milgram experiment. I would suggest either providing a more robust justification for this framing or considering removing/de-emphasizing it if the connection is not central to the method.\n- It's a shame the researchers did not provide more prompt templates or experiment more with the use of nesting. Instead, the paper focuses on a specific implementation of the jailbreak using the single nested prompt structure shown in the paper (section 3.3). Given its success, I'd be interested in seeing more variations on using nesting to jailbreak a model.\n- An existing body of work exists demonstrating the efficacy of nested scenarios in jailbreaking LLMs, such as Ding et al. 2024 (https://arxiv.org/abs/2311.08268). The usefulness of an additional specific jailbreak template from the category seems questionable."
            },
            "questions": {
                "value": "- Can you elaborate on your key contributions/the novelty of your work compared to existing work done on nested jailbreak such as Ding et al. 2024 (https://arxiv.org/abs/2311.08268), and the literature on prompt injection?\n- Why does the Milgram experiment suggest that nesting would be effective?\n- What does \"Gloss\" refer to in your title? I found the title a bit unnecessarily unclear/obscure. You could consider revising the title to more clearly reflect the paper's content and contributions, perhaps by explicitly mentioning key concepts like \"nested prompts\" or \"multi-layer jailbreak\".\n- Did you experiment with other similar prompt templates? Why did you decide to focus on the specific template presented in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}