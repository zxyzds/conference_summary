{
    "id": "s5N7p5UjgR",
    "title": "Markovian Transformers for Informative Language Modeling",
    "abstract": "Chain-of-Thought (CoT) reasoning holds great promise for explaining the outputs of language models, but recent studies have highlighted significant challenges in its practical application for interpretability. We propose to address this issue via two key components: a technique to factor next-token prediction through intermediate CoT text, ensuring the CoT is causally load-bearing, and a reinforcement learning approach to train CoT to predict future tokens independently of other context. This results in \"Markovian\" language models, where CoT serves as a fixed-size state for future token prediction. Our approach optimizes for \"informativeness\" \u2013 the improvement in next-token predictions using a trained CoT compared to a baseline. We demonstrate our method's effectiveness using Proximal Policy Optimization (PPO) on arithmetic problems and achieve an 11% performance boost on the GSM8K benchmark using Mistral 7B Inst V2. The increased sensitivity of model performance to CoT perturbations provides strong evidence of CoT reliance. This work advances the development of more transparent and interpretable language models, potentially enabling their extension to arbitrarily long contexts and enhancing AI reasoning capabilities across various domains.",
    "keywords": [
        "Chain of Thought Reasoning",
        "Reinforcement Learning",
        "Scalable Oversight",
        "Language Modeling",
        "Proximal Policy Optimization"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We introduce \"Markovian\" RL-training that ensures that LM Chain-of-Thought is causally load-bearing, while improving performance on many-step arithmetic and GSM8K over baselines, thereby advancing more transparent and interpretable AI reasoning.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=s5N7p5UjgR",
    "pdf_link": "https://openreview.net/pdf?id=s5N7p5UjgR",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a framework where the reasoning steps are used as fixed-size states, which limits the model\u2019s context (text bottleneck), and force the model to use the reasoning steps as input. This method design is inspired by the fact that past CoT literature find that the final answer might not be sensitive to the CoT trace. In the experiment, the authors show that the model trained with this method is indeed more fragile against CoT pertubations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The Markovian framework blocks the model from attending back to the original question and force it to use the CoT context for generation. This provides a new view and framework for analyzing CoT effects.\n- The reinforcement learning-based approach demonstrates improved performance on tasks requiring multiple steps, \n- The CoT steps generated from this method seem to be more interpretable, from two dimensions: 1) pertubation of the CoT could lead to more model errors 2) the reasoning can be carried over to another model."
            },
            "weaknesses": {
                "value": "-  While the approach improves the model\u2019s reliance on CoT, it\u2019s uncertain if this CoT is genuinely interpretable by humans. The transferrability between Mistral and Llama should only serve as an indirect proof.\n-  It is actually fine to focus on QA task, but probably we'd like to see how this can generalize to more domains other than arithemtic. Would this paradigm also work for other reasoning task as well?\n- There seem to be no baselines and ablation designed, so it is a bit hard to position the effectiveness of the method against other methods.\n- The fragility analysis is insightful but lacks depth. A more detailed investigation into which types of perturbations impact CoT reliability most could provide valuable insights.\n-  Writing-wise, the paper writing is clean, but probably some adjustment of the sections flow and emphasis would be nice.\n   -  For instance, while the method section is quite detailed, it\u2019s presented before establishing the limitations of existing CoT techniques clearly, which makes it harder to understand the innovation.\n   - There are few tables but quite a few definitinos and equations. I'd suggest consider streamline the method descriptions and move some of them to the appendix, while adding more discussion and insights in the main body."
            },
            "questions": {
                "value": "- Is there a way to combine the interpretability with actual human perception? Though informativeness here can be used to improve model quality, it is also very helpful from human level. This is probably mentioned in F. But it seems to me F is more about how to encode human interpretability in training.\n- In F it is mentioned that \"optimal CoT would be a compression of the question, which can potentially be difficult for humans \". Is this observed in your experiments?\n- Were there more ablation study or comparison conducted?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a metric to measure the informativeness of CoT tokens, and then uses RL to train the model to generate highly informative CoT tokens, in order to improve the correctness of the final answer. Experiments in random addition problems and GSM8K math problems demonstrate the effectiveness of the proposed metric and RL methods. The paper also shows that more informative tokens will bring gains in interpretability."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The technical ideas, including the proposed metric and RL methods, are new, well-motivated, and technically reasonable.  \n\nThe experiments in random addition and GSM8K are positive."
            },
            "weaknesses": {
                "value": "The experiments are limited to a synthetic math problem setting and GSM8K, and the only trained model is Mistral 7B. \n\nThe presentation needs a better organization. E.g., some major results are placed in the appendices, but training details are in the main paper."
            },
            "questions": {
                "value": "Have you tried other open-source models like llama? Not use CoT of Mistral in it, but use your method to finetune Llama."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces and explains the construction of a Markovian Language Model to study causality in chain-of-thought reasoning. With a limited state (the previous state and its observation) on which to condition, the model is trained (fine-tuned) to maximize an informativeness objective through PPO, and is empirically shown to improve performance on mathematics tasks such as GSM8K and toy addition problems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The setup of this causally-guided model is pretty novel, and the finding that this improves performance by optimizing on an \u201cinformation\u201d metric is an impactful finding. \n* The selection of the RL training technique (PPO) is supported by highlighting the limitations of other considered methods (expert iteration and policy gradient).\n* The work is written in a way that was simple to follow, which I appreciated.\n* The gains on GSM8K (24.64% --> 35.71%) are meaningful (although, a bit tucked away in the paper's text)."
            },
            "weaknesses": {
                "value": "* I understand the general intuition surrounding the design of the informativeness function, but it would be good to add some discussion on why the expected reward over the trajectory actually constitutes / addresses \u201cinformativeness\u201d under your construction. \n* While math tasks have a more well-defined structure (the order in which their steps may be pursued), this is less clear for other tasks without such a clear structure in natural language, for instance. It would be good to examine this approach on at least one such task to further support the method\u2019s general efficacy.\n* Despite the intuition-based process of selection for the RL training strategy, there are recent works that advocate in favor of expert iteration and REINFORCE / vanilla policy gradient for LLM reasoning and RLHF [1, 2]. To this effect, including such approaches for comparison in the results section (or in the appendix) would strengthen the defense of the PPO method chosen. It would be helpful to include some evidence supporting the limitations posed.\n* While I appreciate documenting the design choices in Section 4.3, some justification behind them would be beneficial, either through ablations (it\u2019s fine for these to be in the appendix) or relevant references. \n\n1. Teaching Large Language Models to Reason with Reinforcement Learning. Havrilla et al. 2024 \n2. Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs. Ahmadian et al. 2024"
            },
            "questions": {
                "value": "* Is the space of states task-conditional? This isn\u2019t apparent based on the formulation in Section 3.1 (and is unclear by the wording in line 162). If not, then it would seem that the set of relevant \u201cCoT states\u201d would be very sparse relative to the complete space. \n* As posed in the weaknesses section, does this method extend to other reasoning tasks (e.g. in natural language or code) whose structure is less \u201clinear\u201d?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses an issue of Chain-of-Thought (CoT) reasoning in LLMs where the LM's final answer does not always depend on the CoT. The paper's idea is to enforce informativeness by conditioning the answer model on the generated CoT only without other context. To this end, the paper formally defines Markovian Language Models and (informative) update functions, from which the policy gradient procedure is derived. Applying the framework to the specific use case of CoT reasoning, the paper experiments with several RL techniques such as expert iteration, policy gradient, and PPO. The model is applied to a simple arithmetic task of adding 15 numbers as well as GSM8K, and shows that the model a) improves performance on the task b) is sensitive to perturbation in the CoT reasoning and c) produces CoTs that are sensible to a different language model such that its performance on the task is improved."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "* The paper addresses an important limitation in Chain-of-Thought reasoning, which is of relevance to the broader ICLR community.\n* The core idea is intuitive and simple.\n* The paper is well written.\n* The results on the simple arithmetic task and the math task are promising.\n* The claims that the proposed method improves the generated CoTs in terms of interpretability and informativeness are well supported."
            },
            "weaknesses": {
                "value": "The method is evaluated only on few tasks and models, limiting how sure we can be that this is a useful method. Especially an application to language modeling would be very insightful and potentially extremely impactfull. However, while more is always better when it comes to experimental results, I think that this initial set of experiments support the ideas presented well and should suffice for publication."
            },
            "questions": {
                "value": "Please use different line styles in your figures so colorblind people can make sense of them. Otherwise Figure 2 and 3 are really hard to parse!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}