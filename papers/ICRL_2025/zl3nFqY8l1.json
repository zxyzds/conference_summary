{
    "id": "zl3nFqY8l1",
    "title": "RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models for Question Answering",
    "abstract": "Retrieval-augmented generation (RAG) framework has shown promising potential in knowledge-intensive question answering (QA) by retrieving external corpus and generating based on augmented context. However, existing approaches only consider the query itself, neither specifying the retrieval preferences for the retrievers nor informing the generators of how to refer to the retrieved documents for the answers, which poses a significant challenge to the QA performance. To address these issues, we propose Rule-Guided Retrieval-Augmented Generation with LMs, which explicitly introduces symbolic rules as demonstrations for in-context learning (RuleRAG-ICL) to guide retrievers to retrieve logically related documents in the directions of rules and uniformly guide generators to generate answers attributed by the guidance of the same set of rules. Moreover, the combination of queries and rules can be further used as supervised fine-tuning data to update retrievers and generators (RuleRAG-FT) to achieve better rule-based instruction following capability, leading to retrieve more supportive results and generate more acceptable answers. To emphasize the attribution of rules, we construct five rule-aware QA benchmarks, including three temporal and two static scenarios, and equip RuleRAG with several kinds of retrievers and generators. Experiments demonstrate that training-free RuleRAG-ICL effectively improves the retrieval quality of +89.2\\% in Recall@10 scores and generation accuracy of +103.1\\% in exact match scores over standard RAG on average across the five benchmarks, and further fine-tuned RuleRAG-FT consistently yields more significant performance enhancement. Extensive analyses indicate that RuleRAG scales well with increasing numbers of retrieved documents and exhibits generalization ability for untrained rules. Our code and benchmarks are available at [https://anonymous.4open.science/r/ICLR2025_RuleRAG_ICL_FT](https://anonymous.4open.science/r/ICLR2025_RuleRAG_ICL_FT).",
    "keywords": [
        "Rule-Guided Retrieval",
        "Rule-Guided Generation",
        "RAG",
        "Question Answering"
    ],
    "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)",
    "TLDR": "We point out two high-level issues of current RAG and propose a method named RuleRAG, including RuleRAG-ICL and RuleRAG-FT, to effectively improve the performance of multiple retrievers and generators by rule-guided retrieval and generation.",
    "creation_date": "2024-09-14",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zl3nFqY8l1",
    "pdf_link": "https://openreview.net/pdf?id=zl3nFqY8l1",
    "comments": [
        {
            "summary": {
                "value": "This paper designed a retrieval-augmented generation framework named RuleRAG to address the limitations of existing RAG approaches in knowledge-intensive question answering. RuleRAG leverages symbolic rules to guide the retrieval and generation processes \n to ensure that retrieved documents are logically relevant to the query and the generated answers properly refer to the retrieved documents. To validate the effectiveness of the proposed method, the paper introduces five new QA benchmarks that require reasoning and utilize rules based on existing benchmarks. Experimental results show that RuleRAG achieves strong performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- **Neural symbolic method**: RuleRAG explicitly incorporates symbolic rules into a neural language model, providing clear guidance for the retrieval stage and ensuring that the generated answers are logically consistent with the retrieved information.\n- **Comprehensive design and evaluation**: RuleRAG-ICL and RuleRAG-FT are designed for different scenarios, and both demonstrate strong generalization capabilities with various LLMs and retrievers."
            },
            "weaknesses": {
                "value": "- **Possibly limited scope of application**: I'm not sure whether the symbolic rules could be applied to real-world applications, since the motivating example is somewhat straightforward.\n- **Limited rule learning method**: Obtaining high-quality rules is non-trivial. This paper leverages some rule induction tools like AMIE on structured knowledge resources, and what if unstructured text?\n- **Unclear evaluation benchmark**: I do not understand why to construct evaluation data based on these benchmarks, which are not designed for the knowledge-intensive task. Why not consider more popular complex KBQA benchmarks, e.g., HotpotQA? On the other hand, these benchmarks may be seen when model training. Such choices may influence the convincingness of the conclusions."
            },
            "questions": {
                "value": "- **About the retrieval and generation**: I'm not sure whether the mappings between rules and retrieved results are recorded and used in the generation. \n- **About the conclusions in Section 5.2**: The second conclusion (i.e., the introduced rules can provide better guidance when using larger models with the same LLM architecture) and the third one (RGFT is fairly effective and necessary for lightweight LLMs) are somewhat contradictory."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel method that incorporates the rules from the existing knowledge base to aid the retrieval augmented generation. The rules are included in both retrieval and generation stage where the improvements are quite significant compared to the vanilla approach. The authors conduct extensive experiments with state-of-the-art LLMs and demonstrate the generalization of their approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper presents a novel research direction to add rules from knowledge base to help question answering.\n2. The experiments are solid with many existing SOTA models with convincing results.\n3. The authors also demonstrates the generalization of their rules.\n4. The code is open sourced which will help the community."
            },
            "weaknesses": {
                "value": "1. Which mining method is the best or how to choose the mining method is missing in the paper"
            },
            "questions": {
                "value": "Why do the authors not choosing some of the SOTA decoder-only retrievers, such as E5-mistral-7b-instruct? Will it better help the ICT compared to the BERT based methods, e.g. DPR, contriever?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed RuleRAG, a new approach to improve the performance of Retrieval-Augmented Generation (RAG) systems. Specifically, RuleRAG takes symbolic rules from the knowledge graph and introduces them into the retriever and generator. The proposed approach consists of two versions: RuleRAG-ICL (context learning) and RuleRAG-FT (fine-tuning), which have shown significant performance gains in several benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper proposes a graph augmented generation framework, RuleRAG, aimed at improving the RAG system by incorporating entity relationships from the knowledge base.\n2. The authors clearly outline their approach, including both RuleRAG-ICL and RuleRAG-FT. Details of the retrievers and generators are comprehensively explained, along with an in-depth description of the dataset construction.\n3. New rule-aware benchmarks were created to evaluate the proposed method, and extensive experiments demonstrate the method\u2019s effectiveness on these benchmarks ."
            },
            "weaknesses": {
                "value": "1. This paper proposes RuleRAG and claims that it uses a rule-guided method to build a RAG framework for question and answer tasks. However, the experiments conducted are not comprehensive enough: this paper tests the performance of the proposed method only on a self-constructed RuleQA-series dataset, which seems to be a KBQA task in disguise . Many representative RAG tasks such as NQ, TriviaQA (for short-form QA), ASQA ( for long-form QA), HotPotQA (for multi-hop QA) are not tested, which leads us to not be able to fairly observe the performance of RuleRAG in different scenarios.\n\n\n2. The RuleQA dataset proposed in this paper is constructed based on entities in the KB. However, the rules in the rule base, the queries in the test dataset and the documents in the corpus are also derived from the KB, does it mean that for every query in RuleQA, there exist exactly corresponding rules and documents to answer the question? Such a construction seems to be more favorable for RuleRAG, as it makes RuleRAG have correspondences for acquiring and introducing rules, however this is not possible in real-world QA. The authors should conduct experiments on a wide range of publicly credible datasets to validate the effectiveness of their approach.\n\n3. Some methods, such as IRCOT, Self-RAG and GraphRAG, are not  compared. They also focus on extracting more accurate query-related contents for building a more effective RAG system.\n\n4. The backbone retriever is not new. Some methods, such as ANCE and BGE, are not compared."
            },
            "questions": {
                "value": "1. How to control and estimate the quality of the constructed dataset?\n2. The dataset link should also be anonymous."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel framework for improving knowledge-intensive question answering (QA) by integrating symbolic rules into the Retrieval-Augmented Generation (RAG) paradigm. The authors identify two main limitations of standard RAG models: 1) insufficient retrieval relevance, as retrievers often fail to capture logical relationships, and 2) lack of explicit guidance for language models on how to utilize retrieved documents.\n\nTo address these, RuleRAG incorporates symbolic rules to guide both the retrieval and generation phases, enhancing answer accuracy. The framework operates in two modes: **RuleRAG-ICL** (In-Context Learning), which uses rules to steer retrieval and generation in a training-free manner, and **RuleRAG-FT** (Fine-Tuning), which fine-tunes models to strengthen rule adherence. Additionally, the authors develop five rule-aware QA benchmarks to test the system across temporal and static knowledge scenarios. Experimental results show effective improvements in retrieval relevance (Recall@10) and answer accuracy (Exact Match) compared to standard RAG, showcasing RuleRAG's ability to generalize to unseen rules and scale effectively."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper conducted extensive experiments and analyses to validate the effectiveness of the proposed method, despite some biases in the experimental setup.\n2. The motivation for introducing rules to enhance the utilization of documents is reasonable, and experiments have shown that this method indeed helps improve the model's performance on the evaluation datasets compared to direct RAG."
            },
            "weaknesses": {
                "value": "1. The evaluation set is not robust enough: The method's self-constructed dataset seems to favor scenarios where rule-based approaches are required to answer correctly, which introduces some bias. The authors did not evaluate the method on more widely and commonly used benchmarks such as NQ, TQ, HotpotQA, StrategyQA, etc.\n\n2. The baseline models are not comprehensive: Comparing only with direct RAG seems somewhat weak. Currently, there are more variants of RAG, such as decomposing questions before retrieval, which can also address multi-hop retrieval to some extent.\n\n3. Limited generalizability: The paper does not conduct experiments on a broader range of datasets, making it difficult to demonstrate the method's generalizability, especially in scenarios where large models have been fine-tuned."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents RuleRAG, an innovative approach that combines rule-guided retrieval and generation for knowledge-intensive question answering. This method addresses the limitations within existing RAG frameworks by utilizing symbolic rules to direct both retrievers and generators. RuleRAG enhances performance through in-context learning and fine-tuning processes. Additionally, it establishes rule-aware QA benchmarks and shows substantial improvements over standard RAG across various metrics."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. RuleRAG uniquely combines rule-based guidance with retrieval and generation, showing promising results in knowledge-intensive QA.\n2. RuleRAG has the capability to generalize beyond the rules it has been specifically trained on, albeit with less than optimal performance. This ability highlights its significant advantage and potential for broader applications beyond its original rule set.\n3. The construction of five rule-aware QA benchmarks, including both temporal and static scenarios, provides a thorough evaluation of RuleRAG's capabilities and its scalability across different QA tasks.\n4. RuleRAG shows significant improvement over standard RAG, and the experimental results appear solid."
            },
            "weaknesses": {
                "value": "1. The performance of the RuleRAG framework is heavily dependent on the quality and coverage of the mined rules, which may not always be comprehensive or accurate. The effectiveness of RuleRAG hinges on the accuracy of the rule-mining process; if the algorithms used (AMIE3 for static KGs and TLogic for temporal KGs) fail to extract high-confidence rules, the performance of the entire framework could be compromised.\n2. The paper highlights RuleRAG's potential vulnerability to irrelevant or misleading rules, a critical issue for question-answering systems using external knowledge. It lacks detailed mechanisms for filtering such rules, risking suboptimal performance during retrieval and generation phases. The authors could consider implementing or evaluating specific filtering mechanisms, such as incorporating a rule relevance scoring step, or exploring the feasibility of using the LLM itself to assess rule applicability prior to retrieval.   \n3. This paper emphasizes individual rules and their impact on QA performance but does not explore the interactions between multiple rules or complex rule hierarchies, which are essential for handling more sophisticated queries."
            },
            "questions": {
                "value": "1. How does RuleRAG handle exceptions or anomalies, particularly for domain-specific cases (e.g., outliers in temporal data or unrecognized entities) that do not conform to the predefined rules?   \n2. Given that the quality and coverage of these rules can significantly impact overall performance, what specific strategies can be implemented to ensure the diversity and representativeness of the rules extracted from the knowledge graph? For example, how can the authors incorporate techniques to evaluate rule completeness or prioritize rules based on domain relevance?   \n3. At times, the retrieved content might not directly include answers or rules, but still plays a crucial role in helping an LLM understand and respond to inquiries. How does RuleRAG ensure that this valuable contextual information is retained, particularly in cases where the relevant content may be ambiguous or indirect?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}