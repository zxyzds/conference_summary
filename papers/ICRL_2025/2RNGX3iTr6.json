{
    "id": "2RNGX3iTr6",
    "title": "Tabby: Tabular Adaptation for Language Models",
    "abstract": "While advances in large language models (LLMs) have greatly improved the quality of synthetic text data in recent years, synthesizing tabular data has received far less attention. Many of the top-performing approaches to this problem rely on techniques that adapt models originally developed for other modalities, potentially leaving generative performance on the table. We address these disparities in attention and performance for tabular data by introducing Tabby, a simple but powerful post-training modification to the standard Transformer-based language model architecture that enables its use for tabular dataset synthesis. Tabby relies on Gated Mixture-of-Experts layers, allowing each data column to be modeled by a dedicated set of parameters within the transformer multi-layer perceptrons or language modeling heads.  Applying Tabby to Distilled-GPT2 improves synthetic data quality up to 7% compared to previous tabular dataset synthesis methods, achieving performance near or equal to that of real data.",
    "keywords": [
        "tabular",
        "generative",
        "llm",
        "mixture-of-experts",
        "synthesis",
        "transformer"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose Tabby, a post-training architecture modification to transformer-based Large Language Models, which enables the synthesis high-fidelity tabular data.",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=2RNGX3iTr6",
    "pdf_link": "https://openreview.net/pdf?id=2RNGX3iTr6",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes to use a MOE LLM fine tuned on table data for synthetic table data synthesis.  The authors find that their method outperforms previous methods on table synthesis benchmarks."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The proposed method outperforms other methods on most datasets and metrics presented."
            },
            "weaknesses": {
                "value": "* There is no significant difference between tabby and the non-tabby (I assume no MOE?) baseline.  Given that MOE has a lot more parameters, this is a negative finding.\n* The papers contributions are very minor - applying MOE to a narrow problem (table generation).  And the results are not all that strong.\n* It's not easy from the presentation what exactly do the tasks require, what exactly are the baselines and model variations."
            },
            "questions": {
                "value": "Can you please detail the various architectures MMLP, MH and MMLP+MH? \nWhy does MMLP+MH underperform, even though it is more complex?\nDo you replace every layer with MOE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces a new model called Tabby for tabular data. Tabby is an architecture modification that enables transformer-based language models to synthesize more realistic tabular data. It introduces Gated Mixture-of-Experts layers to better model the complex interdependencies and diverse data types found in tabular datasets. Tabby outperforms previous tabular data synthesis methods, achieving outstanding performance on multiple benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Tabby achieves strong performance in benchmark evaluation. It generates high-quality synthethic tabular data in comparison with the baseline methods.\n2. The introduction of MoE shows effectiveness in helping the model understand tabular data structure and generate higher-quality tabular data."
            },
            "weaknesses": {
                "value": "1. The design of MoE layer is complex. For a table with V columns, this article should design an MoE model with V experts to adapt to the table. This is not generalizable to data of diverse formats. It is suggested to modify the model design to be more compatible and more generalizable.\n2. Scalable experiments are advised to be conducted. This study needs to provide experimental results on datasets of larger scales and also more commonly used datasets.\n3. The experiments are advised to be conducted on contemporary large language models, including Llama, Qwen, Mistral, instead of Distilled-GPT2."
            },
            "questions": {
                "value": "1. Have you conducted experiments on the recently released large language models? If yes, which model sizes did you choose?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors present a tabular data synthesis approach, Tabby. The novelty of Tabby lies in two main aspects: (1) modifying the original transformer model by applying MoE-like techniques to better model tabular data, and (2) designing a specialized data format for tabular data. Experimental results show that Tabby achieves comparable performance to the previous state-of-the-art, Tab-DDPM, and outperforms GTT NT."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The model modifications and data organization are well-motivated and intuitive.\n- The distribution of the synthesized data is very close to the natural data.\n- The experimental results looks good."
            },
            "weaknesses": {
                "value": "- Tabby seems achieve comparable results to Tab-DDPM with marginal performance gain in Table 2.\n- The method is quite simple and not much effective in final performance."
            },
            "questions": {
                "value": "Q1: Have you computed the FLOPs for training on different datasets? It seems that Tabby uses a fixed pattern to organize tabular data, which may require more tokens for computation.\n\nQ2: Regarding Claim 2, could you provide a scaling curve showing performance relative to model size or data quantity? It would be interesting to see how Tabby impacts different models and how the amount of Tabby data influences the learning process. Additionally, a comparison of the scaling curve between Tabby data and natural data would serve as evidence of Tabby data being a scalable alternative to natural data.\n\nQ3: I'm not sure if the modification to original network is necessary. Is there an ablation study?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}