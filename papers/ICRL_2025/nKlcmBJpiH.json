{
    "id": "nKlcmBJpiH",
    "title": "Optimistic Gradient Learning with Hessian Corrections for High-Dimensional Black-Box Optimization",
    "abstract": "Black-box algorithms are designed to optimize functions without relying on their underlying analytical structure or gradient information, making them essential when gradients are inaccessible or difficult to compute. Traditional methods for solving black-box optimization (BBO) problems predominantly rely on non-parametric models and struggle to scale to large input spaces. Conversely, parametric methods that model the function with neural estimators and obtain gradient signals via backpropagation may suffer from significant gradient errors. A recent alternative, Explicit Gradient Learning (EGL), which directly learns the gradient using a first-order Taylor approximation, has demonstrated superior performance over both parametric and non-parametric methods. In this paper, we extend EGL by incorporating second-order Taylor corrections via the function's Hessian, improving robustness in complex, highly non-linear problems. Additionally, we refine EGL\u2019s sampling strategy and loss function, enhancing its capability to efficiently address high-dimensional optimization problems. We term our approach as Optimistic Higher-Order Gradient Learning (OHGL) and validate it on the synthetic COCO suite. We also demonstrate its applicability in two high-dimensional real-world machine learning (ML) tasks: adversarial training and code generation. The results show that OHGL produces stronger candidates and serves as a valuable tool for ML researchers and practitioners tackling high-dimensional, non-linear contemporary optimization challenges.",
    "keywords": [
        "Black box optimization",
        "Derivative free optimization",
        "Gradient free optimization",
        "Large language model code geneation",
        "Adversarial training"
    ],
    "primary_area": "optimization",
    "TLDR": "We develop a black-box optimization algorithm that learns gradients with neural models and can be applied to solve non-convex high dimensional real-world\u00a0problems",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=nKlcmBJpiH",
    "pdf_link": "https://openreview.net/pdf?id=nKlcmBJpiH",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces an enhanced method for black-box optimization (BBO) by specifically addressing challenges in high-dimensional and complex optimization landscapes. The proposed approach, termed Optimistic Higher-Order Gradient Learning (OHGL), builds on previous methods like Explicit Gradient Learning (EGL) by incorporating both optimistic gradient estimation and Hessian corrections. By leveraging second-order Taylor expansions and Hessian information, the method improves gradient accuracy. The authors propose a sampling profiler to reduce required samples and refine trust region adjustments for enhanced convergence efficiency. It demonstrates strong potential in machine learning tasks where gradient information is inaccessible or costly to compute."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper appears to be well-grounded in both theoretical and experimental methodologies. The use of both first- and second-order gradient information for better accuracy in high-dimensional spaces is a technically sound choice, demonstrating an understanding of gradient approximation complexities in BBO.\n\nThe breakdown of components like Optimistic Gradient Learning and Higher-Order Gradient Learning helps clarify how each extension builds on EGL, and the inclusion of figures and empirical results assists in understanding performance benefits. However, a possible point for improvement might be adding more intuitive explanations or visual aids for readers less familiar with gradient learning concepts."
            },
            "weaknesses": {
                "value": "1. Computational Complexity and Efficiency:\nThe inclusion of Hessian corrections, while beneficial for accuracy, introduces substantial computational cost, particularly in high-dimensional spaces. \n\n2. Generalization of Results:\nThe results focus heavily on the COCO test suite and two specific applications. However, broader generalizability to other complex, black-box settings, such as reinforcement learning or sequential decision-making tasks, remains unaddressed. \n\n3. Trust Region's Limitations:\nThe algorithm's handling of boundary conditions may slow down convergence in real-world, high-dimensional applications due to excessive shrinking, which could be inefficient in practice."
            },
            "questions": {
                "value": "1. The inclusion of comparisons to more recent methods like Bayesian optimization frameworks or evolutionary algorithms would strengthen the claims regarding OHGL\u2019s effectiveness.\n\n2. The organization could be refined for flow, as several sections (like trust region management and adaptive sampling) feel slightly disjointed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Optimistic Higher-Order Gradient Learning (OHGL), using second-order Taylor approximation, for solving black-box optimization (BBO) problems. Furthermore, the authors refine EGL's sampling strategy and loss function to improve its efficiency in high-dimensional settings. The proposed OHGL method is validated through experiments on the synthetic COCO suite and applied to two real-world machine learning tasks: adversarial training and code generation. The experimental results indicate that OHGL generates stronger candidate solutions, positioning it as a valuable tool for machine learning researchers and practitioners addressing high-dimensional, non-linear optimization challenges."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The concept of enhancing Explicit Gradient Learning (EGL) with second-order corrections is both interesting and novel, to the best of my knowledge.\n2. The paper is generally well-written and presents its ideas in a clear and accessible manner."
            },
            "weaknesses": {
                "value": "1. My primary concern relates to the computational cost associated with Hessian computation or approximation, which may hinder the scalability of the proposed method. Given that memory- and computation-efficient Hessian approximation techniques are well-established in the literature, I strongly recommend that the authors evaluate their proposed method using more efficient approximations at larger scales.\n\n2. The enhancements over EGL are attained through the integration of several design elements; however, a thorough ablation study is notably absent from the paper."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes an OHGL framework to solve black-box optimization. It improves the previous EGL framework by incorporating second-order Taylor corrections via the function\u2019s Hessian and assigning higher weights to samples with lower objective values. Experiments on adversarial attack and code generation show its potential in high-dimension real-world problems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed Hessian corrections and weighed gradient are well-motivated, effectively addressing the limitations of the previous EGL framework.\n2. Real-world high-dimension examples, including adversarial attacks and code generation, demonstrate the scalability and efficiency of this framework."
            },
            "weaknesses": {
                "value": "1. Compared to previous work, such as EGL or model-based methods [1], the theoretical understanding of the proposed Hessian corrections and weighted gradients is lacking. A more rigorous theoretical analysis would help justify the design choices\n\n2. The work introduces several algorithm-level designs in Sec. 5, such as adaptive sampling size and trust-region management. The tolerance analysis in Section 6.2 suggests that the influence of these designs can accumulate over time, potentially leading to significant changes in performance. However, the paper does not provide clear guidance on how to tune these parameters for different black-box optimization tasks with a limited budget.\n\n[1] Kumar, A., & Levine, S. (2020). Model inversion networks for model-based optimization. Advances in neural information processing systems, 33, 5126-5137."
            },
            "questions": {
                "value": "1. The OHGL framework incorporates Hessian information during the training stage. However, it is unclear why the second-order information is not utilized when updating the next point (line 278).\n2. Is the hyperparameter tuning process conducted for all baselines? \n3. Which design in the OHGL framework contributes the most to the observed performance improvements?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper extends the gradient descent learning method by incorporating second-order Taylor corrections through the function's Hessian, enhancing robustness in complex, highly non-linear problems. The OGL, HGL, and OHGL algorithms are presented and validated on the synthetic COCO suite, as well as in two scenarios: adversarial attacks and code generation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The idea of Optimistic Gradient Learning and Higher-Order Gradient Learning is novel.\n2. These new algorithms have achieved better results in experiments."
            },
            "weaknesses": {
                "value": "1. The reference format is not uniform. I suggest the authors improve them.\n2. There are some mistakes in this paper, for example, the CE loss in the title of Table 3 is not reflected in the table.\n3. It seems that the most recommended algorithm in this paper, OHGL, has not been applied in real-world scenarios."
            },
            "questions": {
                "value": "1. I think the experiments of HGL and OHGL in adversarial attacks and code generations can further enhance persuasiveness.\n2. Format modification is necessary, reference and appendix are confusing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}