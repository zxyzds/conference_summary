{
    "id": "IiagjrJNwF",
    "title": "Memory Mosaics",
    "abstract": "Memory Mosaics are networks of associative memories working in concert to achieve a prediction task of interest. Like transformers, memory mosaics possess compositional capabilities and in-context learning capabilities. Unlike transformers, memory mosaics achieve these capabilities in comparatively transparent way (\u201cpredictive disentanglement\u201d). We illustrate these capabilities on a toy example and also show that memory mosaics perform as well or better than transformers on medium-scale language modeling tasks.",
    "keywords": [
        "predictive disentanglement",
        "Associative memory",
        "language model"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=IiagjrJNwF",
    "pdf_link": "https://openreview.net/pdf?id=IiagjrJNwF",
    "comments": [
        {
            "comment": {
                "value": "Please read our general responses, especially about the (non)-relation to Linear Transformers.\n\n1) *The core concept presented in Figure 3 bears similarities to Linear Transformers [1], particularly RetNet [2] (see RetNet Eq. 6). However, the paper fails to adequately discuss or cite these highly relevant works*\n\nAs explained in the general response, our work is entirely orthogonal to work on Linear Transformer. In particular, equation (7) makes clear that the computational cost is quadratic, like that of an ordinary transformer. The only connection to Linear Transformer are the engineering opportunities mentioned as future work in the last paragraph of the conclusion.\n\n2) *The paper appears to present an incremental advance on linear transformers, repackaged with new terminology and concepts. While this approach creates an impression of novelty, its actual contribution to advancing the field seems limited. The work seems to complicate a relatively straightforward evolution of linear transformer models with complex conceptual packaging, which may hinder rather than help the development of the research community and the broader field.*\n\nWe can affirm that this paper does not contain an incremental advance to linear transformers, because it is not about linear transformers at all. \n\n3) *The authors introduce persistent memory units (P-mem) but do not clearly differentiate them from standard MLP layers. They state that P-mem can be seen as MLP layers (lines 328-330), but fail to elucidate the advantages or unique properties of P-mem that justify its introduction as a new concept. This raises concerns about the necessity and value of creating this new terminology.*\n\nPlease check our general response 3.\n\n4) *While the paper uses RegBench to assess in-context learning, this benchmark alone may not provide a comprehensive evaluation of the model's memory capabilities. The performance of state-space models like Mamba can be significantly impacted by the inclusion of short convolutions, as noted in DeltaNet [4] (See Figure 2). A more thorough evaluation...*\n\nApart from the Memory Mosaic results, all results in Figure 9 were obtained by Akyurek et al. in the RegBench paper. Their code uses the publicly available implementations in a very well organized way. Since our work is not related to Linear Transformer and variants, we do not see a reason to relitigate the Regbench results. Note however that there is a regime in which the Mamba and H3 performances exceeds that of transformers, but, even in this regime, both are much worse than Memory Mosaics (and that is our point).\n\n5) *The paper focuses primarily on language modeling tasks but lacks evaluation on standard common reasoning benchmarks that are crucial for assessing language models' capabilities. Tasks such as ARC Challenge, ARC Easy, PIQA, and Hellaswag are widely used to evaluate smaller-scale language models and provide valuable insights into a model's reasoning abilities. The absence of these evaluations limits our understanding of the model's true capabilities and makes it difficult to compare with other approaches in a comprehensive manner. Including these benchmarks would provide a more holistic view of the model's performance across various aspects of language understanding and reasoning.*\n\nGlad you asked.  At the relatively small GPT-2 scale, we find that these benchmarks tell very little about the actual model performance at scale. See for instance the recent Apple paper (https://arxiv.org/pdf/2410.05229) revealing the brittleness of such \"reasoning\" benchmarks.\n\nWe like the TinyStories approach (https://arxiv.org/abs/2305.07759) because it provides the means to probe large language modeling concerns in small models. We find that a deep dive into the TinyStories/BabyStories performance is a much better predictor of the general language modeling performance of a similar model on a much larger scale. Our ongoing scaling work certainly agrees with this claim made in the TinyStories paper."
            }
        },
        {
            "comment": {
                "value": "Many thanks for your very detailed review.\n\n1) *From statistical physics it is well known that the normalizing factor (Z in Equation 2) is nearly always intractable for large enough systems.*\n\nThe normalization factor in equation 2 is a sum of n terms where n is the number of key/values pair stored in the memory, which is equal to the number of past tokens visited in the input sequence, and always inferior to the maximal context length. This number of terms is perfectly tractable.  The normalization of the key vectors does not change it.  \n\n2) *The authors discuss \"peeking\" in the Memory Mosaic, [...] ahead would be considered equivalent to a bug.*\n\nThis is indeed a key component of our architecture with nontrivial consequences. Please check our general response 2 for a rephrasing of this mechanism and its consequences. When presenting our work in front of an audience, we found it useful to spend more time on the explanation in lines 99-102 of the paper. Maybe these lines should be highlighted more prominently.\n\n3) *If my understanding of the paper is correct, training a neural network [...] is presented here as meta-learning, with the \"regular\" learning being the updates to the associative memories performed when prediction of a time series occurs. This is a fairly radical change in perspective,*\n\nThis is exactly what we mean (see lines 155-159) and this interpretation is critical for our paper.\n\n4) *Does the Memory Mosaic always benefit from fast-learning memory units? [...] is it possible that slow-learning memory units result in more accurate predictions (for any number of reasons).*\n\nThis is a good question. What we observe is that the (meta)-training process wants fast-learning memory units, and therefore constructs disentangled memory units. It might be that this is not the optimal way to model sequences that strictly follow the training set distribution. However, this property comes handy when the input sequence is out-of-distribution because it allows the network to quickly adapt to this new distribution. See Figures 8 and 9 for examples. This is key to in-context learning which itself plays an important role in the performance of transformer-like architectures.\n\n5) *...  if this specific kind of \"easy\" disentanglement is present in other, non-toy datasets. Mining this sort of information from, say, a language-based dataset would be nigh impossible, but some discussion about whether the property is present in other datasets (or even if this is likely to occur) would make for a nice conclusion to the Section*\n\nWe actually discuss this point lines 361-367 in the beginning of the language Section.  In addition, mining this sort of information from text might in fact be easier because, unlike moons, language was constructed to be easily learnable (an old argument in structural linguistics).\n\n6) *Is the correct interpretation here that the rows of these matrices could be permuted freely, and hence are equivalent to the identity? [In the three moons network]*\n\nThis is correct. The network architecture is symmetric with respect to an arbitrary permutation of the three heads.  \n\n7) *The main language-based task presented in this paper is based on BabiStories. [...] Due to the similarities to TinyStories, it seems that BabiStories must include some additional structure that makes it useful when working with Memory Mosaics.*\n\nWe would have preferred to use the original TinyStories dataset. The unfortunate legal reason for constructing a new one is evoked line 376. To our knowledge, there is nothing in the BabiStories dataset that favors Memory Mosaics. Instead we went to some length to ensure that the BabiStories dataset behaves like the TInyStories dataset. \n\n8) *The performance gap is only substantial for a model depth of 1. [...] It is still interesting that GPT-2 and Memory Mosaics perform equally well for larger depths, does this hint that perhaps the equivalent performance is due to the dataset rather than the architecture i.e. both architectures can saturate to the best possible accuracy on BabiStories?*\n\nThe gap at model depth 1 is explained in footnote 6. What these plots show is that Memory Mosaics (without hyperparameter search) essentially match Transformers for in-distribution performance.  However, surprisingly strong differences appear with out-of-distribution inputs as in Figure 8 and 9.\n\n9) The hyperparameter transfer from GPT-2 to Memory Mosaics is also terrifically interesting\n\nWe do not believe that the GPT2 optimal hyperparameters are optimal for Memory Mosaics. We simply find it too easy to manipulate the performance by working on the hyperparameter search. Therefore we chose a protocol that completely avoids this risk."
            }
        },
        {
            "comment": {
                "value": "Please check our general response with respect to the work on the modern Hopfield networks (we find this work interesting but we also believe that it is only weakly related, and that it does not provide the right mindset to understand our paper) . \n\n1) \u200b\u200b*Please highlight a clear definition of predictive disentanglement and explain intuition behind it.*\n\nPlease see our general response 2.\n\n2) *Please make the discussion of associative memory based modifications of Transformers more scientifically balanced. At present, the submission leaves a strange impression that the work from FAIR is unreasonably promoted (e.g., Bietti et al, Weston et al., etc), but the work from elsewhere is ignored.*\n\nPlease see our general response 1 and observe that our use of inference time memories is far more connected to the memory networks of Weston et al (which predate transformers) than to the modern Hopfield network (see our second sentence line 20). We'll make this clear.\n\n3) *Are the proposed networks energy-based or not?*\n\nThey are not. \n\n4) *Associative memories with Gaussian kernel smoothing (equation 2) have been studied in End-to-end Differentiable Clustering with Associative Memories, ICML 2023. Please explain in the revised paper how your approach is different/similar to that prior work.*\n\nWhat we do is a direct application of the Nadaraya-Watson estimator of 1964, also known as kernel regression. We\u2019ll cite this original reference as well as the reference you suggest (which contains additional ideas that are not relevant to our work).\n\n5) *What aspect of P-mem makes it persistent? Please include a detailed description of this network with explicit formulas describing how it is implemented. The figure presented in (Fig 6 right) is insufficient for understanding how it is defined.*\n\nPlease see our general response 3.  Persistent memories only play a supporting role in this paper. We call them \u201cpersistent\u201d because they store key/value pairs that are determined during training and frozen during inference. In contrast the other memories are cleared at the beginning of each test sequence and are filled during inference.\n\n6) *Formula 5 confuses me. Wouldn\u2019t such a definition of values trivially leak the information about the future to the output of the model? In other words, what prevents the model from simply copying that future token and output that copy as a next token prediction?*\n\nAs rephrased in our general response 2, the output of the memory unit $y_T$ at time $T$ only depends on the past of the input sequence. It does not depend on $v_T$ which is only computed at time $T+1$ in order to store a new pair $(k_T,v_T)$ into the memory. There is no leak of information here. You can also see this in equation (7) where the summation stops at $T-1$ in order to exclude $v_T$.\n\n7) *Why were the hyperparameters on language modeling tuned for Transformers, but not for the proposed model?*\n\nWe did this in order to be absolutely certain that our model matches or outperforms the transformer. It is far too easy to obtain good numbers with a subtly fancier hyper parameter search. Therefore we follow a protocol that totally avoids this risk."
            }
        },
        {
            "comment": {
                "value": "1) *Could you dive a bit deeper into the architecture of the Memory Mosaics for us? I'm particularly curious about the specifics of the associative memory units, what kind of activation functions you're using, and how the layers are configured. And also the comparison with other non-transformer models like rwkv and mamba?*\n\nPlease check our general responses as well as sections 2 and 6 in the paper. In particular there are no activation functions in our networks. The only non-linearities are the RMS normalizations and those of equation (7). The layer configurations are explicit and footnote 5 precisely compares the number of weights in our model with those of the GPT-2 model.\n\n2) *I'd love to understand more about how you set up the associative memories initially. How do you initialize and update the parameters during training?*\n\nPlease check our general response 2. The memories operate at inference time. They\u2019re cleared at the beginning of each sequence and acquire new key/value pairs at each time step. The gradient training process only determines the nature of the stored keys and values.\n\n3) *What led you to choose the hyperparameters that you did for this study? And how does changing these hyperparameters affect the model's performance?*\n4) *Did you perform any hyperparameter tuning experiments? If yes, could you share how you went about it and what the outcomes were?*\n\nIn section 6, we chose to stay as close as possible to the GPT2 architecture, and decided to simply reuse the hyperparameters that were optimal for the GPT2 baseline, without searching for better ones. Although this places Memory Mosaics at a slight disadvantage, it ensures that our positive results are not a consequence of a fancier hyperparameter search. \n\n5) *Are there any ablation studies that you've conducted or plan to conduct? It would be really helpful to see how each component of the model contributes to its overall performance.*\n\nThere is not much to ablate without making the system incoherent. We have experimented with various replacements for the key and value extraction functions (equation 6) and found that they made little difference as long as they each involved at least two successive inputs. \n\n6) *I'm interested in the predictive disentanglement feature of your model. Could you explain how altering or removing this feature affects the model's performance?*\n\nPredictive disentanglement is not a feature that we can add or remove at will, but a phenomenon that can be understood when one realizes that the training process is in fact a meta-training process. Please refer to the general response 2.\n\n7) *Have you conducted any scalability tests, particularly in comparison with transformers? I'm curious about how the model scales with larger datasets and how computationally efficient it is.*\n\nSince writing the paper, we have scaled the architecture to models with a couple billion parameters without particular problems. We also made further strides in leveraging the good properties suggested in figures 8 and 9. However this new research is not ready for publication at the current time."
            }
        },
        {},
        {
            "summary": {
                "value": "\"Memory Mosaics\" introduces an architecture where multiple associative memories are employed in unison to enhance predictive capabilities for tasks like language modeling. This architecture aims to combine the benefits of transformer models with greater transparency and efficiency through what the authors term \"predictive disentanglement.\" The paper is technically rich, articulating a clear hypothesis, describing the underlying mechanisms in detail, and providing comparative analysis against existing models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The integration of associative memories to replicate and surpass the capabilities of transformers.\n2. The concept of predictive disentanglement is novel and also rooted in a solid theoretical framework.\n3. The theoretical motivations behind predictive disentanglement are well-explained."
            },
            "weaknesses": {
                "value": "1. The paper does not provide exhaustive details on the architecture's configuration.\n2. Lack of detailed discussion on the choice and impact of hyperparameters.\n3.The experimental validation is limited to certain language tasks. \n4. Lack of Ablation Studies"
            },
            "questions": {
                "value": "1. Could you dive a bit deeper into the architecture of the Memory Mosaics for us? I'm particularly curious about the specifics of the associative memory units, what kind of activation functions you're using, and how the layers are configured. And also the comparison with other non-transformer models like rwkv and mamba? \n2. I'd love to understand more about how you set up the associative memories initially. How do you initialize and update the parameters during training?\n3. What led you to choose the hyperparameters that you did for this study? And how does changing these hyperparameters affect the model's performance?\n4. Did you perform any hyperparameter tuning experiments? If yes, could you share how you went about it and what the outcomes were?\n5. Are there any ablation studies that you've conducted or plan to conduct? It would be really helpful to see how each component of the model contributes to its overall performance.\n6. I'm interested in the predictive disentanglement feature of your model. Could you explain how altering or removing this feature affects the model's performance?\n7. Have you conducted any scalability tests, particularly in comparison with transformers? I'm curious about how the model scales with larger datasets and how computationally efficient it is."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This submission proposes replacing parts of the Transformer block with associative memories. The notion of predictive disentanglement is discussed. The proposed architecture is evaluated on 3-moon problem and small scale language modeling tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "In general, this is a though-provoking paper with some interesting ideas. Exploration of the relationship between associative memories, attention, and Transformer blocks is valuable, although the current presentation is heavily biased, and omits related ideas from prior work.  \n\nThe empirical results on language modeling are encouraging, although small scale."
            },
            "weaknesses": {
                "value": "One problem with this submission is that the presentation almost entirely ignores the work on modern Hopfield networks and dense associative memories, which tackles closely related motivation and ideas. Specifically, the authors\u2019 proposal is closely related to [Energy Transformer (NeurIPS 2024)](https://proceedings.neurips.cc/paper_files/paper/2023/file/57a9b97477b67936298489e3c1417b0a-Paper-Conference.pdf) and related literature, which replaces elements of Transformer block with associative memories. The only cited paper representing that line of work (Ramsauer et al, 2020) is only briefly mentioned. Please notice, that paper only established a connection with associative memory at a single step update, which is different from the setting used in this submission - dynamics unfolded in time with shared weights, if I understood equations 4 and 5 correctly. In contrast, the Energy Transformer formalizes attention operation and the entire Transformer block as a form of associative memory, with recurrent dynamics unfolded in time and shared weights. There are, of course, many differences between these prior works and the current proposal. For instance, it is unclear to me if energy-based description is important at all for this submission. Regardless these similarities/differences this and related works need to be referenced at a prominent place in the introduction and similarities/differences with the current proposal need to be extensively discussed in the revised manuscript. \n\nI have read this submission several times, but to be frank with the authors I am not sure I understand what they mean by predictive disentanglement, which seems to be the core concept here, but defined only vaguely. Unfortunately, I do not have a specific recommendation how one could improve the presentation in section 3. But, at present, it is insufficient for me. What architectural aspects of memory mosaics suggest that predictive disentanglement should hold?  \n\nP-mem network is insufficiently explained. Please include specific operations that are used in it in the revised paper (with formulas)."
            },
            "questions": {
                "value": "1. Please highlight a clear definition of predictive disentanglement and explain intuition behind it.\n2. Please make the discussion of associative memory based modifications of Transformers more scientifically balanced. At present, the submission leaves a strange impression that the work from FAIR is unreasonably promoted (e.g., Bietti et al, Weston et al., etc), but the work from elsewhere is ignored. \n3. Are the proposed networks energy-based or not? \n4. Associative memories with Gaussian kernel smoothing (equation 2) have been studied in [End-to-end Differentiable Clustering with Associative Memories, ICML 2023](https://proceedings.mlr.press/v202/saha23a/saha23a.pdf). Please explain in the revised paper how your approach is different/similar to that prior work. \n5. What aspect of P-mem makes it persistent? Please include a detailed description of this network with explicit formulas describing how it is implemented. The figure presented in (Fig 6 right) is insufficient for understanding how it is defined. \n6. Formula 5 confuses me. Wouldn\u2019t such a definition of values trivially leak the information about the future to the output of the model? In other words, what prevents the model from simply copying that future token and output that copy as a next token prediction? \n7. Why were the hyperparameters on language modeling tuned for Transformers, but not for the proposed model? \n\n\nI am keeping an open mind, and willing to consider increasing the scores depending on authors\u2019 responses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new neural network architecture, the Memory Mosaics, which employ associative memories to complete prediction based tasks. The Memory Mosaic architecture consists of a number of \"elementary memory units\", which in turn consist of a feature extractor (outputting a time series of \"key\" vectors and a time series of \"value\" vectors), which feeds an associative memory. Prediction in the Memory Mosaic has the associative memories update based on new information from the task, rather than having weights frozen from some previous training, such as in traditional deep feed-forward neural networks. The authors link the Memory Mosaic architecture to the transformer architecture (in one specific example in Section 6, GPT-2) and the attention mechanism. This link is derived from Gaussian kernel smoothing and enforcing key vectors to have equal magnitude. To differentiate this work from other works linking associative memories and attention (e.g. Ramsauer et al. 2020) this paper focuses on the ability of the Memory Mosaic to \"peek\" one step ahead in prediction tasks. The paper asserts that the learning process of Memory Mosaics can be framed as a meta-learning process. The meta-learning process selects for associative memories that are faster at learning new tasks, and hence can produce useful value estimates in fewer time steps. The authors continue to discuss \"predictive disentanglement\"; a framework in which discrete elementary memory units may learn discrete aspects of the prediction task, allowing for interpretable networks with modules of definitive responsibilities (at least, for tasks that can be easily disentangled). The authors demonstrate predictive disentanglement with a toy predictive task based on modelling the orbit of three moons (represented as a time series of positions). In the presented example, the Memory Mosaic is capable of separating the independent orbits of each moon, allowing the network to make more accuracy predictions earlier, i.e. before all moons have finished an orbit. The next example tests the Memory Mosaic in language based tasks. The authors present an altered memory unit that has architectural similarities to the GPT-2 transformer. The authors create a new dataset based on TinyStories (Eldan & Li 2023) titled BabiStories. BabiStories is generated from the Mixtral-8x7B language model and is otherwise similar to the TinyStories dataset, except with an increased diversity of stories and requiring first names and specific opening words for each story. Memory Mosaics demonstrate improved losses compared to GPT-2 for small networks, but has strikingly similar performances for deeper networks. Out-of-distribution language-based experiments were conducted on the Simple English Wikipedia dataset, in which Memory Mosaics appear to perform much better than GPT-2. Further experiments are performed using RegBench, in which Memory Mosaics appear to outperform several state-of-the-art architectures in both accuracy and total variation distance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper presents an interesting, seemingly novel architecture for a prediction model based on associative memories. The architecture seems well justified from first principles and links nicely with existing works on the attention mechanism. This work is of good quality and presents its ideas reasonably well, including rigorous formalizations and numerous figures explaining the proposed architectures. The paper benchmarks the Memory Mosaic against a toy dataset to demonstrate properties of the network, as well as several language-based datasets which allow for comparisons with transform architecture models. These benchmarks add to the quality of the paper, and the newly introduced BabiStories benchmark is original."
            },
            "weaknesses": {
                "value": "To my reading, the architecture is not differentiated strongly enough from existing, similar work on associative memories and the attention mechanism. The key property of the architecture, \"peeking\" in the value predictions, is not explained clearly enough for a reader to understand the significance. The meta-learning interpretation in Section 3 is somewhat confusing in how this relates to the broader field of meta-learning. The toy dataset, while very useful in understanding \"predictive disentanglement\", appears to be crafted specifically to show this property and it is not clarified if such a property is present in other, non-toy datasets. The paper introduces a new language-based dataset, BabiStories, which is similar to the existing TinyStories, but it is not immediately apparent (from the main text or the appendices) why a new dataset is required. Further, without benchmarking of the introduced BabiStories dataset with several models it is not clear if the dataset is useful in assessing the performance of the Memory Mosaic."
            },
            "questions": {
                "value": "Equation 2 offers a nice, specific implementation of a conditional expectation set forth in Equation 1. The discussion here flows very smoothly and is easy to understand. However, the exact form of Equation 2 --- the Gaussian kernel smoothing --- bears striking similarity to the softmax distribution (and in turn the attention mechanism, as the authors discuss later) as well as the Boltzmann distribution. From statistical physics it is well known that the normalizing factor (Z in Equation 2) is nearly always intractable for large enough systems. In the context of Memory Mosaics, it is not immediately apparent what this would correspond to, but it would certainly be of interest to discuss where this implementation would stumble based on this factor. The authors do sidestep the intractability issue in Equation 3 by asserting that the magnitude of the key vectors (rather, the squared norm) are equal, which nicely reduces Equation 2. Again, it is not immediately clear if that is always going to be the case for Memory Mosaics, if it will always be possible to enforce this constraint in applications, or if this only occurs in the datasets presented in the paper. In any case, a brief discussion on the possibility of non-uniform squared norm key vectors may be of interest to readers and help ground the Memory Mosaic in the landscape of prediction models. The discussion on why this paper limits itself to only Gaussian kernel smoothing derived associative memories is excellent; clear and concise. Emulating this level of discussion for these other points may prove helpful. In short: is the calculation of the normalizing factor Z as intractable as it appears? What factor of the Memory Mosaic / dataset makes the normalization factor impossible to calculate? Does the uniform-squared-norm trick in Equation 3 work for all datasets, or is it a rare occurrence that makes the network otherwise untenable?\n\nThe authors discuss \"peeking\" in the Memory Mosaic, which allows the memory units to look one time step ahead in the \"key\" series to predict the next item in the \"value\" series. After several read-throughs, it is still difficult to understand what this means for the Memory Mosaic. In a more traditional prediction model looking a timestep ahead would be considered equivalent to a bug. In the Memory Mosaic this is seemingly not the case, but the reasoning is still hazy to me. Am I correct in my understanding that \"peeking\" is allowable in this model, and if so, why is this distinct from other associative memories? Is it possible to clear up the discussion on this point, particularly the subsection in Section 2, to explain the nuances of \"peeking\" more? Considering the importance of the \"peeking\" mechanism in this work, it is paramount it is easy to digest. \n\nThe Memory Mosaic architecture has striking similarities to associative memories linked to attention mechanisms. In particular, Ramsauer et al.'s influential \"Hopfield is All You Need\" has almost the same derivations found in Section 2 of this paper. The authors discuss the Ramsauer paper at the end of Section 2, but the differentiation of the works does not appear substantial. In particular, the \"peeking\" of the Memory Mosaic is not well explained, which makes the reliance on it for the basis of novelty compared to Ramsauer et al. troublesome. With some further discussion of the \"peeking\" mechanism (as noted above), and potentially some more clarity on the section discussing Ramsauer et al., this paper would provide a much stronger foundation for Memory Mosaics in the context of existing literature.\n\nThe meta-learning process at the start of Section 3 is somewhat confusing when primed to think about both meta-learning as a field and traditional training of neural networks. If my understanding of the paper is correct, training a neural network (e.g. writing a training loop, feeding in training data, performing backpropagation, updating weights, and so on) is presented here as meta-learning, with the \"regular\" learning being the updates to the associative memories performed when prediction of a time series occurs. This is a fairly radical change in perspective, although an interesting interpretation, but again it seems that the paper could benefit from having this stated very directly as to guide the reader to this way of thinking about meta-learning in the Memory Mosaic. Is this interpretation of meta-learning in the Memory Mosaic correct? If so, the steam-roller metaphor (Figure 2) gives rise to another interesting question that is seemingly not addressed in the main body of the paper; does the Memory Mosaic always benefit from fast-learning memory units? Certainly I can see the benefit of having a predictor model that requires a smaller context window to start spitting out predictions, but is it possible that slow-learning memory units result in more accurate predictions (for any number of reasons). If that is the case, it would raise concerns about the gradient / meta-learning presented in the paper, and if that is not the case I have not been convinced by the text. Is it correct to say that faster-learning memory units in the context of Memory Mosaics are always better? \n\nThe toy example presented --- the three moons problem --- is excellent at showing the application of \"predicative disentanglement\". However, it is less clear after reading through the main text and the appendices if this specific kind of \"easy\" disentanglement is present in other, non-toy datasets. Mining this sort of information from, say, a language-based dataset would be nigh impossible, but some discussion about whether the property is present in other datasets (or even if this is likely to occur) would make for a nice conclusion to the Section. Moreover, the authors mention several times that the weight matrices in the three moons problem could be replaced with the identity to improve results. In Appendix A these matrices are seemingly the anti-diagonal identity. Is the correct interpretation here that the rows of these matrices could be permuted freely, and hence are equivalent to the identity? Otherwise, these do not seem to be identity matrices, which makes the previous discussion odd!\n\nThe main language-based task presented in this paper is based on BabiStories. The new dataset is seemingly introduced without explanation as to why it is needed. Due to the similarities to TinyStories, it seems that BabiStories must include some additional structure that makes it useful when working with Memory Mosaics in particular, but this is not stated clearly in the main text. Appendix B may mention this briefly, but it is not clear if the dataset is created for this reason or not. Is it possible to use TinyStories to train Memory Mosaics and use these results in place of BabiStories? If not, perhaps a description of why not could be included in the main text to explain to the reader why a new dataset is required.\n\nFigure 7 shows the performance gap between the Memory Mosaic and GPT-2 models when trained on the BabiStories dataset. These are promising curves and show excellent results. However, this performance gap is only substantial for a model depth of 1. Even at a model depth of 8, the second sub-figure, the gap between GPT-2 and Memory Mosaics has seemingly disappeared. It is still interesting that GPT-2 and Memory Mosaics perform equally well for larger depths, does this hint that perhaps the equivalent performance is due to the dataset rather than the architecture i.e. both architectures can saturate to the best possible accuracy on BabiStories? Also, is it possible to focus on specifically the region where there is a difference in performances? The jump from a depth of 1 to a depth of 8 in this Figure seems to leave out the most interesting region in the context of the paper. Does the Memory Mosaic continue to outperform GPT-2 at a depth of 2 or 3, or do the two architectures converge immediately after a depth of 1? The hyperparameter transfer from GPT-2 to Memory Mosaics is also terrifically interesting --- considering how fickle these networks can be, and the equal performance show in Figure 7, does this hint at some deeper connection between Memory Mosaics and the GPT-2 architecture, perhaps even a full equivalence of the models? This last question is less a review of the current manuscript and more of a potential lead into future research, so no need to address this point in your comments!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Memory Mosaics, a neural network architecture composed of associative memory units for language modeling tasks. The authors present the concept of \"predictive disentanglement\" to explain how the model decomposes prediction tasks. Experiments show comparable or better performance to transformers on language modeling benchmarks, with claimed advantages in interpretability and in-context learning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "-  The paper addresses an interesting problem in developing more interpretable language models.\n- The work is well-organized and clearly presented."
            },
            "weaknesses": {
                "value": "- The core concept presented in Figure 3 bears similarities to Linear Transformers [1], particularly RetNet [2] (see RetNet Eq. 6). However, the paper fails to adequately discuss or cite these highly relevant works. Linear Transformers are only briefly mentioned in Figure 9, without proper citation or in-depth comparison. This oversight is particularly concerning as it appears the authors are aware of the similarities, given the inclusion of Linear Transformer, RetNet, and GLA [3] in Table 3 and Figure 9. The lack of a detailed explanation comparing their approach to the evolution of Linear Transformer methods (such as GLA/RetNet) raises questions about the paper's originality and positioning within the field.\n\n- The authors introduce persistent memory units (P-mem) but do not clearly differentiate them from standard MLP layers. They state that P-mem can be seen as MLP layers (lines 328-330), but fail to elucidate the advantages or unique properties of P-mem that justify its introduction as a new concept. This raises concerns about the necessity and value of creating this new terminology.\n\n- While the paper uses RegBench to assess in-context learning, this benchmark alone may not provide a comprehensive evaluation of the model's memory capabilities. The performance of state-space models like Mamba can be significantly impacted by the inclusion of short convolutions, as noted in DeltaNet [4] (See Figure 2). A more thorough evaluation, possibly including benchmarks like MQAR [5], would provide a more robust assessment of the model's capabilities relative to existing approaches. I understand that there may be not enough time for running such experiments in the rebuttal stage, so the author can just try to rerun run the experiment with/without the short convolution in RegBench.\n\n- The paper focuses primarily on language modeling tasks but lacks evaluation on standard common reasoning benchmarks that are crucial for assessing language models' capabilities. Tasks such as ARC Challenge, ARC Easy, PIQA, and Hellaswag are widely used to evaluate smaller-scale language models and provide valuable insights into a model's reasoning abilities. The absence of these evaluations limits our understanding of the model's true capabilities and makes it difficult to compare with other approaches in a comprehensive manner. Including these benchmarks would provide a more holistic view of the model's performance across various aspects of language understanding and reasoning.\n\n- The paper appears to present an incremental advance on linear transformers, repackaged with new terminology and concepts. While this approach creates an impression of novelty, its actual contribution to advancing the field seems limited. The work seems to complicate a relatively straightforward evolution of linear transformer models with complex conceptual packaging, which may hinder rather than help the development of the research community and the broader field.\n\nIf the authors can appropriately address my concerns or correct me if I am wrong, I would be willing to increase my score.\n\n[1] Katharopoulos, Angelos, et al. \"Transformers are rnns: Fast autoregressive transformers with linear attention.\" International conference on machine learning. ICML, 2020.\n\n[2]Sun, Yutao, et al. \"Retentive network: A successor to transformer for large language models.\" arXiv preprint arXiv:2307.08621 (2023).\n\n[3]Yang, Songlin, et al. \"Gated Linear Attention Transformers with Hardware-Efficient Training.\" ICML, 2024.\n\n[4]Yang, Songlin, et al. \"Parallelizing Linear Transformers with the Delta Rule over Sequence Length.\" NeurIPS, 2024.\n\n[5]Arora, Simran, et al. \"Simple linear attention language models balance the recall-throughput tradeoff.\" ICML, 2024."
            },
            "questions": {
                "value": "See my weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}