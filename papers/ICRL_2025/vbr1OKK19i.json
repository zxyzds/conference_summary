{
    "id": "vbr1OKK19i",
    "title": "Why context matters in VQA & Reasoning: Semantic interventions for VLM input modalities",
    "abstract": "The various limitations of Generative AI, such as hallucinations and model failures, have made it crucial to understand the role of different modalities in Visual Language Model (VLM) predictions. Our work investigates how the integration of information from image and text modalities influences the performance and behavior of VLMs in visual question answering (VQA) and reasoning tasks. We measure this effect through answer accuracy, reasoning quality, model uncertainty, and modality relevance. We study the interplay between text and image modalities in different configurations where visual content is essential for solving the VQA task. Our contributions include (1) the Semantic Interventions (SI)-VQA dataset, (2) a benchmark study of various VLM architectures under different modality configurations, and (3) the Interactive Semantic Interventions (ISI) tool. The SI-VQA dataset serves as the foundation for the benchmark, while the ISI tool provides an interface to test and apply semantic interventions in image and text inputs, enabling more fine-grained analysis. Our results show that complementary information between modalities improves answer and reasoning quality, while contradictory information harms model performance and confidence. Image text annotations have minimal impact on accuracy and uncertainty, slightly increasing image relevance. Attention analysis confirms the dominant role of image inputs over text in VQA tasks. In this study, we evaluate state-of-the-art VLMs that allow us to extract attention coefficients for each modality. A key finding is PaliGemma's harmful overconfidence, which poses a higher risk of silent failures compared to the LLaVA models. This work sets the foundation for rigorous analysis of modality integration, supported by datasets specifically designed for this purpose. The code is available at https://gitlab.com/dekfsx1/si-vlm-benchmark and the tool and dataset are hosted at https://gitlab.com/dekfsx1/isi-vlm.",
    "keywords": [
        "Vision Language Model",
        "Vision Question Answering",
        "model failure",
        "multimodality",
        "interpretability",
        "semantic intervention"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "An analysis of the impact of modalities in a multimodal setting in VQA and reasoning tasks, supported by a well-curated novel dataset and an interactive tool.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vbr1OKK19i",
    "pdf_link": "https://openreview.net/pdf?id=vbr1OKK19i",
    "comments": [
        {
            "summary": {
                "value": "This work investigates the impact of different modalities \u2013 image and text \u2013 on the performance of Visual Language Models in Visual Question Answering (VQA) tasks. \nThe authors examine how the combination and interplay of these modalities affect accuracy, reasoning quality, model uncertainty, and attention attribution. \nThey collect a novel dataset (SI-VQA) with controlled interventions and an interactive tool (ISI) for manipulating image and text inputs to study VLM behavior. \nThis work sets the foundation for further analysis of modality integration in VQA, hightlighting the crucial role of context in guiding future model developments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This work introduces the SI-VQA dataset, which is designed to require image-based answers, ensuring that visual content is essential for solving VQA tasks. This setup allows researchers to analyze how different modalities (image, text, context) influence the model\u2019s accuracy, reasoning, and uncertainty\u200b.\n\n2. Comprehensive Benchmarking of VLMs: This work establishes a robust benchmark by evaluating various state-of-the-art VLMs under diverse modality configurations. This benchmarking approach highlights the contributions and limitations of each modality, as well as the strengths and weaknesses of different VLM architectures.\n\n3. This work introduces the ISI Tool, enabling researchers to perform semantic interventions on VLM inputs, which supports fine-grained analysis of VLM behavior."
            },
            "weaknesses": {
                "value": "Please also refer to the questions section.\n\n1. There are some concerns about the dataset.\n2. The work lacks comparisons with other current datasets.\n3. The work lacks supporting evidence for its claims.\n4. The work lacks formal definitions of certain terms."
            },
            "questions": {
                "value": "Here are the corrected versions of the reviews:\n\n1. The proposed dataset contains only 100 samples, which is quite limited in this domain.\n\n2. The answers are limited to \"Yes\" or \"No.\" Moreover, the paper does not specify the distribution of \"Yes\" versus \"No\" answers in the dataset. This leads to the following two concerns:\n\n   - **Model Bias**: If the dataset is heavily skewed toward one answer (e.g., mostly \"Yes\" answers), it could introduce bias in the models, potentially leading them to favor that answer even when the visual information suggests otherwise.\n\n   - **Impact of Interventions**: Without knowing the baseline distribution of answers, it is challenging to isolate the true effect of the semantic interventions (complementary context, contradictory context, image annotations) on the models' performance. For example, if the dataset already has a majority of \"Yes\" answers, an intervention that improves performance on \"Yes\" questions might not necessarily reflect a genuine improvement in the model's ability to understand the visual information.\n\n3. Even though each sample is well-annotated (i.e., an image, a corresponding question with a ground truth Yes/No answer, a text-annotated version of the image, a contradictory context, and a complementary context), there are no comparisons between the proposed dataset and state-of-the-art (SOA) datasets regarding its advantages in Image-dependent Answers and Content Domain Diversity.\n\n4. Regarding the claim that image text annotations have minimal impact on accuracy, or even decrease accuracy, the authors list some potential reasons for this, e.g., VLMs may already extract relevant information from images. It would be helpful to provide some qualitative or quantitative results to further support these explanations.\n\n5. The term \"modality relevance\" is first mentioned in the abstract. However, there is no formal definition provided for it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the impact of contextual information on Visual Question Answering (VQA) and reasoning within Vision-Language Models (VLMs). The study introduces the Semantic Interventions (SI)-VQA dataset and the Interactive Semantic Interventions (ISI) tool to evaluate how image and text modalities interact to affect model performance, accuracy, and uncertainty. The methodology involves benchmarking multiple VLM architectures under different configurations, integrating complementary or contradictory text with images. Experimental results indicate that integrating complementary information enhances model accuracy and reasoning quality, whereas contradictory information significantly degrades performance. Moreover, VLMs show a bias toward image inputs over textual context, with PaliGemma exhibiting notable overconfidence, leading to increased silent failures compared to LLaVA models. The study emphasizes the crucial role of modality integration and provides tools for better understanding VLM behavior in multimodal tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1)\tThe impact of context and different modalities on VQA has always been a noteworthy topic. This paper's discussion, incorporating VLM, is insightful for future researchers.\n2)\tThe experimental design in this paper is very thorough, with detailed consideration given to seven different input configurations.\n3)\tSome of the findings in the experimental results of this paper are very interesting and offer valuable insights for the design and application of future VLMs.\n4)\tThis paper has released the dataset and the ISI tool."
            },
            "weaknesses": {
                "value": "1)\tMy primary concern is that the paper mainly describes the observed phenomena in the experimental results without providing sufficient analysis of why these results occur (though there is some experimental analysis). In particular, the paper does not explain how these results could be useful for advancing future VQA work or analyze what could be done to address some of the issues identified in the results. Additionally, some of the findings are not particularly novel, making the paper seem more like an experimental report.\n2)\tAs the authors pointed out in the paper, the SI-VQA dataset has too few samples, with only one hundred entries. Although the authors believe these data are representative, they should at least analyze why the results from these one hundred samples are convincing. Is it because these one hundred samples are of high quality and diversity?"
            },
            "questions": {
                "value": "1)\tIt appears that image text annotations have little effect on some of the model's metrics; for example, the results of Q+I_A+C_+ in Figure 2a are not optimal. Could the authors analyze the reason for this phenomenon?\n2)\tI don't quite understand why the initial hypothesis is introduced in Section 5.1, as it doesn't seem to be strongly related to the main part of the experiments.\n3)\tCould the authors explain specifically how GPT-4o is used as an evaluator of reasoning ability? Since the SI-VQA dataset has only 100 samples, why didn\u2019t the authors consider using human evaluation instead? Would that provide more accurate results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates the limitations of Generative AI, particularly in Visual Language Models (VLMs), focusing on how the integration of image and text modalities affects performance in visual question answering (VQA) and reasoning tasks. They use only 100 samples to gain some conclusions in the paper, such as \"complementary information between modalities improves answer and reasoning quality\"."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to understand.\n2. The experimental analysis is comprehensive.\n3. The conclusions drawn are intuitively credible."
            },
            "weaknesses": {
                "value": "1. The dataset contains only 100 samples, and the conclusions drawn lack novelty; they are basic findings that have been established in previous multimodal research. The importance of multimodal complementarity is widely recognized in the field, so the conclusions of this article lack originality.\n\n2. Overall, this article is a fairly good technical report that provides a comprehensive experimental analysis."
            },
            "questions": {
                "value": "Are there any other interesting conclusions or exploratory directions for uncovering the importance of multimodal complementarity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents to evaluate the robustness of VLLMs.\nIn particular, there are two dimensions that the advocated evaluation protocol considers: \n1) modality bias - whether VLLMs make predictions based on the linguistic relations;\n2) context - whether the context helps in reasoning. \n\nBased on this idea, this paper collects a new dataset and then evaluates various VLLMs, including LLaVA, and Pali-Gemma.\nBesides, the authors also provide some analysis from the dimension of semantic entropy and attention distribution."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The studied problem - the robustness of VLLMs, is practical and interesting for the research community.\n- The authors adopt two different families of models for evaluation, including both LLaVA and Pali-Gemma.\n- There are some more dimensions that are considered by this paper, like semantic entropy and attention distribution."
            },
            "weaknesses": {
                "value": "- The biggest limitation of this paper lies in its limited dataset size. \nSpecifically, there are only 100 instances of the collected dataset.\nFrom this point of view, most of the conclusions from this work might be plausible and not stand.\nAdditionally, we cannot name a scale of such a dataset as ``comprehensive``.\n- The authors are suggested to test larger model sizes, such as 13B models - LLaVA-1.5-vicuna-13B.\n- It seems like there is a strong connection between this work and several well-studied problems such as modality bias (language prior) in VQA [1][2], and visual commonsense reasoning (VCR) [3].\n\n[1] On Modality Bias Recognition and Reduction. \n\n[2] Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning.\n\n[3] From Recognition to Cognition: Visual Commonsense Reasoning."
            },
            "questions": {
                "value": "See the weakness part for a detailed explanation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}