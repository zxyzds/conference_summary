{
    "id": "qqZijHRcA5",
    "title": "Impact of Dataset Properties on Membership Inference Vulnerability of Deep Transfer Learning",
    "abstract": "We analyse the relationship between privacy vulnerability and dataset properties, such as examples per class and number of classes, when applying two state-of-the-art membership inference attacks (MIAs) to fine-tuned neural networks. We derive per-example MIA vulnerability in terms of score distributions and statistics computed from shadow models. We introduce a simplified model of membership inference and prove that in this model, the logarithm of the difference of true and false positive rates depends linearly on the logarithm of the number of examples per class. We complement the theoretical analysis with empirical analysis by systematically testing the practical privacy vulnerability of fine-tuning large image classification models and obtain the previously derived power law dependence between the number of examples per class in the data and the MIA vulnerability, as measured by true positive rate of the attack at a low false positive rate. Finally, we fit a parametric model of the previously derived form to predict true positive rate based on dataset properties and observe good fit for MIA vulnerability on unseen fine-tuning scenarios.",
    "keywords": [
        "Membership Inference Attack",
        "Transfer Learning",
        "Few-shot Learning",
        "Image Classification"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We derive a power-law relationship between data set properties and vulnerability to membership inference and support it with extensive experiments.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=qqZijHRcA5",
    "pdf_link": "https://openreview.net/pdf?id=qqZijHRcA5",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates the impact of dataset properties on membership inference attacks (MIAs). The authors analyze two state-of-the-art MIA methods, LiRA and RMIA, by deriving a relationship between dataset properties (e.g., number of examples per class) and MIA vulnerability. They introduce a simplified model for membership inference and demonstrate that a power-law relationship exists between per-example MIA vulnerability and the dataset's properties. The authors also fit a regression model to predict MIA vulnerability, providing insights into privacy risks for fine-tuned classifiers."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe paper formalizes the effect of dataset properties on membership inference vulnerability for two advanced MIA methods (LiRA and RMIA), enriching the analytical framework for MIA.\n2.\tThe study includes a comprehensive and well-designed set of experiments across multiple datasets."
            },
            "weaknesses": {
                "value": "1.\tIn the abstract and introduction, the authors emphasize that their focus is on the privacy analysis of fine-tuned image classification models. However, sections 2 and 3 lack specific analyses that are tailored to fine-tuned neural networks, relying instead on a general theoretical model.\n2.\tThe authors need to clarify the differences in membership inference vulnerability between standard image classification models and fine-tuned models. This would strengthen the argument for fine-tuning as a key focus in their work.\n3.\tAs the regression model is presented as one of the main contributions, the methods section would benefit from a more detailed description of this model, its design, and its implementation process.\n4.\tAlthough LiRA and RMIA are currently the best methods, it is also necessary to analyze other MIA methods to validate the generalizability of the proposed arguments."
            },
            "questions": {
                "value": "1.\tAs the authors mention in the introduction, prior work has already explored the impact of the number of examples per class (shots) on MIA vulnerability. How does this study differ from previous work, and what new insights does it provide beyond existing findings?\n2.\tWhile LiRA and RMIA represent state-of-the-art approaches in MIA, it is also essential to assess the generalizability of the presented results to other MIA techniques. How might other methods differ in vulnerability patterns under the same dataset conditions?\n3.\tAs a widely used method for privacy protection, it is necessary to discuss the impact of differential privacy on the arguments presented in this paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a theoretical result that links the success of membership inference attacks (MIAs) to the number of training data points per class. This theoretical link is shown for the RMIA and LiRA attack, and empirical evaluation is performed through fine-tuning vision encoders on multiple downstream datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "First, I'd like to note that I did enjoy reading the paper: it is clearly written, nicely presented, and well executed. Particularly the plots are extremely visually appealing.\n\nOverall, the attempts to provide some interesting theoretical underpinning for a long known and well studied problem."
            },
            "weaknesses": {
                "value": "The core weaknesses of the paper can be grouped as follows:\n\n**Studying a well established problem, while not taking prior work into account**\n\nThe underlying causes of MIA risks have been studied in a long line of work since 2017. The findings made in the paper, showing that vulnerability decreases as you have more samples per class, and with fewer classes have been made already. Hence, the expected impact and relevance of the presented findings is negligible. Given the enormous amounts of prior work, it is particularly surprising that the paper does not even have a related work section.\n\nThis section could start at early work. E.g. [1] (2021) has an extremely related approach where they assign privacy risk scores that implicitly expresses a similar phenomenon: with the probability of a sample being part of the training set yielding a per point privacy risk: here, it is over classes. Additionally, they add a very valuable angle (not considered in this present work here excessively): Classes with less members have on average lower probability: \"the class labels with high generalization errors tend to have higher privacy risk scores\".\nGiven the results by [2], this seems extremely crucial to analyze. For the given work, I am wondering how much this power law connection really holds when data is not well curated. We can imagine a really \"malformed\" class that consists of extremely rare and outlier data points with high variance. How much will having more of those really help reduce membership risk?\nStarting from these works, a related work section could help put the given work into context.\n\nAdditionally, another line of work that needs to be taken into account is the one around [3,4,5] where it has been shown that with less data points per class, the class is more vulnerable and that privacy attacks have disparate success.\n\n**Generality of Results**\n\nWhile LiRA and RMIA are *current* state-of-the-art, it is, undoubtably reasonable to derive the bounds for those concrete attacks. However, the results are presented \"as is\", i.e., without embedding them into any broader context and without generalization. This makes it questionable, how well the results will age, i.e., how much can we benefit from them in 2-3 years, when LiRA and RMIA are obsolete and have been replaced by new and more powerful attacks.\n\nIn a similar vein, a suggestion would be to improve the writing in Section 3.4. The results are interesting, but it is not discussed what they mean: \"Hence we can have the LiRA vulnerability arbitrarily close to zero in this non-DP setting by simply increasing S\" -> this seems to mean that if every class has more and more train data, the membership risk decreases. Formulating such intermediate \"checkpoints\" in plain English would be genuinely helpful to position the results.\n\n**Evaluation done for fine-tuning might skew the results**\n\nOverall, I am not convinced that the chosen evaluation setup is sensible for the presented results. The paper considers transfer learning where the heavy lifting is done by a pre-trained vision encoder. However, the pre-training data itself has a distribution that will play a role for the vulnerability of the fine-tuning data. Data points that have high overlap with the classes / concepts present in the pre-training will have a significantly different vulnerability than data from an entirely different distribution. Hence, to underline the theoretical results, the correct approach, would have been training from scratch.\n\nWhile the paper tries to argue that the pre-training encoder has no influence (\"Figure 4a shows that the regression model is robust to a change of the feature extractor\"), this statement is too broad and not backed up by results: The experiments were conducted on only two encoders, both pre-trained on ImageNet. These encoders will have the same biases, and will expose the fine-tuning data equally. If any, to make such a statement, it would be required to assess more encoders, especially ones trained on entirely different distributions, including non-object centric dataset.\n\nThe fact, that the expectations do not match the from-scratch training (\"underestimates the vulnerability of the from-scratch trained target models\") is another indicator that comparing with fine-tuning is not the right approach. \n\n\n**Minor Comments**\n\n- In the intro, the paper says: \"and apply two state-of-the-art MIAs\", it would be good to cite them there already for the readers to understand what to expect\n- Would it make sense to add the values from the theoretical model in Figure 3, such that one can compare how much the empirical one deviates?\n\n**References**\n\n[1] Song, L., & Mittal, P. (2021). Systematic evaluation of privacy risks of machine learning models. In 30th USENIX Security Symposium (USENIX Security 21) (pp. 2615-2632).\n\n[2] Yeom, Samuel, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. \"Privacy risk in machine learning: Analyzing the connection to overfitting.\" In 2018 IEEE 31st computer security foundations symposium (CSF), pp. 268-282. IEEE, 2018.\n\n[3] Suriyakumar, Vinith M., Nicolas Papernot, Anna Goldenberg, and Marzyeh Ghassemi. \"Chasing your long tails: Differentially private prediction in health care settings.\" In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 723-734. 2021.\n\n[4] Bagdasaryan, Eugene, Omid Poursaeed, and Vitaly Shmatikov. \"Differential privacy has disparate impact on model accuracy.\" Advances in neural information processing systems 32 (2019).\n\n[5] Kulynych, Bogdan, Mohammad Yaghini, M. Cherubin, G. Veale, and C. Troncoso. \"Disparate vulnerability: On the unfairness of privacy attacks against machine learning.\" In 22st Privacy Enhancing Technologies Symposium. 2022."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the relationship between inherent properties of a dataset, such as the number of classes and the number of samples belonging to each class, and its impact on privacy. Privacy is examined through the perspective of membership inference attacks (MIAs), which seek to determine if a sample has been used to train a model of interest. The authors present theoretical results that establish a power-law relation between dataset properties and vulnerability to MIAs, and support their claims with experimental evaluations."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Membership inference is an important attribute that can explain vulnerability of machine learning models to privacy attacks. The extensive theoretical results help provide generalizable and explainable interpretations of impacts of multiple characteristics of datasets on model vulnerability. \n\n2. Theoretical results are presented in a manner that is easy to follow for an informed reader, and supported by detailed proofs in the Appendix. \n\n3. Extensive experiments on multiple datasets validate theoretical claims. The datasets have been carefully chosen to encompass different numbers of classes and image domains. \n\n4. The work presented has the potential for high impact beyond traditional computer science, for example, balancing the requirements of various regulatory regimes such as GDPR with a need to obtain `useful' information from possibly sensitive data."
            },
            "weaknesses": {
                "value": "1. The authors indicate that the paper considers two types of MIAs. There have been a wide variety of MIAs examined in the literature, including some which may not rely on confidence scores (e.g., label-based MIAs). Is there a claim (perhaps speculative) that the authors can make about similar relationships for other classes of MIAs? \n\n2. The experiments focus on datasets and models related to image classification. Can the results be extended to other domains, such as audio or video? Multi-modality can strengthen the scope of the proposed contribution. \n\n3. The authors point out that analyses and experimental results rely on an assumption that the datasets are balanced. Some perspective on what might happen if this is not the case can shed more light on real-world applicability of the work. \n\n4. A more detailed discussion or comparison of the effect of dataset characteristics on other aspects of privacy- e.g., differential privacy, which has an extensive literature and supported by theoretical guarantees- will strengthen the submission. Could the authors comment on this?"
            },
            "questions": {
                "value": "Please see Weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The study investigates privacy vulnerabilities through Membership Inference Attacks (MIA), with a particular focus on how dataset size affects model privacy. Through detailed per-sample evaluations, the researchers discovered that models become more resistant to privacy attacks as the number of examples per class increases, following a power law distribution. To further examine this phenomenon, the researchers evaluated the findings using few-shot learning and developed a regression model to predict a model's vulnerability to MIA based on dataset properties, such as the number of classes and examples per class."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The work presents comprehensive experiments on privacy vulnerabilities using two SOTA MIA methods. It conducts a fine-grained analysis of vulnerabilities at the individual sample level. \n- The findings are robustly validated across diverse architectures and datasets.\n- The study provides theoretical analyses of a power-law relationship between dataset properties (e.g., shots per class) and MIA vulnerability."
            },
            "weaknesses": {
                "value": "1. The paper needs to clarify its motivation. While it discusses transfer learning and few-shot learning, it's unclear whether these techniques are studied as potential privacy protection methods or whether the authors simply examine their privacy risks. The introduction introduces these learning approaches but doesn't explicitly position them against existing differential privacy (DP) defenses. This makes it difficult to understand whether the main goal is to propose new privacy protection strategies or to analyze privacy vulnerabilities in these methods.\n\n2. The paper's organization should be reversed. It would be clearer to show the experimental results first and then explain the theory behind them rather than the other way around. \n\n3. The authors need to revise their claimed contributions. For example, their first contribution about analyzing privacy attacks at the individual sample level isn't new - other researchers have already done similar work (such as [1], [2]).\n\n4. It is unsurprising that larger datasets exhibit lower overall vulnerability to MIA, as models trained on larger datasets tend to rely less on individual samples, reducing the risk of memorization. \n\n5. The authors predicted the overall dataset vulnerability using a regression model with properties such as the number of classes and examples per class. While it offers useful insights,  individual-level vulnerabilities are often more actionable for user privacy protections. It's much more meaningful to investigate the per-sample vulnerabilities. However, considering varying dataset sizes, it may be challenging to derive consistent per-sample results, as the same sample could behave as an outlier in some datasets but not in others.\n\n[1] Carlini, Nicholas, et al. \"The privacy onion effect: Memorization is relative.\" Advances in Neural Information Processing Systems 35 (2022): 13263-13276.\n\n[2] Wen, Rui, Michael Backes, and Yang Zhang. \"Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?.\" arXiv preprint arXiv:2409.03741 (2024)."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}