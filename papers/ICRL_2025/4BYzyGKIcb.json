{
    "id": "4BYzyGKIcb",
    "title": "Sharpness-Aware Geometric Defense for Robust Out-Of-Distribution Detection",
    "abstract": "Out-of-distribution (OOD) detection ensures safe and reliable model deployment. Contemporary OOD algorithms using geometry projection can detect OOD or adversarial samples from clean in-distribution (ID) samples. However, this setting regards adversarial ID samples as OOD, leading to incorrect OOD predictions. Existing efforts on OOD detection with ID and OOD data under attacks are minimal. In this paper, we develop a robust OOD detection method that distinguishes adversarial ID samples from OOD ones. The sharp loss landscape created by adversarial training hinders model convergence, impacting the latent embedding quality for OOD score calculation. Therefore, we introduce a **Sharpness-aware Geometric Defense (SaGD)** framework to smooth out the rugged adversarial loss landscape in the projected latent geometry. Enhanced geometric embedding convergence enables accurate ID data characterization, benefiting OOD detection against adversarial attacks. We use Jitter-based perturbation in adversarial training to extend the defense ability against unseen attacks. Our SaGD framework significantly improves FPR and AUC over the state-of-the-art defense approaches in differentiating CIFAR-100 from six other OOD datasets under various attacks. We further examine the effects of perturbations at various adversarial training levels, revealing the relationship between the sharp loss landscape and adversarial OOD detection. The implementation code will be released upon paper acceptance.",
    "keywords": [
        "Robust out-of-distribution detection",
        "Adversarial training",
        "Sharpness-aware minimization"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4BYzyGKIcb",
    "pdf_link": "https://openreview.net/pdf?id=4BYzyGKIcb",
    "comments": [
        {
            "summary": {
                "value": "This paper argues that adversarial examples should be classified as in-distribution samples rather than outliers. The authors imagine out-of-distribution (OOD) detection scenarios where input data may be subject to adversarial attacks. They demonstrate that their proposed method maintains robust OOD detection performance even when the data contains adversarial perturbations. The authors achieve strong experimental results by incorporating several established techniques in their approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The paper presents a novel angle by examining OOD detection under potential adversarial attacks - a scenario that has received limited attention.\n- The experimental evaluation is comprehensive and thorough."
            },
            "weaknesses": {
                "value": "## Should adversarial examples be classified as in-distribution samples rather than outliers?\nIt is clear that by adding adversarial perturbation, the distribution shifted, why it still should be in-distribution?\n\n## About the scenario\nWhat are some real-world applications where out-of-distribution detection must handle potentially adversarially attacked images? \n\n## About the Contribution\nThe contribution of this work should be carefully justified. Most of the subsection in section 3 are existing methods. In the introduction section, the authors claim that the smoother regularizer introduced in 3.3 is the key contribution, but I do not agree that this intuition based smoothing regularizer is enough to let this paper be accept.\n\nGiven the limited practical relevance of the scenario and modest contributions, I recommend rejection."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N / A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a sharpness-aware method for improving OOD detection in adversarial training. Specifically, a multi-geometry projection network is trained to extract the hypersphere and hyperbolic features using jitter-based adversarial samples. Moreover, the network is optimized by sharpness-aware loss minimization using RSAM. Extensive experiments demonstrate the effectiveness of the proposed method. However, I have some concerns about this paper. My detailed comments are as follows."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe authors investigate various adversarial attacks on different OOD detection approaches. Extensive experiments demonstrate the effectiveness of the proposed method.\n2.\tThey introduce Jitter-based perturbation in adversarial training to extend the defense ability against unseen attacks.\n3.\tThey employ Multi-Geometry Projection (MGP) and Riemannian Sharpness-aware Minimization (RSAM) for the OOD detection."
            },
            "weaknesses": {
                "value": "1.\tMy first concern is the reasonability of the research setting. The paper presents a method to classify adversarial examples as in-distribution (ID) samples in the context of out-of-distribution (OOD) detection. However, I find the rationale for this setting questionable for two main reasons:\n* Adversarial examples, by design, deviate significantly from the natural data distribution, even if they remain close in image space. Treating them as OOD samples aligns with standard OOD detection objectives, as these samples no longer represent the semantic consistency of ID data.\n* Detecting adversarial examples as OOD is practically advantageous, as it helps prevent their influence on model predictions. For most applications, identifying adversarial samples as OOD is a more effective way to mitigate potential risks, while treating them as ID can increase vulnerability to attacks.\n\n2.\tThe novelty of the methodology is limited. The proposed method appears to be a fusion of the MMEL approach [1] and the RSAM technique [4], denoted as MPG and RSAM, respectively, within the present paper.\n3.\tThe motivations of the introduction for the three components in the approach are not clear. Why do you use MGP, RSAM and Jitter-based perturbation?\n4.\tThe content of Figure 2 appears to have been adapted from Figure 1 in the referenced paper [1].\n5.\tThe significance of the sharp loss landscape seems to be self-evident, as it has been extensively explored in the existing literature [2, 3]. Regrettably, I fail to discern any novel contribution from the current paper in this regard.\n\n[1] Learning Multi-Manifold Embedding for Out-Of-Distribution Detection\n\n[2] Sharpness-Aware Minimization for Efficiently Improving Generalization\n\n[3] Detecting Adversarial Samples through Sharpness of Loss Landscape\n\n[4] Riemannian SAM: Sharpness-Aware Minimization on Riemannian Manifolds"
            },
            "questions": {
                "value": "1.\tIs the hyperspherical geometry learning a type of adverbial defense method? I did not see the related description in the section \u201cADVERSARIAL DEFENSES\u201d.\n2.\tWhy not use other attack methods to perform adversarial training?\n3.\tHow to obtain the class prototype $\\mu_k$ ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a robust method for out-of-distribution (OOD) detection that effectively separates adversarial in-distribution (ID) samples from OOD ones. It introduces the Sharpness-aware Geometric Defense (SaGD) framework, which smooths the irregular adversarial loss landscape within the projected latent space. By improving the convergence of geometric embeddings, the framework enhances the characterization of ID data, strengthening OOD detection in the presence of adversarial attacks. Additionally, the use of jitter-based perturbations in adversarial training expands the defense against unseen threats. Experimental results demonstrate that the SaGD framework achieves significant improvements in false positive rate (FPR) and area under the curve (AUC) compared to state-of-the-art methods, particularly in distinguishing CIFAR-100 from six other OOD datasets under various attack scenarios."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. It introduces a novel sharpness-aware method for improving OOD detection in adversarial training. The proposed method investigates the combination of Riemannian geometries under adversarial conditions. This expansion of geometry space sharpens the proposed defense against adversarial attacks and avoids reliance on large OOD datasets for auxiliary training.\n\n2. The proposed SaGD sets a new SoTA for OOD detection, excelling in $FPR_{95}$ and AUC metrics, both with or without attacks.\n\n3. It performs ablation experiments to analyze the relations between the minimization of a sharp loss landscape and OOD detection performance under various adversarial conditions."
            },
            "weaknesses": {
                "value": "1. It should provide a detailed analysis of the computational complexity involved in computing the OOD score. Additionally, it is important to examine how the number of in-distribution (ID) training samples affects the performance of the OOD score, as this can influence the scalability and generalizability of the approach.\n\n2. Choosing an appropriate threshold $\\lambda$ for the OOD score can be challenging in real-world applications. The paper should include a clear, practical procedure for determining this threshold to ensure consistent performance across diverse datasets and scenarios.\n\n3. To thoroughly validate the robustness of the proposed defense, it should incorporate adaptive attacks specifically designed to exploit the OOD scoring mechanism. Following the recommendations in [1], it should evaluate the effectiveness of the defense against these adaptive attacks to demonstrate its resilience under targeted adversarial conditions.\n\n[1] Tramer, Florian, et al. \"On adaptive attacks to adversarial example defenses.\" Advances in neural information processing systems 33 (2020): 1633-1645."
            },
            "questions": {
                "value": "1. Could the authors analyze the computational complexity of computing the OOD score, and how does it scale with the size of the dataset? Additionally, can the authors provide insights into how the number of in-distribution (ID) training samples affects the performance of the method?\n\n2. In practical applications, selecting the threshold $\\lambda$ for the OOD score can be challenging. Could the authors elaborate on the procedure for choosing an optimal threshold, especially under varying dataset conditions and deployment scenarios?\n\n3. Could the authors design adaptive attacks that directly target the proposed OOD scoring mechanism and evaluate the proposed defense against such adaptive attacks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}