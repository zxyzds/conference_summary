{
    "id": "EP6n8LCEK6",
    "title": "Understanding Prejudice and Fidelity of Diverge-to-Converge Multi-Agent Systems",
    "abstract": "Large language model (LLM) agents have demonstrated substantial potential across various tasks, particularly in multi-agent systems. Among these, \\textit{Diverge-to-Converge} (D2C) frameworks stand out for their ability to iteratively diversify and converge intermediate thoughts to improve problem-solving. In this paper, we conduct a comprehensive study on the \\textit{\\textbf{prejudice}} and \\textit{\\textbf{fidelity}} of typical D2C frameworks, including both model-level and society-level frameworks. \n\\ding{182} In the \\textit{prejudice} section, we uncover an inherent \\textit{confirmation bias} in D2C systems, which not only leads to suboptimal performance, but also amplifies social biases, such as gender discrimination and political partisanship. Surprisingly, we find that by reframing open-ended problems into binary questions, this bias can be leveraged to foster more equitable and effective agent interactions, ultimately improving performance.\n\\ding{183} In the \\textit{fidelity} section, we explore the scaling laws of D2C frameworks at different granularities, revealing that increasing the number of agents enhances performance only when the system is not yet saturated---such as in complex tasks or with weaker agents. In saturated scenarios, however, adding more agents can degrade performance. \nTo facilitate further study, we develop \\texttt{APF-Bench}, a benchmark specifically designed to evaluate such inherent weaknesses of D2C frameworks. \nWe hope our findings offer instructional insights into the strengths and limitations of D2C multi-agent systems, offering guidance for developing more robust and effective collaborative AI systems.",
    "keywords": [
        "Large language model agents",
        "Multi-Agent System",
        "Benchmark"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose to benchmark uncovered weaknesses of multi-agent systems",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=EP6n8LCEK6",
    "pdf_link": "https://openreview.net/pdf?id=EP6n8LCEK6",
    "comments": [
        {
            "summary": {
                "value": "This paper examines the limitations of Diverge-to-Converge (D2C) frameworks in large language model (LLM) agents, focusing on prejudice and fidelity. It reveals a confirmation bias in D2C systems that hampers performance and amplifies social biases, but reframing open-ended problems as binary questions mitigates these effects. The study also shows that increasing the number of agents only improves performance under unsaturated conditions. Additionally, the authors introduce APF-Bench, a benchmark to evaluate these weaknesses, providing insights for building better collaborative AI systems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. It uncovers and addresses *confirmation bias* in D2C frameworks, providing practical solutions to mitigate performance issues and social biases.\n2. The study examines both*prejudice* and *fidelity*, offering a detailed understanding of D2C frameworks across multiple levels.\n3. By demonstrating how reframing problems into binary questions improves fairness and effectiveness, the research has real-world applicability.\n4. The development of *APF-Bench* as a dedicated tool for evaluating D2C systems is a valuable resource for future research.\n5. The analysis of scaling laws provides essential guidelines for optimizing agent collaboration in different task scenarios."
            },
            "weaknesses": {
                "value": "1. Limited Real-World Testing: The findings might lack generalizability if not tested in diverse, real-world multi-agent scenarios.\n2. Potential Oversimplification: Reframing problems as binary questions may oversimplify complex tasks, possibly limiting the depth of solutions.\n3. Scalability Constraints: The performance degradation observed in saturated systems indicates a limitation in scaling D2C frameworks effectively.\n4. Bias Mitigation Trade-offs: While the approach reduces biases, it may inadvertently introduce new limitations or biases in certain contexts."
            },
            "questions": {
                "value": "* Why do you study D2C frameworks rather than other MAS frameworks? Is D2C a typical and widely adopted MAS framework? What are the incentives behind this choice?\n* What is the main *contribution in scientificity* that the paper claims? This paper does a lot of evaluation and analysis on different LLMs, but they are the existing ones. Could you provide insights into designing LLMs that can inherently avoid or mitigate confirmation bias? Or, can you give a discussion on the underlying causes of such bias, which could possibly arise at the data level or the pre-training/fine-tuning level instead of solely empirical discovery? \n* See also Weaknesses for other questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors investigate reasoning pathologies of a certain class of multi-agent LLM systems, Diverge-to-Converge (D2C) frameworks, both at the model- and society-level. The authors identify an inherent confirmation bias in D2C systems but results in social biases and task underperformance which can be alleviated if open-ended questions are re-phrased as binary. The authors then study the scaling laws of D2C frameworks, finding that more agents does only result in performance improvements if the system is not yet saturated but can otherwise even degrade performance. The authors suggest remedies for both these pathologies and release APF-Bench to specifically evaluate these weaknesses."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Very timely and can provoke thought - e.g. trade-off bias/compute\n* Robust evaluation across multiple datasets\n* the idea to use reframing to tackle biases seems novel"
            },
            "weaknesses": {
                "value": "* Could have discussed a greater variety of biases other than confirmation bias\n* It isn't clear how questions of real-world importance that are open-ended can always be brought into binary form.\n* conceptual advances are limited - scaling laws / reframing techniques themselves feel rather incremental\n\nline 216 \"menifest\""
            },
            "questions": {
                "value": "* How do you prevent bias in the debate judgements?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper conducts a study on confirmation bias from initial responses in different multi-agent LLM setups and comes up with a technique to prevent this bias (and thus improve benchmark performance) by changing the framing of questions. It then presents very initial work on how multi-agent system performance scales with the number of agents and tokens."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- The paper presents a very comprehensive review of existing multi-agent LLM research and fits in prejudice and fidelity quite well in these settings. This provides useful context to understand the paper\u2019s key contributions.\n\n- The problem reframing method is reasonably novel and the experimental evaluations are comprehensive enough to demonstrate improvements with this method.\n\n- It presents initial interesting results around differences in scaling the number of agents/LLM calls versus the number of tokens per generation. This could allow for a lot more future work in multi-agent LLM research.\n\n- APF-Bench encompasses other benchmarks and can act as a useful starting point for similar research directions."
            },
            "weaknesses": {
                "value": "- The paper explores only problem reframing as a bias mitigation strategy. However, not every problem can be converted into a binary problem, and other strategies are not explored at all.\n\n- The paper does not perform evaluations on any open source models.\n\n- The refinement strategy for datasets could introduce selection bias and skew results. I would be interested in seeing results across a random subset of the test set on the benchmarks used.\n\n- The paper spends its first 5.5 pages providing a background on the problem and multi-agent LLM settings. This takes away from its key contributions, which are limited to the problem reframing strategy and very introductory work on scaling laws around fidelity. Section 6.2 is extremely limited and does not back up its claims with linked experiments.\n\n- The appendix presents examples of model outputs, however it does not provide examples of inputs to the models (especially in the problem reframing setting). I\u2019ve posed questions around these examples in Questions section of my review."
            },
            "questions": {
                "value": "- Page 18, Case 2, GSM8k: Could the authors provide complete inputs to the models and their outputs for each iteration?\n\n- Is there a hypothesis around why the results hold and such biases occur in language models? Are there reasonable tests that can be conducted around this?\n\n- Could there exist better reframing techniques? Why was the binary reframing technique selected? Will it work for all tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on the Diverge-to-Converge (D2C) frameworks and highlights the challenges of prejudice and fidelity in D2C frameworks. The authors define prejudice and fidelity as the performance variation under changed conditions and scaling laws, respectively. To evaluate prejudice and fidelity, this paper introduces APF-Bench using the proposed Dataset Refinement. The results confirm the findings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-structured and easy to follow. The inclusion of informative figures and tables enhances clarity.\n2. This paper reveals two key challenges: the impact of initial conditions and the number of agents on the final performance.\n3. The experiments span many task-domains and multiple models."
            },
            "weaknesses": {
                "value": "1. This paper mainly proposes a benchmark to test and validate these challenges rather than further addressing them.\n2. In more complex scenarios, problem reframing is difficult."
            },
            "questions": {
                "value": "Minor comments:\n1. In Figure 1, should the question be \"A ship travels 80 miles east/west and 150 miles north. How far is the ship from its starting point?\".\n2. D2C instead of C2D, e.g. Section 5 Debatepedia, Dataset Problem Reframing, etc.\n3. Inconsistent symbol representation. In Section 3, C stands for the total number of calls, whereas in Section 4, C stands for Agent Count."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}