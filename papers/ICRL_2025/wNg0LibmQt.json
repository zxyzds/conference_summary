{
    "id": "wNg0LibmQt",
    "title": "Gradient-based Jailbreak Images for Multimodal Fusion Models",
    "abstract": "Augmenting language models with image inputs may enable more effective jailbreak attacks through continuous optimization, unlike text inputs that require discrete optimization. However, new *multimodal fusion models* tokenize all input modalities using non-differentiable functions, which hinders straightforward attacks. In this work, we introduce the notion of a *tokenizer shortcut* that approximates tokenization with a continuous function and enables continuous optimization. We use tokenizer shortcuts to create the first end-to-end gradient image attacks against multimodal fusion models. We evaluate our attacks on Chameleon models and obtain jailbreak images that elicit harmful information for 72.5% of prompts. Jailbreak images outperform text jailbreaks optimized with the same objective and require 3x lower compute budget to optimize 50x more input tokens. Finally, we find that representation engineering defenses, like Circuit Breakers, trained only on text attacks can effectively transfer to adversarial image inputs.",
    "keywords": [
        "jailbreak",
        "adversarial examples",
        "multimodal",
        "language models"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We introduce the notion of a tokenizer shortcut that enables the first gradient-based image jailbreak attack against multimodal fusion models.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wNg0LibmQt",
    "pdf_link": "https://openreview.net/pdf?id=wNg0LibmQt",
    "comments": [
        {
            "title": {
                "value": "Answers to your questions"
            },
            "comment": {
                "value": "Now, we address your questions:\n\n> How can the robustness of the proposed method be improved to maintain effectiveness across diverse settings, especially in the absence of the tokenizer shortcut? Can the authors evaluate performance under different tokenization schemes?\n\nAs we have seen with GCG on text prompts, it is likely that better optimization methods can achieve better transferability. As we mentioned above, this is still an open question that does not seem trivial to solve given existing work. Also, we only have Chameleon as an open-source early-fusion model. As more models are released, it is likely that ensembling over several models may increase transferability as in the original GCG work.\n\nWe are not sure what you mean with \u201cdifferent tokenization schemes\u201d. Could you clarify?\n\n> Why do we need the enhanced loss function? \n\nWe ablated different natural choices for the loss function to illustrate performance and help future work on optimization objectives. We find that using longer contextualized targets for each prompt results in a huge increase from only optimizing for the token \u201cSure\u201d.\n\n > Can the authors evaluate their method on other defenses, such as those mentioned by Jain et al. [1]?\n\nSince we focus on a white-box threat model, we evaluate only on defenses that are directly baked into the model weights and cannot be trivially disabled by adversaries. The strongest available defense for white-box attacks is Circuit Breakers. Circuit Breakers aim to modify the weights in such a way that even with white-box access attackers cannot access the knowledge. All measures in the referenced paper assume black-box deployment of the model. Nevertheless, our perplexity analysis could be seen as the implementation of the most popular defense (self-perplexity filtering) in the cited work by Jain et al.\n\n> What are the ablation results of changing the number of layers in the fully connected network or replacing it with other simple architectures?\n\nWe evaluated also a linear layer. Results are around 20% lower ASR for the best performing attacks. We will try to include an ablation analysis in the Appendix.\n\n> How can PPL be further validated as a reliable metric?\n\nThis has been explored in previous work, as the one you referenced earlier by Jain et al. However, perplexity filtering would again be a defense on a black-box threat model and we include the results to illustrate the behavior of the attack (i.e. we find images with high likelihood under then model unlike text attacks)."
            }
        },
        {
            "comment": {
                "value": "Thank you for your time and valuable feedback. We wrote a [general response](https://openreview.net/forum?id=wNg0LibmQt&noteId=0fWxmenJlb) with clarifications about our contributions and threat model. We believe some of the weakness raised may be motivated by a not-so-great explanation of the threat model and motivation in the paper. We are working to implement this together with other feedback from reviewers. We hope the general response and our comments below may clarify this, but we are happy to discuss further. We address the specific weaknesses below:\n\n> The effectiveness of the proposed approach is not well-validated. \n\nWe discuss the effectiveness of the method in our specific white-box threat model (more on this in the general response). Additionally, we focus on presenting a method that enables optimization. We expect future work to look for novel optimization objectives that, combined with our optimization method, can result in more transferable attacks. Concurrent work suggests transferability is not a trivial problem to solve [1] \n\n[1] Schaeffer, Rylan, et al. \"When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?.\" arXiv preprint arXiv:2407.15211 (2024).\n\n> From Table 2, the attack success rate drops when adding the refusal prefix part.\n\nIn general, optimizing text tokens is a very brittle process. As we add more objectives, it is easier to find solutions that do not necessarily suit the attacker objective. For example, we find that reducing the probability of \u201cI am sorry\u201d increases the probability of alternatives like \u201cUnfortunately, as an AI assistant\u2026\u201d while not necessarily maximizing non-refusal. This is why we believe only optimizing for the target tokens is more effective.\n\n> The approach's effectiveness is further limited when defenses are in place\n\nWe like to see this negative result as a positive and interesting finding by itself. There is no apparent reason why we should expect text safeguards to transfer to white-box image attacks. In fact, in the original Circuit Breaker work, they found that vision language models (adapter-based) should also be optimized on images to get an effective defense. In our case, we find that safeguards in early-fusion architectures may transfer better across modalities, this is worth exploring further by future work. As we are starting to see for Circuit Breakers in text, we expect that more complex optimization objectives will also find successful jailbreak images.\n\n> In direct attack scenarios, the method presumes the target model has been modified to include the shortcut, but it is unlikely defenders would incorporate this modification.\n\nWe contextualize our work on a white-box threat model where the attacker has access to the weights of the model and can perform any manipulation. See the general response.\n\n> The approach also lacks sufficient transferability.\n\nAlthough we agree this is a negative result of our attack, we believe this is a very interesting finding that was also found by concurrent work in adapter-based architectures [1]. It is not trivial why text attacks like GCG transfer and tokenized images in our setup do not. This opens interesting questions for future work and finding transferable images should be a project of itself. Our focus was placed on building a method for gradient propagation that could enable future work to design better optimization objectives that could result in transferable images, among other improvements.\n\n> The compared baselines are limited, just focusing primarily on text-based attacks GCG. A broader selection of attack methods would improve the robustness of the evaluation.\n\nOur goal was never to find the best possible attack, but rather to introduce a new method that enables end-to-end optimization. The comparison with GCG tries to motivate why jailbreak images may be more efficient to optimize than text while obtaining similar performance. We believe that, the same way that we could use more complex baselines, our method could be used to instantiate more complex attacks that also increase the ASR. So we would like not to place to much focus on specific ASR.\n\n> The use of PPL to measure adversarial prompt effectiveness lacks sufficient validation as a reliable metric.\n\nPerplexity filtering has been widely studied as a filter for GCG prompts [2] and is presumably implemented in most production models like ChatGPT. Nevertheless, in a white-box threat model we do not expect attackers to deploy optional protections. We use PPL to illustrate an interesting property that may motivate future work: image attacks, unlike text, do not increase perplexity of the prompt. In fact, they find images that have high likelihood under the model.\n\n[2] Alon, Gabriel, and Michael Kamfonas. \"Detecting language model attacks with perplexity.\" arXiv preprint arXiv:2308.14132(2023)."
            }
        },
        {
            "comment": {
                "value": "Thank you for your time and valuable feedback. We wrote a [general response](https://openreview.net/forum?id=wNg0LibmQt&noteId=0fWxmenJlb) with clarifications about our contributions and threat model. We believe some of the weakness raised may be motivated by a not-so-great explanation of the threat model and motivation in the paper. We are working to implement this together with other feedback from reviewers. We hope the general response and our comments below may clarify this, but we are happy to discuss further. We address the specific weaknesses below:\n\n> The proposed method of modifying the model architecture (replacing the original tokenizer) to elicit the jailbreak does not make much sense\n\nUnder a white-box threat model, the goal is to find any method that is able to elicit knowledge that was stored in the model weights. In the text domain we have seen methods like refusal suppression, included as a baseline, that also require modifying the architecture or activations at inference time. It is important to contextualise our contributions in the white-box threat model, we will try to incorporate these details in the paper. (More on this in the general response)\n\n> The perturbed (attacked) images lack transferability.\n\nAlthough we agree this is a negative result of our attack, we believe this is a very interesting finding that was also found by concurrent work in adapter-based architectures [1]. It is not trivial why text attacks like GCG transfer and tokenized images in our setup do not. This opens interesting questions for future work and finding transferable images should be a project of itself. Our focus was placed on building a method for gradient propagation that could enable future work to design better optimization objectives that could result in transferable images, among other improvements.\n\n> Text attacks already achieve this attack.\n\nIn Section 5.2 we discuss the main differences between text and image attacks. On the one hand, image optimization is more stable leading to higher attack success rates and, more importantly, to a significant speed up in the optimization process (at least 3x). Augmenting the attack surface and the tools available for adversarial evaluation of models and protections is important.\n\n> Lack of comparison to existing works of VLLMs.\n\nWe are sorry if this was not clear enough. Our paper is motivated by the difference that exists between VLLMs and early-fusion models. The models explored in previous work (we will include a citation to the mentioned work), are adapter-based. This means that there is already a differentiable path between the output and the image spaces. However, these architectures are very limited and the field is moving towards image tokenization. Our work thus focuses on introducing a method that enables end-to-end gradient propagation (similar to what we saw in previous work with VLLMs) on this new architectures. This is discussed in the second paragraph of the introduction and the \u201cJailbreaking language models\u201d in the preliminaries. Please, let us know if this is not clear enough so that we can clarify it in a future version of the paper.\n\n\nWe now answer your questions:\n\n> Transferability in other setups.\n\nImages do not transfer from 30B to 7B either. \n\n> Why embedding attack has higher ASR than 1-hot?\n\nThis is a great question and also related to a question by reviewer `zzub`. We will include a discussion comparing both methods in the future version of the paper. The embedding space gives a more expressive representation to find successful attacks since optimization is unrestricted. Optimizing the 1-hot representation encourages changes in the image that result in changes in the tokenization. Since the mapping from tokens to embeddings is deterministic and sparse, this can be seen as a regularization that may hinder the attack at the cost of better transferability if the shortcut is turned off.\n\n[1] Schaeffer, Rylan, et al. \"When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?.\" arXiv preprint arXiv:2407.15211 (2024)."
            }
        },
        {
            "comment": {
                "value": "Thank you for your time and valuable feedback. We wrote a [general response](https://openreview.net/forum?id=wNg0LibmQt&noteId=0fWxmenJlb) with clarifications about our contributions and threat model. We believe some of the weakness raised may be motivated by a not-so-great explanation of the threat model and motivation in the paper. We hope the general response and our comments below may clarify this, but we are happy to discuss further. We address the specific weaknesses below:\n\n> Comparison to other methods to make quantization differentiable.\n\nWe found our shortcuts to be the most effective among several approaches we considered. Do you have any other suggestions we could evaluate against? The main contribution of our work is introducing a method that enables end-to-end optimization of images in new early-fusion architectures. We expect future work to combine this method with improved optimization objectives that can improve upon different axis of success like transferability.\n\n>  It seems that the proposed method can generate jail-break images in very limited settings\n\nOur method is meant to elicit dangerous capabilities in a white-box threat model with full access to the model. We explain why this is important and different applications (e.g. defense evaluation) in the general response.\n\n> Importance of white-box threat model.\n\nWe agree with your point here. We will extend a bit on the importance of white-box threat models. As open-source models become more powerful, it is more likely that adversaries use these models to automate harmful activities rather than actively jailbreaking closed-source models. Uncovering vulnerabilities under white-box access is important to motivate the need of new defenses and serve as hard evaluations for such defenses.\n\nAdditionally, white-box evaluations allow us to assess worst-case performance of models. For example, these methods can be used to stress test the models and evaluate whether they indeed possess harmful knowledge. This can be used to robustly evaluate defenses and unlearning methods.\n\n> Lack of transferability\n\nAlthough we agree this is a negative result of our attack, we believe this is a very interesting finding that was also found by concurrent work in adapter-based architectures [1]. It is not trivial why text attacks like GCG transfer across models and tokenized images in our setup do not. This opens interesting questions for future work and finding transferable images should be a project of itself. Our focus was placed on building a method for gradient propagation that could enable future work to design better optimization objectives that could result in transferable images, among other improvements.\n\n> Insufficient explanations.\n\nThanks for bringing these two issues up. For the \u201cimage jailbreaks are superior\u201d we were focusing on models without additional defenses. Also, comparison of Circuit Breakers is tricky since some of the successes for GCG are not really useful to solve the task and our result may slightly overestimate the success of the method and the differences are not that clear in that setup.\n\nOn the second point, we definitely agree we should include early-fusion in the statement to make it more accurate. Thanks for the catch.\n\nFinally, to your question:\n\n> Suggestion to move related work\n\nWe believe most relevant aspects of the related work are contained in the preliminaries section. What content did you find important and not self-contained in previous sections? This can help us assess whether it would be better to move the entire section or just include additional details in existing sections."
            }
        },
        {
            "comment": {
                "value": "Thank you for your time and valuable feedback. We wrote a [general response](https://openreview.net/forum?id=wNg0LibmQt&noteId=0fWxmenJlb) with clarifications about our contributions and threat model. We address the specific weaknesses below:\n\n> The choice of the two shortcut is not clearly explained in section 3.\n\nThe embedding and 1-hot space are the only two continuous input spaces to the model that allow us to create a fully differentiable end-to-end path. Is this type of motivation for the choice what you had in mind?\n\n> More qualitative analysis.\n\nWe are working to include successful jailbreak generations from the models in the Appendix of the paper. With respect to images, most images just include perturbations that look pretty arbitrary to humans; we believe there are no observable differences between successful and unsuccessful images. We are happy to include several examples in the Appendix if you think this could improve the understanding of our method.\n\n> Ideas for future work.\n\nThanks for bringing this up. Our work indeed was meant to introduce a method for gradient propagation and not necessarily the best attack possible. We believe there is a lot of room for future work to use this method in combination with better optimization strategies to obtain stronger attacks. Since there are no other open-source early-fusion models makes research also challenging at the moment because methods can easily overfit to the Chameleon architecture and not transfer more generally.  We have included several directions that naturally follow from our work in the discussion section. All our code is open-source so reproduction of our results should be easy for people to build upon our work. What details of transferability experiments would you like to see in the paper? We are happy to include those."
            }
        },
        {
            "comment": {
                "value": "Thank you for your time and valuable feedback. We wrote a [general response](https://openreview.net/forum?id=wNg0LibmQt&noteId=0fWxmenJlb) with clarification about our contributions and threat model. We address the specific weaknesses below:\n\n> The dataset is quite small\n\nWe agree with this, but decided to stick to what we consider to be the best jailbreak benchmark as of today. This benchmark already curated prompts from other existing benchmarks. Although we could extend results to other benchmarks, most prompts will be redundant and similar to those in JailbreakBench. We are happy to discuss further experiments if you think this can bring fundamental new insights to the method.\n\n> The paper does not include any examples of jailbroken model responses\n\nThis is a great point! We are working to include those.\n\n> Include results without shortcut in Tables 2 and 4.\n\nThank you! We are working to include these in the Appendix. We initially left them out since we are assuming a white-box threat model where the attacker can use the strongest attack available. See our general response for clarifications on the threat model.\n\nNow, we answer your question:\n\n> Do the authors have an explanation for why the embedding space shortcut attacks do not transfer to non-shortcut models while the 1-hot attacks do?\n\nOur intuition is that using the 1-hot attack creates an inductive bias where the gradients incentivize changes in the image that produce changes in the actual tokenization. Thus, the resulting image is more likely to be tokenized in a way that generalizes when the shortcut is turned off. However, using the embedding is likely to result in changes that do not necessarily affect tokenization and thus are lost once the shortcut is turned off. We will include this hypothesis in the discussion section."
            }
        },
        {
            "title": {
                "value": "Clarifications about threat model and research goals"
            },
            "comment": {
                "value": "We thank all the reviewers for their time and feedback. In this general response, we would like to clarify some aspects of the paper, especially regarding the threat model we operate in and why we believe this is relevant. We will try to clarify these points in the paper as well.\n\n**Main goal of the paper**: design a new method that enables end-to-end optimization for jailbreak images in novel early-fusion architectures. This attack can be use to **elicit harmful information stored in models**. We believe this is a contribution in itself and future work can improve optimization targets to elicit more transferable attacks, as we have seen the community iterate on text attacks like GCG.\n\n**Threat models and assumptions**: In our work, we operate under a *white-box threat model*. What this means is that the attacker can download the weights of the model and run it locally with any desired modifications and disabling any optional safeguards. This threat model is relevant for two reasons:\n\n* As open-source models become more capable, attackers are likely to use open-source models for harmful tasks, rather than jailbreaking closed source models.  \n* White-box evaluations are important to understand worst-case behavior of models. Namely, if you are developing a model, white-box attacks can *elicit the worst-case performance of the model*, which you can assume to be an upper bound on black-box vulnerabilities. *White-box jailbreaks are thus also a useful tool to evaluate defenses.*\n\nSince we operate in white-box threat model, we only consider the Circuit Breakers as a defense, as it is the strongest jailbreak defense available that is **directly baked into the model weights** and thus cannot be trivially disabled by adversaries (like additional filters on the inputs and outputs).\n\nWe include transferability results since we believe it is interesting to highlight that our findings agree with those of concurrent work that report transferability of image jailbreaks to be harder than expected and an open problem \\[1\\]. We expect future work to be able to use our shortcut technique to optimize other objectives that might result in more transferable images.\n\n\\[1\\] Schaeffer, Rylan, et al. \"When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?.\" *arXiv preprint arXiv:2407.15211* (2024)."
            }
        },
        {
            "summary": {
                "value": "* The paper develops a white box gradient-based image jailbreak method for multimodal fusion models. Prior work on gradient-based image jailbreaks has focused on VLMs due to the lack of open source fusion models, but this has recently changed with the release of Chameleon.  \n* The core challenge of doing this with fusion models is that gradients do not flow through to the input image due to a non-differentiable step in tokenization.   \n* The authors solve this problem by introducing a novel \u201ctokenizer shortcut\u201d technique, where they train a small MLP to approximate the image tokenizer in a differentiable way. The tokenizer is then replaced by this approximation during adversarial image optimization, allowing gradient-based optimization to succeed.  \n* Two versions of the tokenizer shortcut are developed, one mapping directly to embedding space and one producing a one-hot vocabulary encoding.  \n* A comprehensive set of experiments are conducted. Key findings:  \n  * Both shortcut methods produce images with high Attack Success Rate, but only the 1-hot shortcut images transfer to versions of the model that do not use the shortcut.  \n  * Circuit breakers substantially reduce ASR  \n  * Jailbreak images transfer easily across prompts but do not transfer across models  \n* The authors also conduct a series of ablations, including on response prefix, softmax temperature, and number of train prompts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "* This is a novel method that solves the core challenge of creating gradient-based image jailbreaks for multimodal fusion models.  \n* Understanding the vulnerabilities in multimodal models is important for developing more robust systems, and gradient-based jailbreaking of fusion-based models has been under-explored.  \n* The authors use good baselines for their experiments (GCG and refusal direction attacks), and convincingly demonstrate the success of their method  \n* The experiments are thorough and informative, testing attack transfer as well as defence using circuit breakers. Several ablations are also performed."
            },
            "weaknesses": {
                "value": "* The dataset used is quite small, with only 80 prompts in the test set for direct attacks and 20 in the test set for transfer attacks. The results would be more convincing if done on a larger dataset. In addition, only a single dataset is tested.  \n* The paper does not include any examples of jailbroken model responses - these are helpful for qualitative understanding of the attack.\n* With the exception of table 1, the results given are all for models using the tokenizer shortcut. It would be helpful to also include  the results when using the 1-hot jailbreak images on models without the shortcut in Tables 2 and 4."
            },
            "questions": {
                "value": "* Do the authors have an explanation for why the embedding space shortcut attacks do not transfer to non-shortcut models while the 1-hot attacks do?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies white box attacks against multimodal fusion models. This setting is considered interesting because these models convert all inputs - both text and images - into a shared tokenized space. This approach could make the models vulnerable to more efficient attacks through image optimization, since images offer a continuous space to optimize (unlike text which is discrete). In order to optimize potential attack inputs, they develop the tokenization shortcut method, mapping image embeddings to a continuous model input space before quantization. They find that for whitebox optimization attacks, images are more effective than text, however they do not beat other competitive baselines like representation engineering."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The choice of studying robustness of multimodal fusion models is timely.\n- The selection of research questions is fitting for a first study in a fast-paced field. The hypothesis that it may be easier to attack models with this architecture is interesting, and is very useful to study early in the uptake of architectures.\n- The paragraph writing style is easy to read, and the work can serve as an interesting log of experiments for other practitioners."
            },
            "weaknesses": {
                "value": "- The choice of the two shortcut is not clearly explained in section 3. It would be useful to spell it out.\n- It would be useful to have more qualitative analysis or at least examples of jailbreaking images vs images that fail."
            },
            "questions": {
                "value": "- Your experiments seem interesting, and it seems like you may have opinions on future work. While you have already provided motivation for experiment design, it would be useful to add more detail to your results so that it is easier to judge what puzzles are worth investigating. For instance, it would be great to spell out the details of transferability experiments. The observation itself is cool, but the current presentation of the work makes it so that readers will have to reimplement your work to get started on forming hypotheses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method generating jail-break images that cause early-fusion VL models (especially those with discrete image tokens) to generate harmful content when jail-break images are appended with harmful prompts.\nUnlike adapter-based VL models, which do not use image token discretization, the discrete tokenization of images in early-fusion VL models makes direct optimization through gradients challenging and limits the applicability of existing methods.\nTo address this, the paper proposes a tokenizer shortcut that bypasses the discrete tokenization process by replacing quantization with a shallow MLP module, enabling the computation of gradients. \nThe experiments demonstrate the effectiveness of the proposed method for generating jail-break images under certain settings\u2014specifically, a white-box attack."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. This paper addresses an important reasearch topic of jail-breaking in VL-LLM models, considering the significant growing use of VL models in real world applications. Research in this direction seems essential.\n2. This paper is well presented, making the paper easy to follow and understand."
            },
            "weaknesses": {
                "value": "1. There is a lack comparison or discussion with other condidates to make quantizqation differentiable. If the proposed method achieves very strong performance in generating jail-breaking iamges, current approach would be acceptable. However, it seems that the proposed method can generate jail-break images in very limited settings: with shortcut or non-transfer setting.\n2. As far as i understand, the white-box attack scenario is important because, although it may be impractical and unrealistic, it serves as a useful benchmark for black-box attacks. However, for the \"with shortcut\" results, it effectively becomes equivalent to altering the model itself, which makes discussions of attack performance somewhat meaningless. Nonetheless, the proposed method is primarily evaluated using the shortcut when demonstrating its strong performance, (Table 1, 2, 3, 4).\n3. Optimizing within the input (image and text) space is important, as it is a prerequisite for black-box settings or model transfer. However, as shown in Table 5, the proposed method fails to produce transferable samples and underperforms compared to the baseline.\n4. (Minor) The paper seems to contain overclaims or insufficient explanations. For example:\n\t- The title of Table 3 is \"Image jailbreaks outperform text attacks,\" but the proposed method performs worse than the text-only attack, GCG, in the Circuit Breaker setting. Additionally, comparing GCG with the proposed method \"with shortcut\" seems unfair, as \"with shortcut\" is equivalent to changing the model.\n\t- In discussions and future works, the paper states, \"(412) Our work is the first attempt to jailbreak multimodal architectures using end-to-end gradient attacks\" and \"(423) this problem also persists in multimodal models,\". I guess the \"fusion-based model\" shall be more appropriate."
            },
            "questions": {
                "value": "1. Please address the weaknesses.\n2. (Suggestion) Moving the related work section front or refering that more related work is in the later part shall improve the understanding of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed jailbreak attacks on multimodal fusion models by introducing a *differentiable* tokenizer shortcut. This allows for continuous optimization of adversarial images intended to bypass model safeguards. It evaluates the effectiveness of such attacks on Chameleon models, achieving a higher attack success rate than text-only jailbreaks. The results suggest that representation engineering defenses for text attacks could also adapt to adversarial image inputs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- **Well-structured:** The paper is well-written and describes the proposed method clearly.\n- **Introduced Differentiable Tokenizer:** This paper proposes using a two-layer neural network to make image tokenization in a multimodal fusion model feasible, enabling continuous optimization and revealing its threats to jailbreak."
            },
            "weaknesses": {
                "value": "- The proposed method of modifying the model architecture (replacing the original tokenizer) to elicit the jailbreak does not make much sense; also, the perturbed (attacked) images lack transferability. Given that a text-based attack is already feasible to pose such threats, I tend to buy the proposed method that applies the traditional method of generating adversarial perturbations to a multimodal fusion model. This method, however, is neither novel nor practically applicable to my understanding.\n\n- Using adversarial images to elicit model jailbreak is also not novel; the paper lacks some discussion and comparison with existing works on VLLM [1].\n\n[1] Visual Adversarial Examples Jailbreak Aligned Large Language Models (AAAI 2024)"
            },
            "questions": {
                "value": "- Could you provide more insights into why the experimental results demonstrated a higher Attack Success Rate (ASR) using the embedding shortcut compared to the 1-hot shortcut?\n- While it is understandable that jailbreak images optimized for Chameleon-7B might not transfer effectively to larger models, have you explored or observed whether jailbreak images optimized on larger models could be effectively transferred to smaller ones, such as from a Chameleon-30B to a Chameleon-7B model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a gradient-based jailbreak method for multimodal fusion models. The authors introduce tokenizer shortcuts to solve the problem of continuous optimization not being carried out in the multimodal fusion model due to the discretization of input modalities. The experimental evaluation is carried out on the Chameleon multimodal fusion model. The results show that their method can trigger the generation of harmful information."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The approach is straightforward, relying on a fully connected network structure to approximate image tokenization.\n2. The research addresses an important problem by targeting vulnerabilities in multimodal fusion models."
            },
            "weaknesses": {
                "value": "1. The effectiveness of the proposed approach is not well-validated. Without the tokenizer shortcut, the method's performance declines significantly, suggesting it may lack robustness in different settings.\n\n2. From Table 2, the attack success rate drops when adding the refusal prefix part. The enhanced loss function, which aims to reduce the probability of generic refusal tokens, does not demonstrate a clear benefit in the experiments.\n\n3. The approach's effectiveness is further limited when defenses are in place, raising concerns about its resilience against common protective measures.\n\n4. Practical applicability is limited as the approach relies on assumptions that may not align with realistic conditions.\n\n4.1 In direct attack scenarios, the method presumes the target model has been modified to include the shortcut, but it is unlikely defenders would incorporate this modification.\n\n4.2 The approach also lacks sufficient transferability, reducing its usability across different models or settings.\n\n5. The compared baselines are limited, just focusing primarily on text-based attacks GCG. A broader selection of attack methods would improve the robustness of the evaluation.\n\n6. The use of $\\Delta$PPL to measure adversarial prompt effectiveness lacks sufficient validation as a reliable metric."
            },
            "questions": {
                "value": "1. How can the robustness of the proposed method be improved to maintain effectiveness across diverse settings, especially in the absence of the tokenizer shortcut? Can the authors evaluate performance under different tokenization schemes?\n\n2. Why do we need the enhanced loss function? \n\n3. Can the authors evaluate their method on other defenses, such as those mentioned by Jain et al. [1]?\n\n5. What are the ablation results of changing the number of layers in the fully connected network or replacing it with other simple architectures?\n\n6. Can the authors include additional baseline methods to more comprehensively assess the robustness and effectiveness of the proposed method, such as the FGSM, PGD, or any other reliable attack methods?\n\n7. How can $\\Delta$PPL be further validated as a reliable metric? For example, evaluating $\\Delta$PPL's effectiveness in multimodal fusion models with an F1 score would provide a clearer, more reliable assessment.\n\n[1] Jain, N., Schwarzschild, A., Wen, Y., Somepalli, G., Kirchenbauer, J., Chiang, P. Y., ... & Goldstein, T. (2023). Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}