{
    "id": "PNMv4r7s1i",
    "title": "Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization",
    "abstract": "Reinforcement learning from human feedback (RLHF) is an effective method for aligning large language models (LLMs) with human values. However, reward over-optimization remains an open challenge leading to discrepancies between the performance of LLMs under the reward model and the true human objectives. A primary contributor to reward over-optimization is the extrapolation error that arises when the reward model evaluates out-of-distribution (OOD) responses. However, current methods still fail to prevent the increasing frequency of OOD response generation during the reinforcement learning (RL) process and are not effective at handling extrapolation errors from OOD responses. In this work, we propose the *Behavior-Supported Policy Optimization* (BSPO) method to mitigate the reward over-optimization issue. Specifically, we define *behavior policy* as the next token distribution of the reward training dataset to model the in-distribution (ID) region of the reward model. Building on this, we introduce the behavior-supported Bellman operator to regularize the value function, penalizing all OOD values without impacting the ID ones. Consequently, BSPO reduces the generation of OOD responses during the RL process, thereby avoiding overestimation caused by the reward model\u2019s extrapolation errors. Theoretically, we prove that BSPO guarantees a monotonic improvement of the supported policy until convergence to the optimal behavior-supported policy. Empirical results from extensive experiments show that BSPO outperforms baselines in preventing reward over-optimization due to OOD evaluation and finding the optimal ID policy.",
    "keywords": [
        "Reward Over-Optimization",
        "Reinforcement Learning from Human Feedback",
        "Large Language Model"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PNMv4r7s1i",
    "pdf_link": "https://openreview.net/pdf?id=PNMv4r7s1i",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces an approach named Behavior-Supported Policy Optimization (BSPO) to address the challenge of reward over-optimization in Reinforcement Learning from Human Feedback. The core issue addressed is the extrapolation error that arises when the reward model evaluates out-of-distribution responses, leading to discrepancies between the performance of LLMs under the reward model and true human objectives. The authors propose using a behavior policy to model the in-distribution region of the reward model and introduce a behavior-supported Bellman operator to regularize the value function, penalizing out-of-distribution values without impacting in-distribution ones. Theoretical proofs are provided to show that BSPO guarantees monotonic improvement of the supported policy until convergence to the optimal behavior-supported policy. Empirical results demonstrate BSPO's effectiveness in preventing reward over-optimization and finding the optimal ID policy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-organised and articulated with clarity, the complex concepts are explained in a clear and concise manner. \n2. The paper provides theoretical justifications, and the results are convincing."
            },
            "weaknesses": {
                "value": "1. The paper primarily focuses on the synthetic set for evaluating reward over-optimisation, the alignment of the reward with real human annotators\u2019 evaluation remains insufficiently validated.\n2. Although the paper did experiments on the robustness of BSPO with noisy data, a more in-depth analysis of how BSPO performs under various types of noise or distributional shifts could strengthen the paper."
            },
            "questions": {
                "value": "The paper mentions that using V values instead of Q values provides greater stability, mainly because state transitions are deterministic in the context of LLMs. Specifically, for a given input prompt and a sequence of generated tokens, the generation of the next token is deterministic\u2014given the current state and action, the next state is uniquely determined. In this case, there is a direct relationship between V values and Q values. But what if the next token generation is sampled, introducing uncertainty into the transition function? How does BSPO work in such a situation?\n\nThere is a lack of analysis of the experimental results. What causes the drop observed during the training of CPPO?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work addresses reward over-optimization in RLHF by penalizing out-of-distribution (OOD) responses, which are a significant source of extrapolation errors during evaluation. The proposed approach, Behavior-Supported Policy Optimization (BSPO), identifies OOD tokens by checking if the predicted probability from the reward model falls below a defined threshold. It integrates an auxiliary loss based on the supervised fine-tuning (SFT) objective into the reward model\u2019s training. The proposed mechanism helps mitigate OOD overestimation without impacting the model\u2019s performance on in-distribution (ID) data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper effectively addresses a critical and well-motivated issue in RLHF, reward over-optimization due to OOD responses.\n\n2. The proposed Behavior-Supported Policy Optimization (BSPO) method introduces a unique approach by leveraging a behavior policy for OOD detection and integrating it with value regularization.\n\n3. The empirical validation through synthetic experiments demonstrates that BSPO outperforms baseline methods.\n\n4. The paper\u2019s methodology is rigorously supported by theory, with proofs ensuring the convergence and stability of the behavior-supported value functions."
            },
            "weaknesses": {
                "value": "1. A significant concern is that the experiments are conducted solely on synthetic data. This limitation raises questions about the applicability and effectiveness of the proposed method in real-world scenarios.\n\n2. The experimental results lack an ablation study on the sensitivity of the parameter $\\epsilon_\\beta$. \n\n3. The paper does not adequately discuss related works that incorporate SFT loss into the training objective, albeit for different purposes, such as in policy loss. Notable examples include:\n- Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer\n- Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF"
            },
            "questions": {
                "value": "1. How do you select the value of $V_{min}$ and $\\epsilon_\\beta$?\n\n2. If the SFT loss is not added to the training of reward model, can the reward model still recognize OOD prompts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses reward over-optimization in Reinforcement Learning from Human Feedback (RLHF), where large language models (LLMs) sometimes align poorly with human objectives due to extrapolation errors when evaluating out-of-distribution (OOD) responses. The authors propose Behavior-Supported Policy Optimization (BSPO), which models the in-distribution (ID) region by defining a behavior policy based on the reward training dataset\u2019s next token distribution. To regularize the value function, BSPO introduces a behavior-supported Bellman operator, penalizing OOD values while leaving ID values unaffected, thus reducing OOD response generation. BSPO is shown theoretically to guarantee monotonic improvement of the policy within the ID region, leading to convergence at the optimal policy. Empirical results confirm BSPO\u2019s superiority over baselines in preventing reward over-optimization, making it effective in aligning LLMs with human intent."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The research focus is clearly presented, providing readers with the essential background needed to understand the main contributions of the paper.\n\n- The core idea of Behavior-Supported Policy Optimization (BSPO) is intuitive and easy to implement, with a strong theoretical ground covering key properties like contractivity, monotonicity, and convergence.\n\n- The synthetic experiments are well-designed to demonstrate the rigor of the proposed method. The results are presented and interpreted clearly, making the comparisons with baseline methods easy to follow and effectively highlighting the strength of BSPO.\n\n- The literature review is thorough, giving readers, even those unfamiliar with reward over-optimization, a clear view of the related research landscape."
            },
            "weaknesses": {
                "value": "- The experimental results presented in this paper are demonstrated exclusively on the UltraFeedback dataset, making it essential to evaluate the generalizability of the proposed method by conducting experiments on additional benchmark datasets.\n\n- The paper emphasizes the concept of the \"In-Distribution (ID) region of the reward training dataset\" as a key to avoiding reward over-optimization, with BSPO\u2019s foundational idea built on this concept. Given this emphasis, it would have been helpful to see a performance comparison with DPO-based methods, which directly utilize the ID region of reward training datasets (e.g., win response $y_w$ and lose response $y_l$), to further contextualize BSPO\u2019s effectiveness.\n\n- As noted in the limitations, significant differences may exist between human preferences and model-predicted preferences, underscoring the importance of human evaluation for generated responses. In cases where human judgment is not feasible, using an LLM-as-a-judge (please refer to Reference) could serve as a practical alternative for conducting pseudo-human evaluation.\n\n- Some visual content is challenging to interpret at first glance. For instance, it seems unclear in Figure 1(c) what \"correct/incorrect\" specifically indicates, as this is not explicitly mentioned in the main text. Additionally, Figure 2(a) is somewhat confusing, as it places the \"query+answer (given past tokens)\" at the top and the \"behavior distribution (next tokens)\" at the bottom, unlike the conventional layout where the past context is on the bottom and the future sequence is on the top. A reversed figure layout might have improved clarity.\n\n- Some explanations were not detailed enough. For example, it would be helpful if there were some sentences describing where the (pretrained) proxy reward model $R\\_{\\phi}(\\cdot)$, which is optimized by $\\mathcal{L}\\_{\\text{ScoreLM}}(\\phi; \\mathcal{D})$, is placed at the calculation of $\\mathcal{L}\\_{V}(\\phi; \\pi)$.\n\n**Reference**\n- Zheng, L., Chiang, W. L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., ... & Stoica, I. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 46595-46623."
            },
            "questions": {
                "value": "- In Eq (6), is the supervised loss calculated with respect to both $y_w$ and $y_l$?\n\n- Regarding the main results in Figure 3, how were the remaining 27k samples, which were not used for training the proxy reward model, utilized? What was the rationale behind splitting the dataset into 30k and 27k?\n\n- As noted in the Weakness section, is there a specific reason for not comparing BSPO with direct preference optimization methods like DPO, KTO, and CTO?\n  - In what ways do you consider BSPO to be superior to DPO-related methods?\n\n- In the first line of Eq (15) in the Appendix, should it be $a' \\sim \\pi(\\cdot|s')$ instead of $a' \\sim \\pi(\\cdot|\\pi)$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the issue of reward over-optimization in RLHF and introduces Behavior-Supported Policy Optimization (BSPO) to prevent the generation of out-of-distribution responses during RL training. By learning the value function using in-dataset actions, the author refines the policy through PPO updates. Empirical results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The studied problem is important for RLHF and the proposed method is well-motivated.  \n\n* The idea of using reward model training distribution to regularize the value function learning for LLMs is novel, to the best of my knowledge.\n\n* The empirical evidence shows promising results to alleviate the overoptimization issue."
            },
            "weaknesses": {
                "value": "Despite the strengths of this paper, I identified several limitations that require further attention:\n\n1. The paper claims that prior constraint optimization methods \"only suppress overestimation in the ID region.\", which I disagree. The worst-case optimization method with ensemble has been shown to mitigate over-optimization for OOD actions [1]. During PPO training, a worst-case reward can also penalize OOD responses generated by the trained LLM. The authors should provide a more accurate comparison of BSPO with previous methods.\n\n2. There seems to be a problem in the proof of Corollary 1. The authors suggest that optimizing the objective in Eq 23 forces the learned policy to choose behavior-support actions. However, there are cases where all actions are OOD for a given prompt $s$, which could inevitably introduce OOD actions. This situation can arise when the SFT model and reward training use different datasets. Thus, the argument may not hold.\n\n3. The current BSPO can only be applied to token-level RLHF, while many studies focus on response-level RLHF. Can the proposed method also be adapted for response-level RLHF?\n\n4. The evaluation is restricted to reward models smaller than 3B. Validating on 7/8B reward models would strengthen the experiments.\n\n5. The authors should explore how the two threshold hyperparameters, $V_{min}, \\epsilon_{\\beta}$, affect the method's performance.\n\n6. Prior work [2][3] considers a noisy label setting, where the over-optimization problem is more severe. Can BSPO be effective in this challenging setting?\n\n7. Typos: In Eq 36, the use of Q function is not correct.\n\n**References**\n\n[1] An G, Moon S, Kim J H, et al. Uncertainty-based offline reinforcement learning with diversified q-ensemble[J]. Advances in neural information processing systems, 2021, 34: 7436-7447.\n\n[2] Ram\u00e9 A, Vieillard N, Hussenot L, et al. Warm: On the benefits of weight averaged reward models[J]. arXiv preprint arXiv:2401.12187, 2024.\n\n[3] Yang R, Ding R, Lin Y, et al. Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs[J]. arXiv preprint arXiv:2406.10216, 2024."
            },
            "questions": {
                "value": "Please refer to the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}