{
    "id": "mrjOaRyefn",
    "title": "Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs",
    "abstract": "Although safely enhanced Large Language Models (LLMs) have achieved remarkable success in tackling various complex tasks in a zero-shot manner, they remain susceptible to jailbreak attacks, particularly the unknown jailbreak attack. To enhance LLMs' generalized defense capabilities, we propose a two-stage adversarial tuning framework, which generates adversarial prompts to explore worst-case scenarios by optimizing datasets containing pairs of adversarial prompts and their safe responses. In the first stage, we introduce the hierarchical meta-universal adversarial prompt learning to efficiently and effectively generate token-level adversarial prompts.  In the second stage,  we propose automatic adversarial prompt learning to iteratively construct out-of-distribution adversarial prompts, further enhancing LLM\u2019s defense capabilities.  We conducted comprehensive experiments on three widely used jailbreak datasets, comparing our framework with six defense baselines under five representative attack scenarios. \\fan{ Specifically, for the computational efficiency of generating token-level adversarial prompts, we demonstrate both empirically and theoretically that our method achieves approximately a 15x speedup. Additionally, our methods exhibit superior defense performance against both known and unknown jailbreak attacks. Importantly, our adversarial tuning framework shows broad generalizability across various attack strategies and target LLMs (including the large 110B model), highlighting its potential as a transferable defense mechanism.",
    "keywords": [
        "jailbreak defense",
        "jialbreak attack",
        "LLM"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-15",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mrjOaRyefn",
    "pdf_link": "https://openreview.net/pdf?id=mrjOaRyefn",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes an innovative approach to enhance the security of Large Language Models (LLMs) through efficient adversarial training. Specifically, it introduces two key techniques aimed at improving the generation of adversarial prompts during the adversarial training process. The first technique involves generating token-level adversarial prompts in a hierarchical manner. Initially, a universal adversarial prompt is created, which serves as a foundation for generating individual adversarial prompts. This hierarchical approach significantly accelerates the generation process of token-level adversarial prompts. The second technique focuses on producing a broader range of out-of-distribution (OOD) adversarial prompts. This diversity is crucial for effective adversarial training, enabling the model to better defend against potential future attacks. The experimental results provided in the paper demonstrate the effectiveness of these methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper addresses an important and timely issue in the field of machine learning, particularly regarding the vulnerabilities of LLMs to adversarial attacks.\n  \n- By targeting both seen and unseen attacks, the proposed method enhances its practical applicability and effectiveness in real-world scenarios.\n\n- Figure 1 is well-crafted and effectively illustrates the concepts discussed, aiding in reader comprehension.\n\n- The authors utilize a diverse set of models, datasets, and various attack/defense methods for their evaluations, ensuring a comprehensive assessment of their approach."
            },
            "weaknesses": {
                "value": "* The term \"continuous\" in this context requires clarification. Once adversarial training concludes, the model typically enters a deployment phase without further improvements. If \"continuous\" refers to iterative training during adversarial training, many prior methods also involve multiple training rounds, which may diminish the novelty of this approach.\n\n* While addressing the speed of adversarial prompt generation is a key goal, the reliance on time-consuming token-level methods raises questions. Why not utilize more efficient generation methods, such as PAIR?\n\n* Although starting with universal adversarial prompts may expedite individual prompt generation, there is concern that this approach could reduce diversity among generated prompts, potentially compromising defense efficacy against a wider array of attacks.\n\n* The paper should elucidate the core insight behind generating diverse OOD adversarial prompts. Allowing models to autonomously generate these prompts poses challenges since models tend to resist such requests.\n\n* The results indicating that the proposed method can reduce the attack success rate (ASR) of unknown attacks to nearly zero warrant scrutiny. An explanation is needed for how such impressive performance is achieved, especially given the vastness and complexity of unknown attack spaces that cannot be fully enumerated in adversarial training phases."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a two-stage adversarial training method to enhance the defense capabilities of LLMs. The first stage focuses on defending against token-level adversarial prompts, and the second stage targets out-of-distribution adversarial prompts. Extensive experimental results demonstrate that the proposed method significantly improves defense performance against jailbreak attacks. The experiments also reveal a trade-off between model utility and robustness, and an advanced method to balance this trade-off is also proposed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The method is novel.\n* The strategy is sound.\n* Comprehensive experiments demonstrate that the method significantly improves the robustness."
            },
            "weaknesses": {
                "value": "* In addition to generalizing to various jailbreak methods, the most important aspect of training a robust LLM is balancing the trade-off between robustness and model utility. Therefore, it is better to demonstrate the performance of both model utility and robustness simultaneously and compare the proposed method to previous work.\n\n* In Section D.4, the results only show that hybrid adversarial tuning can improve model utility, while the defense performance is overlooked.\n\n* The Llama-2-7B already performs well in defending against many kinds of jailbreak attacks. It seems unconvincing to perform experiments on it. It would be better to show the performance of other models, such as Llama-3.1 and Phi-3. Nevertheless, I understand that the rebuttal time may be tight for adding these experiments. \n\n* Why does the GCG only achieve a 15.38% attack success rate (ASR) when attacking Vicuna-13B? According to recent papers, such as HarmBench, it can achieve over 80% ASR.\n\n* More related work should be discussed and compared, for instance, [1]. \n\n\n[1] Xhonneux, Sophie, et al. \"Efficient adversarial training in llms with continuous attacks.\" arXiv preprint arXiv:2405.15589 (2024)."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose adversarial tuning, a two-stage jailbreak defense for LLMs. In the first stage, the authors propose meta-universal prompt learning to efficiently generate token-level jailbreak prompts. In the second stage, the authors propose automatic adversarial prompt learning to construct out-of-distribution training data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1 This paper is easy to follow.\n\n2 The framework overview in Figure 1 makes the pipeline very clear.\n\n3 The experimental section is quite solid."
            },
            "weaknesses": {
                "value": "1 Minor errors are observed. \n\n- in Line 63, \"defe es\". \n- in Line 1042, the referred figure is broken.\n\nI would suggest authors perform proofreading very carefully.\n\n2 The proposed method requires finetuning the parameters of the models. Therefore, we are unable to validate its effectiveness in closed-source models, which undermines its practicality.\n\n3 The time analysis of the method is needed. As far as I know, the pipeline of the proposed method is not very simple. Therefore, comparison is needed to see how much time it indeed can save.\n\n4 The author aims to include a sufficient amount of content in the main text. However, in my opinion, this results in overly bulky content. In addition, this also results in very narrow gaps between different sections (especially the gap between the related work and conclusion section). In my opinion, the authors place the section transferability of adversarial fine-tuning data in the main text is unnecessary. You can place them in the Appendix section to avoid crowding.\n\n5 It seems that authors generate the adversarial sample offline instead of online. I am not sure whether it will increase the risks of models attacked by adaptive attacks."
            },
            "questions": {
                "value": "1 Although the authors compare the proposed method with some of the popular baselines in the experimental section. However, from my view, closer baselines should be those that study adversarial tuning in previous studies, for example [1] and [2]. Can the proposed method outperform those techniques on the baseline models?\n\n2 Although the authors evaluate model utility on five benchmarks. However, as far as I know, MT-bench [3] is a popular benchmark that evaluates the capability of LLMs. Can the proposed method perform well in this benchmark?\n\n[1] Fight Back against Jailbreaking via Prompt Adversarial Tuning, in NeurIPS 2024.\n\n[2] HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal, in ICML 2024.\n\n[3] Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena, in NeurIPS 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a two-stage adversarial tuning method that optimizes a safe prompt to defend LLMs against jailbreaking. The first stage \nfocuses on generating token-level adversarial prompts. The second stage automatically generates out-of-distribution adversarial prompts to generalize the safe prompt to unseen attacks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The defense considers unseen attacks (i.e. out-of-distribution adversarial prompts), which is an important and overlooked problem.\n1. Experiments demonstrate that the defense can almost eliminate jailbreaking attack success rates on multiple models."
            },
            "weaknesses": {
                "value": "1. Copy editing issues. This paper exhibits many presentation and format issues, including but not limited to:\n  - In the Abstract in the forum, the latex command `\\fan{` was not removed.\n  - Page 2, the paragraph `Semantic-Level Adversarial Prompt` was not started in a new line.\n  - Page 10, the margin of Conclusion was significantly squeezed.\n  - Page 26, the watermark is covered by the frame.\n  - Page 27, Section D.3 is empty.\n2. There are 2 existing works using adversarial training on prompts to defend against jailbreaking, but the differences between the proposed method and them are not well discussed.\n- Fight Back Against Jailbreaking via Prompt Adversarial Tuning\n- Robust prompt optimization for defending language models against jailbreaking attacks.\n\n3. There are no sufficient experiments that discuss the influence of the method on the natural performance of LLMs. Only experiments on 2 models are presented in the appendix.\n\n4. The main experiment fails to include enough comparisons with baselines. Only experiments on 2 models are compared with baselines, but Figure 7 only includes comparison with no defense, weakening the generalizability of the method on different models.\n\n5. The theoretical analysis appears to be straightforward, and it relies on overly strong assumptions."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}