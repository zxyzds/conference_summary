{
    "id": "9ngFxN83j2",
    "title": "Towards Understanding Token Selection in Self-Attention: Successes and Pitfalls in Learning Random Walks",
    "abstract": "As a key component of the transformer architecture, the self-attention mechanism is known for its capability to perform token selection, which can often significantly enhance model performance. However, when and how self-attention can be trained to perform effective token selection remains poorly understood in theory. In this paper, we study the problem of using a single self-attention layer to learn random walks on circles. We theoretically demonstrate that, after training with gradient descent, the self-attention layer can successfully learn the Markov property of the random walk, and achieve optimal next-token prediction accuracy by focusing on the correct parent token. In addition, we also study the performance of a single self-attention layer in learning relatively simpler \"deterministic walks\" \non circles. Surprisingly, in this case, our findings indicate that the self-attention model trained with gradient descent consistently yields next-token prediction accuracy no better than a random guess. This counter-intuitive observation that self-attention can learn random walks but struggles with deterministic walks reveals a potential issue in self-attention: when there are multiple highly informative tokens, self-attention may fail to properly utilize any of them.",
    "keywords": [
        "self-attention",
        "token selection"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=9ngFxN83j2",
    "pdf_link": "https://openreview.net/pdf?id=9ngFxN83j2",
    "comments": [
        {
            "summary": {
                "value": "This paper analyzes the gradient dynamics of one-layer transformer (with parameters V and W, which is self-attention pairwise logits) on predicting the next state of Markov chains in two synthetic cases (1) when the Markov chain is a random walk on a circular graph and (2) when the Markov chain is a deterministic walk (either clockwise or counter-clockwise). The conclusion is surprising: for random walk the Transformer is able to fully model the transition matrix of Markov chain (in V), and the prediction accuracy is optimal, while for deterministic walk, the prediction is random and V converges to all 1 matrix (Theorem 3.2). The paper also performs experiments to justify the results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper performs rigorous mathematical study to analyze gradient dynamics in the specific Markov cases. \n2. The paper is relatively well-written."
            },
            "weaknesses": {
                "value": "1. I am a bit skeptical about whether Theorem 3.2 is empirically meaningful. The initial condition in the theoretical analysis is W = V = 0, which may lead to the all 1 matrix V in Theorem 3.2. What if the symmetry is broken and there is some small initial noise of W and V? In that case, will the transformer converge to something similar to Theorem 3.1? I checked the appendix and it looks like the Theorem 3.2 is indeed due to perfect symmetry in attention scores (Lemma C.3) and the gradient of V (Lemma C.2). Note that Task 1 already has noise in its input/output relationship, which Task 2 does not have. This may lead to sharp contrast between Theorem 3.1 and 3.2, which is not an issue empirically. \n\nIf Theorem 3.2 is purely because the symmetry initialization, then it would hurt the generalization of the main conclusion. If theoretical study is non-trivial with random initialization, authors can also use experiments to demonstrate that the conclusion still holds with small random initialization. \n\n2. How does this analysis extend to more general cases? Can it handle more general Markov chains? What if there is FFN and nonlinearity on top of the attention layer?"
            },
            "questions": {
                "value": "1. Could the authors investigate how small random initializations of W and V affect the convergence behavior described in Theorem 3.2? This would help clarify whether the observed behavior is solely due to the symmetric initialization or if it persists under more realistic conditions.\n\n2. Could the authors comment on how their analysis might extend to more complex Markov chains, such as those with large and compositional state/action spaces, or non-uniform transition probabilities? Additionally, it would be helpful if they could discuss the potential impact of adding nonlinearities or feed-forward layers to the transformer architecture on their theoretical results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates the ability of the self-attention mechanism in transformers to perform token selection by examining its performance in learning two types of sequential data: random walks vs deterministic walks on circles. The authors theoretically demonstrate that a single-layer self-attention mechanism can successfully learn the transition probabilities of random walks but fails to predict deterministic walks. This contrast reveals a limitation of self-attention when dealing with multiple equally informative tokens."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper provides a theoretically rigorous examination of token selection by self-attention in transformers. However, analysis is limited to a very specific implementation in a simplified model with a single attention layer.\n- If confirmed in more complex architectures and outside of toy problems, the results could be important."
            },
            "weaknesses": {
                "value": "- The paper's focus on one-layer transformers and a single overly simple toy task limits its generalizability to more complex and realistic scenarios involving deep transformers. It is not clear whether the effect observed would arise in more complex settings.\n- The initialization of all weights to zero seems to be the main cause for the problem, since it would break the symmetry in the initial softmax weighting (as per step 1 of \"training dynamics in learning deterministic walks\".\n    Is there a particular reason why weights were initialized to zero?\n\n- A more thorough validation with a wider range of token representations and architectures is required to support the conclusions of the paper.\n\n- Minor: positional embeddings were concatenated here, but in practical applications they are typically added element-wise; I am not sure if this would have an impact on the observed phenomenon."
            },
            "questions": {
                "value": "See above in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The current work introduces two case studies that highlight a rather counter-intuitive phenomena \u2014 that transformer models can effectively learn to predict the next token in a simple random walk task along a graph, but fail to learn this task when the walk is deterministic. The authors put forward theoretical reasons as to why this is the case and proceed to empirically show that their theory holds. The theoretical argument is elegant and makes an initially puzzling finding intuitively obvious, which I really liked."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is overall well written and easy to follow (except for maybe Section 2.1, more later).\n2. The authors give a theoretical account of the phenomena they are studying, complete with proofs supporting their point (though I only skimmed them, so cannot vouch for their correctness). Additionally, they give an intuitive, non-formal argument for why transformes succeed and fail in their test cases, which I find convincing.\n3. The authors support their conclusion with empirical tests, though I think they could \u2014 and should do \u2014 more (see below).\n4. The previous literature is well surveyed, though I admit that is not my primary area of expertise, so I am unlikely to know if there is anything missing."
            },
            "weaknesses": {
                "value": "There are two main issues in my opinion with this article:\n\n1. The insight the authors gain from their test cases are interesting, but it their proof is based on a particular assumption \u2014 initialisation of the attention matrices to zero. How often does this happen in practice though? I understand that other assumptions such as aggregating key and query matrices into a single one to facilitate their analysis, but the other one seems a bit artificial. While this doesn\u2019t invalidate the insight, it would be good to either see: \n    1. Experimental evidence for what happens if they use standard initialisations\n    2. A good reason as to why this is not needed beyond \u201cit makes our analysis easier\u201d.\n2. Related to the above, while the work is well situated within it\u2019s literature, I am not convinced on how this is \u201cinteresting\u201d. To what does this novel insight apply? Is there a specific domain or task where the authors believe this is a problem? If it is, have other researchers proposed a solution, in which case this work would be an explanation for those issues (which is good in my opinion)?"
            },
            "questions": {
                "value": "My questions are embedded within the weaknesses.\n\n246 - \u201cseries\u201d instead of \u201cserious\u201d\n\nThere might be more, I am terrible at spotting these.\n\nAs it stands, I recommend reject. But I think the issues are fixable, mostly by addressing the concerns outlined above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an intriguing observation about self-attention mechanisms. It finds that self-attention excels at learning the Markov property of random walks but struggles with deterministic walks, which are much simpler. This suggests that if all tokens contain similar informative features, self-attention may fail to learn them effectively. The paper also provides robust experiments to support this observation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "+ The paper is well-written and easy to follow.\n+ The observation is intriguing and sheds light on training strategies and feature selection for transformer-based models.\n+ It provides a solid theoretical analysis.\n+ The experiments effectively support the findings."
            },
            "weaknesses": {
                "value": "- Lacks experiments with real-world data.\n- Only tests on single-layer self-attention. It would be beneficial to scale up to multi-layer and multi-head self-attention to explore the results further."
            },
            "questions": {
                "value": "According to the weaknesses. \n\nSimilar to a single-layer neural network, where a naive perceptron struggles as a good learner, a deep neural network with multiple linear layers can fit various functions and tasks. Given the prevalence of large language models (LLMs) today, and the shift in focus towards them, I wonder if scaling up self-attention\u2014whether in width (multi-heads) or depth (number of layers)\u2014would still present the same issues identified in this study?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}