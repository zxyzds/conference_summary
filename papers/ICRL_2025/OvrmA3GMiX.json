{
    "id": "OvrmA3GMiX",
    "title": "Learning Transferable Sub-goals by Hypothesizing Generalizing Features",
    "abstract": "Although transfer is a key promise of hierarchical reinforcement learning, current methods discover nontransferable skills.\nTypically, skills are defined over all state features simultaneously, preventing generalization as some state features reliably support generalization while others do not.\nFor an agent to effectively transfer a skill it must identify features that generalize and define the skill over this subset.\nHowever, this task is under-specified as the agent has no prior knowledge of what future tasks may be introduced.\nSince successful transfer requires a skill to reliably achieve a sub-goal from different states, we focus our attention on ensuring sub-goals are represented in a transferable way. \nFor each sub-goal, we train an ensemble of classifiers while explicitly incentivizing them to use minimally overlapping features.\nEach ensemble member represents a unique hypothesis about the transferable features of a sub-goal that the agent can use to learn a skill in previously unseen portions of the environment.\nEnvironment reward then determines which hypothesis is most transferable for the given task, based on the intuition that transferable sub-goals lead to better reward maximization.\nWe apply these reusable sub-goals to MiniGrid and Montezuma's Revenge, allowing us to relearn previously defined skills in unseen parts of the state-space.",
    "keywords": [
        "hierarchical reinforcement learning"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We use a diverse ensemble to generalize a given sub-goal to relearn previously discovered skills.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=OvrmA3GMiX",
    "pdf_link": "https://openreview.net/pdf?id=OvrmA3GMiX",
    "comments": [
        {
            "comment": {
                "value": "Thank you for your review. We are glad to see that you deeply understood our paper. We plan to revise the paper with a mathematical definition of a hypothesis as well as clarify some of our experiment decisions.\n\n> I am very interested in your formulation of \u201chypothesis\u201d but at the same time was disappointed for not seeing enough explanation or theorem that covers your idea. How did you exactly formulate \u201chypothesis\u201d in your code? Can you provide any theorem and/or math notations that support your formulation?\n\nWe plan to update the paper with a formal definition. Given a dataset *D* we can define a space of functions *f* that accurately fit the data *D*. Our hypotheses are sampled from this space which is very large as there are many ways to fit a finite collection of points. Because a neural network is approximating a function, we represent each hypothesis as an individual neural network.\n\n>  In addition, no pseudocode was provided nor any clear statements about the connection with The Diversity-By-disAgreement Training (D-BAT) algorithm. More discussion on these areas would be much appreciated.\n\nD-BAT uses unlabeled data to sample a set of diverse functions from our hypothesis space. This is done by requiring that each function disagree on a set of unlabeled data. This disagreement ensures each hypothesis is varied in a significant way while maintaining consistency with the labelled dataset which is why we chose to leverage D-BAT.\n\n> In what respect does your use of representation learning differ from using dimension reduction? \n\nWhen performing dimensionality reduction you are always discarding information. A good reduction would remove irrelevant information while a bad reduction would also discard pertinent information. If you have a single subgoal to generalize from, it is not clear what features should be removed and which should be kept. By learning a diverse ensemble, each ensemble member discards different parts of information and so is performing a kind of dimensionality reduction.\n\n> How do you exactly provide the high-level policy?\n\nOur high-level policy is a PPO algorithm that receives the full state and must select an action, each of which is a low-level policy that is tasked with satisfying a single subgoal from the ensemble of classifiers. In the MiniGrid experiment, we started with 5 options and used a 3 head ensemble for each. The high-level policy had an action space of 15 actions, one action for each ensemble member for all 5 ensembles.\n\n> In Figure 3, you mentioned that \u201cthis performance is on a collected set of data and does not fully encompass all the possible states an agent may encounter during policy learning.\u201d How would the result differ from not using a collected set of data or any suggestions to overcome this?\n\nIn the Montezuma\u2019sRevenge experiment we use a state composed of a stack of the previous 4 frames. As a result our state is now a factor of the previous 4 actions, atari has 18 possible actions, so there are a large number of possible states. We used expert collected trajectories for our data and so did not cover all possible states for every ClimbDownLadder subgoal. If this data was coming from an existing option, it is likely that there would be larger variation in these states as the policy executes random actions during its initial training. However, this cannot be guaranteed if we are allowing the agent to autonomously discover and train different skills and subgoals.\n\n> Why did you use the PPO and DQN agents to compare with? Similarly, why did you choose the CNN classifiers to compare with? A bit more discussion on these choices would be helpful.\n\nWe agree that more discussion on these choices is needed and will be adding our motivations in the paper revision. Our ensemble agent uses DQN for the lower-level option policies and PPO for the higher-level policy. Our motivation for testing against these algorithms was to definitively show that the performance of the ensemble agent was not due to the power of these deep RL methods but instead our transferred subgoals.\n\nFor the Montezuma\u2019sRevenge experiments we choose to test against a CNN because this is traditionally how subgoals would be defined and so clearly shows how our method performs against current techniques as well as highlighting the difficulties for current CNN based methods.\n\n\nWe would again like to thank the reviewer for their thoughtful review and will address their concerns in our revision. We are happy to address any further questions or concerns."
            }
        },
        {
            "comment": {
                "value": "> For MiniGrid again, what are the standard techniques used to solve these tasks? If not DDQN or PPO, why did you select them? Sounds like this is a hard exploration problem, so RND of DCEO (Klissarov et al., 2023) could be used instead. If it is them, how did you select their hyperparameters?\n\nWe did not properly explain our selection of comparisons in the paper and plan to rectify this. Our ensemble agent uses DQN networks for the lower level policies and PPO for the higher level policy. We compare against pure DQN and PPO with the same hyperparameters because we want to make it clear that our ensemble agent is able to learn, not because of the power of these deep RL methods, but because of the use of our subgoals. Keep in mind that the ensemble agent is provided 15 options, of which only a handful may be useful, so we wished to determine if this large number of ill-fitting options would harm our agents.\n\n> how do you explain that the CNN has high accuracy and yet leads to bad option policies? To me, this means that there is some problem with your accuracy calculation that does not capture transferability.\n\nOur data is collected from a handful of expert trajectories. For Montezuma\u2019sRevenge the input is a stack of the 4 previous frames. As such our state is dependent on our previous 4 actions, which considering atari has a total of 18 actions, may result in a large number of possible states for a single room and are not fully captured in our expert collected trajectories. During policy training, as the agents start by executing random actions, they are likely to encounter previously unseen states. This result shows that the CNN struggles to generalize to rooms it has seen if there are unique combinations of actions being executed which may not have been detected if testing is not done on every possible state.\n\n> How exactly did you pick the selected data for the training of the classifiers? More explicitly, what type of procedure did you follow?\n\nWe used a handful (2-5) of expert trajectories designed to move the agent around the room but do not fully encompass the entire state space of the tasks.\n\n> The paper claims that the rightmost image in Figure 5 is evidence of the option generalizing the notion of going down the ladder, but it shows the opposite to me. It shows that the agent only learned to go down a few steps, a similar or smaller number of steps than when it goes down in the original shorter stairs. Am I interpreting erroneously the dots being displayed?\n\nFirst, we will better contextualize the images in the revised paper. The agent starts at the top of the ladder shown in the middle image. If the agent travels left it reaches the room depicted in the left image. If the agent travels down the ladder, it reaches the room to the right. It is true that the agent will erroneously terminate after a few steps down the ladder in some cases. However, all these points are taken from the same policy and subgoal classifier which is also able to travel past these erroneous states to the true termination depicted on the right. This same policy is also able to travel to the left most room to another true termination.These points are the position of the agent in the space of the game but do not fully describe the entire state of the agent, which is a function of the four previous frames, so it is possible for the same policy to travel the entire length of the ladder and erroneously terminate after a few steps depending on what state the agent is in. An interesting point to note is that the CNN is never able to get past the first ladder into the right-most room with the true termination.\n\nWe will clarify the presentation to accurately convey the goal of our work and the setup of the experiments. We will inform you when these changes have been completed.\n\nThank you again for your time and review and we look forward to any additional questions you may have."
            }
        },
        {
            "comment": {
                "value": "> What happens when the agent is not placed on top? More generally, what happens in other tasks where there is no control over this initial state?\n\nIn the Montezuma\u2019sRevenge experiment, an option execution is considered a success if any ladder bottom is reached. This means that the agent may not have climbed down a ladder to reach the base of a ladder, but could instead walk to an adjoining room that contains a ladder base. Additionally, in the case of the Minigrid DoorKey experiment the task had no control over the initial state. Each option could be executed at any point as long as the state did not already satisfy the subgoal. In these cases the policy learns to satisfy the subgoal, if possible.\n\n> There is no evidence of transfer under different reward functions or dynamics\n\nOur option policies are trained only to satisfy the corresponding subgoal classifier. As such we do not rely on the reward function. Each policy is trained from a random initialisation and so if we attempt to generalize to a subgoal in an environment with different dynamics our option policy will train as normal.\n\n> For MiniGrid, the option GoToGoal arguably already encodes a lot of information about the original reward function.\n\nThis is true, however, given that the door is always red in our test environments, the CollectBlueKey and CollectGreenKey do not correlate to the environment reward.\n\n> The paper makes no comparison with any other hierarchical method. Is there any advantage to the proposed approach?\n\nWe do not compare to subgoal discovery methods because our method is intended to work alongside these methods. We do not compare to existing skill generalization work as all the works we have found either require a library of prelearnt skills or a distribution of tasks from which to sample. Since our aim is to generalize from one subgoal these comparisons would be unfair.\n\n> The paper makes no empirical attempt to prove that the reason why any of the papers mentioned in the Background and Related section fail is because of the poor state generalization.\n\nWe previously mentioned why existing skill generalization work differs from our research. Our classifier experiments compare against a CNN which is traditionally how a subgoal classifier would be represented in subgoal discovery methods. In Figure 3 and 4 we show a CNN struggles to generalize from a single option instance.\n\n> How are the sub-goals being used to train the option policies? Do they define a sparse reward that is equal to 0 unless the goal set is reached?\n\nWe will clarify, in a revision to the paper, how these subgoals are used during policy training. Option policies are given a reward of 1 if the subgoal classifier returns true and 0 otherwise.\n\n> How can you guarantee that learning this is not just as difficult as solving the original task?\n\nThere is no guarantee. Additionally it is possible that some ensemble members will learn subgoals that can never be met by the agent. In these cases the option does not correlate well to the overall task and so the hierarchical agent will stop selecting these options as they result in low environment reward.\n\n> For MiniGrid, did you try learning a high level policy with the options that were used to train the ensemble classifiers? If so, how does it compare to the one using the classifiers?\n\nWe have trained an agent with perfect subgoal classifiers (note this agent has only 5 options as opposed to the 15 of the ensemble agent). This agent is able to learn the task faster than our ensemble agent, which is expected as there are fewer lower level policies to train.\n\n> For MiniGrid, in total how many interactions did it take to train the original 5 options and then the 15 option policies corresponding to the ensembles? What happens if you run DDQN or PPO for that many steps?\n\nThe \u201cinitial 5 options\u201d are described only as subgoals, no policies were trained in the original seed (seed=0). During testing the agent was not provided pretrained options and so was required to train the lower level policy in conjunction with the higher level policy which is the 1.5 million steps reported."
            }
        },
        {
            "comment": {
                "value": "Thank you for critical and insightful comments that expose weakness in our communication. It has been clearly stated that \n\n> The goal is to provide a scalable method to learn transferable options, and the claims are that no HRL method does this\n\nWe agree that we have not achieved these goals. However, this was not the goal of our research, which we will make clear in our revision of the paper. Rather, our goal is to generalize subgoals from one previously given option instance. We agree that we do not perform goal discovery (c and f) because we intend to work with existing option discovery methods, which identify single, unique subgoals which are not reusable outside their discovering instance. This is a result of two problems a) the subgoal is defined by a collection of states, labeled as either inside or outside the subgoal which can only be sampled from the initial discovered subgoal leading to a very small dataset with very little diversity/coverage, and b) this dataset of samples does not provide the ability to discern features important for the subgoal as this is fundamentally underspecified (we argue this point with an illustrating example in section 3.2).\n\nThe review correctly points out existing works that tackle skill generalization. These useful references will be added to our Learning transferable skills related works section. The works mentioned (a,b,d,e,g) as well as the works already referenced, approach skill generalization retroactively; these works either require large libraries of skills that can be compressed or changed to form generalized skills (b) or assume sampling from a distribution of tasks which implicitly assumes an expert has curated a collection that encompasses what the agent must learn (a,d,e,g). Our goal is instead to approach skill generalization in the *forward* direction, i.e. if we are given one example of the skill (which is what most subgoal discovery algorithms provide) how do we generalize this skill to future tasks which are likely in never-before seen states? If we wish to make use of the rich body of work on subgoal discovery, it is important to be able to generalize these skills from the single instance in which they are discovered by these algorithms. \n\nThe representation learning work again differs from our scenario as it uses full episode trajectories to learn representations that allow for better learning by removing irrelevant features. However, from one example of a subgoal it is not possible to decide which features are truly irrelevant. Especially when considering the fact that what may be irrelevant to an agent (such as the color of an object during object manipulation) may not be irrelevant to a subgoal classifier. Thank you for bringing these works to our attention and will add them to our related works section.\n\n> There is no mechanism to discover the goals. The images used to train DBAT were hand-picked. How can this process work in more general settings?\n\nWe hand-picked subgoals since our aim is to evaluate how well our method can generalize which requires knowing what our ensemble must generalize to. However the data used to train the D-BAT classifiers are what would typically be generated by an off-the-shelf skill discovery algorithm.\n\n> It is necessary to train preliminary options to generate rich enough data to train the ensemble classifiers. Where do these options come from?\n\nSince we intend to work in tandem with existing subgoal discovery methods, these preliminary options can come from any subgoal discovery method that generates subgoals.\n\n> How can we guarantee that they will generate rich enough data?\n\nThe core problem is that we cannot guarantee this. In fact, we can almost always guarantee the opposite: since we are gaining data from a small collection of states, it is not rich enough to effectively train a traditional classifier to generalize. This is why we leverage multiple diverse classifiers which each attend to separate features from the provided data.\n\n> Having those options already, why bother to use a classifier to learn options that carry out the same task as the original options?\n\nThese subgoals all represent options that we want the agent to continue to use. We define these subgoals only for seed=0, a single possible Minigrid DoorKey configuration. If we change the seed however, we want the agent to continue to use these options.\n\n> The way in which the option policies are obtained from the classifiers is unclear and possibly does not scale to more general settings\n\nWe learn a policy for each ensemble member. The policy is given a reward of 1 if the relevant ensemble member returns 1 and 0 otherwise."
            }
        },
        {
            "comment": {
                "value": "Thank you for your thoughtful review. We agree that our presentation can be improved. In particular, we should first clarify that our work is not a new method for skill discovery. It is a method to generalize subgoals and so should be used in tandem with a skill discovery method.\n> I don't think that three seeds per curve is sufficient to reasonably compare the methods\n\nWe are actively running more seeds and will update all graphs with additional seeds.\n\n> can you explain the baseline how the accuracy is calculated in section 4.1?\n\nFor evaluation, we collected data from across Montezuma\u2019sRevenge and labeled which states lie within and outside the subgoal. We calculate our accuracy against these expert labeled samples. \n\n> I'm also not sure I agree with the claim that the performance when only one ladder instance has been seen is the most important given how low the accuracy is for all three models at that point compared to the two-ladder case.\n\nThe goal of our work is to generalize a subgoal from a single example, which is the expected output of an off-the-shelf subgoal discovery algorithm. As such, illustrating how well our method performs for the one-ladder case indicates how well the method works if used along-side a subgoal discovery method. Although 70% accuracy may appear to be low at first glance, especially when compared to the near 100% accuracy achieved after two input examples, it means that we detect previously unseen states that fall inside our ClimbDownLadder subgoal\u2014each detection means a new skill need not be discovered from scratch, but can simply be instantiated as a generalization.\n\n> This environment is not so large that DQN/PPO should be unable to succeed when trained over 1.5 million steps, so it makes me think there is an error in the evaluation.\n\nThe failure of the baselines is due to the episode length of 500 steps, which makes it a more challenging exploration problem. If we increase the episode length to 1000 steps, both DQN and PPO solve the task. We chose these baselines because our hierarchical agent uses  DQN for its lower level policies and PPO is used for the higher-level policy. Our intention is to convincingly show that our agent learned the task not because of the power of these deep RL methods but because of the subgoals provided by our ensembles. We will clarify this argument in the revision to our paper.\n\n> can you better explain the training process for the MiniGrid environment in section 4.3?\n\nWe collect labeled data from a single seed (seed=0) and unlabeled data is collected from seeds 1 and 2. We then evaluate the agent in unseen seeds (10, 11, 12). When training starts, the ensembles have seen only data from seeds 0, 1 and 2. The additional states seen in the testing seeds are not shared across the ensemble agents. This was not clearly communicated in the current paper.  We can address this completely with a careful experiment description, in revisions we will implement.\n\n> In Figure 6, why is the standard error shown instead of standard deviation?\n\nStandard error is standard deviation divided by the square root of the number of runs; this is a standard metric, but we can report standard deviation if you think that will substantially increase the quality of the paper.\n\nThank you again for your thoughtful review. The points you raise will be reflected in our revisions and we welcome any additional questions."
            }
        },
        {
            "comment": {
                "value": "Thank you for  your comments which expose a weakness in our presentation which we will address. Specifically, the goal of the paper could be misconstrued as applying D-BAT to the hierarchical reinforcement learning setting. This would indeed limit the novelty significantly to the D-BAT method which would not justify publication. We want to emphasize that this is not the main goal of our work. Current subgoal discovery methods (which are the vast majority of skill discovery methods) identify subgoals which are considered standalone and unique. Current skill generalization methods take a retroactive approach, by either assuming a large library of learnt skills which are then compressed into a small collection of generalized skills or assuming the agent has access to a distribution of tasks to sample from during training. This latter case implicitly assumes an expert has informed the agent of what skills it must have. This retroactive view means current skill generalization techniques are not compatible with the rich collection of skill discovery algorithms the community has developed.\n\nWe take a forward looking approach, generalizing a unique subgoal, which can be provided by any subgoal-based skill discovery method, to unseen states. This leaves two main problems 1) we have a subgoal described using  a small labeled dataset, but with very low diversity/coverage, and 2) based purely on one instance, the true intention of the subgoal is unclear (we argue this with an illustrating example in section 3.2). To overcome these challenges, we propose learning an ensemble of classifiers, each achieving high accuracy on the training dataset, but attending to separate features of the data. We make use of D-BAT because it is the only method we found capable of creating multiple classifiers that vary meaningfully on out-of-distribution data with their use of unlabeled data. So, while D-BAT can be regarded as a robustness algorithm, we are not using it to increase robustness of a provided subgoal, but instead to create a collection of subgoals based on a sufficiently descriptive subset of the data. As such, our method is not tied to the D-BAT method and any algorithm that produces a set of diverse classifiers can replace D-BAT.\n\nEach experiment was carefully selected. For our subgoal classifier evaluation, we compare to a CNN because this is traditionally how subgoals are represented in the vast majority of GCRL and HRL works.  In the MiniGrid-DoorKey experiment, we show two things: (1) giving the agent skills that are not aligned with the overall task will not irreparably damage policy learning, and (2) the strong performance of our ensemble agent cannot be explained by our use of DQN (for learning the policy over options) and PPO (for learning low-level option policies).\n\nAs the review report correctly indicates, this is not communicated clearly in the paper. We did not compare to state-of-the-art skill discovery methods because our methodology can be applied with any subgoal based skill discovery method. We did not compare against any skill generalization techniques because these methods either assume prior knowledge of the collection of tasks the skill must generalize to or they assume a large library of prior learnt skills. These would therefore not be a fair comparison.\n\nWe will improve the current draft of the paper by a) removing the overgeneralizations of HRL b) better communicating the goal of our work and our algorithm c) better explaining the selection of comparisons in our experiments. \n\nThank you again for a thoughtful review, we are happy to address any additional concerns."
            }
        },
        {
            "summary": {
                "value": "Learn a collection of hypotheses which take in varying collections of features using D-BAT, a method which . Then identify the hypothesis that has the best transferability. Relies on labeled training data for subgoals. Then give the agent the transferable subgoals."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The problem of robust skill generalization is necessary.\n\nThe motivation for the problem is clear"
            },
            "weaknesses": {
                "value": "This work makes several overgeneralizing claims about the positioning of HRL. Some examples include (line 36) that the options based framework utilizes subgoals (the termination condition does not need to be goal based). The termination condition must be a set (line 80), when it is often formulated as a probabilistic condition function. The equivalence between termination sets and subgoals (line 83), since a set might contain many subgoals. The use D-BAT to claim generalizability (line 162): While there may be some amount of robustness added, there is an implicit assumption that by learning a robust classifier of subgoals, this implies that the same assumptions can be applied to goals in RL, which is both a different context and not necessarily true. This point would need to be proven theoretically and the empirical results would need to support this claim more directly.\n\nThe most glaring weakness is the level of imprecision realted to the method itself. In particular, it is not clear what the algorithm actually is. It appears to be 1) run D-BAT to get some features. 2) learn an option to achieve good hypothesis classification. However, it is not made clear what the inputs are for D-BAT, the reward function for the skills, the hierarchy, or almost any other detail of the algorithm.\n\nThe experimental results lack several components. First, the baselines compared are deep RL algorithms, not state of the art HRL algorithms. Second, neither method uses factorization or exploration. Third, the main paper lacks even a complete coverage of the tasks, since downstream performance is only evaluated in one task.\n\nFinally, the work is entirely not self contained, since it relies heavily on D-BAT, without actually providing an adaquate formalism to describe D-BAT. Instead, the reader is expected to read this work. Furthermore, it is not obvious why this particular hypothesis algorithm is chosen over others, nor are there any ablations to indicate that D-BAT is preferable to other robust hypothesis algorithms---only comparison to a CNN."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a new method for skill discovery in hierarchical reinforcement learning which is designed to find skills which can be effectively transferred between tasks.\nIn order to do this, the authors focus on learning sub-goals with representations that facilitate this transfer by training ensembles of diverse classifiers for use as sub-task termination predicates.\nThe authors detail several experiments which evaluate different aspects of their method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper addresses the important topic of learning transferrable skills for hierarchical RL and is generally well-written.\n\nThe method presented is a novel and interesting application of a diversity-based classification method to skill discovery.\n\nSeveral experiments are included which are each intended to answer a different useful question."
            },
            "weaknesses": {
                "value": "Some parts of the experiments are unclear and overall the presented results are not convincing. Specific points follow.\n\nFor the quantitative results (Figures 3, 4, 5), I don't think that three seeds per curve is sufficient to reasonably compare the methods. While there isn't a definitive way to determine how many are needed, I would expect to see at least five to assess the variance due to random model initialization when the environment is deterministic.\n\nIn section 4.1, it is not clear what the ensembles are being compared against to determine the accuracy. I'm also not sure I agree with the claim that the performance when only one ladder instance has been seen is the most important given how low the accuracy is for all three models at that point compared to the two-ladder case.\n\nFor the MiniGrid results in Section 4.3, I am concerned about the lack of success with DQN and PPO. This environment is not so large that DQN/PPO should be unable to succeed when trained over 1.5 million steps, so it makes me think there is an error in the evaluation. Based on the hyperparameters in the appendix, one potential major factor for DQN is a lack of exploration.\n\nThe training process in Section 4.3 is not clear. It seems to state that data is collected from the different seed runs and used together to train the classifier ensembles, but this doesn't make sense with results being aggregated across the three seeds."
            },
            "questions": {
                "value": "As mentioned above, can you explain the baseline how the accuracy is calculated in section 4.1?\n\nAs mentioned above, can you better explain the training process for the MiniGrid environment in section 4.3?\n\nIn Figure 6, why is the standard error shown instead of standard deviation?\n\n$ $\n\nMinor Notes:\n\nLine 320: I believe this is meant to be a reference to Figure 4 rather than Figure 5.\n\nManhattan is misspelled several times."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper argues for the use of an existing technique called DBAT to learn ensembles of state binary classifiers in the context of Reinforcement Learning (RL). The use of each classifier is to encode a goal, understood as a set of states, which can be used to train an option policy. Relying on the motivation for DBAT, the paper explains and motivates empirically the use of an ensemble of classifiers, rather than a single one. In particular, they show that the ensemble classifiers do generalize to unseen states and that option policies can indeed be learned from them in the environment Montezuma's Revenge, while this is not the case for single classifiers (referred to as CNNs in the text). Moreover, they show how a hierarchical use of the options learned from   an ensemble of classifiers can solve a MiniGrid task where standard end-to-end deep RL algorithms fail."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Originality (and significance):\n- The paper highlights an important and typically overlooked aspect of the problem of learning transferable state and goal representations in RL: it is not possible to tell what information is useful to transfer and for this reason multiple hypothesis, or a similar flexible representation, should be used, as opposed to a single standard architecture that tries to identify \"the best\" features.\n- The paper introduces the use of a non-standard method to learn state classifiers in RL.\n\nQuality:\n- The paper provides multiple pieces of empirical evidence (three, to be exact) to demonstrate that the chosen technique, DBAT, is useful to encode goals, or, more generally, sets of states. \n\nClarity:\n- The organization of the paper is good. The problem and its motivation are well motivated and the exposition of the proposed solution follows a logical order.\n- The Results section contains an explicit list of questions to be answered and it does attempt to explicitly answer each and every one of them."
            },
            "weaknesses": {
                "value": "The Introduction states that \"To fully realize the benefits of HRL, learned options should be transferable. (...) existing HRL methods are unable to generalize an option from one context to another. This is primarily because all three components of the option are conditioned on the entire state, which includes spurious features unnecessary for successful execution.\" From this, I identify a main goal and two claims. The goal is to provide a scalable method to learn transferable options, and the claims are that no HRL method does this, and that the reason for why they fail is that, whatever methods are being used to learn option policies, they result in architectures that rely on spurious correlations to components of the input sensory stream. In my opinion, the goal is not achieved and the claims are not supported.\n\nWhy is the main goal not achieved?\n1. There is no mechanism to discover the goals. The images used to train DBAT were hand-picked. How can this process work in more general settings?\n2. It is necessary to train preliminary options to generate rich enough data to train the ensemble classifiers. Where do these options come from? How can we guarantee that they will generate rich enough data? In this case they were clearly well chosen, e.g., the MiniGrid environment case contains the three option policies that are required to complete the general task (CollectRedKey,\nOpenRedDoor, and GoToGoal). This begs the additional question. Having those options already, why bother to use a classifier to learn options that carry out the same task as the original options?\n3. The way in which the option policies are obtained from the classifiers is unclear and possibly does not scale to more general settings. In particular, for the Montezuma's Revenge environment, the manuscript mentions that an agent is manually placed at the top of some stairs and then it is supposed to learn to go down the stairs. What happens when the agent is not placed on top? More generally, what happens in other tasks where there is no control over this initial state?\n4. There is no evidence of transfer under different reward functions or dynamics. For MiniGrid, the option GoToGoal arguably already encodes a lot of information about the original reward function.\n\nWhy are the claims not supported?\n1. The paper ignores relevant literature in reward-agnostic option discovery, representation learning for RL, and multi-task reinforcement learning that partially solve the main goal addressed in the paper. For example:\\\n   a. Agarwal R., et al., Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning, ICLR, 2021.\\\n   b. Barreto A., et al., Combining Skills in Reinforcement Learning, NeurIPS, 2019.\\\n   c. Eysenbach B. et al., Diversity is All You Need: Learning Skills without a Reward Function, ICLR, 2019.\\\n   d. Frans K., et al. Meta Learning Shared Hierarchies, ICLR, 2018.\\\n   e. Gomez D., et al. Information Optimization and Transferable State Abstractions in Deep Reinforcement Learning, IEEE TPAMI, 2022.\\\n   f. Klissarov, M. et al., Deep Laplacian-Based Options for Temporally-Extended Exploration, ICML, 2023.\\\n   g. Touati, A. et al., Does Zero-Shot Reinforcement Learning Exist?, ICLR, 2023.\\\n   h. Zhang A., et al., Learning invariant representations for reinforcement learning without reconstruction, ICLR, 2021.\\\nMore specifically, a., e., and h. provide techniques to learn transferable representations, similar to the proposed ensemble of classifiers; b., c., d., and f. introduce techniques to learn transferable skills, and g., provides a global policy that solves any given task provided access to its reward function.\n2. The paper makes no comparison with any other hierarchical method. Is there any advantage to the proposed approach?\n3. The paper makes no empirical attempt to prove that the reason why any of the papers mentioned in the Background and Related section fail is because of the poor state generalization."
            },
            "questions": {
                "value": "- How are the sub-goals being used to train the option policies? Do they define a sparse reward that is equal to 0 unless the goal set is reached? How can you guarantee that learning this is not just as difficult as solving the original task?\n- For MiniGrid, did you try learning a high level policy with the options that were used to train the ensemble classifiers? If so, how does it compare to the one using the classifiers?\n- For MiniGrid, in total how many interactions did it take to train the original 5 options and then the 15 option policies corresponding to the ensembles? What happens if you run DDQN or PPO for that many steps? \n- For MiniGrid again, what are the standard techniques used to solve these tasks? If not DDQN or PPO, why did you select them? Sounds like this is a hard exploration problem, so RND of DCEO (Klissarov et al., 2023) could be used instead. If it is them, how did you select their hyperparameters? \n- What is the false positive rate of the CNN? If it is small, how do you explain that the CNN has high accuracy and yet leads to bad option policies? To me, this means that there is some problem with your accuracy calculation that does not capture transferability.\n- How exactly did you pick the selected data for the training of the classifiers? More explicitly, what type of procedure did you follow? It sounds cumbersome separating all the images generated from the interaction with the environment.\n- The paper claims that the rightmost image in Figure 5 is evidence of the option generalizing the notion of going down the ladder, but it shows the opposite to me. It shows that the agent only learned to go down a few steps, a similar or smaller number of steps than when it goes down in the original shorter stairs. Am I interpreting erroneously the dots being displayed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies transferable skills and sub-goal generalization in the hierarchical reinforcement learning framework; the authors propose that the RL agent maintain several, diverse hypotheses over which features of the state might generalize in the future, focusing their attention on ensuring sub-goals are represented in a transferable way. The agent then selects from among these hypotheses, tests one of them in the environment, and updates its beliefs over which member of the ensemble has learned transferable features. Their work focuses on generalizing forward from a single option instance instead of retroactively compressing previously learned skills. Their work differs from previous related research by learning multiple hypotheses, each of which is a unique state representation. They apply these reusable sub-goals and perform an empirical study to MINIGRID and MONTEZUMA\u2019S REVENGE environments, allowing agents to relearn previously defined skills in unseen parts of the state-space. Lastly, the authors seek several essential questions and tend to look for answers with their experiments and to test a consolidated RL agent on a challenging sparse-reward problem. \n.\n\nThis paper is a magnificent algorithmic contribution to the hierarchical RL and reinforcement learning community in general and I strongly accept it as it is considered to be an authentic contribution to the ICLR conference.\n\nThe main contributions are the following:\n- Forming each ensemble member as a unique hypothesis about the transferable features of a sub-goal that the agent can use to learn a skill in previously unseen portions of the environment.\n- In contrast to previous methods that focus on the initial discovery of sub-goals their work focuses on transferring an existing subgoal and so can be used in tandem with any of these methods. \n- Presenting sub-goals in a transferable way and empirically performing sub-goal generalization."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "This paper does a great job of presenting novel authentic contributions. For originality, the paper tackles an important problem: How can learned features be transferable in HRL? The authors propose a very revolutionary concept;\u201chypothesis\u201d to learn about the transferable features of a sub-goal that the agent can then use to learn a skill in previously unseen portions of the environment. Furthermore, their work builds upon and intersects with different methods: 1. Identifying sub-goals by focusing on transferring an existing subgoal and so can be used in tandem with any of the existing methods. 2. Learning Transferable Skills by generalizing forward from a single option instance instead of retroactively compressing previously learned skills. 3. Unsupervised Representation Learning by learning multiple\nhypotheses, each of which is a unique state representation. The use of the Diversity-By-Disagreement Training (D-BAT) Algorithm by generating a set of labeled data representative of the option sub-goal with the intention of generalizing this option to unseen contexts. Eventually, the authors build brilliantly upon the RL hypothesis, i.e. the most transferable hypothesis will lead to higher external reward, providing a high-level policy. When this high-level policy maximizes extrinsic reward, it will naturally begin selecting hypotheses that best support transfer.\n\n\nThe significance of their work is integrating D-BAT which leverages both labeled and unlabelled data to train an ensemble of classifiers, each attending to a different set of features. Each ensemble member is encouraged to reduce the labeled loss while also decreasing agreement on the unlabeled data. As a result, each ensemble classifier represents a unique hypothesis of what features will generalize to out-of-distribution data, informed by the unlabelled data that was provided. \n\nThe empirical experiments were conducted to investigate and answer each question. The quality of their investigations is good and reflects each question considering both accuracy and the amount of labeled data required for successful sub-goal generalization as clearly shown in Figure 3. The authors succeed in validating each component of their algorithm. Additionally, experiments were done in MiniGrid DoorKey and MONTEZUMASREVENGE. Environments conclude that the D-BAT ensemble sub-goals can be used to learn policies that transfer the initial option as shown in Figure 5 and Figure 6. In general, this paper does a great job theoretically and empirically."
            },
            "weaknesses": {
                "value": "There seems no math or theorem that further explains your method or explains your formulation of \"hypothesis\" as a mathematical concept. This is particularly called into question due to the lack of math notations. In addition, no pseudocode was provided nor any clear statements about the connection with The Diversity-By-disAgreement Training (D-BAT) algorithm. More discussion on these areas would be much appreciated.\n\nFor the empirical study, the choice of baselines in the MONTEZUMA\u2019S REVENGE Environment \nwas restricted to the CNN classifiers without any clear explanation which makes it unclear for the reader. Similarly, there was no discussion of the choice of baselines in the MINIGRID DOORKEY Environment. \n\nMinor comments:\nP.6, Line 1 \u201cneed to be\u201d instead of \u201cneed be\u201d."
            },
            "questions": {
                "value": "I am very interested in your formulation of \u201chypothesis\u201d but at the same time was disappointed for not seeing enough explanation or theorem that covers your idea. How did you exactly formulate \u201chypothesis\u201d in your code? Can you provide any theorem and/or math notations that support your formulation?\n\nIn what respect does your use of representation learning differ from using dimension reduction? How do you exactly provide the high-level policy? \n\nIn Figure 3, you mentioned that \u201cthis performance is on a collected set of data and does not fully encompass all the possible states an agent may encounter during policy learning.\u201d How would the result differ from not using a collected set of data or any suggestions to overcome this? Why did you use the PPO and DQN agents to compare with? Similarly, why did you choose the CNN classifiers to compare with? A bit more discussion on these choices would be helpful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 10
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}