{
    "id": "RG806nMtQr",
    "title": "Revisiting Adversarial Examples from the Perspective of Asymptotic Equipartition Property",
    "abstract": "Adversarial examples, which can mislead neural networks through subtle perturbations, continue to challenge our understanding, raising more questions than answers. This paper presents a novel perspective on interpreting adversarial examples through the Asymptotic Equipartition Property (AEP). Our theoretical analysis examines the noise within these examples, revealing that while normal noise aligns with AEP, adversarial noise does not. This insight allows us to classify samples in high-dimensional space as belonging to either the typical or non-typical set, corresponding to normal and adversarial examples, respectively. \nOur analyses and experiments show adversarial examples arise from AEP in high-dimensional space and derive some key properties regarding their quantity, probability, and information capacity. These findings enhance our understanding of adversarial examples and clarify their counterintuitive phenomena, such as adversarial transferability, the trade-off between robustness and accuracy, and robust overfitting.",
    "keywords": [
        "Adversarial examples",
        "Adversarial robustness",
        "Asymptotic equipartition property"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=RG806nMtQr",
    "pdf_link": "https://openreview.net/pdf?id=RG806nMtQr",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces the concept of asymptotic equipartition property (a direct consequence of the weak law of large numbers) to divide the whole data space into two parts: typical set and non-typical set. Then the paper attempts to show that the normal samples (the raw data perturbed by normal noise) correspond to the typical set and the adversarial samples  (the raw data perturbed by adversarial noise)  correspond to non-typical set. Experiments support their idea and further enhance the understanding of adversarial examples and clarify their counterintuitive phenomena."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The basic idea of using the results of the weak law of large numbers to study the behavior of the normal/adversary noises is interesting. The findings based on this idea help us better understand adversarial examples and shed some light on counterintuitive phenomena such as adversarial transferability, the trade-off between robustness and accuracy, and robust overfitting."
            },
            "weaknesses": {
                "value": "1. My main concern is that the existing basic theoretical foundations of the paper are relatively weak (both Theorems 1 and 2 are known results) and are not clearly explained. I'm trying to understand the theoretical results (especially the proof of Lemma 2), but the vague expression (including the definition of notations, the introduction of background knowledge, and the mathematical analysis and discussion of results) made it difficult for me to understand these results theoretically. See Questions for more details. \n\n2. The paper is not well-written, many concepts and symbols are not defined, theorems and lemmas are not stated rigorously, and many notations are used inconsistently. I would suggest the authors go through the paper and polish the languages and statements."
            },
            "questions": {
                "value": "1. Theorem 1, the most important concept of the paper, is an informal statement. What is \"X\" here? Is the convergence based on probability? and the entropy rate is not introduced. In addition, throughout the paper, the notations $H$ and $\\mathcal{H}$, $Pr$ and $p$ have been used interchangeably.  Further, is $log$ means $\\log_2$? If it is, it would be better to declare it.\n\n2. Page 3, line 148, the paper states that \"assuming n $\\to \\infty$, ......, and all such sequences have the same probability $2^{-nH}$\". What does \"all such sequences have the same probability\" mean? \n\n3. In page 3, when defining \"Typical set\", $\\epsilon$ is simply introduced as a constant, but it is very important, corresponds to the convergence rate in Theorem 1. $\\epsilon$ should be discussed carefully here. For example, if the distribution is subGaussian, what is the order of $\\epsilon$, and what are the lower and upper bounds of (3).\n\n4. In page 5, why the adversarial noise $Z_{adv}$ is drawn from $P(Z|G)$? how to define $G$?\n\n5. The proof of Lemma 1 should be the proof of Lemma 2, and the proof of Lemma 1 is not given.\n\n6. Page 7, line 346, there is \"Section ??\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates adversarial perturbations using asymptotic equipartition property (AEP). The authors utilize theoretical tools based on AEP and some related numerical experiments to explain some key different observations in adversarial training, compared with training on clean input, including large model / dataset requirement, adversarial overfitting, clean accuracy / robust accuracy trade-offs and attack transferability."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. It is interesting to investigate the properties of adversarial examples from a probabilistic aspect.\n\n2. The manuscript is relatively well-written and easy to follow, the major message is clear: inputs perturbed by the random noise are typical while the ones perturbed by the adversarial perturbations are non-typical."
            },
            "weaknesses": {
                "value": "I have the following concerns about this manuscript.\n\n1. My major concern is that the theoretical analyses or explanations in this paper lack some quantitative results or conclusions. From my point of view, normal training (i..e, training on clean inputs) can be considered as a special form of adversarial training when the size of the adversarial budget is $0$. Therefore, any theoretical conclusions concerning the properties of the adversarial perturbations should depend on the size of the adversarial budget. In the formulation of this paper, I can say that the ``adversarial perturbation'' can also be typical when the maximum allowed magnitude is very small. However, the author's analyses and theoretical results do not reflect this.\n\n2. The motivation of AEP-AT is not very clear, the authors need more words to elaborate this part. In addition, the authors claim optimal trade-offs of AEP-AT in line 443, a ROC curve is expected. The advantages of AEP-AT are unclear either.\n\n3. AutoAttack is the strongest adversarial perturbation so far, when evaluating the robustness and reporting robust accuracy in the table, so it is better to evaluate robustness by AutoAttack instead of $10$-step PGD.\n\n4. More empirical simulations are expected for Section 4.3 and 4.4, since the conclusions are already included in existing works.\n\n5. The observations and the conclusions in this paper are already known in this community, making it different to point out the contributions of this paper."
            },
            "questions": {
                "value": "The questions are pointed out in the \"weakness\" part above, please take a look and try to address them in the rebuttal. The current version of the manuscript is not ready for publication."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper offers a perspective on adversarial examples by partitioning the input space into typical and non-typical sets. It defines the typical set as the original data samples, while adversarial examples are categorized as non-typical. The authors use this framework to explain various phenomena, including the trade-offs involved in training robust classifiers and the concept of transferability."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Comprehensive Conceptualization: The paper presents the concept of adversarial examples from a wide range of research areas within the field, providing a rich and varied perspective.\n\nEngaging Experiments: The authors conduct intriguing experiments related to adversarial training, which enhance the understanding of the proposed concepts and their implications."
            },
            "weaknesses": {
                "value": "Lack of Comparative Analysis: The paper provides naive empirical results without adequately comparing the proposed adversarial training method to current state-of-the-art approaches (as shown in Table 2 and Figure 5). Additionally, there is insufficient comparison between the results of this method and standard adversarial training (which does not introduce extra noise), as well as a lack of discussion on the balance between clean and adversarial training sets in regular AT (also in Figure 5).\n\nInsufficient Theoretical Discussion: The paper lacks a discussion of existing theories that address the notion of adversarial examples being off the data manifold. There is little exploration of how the concept of non-typical examples relates to the similarity between off-manifold examples and those deemed non-typical."
            },
            "questions": {
                "value": "1. If random noise is added to training images and still included in the typical set, why is the adversarial (non-typical) set considered larger?\n\n2. How does the AEP-AT trained classifiers perform compared to other existing robust classifiers?\n\n3. What do you perceive as the key differences between off-manifold and non-typical subspaces?\n\n4. Does AEP-AT help reduce overfitting, considering that the generated adversarial examples are more \"universal\"?\n\n5. Regarding transferability, does this imply that only noise independent of the gradient should be transferable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the notion of the Asymptotic Equipartition Property (AEP) and then argues that this partitions the input space into a \"typical\" and \"non-typical\" set. The paper states that natural examples fall into the typical set, while adversarial examples fall into the non-typical set. The paper argues that adversarial vulnerability arises from"
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper addresses an important topic\u2014characterizing the ways in which adversarial examples can be distinguished from natural examples. This remains an open topic and would have important implications if resolved, such as the ability to construct robust classifiers, and further understanding of the nature of adversarial examples."
            },
            "weaknesses": {
                "value": "This paper is not very clearly written and therefore difficult to evaluate. I think even if all technical issues are addressed, the paper would need to re-written to be reproducible, which, in my opinion, is necessary prior to being publication-ready.\n* The notion of \"natural\" samples and \"adversarial\" samples is not formally defined, despite being used formally (such as in Lemma 1 & 2). Related, Lemmas 1 & 2 are not stated formally.\n* The definitions of Non-Typical I and Non-Typical II are not given.\n* The definition of Typical perturbations is not clear, see Questions section.\n* It's not precise how the methods were implemented, how exactly is adversarial training performed and how is adversarial accuracy evaluated? (See questions)\n* In Table 1, it's not clear what the rows represent\u2014on the top, you have listed three different datasets, and on the bottom, four different models. Which model is used for which dataset? It seems as if the first row of the datasets & of the models is copied.\n* Figure 5 doesn't specify which dataset is being used (the numbers seem to indicate it is CIFAR-10, by checking with Table 2, but please confirm this)\n\n\nThe theory in this paper is not rigorous. The main novel theoretical contributions of this paper are Lemma 1 and Lemma 2. \n* As discussed above, neither lemmas are stated formally and so it is unclear what is being stated. \n* There is no proof of Lemma 1. (In the appendix, there is a proof that is labeled \"Lemma 1\" but this actually refers to Lemma 2 from the main paper).\n* The proof of Lemma 2 (labeled \"Lemma 1\" in the appendix) does not seem not rigorous: specifically the argument that $I(Z;G)>0$ does not seem to be supported by any formal proof (please see Questions section).\n\n\nExperimental & Implementation details\n* While it is impossible for me to verify, the claimed robustness of the AEP-AT trained models in Table 2 raise a red flag for me to the point where I believe they might indicate a possible bug in the code that generates adversarial attacks\u2014for example, the authors are claiming that the adversarial accuracy. It is somewhat difficult for me to believe that the method proposed by the authors\u2014adding a grid pattern  to adversarial examples during training (as in Figure 4)\u2014would increase adversarial performance by nearly 30 points. In order to verify that the models are actually as robust as the authors claim, the authors should implement adaptive attacks such as AutoAttack. If this is not a bug and holds up under other attack strategies, this is an impressive result. (Nonetheless, my claims about the writing above still stand.)"
            },
            "questions": {
                "value": "On line 811, what is meant by \"From PGD attack [Madry et al, 2018] we know that gradient information $G$ is closely related to $Z$.\" Do you have a reference to what specifically you are referencing from Madry et al., 2018? Is this referring to any formal claim, or just that one can construct an adversarial example by taking gradient steps?\n\nOn line 232, you say that for typical set examples, you add AEP-compliant noise, and then give two examples as possible noise distributions, but what specific distribution(s) do you use for your experiments?\n\nOne of your primary assumptions is that adversarial examples intervene on the noise component of the data. However, this is the opposite claim of what Ilyas et al., 2019 claims. Can you reconcile your assumption with their claims?\n\nOn line 320 you're defining your adversarial accuracy in terms of a PGD attack, but with what perturbation budget? Also, what type of adversarial examples are you using to evaluate the model (e.g., standard adversarial examples or plus {gaussian noise, uniform noise, NT-I, NT-2, etc}?\n\nReferences:\n\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander\nMadry. Adversarial examples are not bugs, they are features. In NeurIPS, pp. 125\u2013136, 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}