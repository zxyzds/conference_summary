{
    "id": "vVHc8bGRns",
    "title": "RecFlow: An Industrial Full Flow Recommendation Dataset",
    "abstract": "Industrial recommendation systems (RS) rely on the multi-stage pipeline to balance effectiveness and efficiency when delivering items from a vast corpus to users. Existing RS benchmark datasets primarily focus on the exposure space, where novel RS algorithms are trained and evaluated. However, when these algorithms transition to real-world industrial RS, they face a critical challenge: handling unexposed items\u2014a significantly larger space than the exposed one. This discrepancy profoundly impacts their practical performance. Additionally, these algorithms often overlook the intricate interplay between multiple RS stages, resulting in suboptimal overall system performance. To address this issue, we introduce RecFlow\u2014an industrial full-flow recommendation dataset designed to bridge the gap between offline RS benchmarks and the real online environment. Unlike existing datasets, RecFlow includes samples not only from the exposure space but also unexposed items filtered at each stage of the RS funnel. Our dataset comprises 38M interactions from 42K users across nearly 9M items with additional 1.9B stage samples collected from 9.3M online requests over 37 days and spanning 6 stages. Leveraging the RecFlow dataset, we conduct courageous exploration experiments, showcasing its potential in designing new algorithms to enhance effectiveness by incorporating stage-specific samples. Some of these algorithms have already been deployed online, consistently yielding significant gains. We propose RecFlow as the first comprehensive benchmark dataset for the RS community, supporting research on designing algorithms at any stage, study of selection bias, debiased algorithms, multi-stage consistency and optimality, multi-task recommendation, and user behavior modeling. The RecFlow dataset, along with the corresponding source code, is publicly available at \\textcolor{red}{\\url{https://github.com/RecFlow-ICLR/RecFlow}}. The dataset is licensed under CC-BY-NC-SA-4.0 International License.",
    "keywords": [
        "recommendation system",
        "recommendation dataset"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vVHc8bGRns",
    "pdf_link": "https://openreview.net/pdf?id=vVHc8bGRns",
    "comments": [
        {
            "summary": {
                "value": "This paper first proposes a full-flow recommendation dataset collected from the industrial video recommendation scenarios. The overall process includes retrieval, pre-ranking, coarse ranking, ranking, re-ranking, and edge ranking. The logs are collected from January 13 to February 18, 2024. The datasets can be accessed via a half-anonymized link that denotes the authors' institute."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The proposed full-flow dataset provides a strong groundwork for follow-up research. For example, models can learn how to alleviate selection bias due to the discrepancy between the training and inference stages.\n2. The authors performed comprehensive experiments and presented the results of the experiments with means and variances.\n3. The complete datasets are available for further research."
            },
            "weaknesses": {
                "value": "1. The paper's current presentation lacks clarity and coherence, making it difficult to follow. Additionally, there are numerous minor grammatical and structural errors throughout the text.\n2. While the initial explosion stage involves large-scale data, the subsequent re-ranking and edge-ranking stages utilize significantly smaller datasets. This inconsistency undermines the paper's claim of working with large-scale industrial data.\n3. The paper's novelty is not effectively demonstrated through comparative analysis with existing work. Particularly in the introduction, while the authors enumerate the merits of the RecFlow dataset, they fail to provide meaningful comparisons with related work. The innovation of this research can only be discerned through prior knowledge of the field rather than through the authors' presentation."
            },
            "questions": {
                "value": "1. Regarding the ten merits presented in the introduction, it remains unclear which characteristics are unique to the RecFlow dataset compared to existing benchmarks.\n2. As in line 143, what is the rationale behind the number of videos selected for each stage?\n3. Also, can you explain why you chose 200 negative samples for each positive?\n4. Some typos in the paper; for example, in line 379, recall 100 happens twice. This error occurs lots of times."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The proposed benchmark contains lots of users features, such as age, gender, province, etc."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper mainly focuses on introducing a dataset, RecFlow. This dataset contains full-flow recommendation data, including retrieval, pre-rank, coarse ranking, ranking, re-ranking, and edge ranking. Containing two periods, the datasets provide an opportunity to study full-stage recommendations in industry. The full-stage recommendation is widespread in industry recommendations and is supposed to be investigated. Some experiments are conducted to give examples of how to utilize this dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. An essential and practical problem in real industry recommendation. The full-stage recommendation is widespread in the industry; this dataset really provides a new perspective on this problem.\n\n2. The collection strategy is provided, and privacy protection is carefully considered.\n\n3. Experiments are provided to show how to use this dataset."
            },
            "weaknesses": {
                "value": "1. Despite providing collection and analysis, the collection procedure should be provided in more detail to show that it is reasonable and correct. Moreover, the analysis is too simple, and more intuition about this dataset can be given.\n\n2. The experiments provided to show how to use this dataset are interesting. However, in line 079, the author argues that Recflow can provide merits of ten tasks. It should be supposed that the experiments on these tasks should be provided.\n\n3. There are some typos. For example, Line 314 Recall@100,500,100 should be 1000. The whole paper should be proofread."
            },
            "questions": {
                "value": "1. Actually, the samples in every stage are based on the filtered strategy in the previous stage. So, will this strategy bring bias? And if we use a different strategy, can the conclusion still hold? For example, in industry, from retrieval to pre-ranking usually consists of several strategies. How does this benchmark reflect this?\n\n2. Are the results from Table 4 to Table 7 reproducible?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents RecFlow, an industrial-scale recommendation dataset that captures the full recommendation pipeline with multiple stages, featuring 38M user interactions and 1.9B samples collected from 42K users and 9M items over a span of 37 days. One of RecFlow\u2019s innovations is its inclusion of unexposed items at each pipeline stage, allowing for important analysis of distribution shifts between training and serving environments. The dataset also supports multi-task recommendation and behavior modeling by capturing various user feedback signals.\n\nExperiments show that modeling stage-specific interactions and addressing distribution shift with RecFlow data improves recommendation performance, with some methods proving effective in real-world systems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The paper presents the first comprehensive large-scale dataset that captures the complete recommendation pipeline, filling a critical gap in the field where existing datasets only contain exposure data. It could enable further research into real-world problems that were previously difficult to study, eg: distribution shift, stage interaction effects.\n2. Good motivation is provided by clearly articulating the limitations of existing datasets and the importance of studying full recommendation pipelines.\n3. The dataset is well documented with clear descriptions of features, collection methodology and privacy protection measures. The privacy protection approach is also robust, using a combination of user consent, feature anonymization and careful data filtering.\n4. Experimental validation is thorough with multiple runs, standard deviation reporting and comprehensive ablation studies across different stages."
            },
            "weaknesses": {
                "value": "1. The paper doesn't adequately address the computational challenges of working with such a large dataset. Details about storage requirements and recommended sampling strategies would be valuable for practitioners.\n2. The multi-task learning potential of the dataset is mentioned but not thoroughly explored. Given the rich set of user feedback signals, this seems like a missed opportunity.\n3. While the authors mention online A/B testing validation, the details are sparse. More information about the production deployment and real-world performance would strengthen the paper's practical impact claims.\n4. Analysis of how the stage samples could help with cold-start recommendations problem could be a useful contribution."
            },
            "questions": {
                "value": "1. What measures did you take to ensure the dataset is representative?\n2. How can the dataset help handling cold-start users/items better?\n3. Could you please provide more details on the online A/B testing setup and results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper published a new dataset for multi-stage-funnel-based recommendation system, where the key difference from existing datasets is the inclusion of unexposed samples. Most existing datasets only contain samples that are exposed to users, and ranking and earlier-stage models are typically trained on exposed samples with user feedback. So inclusion of unexposed samples can facilitate research for many interesting problems in multi-stage recommendation, such as the distribution gap between training and serving, multi-stage consistency etc."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "As far as I know, this is a first contribution of benchmark datasets that includes multi-stage and unexposed samples. Could be useful for researching important problems in multi-stage recommendation systems."
            },
            "weaknesses": {
                "value": "The evaluation criterion for the quality of a benchmark dataset for industrial recommendation system should be fidelity to an actual online recommendation system. For example, if researchers come up with new algorithms with metrics improvement using this dataset, then when it\u2019s deployed to a real online system, such improvement can be validated. So it would be great if the authors can demonstrate such fidelity to some extent, e.g., by running online A/B test to compare the online performance and offline metrics to see the correlation. \n\nThe rules for determining how many samples for each stage seem quite ad-hoc w/o explanation of considerations. Could you explain the considerations that went into determining the number of samples collected at each stage? Are these numbers representative of typical production systems?\n\nsome typos: \nline 239: datatse -> dataset\nline 256: quote in wrong direction \nline 296/399: 1e-1/1e-2 not well formatted \nline 308/361: randomly sampling -> randomly sampled"
            },
            "questions": {
                "value": "what\u2019s the rationale for partitioning the data collection into two periods? Understanding the rationale for partitioning the data collection into two periods would help readers assess the dataset's representativeness and potential use cases. Could you explain the reasoning behind this decision and discuss any differences between the two periods that researchers should be aware of?\n\ndo you also log content features for the items? content features such as text/image/video/etc. content description (e.g., metadata, or embedding representations etc.) can be very useful features for recommendation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "this paper collected user data from an online recommendation system. The paper claim to have user consent and annoymized user-identity the technical preprocessing of the data (e.g., hashing etc.), but probably need double check privacy concerns when it's published."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}