{
    "id": "LvRQgsvd5V",
    "title": "Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching",
    "abstract": "In inverse reinforcement learning (IRL), an agent seeks to replicate expert demonstrations through interactions with the environment. Traditionally, IRL is treated as an adversarial game, where an adversary searches over reward models, and a learner optimizes the reward through repeated RL procedures.\nThis game-solving approach is both computationally expensive and difficult to stabilize.\nIn this work, we propose a novel approach to IRL by direct policy optimization: exploiting a linear factorization of the return as the inner product of successor features and a reward vector, we design an IRL algorithm by policy gradient descent on the gap between the learner and expert features.\nOur non-adversarial method does not require learning a reward function and can be solved seamlessly with existing actor-critic RL algorithms.\nRemarkably, our approach works in state-only settings without expert action labels, a setting which behavior cloning (BC) cannot solve.\nEmpirical results demonstrate that our method learns from as few as a single expert demonstration and achieves improved performance on various control tasks.",
    "keywords": [
        "Inverse Reinforcement Learning",
        "Imitation Learning",
        "Successor Features"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=LvRQgsvd5V",
    "pdf_link": "https://openreview.net/pdf?id=LvRQgsvd5V",
    "comments": [
        {
            "summary": {
                "value": "This work considers inverse RL in the state-only setting. Leveraging the linear structure of returns, as inner product of successor features and reward weight, the authors could model inverse RL without adversarially optimizing the reward weight, while is often done in prior works. This allows one to directly optimize policy through minimizing the gap between the successor features of expert policy and of the learned policy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well organized and easy to follow\n\n- The motivation is sound and the resulting algorithm is straightforward (without requiring min-max optimization)"
            },
            "weaknesses": {
                "value": "- It is quite surprising that IQL failed most of the tasks in Figure 3\n\n- The choice of baselines could be expanded. While SFM considers the state-only setting, GAIfO is the only baseline that originally proposed for this setting. It might be reasonable to include a few baselines that are designed for state-only setting, for example [1, 2] (with public implementation if my memory serves me right) and some more up-to-date baselines are appreciated. \n\n----\n\nNote: I have not actively followed the recent literature on successor features, so I am uncertain about its novelty when applied to the IRL setting. As a result, I have a low confidence score.\n\n[1] Gangwani, Tanmay, and Jian Peng. \"State-only imitation with transition dynamics mismatch.\" arXiv preprint arXiv:2002.11879 (2020).\n[2] Zhu, Zhuangdi, et al. \"Off-policy imitation learning from observations.\" Advances in neural information processing systems 33 (2020): 12402-12413."
            },
            "questions": {
                "value": "- what are the hyperparameter search space for SFM and baselines respectively?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Successor Feature Matching (SFM), a novel non-adversarial approach to Inverse Reinforcement Learning (IRL). The key innovation is reformulating IRL as a direct policy optimization problem that matches successor features between the agent and expert demonstrations. The method has three notable contributions:\n\n1. A non-adversarial approach that avoids the computational expense and instability of traditional adversarial IRL methods\n2. The ability to learn from state-only demonstrations without requiring expert action labels\n3. Strong performance from as little as a single expert demonstration\n\nThe method leverages successor features to estimate expected cumulative features and uses policy gradient descent to minimize the gap between learner and expert features. The authors demonstrate superior performance compared to both adversarial and non-adversarial baselines across multiple control tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The non-adversarial approach using successor features is novel and elegantly simple compared to existing methods\n2. The approach is well-justified with clear theoretical analysis and proofs\n3. The method works with state-only demonstrations and single examples, making it widely applicable\n4. SFM integrates seamlessly with existing actor-critic frameworks\n5. Demonstrates superior performance across multiple tasks and metrics\n6. Shows consistent performance even with weaker policy optimizers"
            },
            "weaknesses": {
                "value": "1. The paper doesn't fully address how SFM handles the exploration problem inherent in IRL\n2. The current implementation is tied to deterministic policy gradients, potentially limiting its applicability\n3. The method currently only works with state-only base feature functions\n4. While comprehensive, the evaluation is limited to DMC suite tasks; testing on more diverse environments would strengthen the claims"
            },
            "questions": {
                "value": "1. How would the method perform on tasks with sparse rewards or requiring significant exploration?\n2. Could the approach be extended to work with stochastic policies?\n3. How does the method compare to recent offline IRL approaches?\n4. What are the computational requirements compared to adversarial methods?\n5. How sensitive is the performance to the choice of successor feature architecture?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Successor Feature Matching (SFM), a novel method to Inverse Reinforcement Learning (IRL) which is non-adversarial and does not require expert action labels. SFM directly optimizes a policy to match the expert's successor features (SFs) using policy gradient ascent. The paper claims that SFM offers several advantages over existing IRL methods, including simplified training, state-only learning and robustness to optimizer choice.\nThe paper supports these claims through empirical evaluations on the DeepMind Control suite, demonstrating that SFM outperforms state-of-the-art adversarial and non-adversarial baselines on a variety of single-demonstration tasks. Additionally, the paper explores the impact of different base feature functions on SFM's performance, finding that Forward Dynamics Models (FDM) lands the best results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- **Novel and well-motivated approach:** SFM provides a new angle in IRL by leveraging SFs for direct policy optimization, offering an alternative to adversarial methods.\n- **Strong empirical results:** Experiments demonstrate the superiority of SFM over sota methods on normalized return and optimality gap across single-demonstration tasks, demonstrating its effectiveness.\n- **State-only learning:** The ability to learn from state-only demonstrations is a great contribution.\n- **Robustness to RL optimizer choice:** SFM's performance with both strong (TD7) and weaker (TD3) optimizers shows versatility, making it potentially useful for resource-limited applications.\n- The experiments are conducted in clear logic and the analysis and observations are novel and interesting."
            },
            "weaknesses": {
                "value": "- **Dependence on deterministic policy gradients:** The current formulation of SFM is tied to deterministic policy gradient algorithms. Exploring extensions to stochastic policies would broaden its scope. However, it's clearly mentioned by the authors in the discussion part.\n- **Exploration Challenges:** as stated by the authors, the presented method does not fully resolve IRL exploration issues. I wonder how the performance may vary significantly across domains requiring extensive exploration."
            },
            "questions": {
                "value": "- How does SFM handle scenarios where expert demonstrations do not cover the entire state space, particularly in sparse-reward or high-dimensional tasks?\n- Can the authors elaborate on potential extensions of SFM to stochastic policy optimizers, and any early experiments indicating feasibility?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Successor Feature Matching (SFM) in the Inverse Reinforcement Learning (IRL) problem.  The authors focused generalized reward-like properties of  successor features (Barreto et al. 2017), e.g. recovering reward, discounted accumulation. These allowed SFM for alternative formulation of policy optimization using L2 loss in Eq. (4). The experimental results suggest that the proposed SFM yields good scores in single-demonstration benchmarks and much more impressive scores among state-only IRL algorithms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed SFM, in contrast to adversarial methods, is more rooted in the historical context of early IRL studies.\n2. In my opinion, IRL (or even RL) methods do not have to restrict themselves to formulating scalar reward signals. Adaptation SF (Barreto et al., 2017) expands the notion of IRL into feature learning, which can relax some rigidity in RL-IRL formulation. Since the features have successfully formulated vectorized signals in the transfer learning tasks in RL, this could be a significant contribution that relaxes studies of IRL. \n3. The proposed method achieved good performance among modern IRL methods.\n4. Propositions 1-3 are straightforward to understand."
            },
            "weaknesses": {
                "value": "1. I do not understand why \"non-adversarial\" property is framed as the main contribution throughout the paper. Adversarial learning and SFM are not mutually exclusive. Eq. (3) shows that the base feature can be trained with adversarial learning by setting $\\mathcal{L}_\\mathrm{feat} = \\mathrm{Eq. (3)}$. If the author intended to deepen our understanding of the role of adversarial learning in IRL, they should have provided (1) another theoretical justification of non-adversarial learning and (2) ablation studies, including Eq. (3) in Fig. 6, and (3) results of combining various base feature losses that verifies adversarial losses might deteriorate IRL performance.  \n2. Compared to Figs. 4 and 5, single-demonstration task performance in Fig 3 is only sometimes good, suggesting that SF is more focused on representing states. For some scenarios, action representation performance might be necessary.   \n3. To demonstrate scalability, including more complex (or demonstrative) benchmarks in Fig. 3 would be beneficial to IRL researchers to grasp the model's performance. \n4. There are no supporting tables in the appendix that measure the performance of the learning curves in Figs. 3, 5, 6."
            },
            "questions": {
                "value": "1. How applying l2-loss  in Eq.(4) can be understood for the policy $\\pi_\\mu$? \n2. I think SFM (random) performed well; what is the reasoning behind this and presenting Eqs? (9-11)? Could the base feature only trained with arbitrary unsupervised learning with only states without the next states, actions, or goals?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}