{
    "id": "1d8Egv45of",
    "title": "Learning Multiple Semantic Views For Self-explaining Physiological Signal Stratification",
    "abstract": "Explainable artificial intelligence (XAI) offers enhanced transparency by revealing key features, relationships, and patterns within the input data that drive model decisions. In healthcare and clinical applications, where physiological signals serve as inputs to the models for decision making, such transparency is critical for ensuring reliability, identifying biases, and uncovering new insights. However, despite the potential to reveal clinically-relevant information used for inference, generalized solutions for explainability have remained limited in this domain. In this work, we propose a generalized self-explaining multi-view deep learning architecture, that generates task-relevant human-interpretable representations during model inference, for stratifying health information from physiological signals. Specifically, the proposed network architecture employs a mask network to produce multiple mask-modulated versions of the signal, referred to as \u201csemantic views\u201d, highlighting distinct regions of the signal that may be relevant to clinically significant information. These views offer complementary perspectives to enhance interpretability and feature extraction. A shared embedding network is used to extract task-related features from each semantic view, which are used to produce the model's output. Through supervised training with labels, each semantic view is updated based on the saliency information between the semantic view and the model's output, toward fitting the model's output to the task labels. Validated on 4 different clinically-relevant classification and regression tasks taking electrocardiogram (ECG) or photoplethysmogram (PPG) as input, the proposed multi-view architecture displays universal usability, achieving comparable or superior performance across all tasks, when compared to state-of-the-art methods designed for each task. Unlike current state-of-the-art models, which lack task-agnostic human interpretability, our model uniquely provides interpretable outputs. As it is shown, the semantic views generated by the proposed model highlight task-specific characteristic regions in the input signal, aligning closely with the domain knowledge of human experts for each task. Overall, the proposed method offers new directions for interpretable machine learning and data-driven analysis of physiological signals, envisioning self-explaining models for clinical applications.",
    "keywords": [
        "Explainable artificial intelligence (XAI)",
        "Interpretable machine learning",
        "Interpretability",
        "Deep learning",
        "Time Series Analysis",
        "Segmentation",
        "End-to-end",
        "Self-explaining models",
        "Physiological signals",
        "Photoplethysmogram (PPG)",
        "Electrocardiogram (ECG)",
        "Obstructive sleep apnea (OSA)",
        "Atrial fibrillation (AF)",
        "Heart rate variability (HRV)",
        "Blood pressure (BP)"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We propose a generalized self-explaining multi-view deep learning architecture, that generates task-relevant human-interpretable representations during model inference, for stratifying health information from physiological signals.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1d8Egv45of",
    "pdf_link": "https://openreview.net/pdf?id=1d8Egv45of",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a multi-view deep learning model aimed at self-explaining predictions for various physiological signal-based tasks, such as obstructive sleep apnea (OSA) and atrial fibrillation (AF) detection. The proposed model generates \u201csemantic views\u201d by using mask networks to isolate task-relevant regions of the input signals. These views are used to enhance interpretability and yield clinically relevant insights."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1- The multi-view segmentation approach is an innovative contribution, adding potential value to explainability in machine learning for healthcare, where interpretability is crucial.\n\n2- The paper proposes a unified architecture applicable to both classification and regression tasks, which shows adaptability to a variety of physiological signal processing tasks."
            },
            "weaknesses": {
                "value": "1- The paper introduces multiple semantic views (2, 3, or 4 views) but does not explain why these specific numbers of views are optimal across tasks. This arbitrary choice may limit the interpretability and generalizability of the approach. Further discussion or empirical testing regarding the impact of varying the number of views on interpretability and performance would strengthen the approach.\n\n2- The experimental setup could have greatly benefited from ablation studies that justify the architectural decisions, such as the number of mask networks or the use of shared embedding networks. These studies would help clarify the impact of each component on the model\u2019s performance and interpretability, providing a stronger empirical basis for the architectural choices.\n\n3- The authors claim alignment between the generated views and clinical knowledge, yet this is primarily presented through visual inspection. Providing more robust, quantitative evaluations of interpretability, ideally verified with domain experts, would lend credibility to these claims.\n\n4-  The results are not entirely convincing, as the proposed model fails to outperform current state-of-the-art implementations on 3 out of 4 datasets. Additionally, the ablation study in Table 1 indicates that the multi-view architecture offers only a marginal performance improvement. \n\n5- There is a lack of comparison with established explainability approaches like SHAP or LIME. Although these methods may not offer the same level of task-specific interpretability, a comparison would clarify the relative benefits of the proposed model. \n\nMinor: \n\n1- The naming conventions in Table 1 could be clearer. Terms like \u201cSOTA\u201d and \u201cablation\u201d could be replaced with more descriptive labels that specify the method or configuration used, making it easier for readers to understand the comparison."
            },
            "questions": {
                "value": "Major: \n\nSee above (\"Weaknesses\").\n\nMinor:\n\n1- Given the reliance on task labels to optimize segmentation, how does the model perform on tasks with sparse or noisy labels? Does this affect interpretability? It would be interesting if authors could've addressed that and potentially compare it with the SOTA method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The work involves an inherently explainable AI approach for clinically relevant supervised tasks based on electrocardiogram (ECG) or photoplethysmogram (PPG) inputs. The explainability is achieved by exploiting trainable masks to identify regions of ECG/PPG contributing to clinically significant information."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This work presents an original idea for a generalized explainable deep learning architecture that can potentially have significant implications for AI-based medicine and XAI. Specifically, it incorporates model and sample level interpretability, by introducing a prior constraint on the number of semantic views which are trainable (model level explainability), based on which each sample produces a unique segmentation mask (sample level explainability). This is contrary to post hoc explainability techniques, where the sample-level explanation can be independent, ambiguous (model approximations), and inconsistent across techniques."
            },
            "weaknesses": {
                "value": "The proposed deep learning architecture can be relevant for signal segmentation tasks but the level of explainability is quite coarse. The method involves qualitative (visual) exploration of the learned masks that may be quite difficult for slightly more complex tasks. This is also evident from the fact that performance is only optimum for 2 or 3 masks, with the design preventing the integration of multi-dimensional concepts. Moreover, the learned views do not necessarily seem to provide unique insights into model explainability, without knowing which semantic states, and which part of the signals in each mask, contribute to the networks\u2019 decisions (e.g., in the example of AF, the presence or absence of P waves may be more indicative for the detection of the disease, than actual QRS peaks). The selection of tasks is also quite limited in showing interpretability properties, considering that all tasks in the paper are defined as conditions related to peak-to-peak interval variability."
            },
            "questions": {
                "value": "Abstract\n\nMethods in the Abstract do not need to be so extensive e.g., The section: \u201cSpecifically, the proposed network\u2026 to the task labels\u201d could be omitted.\n\nMethods\n\nLine 181. \u201c\u2026 each sample x in the time series S to be attributed to one of the N semantic states\u201d.\nWhy should one sample be attributed to only one semantic state? Intuitively, it seems that specific parts of a physiological signal could reflect several latent semantic states.\n\nEquation (4). By applying softmax activation at each time sample, we allow the information to leak into all masks with varying amplitudes, which deviates from the original strict binary definition. In this case, how do we know whether this information is amplified or suppressed by the subsequent embedding networks \u2013 and hence attribute explainability to the high-amplitude parts?\n\n3.2.2. Considering that the semantic segmentation masks provide interpretability, why do we need weight sharing in the embedding network features?\n\n3.2.2. What\u2019s the role of the differential embedding vector? Is there any empirical evidence that the decision network can\u2019t exploit such relationships?\n\nResults\n\nTable 1. It would be preferable to report AUC metrics instead of accuracy, considering that accuracy will be sensitive to each model and binary threshold (was there any threshold tuning?) \n\nLine 337. \u201cwhich suggests the effectiveness\u2026 from full-time series interval\u201d. The comparison here may not be fair, considering that removing the mask network significantly reduces the number of parameters in the model, which will solely rely on one embedding network to receive input from the original signal.\n\nLine 370. \u201cFrom Figure 3\u2026 clearly capture such information\u201d. The heart rate variation is not very prominent in the figures. Maybe you could show smaller windows or wider X-axes?\n\nAppendix\n\nLine 926. \u201cwe enforce a minimum duration L\u201d. How did you select L for each task? I don\u2019t think these numbers are mentioned in the paper.\n\nGeneral\n\nPlease generate in-text citations with brackets.\n\nFrom a physiological signal interpretation perspective, how do these semantic views compare to existing post-hoc explainability methods? E.g., clustering techniques at the sample level [1].\n\nPotential Work\n\nThe assumptions behind the semantic masks (semantic states attributed to specific time samples) and the need for prior selection of the number of semantic states N may introduce limitations as a general-purpose explainability mechanism. Could the network somehow discover the optimal number of semantic states? (instead of predefining N).\n\nReferences:\n\n[1]  Boubekki, A., Fadel, S.G., & Mair, S. (2024). Leveraging Activations for Superpixel Explanations. ArXiv, abs/2406.04933."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a self-explaining deep learning model architecture designed to enhance interpretability in the analysis of physiological signals, an issue often overlooked in existing deep models. The architecture employs a multi-semantic view approach, which generates multiple mask-modulated signal versions through a mask network. This process attributes model inputs to distinct semantic states, uncovering hidden patterns within the input data. The paper tests this architecture on four clinically relevant tasks involving ECG or PPG signals for classification and regression. Experimental results indicate that the multi-view approach demonstrates improved model interpretability, providing clearer insights into the model's decision-making process."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The research topic of this paper focuses on the interpretability of medical artificial intelligence, which is a field of great concern and has significant practical importance.\n\n- The method proposed in this paper maps different parts of the input signal to different semantic state spaces, revealing hidden patterns in the input signal that are related to model decisions, thereby enhancing the model's interpretability.\n\n- This paper has been validated on multiple datasets, and the experimental results show that the model's decision focus aligns with domain knowledge, verifying the effectiveness of the method."
            },
            "weaknesses": {
                "value": "- Data diversity: The dataset used in this article is limited to a single type of physiological signal, utilizing either ECG or PPG signals exclusively (although differentiated PPG signals were employed). This limitation raises questions about the effectiveness of the proposed method when applied to mixed types of physiological signal inputs, which warrants further validation.\n- Semantic state complexity: The number of semantic states in the paper is relatively small (2, 3, or 4). For complex inputs or tasks, a limited number of semantic states may not adequately reflect the model's decision-making process. The performance of the model with a higher number of semantic states requires further investigation.\n- Visualization challenge: The visualization results are discernible when the number of semantic states is small (e.g., 2). However, as the number of semantic states increases, these visualizations become difficult to recognize effectively. This can lead to a decreased understanding of the model's decision focus, thereby reducing the model's interpretability.\n- Evaluation metrics: There are concerns with the evaluation metrics used in the dataset. In classification problems, accuracy is employed as the evaluation metric, but this metric is susceptible to the impact of class imbalance. More robust metrics, such as AUC or F-score, should be considered for a more reliable assessment.\n- Temporal data representation: The method proposed in the paper classifies semantic states for individual sample time points. However, data from a single time point may not capture sufficient semantic information, especially when the signal sampling frequency is high, which could limit the representation of meaningful physiological information."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an architecture for processing medical waveforms with enhanced explainability. The author claims the learned representations are task-relevant and human-interpretable."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is generally easy to read, but its clarity can be enhanced by better diagrams."
            },
            "weaknesses": {
                "value": "1. In general, there is no significant technical innovation specially designed for clinical applications or medical waveforms. The author should explain how the proposed method differs from previous general xAI methods and compare the performances.   \n2. The author claims the method generates human-interpretable features, but the embedding and decision networks are not easily interpreted (limiting the transparency significantly).  \n3. There seems to be no user study with clinicians on the relevance of extracted features.   \n4. Figure 2 is too brief, consider adding some sub-figures to illustrate the ideas. It\u2019s only about half the page width now.  \n5. Some equations on Page 5 seem un-necessary, and the notation can be simplified.  \n6. The tasks selected are not representative in general, and the SOTA methods cited are old in general.   \n7. The ablation is only limited to the number of views.   \n8. The results reported in Section 5.2 have strong selection bias (correctly-classified ones are shown)."
            },
            "questions": {
                "value": "1. \\[Line 16\\] Can you explain how xAI \u201censures\u201d reliability without casual inference study?  \n2. \\[Line 31\\] Why do you believe validating on only 4 tasks with only 2 waveforms \u201cdisplays universal usability\u201d?   \n3. \\[Line 37\\] Is highlighting relevant regions sufficient for transparency? How does it relate to clinical decision making? Is there any assumption to be made here for how clinicians interact with the model you developed?  \n4. \\[Line 328\\] It seems 4-view is worse than 3-view. Is there any explanation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}