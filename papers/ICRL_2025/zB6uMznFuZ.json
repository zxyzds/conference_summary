{
    "id": "zB6uMznFuZ",
    "title": "TimeAutoDiff: Generation of Heterogeneous Time Series Data via Latent Diffusion Model",
    "abstract": "In this paper, we leverage the power of latent diffusion models to generate synthetic time series tabular data.\nAlong with the temporal and feature correlations, the heterogeneous nature of the feature in the table has been one of the main obstacles in time series tabular data modeling. \nWe tackle this problem by combining the ideas of the variational auto-encoder (VAE) and the denoising diffusion probabilistic model (DDPM).\nOur model named as \\texttt{TimeAutoDiff} has several key advantages including \n(1) \\textit{\\textbf{Generality}}: the ability to handle the broad spectrum of time series tabular data with heterogeneous, continuous only, or categorical only features; \n(2) \\textit{\\textbf{Fast sampling speed}}: entire time series data generation as opposed to the sequential data sampling schemes implemented in the existing diffusion-based models, eventually leading to significant improvements in sampling speed, \n(3) \\textit{\\textbf{Time varying metadata conditional generation}}: the implementation of time series tabular data generation of heterogeneous outputs conditioned on heterogenous, time varying features, enabling scenario exploration across multiple scientific and engineering domains.\n(4) \\textit{\\textbf{Good fidelity and utility guarantees}}: numerical experiments on eight publicly available datasets demonstrating significant improvements over state-of-the-art models in generating time series tabular data, across four metrics measuring fidelity and utility; \nCodes for model implementations are available at the supplementary materials.",
    "keywords": [
        "Time series data",
        "Tabular data",
        "Heterogeneous",
        "Diffusion model",
        "VAE",
        "Generative model"
    ],
    "primary_area": "generative models",
    "TLDR": "We develop a time series tabular synthesizer, combining VAE and diffusion model.",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zB6uMznFuZ",
    "pdf_link": "https://openreview.net/pdf?id=zB6uMznFuZ",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a novel method, TimeAutoDiff, to generate time series. It combines a variation autoencoder and DDPM. The method empirically outperforms prior methods on several datasets and allows for conditional time series generation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- **Writing** The paper is well-written and well-structured.\n- **Methodological contribution** The paper presents a clear method similar to diffusion methods in other domains. The technical details are discussed in detail. It seems straightforward to reproduce the results.\n- **Research problem** The presented problem is essential and timely. In particular, synthetic time series generation is critical for foundation model development, as the authors mention in the discussion."
            },
            "weaknesses": {
                "value": "See Questions for detailed comments and suggestions.\n\n- **Comparison to prior methods** It is worth discussing more the key differences to other diffusion-based models such as Diffusion-ts (https://github.com/Y-debug-sys/Diffusion-TS), and TimeDiff (https://openreview.net/pdf?id=ESSqkWnApz). Additionally, it is worth expanding metrics, such as consistency and diversity, as discussed in the TSGM framework (https://github.com/AlexanderVNikitin/tsgm).\n\n- **Limitations and fairness**. Limitations and fairness are not discussed enough. In particular, can generated data be biased with respect to conditional metadata? \n\n- **Design choices**. It would be great to have more details on the design choices of the decoder and $\\epsilon_\\theta$. Do other architectures significantly affect the performance of the method?"
            },
            "questions": {
                "value": ">The sampling time column ranks models by their speed, with lower numbers indicating\nfaster sampling\n\nPlease add details.\n\n\n> We set the dimensions of output features in this way as we used\nthe mean-squared (MSE), binary cross entropy (BCE), and cross-entropy (CE) in the Pytorch package.\n\nIt looks disconnected from the main text. Could you clarify?\n\n>L: 249 More details are deferred in the Appendix J.\n\nAppendix J does not provide additional details on the selection of a decoder architecture.\n\n> LL: 171-173\n\nDoes the current model generate data for textual metadata? If so, it would be great to provide more experiments/demonstrations. If not \u2013 it may be worth removing this example."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to use latent diffusion models to generate synthetic time series tabular data.\nThey propose to combine VAE with DDPM and term the proposed method TimeAutoDiff.\nThey claim many advantages of this proposal and provide some experiments."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "NA"
            },
            "weaknesses": {
                "value": "**Originality.** DDPM or VAE for time series generation is not new. Most techniques used are also from prior works.\n**Soundness.** It\u2019s well-known that VAE is considered 1-step DDPM. I find the motivation and reasoning in `line 144-154` very unconvincing. It\u2019s still unclear why DDPM is good at handling time series data, why VAE, as a special case is DDPM, can bring more to the table. Also, I don\u2019t think citing unpublished and prior venue\u2019s rejections as SOTA method is convincing, e.g., TSGM in `line 156`. \n**Clarity.** The captions of Fig 2,3 provide zero information. Many notations are introduced without definition, e.g., x_cont in `line 192`. Overall clarity can be further improved.\n**Significance.**  The significance is hard to parse due to limited clarity. At best, I find it lacking due to its assembly nature. Additionally, I question the correctness of the experimental evaluation."
            },
            "questions": {
                "value": "Why are most experiments on regression datasets? How can these validate heterogeneous properties of \u201ctabular\u201d time series data?\n\nThe experiments lack convincing evidence. Please include additional summary statistics of the generated data and compare with baselines. For instance, stock data (nasdaq100) are known for key summary statistics like volatility and moving averages. Demonstrating that the proposed method closely matches these statistics would provide stronger support for its efficacy. Can you provide a generated example of your trained nasdaq100 model?\n\nWhy is Section 4.2 titled \u201cUtility Guarantees\u201d when there is no theoretical analysis provided?\n\nThese weaknesses are not meant to be exhaustive. I believe they are sufficient to show that this paper is not ready for publication."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The proposed work presents a framework of latent diffusion models for heterogeneous time series data. The framework utilizes an encoder within a variational autoencoder (VAE) to project both discrete and continuous features of time series data into a continuous latent space. Subsequently, a diffusion model is employed to model the distribution of the latent codes of the time series data. The encoder employed in the VAE is an RNN, while the diffusion model is DDPM[1]. Notably, the denoising network within DDPM employs cyclic encoding to incorporate time-stamp information into the time series and adopts a bi-directional RNN architecture. The proposed latent diffusion model can also be conditioned on meta-data for incorporating conditional information. Evaluation results on unconditional generation demonstrate that the proposed approach generates high-quality synthetic data and outperforms existing approaches.\n\nReferences\n\n[1] Ho, Jonathan, Ajay Jain, and Pieter Abbeel. \u201cDenoising diffusion probabilistic models.\u201d Advances in neural information processing systems 33 (2020): 6840-6851."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The work has the following notable strengths:\n1. The works did comprehensive evaluation on unconditional and conditional generation tasks, taking both lower-order and higher-order statistics into account. The proposed approach shows superior performance over existing approaches on both tasks across the datasets considered in the work. In addition the work also did comprehensive over different hyper-parameters variations and ablations to reveal the impact of different components to the model.\n2. The presentation quality of the work is good and the proposed method of the work is easy to follow."
            },
            "weaknesses": {
                "value": "The work exhibits several significant weaknesses:\n1. The work lacks innovative methodology. Similar ideas for tabular data, which arguably encompass time-series data, have been explored in existing works [1]. Despite some crucial technical differences between the two works, including the design of the denoising network $\\epsilon_\\theta$ and the approach to the latent space, the high-level framework of both works is both a latent diffusion model that projects structured data to a latent space and uses a diffusion model to model the distribution of latent codes.\n2. The majority of the experiments are limited to unconditional generation of time-series data, and it is unclear how the proposed model can be readily adapted to tasks with more practical applications, such as forecasting and imputation.\n\nReferences\n\n[1] Zhang, Hengrui, et al. \u201cMixed-type tabular data synthesis with score-based diffusion in latent space.\u201d arXiv preprint arXiv:2310.09656 (2023)."
            },
            "questions": {
                "value": "1. I would like to inquire about the rationale behind employing RNN as the encoder for time-series data and capturing temporal dependencies between features across various timestamps. The temporal dependencies between features can also be captured by the denoising network of the diffusion model. Therefore, is it necessary to utilize an RNN to capture temporal dependencies when encoding the data?\n\n2. I would encourage the author to provide a more comprehensive discussion and emphasize the values or practical significance of the tasks the model is evaluated on, specifically unconditional generation and time-variant meta-data conditioned generation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces TimeAutoDiff, a time series tabular data synthesizer that combines the Variational Auto-Encoder (VAE) and Denoising Diffusion Probabilistic Model (DDPM). It effectively manages heterogeneous features and enhances data generation fidelity and temporal dependencies. The model integrates a latent diffusion framework with a specialized VAE, improving performance in high-dimensional settings. It supports applications such as missing data imputation, privacy, and interpretability, and demonstrates superior results compared to existing models in generating synthetic time series data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is easy to read and the experiments are comprehensive."
            },
            "weaknesses": {
                "value": "One concern is its current inability to generate interpretable results. In many practical applications, especially in high-stakes fields like finance and healthcare, stakeholders must clearly understand how models make decisions. The lack of interpretability can lead to skepticism and reluctance to adopt the model, as users may hesitate to trust a system whose inner workings are opaque. This limitation could hinder the model's applicability in scenarios where understanding the rationale behind generated data is crucial for decision-making; Another point is the pure focus on continuous data: While the method demonstrates good performance with continuous data, its methods are primarily tailored for this data type. This focus raises concerns about the model's effectiveness when dealing with heterogeneous datasets that include categorical features. The inability to seamlessly integrate and process diverse data types could restrict the model's usability in real-world applications where such heterogeneity is common; another point is the dependence on latent space reduction: The paper notes a performance drop when the feature sizes increase, which necessitated a reduction in the latent space dimension. This reliance on dimensionality reduction to maintain performance raises concerns about the model's scalability and robustness. If the model's effectiveness is sensitive to the dimensionality of the latent space, it may struggle to perform well in high-dimensional settings without careful tuning."
            },
            "questions": {
                "value": "1, How can the model be made more robust to high-dimensional feature spaces without relying heavily on latent space reduction?\n\n2, What modifications or extensions to the framework would be necessary to effectively handle heterogeneous datasets that include both continuous and categorical features?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}