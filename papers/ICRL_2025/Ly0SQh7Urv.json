{
    "id": "Ly0SQh7Urv",
    "title": "Automated Red Teaming with GOAT: the Generative Offensive Agent Tester",
    "abstract": "Red teaming assesses how large language models (LLMs) can produce content that violates norms, policies, and rules set during their safety training. However, most existing automated methods in the literature are not representative of the way humans tend to interact with AI models. Common users of AI models may not have advanced knowledge of adversarial machine learning methods or access to model internals, and they do not spend a lot of time crafting a single highly effective adversarial prompt. Instead, they are likely to make use of techniques commonly shared online and exploit the multi-turn conversational nature of LLMs. While manual testing addresses this gap, it is an inefficient and often expensive process. To address these limitations, we introduce the Generative Offensive Agent Tester (GOAT), an automated agentic red teaming system that simulates plain language adversarial conversations while leveraging multiple adversarial prompting techniques to identify vulnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by prompting a general-purpose model in a way that encourages reasoning through the choices of methods available, the current target model\u2019s response, and the next steps. Our approach is designed to be extensible and efficient, allowing human testers to focus on exploring new areas of risk while automation covers the scaled adversarial stress-testing of known risk territory. We present the design and evaluation of GOAT, demonstrating its effectiveness in identifying vulnerabilities in state-of-the-art LLMs, with an ASR@10 of 97% against Llama 3.1 and 88% against GPT-4 on the JailbreakBench dataset",
    "keywords": [
        "red teaming",
        "adversarial machine learning",
        "adversarial examples",
        "attacks on language models"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Ly0SQh7Urv",
    "pdf_link": "https://openreview.net/pdf?id=Ly0SQh7Urv",
    "comments": [
        {
            "summary": {
                "value": "This work offers a novel way to simulate and automate how human users may use publicly available jailbreak methods to circumvent the safety training of LMs in long multi-turn conversations. The proposed algorithm (GOAT) is successful and competitive for five tested models, cost-effective ro tun (e.g., compared to fine-tuning or gradient-based attacks), and easily updated as it relies on in-context examples."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The proposed automated red-teaming algorithm is novel and elegantly simple, making it scalable and usable for future works \n- The shown results demonstrate the potency of the attack for all five tested models\n- The methodology is clearly communicated and the paper is generally well-presented and easy to follow\n- Compared to other, more involved attacks, the proposed algorithm is cost-effective and can be expanded with newer jailbreak prompts"
            },
            "weaknesses": {
                "value": "- The proposed algorithm is limited to \"conversational\" jailbreak prompts. While it offers a significant degree of automation and value to current model red-teaming, this attack approach might be easier defended against with more advances in safety training. Also, it is unclear if the proposed algorithm can be used to discover novel attacks. For example, would it work to ask the attack model to evaluate previous conversations and come up with a novel attack?\n\n- The authors state that the paper's goal is not to compare models comprehensively; they only want to focus on a prominent model family for open-source and closed models. However, I think the results of this work could be significantly strengthened by adding more capable models in the chosen families (GPT-4o, or larger than 8B Llamas) and, ideally, adding a third model family like Claude or Gemini.\n\n- The described methodology/experimental setup is vague on what LM was used for the attacker model. It is only described as a \"generic helpful-only language model\" and that it was \"exposed\" to numerous samples of being overcompliant instead of replying safely. The vagueness used to describe the model limits reproducibility. The contribution of this work to the ICLR community would be improved by either clearly stating how this model was created (base model, number of parameters, training data, fine-tuning data, ...) or at least stating some information that can be used to estimate the model capability (number of parameters and benchmark performance) necessary for the proposed algorithm to work and acknowledging that it is a proprietary model.\n\n- The line widths, axis labels, axis ticks, plot legend, and plot titles are too small and unreadable in Figure 3. In addition, the font in Figure 1 is also small and hard to read."
            },
            "questions": {
                "value": "- Please address the concerns I see in the weaknesses. I am willing to increase my score if you extend your analysis to more models or demonstrate generalized usability in another way (e.g., discovering new attacks etc.)\n\n- Please add a trigger warning for offensive content to your abstract! The presented harmful examples in the main body and appendix of a racist text and a nazi-glorifying poem require such a warning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces GOAT (Generative Offensive Agent Tester), an automated system for red teaming language models through multi-turn adversarial conversations. Unlike previous approaches that focus on finding single adversarial prompts, GOAT simulates how real users interact with LLMs through extended conversations. The system uses a general-purpose LLM as an attacker that can dynamically employ multiple jailbreaking techniques while maintaining a conversation. The authors demonstrate GOAT achieves high attack success rates with only 5 queries."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Novel approach that addresses a realistic threat model - focusing on multi-turn conversational attacks rather than single-prompt attacks\n- Well-engineered system that effectively combines multiple existing attack techniques in a dynamic way\n- Strong empirical results showing high success rates with relatively few queries\n- Extensible framework that can easily incorporate new attack techniques"
            },
            "weaknesses": {
                "value": "- Evaluation only considers two model families (llama and gpt) and one baseline. Claude should be tested, as it is the most adversarially robust model. While I understand the lack of multi-turn baselines, it is possible to convert single-turn baselines such as PAIR to multi-turn by applying the attack multiple times. \n- Limited analysis of why certain attack combinations work better than others - more insight into patterns of successful attack sequences would be valuable\n- No evaluation or discussion of defenses [1,2]. To my knowledge all defenses were proposed for single-turn settings, would they still be effective in this setting?\n\n[1] Robey, A., Wong, E., Hassani, H., & Pappas, G.J. (2023). SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. ArXiv, abs/2310.03684.\n[2] Zhou, A., Li, B., & Wang, H. (2024). Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks. ArXiv, abs/2401.17263."
            },
            "questions": {
                "value": "What makes the LLM select certain attacks over other ones, especially on the first iteration? Some analysis/discussion on this would be helpful\n\nHow does this automated attack compared to human multi-turn jailbreaks [1]? This is a major motivation in the paper but GOAT is not compared against human attacks. How do the attack strategies differ than how humans conduct multi-turn jailbreaks?\n\n[1] Li, N., Han, Z., Steneker, I., Primack, W., Goodside, R., Zhang, H., Wang, Z., Menghini, C., & Yue, S. (2024). LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet. ArXiv, abs/2408.15221."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes the Generative Offensive Agent Tester (GOAT), an automated red-teaming system that conducts multi-turn jailbreaking. GOAT allows the attacker to dynamically adapt its strategy based on the conversation context. It includes an action set with seven distinct attack strategies and is easily extendable. The paper demonstrates GOAT's efficacy on the JailbreakBench dataset, outperforming the Crescendo method. Notably, GOAT achieves a high attack success rate within just five conversation turns."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The motivation for simulating human red-teaming is well-justified.\n- The method offers strong flexibility due to its extendable set of attack strategies.\n- This approach leverages the strengths of existing attack methods (seven prior strategies) along with the flexibility of multi-turn conversation.\n- It achieves a high ASR within 5 turns, which is impressive."
            },
            "weaknesses": {
                "value": "- The method does not introduce particularly novel insights; the concept of multi-turn adversarial interaction has been explored in previous works.\n- The paper primarily compares GOAT to only one existing method, Crescendo, without assessing its performance against other red-teaming or adversarial attack approaches.\n- The paper mentions limitations of the attacker model\u2019s capabilities, such as context window length and prompt-following strength, but it lacks empirical analysis of how these factors impact GOAT\u2019s performance."
            },
            "questions": {
                "value": "- The paper describes the Attacker LLM as a \"general-purpose helpful language model.\" However, the exact model used is unspecified. Key questions: What is the model type? What are the specific implementation details? How might the choice of model, particularly its capabilities and prompt-following strength, impact the results?\n- Could you elaborate on the rationale for selecting smaller Llama models and larger GPT models as target models? How might GOAT perform on other popular models with varying sizes or levels of safety alignment? (Note: It is reasonable to use those models since we are always facing the limited resources.)\n- Is GOAT\u2019s multi-round approach strictly superior to other methods? How would it perform compared to alternative attack types, such as Tree of Attacks [1], or other single-round and multi-round adversarial strategies?\n- Given that the Crescendo attack requires more conversation turns to reach its peak performance, would GOAT still maintain an advantage if the number of turns were increased? How does this compare to other methods, and are there trade-offs associated with the number of queries?\n\n[1] Tree of Attacks: Jailbreaking Black-Box LLMs Automatically"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There is potential for misuse of this method, and it may be challenging to mitigate these risks."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents GOAT, an automated agent-based red teaming framework designed for testing the robustness of large language models (LLMs) by simulating adversarial multi-turn conversations. GOAT leverages a general-purpose LLM as an attacker that utilizes multiple pre-defined adversarial prompting strategies to elicit policy-violating responses from target models. The methodology incorporates a step-by-step reasoning structure inspired by chain-of-thought prompting, allowing GOAT to adapt dynamically during conversations. The framework was evaluated against state-of-the-art models like GPT-4-Turbo and Llama 3.1, demonstrating a significant attack success rate (ASR@10) and outperforming existing multi-turn approaches such as Crescendo."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The framework achieved impressive results, with ASR@10 rates of 97% for Llama 3.1 and 88% for GPT-4-Turbo on the JailbreakBench dataset, surpassing existing methods.\n\n2. GOAT is extensible, incorporating seven distinct adversarial strategies, such as Persona Modification and Response Priming, to simulate a variety of red teaming approaches."
            },
            "weaknesses": {
                "value": "1. The paper didn't mention which specific large language model (LLM) was used as the attacker in the GOAT framework.\n\n2. The maximum number of conversation turns was capped at five due to context window limitations. This constraint might impact the effectiveness of certain attack strategies and limits the scope of adversarial escalation in longer dialogues."
            },
            "questions": {
                "value": "1. How does GOAT handle longer multi-turn conversations beyond the current limit of five turns, and what modifications would be needed to extend this capability?\n\n2. What criteria were used to select the attacker LLM, and how might changes to this choice impact GOAT\u2019s performance?\n\n3. How adaptable is GOAT to LLMs that are continuously updated with new safety measures? Would it require significant re-training or additional adversarial strategy development?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces GOAT, an agent framework to automatically jailbreak models with multi-turn attacks. The paper evaluates GOAT on JailBreakBench and demonstrates its ability to jailbreak several frontier closed-source and open-source models. GOAT outperforms another multi-turn automated jailbreaking method, Crescendo."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper provides an automated agent to jailbreak the most powerful systems. Additionally, the type of attacks the system generates are more realistic and therefore close in distribution to the attacks from real users. Hence this agent is more valuable for auditing than other jailbreaking techniques.\n- The agent achieves SOTA and is automated, suggesting that this is a reasonable auditing technique for current models.\n- The agent should scale as more jailbreaks get discovered.\n- The paper is well-written and the qualitative examples + figures are helpful for understanding.\n\nIn summary, the paper is well-executed (good quality), well-written (good clarity), and tackles the challenge of identifying multi-turn jailbreaks (reasonable significance)."
            },
            "weaknesses": {
                "value": "While this paper is useful, I think it misses an important opportunity to help develop community norms or guide research on multi-turn jailbreaks. This miss is amplified by the fact that this paper appears one of the first automated multi-turn jailbreaking methods. Concretely, the paper:\n- **Lacks conceptual insights.** I gained almost no additional information from reading the paper compared to reading only the abstract + Figure 1. Specifically, I recommend \n     -  Explore the design space of multi-turn jailbreaking agents by ablating different components of the agent. While the authors do not need to incorporate all of following suggestions, I think exploring a few of these design decisions could make the paper much more informative for future work. I've italicized the top three imo: \n         - *How much does the reasoning part of the prompt help? Can you ablate this part of the prompt?*\n         - *Does using a stronger LLM improve ASR?*\n         - What is the impact of training data on the attacker LLM?\n         - Is the attacker LLM still able to succeed with fewer jailbreaks?\n         - How does the ASR scale with number of jailbreakers?\n         - *Are there some jailbreaks that are more important s.t. removing them dramatically reduces performance?*\n         - Does passing evaluation of success to an external judge help (at the expense of more API calls)?\n         - Is it possible to have the attacker LLM be a heavily safety-trained model?\n         - Does increasing the diversity of initial strategies tried by the LLM improve the ASR? \n    - Explore the realism of the data generated by GOAT. Understanding the following questions would help understand the qualitative nature of the method more precisely:\n         - Given the human-written distribution of mutli-turn jailbreaks in [1], how similar are the jailbreaks generated by GOAT?\n         - Could training to refuse the jailbreaks from GOAT serve as a defense method against human jailbreaks?\n         - Is it possible to make GOAT attacks match the distribution of human attacks?\n- **Has weak empirical rigor.** I think some of the experiments / writing could be clarified, and I hope the authors will address this. In particular,\n    - What exactly is the attacker LLM? How was it trained? The paragraph from L296-304 is extremely vague. \n    - Does GOAT work with other attacker LLMs? Can I just feed the attacker prompt directly into GPT-4o or Claude-3.5-Sonnet and have it act as my attacker LLM?\n   - GOAT appears to be plateauing at a faster rate than Crescendo. Will the two ASRs @ 10 match with enough turns? What does the graph looks like with 10 turns of dialog?\n- **Lacks originality.** This is mostly tied to the first weakness, so I think this is more of a minor issue. Given that there aren't many scientific insights in the paper, I feel like the paper itself isn't very novel. In particular, it feels like a multi-turn agent with CoT prompting applied to jailbreaks. Although I think prompting is very valuable, there doesn't seem to be any particularly new ideas in this paper. It would be cool if the authors could consider implementing an additional 'feedback stage' to see if the agent could generate its own jailbreaks given examples of previously failed trajectories / strategies. \n\nI'm willing to adjust my score to a 6 if some of the weaknesses are addressed, especially the lack of empirical rigor.\n\n[1] Li, Nathaniel, et al. \"Llm defenses are not robust to multi-turn human jailbreaks yet.\" arXiv preprint arXiv:2408.15221 (2024)."
            },
            "questions": {
                "value": "Suggestions:\n- Lines L89 - 103 are verbose and don't seem to have any connection. Are they simply motivating that multi-turn jailbreaks should be dynamic? I feel like it would be helpful to shorten the exposition in the intro and instead focus on the insights gained from developing GOAT.\n- The text in Figure 1 is small and too hard to read. It would be nicer if you shortened some of the text and had bigger blocks that highlighted the overall strategy of the agent. \n- In L195, say where the 7 attacks were sourced from or how they were developed.\n- Replace the algorithm with a table that highlights the different parts of the attacker prompt.\n- Include more qualitative scenarios. I thought the red / blue tables were very helpful but there were only two included."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}