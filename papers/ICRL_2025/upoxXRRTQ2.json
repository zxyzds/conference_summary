{
    "id": "upoxXRRTQ2",
    "title": "The impact of allocation strategies in subset learning on the expressive power of neural networks",
    "abstract": "In traditional machine learning, models are defined by a set of parameters, which are optimized to perform specific tasks. In neural networks, these parameters correspond to the synaptic weights. However, in reality, it is often infeasible to control or update all weights, particularly in biological systems like the brain, where evidence suggests that only a subset of synaptic weights are modified during learning. Motivated by these insights, we theoretically investigate how different allocations of a fixed number of learnable weights influence the capacity of neural networks. Using a teacher-student setup, we introduce a benchmark to quantify the expressivity associated with each allocation. We establish conditions under which allocations have `maximal' or `minimal' expressive power in linear recurrent neural networks and linear multi-layer feedforward networks. For suboptimal allocations, we propose heuristic principles to estimate their expressivity. These principles extend to shallow ReLU networks as well. Finally, we validate our theoretical findings with empirical experiments. Our results emphasize the critical role of strategically distributing learnable weights across the network, showing that a more widespread allocation generally enhances the network\u2019s expressive power.",
    "keywords": [
        "subset learning",
        "theoretical neuroscience",
        "expressive power",
        "neural networks",
        "recurrent neural network"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "This paper takes the first step to theoretically analyzing the expressive power of different allocation strategies in subset learning.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=upoxXRRTQ2",
    "pdf_link": "https://openreview.net/pdf?id=upoxXRRTQ2",
    "comments": [
        {
            "summary": {
                "value": "The paper investigates the conditions under which 'allocations' have maximal or minimal expressive power in linear RNN and linear MLPs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Originality: I think allocation is an interesting concept though it seems to significantly overlap with the concept of subnetwork in lottery ticket hypothesis. \n\nClarity: The paper is clearly written."
            },
            "weaknesses": {
                "value": "Quality: Studying expressivity of RNNs/MLPs from the perspective of linear solvable equations require strong assumptions such as 'the distribution $\\mathcal X$ is such that any drawn square matrix is invertible' (l197) and I'm having trouble extracting insights from the theorems. \n\nSignificance: Increasing the percentage of learnable weights leads to more expressivity seems trivial to me (maybe there are insights I'm not seeing from the paper?) so I'm not quite sure what we can learn from the submission."
            },
            "questions": {
                "value": "a. Could the authors comment on the practical implications of the theorems? \n\nb. L147 is joint not conditional? If so notation is confusing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper develops a theoretical framework for analyzing the expressive power of neural networks when only allowing a subset of the network to learn. The authors focus on a student-teacher setup and defines expressivity as the likelihood that the student can replicate the teacher\u2019s outputs. Using this metric, this work analyzed linear RNNs and deep linear feedforward networks, and evaluated the expressive power of different allocation strategies, then extended their analysis to one-layer ReLU feedforward networks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper presents several theoretical results about the expressive power of linear networks when allowing a subset of the network to learn, and concludes that spreading the learnable weights in different parts of the network allows for stronger expressive power."
            },
            "weaknesses": {
                "value": "1.\tThere are many ways to evaluate allocation strategies, including generalization performance, capabilities of transfer learning and continual learning, etc. These are the interesting problems that the authors used to motivate their work. However, this paper particularly focuses on how allocation strategy affects the expressivity of a network, in a very specific setting, limiting the scope and potential insights that could be obtained from this work.\n2.\tThe result of the work is restricted to a student-teacher framework in the sense that the central definition of the match probability is defined based on a student-teacher framework, where the student and the teacher have matching architectures, making it difficult to extend the results to real-world applications even heuristically. The numerical results shown in this paper is also limited to this synthetic task only.\n3.\tVarious assumptions are dispersed and showed up in various places within the text, it would be better if the authors could explicitly list their assumptions when stating the theorems.\n4.\tI did not check all proofs carefully but multiple proofs in this paper is written in a very informal way, making it difficult to judge the validity of the proofs."
            },
            "questions": {
                "value": "1.\tBoth the student and the teacher weights are drawn from i.i.d. Gaussian, have you considered correlated weights in the teacher, or introducing structures into the teacher weights? \n2.\tThe central result that it is better to allocate the learnable weights throughout the network. Is this consistent with existing findings about learning in the brain?\n3.\tWhen stating that \u201cthe probability of finding at least one solution increases with the number of polynomials\u201d, does the degree of the polynomials play a role?\n4.\tIn the proof of theorem B1, do you assume that W is diagonalizable? The sentence \u201cthe eigenvector xxx is a space\u201d does not make sense to me. Furthermore, I do not see why it is important to prevent the network from being degenerate, it is still a valid function of the input sequence, and it\u2019s simply that the degree of freedom is less than the amount of effective linear weights."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides a theoretical investigation into the distribution of learnable weights in neural networks, drawing inspiration from brain mechanisms. The study measures network expressivity as a function of these learnable weights, examining various network architectures and validating the theory through experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Congratulations on your submission to ICLR! I commend the authors on producing this great piece of research. Below is my detailed review of the work:\n\n**Originality**\nThis paper introduces a theoretical framework for analyzing network expressivity as a function of learnable weight allocation. Its originality lies in drawing parallels with brain mechanisms and adopting a fresh theoretical approach.\n\n**Quality**\nThe paper demonstrates high quality with well-substantiated theoretical findings that are clearly explained and motivated. Key strengths include:\n- Thorough and detailed theoretical results, effectively show that predictions from smaller models generalize well to non-linear, shallow ReLU architectures.\n- Theoretical analysis is rigorously structured, adding robustness to the findings.\n- The mathematical derivations appear accurate and sound.\n\n**Clarity**\nThe paper is well-organized and accessible.\u00a0\n\n- The use of a simplified linear network model as a toy example helps to build intuition, significantly enhancing readability.\u00a0\n- The figures are clear, informative, and effectively support the main content.\n\n**Significance**\nThis work is a valuable contribution to the machine learning research community, especially in deepening our understanding of model expressivity as a function of trainable weights in the context of increasing training costs. Additionally, the proposed theoretical framework may offer valuable insights for the neuroscience community."
            },
            "weaknesses": {
                "value": "**Originality**\nNo notable weaknesses in originality.\n\n**Quality:**\nThe paper is of high quality, yet there are areas where further clarity and detail could strengthen its impact:\n\n- The paper would benefit from a more explicit discussion of its limitations. While valuable, the theoretical approach may not fully capture some empirical phenomena seen in practical neural networks. Highlighting these potential divergences between theory and real-world observations would add useful nuance.\n- A clearer connection to applications in neuroscience would enhance the paper\u2019s relevance to that field.\n\n**Clarity**\nThe clarity of the paper is generally high but could be improved in specific areas:\n\n- An introductory figure would be helpful, particularly for readers unfamiliar with the topic. For instance, a visual summary of the student-teacher setup could make the content more accessible.\n- Including a discussion in the introduction on the growing computational demands\u2014now requiring substantial resources\u2014would add timely context to the study.\n- Strengthening the link between neural networks and brain mechanisms would clarify the relevance of the approach.\n- Developing Theorem 3.3 further, potentially in the appendix, would also improve clarity.\n\n**Significance**\nThe significance of the study could be enhanced with a few adjustments:\n- The benchmarks used primarily involve toy models, which may limit the generalizability of the findings. Extending the analysis to larger models could increase the impact and relevance of the results."
            },
            "questions": {
                "value": "-  Are there any constraints preventing from running the experiments with gradient descent? Is it correct to think that if you did you could have access to results with larger networks?\u00a0\n- Do you have any intuition on how these insights might scale to more complex networks? Specifically, if larger models contain an increasing number of polynomials increases the probability to find a solution , does this imply that weight distribution becomes less significant, given that the transition would be so abrupt that the network would almost never (with a probability tending to zero) have no solution or expressivity zero ? \n- (This may be outside the intended scope, but I\u2019m curious.) I understand that the approach abstracts away the specifics of the learning algorithm. However, I\u2019m interested in how your findings might depend on the network\u2019s initialization under a particular learning algorithm (e.g., the scale and relative scaling of layers). Given that different initializations can influence the learning rates of certain weights more than others\u2014effectively controlling which parts of the network learn more actively\u2014how does the network\u2019s expressivity vary under these conditions?\n\nI hope this review is helpful for the further development of the paper. I encourage the author to continue this research and incorporate the feedback provided."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors investigate the capacity of neural networks under constrained conditions where only a fixed number of weights are plastic and can be changed. The authors present theoretical work and then relate the theory to empirical simulations in simple networks. \nThis work is quite interesting in that it forces us to think that not all weights are equal, that not all weights are needed, and that astute ways of choosing which weights to change can have a large impact in the capacity and efficacy of a neural network."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "This work introduces an interesting question about the possibility to modify only certain weights in a network and ask how the choice of plastic weights impacts neural network capacity.\nThere has not been much work along these lines. Perhaps one related idea relates to continual learning. Upon learning new things, it may be useful to strategically decide which weights to change more. One instantiation of this is Kirkpatrick et al PNAS 2017 (the authors cite other work in this direction as well).\nThe work presents both theory and empirical simulations. \nThe work introduces a benchmark that would be useful for the field to compare different potential mechanisms to allocate learnable parameters."
            },
            "weaknesses": {
                "value": "The abstract states that \u201cbiological systems like the brain where evidence suggests that only a subset of synaptic weights are modified during learning\u201d. This is a grandiose claim that is not substantiated by any references. I strongly suggest that they remove this claim. Otherwise, the authors should provide compelling evidence about this statement. The introduction provides a better assessment: \u201cthe scale of this process in the brain is unclear\u201d. \nThe experiments presented are based on rather simple NNs. It is not clear how the conclusions extrapolate to more complex NNs. But this point does not distract from the beauty of the work presented here."
            },
            "questions": {
                "value": "The potential connections between the work presented here and weight protection strategies in continual learning is probably worth exploring further (though this is not a requirement by any means for the current work)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}