{
    "id": "wTLc79YNbh",
    "title": "TimeKAN: KAN-based Frequency Decomposition Learning Architecture for Long-term Time Series Forecasting",
    "abstract": "Real-world time series often have multiple frequency components that are intertwined with each other, making accurate time series forecasting challenging. Decomposing the mixed frequency components into multiple single frequency components is a natural choice. However, the information density of patterns varies across different frequencies, and employing a uniform modeling approach for different frequency components can lead to inaccurate characterization. To address this challenges, inspired by the flexibility of the recent Kolmogorov-Arnold Network (KAN), we propose a KAN-based Frequency Decomposition Learning architecture (TimeKAN) to address the complex forecasting challenges caused by multiple frequency mixtures. Specifically, TimeKAN mainly consists of three components: Cascaded Frequency Decomposition (CFD) blocks, Multi-order KAN Representation Learning (M-KAN) blocks and Frequency Mixing blocks. CFD blocks adopt a bottom-up cascading approach to obtain series representations for each frequency band. Benefiting from the high flexibility of KAN, we design a novel M-KAN block to learn and represent specific temporal patterns within each frequency band. Finally, Frequency Mixing blocks is used to recombine the frequency bands into the original format. Extensive experimental results across multiple real-world time series datasets demonstrate that TimeKAN achieves state-of-the-art performance as an extremely lightweight architecture.",
    "keywords": [
        "Kolmogorov-Arnold Network; Time Series Forecasting"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wTLc79YNbh",
    "pdf_link": "https://openreview.net/pdf?id=wTLc79YNbh",
    "comments": [
        {
            "summary": {
                "value": "The paper presents TimeKAN, a Kolmogorov-Arnold Network (KAN)-based model for long-term time series forecasting, designed to handle complex multi-frequency patterns in real-world data. Traditional models struggle with mixed frequencies, but TimeKAN addresses this with a three-part architecture: Cascaded Frequency Decomposition to separate frequency bands, Multi-order KAN Representation Learning to model each band\u2019s specific patterns using adaptable polynomial orders, and Frequency Mixing to recombine frequencies effectively. Experiments show that TimeKAN achieves superior accuracy and efficiency compared to existing models, making it a robust, lightweight solution for complex TSF tasks."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "**Exploratory Application of KAN to Time Series Forecasting**: The paper attempts to introduce Kolmogorov-Arnold Networks (KAN) into time series forecasting, using multi-order polynomial representations to handle the complexities of different frequency components. While KAN has not been widely applied in this area, this effort demonstrates its potential flexibility in data fitting and offers an alternative approach to traditional MLPs.\n\n **Comprehensive Experimental Design**: The paper includes experiments across various time series datasets, such as weather, electricity, and energy data, covering diverse scenarios. Additionally, it conducts ablation studies to examine the effects of each module. These experiments help to assess TimeKAN\u2019s performance and may provide a reference point for further research."
            },
            "weaknesses": {
                "value": "**Lack of Innovation**: The primary contribution of this paper is the integration of the Kolmogorov-Arnold Network (KAN) into time series forecasting, yet the work does not introduce novel methods or substantial breakthroughs in methodology. While the inclusion of KAN is somewhat new, other components, such as frequency decomposition and mixing, are mature techniques, and the paper does not propose innovative applications or enhancements to these. Overall, this work appears more like a combination of existing technologies rather than a genuinely innovative study.\n\n**Absence of Comparison with Cutting-Edge Models**: The experiments lack direct comparisons with state-of-the-art models, especially those in high demand for time series forecasting, such as large language models (LLMs) and foundation models. Given current research trends, these models have become widely adopted benchmarks. Without such comparisons, the effectiveness of the proposed method remains unclear, especially as the improvements presented are relatively limited compared to advancements in mainstream approaches.\n\n **Reliance on KAN, a Model with Limited Validation**: The foundation of TimeKAN is the KAN model, which has not yet been widely validated or accepted. Its theoretical correctness and practical effectiveness remain uncertain, which casts doubt on the reliability and generalizability of TimeKAN as a whole. If there are inherent issues with KAN, the predictive performance and stability of TimeKAN could be compromised, making the paper's conclusions less convincing.\n\n**Insufficient Analysis of Computational Efficiency**: While the paper claims that TimeKAN is more lightweight than existing methods, it lacks an in-depth analysis of its actual computational efficiency, especially compared to more mainstream and optimized time series models. Additionally, there is no quantification of the computational cost associated with KAN\u2019s multi-order polynomial calculations when handling long-sequence data. Given that many time series tasks require efficient real-time computations, focusing solely on parameter reduction does not adequately demonstrate TimeKAN\u2019s advantage in computational efficiency; the absence of data on inference speed and computational cost undermines its practical applicability.\n\n\n**Focus on Single-Task Performance Rather Than Generalized Representation**: A dominant trend in time series modeling now follows the approach of large language models (LLMs) to develop foundation models and leverage self-supervised representation learning. This approach enables generalization across various tasks and domains, ultimately aiming for a \u201cone-fit-all\u201d solution. However, this paper remains focused on improving single-task performance in time series forecasting (TSF), which may be of limited value in light of the broader goals of the field. Furthermore, the improvements reported in the experimental results are relatively modest, and without statistical significance testing, it remains unclear if these gains are truly meaningful or could simply be attributed to random variation."
            },
            "questions": {
                "value": "**To my knowledge, KAN itself has not been formally accepted, meaning it has not undergone rigorous peer-reviewed validation. If KAN\u2019s theoretical foundation is later found to be flawed, would this impact the validity of this paper?** \u2013 If the underlying theory or structure of KAN is later shown to have limitations or inaccuracies, would the overall reliability of TimeKAN be compromised? Has the author considered this risk, and are there alternative solutions in place?\n\n**In the experimental section, this paper does not compare TimeKAN with current state-of-the-art models (such as large language models or foundation models), making it difficult to assess its actual performance** \u2013 Without direct comparisons with these advanced models, can TimeKAN demonstrate a significant advantage? If the authors believe TimeKAN holds particular value in computational efficiency or predictive accuracy, could more data be provided to quantify this advantage?\n\n**A major trend in time series research is developing foundation models, inspired by large language models, to generalize across domains and tasks. However, it is unclear if TimeKAN\u2019s current design can support such robust representation learning** \u2013 Can TimeKAN truly compete with established frameworks like Transformers in terms of generalization and adaptability? If not, have the authors considered alternative approaches to enhance TimeKAN\u2019s structural robustness and flexibility for broader applications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose a time series forecasting method based on KAN. TimeKAN uses frequency decomposition and KAN to effectively capture temporal correlations of the data. Experiments show the effectiveness of TimeKAN based on real-world datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. A vivid introduction and related work section to explain the background of time series forecasting and KAN.\n2. A clear figure to illustrate the overall framework of TimeKAN. In the methodology section, all components are detailed. \n3. Good ablation study to test the effectiveness of the different components of the model."
            },
            "weaknesses": {
                "value": "1. The author does not explain why TimeKAN does not perform well in the Electricity dataset. It would be helpful if the authors could provide potential reasons or hypotheses for why TimeKAN underperforms on the Electricity dataset specifically. \n2. From Table 4, I do not see a huge increase with KAN compared to MLP models. Generally, if these results are similar, mostly, KAN is much slower than MLP. It would be good to see runtime comparisons between KAN and NLP implementations. Additionally, if KAN is slower than MLP in practice, it would be beneficial for authors to discuss more reasons why we prefer KAN over MLP. \n3. For the look-back window, the authors do not compare TimeKAN with other models. For most models, when the prediction length is fixed, the prediction accuracy will increase as the look-back window increases. It is beneficial to provide a comparative analysis of TimeKAN's performance with varying look-back windows compared to other baseline models. This would provide a more comprehensive evaluation of TimeKAN's capabilities relative to existing methods.\n4. For baseline methods, it is better to choose more frequency-based (such as FreTS) methods since frequency decomposition is a key contribution."
            },
            "questions": {
                "value": "1. See weaknesses.\n2. Could you provide a short theoretical analysis about why in some cases in time series forecasting, KAN is better than MLP?\n3. For Table 2, why not include the electricity dataset?\n4. For KAN, you mentioned that the Kolmogorov-Arnold representation theorem states that any multivariate continuous function can be expressed as a combination of univariate functions and addition operations. Could you explain more about how it can capture multivariate correlations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "TimeKAN is a time series forecasting model that combines frequency decomposition, representation learning, and mixing. It first uses a moving average to separate high and low frequencies, creating multi-level sequences that are embedded into a high-dimensional space. Cascaded Frequency Decomposition (CFD) blocks progressively isolate each frequency band. The Multi-order KAN Representation Learning (M-KAN) blocks use Kolmogorov-Arnold Networks to capture temporal patterns within each frequency band independently. Finally, the Frequency Mixing blocks recombine these decomposed bands to restore the original sequence, which is then used for forecasting through a linear layer"
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1\tClarity and Structure: The paper follows a logical flow from problem statement to conclusions, making complex ideas accessible.\n\t2.\tThorough Background: A strong review of related work provides valuable context, situating the contribution within the field.\n\t3.\tDetailed Experiments: Comprehensive experiments across multiple datasets support the model\u2019s performance claims, with ablation studies highlighting component effectiveness.\n\t4.\tFocused Writing: The paper stays on topic, avoiding unnecessary details and maintaining focus on the core contribution"
            },
            "weaknesses": {
                "value": "Depth in Explanation: The methodology section could offer more detail on complex components like Kolmogorov-Arnold Networks for greater accessibility."
            },
            "questions": {
                "value": "What if we split the frequency band to more layers (more than 3 for example ). Will it increase performance ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores a method for decomposing mixed frequency components into distinct single-frequency components to improve time series forecasting accuracy. The proposed approach, called TimeKAN, is based on the Kolmogorov-Arnold Network (KAN). TimeKAN's process consists of three key components: (1) Cascaded Frequency Decomposition (CFD) blocks, which use a bottom-up cascading approach to obtain series representations for each frequency band; (2) Multi-order KAN Representation Learning (M-KAN) blocks, which capture and represent specific temporal patterns within each frequency band; and (3) Frequency Mixing blocks, which recombine the separated frequency bands back into the original series format.\n\nThe study demonstrates that TimeKAN outperforms several state-of-the-art forecasting methods, including Autoformer, FEDformer, and iTransformer, by achieving lower MSE and MAE across multiple time series datasets such as Weather, ETTh2, and ETTm2."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. Figure 1 is beautifully designed and provides an intuitive overview of each component in the new TimeKAN method, as well as how they connect.\n\n2. The study makes effective use of large-scale datasets and performs comparisons with a variety of other methods (including CNN-based and Transformer-based models), demonstrating the advantages of TimeKAN. The model is also tested across different prediction lengths, and for datasets where performance is less optimal, the paper offers thorough explanations and detailed insights.\n\n3. The analysis delves into several key components of TimeKAN, such as Upsampling, Depthwise Convolution, and Multi-order KANs. I especially appreciated this section, as it not only establishes that TimeKAN outperforms other deep learning methods but also shows that each individual component of TimeKAN is optimally designed."
            },
            "weaknesses": {
                "value": "1. Section 3.2 appears somewhat disorganized. While the overall logic is clear, the expression could be refined for clarity. Additionally, more mathematical details and background should be provided, which can be included in the appendix.\n\n2. If possible, please add more data to Table 5 in Section 4.3. Supplement it with the performance of other methods in Table 1 on parameters (params) and MAC across these six datasets."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}