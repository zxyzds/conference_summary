{
    "id": "eIFHoPsIkw",
    "title": "Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting",
    "abstract": "Vision foundation models pre-trained on massive data encode rich representations of real-world concepts, which can be adapted to downstream tasks by fine-tuning. However, fine-tuning foundation models on one task often leads to the issue of concept forgetting on other tasks, and this issue is exacerbated by the typically limited data for fine-tuning. Recent methods of robust fine-tuning aim to mitigate forgetting of prior knowledge without affecting the fine-tuning performance. Knowledge is often preserved by matching the original and fine-tuned model weights or feature pairs. However, such point-wise matching can be too strong, without explicit awareness of the feature neighborhood structures that encode rich knowledge as well. We propose a novel regularization method Proxy-FDA that explicitly preserves the structural knowledge in feature space. Proxy-FDA performs Feature Distribution Alignment (using nearest neighbor graphs) between the pre-trained and fine-tuned feature spaces, and the alignment is further improved by informative proxies that are generated dynamically to increase data diversity. We show in end-to-end fine-tuning experiments that Proxy-FDA significantly reduces concept forgetting, and we find a strong correlation between forgetting and a distributional distance metric (in comparison to L2 distance). We further demonstrate Proxy-FDA's utility in both few-shot (based on prompt tuning) and continual fine-tuning settings, where we achieve consistent gains over the corresponding baselines.",
    "keywords": [
        "Proxy-FDA",
        "robust fine-tuning",
        "concept forgetting",
        "vision foundation model"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "Forgetting-free fine-tuning of vision foundation models via Proxy-based Feature Distribution Alignment (Proxy-FDA)",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=eIFHoPsIkw",
    "pdf_link": "https://openreview.net/pdf?id=eIFHoPsIkw",
    "comments": [
        {
            "summary": {
                "value": "This work proposes a regularization method to mitigate the forgetting problem during fine-tuning foundation models by aligning the nearest neighbor graphs between the pre-trained and fine-tuned feature spaces. The authors experimentally demonstrate that this method outperforms baseline models on both few-shot image classification and continual learning tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.This work is clearly presented and easy to understand.\n\n2.Experimental results show that this method outperforms some baseline methods."
            },
            "weaknesses": {
                "value": "1.why does aligning feature distributions help alleviate forgetting? The essence of feature subspace alignment is to make the feature distributions learned by the pre-trained and fine-tuned models more consistent. In the extreme case, this alignment could lead to a collapse back to the original state. \n\n2.While the authors argue for the advantages of Proxy-FDA, it would be beneficial to include a theoretical foundation or empirical evidence that explains why this approach is expected to outperform traditional methods.\n\n3.One potential concern is that the comparative methods presented by the authors do not include recent works from 2024 onward, which could result in an incomplete or potentially inaccurate assessment of the method's performance.\n\n4.The author\u2019s motivation is difficult to discern from Figure 1, which could benefit from clearer visual cues or annotations to enhance interpretability. Figure 2 could be improved to more effectively present the technology in a clear and understandable manner."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Proxy-FDA, a feature-based regularization method for fine-tuning vision foundation models (e.g., CLIP, DINOv2) on downstream tasks. Unlike point-wise regularization, FDA preserves the structure-wise knowledge by aligning the feature space of the pre-trained and fine-tuned models, incorporating local neighborhood structures. Proxy-FDA further generates synthetic features (proxies) to increase data diversity. The method is evaluated across end-to-end fine-tuning, few-shot tuning, and continual learning scenarios."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "-  The method is well-motivated. \n- The evaluation seems comprehensive and the results appear impressive."
            },
            "weaknesses": {
                "value": "- Some recent related works lack discussion in the related work section but are directly compared in the experimental section, such as CLIPood and PromptSRC, which are both regularization-based fine-tuning approaches. Including a discussion of these methods in the related work section would provide better context for readers.\n- Several technical details require further clarification. (1) Details about the network architecture and training parameters for the proxy generator are absent. (2) The hard class mining strategy specifies $n=4$ (Line 777), but it is unclear how this approach is used in few-shot cases (e.g., 1- or 2-shot) (3) The scalar $s=0.4$ is not introduced or explained in Section 3.2 but it is discussed and analyzed in hyper-parameter analysis (e.g., Figure 5)\n- Important implementation details are not provided. (1) Detailed specifications for hype-parameters (e.g., temperature $\\tau$, bias $b$, loss coefficient $\\lambda$) are missing. (2) The specific $K$ value for each dataset is also absent. Providing these details would benefit the community. (3) Implementation details are missing for the continual fine-tuning setting (e.g., Table 8), making it difficult to confirm evaluation fairness across methods.\n- Evaluation should be improved. (1) How does FDA/Proxy-FDA perform when compared to previous SoTA FD-Align in its official evaluation setting (e.g., compare APE-T+FD-Align and APE-T+Proxy-FDA by training the model on ImageNet and testing on ImageNet V2/Sketch) ? (2) CLAP [1], another regularization-based linear probing method, seems more efficient as it does not require a validation set for extensive hyper-parameter selection. Including a comparison with CLAP would clarify the strengths and limitations of the proposed method.\n- Ablation study on batch size. Since the FDA relies on batch-based feature alignment, it is essential to analyze the impact of varying batch sizes.\n\n[1] A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models. In CVPR, 2024"
            },
            "questions": {
                "value": "- Please find the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a new approach to mitigate concept forgetting in model fine-tuning (robust fine-tuning) by building on existing feature-matching methods. Specifically, the authors aim to align the feature structure by regularizing the feature space using k-nearest neighbors (KNN) within each batch. They also propose generating proxies from the data to preserve diversity across datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Robust fine-tuning is a highly active and valuable area of research with significant relevance and potential.\n- The motivation for the proposed method is strong, as preserving data structure during feature matching is both reasonable and innovative.\n- The writing is clear, and the approach is presented in a way that is easy to follow."
            },
            "weaknesses": {
                "value": "- Motivation and Selection of Distribution Alignment Method: The motivation for using distribution alignment through feature matching, specifically with regularization on nearest-neighbor features, is not fully explained. This approach resembles knowledge distillation; therefore, clarification on how this method differs from distillation between the original and fine-tuned models would strengthen the argument.\n\n- Design of Equation 2: Equation 2 appears to follow a sigmoid loss structure but replaces traditional labels (+1, -1) with a weight, $w_{ij}$\u200b. Is this weight defined as $w_{ij}=\\cos\u2061(x_i,x_j)$? An additional explanation of the rationale and comparative benefits of this design choice would be helpful.\n\n- Clustering with KNN: The method clusters features in each batch using KNN, which may not adequately group samples with the same or similar labels, especially during fine-tuning. The clustering might benefit from label-based constraints, either including or excluding samples based on label proximity, to enhance feature alignment.\n\n- Proxy Generator Motivation: The purpose and function of the proxy generator remain unclear. Specifically, the reason for incorporating an attention layer is not well-justified, as it is unclear how it assists in integrating information across positive and negative feature sets. An ablation study or additional analysis of the generator architecture would clarify its contribution and effectiveness.\n\n- (minor) Some descriptions need clarification: \n Line 167: What is meant by \"low task loss $L_{task}$\u200b\"?\n Line 168: Please clarify \"whilst preventing concept forgetting on any target dataset $D\\neq D_{ft}$\"."
            },
            "questions": {
                "value": "See the weakness above.\nGenerally, this is a solid paper. However, some remaining concerns should be further discussed. I may adjust the score according to the response from the authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Proxy-FDA, a structural regularization method designed to mitigate concept forgetting in fine-tuning large pre-trained vision models. Proxy-FDA achieves this by preserving local feature neighborhood structures in the pre-trained feature space through\nFeature Distribution Alignment (FDA), leveraging nearest neighbor graphs. Additionally, Proxy-FDA employs dynamic proxy generation to increase data diversity, which enhances feature alignment. The method is evaluated in few-shot and continual fine-tuning contexts,\ndemonstrating strong performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Preserving the neighbor structure within the pre-trained feature space beyond class semantics is novel and may bring some insight to the model adaptation community.\n2. State-of-the-art performance validates the effectiveness of the proposed approach in reducing concept forgetting.\n3. This paper is well-written and organized."
            },
            "weaknesses": {
                "value": "1. As the output pooling weights are softmax-normalized and convex combinations are expected, figure 2(c) may be problematic. Neither the synthesized proxy P+ nor P- lies within the convex hull of corresponding neighbors.\n2. In L261, why do both the positive set and the negative set lack diversity? How does hard-class mining lead to the semi-hard nature of X^-? And how does this nature alleviate \u201climited diversity\u201d? More explanations for the above questions should be provided."
            },
            "questions": {
                "value": "1. How is the number of proxy positives n^{p+} and negatives n^{p-} determined? Is it dynamic across different batches?\n2. How can the generated proxies within the convex hull help shape the decision boundary?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}