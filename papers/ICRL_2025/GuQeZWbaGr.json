{
    "id": "GuQeZWbaGr",
    "title": "AnyView: Few Shot Personalized View Transfer",
    "abstract": "Fine-tuning generative models for concept driven personalization have witnessed tremendous growth ever since the arrival of methods like DreamBooth, Textual Inversion etc. Particularly, such techniques have been thoroughly explored for style-driven generation. Recently, diffusion models have also demonstrated impressive capabilities in view synthesis tasks, setting the foundation for exploring view-driven generation approaches. Motivated by these advancements, we investigate the capacity of a pretrained stable diffusion model to grasp ``what constitutes a view\" without relying on explicit 3D priors. Specifically, we base our method on a personalized text to image model, Dreambooth, given its strong ability to adapt to specific novel objects with a few shots. Our research reveals two interesting findings. First, we observe that Dreambooth can learn the high level concept of a view, compared to arguably more complex strategies which involve fine-tuning diffusions on large amounts of multi-view data. Second, we establish that the concept of a view can be disentangled and transferred to a novel object irrespective of the original object\u2019s identity from which the views are learnt. Motivated by this, we introduce a learning strategy, AnyView, which inherits a specific view through only one image sample of a single scene, and transfers the knowledge to a novel object, learnt from a few shots, using low rank adapters. Through extensive experiments we demonstrate that our method, albeit simple, is efficient in generating reliable view samples for in the wild images. Code and models will be released.",
    "keywords": [
        "View Transfer",
        "Diffusion",
        "Generative"
    ],
    "primary_area": "generative models",
    "TLDR": "We address the task of learning a view from an image sample and then transfer it to novel objects.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=GuQeZWbaGr",
    "pdf_link": "https://openreview.net/pdf?id=GuQeZWbaGr",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces AnyView, a method for transferring specific viewpoints to novel objects using few-shot learning with diffusion models. Building upon DreamBooth, the authors demonstrate that a pretrained stable diffusion model can learn the high-level concept of a view from a single image without relying on explicit 3D priors. They use Low-Rank Adaptation (LoRA) to separately learn the view and object concepts and then merge them to generate images of the novel object from the desired viewpoint. Experiments on the DTU dataset and natural images show that AnyView can efficiently generate reliable view samples, outperforming several methods in certain metrics."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This work studies a new task, learning a view from a single image.\n- The authors provide evidence that diffusion models can learn and transfer high-level concepts like views, which could have broader implications for generative modeling."
            },
            "weaknesses": {
                "value": "- The method may struggle with complex scenes involving multiple objects or significant occlusions, as noted in the limitations.\n- The results shown in experiments are all simple or celebrities, which SDXL model is highly possible to have priors. Using some unique objects will make the method more convincing. \n- The reference images of rare object shown in appendix already have diverse views, some are even very similar to the view to learn. This undermines the effectiveness of the method. \n- The method is simple combination of DreamBooth and ZipLoRA, which has limited innovation. \n- The figures in the paper are very low-resolution and unlcear, making it difficult to read."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel approach for enhancing personalization in visual tasks through a method that integrates personalization and Low-Rank Adaptation (LoRA) fine-tuning for view transfer. The authors address the \"forgetting\" problem prevalent in personalization models when learning multiple IDs simultaneously, providing a clear motivation for their proposed architecture. Through thorough experimentation, the method demonstrates competitive performance results against various baselines, including methods that have been trained on more data. The findings are substantiated across multiple evaluation metrics."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is clearly articulated, with a well-defined methodology supported by diagrams, effectively motivating the proposed architecture and addressing the \"forgetting\" problem in personalization models.\n- The method demonstrates notable performance improvements over comparable baselines, achieving competitive results against specialized novel view synthesis (NVS) methods despite relying on a simpler approach with fewer input images. The authors provide comprehensive results across various metrics, including SSIM, PSNR, and LPIPS, highlighting the effectiveness of their approach in diverse scenarios.\n- The authors provide a thorough analysis of design choices, including the impact of background in training data and object complexity on the method\u2019s performance.\n- The originality of the approach is noteworthy, particularly in its application of personalization and LoRA fine-tuning for view transfer tasks."
            },
            "weaknesses": {
                "value": "- **Evaluation Metrics:** The evaluation method used, particularly regarding masking, are unclear and warrant further explanation.\n- **Qualitative Results:** Some views selected for qualitative results appear standard, especially in Figure 5 with Zero-1-to-2 comparison visualizations. Additionally, there is a lack of qualitative results for other baselines- although some are presented in Figure 13 in the appendix, they are not super convincing as Anyview results seem to lack fine-grained viewpoint consistency in view synthesis, while also lacking some fine-grained detailed in complex tasks. \n- **Background Sensitivity:** The performance of the method is inconsistent, particularly sensitive to background context. Structured backgrounds (e.g., forests, tables) yield more reliable results than uniform backgrounds (e.g., grass). This sensitivity suggests the model may learn unintended spatial cues, potentially hindering its generalizability across diverse environments and highlighting a brittle understanding of 3D relationships.\n- **Evaluation Gaps:** The paper lacks an analysis of failure cases and their characteristics, which is critical for understanding the limitations of the approach.\n- **Technical Limitations:** There is no clear strategy for managing multiple objects or complex scenes within the proposed framework."
            },
            "questions": {
                "value": "1. What is the precise definition of \"masking\" in the context of your evaluation methodology?\n2. How is the view generated from a single image, considering that views are inherently relative? Is it relying on the semantic prior of \"canonical pose\" of diffusion models? Would incorporating multiple images of the same view but with different objects improve performance? Additionally, how does the method address viewpoint ambiguity?\n3. What does Figure 7 represent? Are all the images inference attempts to generate an ideal view? The inconsistencies in these images raise questions regarding their interpretation.\n4. What specific failure cases or limitations can you identify in your approach?\n5. Is it feasible to extend this method to effectively handle multi-object scenes, and if so, how would that be accomplished?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a method for generating personalized view transfers in image synthesis using diffusion models through the DreamBooth setup combined with low-rank adaptation (LoRA). More specifically, it proposes AnyView, a system that disentangles \"view\" as an independent high-level concept and leverage the personalization/transfer-learning algorithm of Dreambooth to extract this concept. Experiments on DTU and DreamBooth dataset validate its performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper brings forward a novel approach by defining the concept of \"view\" in image synthesis. By the Dreambooth personalization algorithm, the method requires minimal 2D data and no 3D information to extract such concept.\n2. The experimental setup is robust, with evaluations on datasets such as DTU and comparisons some benchmarks, demonstrating AnyView's performance.\n3. The paper provides a comprehensive analysis to give insight on how the setup like LoRA, background, complex object would affect the approach."
            },
            "weaknesses": {
                "value": "1. Evaluation Metrics. My strongest concern is the use of evaluation metrics to assess the method's performance. While traditional metrics like SSIM and LPIPS are presented, they may not entirely capture the visual quality of the view-transfered novel images. Incorporating perceptual metrics or human study evaluations is critical for this few-shot synthesis technique, which can assess the fine details of the generated images.\n2. Dependence on Background Cues. While the model effectively generates personalized views, it relies significantly on spatial cues from backgrounds, as discussed in Sec. A.5. This dependence may limit its adaptability to varying or abstract backgrounds.\n3. Complex Multi-Object Scenes. While the paper acknowledges that the method faces challenge in maintaining view consistency and object separation for the multi-object scenes, more experimentation on such cases could further validate the method's efficacy and potential modifications to address this complexity."
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper is about customized image synthesis of an object with a specific view point. They leverage the DreamBooth customization approach in order to learn object and view features. The model requires few-shot examples for custom object and view synthesis."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Based on the claims of the paper \n1. The method does not require any prior 3D data, to synthesize a custom object with a specific view.\n2. It is a few-shot approach, requiring 3-4 examples for object and only 1 example for view to synthesize the image.\n3. They showcase the background features importance for view synthesis."
            },
            "weaknesses": {
                "value": "1. Weak scientific novelty. They just use the DreamBooth to learn 2 different concepts leveraging the LoRA method for it.\n\n2. Limited quantitative and qualitative results. The authors provide results using only 1 dataset (DTU MVS), which is not enough to prove the claims. Also, they can be cherry picked, especially, when the authors manually estimate the transformation of competitor model (Zero123) for the comparison.\n\n3. Not convinced with the claim of disentangling the view synthesis from the concept. Not enough evidence are shown in the paper. The model is able to learn concepts (whether they are style, object, or scenes). There is a more chance that it attempts to learn the scene, not the view. That is why in almost all results, any image synthesized from the top camera view, object is faced to the camera, while, the reference object does not have that position. Also, the background is always changed in synthesized images (probably because of LoRA), which works against the claim of view disentanglment. \n\n4. Poor qualitative results. The synthesized images are not appealing qualitatively (Fig 5, Fig 12). The customized objects have poor fidelity.\n\n5. Lack of details how the training is done, how the quantitative metrics are computed."
            },
            "questions": {
                "value": "I am willing to reconsider my review if authors address the weaknesses mentioned above and convince me otherwise."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}