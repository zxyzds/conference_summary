{
    "id": "wUbum0nd9N",
    "title": "On Calibration of LLM-based Guard Models for Reliable Content Moderation",
    "abstract": "Large language models (LLMs) are exposed to significant risks due to their potential for malicious use. Existing studies have developed LLM-based guard models designed to moderate the input and output of threat LLMs, ensuring adherence to safety policies by blocking content that violates these protocols upon deployment. However, limited attention has been given to the reliability and calibration of such guard models. In this work, we empirically conduct comprehensive investigations of confidence calibration for 9 existing LLM-based guard models on 12 benchmarks in both user input and model output classification. Our findings reveal that current LLM-based guard models tend to 1) produce overconfident predictions, 2) exhibit significant miscalibration when subjected to jailbreak attacks, and 3) demonstrate limited robustness to the outputs generated by different types of response models. Additionally, we assess the effectiveness of post-hoc calibration methods to mitigate miscalibration. We demonstrate the efficacy of temperature scaling and, for the first time, highlight the benefits of contextual calibration for confidence calibration of guard models, particularly in the absence of validation sets. Our analysis and experiments underscore the limitations of current LLM-based guard models and provide valuable insights for the future development of well-calibrated guard models toward more reliable content moderation. We also advocate for incorporating reliability evaluation of confidence calibration when releasing future LLM-based guard models.",
    "keywords": [
        "Content Moderation",
        "LLM-based Guard Models",
        "Calibration",
        "Safety"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wUbum0nd9N",
    "pdf_link": "https://openreview.net/pdf?id=wUbum0nd9N",
    "comments": [
        {
            "summary": {
                "value": "This work examines how calibration can affect and potentially improve LLM-based guard models. The study finds most guard models are poorly calibrated, especially under jailbreaking attacks, but that off the shelf calibration methods don't seem to provide consistent calibration benefits (at least not as tested)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ This is an extensive and comprehensive evaluation of a variety of calibration methods, models, and datasets. It clearly took a lot of effort.\n+ Overall, this highlights the poor calibration of guard models and  could be good motivation for more consistent methods. The eval harness could also be the building block for an evaluation suite to test some improved methods down the line."
            },
            "weaknesses": {
                "value": "+ The effect sizes for some of the calibration methods on some datasets were rather small and it\u2019s a bit unclear what the potential variance here is. It would be great to have a table 3 with confidence intervals, though given the size of the table this might be a heavy lift. This becomes more important if there was sampling at test time and less important if there was greedy decoding (see question below).\n+ It\u2019s not clear that batch calibration didn\u2019t work as well because of the selection of the batch of unlabeled samples or other choices. I don\u2019t think this is a major issue, but should be called out more prominently as a limitation. Similarly for other decisions and methods. This is done to some extent, but a standalone limitations section might be warranted. Overall there seem to be some major caveats for design decisions as to the generalizability of the takeaways from the calibration method study."
            },
            "questions": {
                "value": "+ Were the guard models sampled from or is it using greedy decoding?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper examines the reliability and calibration of guard models based on LLMs used in content moderation. The authors highlight that while these models achieve strong classification performance, they often produce overconfident and poorly calibrated predictions, particularly when faced with adversarial attacks like jailbreaks. Through an empirical evaluation of nine guard models across 12 benchmarks, the study identifies significant calibration issues, such as overconfidence and inconsistent robustness across different response models. To address these challenges, the paper explores post-hoc calibration techniques, demonstrating the effectiveness of temperature scaling for response classification and contextual calibration for prompt classification. The findings underscore the importance of improving model calibration to enhance the reliability of guard models in real-world content moderation scenarios."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper conducts an in-depth analysis of LLM-based guard models and explores potential design improvements."
            },
            "weaknesses": {
                "value": "Overall, I think the paper does a good job of presenting the \"what\"\u2014namely, the findings and results\u2014but it would benefit from delving deeper into the \"why,\" or the reasons behind these observations (Sec 6 has some \"understanding\" results, but seems to be distantly related). Without this, the paper feels more like a dataset and benchmark track submission (which ICLR does not specifically have) rather than a main track paper."
            },
            "questions": {
                "value": "Q1: For results on jailbreak prompts, the author said that \"The results demonstrate that the ECE for prompt classification is\ngenerally higher than that of response classification, indicating that guard models tend to be more reliable when classifying model responses under adversarial conditions. \" However, it's not clear whether the guard model is vulnerblae due to the jailbroken nature of the prompts, or due to some suprrious correlations (eg length, patterns). It will be great if the authors can explain or design experimetns to ablate such factors.\n\nQ2: It's interesting to see the variability in guard model performance across different response models (Table 2). However, it would be more insightful to understand the causes of these discrepancies. For example, why do all the models perform particularly poorly with Llama2's responses? Is there a qualitative or quantitative explanation for this?\n\nQ3: Regarding the calibration results in Table 3, the improvements appear relatively modest (e.g., at most around 2% ECE reduction). It would be helpful to contextualize how significant these improvements are. Additionally, it seems that contextual calibration (CC) and batch calibration (BC) sometimes degrade performance. Understanding the reasons behind this would provide valuable insights.\n\nQ4: Most of the evaluated models in Table 2 are open-weight models, so it\u2019s unclear how these findings would transfer to proprietary models like ChatGPT, Gemini, and Claude."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The proposed study conducts investigations of confidence calibration for 9 existing LLM-based guard models on 12 benchmarks in both user input and model output classification. The resultant findings are that these guard models are overconfident, are miscalibrated with respect to jailbreak attacks, and have limited robustness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Figure 1 is well thought out and easy to follow. sets the stage well\n- the experimental setup is solid, with a variety of benchmarks that are used in industry. In particular, the use of benchmarks for which typical statistics are provided for these guardrail models is smart. \n- the breadth of guardrail models used is admirable"
            },
            "weaknesses": {
                "value": "- discussion of limitations is lacking, would be interesting to see where the pitfalls of this approach are and how they could be improved."
            },
            "questions": {
                "value": "1. What are some of the limitations of this experimental setup? How do these limitations affect the resultant outputs and findings?\n2. Have you tried the setup on a larger model that is being prompted to act as a guardrail? It would be interesting as a comparison point to these guardrail-specific models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This papers conducts an analysis on the reliability and calibration of LLM-based guard models for safety moderation. The findings reveal that these guard models tend to be overconfident in predictions, show miscalibration when subjected to jailbreak attacks, and different response models also have different calibration performance. Based on these insights, the authors also propose some easy-to-implement methods to improve calibration."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper points out an important yet under-studied problem of (over) confidence in LLM safety guard models and analyzes the confidence calibration problem. \n- The evaluation covers a wide range of models and benchmarks for evaluation. \n- The work finds that lightweight approach like contextual calibration can be effective mechanisms for improving confidence calibration. \n- The paper is written clearly and the overall flow is easy to follow."
            },
            "weaknesses": {
                "value": "- The proposed calibration techniques does not show strong improvement on the ECE metrics, and in some cases even make the ECE score higher (Table 3). There is no statistical significance tests (eg. Multiple runs, variance, hypothesis testing) to show that the methods are indeed beneficial. \n- CC is primarily designed for binary or few-class settings, and the framework appears to be challenging to extend to a multi-class setup. The influence of a content-free token might not translate well to all classes, especially if some classes are rare or highly specialized. It will also be harder to interpret and apply, because the baseline bias captured from a content-free input could vary inconsistently across datasets. \n- The assumptions for the BC method are not realistic in actual LLM setting. It also may inadvertently adapt to an adversarial distribution shift. \n- For temperature scaling, the authors used XSTest as the validation set for optimization. However, since XSTest is a dataset for estimating over-refusal, there is likely bias in the temperature optimized on it. \n- The authors do not discuss the distribution of safe/ unsafe prompts in the dataset being studied. The ECE metric could be affected by dataset imbalance."
            },
            "questions": {
                "value": "- More discussion on the generalizability of the techniques.\n- Can you show the statistics for false positives and false negatives as well? Would be useful to know that which models are showing what types of over-confidence behavior.\n- What is the \u201cunsafe\u201d token used for experiments in the discussion section?\n- Could you provide more explanation or intuition on the calibration trade-offs across models? Why certain methods are better for response classification while some work better for prompt classification. \n- How does the size / training data characteristics affect calibration? It would be better to understand why certain methods work better for certain scenario."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}