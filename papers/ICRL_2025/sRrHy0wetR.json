{
    "id": "sRrHy0wetR",
    "title": "On Re-Encoding Short-Term Memory of Large Language Models in Conversations",
    "abstract": "Large language models (LLMs), such as GPT-4, are adept at generating coherent and fluent responses within conversational contexts. \nHowever, there has been a paucity of comprehensive research exploring LLMs to dynamically update their knowledge in response to corrections of misinformation provided by users during dialogue sessions. \nFrom the cognitive psychology perspective, such an adaptive process is akin to memory re-encoding (MRE), which entails the modification of previously stored information in human memory, typically for rectifying inaccuracies.  \nIn this paper, we present a novel framework termed Knowledge Editing In Conversation (KEIC), along with an accompanying dataset, devised to assess the efficacy of LLMs in emulating the MRE process in an in-context setting.\nThrough in-depth investigations, we observe that the contemporary LLMs exhibit a modicum of proficiency in this task.\nTo enhance their in-context MRE abilities, we propose a structured strategy to handle the information update for LLMs in a multi-turn conversation.\nWe demonstrate that our approach is effective and suggest insights for research communities in this emerging and essential issue.",
    "keywords": [
        "LLM",
        "misinformation correction",
        "zero-shot self-correction"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We present the KEIC task for LLMs to update their knowledge based on user corrections, construct a 1,781 human-labeled dataset under this framework, and propose a structured approach, including a theoretical algorithm for self-correction.",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=sRrHy0wetR",
    "pdf_link": "https://openreview.net/pdf?id=sRrHy0wetR",
    "comments": [
        {
            "title": {
                "value": "Response to T49M (cont.)"
            },
            "comment": {
                "value": "## Additional Results\n> (1) Since the KEIC problem is a reflection of how good LLMs are at attending to their in-context memory, it warrants analysis in terms of the length of the input. For example, does the update % change with longer distance between the $T_u$ (update turns) and $T_i$ test turns. Similarly, does the update % vary with longer distance between $T_e$ (error turns) and $T_u$?\n\nAs another reviewer asks a similar question, we will analyze this later.\n\n> (2) Some of the correction templates improve coreference resolution at the update step while other don't. How would the results look for one template vs. the other? On that note, is there one or more templates that consistently stay in the top-1,3,5 results?\n\nIt really depends on the settings. In this paper, we have {OTC, Verification, Recall} x {CAM, CBA} x {3 MTurk Responses}.\nHere we list the roughly scanned trend (**not sorted by performance**) of GPT-3.5 (0125): \n\nOTC: 2, 3, 4, 6, 8, 9.\n\nVerification: 5, 7, 10, 11, 12.\n\nRecall: 2, 3, 4, 6, 8, 9.\n\n> (3) It would be great to see results from some of the latest open-source models such as LLaMA 3.1, OlMo, and perform analysis on the attention weights is possible.\n\nWe report the open-source Gemma-2 LLM (2B, 9B, and 27B) in this paper (the release date is Jun 2024). Since we conducted LLaMA 2 before LLaMA 3 was released, we chose not to use LLaMA 3.1 (highly similar). However, we could test LLaMA 3.1 in some experiments and report the results in the Appendix after the anonymous period.\n\n## Confusing Jargon\n\n> The authors have introduced some terms that may be prohibitive in understanding the results correctly. For instance, the reported metric is 'Update %' in Figure 5. It is unnecessary since it has already been made clear that the metric is simply the accuracy of the Yes/No question. I would suggest that the authors stick to 'Accuracy' to reduce confusion. \n\n**We use \"Update\" instead of Accuracy (Acc) in Lines 308-311 because we also report how many percent of data that an LLM stick to its original knowledge (\"No Update\")**, given that the user corrects the false statement. Hence, we use \"Update\" (there is no opposite term of \"No Acc\" from our knowledge).\n\n> Another example is the use of 'Recall' for a method, which is a frequently used term for evaluation metrics. Using this term as a method makes for confusing reading. \n\nThe term \"recall\" has long been used in psychology, which can be dated back to 1885 (as stated on the Wiki page). We include the work of (Bartlett, 1995) in Section 3 under the Recall method (in Line 250).\n\n> Another term is 'Previous Phase' which really refers to the 'Irrelevant turns'.\n\n**These turns are not always \"irrelevant\" in CoQA**, as stated in Lines 185-186, 255-257, and Appendix D (the initiative of the IC-MRE algorithm). Hence, renaming this could be more misleading from this perspective.\nMoreover, the CoQA paper also states that their dataset needs to answer the question based on the previous question/answer (coreference, or, incremental aspect), not the passage only. Please refer to Figure 2 in their paper.\n\nOn the other hand, since we use $\\mathbf{T_i}$ to denote the test phase (we name it from the $i$-th question's perspective; $\\mathbf{T_t}$ looks weird), it would also contradict the \"irrelevant phase\" (if we use the first letter of irrelevant, which is \"i\").\n\n## Questions\n\n> Who do you think that MRE is a standalone problem that should be studied and not merely a different sub-perspective of the conversational memory problem that could be potentially solved, for example, by using RAG + external memory module or better long-context modeling approaches?\n\nSee the 4th paragraph in the Introduction. Some cases are easy enough and using RAG would overkill this task (see Figure 1). \n\nThough using RAG + external memory could potentially solve this problem, it still persists if an LLM is not trained with such adaptability. There are other drawbacks when using these methods (see Lines 93-96).\n\nPlease also refer to Lines 930-936 in Related Work (for future dataset construction):\n* In such cases, to align with the user's updated knowledge, we highlight that the chatbot sometimes even needs to contradict its previous in-context response to ensure the conversation remains accurate and coherent (see Figure 1). We hypothesize that these conversational datasets, although aiming to improve an LLM\u2019s consistency and reduce self-contradiction is of paramount importance, may hamper its adaptability$-$an emerging issue of contemporary LLMs. In light of this, balancing between the two seemingly paradoxical yet highly correlated tasks during training would be one of the key challenges and opportunities for future work.\n\n> Suggestion to separate CBA and CAM results in figures to allow the reader to consider both settings individual in terms of the four proposed methods.\n\nWe can add both CBA and CAM results in Figure 13 on page 30 (the current version only reports the best for now)."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer T49M"
            },
            "comment": {
                "value": "## Limited Generalizability of Results\n\n> The correction templates adopted by the authors (and listed in Appendix B) indicate that the knowledge updates that are present in the collected dataset are of a single kind i.e., fixing errors in previously reported events. This leaves out some other highly-plausible knowledge editing scenarios in conversations such as (1) temporal change in knowledge i.e., previous knowledge was true at that time but it has now evolved to a different state (such as change in Presidents) (2) evolving of user preferences or opinions (such as a change in music taste or other user preferences like name) etc. This also stems partly from the scope of the CoQA dataset which has a format of two participants discussing a passage from news or fiction. \n\nThe CoQA dataset indeed does not contain temporal change in knowledge and user preferences because the discussion is often fiction or news. However, existing datasets encompass this like MultiWoZ datasets are quite noisy (lots of annotation errors) in their papers (please refer to the evolvement of MultiWOZ 2 to 2.4 and other conference/workshop papers addressing this issue). As for the others, they generally are not long-term dialogue $-$ rather easy in this sense $-$ so we exclude them to test our framework comprehensively.  \n\nAs a result, we choose to use the CoQA dataset in this paper (long-term dialogue + the false fact lies within a long (user) utterance).\n\nMoreover, our pilot study in the MultiWoZ dataset when only using \"exact match\" (i.e., strict string matching) shows that the result still underperforms. Applying this to other datasets, or proposing an effective way to evaluate the open domain question (without losing explainability) is also a promising avenue for future work.\n\nNote that even if the CoQA dataset does not contain temporal change in knowledge and user preferences, our correction templates can also be applied to these two facets:\n\n* Given (1) old fact = \"I like jazz\" (``jazz``) and (2) new fact = \"I like classical music\" (``classical music``), we can directly use these templates and say (a) \"Actually, I like classical music\" and (b) \"Oh, I'm sorry. Should have been \"I like classical music,\" not \"I like jazz.\"\n\n* Incidentally, if those datasets do not label the entire sentence (i.e., ``jazz`` and ``classical music``), then it is more natural for us to say (a) \"Actually, classical music\" and (b) \"Oh, I'm sorry. Should have been classical music, not jazz.\"\n\n* For another example of changing the current president, it would be something like (a) \"Actually, Donald Trump is the U.S. president\" (label the entire sentence) and (b) \"Oh, I'm sorry. Should have been Donald Trump, not Joe Biden.\" (label the name only)\n\n> Further, the authors do not make the distinction between real-world facts and narrative fiction in this dataset. It is unclear from the results if it is hard to edit knowledge in an in-context manner if that knowledge is present in the parametric memory of the model (it is also entirely possible that GPT models are trained on the CoQA models and so this data exists in their parametric memory).\n\nAs the dataset is 1,781 and further partitioning the KEIC dataset would make fact and non-fact data < 1,000, we do not include this error analysis in this paper. However, if the reviewer is indeed interested in this question, we will provide some analyses later.\n\n## Doubts about the persistency of the KEIC scenario\n\n> Since this problem of models underperforming in the OTC scenario can be traced back to the issue of LLMs not attending to their context correctly, I am unsure if this will continue being a problem if it can simply be fixed by better positional encodings, training data, attention architectures in oncoming versions of LLMs.\n\nThis task is an unsolved, emerging issue of LLMs and several training methodologies are proposed (e.g., DPO and KTO), it is still unknown whether applying these would solve the problem (though we believe using DPO could probably solve it). However, as we mention in the Related Work (see Lines 932-936), applying this should be cautious because this requires more advanced contextual understanding. It may be exploited by malicious users to produce harmful or counter-fact responses, as stated in the Ethics Statement. \n\nAs for the oncoming LLMs, we test a series of state-of-the-art GPT LLMs and demonstrate that the problem persists even in GPT-4 and GPT-4o LLMs (see Figure 6-a; also see Figure 13 on page 30)."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer MxUL"
            },
            "comment": {
                "value": "> 1. While the task setting is intriguing, it seems more like adversarial question answering in a long-term conversational context (i.e., questions that are designed to trick the model into providing wrong answer) than true memory editing. \n\nIt is more related to \"preference alignment\" because we want the LLM to update based on user correction. \n\nIt is adversarial if the correction utterance is somewhat \"misleading\" (see [1] and [2]), such as we ask \"the president of country A (suppose the answer is X),\" but later mention that \"the president of country B is changed to Y,\" then we ask \"Who is the president of country A?\" at the end of the dialogue (the answer is still X).\n\n[1] Jia and Liang, Adversarial Examples for Evaluating Reading Comprehension Systems, EMNLP 2017\n\n[2] Shi et al., Large Language Models Can Be Easily Distracted by Irrelevant Context, ICML 2023\n\n> Context-level memory editing would ideally involve managing an external database that saves and updates facts or events within the conversation [1][2].\n\nThis line of work is mentioned in Lines 93-96 and the Related Work in Lines 944-948 (see Appendix A).\n\nOur (1) \"OTC baseline without update phase\" and (2) \"oracle of Recall\" (see Lines 1512-1513 in Appendix H) can be viewed as having a (perfect) external system to update facts in the story (**because we directly replace the old fact in the story with a new one**, as stated in Line 315).\n\nAs for the Deletion, it can also be viewed as utilizing external systems to update facts because each turn is fed into the two modules (see Lines 260-261 and the corresponding footnote), which are initiated separately (see Appendix F for implementation).\n\n\n> 2. There is a discrepancy between the dataset and the examples in Figures 1 and 3. The figures suggest memory editing in a human-to-human conversational context, while the dataset primarily involves single-turn stories and human-AI QA-style probing. To reduce potential reader confusion, I recommend that the authors use real data examples as motivating illustrations, better aligning the figures with the dataset's actual structure.\n\n**The KEIC task and framework do not differentiate human-human or human-AI conversations and apply to both scenarios. Please refer to the definition in Section 2.**\n* Notably, we illustrate how to map Figure 1's conversation (human-human) into our framework in Section 2.3.\n* As for another type of conversation (human-AI), see Section 2.4 (also see Figure 4 for one of our real KEIC data).\n\n> 3. The authors used 15 different memory-updating prompts but only one prompt for memory probing. Exploring prompts that focus on recent information or limit historical context could yield more convincing results. -- \"Based on the recent correction from the user, how would you answer to this question: {question}?\" can be one example.\n\nThis could be used for future work, yet we directly use the same question in the CoQA dataset. \n\nFrom another perspective, given that the original question does not provide strings like \"Based on the recent correction from the user,\" this demonstrates the difficulty of the OTC baseline (because the questions sound more natural for humans), which further showcases that there is a significant performance boost using the Recall approach in GPT-3.5 and other LLMs in Figure 13 (on page 30)."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer yjK4"
            },
            "comment": {
                "value": "> 1. The organization and notations should be improved. The current readability is somewhat lacking. For example:\n> Line 147-148: the mixture of $r$ and r.\n\nPlease refer to the **same** Lines 147-148, as we explicitly state: Note that the relation \"r\" is in the normal text font, which should not be confused with the notation of fact \"$r$\" (in italics). \n\n> The arrangement of Figures is not very good. The texts on the $n$ page always require checking a figure that appears on the $n - 2/3$ page.\n\nCan the reviewer report which Figures exactly? We have checked that if a Figure is **FIRST** mentioned in text on page x, then that Figure appears on page y (where $y \\geq x$).\n\n* Please see an example of Figure 1: It is first mentioned on page 1, which is placed on page 2. Note that we also refer to Figure 1 on pages 2, 4, and 10.\n\n> 2. The proposed methods involve many additional processes. Three advanced methods cost much more tokens (Table 1. #Input Tokens) , which may subsequently worsen the latency.\n\nThis is the major drawback of most, if not all, CoT papers. \n\n* Please also refer to more time and token consumption in Appendix G.\n\n> 3. Experiments may lack performance evaluation on the general metrics. For example, using BLEU ROUGE to evaluate the quality of the generated dialogues.\n\nAs the question type is Yes/No, using exact match is sufficient in this paper. If open-domain questions are asked, then using these metrics would be suitable."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer t3Go (cont.)"
            },
            "comment": {
                "value": "## Weakness 4 (Notational Problems)\n\n> There is an excessive amount of notation in this paper. I would recommend that the authors rewrite sections in natural language and use notation sparingly.\n\n**This paper standardizes the dataset construction and the proposed KEIC framework, and the notation is consistently used for proving our IC-MRE algorithm**, so the math notation is necessary in Section 2 (also see Lines 954-959 in Related Work).\n\nIf the notations are omitted in the main content, then the dataset construction is not standardized, and we have to re-state the definition of fact and KEIC framework in Appendix D.\n\n> * Often there are two notations used for the same object. For instance, in 2.3 the authors refer to a turn with the $T_i$ notation, when they previously used the $(u_i, b_i)$ convention.\n\nThere are other papers that use \"turn\" to denote a pair of user and chatbot utterances (e.g., **the CoQA paper** and [1]), and, perhaps, after LLM-as-a-Judge [2], the term \"turn\" gradually shifts to this definition from our knowledge. \n\nWe formalize this task and use this as a standard. Consequently, defining each turn $T_i$ containing a pair of user-chatbot utterances $(u_i, b_i)$ is essential.\n\n> * Alternatives in notation are presented for readability (e.g., dropping the indices of $R$, $Q$, and $A$, but often hamper understanding and flow given how frequently they are proposed.\n\nThe subscript is dropped because *we are interested in asking the $i$-th question*, so including this $i$ in the main content is redundant. Also, see Lines 1149-1150 regarding the necessity of dropping indices (subscript of subscript).\n\n**This convention is also used in many papers.** For example, in [3], they write the following sentence in Section 4:\nThroughout this paper, our focus will be on states representing the last subject token $S$ of prompt $p_i$, so we shall abbreviate  $h_i^l = h_{[S]}^l(p_i)$, ...\n\nIf this is still a concern, we can move this abbreviation sentence to Appendix D.\n\n[1] Sun et al., Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models, ACL 2024\n\n[2] Zheng et al., Judging LLM-as-a-judge with MT-bench and chatbot arena, NIPS 2023\n\n[3] Meng et al., Mass-editing memory in a transformer, ICLR 2023\n\n## Weakness 5 (Clarity / Writing)\n\n> The writing is quite confusing, especially in the introduction. For instance, in the introduction, the authors claim that \"Recall\" is an effective technique for MRE, but Recall had not yet been mentioned nor explained and could easily be confused with the recall metric. \n\nThis can be fixed by adding \"method\" or \"\u00a73\".\n\n> Section headings could be much more descriptive. For instance, Section 2.2 is titled \"Fact\" when it contains information about the representation of facts in relation form. \n\nWe already mention this in the *first* paragraph of Section 2.\n* See Lines 119-120: Next, we define how to elicit knowledge stored in LLMs and formalize its form in a conversation.\n\n> Finally, the KEIC benchmark seems to be one of the main contributions, but most details about its construction and annotation are relegated to the Appendix.\n\nThe construction of our dataset is in Section 4.1 (which does *not mostly* appear in the Appendix) because we already define the \"effective\" in Section 2.2.\n* Despite this, how exactly a fact is \"effective **and fluent**\" depends on each individual. For transparency, we include Appendix E to give examples of how we accept responses that are \"effective, fluent, and ethically sound\" (as stated in Lines 277 and 281).\n* Note also that the dataset construction is also mentioned in the previous sections (e.g., Section 2.4, as stated in Line 276).\n\n## Question\n\n> 1. What is the purpose of the proof section in Appendix D? Is the main takeaway that if the proposed methods in Section 3 worked perfectly, then questions would be perfectly answered?\n\nYes, you got it right. It serves as the purpose that the context (re-written by LLMs) does not have old knowledge, given that the Inconsistent and Delete modules are perfect.\n\n> 2. For the deletion method, did the authors try having LLMs rewrite the inconsistent turns in the conversation instead of deleting them? This might be a better strategy since a turn may contain some relevant information to an answer even if some other information is incorrect.\n\nIt **is** \"re-write,\" the \"delete\" means \"deleting the old fact.\"\n* See the prompt in Appendix F for details, as stated in the Reproducibility Statement.\n\n> 3. For the deletion method, how was the inconsistency of a turn determined by an LLM?\n\nPlease see Appendix F and Figure 9 (on page 21). In this paper, GPT-3.5 (0613) judges on its own.\n\n> 4. How do models adapt answers when multiple facts are corrected in a single conversation?\n\nThis is an interesting work for future research as stated in the Limitations section in Lines 913-914 (on page 17)."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer t3Go"
            },
            "comment": {
                "value": "## Weakness 1 (MRE)\n\n> The connection to MRE is quite tenuous. Since MRE seems to involve updating memory, this phenomenon seems most similar to the knowledge editing literature, where model parameters are updated.\n\nWe clearly state the definition of MRE on page 1 of this paper. \n\n* Please refer to **in-context MRE** in Line 40 with footnote 1 (Here, \"in-context\" is \"in a conversation.\" To save space, we often abbreviate \"in-context MRE\" as MRE).\n\n* Moreover, our title has \"short-term\" and can also be found in Line 98 (Practically, if we can re-encode an LLM's short-term memory on the fly, there would be...).\n\n\n## Weakness 2 (Benchmark Limitations)\n> KEIC only consists of dialogues in CoQA with a Yes/No answer. \n\nWe use exact match and do not want LLM to evaluate the other types of questions (as it often lacks explainability, even though there is a huge amount of papers doing so). \n* Please refer to Lines 157-158: Because answers are free-form in CoQA, we focus on Yes/No (YN) questions to simplify the analysis, ... \n\n* We also explicitly state the rationale behind why we choose the Y/N answer in the Limitation section (see the \"KEIC Dataset\" paragraph on page 17).\n\n> From the example in Figure 4, it also seems like the questions are quite simple and closely related to the corrected information e.g. \"young / old lady\" and \"Is Sarah old?\". These choices make KEIC seem quite limited. For instance, the benchmark does not seem to consider how corrected information alters deductive reasoning and inferences e.g. if \"Mary had blue and yellow paint\" is corrected to \"Mary had blue and purple paint\", then the answer to \"Can Mary make green paint?\" changes from yes to no. \n\nThe questions are directly re-used from the CoQA dataset. \n* See Figure 2: We consider that an LLM updates its knowledge if its answer to the **same** question is changed.\n* Also see the definition of effective in Section 2.2: The question $q$ is not changed in Line 152. \n\nAs for the updated utterance, the real data in the KEIC dataset is not as simple as shown in Figure 4; it is just a simple but informative example.\n* See Examples 3-7 for the complicated ones in Appendix E (mentioned in Lines 285-286 and Reproducibility Statement).\n* Please also refer to the caption of Figure 3 (We extend the CoQA data by adding a non-trivial (i.e., multi-hop reasoning), effective fact $R'_4$ that contradicts $R_4$ in the story).\n\n> Additionally, conversations in CoQA tend to be quite short. If the goal of this task is to understand how chatbots deal with corrected information over truly long contexts, a dataset that contains long conversations with topic switches should be studied such as TopiOCQA.\n\nPlease refer to Lines 286-287: The average number of turns in the previous phase is 8.27 and 8.48, respectively.\n\nNote that the TopiOCQA dataset is longer than CoQA **only if** we (1) filter the data with turn $<$ 3 and (2) **always** use the last question for testing. However, It breaks the randomness. Not to mention the rationale is often not properly labeled (compared to CoQA).\n\n```\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"McGill-NLP/TopiOCQA\")\n>>> ds['train']['Turn_no'][2] # has test QA turn\n3\n>>> ds['train']['Context'][2]\n[\"what was australia's contribution to the battle of normandy?\", 'The army personnel and thousands of Australian airmen took part in the battle.', 'was the battle fought in australia?', 'UNANSWERABLE']\n>>> num_turn = [_ for _ in ds['train']['Turn_no']]\n>>> sum(num_turn) / len(num_turn)\n7.41\n>>> tmp = [_ for _ in num_turn if _ >= 3]\n>>> sum(tmp) / len(tmp)\n8.49\n```\n\n## Weakness 3 (Concerns with Evaluation)\n\n> The evaluations in section 5 do not address whether performance on questions unrelated to the modified information changes as corrected information is introduced.\n\nAs this paper focuses on studying whether LLMs can change the corresponding answer, another direction for future work would be asking questions related to the update (\"ripple effect\" in [1]).\nAsking irrelevant questions can be viewed as an (easy) out-of-scope question (see [2]), but they are *slightly* less interested in evaluating the KE task, except they are designed to trick the LLM. See the example below:\n\n```\nold fact: A is the president of country C.\nnew fact: B is the president of country C.\n\nfact (fixed): A's sister is X.\nfact (fixed): B's sister is Y.\nfact (fixed): D is the president of country Z.\n\nin-scope question: Who is the president in country C? (A -> B)\n(\"hard\") out-of-scope question: Who is the president in country Z? (D)\n\"ripple effect\" question: What is the president's sister's name in country C? (X -> Y)\n\nirrelevant (i.e., \"easy\" out-of-scope) question: What is the color of the sky? (blue)\n```\n\n[1] Cohen et al., Evaluating the ripple effects of knowledge editing in language models, TACL 2024\n\n[2] Mitchell et al., Memory-based model editing at scale, ICML 2022"
            }
        },
        {
            "summary": {
                "value": "This paper explores whether LLMs can gracefully recover from false information presented in context when a correction is subsequently issued by the user. The authors connect this setting to the phenomena of memory re-encoding (MRE) from psychology. To study this task, a new benchmark KEIC is proposed that builds on top of CoQA by introducing conflicting statements into conversations through an annotation process. The authors propose various prompting-based methods for LLM-based MRE and also a more expensive deletion method that utilizes an LLM to identify and delete portions of the chat history that contradict the corrected information. The paper also includes an extensive evaluation on KEIC with a wide range of open and closed-source models. The authors claim that their benchmark will aid in the development of chatbots that are adaptable and conducive to long-term single-user usage."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper studies a relevant topic of faithfulness over contexts that are growing increasingly large as new models are deployed.\n2.  The authors draw some interesting and unexpected conclusions from their results including that GPT-4o tends to be \"stubborn\", often not adapting as readily to corrected information in the context compared to GPT-3.5. \n3. The cost evaluation and analysis is relevant given the need for LLMs to analyze long contexts and is thorough."
            },
            "weaknesses": {
                "value": "1. **MRE:** The connection to MRE is quite tenuous. Since MRE seems to involve updating memory, this phenomenon seems most similar to the knowledge editing literature, where model parameters are updated. \n2. **Benchmark Limitations:** KEIC only consists of dialogues in CoQA with a Yes/No answer. From the example in Figure 4, it also seems like the questions are quite simple and closely related to the corrected information e.g. \"young / old lady\" and \"Is Sarah old?\". These choices make KEIC seem quite limited. For instance, the benchmark does not seem to consider how corrected information alters deductive reasoning and inferences e.g. if \"Mary had blue and yellow paint\" is corrected to \"Mary had blue and purple paint\", then the answer to \"Can Mary make green paint?\" changes from yes to no. \nAdditionally, conversations in CoQA tend to be quite short. If the goal of this task is to understand how chatbots deal with corrected information over truly long contexts, a dataset that contains long conversations with topic switches should be studied such as [TopiOCQA](https://arxiv.org/pdf/2110.00768).\n3. **Concerns with Evaluation:** The evaluations in section 5 do not address whether performance on questions unrelated to the modified information changes as corrected information is introduced.\n4. **Notational Problems:** There is an excessive amount of notation in this paper. I would recommend that the authors rewrite sections in natural language and use notation sparingly. Some specific problems are outlined below:\n   * Often there are two notations used for the same object. For instance, in 2.3 the authors refer to a turn with the $T_i$ notation, when they previously used the $(u_i, b_i)$ convention.\n   * Alternatives in notation are presented for readability (e.g., dropping the indices of $R$, $Q$, and $A$), but often hamper understanding and flow given how frequently they are proposed.\n5. **Clarity / Writing:** The writing is quite confusing, especially in the introduction. For instance, in the introduction, the authors claim that \"Recall\" is an effective technique for MRE, but Recall had not yet been mentioned nor explained and could easily be confused with the recall metric. Section headings could be much more descriptive. For instance, Section 2.2 is titled \"Fact\" when it contains information about the representation of facts in relation form. Finally, the KEIC benchmark seems to be one of the main contributions, but most details about its construction and annotation are relegated to the Appendix."
            },
            "questions": {
                "value": "1. What is the purpose of the proof section in Appendix D? Is the main takeaway that if the proposed methods in Section 3 worked perfectly, then questions would be perfectly answered?\n2. For the deletion method, did the authors try having LLMs rewrite the inconsistent turns in the conversation instead of deleting them? This might be a better strategy since a turn may contain some relevant information to an answer even if some other information is incorrect.\n3. For the deletion method, how was the inconsistency of a turn determined by an LLM?\n4. How do models adapt answers when multiple facts are corrected in a single conversation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examines the scenario where context that appears previously in an ongoing conversation with a LLM is corrected by the user and evaluates whether the LLMs can reflect the corrected context in their responses. The authors formalize this scenario as the Knowledge-editing in-conversation (KEIC) framework and refer to the phenomena of knowledge update as memory reencoding (MRE). Specifically, KEIC and MRE refer to non-parametric and in-context knowledge editing i.e., the model parameters are not updated and the model\u2019s behavior is influenced through prompting only. The authors of this paper collect a dataset for evaluating KEIC in contemporary LLMs through four different update methods. The dataset is collected by asking human annotators to edit a fact in the CoQA dataset, which is the updated knowledge. Accordingly, the ground truth answer to the corresponding Yes/No question in the CoQA dataset is updated. With this dataset, the authors propose four update methods: One-Time Correction (OTC), Recall, Verification and Deletion under two different settings i.e., Correction After Mistake (CAM) where the update is applied right after the incorrect fact is presented in the conversation and Correction Before Asking (CBA) where the update is applied right before asking the corresponding Yes/No question. Results are presented for various GPT, Gemma, LLaMA and Vicuna models. Updates are applied through 15 correction templates mined from Daily Dialog. Results show that the Deletion-based update method is most effective, but the Recall-based update method has the best trade-off between effectiveness and token-efficiency. Moreover, the CBA works better than CAM, which is expected because of the proximity of the update tokens to the QA tokens in the auto-regressive model. Surprisingly, gpt4o models are less reactive to these update methods than GPT3.5 models. Overall, there is room for improvement in KEIC in contemporary LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**Interesting topic in LLM conversational memory and well-written paper**: The scenario of KEIC is an interesting, if not one of the most important, topics in the domain of conversational memory of LLMs. The authors do a great job of formalizing the KEIC process, which serves as a framework for effectively thinking about in-context knowledge editing.\n\n\n**Dataset contribution**: The authors contribute an important dataset that contains conversations with edited knowledge and corresponding questions. It is a sizable dataset and can be further extended to questions beyond the Yes/No format in a semi-synthetic manner by leveraging the KEIC framework for future work. \n\n\n**Extensive results**: The authors present several plausible settings, methods and conduct extensive experiments under these settings, showing crucial gaps in the performance of current LLMs. Importantly, these experiments provide further evidence that LLMs are not 100% proficient at attending to context, which can prohibit their deployment in error-sensitive situations. Some of these results are surprising and worth investigating further i.e., GPT4o performs worse than GPT3.5 in the OTC setting."
            },
            "weaknesses": {
                "value": "**Limited Generalizability of Results**: The correction templates adopted by the authors (and listed in Appendix B) indicate that the knowledge updates that are present in the collected dataset are of a single kind i.e., fixing errors in previously reported events. This leaves out some other highly-plausible knowledge editing scenarios in conversations such as (1) temporal change in knowledge i.e., previous knowledge was true at that time but it has now evolved to a different state (such as change in Presidents) (2) evolving of user preferences or opinions (such as a change in music taste or other user preferences like name) etc. This also stems partly from the scope of the CoQA dataset which has a format of two participants discussing a passage from news or fiction. Further, the authors do not make the distinction between real-world facts and narrative fiction in this dataset. It is unclear from the results if it is hard to edit knowledge in an in-context manner if that knowledge is present in the parametric memory of the model (it is also entirely possible that GPT models are trained on the CoQA models and so this data exists in their parametric memory).\n\n**Doubts about the persistency of the KEIC scenario**: Since this problem of models underperforming in the OTC scenario can be traced back to the issue of LLMs not attending to their context correctly, I am unsure if this will continue being a problem if it can simply be fixed by better positional encodings, training data, attention architectures in oncoming versions of LLMs.\n\n**Additional Results**: There are three aspects missing from the Results section: \n\n(1) Since the KEIC problem is a reflection of how good LLMs are at attending to their in-context memory, it warrants analysis in terms of the length of the input. For example, does the update % change with longer distance between the $T_{u}$ (update turns) and $T_{i}$ test turns. Similarly, does the update % vary with longer distance between $T_{e}$ (error turns) and $T_u$?\n\n(2) Some of the correction templates improve coreference resolution at the update step while other don't. How would the results look for one template vs. the other? On that note, is there one or more templates that consistently stay in the top-1,3,5 results?\n\n(3) It would be great to see results from some of the latest open-source models such as LLaMA 3.1, OlMo, and perform analysis on the attention weights is possible.\n\n\n**Confusing Jargon**: The authors have introduced some terms that may be prohibitive in understanding the results correctly. For instance, the reported metric is 'Update %' in Figure 5. It is unnecessary since it has already been made clear that the metric is simply the accuracy of the Yes/No question. I would suggest that the authors stick to 'Accuracy' to reduce confusion. Another example is the use of 'Recall' for a method, which is a frequently used term for evaluation metrics. Using this term as a method makes for confusing reading. Another term is 'Previous Phase' which really refers to the 'Irrelevant turns'."
            },
            "questions": {
                "value": "- Who do you think that MRE is a standalone problem that should be studied and not merely a different sub-perspective of the conversational memory problem that could be potentially solved, for example, by using RAG + external memory module or better long-context modeling approaches?\n\n- Suggestion to separate CBA and CAM results in figures to allow the reader to consider both settings individual in terms of the four proposed methods.\n\nSee Weaknesses for other suggestions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces an in-context memory re-encoding process for large language models (LLMs), allowing memory updates and probing through conversational in-context prompts.\nTo achieve this, the authors created a dataset comprising {background context (a story), pairs of probing Q&A in a human-AI conversational format, and a final probing question}.\n\nThe paper details four in-context prompt memory updating types:\n\n1. One-Turn Correction (OTC): The human corrects the LLM immediately after an error is made.\n2. Verification: The LLM first answers a probing question, followed by a human verification question to confirm the accuracy.\n3. Recall: The LLM rewrites the entire story after human correction, followed by probing.\n4. Deletion: The LLM removes any sentence in the story that contradicts newly updated information.\n\nThe paper evaluates these memory-updating prompt schemes across different LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Introduces a comprehensive dataset for studying in-context memory updating.\n- Proposes four distinct prompting methods for updating memory in LLMs."
            },
            "weaknesses": {
                "value": "The paper has several issues, despite arguing for the importance of in-context memory editing over parametric knowledge editing:\n\n1. While the task setting is intriguing, it seems more like adversarial question answering in a long-term conversational context (i.e., questions that are designed to trick the model into providing wrong answer) than true memory editing. Context-level memory editing would ideally involve managing an external database that saves and updates facts or events within the conversation [1][2].\n\n2. There is a discrepancy between the dataset and the examples in Figures 1 and 3. The figures suggest memory editing in a human-to-human conversational context, while the dataset primarily involves single-turn stories and human-AI QA-style probing. To reduce potential reader confusion, I recommend that the authors use real data examples as motivating illustrations, better aligning the figures with the dataset's actual structure.\n\n3. The authors used 15 different memory-updating prompts but only one prompt for memory probing. Exploring prompts that focus on recent information or limit historical context could yield more convincing results. -- \"Based on the recent correction from the user, how would you answer to this question: {question}?\" can be one example.\n\n[1] Personalized Large Language Model Assistant with Evolving Conditional Memory., Yuan et al., 2024\n\n[2] Evaluating Very Long-Term Conversational Memory of LLM Agents., Maharana et al., 2024"
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates large language models' (LLMs) ability to integrate corrections made during a conversation, a process termed as Memory Re-Encoding (MRE). The authors introduce the Knowledge Editing in Conversation (KEIC) dataset, derived from CoQA, by introducing corrections mid-conversation to test LLMs' adaptation to updated information. Four correction methods\u2014One-Turn Correction (OTC), Verification, Recall, and Deletion\u2014are proposed, with Deletion emerging as the most effective in experimental evaluations on various LLMs, followed by Recall and OTC."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper addresses the important challenge of enabling LLMs to incorporate corrections seamlessly, an essential feature for reliable conversational AI.\n- The authors\u2019 structured approach to enhancing MRE is interesting.\n- Up to section 2, the paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- The KEIC dataset may lack diversity in conversational flow.\n    - Stories are consistently positioned at the start of each dialogue, which simplifies real-world conversational dynamics.\n    - Both stories and conversations are brief, with only yes/no questions, potentially narrowing the scope of the model's evaluation. Expanding question types could offer richer insights with minimal adjustments in answer evaluation.\n    - Correction utterances appear only adjacent to the story or directly preceding the question.\n- The experiments could benefit from a deeper analysis of how model performance varies with factors such as:\n    - The distance between the corrected story and the question.\n    - The position of correction utterances relative to the question.\n- Sections 3\u20135 could be clearer. Please refer to the questions in the Questions section. I will raise my score if misunderstandings due to unclear explanations are addressed."
            },
            "questions": {
                "value": "- Are CAM and CBA the only configurations for correction placement? Testing additional positions for corrections (i.e., various places between the correction and the question) could reveal valuable insights into positional effects on model performance.\n- Line 314: Is the \u201cupdate\u201d baseline the original CoQA? The phrase \"we directly replace the old fact in the story with a new one\" suggests the original story is used without an explicit correction within the conversation.\n- Table 1: Why does Deletion require more computational resources in the CAM setting compared to the CBA setting?\n- Lines 354\u2013357: This passage is difficult to follow. It would be helpful to reference Figure 9. What does \"The goal of evaluating the former approach aligns with that of our baseline with no update phase\" mean? Perhaps, this unclarity is related to my question about line 314.\n- Line 374: The concept of \"top-K upper bound performance\" requires clarification and further explanation. Please provide a clearer definition.\n- Lines 377\u2013378: The statement regarding the \u201cbest five out of 15 correction utterances\u201d is confusing. The definition suggests that performance should increase with higher K values since any of the top-K templates should trigger a correct response. However, the performance in the plots fluctuates or even declines as K increases.\n- Figure 5: Does the statement \u201cthe baseline with no update phase has 56.5% of update\u201d mean that GPT\u2019s performance on the original CoQA is only 56.5%?\n- Table 2: The terms \"Update/No Update/Upper Bound\" in the caption are not clearly defined, particularly \u201cUpper Bound.\u201d\n- Line 452: The poor performance of GPT-4 and GPT-4o is quite unexpected. Please share some specific examples of failure cases.\n- Line 464: The paper suggests that the Recall method outperforms OTC. Could this simply be due to Recall positioning the updated story closer to the question than in OTC?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies the Memory Re-Encoding  (MRE) task, which tries to correct the misinformation in the existing dialogue histories on the fly. Consequently, this work proposes a Knowledge Editing In Conversation (KEIC) framework to measure the adaptability of LLMs.\n\nThis work is beyond my research scope,  so my opinions may not be accurate."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed framework KEIC can be used in correcting many general misinformation types (hallucination, notorious, etc.).\n\n2. The proposed framework KEIC does not need to tune the model parameters.\n\n3.  A very detailed and comprehensive problem analysis.\n\n4. This work has built a high-quality dataset.\n\n5. Four model-agnostic MRE methods are proposed and strong experiments are conducted."
            },
            "weaknesses": {
                "value": "1. The organization and notations should be improved. The current readability is somewhat lacking. For example:\n\n- Line 147-148:  the mixture of $r$ and r.\n- The arrangement of Figures is not very good.  The texts on the $n$ page always require checking a figure that appears on the $n-2/3$ page. \n\n2. The proposed methods involve many additional processes. Three advanced methods cost much more tokens (Table 1. #Input Tokens) , which may subsequently worsen the latency.\n\n3. Experiments may lack performance evaluation on the general metrics. For example, using BLEU ROUGE to evaluate the quality of the generated dialogues."
            },
            "questions": {
                "value": "How does the proposed method affect the general performance of related tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}