{
    "id": "Na28j1Drh7",
    "title": "Why Do You Answer Like That? Psychological Analysis on Underlying Connections between LLM's Values and Safety Risks",
    "abstract": "The application scope of Large Language Models (LLMs) continues to expand, leading to increasing interest in personalized LLMs. However, aligning these models with individual values raises significant safety concerns due to harmful information correlated with certain values. In this paper, we identify specific safety risks in value-aligned LLMs and investigate the psychological principles behind these challenges. Our findings reveal two key insights. First, value-aligned LLMs are more prone to harmful behavior compared to non-fine-tuned models and exhibit slightly higher risks in traditional safety evaluations than other fine-tuned models. Second, these safety issues arise because value-aligned LLMs genuinely understand and act according to the aligned values, which can amplify harmful outcomes. Using a dataset with detailed safety categories, we find significant correlations between value alignment and safety concerns, supported by psychological hypotheses. This study offers insights into the ``black box'' of value alignment and proposes enhancing the safety of value-aligned LLMs by corresponding in-context alignment methods.\nWarning: This paper contains contents that may be offensive or upsetting.",
    "keywords": [
        "Value Alignment",
        "Personalized LLMs",
        "AI Safety",
        "Phychological Analysis"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "Value-aligned LLMs pose safety risks by acting on learned values, and our research, supported by psychological hypotheses, reveals these issues and offers insights for improving safety.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Na28j1Drh7",
    "pdf_link": "https://openreview.net/pdf?id=Na28j1Drh7",
    "comments": [
        {
            "summary": {
                "value": "The article evaluates the safety of LLMs fine-tuned using three kinds of datasets. The main purpose is to evaluate the effect of fine-tuning to personal values of users on the models' safety. Traditional NLP tasks (Samsum and Grammar) as well as instruction fine-tuning (Alpaca and Dolly) are used as control groups to contrast the value-aligned LLMs. The safety is evaluated using four datasets. The article shows that different values have different effects on the safety dimensions. The article points out that correlations between values and safety dimensions confirm or are confirmed by the accepted psychological theories. The authors argue that the reason for certain safety violations is the value alignment. To support this claim value-based prompts are proposed which instruct the LLM not to consider the values. Additional standard mitigations are also evaluated and shown to slightly decrease the safety risks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Model safety is an important issue, and personalization is an essential context for this work. \n\n* The authors validate their results using established psychological theories.   \n\n* All the main claims are backed by the results and are properly discussed"
            },
            "weaknesses": {
                "value": "* Minor clarity issues\n\nIn the first contribution, it is not clear whether existing theories support the value-dependent degradation or value-independent degradation of safety.\n\nThe paper shows that correlations of values with particular misbehaviors correspond to the correlations reported in psychological theory. Since the objective of the paper, as expressed in the title, is explaining \"why do they answer like that,\" it is important to show and explain the causal relationships between the variables and not just correlations. \n\n\n\n\n* Value fine-tuning is not followed/accompanied by safety rewards\n\nLLM training methodology includes safety rewards as an integral part of the task-specific rewards. According to the standard forgetting phenomenon, fine-tuning the models on specific tasks naturally drags the model away from the preciously learned state, including stepping away from the safety optima. It may be the case that if the fine-tuning was accompanied by safety rewards, then there would be no difference between value-aligned LLMs and instruction-aligned LLMs.  \n\n\n* The basic LLMs are trained to perform various tasks. If the models used were already trained with the data used as the control, then it would explain the lower impact of these data sets on safety. \n\n* Statistical significance is not reported for results in Tables 2 and 3."
            },
            "questions": {
                "value": "How can the correlation analysis show that value causes misbehavior and that the shown correlations are not a result of a confounding factor? \n\nIs it more challenging to keep the models within the safety boundaries while finetuning for personal values than while other domain adaptations?\n\nAre you sure that the LLMs were not previously trained on the non-value alignment datasets used in the study? \n\nIs the difference between the safety degradation of Touch\u00b4e23-ValueEval and the safety degradation due to other datasets statistically significant?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "details_of_ethics_concerns": {
                "value": "Potentially offensive content. See the trigger warning by the authors on the first page."
            }
        },
        {
            "summary": {
                "value": "The paper explores the relationship between value alignment and possible harms that might emerge in LLMs under psychology lens."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Interesting take on value alignment in LLMs, well grounded in psychology. I particularly enjoyed reading about psychology and the results of this part.\n- The experiments are extensive"
            },
            "weaknesses": {
                "value": "- The paper's flow is a little bit broken so it is hard to follow at certain points. The results in the middle in particular feels parachuted and out of context, while the end is what holds the essence of the paper. \n- The methodology is hard to accept and follow as it is just a list of choices without justification. It would be helpful to know why the datasets were chosen and the personalization techniques were selected, especially in the context of linking the safety alignment tasks to psychology. Maybe also adding some examples would be helpful?\n- Some techniques are mentioned without explanation (e.g., VIM). VIM is a key technique in the paper, and yet, no citation or explanation provided... \n- The results in Fig 2 do not seem important enough to make the conclusions of the paper."
            },
            "questions": {
                "value": "1) Please justify the methodology: The choice of Schwartz theory, the choice of the safety tasks with respect to the psychology analysis. \n2) Please clarify the methodology\n3) Please provide examples in each value rather than just numerical values, it would make the paper more solid"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the safety implications of value alignment in LLMs by analyzing how models fine-tuned to align with specific human values might exhibit increased safety risks. The authors identify certain values as being more correlated with certain categories of harmful behavior and also propose some prompt-engineering strategies to help mitigate the challenge."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper is the first comprehensive study to examine the relationship between value alignment and safety risks in LLMs, and applies psychological analysis to model output analysis. By correlating model behavior with the psychological theory of basic human values, the authors explore how aligning models with values like hedonism or power might influence their likelihood of generating particular types of unsafe responses. The correlation analysis is quite novel. The overall paper flow is clear and easy to follow."
            },
            "weaknesses": {
                "value": "- The entire section 4.2 is based on analysis of 2 datasets and only of 1 model and its fine-tuned versions. Therefore, it is unclear how this generalizes to different models/ datasets with more fine-grained or different harmfulness categorization.\n- The paper analyzes model behavior based on psychological theories of humans, but it is unclear the validity of analyzing AI behavior through human behavior perspectives. \n- The prompt-engineering mitigation strategies are not realistic. The authors discussed that they select extreme distributions (which may not reflect actual human value system and realistic prompts to begin with). However, if this is to be incorporated in real-world systems, there needs to first be a value detector, which is non-trivial to implement with high detect accuracy. \n- The paper attributes the safety difference to value alignment, but does not include enough ablations to disentangle confounding variables. For example, the difference could be impacted by the size of the benign fine-tuning dataset, the fine-tuning method etc. These warrants more detailed study. \n- The proposed strategies also does not guard against prompt injection or other optimization based jailbreking strategies, which has a more direct impact on safety. \n- It is unclear how the psychological analysis could be incorporated from a practical standpoint for improving model safety in the real world."
            },
            "questions": {
                "value": "How exactly is the distribution sampling done? More explanation and detail are needed.\n\nWhat about the performance of more recent models like Llama-3-8B, or other larger models? With better alignment processes and more parameters, are they better at balancing value customization and safety risks?  \n\nWhat exactly does VIM entail? Does it mean value alignment via fine-tuning, specifically with the Touche23-ValueEval dataset? (the acronym was never explained in the paper either)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}