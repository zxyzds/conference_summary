{
    "id": "frsg32u0rO",
    "title": "Block Verification Accelerates Speculative Decoding",
    "abstract": "Speculative decoding is an  effective method for lossless acceleration of large language models during inference. It uses a fast model to draft a block of tokens which are then verified in parallel by the target model, and provides a guarantee that the output is distributed identically to a sample from the target model. In prior works, draft verification is performed independently token-by-token. Surprisingly, we show that this approach is not optimal. We propose *Block Verification*, a simple draft verification algorithm that verifies the entire block jointly and provides additional wall-clock speedup. We prove that the proposed mechanism is optimal in the expected number of tokens produced each iteration and specifically is never worse than the standard token-level verification.\nEmpirically, block verification provides modest but consistent wall-clock speedups over the standard token verification algorithm of 5\\%-8\\% in a range of tasks and datasets.\nGiven that block verification does not increase code complexity, maintains the strong lossless guarantee of the standard speculative decoding verification algorithm, cannot deteriorate performance, and, in fact, consistently improves it, it can be used as a good default in speculative decoding implementations.",
    "keywords": [
        "llm efficiency",
        "speculative decoding",
        "distribution coupling"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=frsg32u0rO",
    "pdf_link": "https://openreview.net/pdf?id=frsg32u0rO",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a new algorithm for draft verification for speculative decoding for LLMs. Speculative decoding is a method that accelerates the inference process of LLMs using predictive heuristics to produce \u201cdrafts\u201d of the likely next tokens the LLM will generate. These drafts must go through a verification process to determine whether or not they should be accepted, depending on how well they fit the target model\u2019s distribution. The authors argue that the prior state-of-the-art approach for draft verification, Token Verification, is not optimal, since it considers the tokens one by one. The authors instead propose a method to consider the tokens as a block by considering their joint probabilities. The authors also demonstrate that their method accepts more tokens on average than the original Token Verification approach, which is more efficient in the long run. Their experimental results demonstrate that despite the standard overheads of LLM inference, the Block Verification algorithm outperforms the Token Verification algorithm in terms of wall clock speedup."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall, the paper is well written and the problem is clearly stated.\n\nThe theoretical proofs are sound to the best of my knowledge."
            },
            "weaknesses": {
                "value": "The paper does not provide sufficient evidence that it provides a considerable speedup.\nSpeedup is from a higher acceptance rate. The paper is using  PALM-2-XXS and XXXS. At this model size, answers tend to be quite bad, which makes it hard for actual response evaluation. \n\nIt was not clear how the probabilities for all subblocks are calculated. \n\n# Minor comments \n\nPage 7, line 336: \u201cI.e., it measures\u201d should be \u201cSpecifically, it measures\u201d\n\nPage 8, line 425: \u201call valid verification algorithm\u201d should be \u201call valid verification algorithms\u201d\n\nPage 8: I think Figure 4 should be labeled as a Table instead of a figure"
            },
            "questions": {
                "value": "- How will the speedup results scale when using larger models?\n\n- How are probabilities of all subblocks calculated?\n\n - Page 7, line 334: How realistic is this experimental setting? How many drafts are usually produced during speculative decoding, and how many copies of the model would we need for evaluating?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Block Verification, a novel algorithm designed to accelerate speculative decoding in large language models. Unlike traditional token-by-token verification, Block Verification evaluates and accepts a group of tokens (a \"block\") together. This method achieves faster generation speeds without compromising output quality."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. **Block Verification** improves efficiency by verifying token blocks rather than individual tokens in speculative decoding, preserving the same distribution.\n\n2. Block verification achieves consistent 5\u20138% speedup and is broadly applicable across models and datasets.\n\n3. Block verification is easy to implement, with minimal modifications to existing speculative decoding systems."
            },
            "weaknesses": {
                "value": "1. Consistency in final distribution does not guarantee identical generation sequences. For instance, the large model might generate \"ABC\" while the small model generates \"ADC.\" It\u2019s possible that in the context of \"ADC,\" the next token\u2019s distribution aligns between the models, but under greedy decoding, the large model should ideally follow \"ABC\" rather than \"ADC.\" The authors state that block verification accepts more tokens than token verification. In the case of greedy decoding, does this imply potential token differences? Would such differences impact the accuracy of the answer, or would block verification revert to token-by-token verification?\n2. The explanations of the formulas and algorithms lack intuitive clarity, making comprehension more challenging, even though the modifications from token verification are not large."
            },
            "questions": {
                "value": "1. In the appendix, a batch size of 8 was used to compare with tree attention. What batch size was used in the main experimental section?\n2. How does the performance compare with Medusa/Eagle's static tree attention?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel verification algorithm called the block verification method, which enhances the efficiency of speculative decoding in large language models (LLMs). This method contrasts with the traditional Token Verification, which optimizes the verification of token blocks to increase efficiency. The study demonstrates that Block Verification outperforms existing methods regarding the expected number of generated tokens (called the block efficiency) and wall-clock times while providing a theoretical guarantee of optimal performance under given conditions. Empirical evaluations show improvements across various tasks, confirming its effectiveness without additional code complexity."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper provides solid theoretical guarantees, ensuring that the proposed method does not compromise the output distribution.\n- The experiments validate the practical benefits, with performance gains across a diverse set of tasks.\n- The proposed algorithm does not increase code complexity, making it easy to integrate into existing LLM architectures.\n- The paper highlights how Block Verification compares favorably to related methods, strengthening its contribution."
            },
            "weaknesses": {
                "value": "- The depth of proofs might be challenging for practitioners without a strong mathematical background, potentially limiting the paper's audience.\n- There is a typo in the manuscript: page 8, line 413: deocding -> decoding"
            },
            "questions": {
                "value": "- Are there specific conditions or types of LLM architectures where the performance gains of Block Verification may be more pronounced or limited?\n- How does the algorithm handle edge cases where the distributions of the drafting and target models diverge significantly?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the problem of optimizing speculative decoding from an algorithm level to achieve inference acceleration. The key idea behind this paper is the observation that verifies multiple draft tokens in a block jointly, rather than token-by-token, can bring improvement in mean accepted tokens, which leads to the wall-clock speedup for LLM inference. Based on the idea, the paper proposes a simple block verification algorithm, which is plug-and-play for some existing speculative decoding methods. The authors theoretically show that block verification is lossless and better than standard speculative decoding, making it a stronger default implementation of speculative decoding. The experiments demonstrate that block verification modestly but consistently improves the mean accepted tokens and the final wall-clock speedup."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The introduction of the block verification in the full distribution is clear and easy to understand, and the paper is well-written with clear explanations to the block verification algorithm.\n2. This method shows great simplicity and is easy to use. It does not incur additional computation or memory overhead, and is very easy to implement. \n3. The authors theoretically show that block verification is lossless and better than standard speculative decoding, making block verification a stronger default verification for speculative decoding."
            },
            "weaknesses": {
                "value": "While I really appreciate the simplicity of the method, the novelty and the contributions of the paper are a little weak for me. The reasons are:\n\n1. The motivation that *\"verifying multiple draft tokens token by token is not optimal\"* is very similar to the motivation in Tree Monte Carlo (TMC) [1], which alleviates the novelty of this paper. Given the empirical results that block verification only brings 5%~8% speedup, I doubt the technical contributions of block verification. The block verification seems more like a trick with theoretical support.\n2. There lacks a significant discussion about the decoding temperature $T$ in LLM inference, and the authors do not report the temperature in the experiment details. **In my understanding, the block verification can only bring speedup with $T > 0$, as the block verification degenerates to the token verification in greedy decoding ($T=0$).** Please correct me if I have any misunderstanding. While speculative decoding provides maximal speedup in greedy decoding settings for some specific tasks, such as coding and math reasoning, block verification cannot provide improvement on these tasks. Besides, additional experiments to investigate the influence of different decoding temperature are necessary.\n3. Currently, I think there exists a large room (more than 1 page) to conduct more experiments.\n   - Experiments for more LLM combinations (e.g. Llama family [2] and Vicuna [3]) across more benchmarks (e.g. MT-Bench [4] and SpecBench [5]) **with different temperature settings**.\n   - Experiments for combining block verification to some latest speculative decoding methods (e.g. Medusa [6] and Eagle [7]).\n\nI know that conducting a wide range of evaluation experiments are costly and time consuming, and I will increase my score if the authors clearly address my concerns.\n\n\n\n[1] Hu, Zhengmian, and Heng Huang. \"Accelerated Speculative Sampling Based on Tree Monte Carlo.\" *Forty-first International Conference on Machine Learning*.\n\n[2] Dubey, Abhimanyu, et al. \"The llama 3 herd of models.\" *arXiv preprint arXiv:2407.21783* (2024).\n\n[3] Chiang, Wei-Lin, et al. \"Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\" *URL https://lmsys. org/blog/2023-03-30-vicuna* 3.5 (2023).\n\n[4] Zheng, Lianmin, et al. \"Judging llm-as-a-judge with mt-bench and chatbot arena.\" *Advances in Neural Information Processing Systems* 36 (2023): 46595-46623.\n\n[5] Xia, Heming, et al. \"Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding.\" *arXiv preprint arXiv:2401.07851* (2024).\n\n[6] Cai, Tianle, et al. \"Medusa: Simple llm inference acceleration framework with multiple decoding heads.\" *arXiv preprint arXiv:2401.10774* (2024).\n\n[7] Li, Yuhui, et al. \"Eagle: Speculative sampling requires rethinking feature uncertainty.\" *arXiv preprint arXiv:2401.15077* (2024)."
            },
            "questions": {
                "value": "I would like to raise some questions to improve the manuscripts.\n\n1. Could you provide a more detailed explanation to the motivating example with partial information? You have mentioned \"with block verification\" in Line 124, but it is not clear how block verification works in this section.\n2. Existing speculative decoding methods mainly adopt a tree-based verification manner, which can significantly enhance the mean accepted tokens. Could you provide some explanation how block verification works in a tree-based verification manner?\n3. Could you provide a more detailed theoretical assumptions for Definition 1, Theorem 1 and Theorem 2? Besides, I think the section Theoretical Guarantees can be further improved by providing some intuitive explanation (e.g. why block verification is optimal for any valid draft verification algorithm), as this can help readers to understand the advantages of block verification over TMC."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}