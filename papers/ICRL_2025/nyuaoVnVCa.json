{
    "id": "nyuaoVnVCa",
    "title": "EMERGENCE OF GROUNDED, OPTIMALLY COMPOSITIONAL SPATIAL LANGUAGE AMONG HOMOGENEOUS AGENTS",
    "abstract": "A mechanism of effective communication is integral to human existence. An\nessential aspect of a functional communication scheme among a rational human\npopulation involves an efficient,  adaptive, and coherent apparatus to convey one\u2019s goal to others. Such an effective macro characteristic can\nemerge in a finite population through adaptive learning via trial and error\nat the individual (micro) level, with nearly consistent individual learning faculty and experience across the population. In this paper, we study and hypothesize\n pertinent aspects of glossogenetics, specifically primal human communication mechanisms, through computational modeling. In particular, we model the\nprocess as a language game within the fabric of a decentralized, multi-agent\ndeep reinforcement learning setting, where the agents with local learning and neural\ncognitive faculties interact through a series of dialogues. Our homogeneous agents seek to achieve the principle of least effort and overcome the poverty of stimulus through efficient concept selection, guided feedback and mirror learning. In our examinations,\nwe observe the emergence of successful and structured communication among static and dynamic agent populations through consistent and continual learning.",
    "keywords": [
        "multi-agent reinforcement learning",
        "Language Emergence",
        "Cultural Evolution"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "Emergent communication in multi-agent population with human language characteristics",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=nyuaoVnVCa",
    "pdf_link": "https://openreview.net/pdf?id=nyuaoVnVCa",
    "comments": [
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "What do you mean by ontology?\n\nThe problem we aim to address is the emergence of a shared language among a finite set of agents, each capable of producing utterances. This problem is modeled as a language game, \nwhere agents interact to convey a goal. In this framework, the agents reside in a graph world and use language to identify vertices. To tackle the issue of language emergence, \nwe must first define what a human language is. In this context, a language is a mapping between a concept space (ontology) $C$ and a vocabulary $V$ (a set of words).\n\nIn our setting, the ontology $C$  consists of sectors, segments, and colors. The sectors and segments represent partitions of space, which may overlap. Thus, a vertex can be identified through a combination of its segment, sector, and color. \nHowever, this identification may not be unique. We refer to this identification as the conceptualization of the vertex.\n\nThe agents interact by communicating these conceptualizations using words. The goal is for them to converge on a coherent or shared language through trial and error and feedback, without any central authority guiding \ntheir learning. Ultimately, the agents must agree on a language system where they share similar mappings of concepts to words. This task is challenging because there is no predefined, fixed mapping to strive for. Instead, the agents\n must find consensus among a vast number of possible mappings. Specifically, if the size of the concept space $\\vert C \\vert = c$ and the size of the vocabulary $\u2223V\u2223=v$, then the total number of possible mappings from \n$C$ to $V$ is  $v^{c}$.  For $N$ agents, the number of potential joint mappings grows to  $(v^{c})^{N}$, which is a super-exponential space to explore. In our example, where  the search space is astronomically large.\n\nMoreover, the challenge is further intensified by the ambiguous feedback in the guessing game:. When an interaction failis, the speaker  only provides the \"topic vertex\" (the specific vertex they are referring to), \nwithout offering explicit feedback on the underlying concepts the speaker intended to convey through their words. It is important to note that the concepts being \ncommunicated are the conceptualizations of the topic vertex, which are not unique. This lack of clarity adds an additional layer of complexity to the task of establishing a shared language.\n\n\nIs it assumed that all agents discretize the continuous environments exactly the same way even before any language is evolved? \n\nYes. This is indeed a very standard assumption. The language emergence is itself a very hard problem to solve. One has to \nassume certain conditions to make it work. All the agents share the same concept space/ontology.\n\n\nWhen communication failed, how is the topic node disclosed to the listener without language?\n\nThis is indeed crucial. The agents do not have telepathic abilities. If communication fails, the concepts the speaker intends to convey must be made tangible to the listener, \nenabling them to learn the correct word mappings. The key idea here is \"tangible.\" To achieve this, the speaker physically moves to the topic vertex, allowing the listener to\nobserve it directly. The listener then conceptualizes this vertex using attributes like <sector, segment, color>\nand associates the speaker's utterance with these concepts. It's important to note that this is a guessing game scenario, because the conceptualization \nof a vertex may not be unique, making the learning process more complex."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "The problem we aim to address is the emergence of a shared language among a finite set of agents, each capable of producing utterances. This problem is modeled as a language game, \nwhere agents interact to convey a goal. In this framework, the agents reside in a graph world and use language to identify vertices. To tackle the issue of language emergence, \nwe must first define what a human language is. In this context, a language is a mapping between a concept space (ontology) $C$ and a vocabulary $V$ (a set of words). In our setting, the ontology $C$  consists of sectors, segments, and colors. The sectors and segments represent partitions of space, which may overlap. Thus, a vertex can be identified through a combination of its segment, sector, and color. However, this identification may not be unique. We refer to this identification as the conceptualization of the vertex.\n\nThe agents interact by communicating these conceptualizations using words. The goal is for them to converge on a coherent or shared language through trial and error and feedback, without any central authority guiding \ntheir learning. Ultimately, the agents must agree on a language system where they share similar mappings of concepts to words. This task is challenging because there is no predefined, fixed mapping to strive for. Instead, the agents\n must find consensus among a vast number of possible mappings. Specifically, if the size of the concept space $\\vert C \\vert = c$ and the size of the vocabulary $\u2223V\u2223=v$, then the total number of possible mappings from \n$C$ to $V$ is  $v^{c}$.  For $N$ agents, the number of potential joint mappings grows to  $(v^{c})^{N}$, which is a super-exponential space to explore. In our experiments with $v=20$, $c=12$ and $N=4$, the search space is astronomically large.\n\nMoreover, the challenge is intensified by the ambiguous feedback in the guessing game: When an interaction failis, the speaker only provides the \"topic vertex\" (the specific vertex they are referring to), without offering explicit feedback on the underlying concepts the speaker intended to convey through their words. It is important to note that the concepts being communicated are the conceptualizations of the topic vertex, which are not unique. This lack of clarity adds an additional layer of complexity to the task of establishing a shared language. This is contrary to the Lewis games where the intended concepts are conveyed to the listener unambiguously.\n\nThe core of our study lies in the complexity arising from the  interplay dynamics, coupled with the guessing game nature and decentralized learning. Despite this complexity, our study demonstrates the emergence of a language with key characteristics such as groundedness, interchangeability, shared understanding, coherence, compositionality, word order, and Zipfian distribution.  \n\"Interchangeability\" denotes a state of homogeneity, where an agent possesses the ability to both speak and listen interchangeably. This stands in contrast to the predominant approach found in existing literature, where agents are typically assigned fixed roles, either as speakers or listeners. Achieving this interchangeability is made possible through the design of mirror networks. Here, we conceptualize the speaking module\n as a mirror image of the listening module, and vice versa. \nWe also consider \"principle of least effort\" within the domain of human language, where  we are essentially describing the principle of minimizing the exertion required for communication. This concept is intricately tied to \nhow individuals conceptualize a given topic. In essence, the level of effort required to communicate effectively depends on the complexity and depth of conceptualization. In our approach, we aim to calibrate the network \nin such a way that it selects all the relevant concepts necessary for expressing a given topic. This ensures that the communication process is streamlined and efficient, with the network automatically prioritizing \nthe most pertinent information. Our intuition regarding this approach is supported by the observation of Zipfian characteristics in language.  Making direct comparisons between our findings and real-world human language settings is somewhat unfair given\n the immense complexity inherent in human communication. However, it remains crucial to assess our results within the framework of existing language emergence settings, which we  review in our introduction.\n\nThe reviewer requests further studies, but the analysis already covers how various components influence the emergent language and its characteristics. Comparing with other models from existing literature isn't feasible, \nas the objectives and models differ, and our focus isn't on commenting about neural models. Our neural architectures adhere to standard RNN models without specific modifications to achieve emergent behavior.\n\nIf you find our arguments agreeable, kindly let us know so that we can restructure the paper as suggested."
            }
        },
        {
            "summary": {
                "value": "This paper proposes a model of emergence of compositional languages in the reinforcement learning framework."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The review of the previous works regarding emergence of language is well organized."
            },
            "weaknesses": {
                "value": "On the other hand, the original contribution of the present work on top of previous works is not clear.\nThere is no result figure in the main part of the paper. They should be moved from Appendix to the main part while method details can be put in the Appendix."
            },
            "questions": {
                "value": "What do you mean by ontology? Is it assumed that all agents discretize the continuous environments exactly the same way even before any language is evolved?\nWhen communication failed, how is the topic node disclosed to the listener without language?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates the emergence of language in a multi-agent setting, in which different agents have to communicate with each other and evolve their own language. To this end, the paper introduces a special environment, which is essentially a guessing game in which a speaker selects a concept, which has to be correctly guessed by a randomly selected listener via the discrete & noise-free communication channel. The paper uses an RL-driven multi-agent algorithm to encourage agents to develop a language maximising a shared return."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The overall analysis and statement of made assumptions seem sound\n- The visualization of the guessing game are indeed nice\n- There is some formal grounding of the paper, ie technical detail is there"
            },
            "weaknesses": {
                "value": "- My biggest concern is that after reading the paper, I don't know its actual contribution. While the paper lists several prior works, to me, it essentially introduces a specific game that may or may not be novel, develops an RL-driven framework to allow the emergence of a communication protocol, and then shows that it works. I will list the points of concern in detail below.\n- Is the game novel? Knowing some related works, it does not seem to deviate from prior environments in any important concept or detail. If the game is novel and allows agents to learn or equip new abilities, there should be at least a comparison against a prior (i.e. weaker) environment showing that this is the case.\n- Is the language channel/discretization novel? Again I don't think so but the paper is not clear on this.\n- While a RL-learning method is introduced, its discussion falls short of pointing out what makes it special from prior work, nor is a good ablation study performed. The paper essentially shows that the framework works, but it is not clear what makes it work nor how it relates to prior works in emergent language learning.\n- While related work is mentioned, the paper would greatly benefit form a related works section clearly stating its contribution and differences to prior work. The current contributions section falls short of describig the novelty of the presented research.\n- The paper is not concise and sometimes goes on tangents, seemingly not relevant to the main contribution of the paper (e.g. section 6.2 to just name a single example). I also want to remind the authors that the reviewers are not obliged to read the supplement, nor should all relevant experimental results be included in the supplement. Strictly speaking, the paper itself (pages 1-10) does not present any results, as all results/tables/figures are in the appendix. The paper needs to be rewritten so that at least the main results are in the first 10 pages; additional experiments or further analyses may be included in the supplement. The paper needs to be more concise and detail its core contribution, core results and detail the impact it has for other researchers/the field.\n- I fail to see the impact of the paper. It shows that a language emerges in the framework, but how is this significant? Is any new problem solved? Can we do anything with the concepts introduced in the paper? I fail to see how the paper would be of interest to other researchers without further discussion on these points."
            },
            "questions": {
                "value": "Please see the weakness above. In my opinion it is not clear what the core contribution of your paper is beyond introducing a method and showing it works. It is unclear where it goes beyond the current state of the art.\n\nThe paper needs some comparison against another baseline, environment or similar. The paper is not the first to introduce a framework for emergent language.\n\nI do think the paper needs an extensive rewrite + additional experiment. I am very unlikely to change my opinion without these concerns being addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates emergent communication and its properties under the paradigm of decentralized multi-agent deep reinforcement learning. The entire investigation is based on a proposed multi-agent \u2018guessing game\u2019 that is a lot more complex than existing Lewis signaling games in terms of vocabulary alignment and learning. A multi-component object function is proposed to learn how to communicate, inducing properties of interchangeability, compositionality, and the principle of least effort. In particular, the paper shows that enabling interchangeability avia mirror learning allows agents to act effectively as both a listener and a speaker. Studying the emergent linguistic patterns developed by agents in this game shows findings that align with natural phenomena like the alignment with Zipf\u2019s law in terms of word frequencies."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper proposes a novel multi-agent guessing game with greater complexity than existing signaling games which could be useful for future works \n- The proposed properties and objective function show aligned properties with languages in the nature"
            },
            "weaknesses": {
                "value": "- The paper is unfortunately poorly written. At times, it becomes difficult to understand what the authors mean. For instance, the abstract lacks clarity with a lot of vague use of vocabulary. An abstract is supposed to allow readers to understand the general idea of the paper. But this abstract is very confusing. The introduction is also poorly written, with a long chain of citations without much of a coherent message. The paper needs a lot of revision before it meets the standard of ICLR\n- There should at least be some key results in the main paper, instead of having all the results in the supplementary materials\n- Figure 1 should have a more elaborate caption\n\nPlaces that need clarifications:\n- Line 74, what do you mean by self-organization in language games?\n- Line 92-93, a vague sentence\n- Line 257, what do you mean by \u2018effective guidance\u2019?"
            },
            "questions": {
                "value": "- In line 67, how does mirror learning ensure continual learning?\n- In line 510, how does the mentioned property mirror patterns observed in human interactions? Please give support and example"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}