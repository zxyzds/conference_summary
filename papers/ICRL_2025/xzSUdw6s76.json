{
    "id": "xzSUdw6s76",
    "title": "PALMBENCH: A COMPREHENSIVE BENCHMARK OF COMPRESSED LARGE LANGUAGE MODELS ON MOBILE PLATFORMS",
    "abstract": "Deploying large language models (LLMs) locally on mobile devices is advantageous in scenarios where transmitting data to remote cloud servers is either undesirable due to privacy concerns or impractical due to network connection. Recent advancements have facilitated the local deployment of LLMs. However, local deployment also presents challenges, particularly in balancing quality (generative performance), latency, and throughput within the hardware constraints of mobile devices. In this paper, we introduce our lightweight, all-in-one automated benchmarking framework that allows users to evaluate LLMs on mobile devices. We provide a comprehensive benchmark of various popular LLMs with different quantization configurations (both weights and activations) across multiple mobile platforms with varying hardware capabilities. Unlike traditional benchmarks that assess full-scale models on high-end GPU clusters, we focus on evaluating resource efficiency (memory and power consumption) and harmful output for compressed models on mobile devices. Our key observations include: i) differences in energy efficiency and throughput across mobile platforms; ii) the impact of quantization on memory usage, GPU execution time, and power consumption; and iii) accuracy and performance degradation of quantized models compared to their non-quantized counterparts; and iv) the frequency of hallucinations and toxic content generated by compressed LLMs on\nmobile devices.",
    "keywords": [
        "Mobile Platforms",
        "Large Language Models",
        "Quantization",
        "Benchmark"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "Benchmarking Large Language Models User Experience for Mobile Deployment",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=xzSUdw6s76",
    "pdf_link": "https://openreview.net/pdf?id=xzSUdw6s76",
    "comments": [
        {
            "summary": {
                "value": "This paper describes the benchmarking results of several quantized Large Language Models on smartphones and edge devices. Specifically, it quantifies the CPU, GPU, memory and energy consumption of running inference on device, along with the accuracy and performance degradation across various dimensions (hallucinations, toxicity) as a consequence of quantization."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper  quantifies the side-effects of quantization in various dimensions in language modelling on downstream tasks, including hallucinations and toxicity. This a valuable insight to the community.\n* The multitude of devices that the authors have integrated are welcome, but lack the highest performing tier on Android (e.g. Snapdragon 8 Gen 2) and Linux (e.g. Jetson Orin AGX).\n* I also greatly appreciate the integration of profiling measurements for reporting low-level CPU, GPU and memory utilization of the LLM inference workload."
            },
            "weaknesses": {
                "value": "* The novel contributions of this paper are significantly fewer than originally stated and have been documented in prior work(s). Such include the evaluation of performance, energy and accuracy degradation of compressed LLMs on various devices, across similar frameworks. While I do agree that the downstream task performance quantification, along with the hallucination/alignment dimension is an important one, it seems to be the main novel contribution.\n* The energy measurement methodology followed by the paper is largely software-based and thus depends on the vendor's implementation. Moreover, comparisons across ecosystems can be quite deceptive.\n* The paper would benefit from a more rounded background/related work section, which describes prior work more faithfully and includes background information about the models and evaluated quantization methods."
            },
            "questions": {
                "value": "### Motivation\n\n* The authors suggest in the introduction that \"running LLMs locally\" can lead to \"increased reliability\". Personally and from experience, I would not be that confident on making such claims, i.e. that cloud deployments are less reliable that deploying locally, especially given the infancy of the evaluated frameworks. I would appreciate some more substantiation here from the authors.\n\n### Methodology\n\n* Wrt methodology and experimental setup, the paper misses substantial information that hurt the overall reproducibility potential. Such omissions include:\n    - Operating system and framework versions\n    - Automation of iOS interface\n    - How different components in Figure 1 actually coordinate and communicate.\n* Would the authors be willing to open-source their benchmarking system?\n* It is unclear whether the authors have converted the models themselves or have used the pre-compiled versions offered in GGUF and MLC repositories.\n* How do the authors ensure that the profiling methodology does not interfere with the behavior of the experiment? Also, how do the authors isolate the power lanes from the USB communication?\n* The authors claim that they are using a \"professional USB power meter for accurate power consumption measurements\". However, it is not clear how this yields the information needed, as devices are battery powered and not USB-powered. As such, the power draw from the USB does not yield any information related to energy consumption from a workload on device.\n\n### Evaluation\n\n* The evaluation does not integrate variance metrics. Should it be assumed that the experiments have run once?\n* What is the decoding strategy and hyperparameters that the authors use during evaluation?\n* With respect to temperature, does the average represent \"average over time\" or \"average over surface\"?\n* A missing dimension that is worth exploring in such a paper is the tradeoff between size due to parameters and due to quantization (or another compression method). For example, is it better using a lower-bitwidth llama-3.1-8b model on 4bits or a llama-3.2-3b model on 8 bits?\n* What is the bitwidth used in Figure 2 across models?\n* In Figure 3, the GPU utilization across phones is quite large, which comes in contrast with the \"memory bounded\" workload claim of the authors. I would appreciate some more insights here.\n* \u00a74.2: \"3-bit quantization results in lower CPU and GPU usage [...] decreased data transfers [...] reduced inference workload\": I am not sure this claim is correct or substantiated properly.\n* A great source of information to the reader, also for showcasing the memory-boundedness of the workload would be to plot a roofline model for devices/models.\n* The authors claim that iPhones provide significantly higher throughputs compared to other devices. This may not be painting the whole picture, as it is not clear whether this throughput can be sustained compared to the actively cooled Jetson Nano for instance.\n\n### Related work\n\n* The authors claim that prior work (MELT) has examined resource utilization and energy efficiency in a limited manner, and did not explore GPU workloads. However, this is not true. \n    * Additionally, the authors do not run llama.cpp on iOS, which prior work has done.\n    * Palmbench does not measure power consumption via hardware probes on phones and do not measure it at all on edge devices.\n    * Palmbench does not report on prefill rates, which prior work does.\n    * Palmbench does not integrate high-end edge devices (e.g. Jetson AGX) and different power profiles, which prior work does.\n* Moreover, the authors have unfortunately missed other related work in the domain of on-device LLM benchmarking [a-c].\n\n[a] Murthy, R., Yang, L., Tan, J., Awalgaonkar, T. M., Zhou, Y., Heinecke, S., ... & Savarese, S. (2024). MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases. arXiv preprint arXiv:2406.10290.  \n[b] Lu, Z., Li, X., Cai, D., Yi, R., Liu, F., Zhang, X., ... & Xu, M. (2024). Small Language Models: Survey, Measurements, and Insights. arXiv preprint arXiv:2409.15790.  \n[c] Xu, J., Li, Z., Chen, W., Wang, Q., Gao, X., Cai, Q., & Ling, Z. (2024). On-device language models: A comprehensive review. arXiv preprint arXiv:2409.00088.  \n\n### Presentation and Other Nitpicking\n\n* Table 1, 2: Please put the captions above.\n* Llama-3/3.1: Missing reference\n* Throughput in Table 2 refers to generation throughput.\n* MELTing: I believe the framework is called MELT.\n* Figure 2: A boxplot presentation would be significantly richer in information, also showing peak memory usage (which can be the main factor for OOM errors).\n* Figure 4: The overlay of the two workloads suggests that they are running simultaneously. An alternative visualization which also annotates what's happening at each time step would be more informational to the reader.\n* \u00a74.2: \"To keep the models in suitable size [...] evaluate the total memory sage of models\": I am not sure about what the authors mean here.\n* \u00a74.2: \"[...] higher quantization escalates GPU memory [...]\": Escalates might not be the correct word here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper describes a benchmarking process for compressed LLMs on mobile devices. It characterizes GPU and CPU utilization, speed, latency, memory requirements, power consumption, temperature, accuracies, and rates of undesirable behavior for several models on mobile and edge devices."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper is in a good area. The efficient execution of LLMs on edge devices is important, both due to practical impact and because these platforms have tight resource constraints that can motivate new ideas that are more broadly applicable.\n\nThe \"comprehensive\" in the title is accurate regarding the range of measures and will set a high bar for others characterizing LLMs running on mobile and edge devices.\n\nThe paper has good tutorial value. Even though the focus is on benchmarking, it tersely and clearly describes several approaches to making LLMs more efficient.\n\nThe benchmarking findings are likely to be of interest to designers and researchers working on using LLMs on resource-constrained platforms."
            },
            "weaknesses": {
                "value": "The paper has several findings regarding relative performance of different compression/quantization schemes. Some of these are counter-intuitive. The paper points these out but does not explain them. This holds true for findings on pages 9 and 10. The paper would probably be more interesting if it went deeper into the reasons for the surprising findings.\n\nBenchmarking papers are generally not as interesting (I am thinking about conference attendees, not reviewers) as papers with novel design approaches or surprising and important scientific findings, and that holds true for this paper in my opinion. However, benchmarking is important so I view this as a small weakness. This work needs to be done and the authors did it well."
            },
            "questions": {
                "value": "I'm not familiar with the 0-bit Llama model you mention on page 6. Isn't the bit count referring to quantization bits? How can that be zero? Is it a typo or just my ignorance?\n\nWhy FLIR in addition to power? There is a strong, deterministic relationship between power and temperature so the FLIR data should be useful primarily when spatial heterogeneity is important.  Is this for user discomfort due to contact with the phone or some other purpose? What was the approximate thermal time constant for the system spanning power dissipation location (mostly transistors) to smartphone surface? Did the benchmarks run long enough for the temperature distribution to reach steady state?\n\nWhy did higher bit width models often have lower quality measures than similar lower bit width models? Is this noise or something fundamental do to the influence of bit width on inductive biases or learning dynamics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper conducted extensive measurements of various on-device metrics and accuracy metrics for several popular LLMs on mobile devices to evaluate the effects of model size, quantization, hardware, inference engine, etc."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors considers a large collection of LLMs, mobile platforms, and quatization schemes. The findings can be helpful for future researchers.\nThe organization of the paper is logical, which makes it easy to follow the author's arugments. \nThe authors give some valuable insights based on the experimental results, such as the usefulness of 2-bit quantization for certain tasks in Section 4.5."
            },
            "weaknesses": {
                "value": "Given it was a heavily empirical paper, the authors did not clearly state whether the experiments were repeated multiple times to validate the results, or better, verified on multiple devices of the same family. It could be a challenge to gather all the devices, but the experimental results can be strengthened if the authors clearly stated that the devices were factory reset and repeated several times.\nEven though the large collections of experiment is comprensive, it can be overwhelming to make sense of the results, especially without the author's interpretation. For example, Sect. 4.2. It is probably more meaningful to emphasize on some comparisons and highlight the difference and take-aways. \nSimilar comments apply to Section 4.4. Any insights why 5-bit and 3-bit quantziation are worse?\nEven though the authors claim it to be a framework, it is not readily reproducible by other labs, hence hard to be benchmarked by external teams."
            },
            "questions": {
                "value": "What does complentary mean in Section 3.3.7: two complementary devices ?\nI didn't get this statement in Sect. 4.2. CPU and GPU Utilization. Additionally, the iPhones exhibited ..., indicating the potential for optimization .... I though lower utilization is a good thing. In that case, why more optimization?\nI wonder whether the inconsistencies, such as in Sect. 4.7, 3 bit is worse than 2 bit GWQ and 4-bit, is due to unrepeated experiments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Serving LLMs on mobile devices is becoming increasingly important due to privacy and security requirements.\nDeploying LLMs on mobile platforms is challenging as the developer may need to trade-off between model quality, inference performance and power efficiency.\nCurrent LLM benchmarks primarily target cloud platforms, with only a few for limited mobile devices.\nThis paper proposes PalmBench, a comprehensive benchmarking framework to evaluate the user experience of LLMs on mobile devices.\nThe framework evaluates LLM on metrics including efficiency, accuracy and toxicity.\nThe framework supports multiple quantized LLMs and multiple mobile devices with different types of OSs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is written in good quality and easy to follow.\n2. The framework supports a range of devices and LLMs.\n3. The experiments are presented in detail with an analysis of the key metrics supported by the framework."
            },
            "weaknesses": {
                "value": "1. Lack of explanation on how the automated testing is performed end-to-end.\n2. Unclear how easy it is to support new mobile devices or new LLMs.\n3. Some of the analysis and results seem to be trivial."
            },
            "questions": {
                "value": "Thank you for submitting to ICLR 2025! \nI enjoy reading the paper. \nHelping developers better understand the tradeoff between model and system metrics for deploying LLMs on mobile devices is an important task.\nI have a few comments and questions about the paper and it would be great if the authors could address them.\n\nFirst, I think the paper should explain how automated testing or profiling of LLMs on the mobile device is conducted end-to-end at a high level.\nThe current Section 3 explains different metrics, LLMs, inference engines and so on in a quite detailed fashion.\nHowever, it is not clear how a user or developer could use the benchmark to perform profiling.\nIt is also unclear how the framework could be extended to profile new mobile devices or new LLMs.\nAlso, do users need external equipment to fully use the benchmark?\nFor example, in Section 3.3.7, the paper mentions that thermal behavior analysis is done using a FLIR C5 thermal imaging camera and a professional USB power meter.\n\nDespite a comprehensive analysis presented in the evaluation, I feel some of them sound trivial.\nFor example, for memory utilization, higher quantization levels consume more memory, while lower quantization than 4-bit reduces memory needs.\nFor CPU and GPU utilization, models with 4-bit quantization utilize more GPU duty cycles than those with 3-bit quantization.\nCould the user get more insights from the results presented by the benchmark?\nFor example, for memory utilization, other than model weight, how much additional memory overhead is present in each setting and what may be the reasons?\nFor CPU and GPU utilization, could we obtain detailed layer-by-layer profiles and analyze what is the bottleneck operation?\nWhat are some important implications or design decisions we can make from utilizing the benchmark?\n\nOther questions:\n1. In Section 4.2, it says \"LLM inference is inherently memory-bound, and its memory utilization can be reduced significantly by memory bandwidth quantization\". What is memory bandwidth quantization?\n2. Can we evaluate the LLMs on mobile devices using CPU-only mode, or does the framework require the devices to have GPUs?\n3. Do you have evaluation numbers for prefill and decode throughput respectively?\n4. What is the batch size used in different settings when evaluating the throughput of LLMs?\n5. How is the exact match and F1 score calculated when comparing quantized and non-quantized models? How to determine if the score is good or bad?\n6. In Section 4.4, it says \"Interestingly, the 5-bit and 3-bit models underperformed slightly. K-quant (ggml) 3-bit model produced more hallucinations and toxic content than the 2-bit model.\" How are hallucination and toxicity observed in Figure 8?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces PalmBench, a benchmark framework for evaluating LLM on edge computing scenarios, and also conducted evaluation on various models under different benchmark datasets. \nThe key goal of this paper is to complement the lacking of an LLM benchmark at the edge.\nThe proposed framework allows automated testing of various LLMs with different compression settings across multiple mobile devices.  This paper also evaluates various popular LLMs with different benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- A benchmark for evaluating LLM for edge computing scenarios is necessary, and this study focuses on this.\n- Experimenting covering iOS, Android, and edge devices with detailed hardware specifications"
            },
            "weaknesses": {
                "value": "- The overall system design of PalmBench is vague to me and not elaborate in the paper; I am not able to get information on how each of the components collaborates in Figure 1 and what's the hierarchy of the system. \n\n- The Benchmark Automation Implementation details are lacking, it's hard to know how PalmBench to process different hardware platform benchmark query.\n\n- There is already have Benchmark [1] for evaluating LLM on Edge-Device, which should be properly discussed in this paper.\n\n\n[1] https://github.com/TianjinYellow/EdgeDeviceLLMCompetition-Starting-Kit?tab=readme-ov-file#submission-requirements"
            },
            "questions": {
                "value": "- What's the control flow (hierarchy) of the framework and how do different components interact? For instance, the paper lacks crucial details about how user queries are processed and evaluated using their framework, particularly in relation to AGI\n\n- The Fig.1 shows a linear flow from models to automation framework to metrics, but how do different quantization methods (Group-wise, GGML, GPTQ, AWQ) integrate into this flow? Shouldn't the quantization block be positioned between Models and Frameworks since models need to be quantized before being deployed through MLC or llama.cpp?\n\n- I was trying to reproduce the supplement materials, and trying to perform some benchmark tasks. However, I am unable to reproduce the experiments shown in the paper. I believe the supplementary materials is not complete.  It looks like it's based on the NVTOP and processes the returning string. I'm trying to perform some benchmarking, could you please provide some details how to run your pipeline, ?\n\n- Could you provide a detailed description showing how AGI, custom apps, and LLM frameworks interact in your automated system? (e.g.,What's the exact interface between your custom apps and the LLM frameworks (MLC/llama.cpp)?\n\n- How you process user's query to evaluate the model? should it must have llama.cpp support, what if my model is not in llama.cpp model zoo?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}