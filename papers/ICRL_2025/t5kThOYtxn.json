{
    "id": "t5kThOYtxn",
    "title": "Stable batched bandit:  Optimal regret with free inference",
    "abstract": "In this paper, we discuss statistical inference when using a sequential strategy to collect data. While inferential tasks become challenging with sequentially collected data, we argue that this problem can be alleviated when the sequential algorithm satisfies certain stability properties; we call such algorithms stable bandit algorithms. Focusing on batched bandit problems, we first demonstrate that popular algorithms including the greedy-UCB algorithm and $\\epsilon$-greedy ETC algorithms are not stable, complicating downstream inferential tasks. Our main result shows that a form of elimination algorithm is stable in the batched bandit setup, and we characterize the asymptotic distribution of the sample means. This result allows us to construct asymptotically exact confidence intervals for arm-means which are sharper than existing concentration-based bounds. As a byproduct of our main results, we propose an Explore and Commit (ETC) strategy, which is stable --- thus allowing easy statistical inference--- and also attains optimal regret up to a factor of 4.\n\nOur work connects two historically conflicting paradigms in sequential learning environments: regret minimization and statistical inference. Ultimately, we demonstrate that it is possible to minimize regret without sacrificing the ease of performing statistical inference, bridging the gap between these two important aspects of sequential decision-making.",
    "keywords": [
        "Batched Bandit",
        "Inference in Bandits"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=t5kThOYtxn",
    "pdf_link": "https://openreview.net/pdf?id=t5kThOYtxn",
    "comments": [
        {
            "summary": {
                "value": "Summary:\nThe problem studied is to design multi-armed bandit algorithms for stochastic bandit problems \nso that the central limit theorem (CLT) holds for the rewards collected for each of the arms in the limit of infinitely many interactions. Let's call a bandit algorithm CLT friendly if it satisfies this criterion.\n\nThe notion of \"stability\" of bandit algorithms is introduced. According to this definition, a bandit algorithm is stable, if for any bandit instance of interest, there exist deterministic sequences $\\{ n_{a,t} \\}$ such that $\\frac{N_{a,t}}{n_{a,t}}$ converges to one in probability where $N_{a,t}$ is the number of pulls of arm $a$ up to round $t$ on the given instance (a random quantity).\nA simple calculation shows that stable bandit algorithms are CLT friendly.\nThe authors also show that ETC (explore for, say, half the time, then choose the better arm out of, say, two) is not stable. They also cite previous research that shows that ETC is not \"CLT friendly\", which indicates that enforcing stability may be necessary for CLT friendliness.\nThen they design an ETC-style method, which explores in the first phase, but then instead of choosing greedily, uses confidence bounds with the data of the first phase to eliminate arms. If multiple arms remain, the algorithm splits the remaining time equally between them (the authors use randomization for this). If a single arm remains, that arm is pulled up to the end. This algorithm is shown to be stable.\n\nSignificance: The problem is not entirely new, several authors looked into CLT friendliness previously. This reviewer is not completely sold on this notion: CLT is truly asymptotic, it is unclear what this notion really buys for practice if anything. Also CLT friendliness could be achieved easily if we did not demand to use all rewards from all the arms: just allocate a fixed, even diminishing portion, of all time steps to uniformly exploring the arms and use the rewards collected during this period. The results of this paper are not strong enough to discard an algorithm like this on reasonable grounds. In other words, not much depth is achieved in the paper.\n\nNovelty: The notion of stability is novel. The proofs are quite standard/automatic (even though I am not completely happy with how, e.g., the proof of Theorem 1 is done).\n\nRelated work: Somehow the authors want to connect this to batched bandits, but at least the main paper did not do much with this. The algorithm presented for the batched case with B cases raises more questions than it answers (maybe a presentation issue).\n\nSoundness: I think the claims made are correct. I verified things to a reasonable degree in the main text.\n\nPresentation:  There are a number of typos, grammatical issues (e.g., line 155: \"We assume Let ...\", line 158: genrality, etc.) I will list a few more of these at the end. In the algorithms, the authors use \"or\", but this should be \"otherwise\" (last line of all algorithms). This was very confusing. Also, the proof of Theorem 1 is quite messy (one of the two proofs in the main body). The authors state \"It suffices to study the behavior of $n_{1,T}$ on the high probability event $\\mathcal{E}_T$\". Why? In what sense? (Later we find out, but this is not the sign of a well written text.) Also, only in the middle of the proof we find out that there will be two cases based on the value of $\\beta$. This proof definitely can use polishing, as can the rest of the paper."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper does make novel contributions."
            },
            "weaknesses": {
                "value": "I did not find the topic well motivated. The paper feels weak on contributions: A strong paper would study the tradeoff between CLT friendliness and performance; nothing of this form is attempted here. The results, while they appear to be correct, do not require much effort. The batch bandit version of the algorithm is, hmm, unexpected. The presentation is poor; it feels that the paper need much work."
            },
            "questions": {
                "value": "It seems that in the batch bandit version of the algorithm in every batch, only data for that batch is used. The effect of previous batches is discarded. At least this is how the algorithm seems to be defined. While this may make the method CLT friendly, this will be a very bad bandit method. What is the point here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work explores statistical inference under sequentially and adaptively collected data. The authors focus on the batched bandit setting and show that ETC type policies are poor with inference due to lack of a property called \"stability\". They show that a form of Successive Elimination algorithm achieves stability and have good asymptotic inference properties as well as optimal regret bounds. They also generalize their results from the 2-batched setting to a multi-batched setting."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The context is relatively easy to follow. The problem considered is interesting."
            },
            "weaknesses": {
                "value": "Though this paper investigates an interesting and important problem, I am afraid the preprint is far from being ready for publish.\n\n1. Contribution. What I am very confused is you contribution.\n- You mention in the abstract you show that popular algorithms including the greedy-UCB algorithm and $\\epsilon$-greedy ETC algorithms are not stable. Maybe I missed it, but where is it in the main context? Seems you only show the instability of the simple $\\epsilon$-greedy ETC. The short argument on Line 268-269 seems to be very vague and built entirely on Zhang et al. (2020).\n- You mention it is possible to minimize regret without sacrificing the ease of performing statistical inference. I think this is an overclaim. Your regret rate is in a non-asymptotic sense, but your inference task is in an asymptotic sense. Convergence in probability is a relatively weak measure --- it is a point-to-point convergence and one does not know the convergence rate. Also, in your result you show a sacrifice of factor $4$ in the regret bound.\n- More importantly, it is very unclear how your work should be placed compared to Khamaru & Zhang (2024) which you also cited in the proof section. Khamaru & Zhang (2024) revisited the stability property, gave a detailed investigation of the asymptotic statistical properties of standard UCB. They also investigated the multi-armed setting (the number of arms $K$ can even be scaling with $T$). \n  - Can you elaborate on your additional contribution? Stability is not new. The algorithms (ETC and Elimination) are not new. The results seems to be similar (or maybe even weaker) compared to those from Khamaru & Zhang (2024).\n  - Also, it seems the technical tools are largely following the literature. Is there any intrinsic difficulty within (a) studying Elimination rather than UCB and (b) studying the batched setting?\n\n2. Writing. The writing of this paper is relatively poor.\n- The results and proofs are written in a rather arbitrary way. Some gave a partial proof with equations, some invoked other works directly, while some simply mentioned that \"a careful look at the proof reveals that one can replace ...\". The content is indeed easy to follow, but the reading experience is not good.\n- In Theorem 1 you are taking $m$ and $T$ to $+\\infty$ such that $T-2m$ goes to $+\\infty$. But in Theorem 2 the scaling of $m$ is unclear. Seems there is no? There are also many typos in the paper. For example, in both Theorem 1 and 2, the empirical variance term is in the wrong place; in Line 796-797, the equation after \"the fact\" is strange and contents after \"Recall\" are not in math format; the usage of \u2234 is not formal. \n- There is only a small simulation example at the beginning of the paper. I understand the paper is focusing on theory, but since you claim you propose new algorithms (which are not hard to implement), it is anticipated that more comprehensive experiments should be conducted.\n\nI believe the authors should provide convincing response to my concerns above (particularly the contribution part), in case there is a small possibility that I misunderstood the paper."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I think there is a possibility that the authors claim something already appeared in the literature as their own contribution. In particular, the definition of stable algorithms and the instability of $\\epsilon$-greedy have been established in Khamaru & Zhang (2024). They cited the paper in the proof section but not in literature review or the contribution section."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce a class of algorithms known as \"stable bandit algorithms\" in which classical statistical methods used for iid data can be used for inference. Most of the results and discussion in the main paper is for 2 arm bandit setting, and this case, the authors show that the CLT holds for the sample means of the arms. The authors also show that the vanilla epsilon greedy explore-then-commit algorithm does not satisfy the stable bandit setting, and further go on to show that the sample means of the arms arms in this strategy does not satisfy the CLT. As a subsequent step, the authors propose a modified version of the algorithm that does belong to the class of stable bandit algorithms, and prove that its regret is asymptotically optimal (upto a constant factor). The authors also introduce another another algorithm \"B-batch algorithm\" that is also shown to be stable, and have nice asymptotic properties of the sample means of the arms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "In my opinion, the strength of the paper is how simple and elegant the theory is. It's nice to see that simple bandit algorithms that require such little computations have nice statistical properties."
            },
            "weaknesses": {
                "value": "1. I do not fully understand the motivation of the results. The authors claim \"bridging gap between statistical inference and minimizing regret\" but I do not understand what they mean by that exactly -- can the authors provide more concrete examples on how they do this? They discuss how previous analysis use \"Martingale structure\" in sequentially collected data for analysis, but so what? \n\n2. I also don't understand what these results lead to in the practical / theoretical sense. The only future work the authors discuss is extension to K arm bandit setting: Can the authors provide specific examples of practical applications or theoretical implications of their results. How can stable bandit algorithms be applied in real-world scenarios?\n\n3. The authors mention \"many\" of the results in the paper extend to K-arm bandit settings, but which ones? I did not see any discussion on this topic -- the authors should let us know how the results extend to K-arm settings, and also the implications.\n\n4. I don't know why these results are complex / novel -- could the authors explain why this analysis is special in the context of bandit literature?\n\nOverall in my opinion, I don't think the paper is appropriate for the venue. Simple, elegant results but it is not motivated enough either in terms of theory or practice."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper considers the problem of statistical inference in context of sequentially / adaptively collected (and hence non-i.i.d.) data as arising in the multi-armed (batched) bandit setting.  In particular, the authors introduce a notion of stability for bandit algorithms, and prove that under their stability condition, sample means for arms are asymptotically normal, which allows the construction of valid confidence intervals.  They show that a commonly considered, simple explore-then-commit (ETC) strategy is unstable (and indeed yields non-normal reward means). They then provide a stable alternative, which, in the first stage, samples arms equally often, and in the second stage samples all plausibly optimal arms equally often. They also provide variants of this scheme, with adaptively selected duration of the first stage (which yields an algorithm with optimal asymptotic minimax regret up to a constant of 4), or with multiple stages of fixed length."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "I very much appreciate the perspective for analysing multi-armed bandits using the lens of stability. While stability has been known to imply generalisation bounds in classical learning theory, its application to the analysis to bandits I had not seen before.  I could imagine it becoming a fruitful direction for future research in this area.  The fact that stability allows treating the data as effectively generated i.i.d. is potentially impactful.  The paper is generally clearly written (albeit there\u2019s a number of typos)."
            },
            "weaknesses": {
                "value": "While very interesting conceptually, the submission only considers a rather restricted setting:  A variant of batched bandits with two arms only.  While in Section 2, the authors claim that \u201cmany\u201d of their results generalise to the K-armed setting, no further information is provided (which results? How do the results generalise?), and in Section 5, they mention the extension to the K-armed case as interesting future work.  In my opinion, stability would become a much more interesting concept if the results could be generalised to richer families of bandit instances, e.g., bandits with structured reward functions (e.g., linear bandits).  Besides the limitation to two arms, all presented results are only asymptotic in nature.  Thus, the work appears a bit preliminary for me.\n\nAnother, and perhaps more severe, concern is with the novelty of the presented framework.  The authors cite a paper entitled \u201cInference with ucb\u201d.  Aiming to understand the relative contributions of the present submission, I found an article with the title \u201cInference with the upper confidence bound algorithm\u201d on arXiv (https://arxiv.org/pdf/2408.04595).  This paper appears to introduce the same notion of stability for bandit algorithms and uses it to analyse the upper-confidence bound (ucb) bandit algorithm (in fact, Lemma 1 appears to be shown in that paper as well).   The paper appears to provide the same general conclusion (i.e., under stability, data can be effectively treated as i.i.d.) as in the ICLR submission.  The main difference appears to be that an analysis is presented for a different family of bandit algorithms (ucb).  With respect to this paper, the ICLR submission looks rather incremental, even more so in light of the rather restricted setting (as argued above).\n\nThe manuscript has a number of typos (here\u2019s some of them):\n- 60: maybe -> may be\n- 158: genrality\n- 224: on consistent estimator of\n- 286: pull both arm"
            },
            "questions": {
                "value": "- Can you please clarify, in your view, the relation and relative contribution to the paper https://arxiv.org/pdf/2408.04595?\n- Can you please elaborate on whether there are meaningful connections to generalisation bounds obtained from stability in classical learning theory [cf. A,B]?  \n- Section 2 makes the claim that \u201cmany\u201d results generalise to the K-armed setting.  Which results?  And how do they generalise?\n- \\sigma_{a,T} as used, e.g., in line (10) seems to refer to the variance, in (11) it seems to be used as standard deviation.  Can you please clarify?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Given a set of arms and a bandit algorithm that collects data, this paper aims to understand the asymptotic properties of the empirical mean of each arm observed by running the algorithm. The focus is on the setting of 2 arms. While existing methods do not typically use information about the algorithm itself, this paper takes an algorithm-dependent approach, via the notion of stability (defined in Section 3.1). In particular, for stable algorithms, the empirical means are asymptotically normal (Lemma 1). This means that one can readily construct confidence intervals for the true means.\n\nThe paper discusses two types of algorithms: 2-batch and $B$-batch. For the former, they show that a vanilla $\\epsilon$-greedy Explore Then Commit (ETC) strategy is not always stable (Lemma 2). They then modify the algorithm into a stable ETC variant (Algorithm 1); its stability is proved in Theorem 1. In both of these algorithms, the learner explores for a fixed number $\\sim m$ of rounds and subsequently acts greedily. The authors then devise Algorithm 2, where they replace $m$ with a random stopping time. They then show that Algorithm 2 is stable (Corollary 2) and has asymptotically optimal regret, up to constants.\n\nLastly, they study a $B$-batch procedure. Algorithm 3 pulls each arm $\\sim m$ times in each one of $B$ batches. Theorem 2 then shows that this algorithm is stable (when $m$ is fixed and $B\\to\\infty$)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Overall, the paper is well-structured and clear, but would benefit from some clarifications (addressed below).\n\n- It is an interesting approach to show asymptotic normality based on properties of the algorithm. \n    \n- I think asymptotic normality is indeed important to be able to do inference on the arms (e.g., construct confidence intervals).\n\n- The algorithms are relatively clear and simple to understand."
            },
            "weaknesses": {
                "value": "- The 2 arm assumption is fairly limited. It might be productive to discuss how this can be extended.\n\n- The paper would benefit from additional intuition on some concepts, such as (i) the definition of stability and (ii) the ratio in the constraint on $m$ of Equation (7).\n\n- In Section 3.1, the authors should define $z_\\alpha$.\n\n- In Corollary 2, the authors should define regret.\n\n- In Theorems 1 and 2, in the convergence in distribution to the Gaussian, shouldn't the LHS be dividing by $\\hat\\sigma_{a,T}$ instead of multiplying?"
            },
            "questions": {
                "value": "- Is there a reason why in Algorithm 3, provided $\\mathcal{A}_{b}$ contains both arms, we pull each one exactly $m$ times as opposed to the other algorithms, where we pull each one with probability $1/2$?\n\n- How do you think the analysis for $K$ arms would work? Would it be a simple extension, or require more sophisticated techniques?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}