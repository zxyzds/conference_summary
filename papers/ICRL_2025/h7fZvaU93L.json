{
    "id": "h7fZvaU93L",
    "title": "Semantically Consistent Video Inpainting with Conditional Diffusion Models",
    "abstract": "Current state-of-the-art methods for video inpainting typically rely on optical flow or attention-based approaches to inpaint masked regions by propagating visual information across frames. While such approaches have led to significant progress on standard benchmarks, they struggle with tasks that require the synthesis of novel content that is not present in other frames. In this paper, we reframe video inpainting as a conditional generative modeling problem and present a framework for solving such problems with conditional video diffusion models. We introduce inpainting-specific sampling schemes which capture crucial long-range dependencies in the context, and devise a novel method for conditioning on the known pixels in incomplete frames. We highlight the advantages of using a generative approach for this task, showing that our method is capable of generating diverse, high-quality inpaintings and synthesizing new content that is spatially, temporally, and semantically consistent with the provided context.",
    "keywords": [
        "diffusion models",
        "video inpainting",
        "conditional generative modeling"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "Conditional video diffusion models are capable of solving challenging video inpainting tasks where current state-of-the-art video inpainting methods struggle.",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=h7fZvaU93L",
    "pdf_link": "https://openreview.net/pdf?id=h7fZvaU93L",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a conditional generative framework for video inpainting that leverages diffusion models to synthesize new, semantically coherent content for large occlusions. Unlike traditional methods dependent on optical flow, this approach achieves realistic object behavior and maintains temporal consistency even when entire content regions must be generated. Evaluated on newly constructed datasets, the framework demonstrates superior quality and consistency in inpainted sequences compared to existing methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The method effectively captures temporal and spatial dependencies, enabling high-quality synthesis for occluded scenes and objects.\n2. Quantitative results show clear improvements over state-of-the-art methods, especially in challenging video inpainting scenarios."
            },
            "weaknesses": {
                "value": "1. Please provide inference costs (e.g., time and peak GPU memory usage) for a specified input size and number of frames, and compare these with other SOTA methods.\n\n2. A dedicated ablation study comparing the method trained on standard datasets used by other methods versus the newly introduced datasets is suggested. This would help isolate the effects of the method itself from those of the datasets, as data alone can often address significant aspects of the problem.\n\n3. The framework lacks clear illustration, making it difficult to grasp the main concepts. For instance, it is unclear how conditional inputs are processed in the forward pass, the roles of the trainable and frozen modules, or if any new layers for controllable generation are introduced.\n\n4. Please also discuss the potential limitations or failure cases of the proposed method. Beyond inpainting cars, can this method inpaint other objects in a zero-shot manner?"
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors extend the idea of Flexible Diffusion Model (FDM) to the video inpainting domain. A different sampling and training scheme is also developed to accomodate the new task."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is easy to follow\n- The idea is intuitive and works also practically well\n- The authors showcased that their model is better than flow- and attention-based video inpainting methods both qualitatively and quantitatively\n- Different sampling schemes are also evaluated extensively."
            },
            "weaknesses": {
                "value": "Given that the authors proposed a variety of inference techniques, it would be interesting to investigate whether different sampling schemes during **training** makes a difference. It seems from Equation 3 that the $\\mathcal{X}$ and $\\mathcal{Y}$ are randomly sampled; would a benefit be gained e.g., when training the model in a similar way to how you sample at inference?"
            },
            "questions": {
                "value": "Is the model trained from scratch? How well does it generalize to other, more complicated open video domains?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "State-of-the-art video inpainting methods focus on propagating visual information but struggle with generating novel content. This paper redefines video inpainting as a conditional generative task using video diffusion models, enabling synthesis of inpaintings with long-range temporal consistency. The approach leverages inpainting-specific sampling schemes to handle incomplete frames and is effective even when objects are partially visible or absent in the initial context."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors invest significant effort in building a large-scale video dataset to train the diffusion model.\n2. Compared to previous video inpainting methods, the proposed model emphasizes filling in unseen content rather than relying solely on propagation."
            },
            "weaknesses": {
                "value": "1. Practicality concerns: The proposed dataset is heavily focused on scenarios involving cars, which restricts its generalizability to broader real-world applications.\n\n2. Lack of control over inpainting content: The proposed model does not offer mechanisms to specify what should fill the missing regions.\n\n3. Limited diversity in demonstrations: All examples shown in the paper involve cars in the inpainted regions. Are there demonstrations with objects other than cars?\n\n4. Comparison with recent methods: Some recent mask-guided video editing techniques can also fill gaps with cars, offering options to specify the car model and color via text prompts. Why is there no comparison with these methods?\n\n5. The claim of \u201cstrong experimental results\u201d is potentially biased. The authors only test their model on BDD-Inpainting and Traffic-Scenes (datasets used for model training), while baselines are only trained on Youtube-VOS and DAVIS. This discrepancy in training data limits fair comparison."
            },
            "questions": {
                "value": "Inference time: How long does the proposed model take to process a typical 2-second video clip?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel video inpainting framework based on conditional diffusion models, which can generate semantically consistent content that remains realistic over extended occlusions and diverse scene complexities. The authors present sampling schemes tailored for inpainting that capture essential long-range dependencies within the context and develop a new method to condition on known pixels in incomplete frames. Results show that the proposed method consistently outperforms methods based on optical flow or attention."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The motivation is clear and the techniques sound reasonable.\n- The writing and organization are easy to follow.\n- The experiment result validates the effectiveness of this approach, especially for complex scenarios where occlusions (input information is scarce) prevent conventional methods from performing well."
            },
            "weaknesses": {
                "value": "1. The author needs to highlight this paper\u2019s innovative contributions in the INTRODUCTION part and present a completed method graph rather than just showing model inputs.\n2. This work relies heavily on the inpainting model to handle object occlusions and interactions. However, the authors do not address how the model performs in highly cluttered or dense environments with multiple possible object placements.\n3. Additional experiments are needed to demonstrate the effectiveness of the sampling schemes and the model's generalization to non-traffic datasets, along with further comparisons/discussions with recent works, such as [1] [FFF-VDI](https://arxiv.org/pdf/2408.11402) [2] [FGT++](https://arxiv.org/pdf/2301.10048) [3] [SViT](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Semantic-Aware_Dynamic_Parameter_for_Video_Inpainting_Transformer_ICCV_2023_paper.pdf)."
            },
            "questions": {
                "value": "- Could you also provide quantitative comparisons of your method on the YouTube-VOS and DAVIS datasets? Additional results for object removal tasks would be more convincing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}