{
    "id": "I1MKOjNVup",
    "title": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science",
    "abstract": "Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models (LLMs). However, to evaluate such systems, researchers typically rely on direct Question-Answering (QA) to the LLM itself or through biomedical experiments. How to benchmark biomedical agents precisely from an AI Scientist perspective remains largely unexplored. To this end, we draw inspiration from scientists\u2019 crucial ability to understand the literature and introduce BioKGBench. In contrast to traditional evaluation benchmarks that focus solely on factual QA, where the LLMs are known to have hallucination issues, we first disentangle \u201cUnderstanding Literature\u201d into two atomic abilities: i) \u201cUnderstanding\u201d the unstructured text from research papers by performing scientific claim verification, and ii) interacting with structured Knowledge-Graphs for Question-Answering (KGQA) as a form of \u201cLiterature\u201d grounding. We then formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based Retrieval-Augmented Generation (RAG) to identify factual errors in existing large-scale knowledge graphs. We collect over two thousand data points for the two atomic tasks and 225 high-quality annotated samples for the agent task. Surprisingly, we find that state-of-the-art general and biomedical agents have either failed or performed inferiorly on our benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent. On the widely used popular knowledge graph, we discover over 90 factual errors, which provide scenarios for agents to make discoveries and demonstrate the effectiveness of our approach.",
    "keywords": [
        "benchmark",
        "biomedical agent",
        "knowledge graph",
        "literature"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We introduce a benchmark to evaluate biomedical agents in checking knowledge hallucinations in KG with literature cross-verification.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=I1MKOjNVup",
    "pdf_link": "https://openreview.net/pdf?id=I1MKOjNVup",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes several tasks related to the scientific literature review and evaluates the performance of LLMs and LLM-based agents. The authors also propose an agent to address the task. These tasks include KG QA, scientific claim verification and the combined KG checking tasks. Lots of mainstream LLMs are tested. Several agent designs are also evaluated with access to the tools defined by the authors."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The constructed test cases for KG QA and claim verification might be helpful in evaluating clinical-related models and agents\n* The designed evaluation mechanism would be used as a good reference for future agent evaluation design"
            },
            "weaknesses": {
                "value": "- Novelty as a benchmark work: \n\t- When evaluating the LLMs only (Table 7), the LLMs do not have access to the KG or literature database, making the task a straightforward task. Similar tasks/datasets have been proposed a lot\n\t- When evaluating the agents (Table 8), the assumption is that the tool design is not part of the agent design (as the same set of tools are provided by the authors instead of designed by the designer of the baseline agents). This assumption makes this benchmark not generalizable and limited by the design of tools used to access KG and literature\n\t- The agent benchmark task is not designed to be result-oriented \n- Comparison with works of the KG-guided QA setting, such as Think-on-graph [1]\n\t- Both lines of work assume that external knowledge sources can be used to answer questions. What is the uniqueness of this work besides applying KG-enhanced QA to the biomedical domain?\n- Soundness of the evaluation metrics\n\t- Why the potential relationship is refute if the relationship is not on the KG (which could totally due to the sparsity of the KG)? What is the difference between existing relationship and potential relationship?\n- Missing details\n\t- For the claim verification task, when predicting label for a claim, does the model have access to the while corpus with 5664 articles, or it only have access to the particular article that the claim is induced from?\n\t- If this task is about global claims (instead of context-specific claims per article), how to handle the potential conflicts between claims?\n\t- What is the additional information the Qwen model used for evaluation has access to (compared with the evaluated LLMs that produce the answers)? \n\t- Does the framework support unseen nodes on the KG, or does it assume all entities used for evaluation have to be on the KG already?\n\t- For the baseline agent without access to the \"our tools\", how do they access the KG and literature pool?\n\n[1] https://arxiv.org/abs/2307.07697"
            },
            "questions": {
                "value": "Please refer to the questions in the \"Weaknesses\" section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces BioKGBench, a benchmark for evaluating AI agents in biomedical science, designed to tackle the challenges of understanding and verifying scientific knowledge. Unlike traditional benchmarks that focus on narrow tasks like question-answering, BioKGBench emphasizes two critical abilities: Knowledge Graph Question Answering (KGQA) and Scientific Claim Verification (SCV). These tasks help assess agents' abilities to analyze both structured (knowledge graphs) and unstructured (scientific literature) data. \nThe authors also introduce BKGAgent, a baseline model that effectively identifies inaccuracies in biomedical knowledge graphs, offering a useful tool for keeping scientific databases accurate and up-to-date."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper proposes three tasks\u2014Knowledge Graph Question Answering (KGQA), Scientific Claim Verification (SCV), and Knowledge Graph Checking (KGCheck).\n\n2. The study successfully identifies 96 inaccuracies within the knowledge graph, marked as \"Refute\" annotations. This finding is particularly significant because it challenges the common assumption that knowledge graphs contain precise information."
            },
            "weaknesses": {
                "value": "1. The proposed method utilizes the Retrieval-Augmented Generation (RAG) approach; however, it lacks comparison with recent RAG-based baselines such as Self-RAG and KG-RAG. This absence makes it challenging to evaluate the effectiveness of the proposed method against state-of-the-art techniques, potentially limiting the credibility of the results.\n\n2. The KGCheck task is primarily focused on verifying knowledge within knowledge graphs. While this is valuable, its narrow application may restrict the overall utility of the task in broader contexts.\n\n3. The approach may face scalability issues when applied to larger and more complex knowledge graphs. \n\n4. The study relies solely on one KG proposed in 2020, which may not reflect the most current state of knowledge representation. Given that the KG used is relatively old, there is a risk that the findings regarding inaccuracies and errors may not accurately reflect the current capabilities and improvements in more recent KGs. As KG evolve, their accuracy and completeness can change, potentially rendering the results of this study less relevant over time.\n\n5. The results of the study may not be easily generalized to other KGs beyond the one used. Different KGs can vary significantly in structure, content, and accuracy, making it uncertain whether the identified issues and the effectiveness of the proposed tasks will hold true across various knowledge representations."
            },
            "questions": {
                "value": "1. What's the difference between KGQA and STaRK [1]?\n\n2. Clinical Knowledge Graph used in the paper has difference references including [32], [33], and [53] in the paper.\n\n[1] Wu, Shirley, Shiyu Zhao, Michihiro Yasunaga, Kexin Huang, Kaidi Cao, Qian Huang, Vassilis N. Ioannidis, Karthik Subbian, James Zou, and Jure Leskovec. \"STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases.\" arXiv preprint arXiv:2404.13207 (2024)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces BioKGBench, which explores the LLMs\u2019 capability to do \u201cKGCheck\u201d - checking existing knowledge graph and cross-referencing with external literature or databases. They divide this task into two atomic sub-tasks: (1) knowledge graph question answering (KGQA), where the agent needs to answer questions of a knowledge graph; and (2) scientific claim verification (SCV), where the agent verifies a claim by querying the evidence from external datasets or literature. \n\nTo solve the tasks, this paper proposes BKGAent, which is a multi-agent framework containing a team leader, KG agent and validation agent equipped with a set of tools. The proposed framework achieves better performance than existing baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe sub-task design of KGCheck aligns well with the methodology of human scientific research, which consists of database queries and literature review. This benchmark may be useful for future study.\n2.\tThe proposed BKGAgent incorporates various tools and interactions between agents, and achieves better performance than baselines."
            },
            "weaknesses": {
                "value": "1.\tFrom Figure 5 and Table 8, the performance of \u201cBKGAgent\u201d is similar to that of \u201cAutoGen w/ our tools\u201d. It seems that the key performance gain is achieved by the tools. Thus, the effectiveness of using three agents in this framework is not clear.\n2.\tThe first sub-task \u2013 KGQA is closely related to the body of work on KBQA. However, the relation with KBQA is not discussed in this paper. Also, is it possible to benchmark KBQA methods in the KGQA subtask and use them instead of KG agents in the KGCheck task?\n3.\tThe performance of proposed framework on the sub-tasks are not reported and compared with baselines (Table 7).\n4.\tThe metrics (understanding, reasoning, efficiency, KG process, information retrieval) are not explained and it is not clear what these scores imply."
            },
            "questions": {
                "value": "1.\tWhat is the proportion of different reasoning types (one-hop, multi-hop, conjunction) involved in the KGCheck task?\n2.\tWhat is the recall rate of errors detected by BKGAgent on the KGCheck task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work investigates the challenge of \"ai agent for scientific literature\" that focus solely on factual QA, leading to LLM hallucinations. A novel benchmark BioKGBench which integrates a Knowledge Graph Question Answering task and a Scientific Claim Verification task is proposed. Detailed construction (data source, statistics etc.) and a baseline is proposed to validate the quality of BioKGBench."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "a) A novel benchmark called BioKGBench for evaluating biomedical ai agent is proposed.\n\nb) Extensive evluation on existing ai agents are compared.\n\nc) A baseline called BKGAgent is proposed."
            },
            "weaknesses": {
                "value": "a) Lack cost analysis of BKGAgent.\n\nb) The generalizability of proposed pipeline to other domains is not discussed.\n\nc) Minor writing issues."
            },
            "questions": {
                "value": "a) How the labels are made for SCV task? Any human agreement tests on the labeling?\n\nb) Do the authors have a vision on the difference between proposed agent method and the existing agent methods? How the difference can impact on the performance on BioKGBench?\n\nc) Do the authors have a vision on how the generalizability of proposed pipeline to other domains?\n\nd) In line 363-364, the authors may want to avoid using the word \"propose\" since the metrics used are common.\n\ne) How the classification is made for SCV task? Is it a self-critique / prompting process or a trainable process? If it's a prompt-based process, what is the scoring criteria?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}