{
    "id": "2TIYkqieKw",
    "title": "DICE: Data Influence Cascade in Decentralized Learning",
    "abstract": "Decentralized learning offers a promising approach to crowdsource computational workloads across geographically distributed compute interconnected through peer-to-peer networks, accommodating the exponentially increasing compute demands in the era of large models. However, the absence of proper incentives in locally connected decentralized networks poses significant risks of free riding and malicious behaviors. Data influence, which ensures fair attribution of data source contributions, holds great potential for establishing effective incentive mechanisms. Despite the importance, little effort has been made to analyze data influence in decentralized scenarios, due to non-trivial challenges arising from the distributed nature and the localized connections inherent in decentralized networks. To overcome this fundamental incentive problem, we propose DICE, the first comprehensive framework for analyzing Data Influence CascadEs in decentralized environments. Our framework characterizes how data influence cascades across the communication network and highlights the interplay between original data and network structure in shaping data influence in decentralized learning. We anticipate that DICE can open new avenues for incentive mechanism design and enable impactful applications of influence in decentralized learning, including anomaly detection, collaborator selection and machine unlearning.",
    "keywords": [
        "Decentralized Learning",
        "Data Influence"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We introduce DICE, the first comprehensive framework for measuring data influence cascades in decentralized learning.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=2TIYkqieKw",
    "pdf_link": "https://openreview.net/pdf?id=2TIYkqieKw",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes the DICE framework for measuring the cascading propagation of data influence\nin decentralized learning networks. Decentralized learning enables large-scale model training\nthrough distributed computation, yet the lack of effective incentive mechanisms can lead to unfair\ncontributions and malicious behavior among nodes. The DICE framework introduces data influence\ncascades (DICE-GT and DICE-E), which respectively measure the direct and indirect influence of data\nwithin the network, addressing the limitations of existing data influence measurement methods in\ndecentralized environments. Experiments validate the consistency and accuracy of DICE across\nvarious network topologies and demonstrate its potential in practical applications like anomaly\ndetection and collaborator selection"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The DICE framework is the first to systematically measure the cascading propagation of data\ninfluence in decentralized learning environments, providing an effective method to assess data\ncontributions among nodes and filling a gap in data influence evaluation within decentralized\nnetworks.\n2. The experiments cover different network topologies (such as ring and exponential graphs) and\ndatasets (such as MNIST, CIFAR-10, and CIFAR-100), validating the applicability and consistency\nof the DICE framework across various scenarios.\n3. The DICE framework provides accurate contribution measurement, laying the foundation for\ndesigning fair and effective incentive mechanisms in decentralized learning systems, with the\npotential to foster equitable collaboration within decentralized networks."
            },
            "weaknesses": {
                "value": "1. Figure 1 lacks legend information, making it difficult to understand.\n2. The performance differences of the DICE framework under different parameters (such as learning\nrate, batch size, etc.) have not been thoroughly discussed. It is recommended to add parameter\nsensitivity experiments to demonstrate the impact of different parameter selections on the\nperformance of the DICE framework, thereby enhancing its practicality."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method for quantifying the impact of data points in decentralized machine learning settings. The influence is measured not only at immediate neighbors but the entire network. This method can be useful for machine unlearning  or to develop new incentive mechanisms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "-  The paper is well-organized, with clear definitions, figures, and explanations that make the methods and results easy to follow.\n- The paper provides a solid theoretical framework, supported by rigorous proofs and analyses."
            },
            "weaknesses": {
                "value": "- Need for more details about the practical use of this technique: While the authors use LLMs as one of the examples in the introduction, it might not be the best example to use in this case. It hard to see how this research addresses a practical problem or application that has real-world significance, or how this framework would be relevant for practitioners.\n- Link with other papers that use gradient to cluster clients should be added, particularly interesting and relevant in the collaborator choice part.  \n- Experiments seem non-exhaustive and many details are missing to replicate the experiments. For instance, no indication on what the anomaly is vs normal client. This is particularly important when using gradients. I expect that the framework would perform differently if the anomaly is label flipping vs if it was noisy features. Additionally, evaluation of the impact of batch size would be particularly important for both scalability and compatibility among clients."
            },
            "questions": {
                "value": "1) Please motivate the approach with practical use-cases. \n2) Please discuss link with clustered federated learning, in particular techniques that use gradients to cluster clients. \n3) Please provide all necessary details to replicate the results.\n4) Please evaluate the impact of batch size (smaller and larger values), to show the scalability of the technique and its robustness in showing the compatibility among clients."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes DICE as a framework for measuring data influence cascades in decentralized environments. The framework explains how data influence propagates through the communication network, emphasizing the interaction between the original data and the network structure in shaping data influence within decentralized learning. The experimental results show that the first-order approximation of the \u201cgold standard\u201d for evaluating data influence in decentralized environment can approaching the truth, and this framework can used for detecting mislabeled anomalies."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper summarizes previous work on measuring data influence and highlights the gaps in applying these methods to distributed scenarios.\n2. This paper proposes a sound \u201cgold standard\u201d and its first-order approximation to quantify individual contributions in decentralized learning."
            },
            "weaknesses": {
                "value": "1. The experiments are weak, and Section 5.3 is unfinished.\n2. The notation \u03b7^t in Theorem 1 is previously appears as \u03b7_t in Algorithm 1."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}