{
    "id": "pXPIQsV1St",
    "title": "Dynamical Similarity Analysis uniquely captures how computations develop in RNNs",
    "abstract": "Methods for analyzing representations in neural systems have become a popular tool in both neuroscience and mechanistic interpretability. Having measures to compare how similar activations of neurons are across conditions, architectures, and species, gives us a scalable way of learning how information is transformed within different neural networks. In contrast to this trend, recent investigations have revealed how some metrics can respond to spurious signals and hence give misleading results. To identify the most reliable metric and understand how measures could be improved, it is going to be important to identify specific test cases which can serve as benchmarks. Here we propose that the phenomena of compositional learning in recurrent neural networks (RNNs) would allow us to build a test case for dynamical representation alignment metrics. By implementing this case, we show it allows us to test whether metrics can identify representations which gradually develop throughout learning and probe whether representations identified by metrics are relevant to the actual computations executed within the network. By building both an attractor- and RNN-based test case, we can show that the recently proposed Dynamical Similarity Analysis (DSA) is more noise robust and identifies behaviorally relevant representations significantly more reliably than prior metrics (Procrustes, CKA). We also show how such test cases can be used beyond evaluating metrics to study new architectures directly. Specifically, we tested DSA in modern (Mamba) state space models, where results suggest that, in contrast to RNNs, these models may not exhibit or require changes in their recurrent dynamics due to their expressive hidden state. Overall, we develop test cases that can demonstrate how DSA\u2019s increased ability to detect dynamical motifs gives it a superior ability to identify the ongoing computations in RNNs and elucidate how tasks are learned in networks.",
    "keywords": [
        "dynamic representations",
        "representations",
        "RNN",
        "SSM",
        "neuroscience",
        "interpretability"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "Our new test case shows that Dynamical Similarity Analysis uniquely captures how representations develop alongside computations during compositional learning in stateful artificial neural networks",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=pXPIQsV1St",
    "pdf_link": "https://openreview.net/pdf?id=pXPIQsV1St",
    "comments": [
        {
            "summary": {
                "value": "The work studies a recently proposed \"Dynamical Similarity Analysis\" (DSA) and benchmarks it against prior methods such as Centered Kernel Alignment (CKA) and Procrustes for analyzing the dynamical representations within neural networks, particularly in Recurrent Neural Networks (RNNs). Through systematic test cases that simulate both noise and compositional learning, the study argues that DSA provides a more robust and behaviorally relevant measure of dynamical alignment compared to established metrics like . Furthermore, the paper explores DSA\u2019s application to new architectures such as Mamba models, suggesting that their internal dynamics operate differently from RNNs."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The designed benchmark is novel and much needed to bridge the fields of computational neuroscience and mechanistic interpretability. Even though both fields look at similar problems, their languages have been very distant from each other for some time. I believe this work is a necessary step towards bringing them closer.\n\n- The study of compositional learning in this context, as far as I am aware, is quite novel as well."
            },
            "weaknesses": {
                "value": "- The biggest weakness is the missing methods section. I see there is a supplementary file (which can be at the end of the original submission as an appendix), but this file does not contain the necessary information to reproduce these experiments. As a rule of thumb, by reading the methods section, without looking at the specific code, one should be able to reproduce the work. The public code is to help facilitate the process of reproduction, but is not a substitute for the writing. For example, what were the learning rates? How long were networks trained etc.? \n\n- Though [1] is cited, I would have loved to see the method of finding and identifying the fixed points for categorizing the similarity of computation between RNNs as a baseline. I understand that not all problems will be solved by fixed points, but it is needed to show that DSA CAN recover the computational structure as efficiently in the benchmarks of [1]. For example, you can consider the 3-bit flip flop task and/or the sine generation task, in which we know the solutions and therefore can test whether DSA would be as effective as the fixed-point finders."
            },
            "questions": {
                "value": "I believe I understood most of the work adequately. My score is based on the current submission and the weaknesses described above. If you can address them, it is likely that my score will increase to match the strengths I described above. \n\n[1] Maheswaranathan, Niru, et al. \"Universality and individuality in neural dynamics across large populations of recurrent networks.\" Advances in neural information processing systems 32 (2019)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper has two main contributions. First, the authors suggest a benchmark framework in which to evaluate dynamic similarity metrics. Second, they use this framework to compare three specific metrics: Procrustes, CKA and DSA. They conclude that DSA is the best one. Finally, applying DSA to a more recent architecture MAMBA, they report little change to the dynamical component through training.\nSpecifically, the benchmark has two parts. The first is sampling from noisy ODEs and comparing between different noisy versions. The expectation is that the metric will be robust to noise, but capture genuine differences in the non-noisy dynamics. Further, when combining data from two ODEs, the authors expect a specific form of linearity.\nThe second part is using trained RNNs on a compositional task. Here the authors ask whether pretraining or constrained training induces representational similarities that are expected by task structure. These expectations are what makes RNNs a benchmark: task structure should induce similarity structure."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper addresses an important problem \u2013 comparing dynamical systems. It is valuable both for neuroscience and potentially for ML (e.g. MAMBA and other state space models). With the introduction of new metrics, there is a need for benchmarks to evaluate such metrics.\nUsing RNNs in a systematic manner is a good and original approach for achieving such a benchmark.\nThe authors simulate a large number of networks, while changing training schedule or task composition in a very systematic manner. This provides opportunities for teasing apart subtle differences between various metrics."
            },
            "weaknesses": {
                "value": "The results are somewhat preliminary. In particular, there are no insights or proofs on why there are differences between the metrics. The benchmark itself is rather qualitative.\n\n\n1.\tThe paper lacks more rigorous expectations on how the benchmark results should look like. Why should we expect linearity in the attractor case? The RNN expectations are somewhat crude, as they only dictate whether one group is more dissimilar than another (also see point 2). \n2.\tAll comparisons in the RNN are to the master network, and yet conclusions are drawn regarding their similarity to each other.\n3.\tClarity of writing can be improved. There were several places where it was quite hard to understand what exactly was done."
            },
            "questions": {
                "value": "4.\tIf this benchmark is widely used, and metrics are optimized to be good at it. Will they be good metrics? In other words, how can we be sure that passing this benchmark generalizes to the broader objectives of metrics? \n5.\tThe objectives of metrics are not fully elaborated. There is some mention of ratios, but this is not fully developed. This is not a trivial question, and not necessarily easy to answer. But it should be properly discussed in a paper suggesting benchmarks.\n6.\tThe writing in the results section 4.1 is a bit jumpy. Definition of ratio. Then noise.\n7.\tDoes the \u2018+ and \u20181/2\u2019\u2019 notation in Fig 2a represent numerical addition and scaling, or a union of data sets from A/B/noise with different ratios? The main text (and appendices) failed to clarify this crucial point making subsequent results difficult to interpret with confidence.\n8.\tResults in Fig 3G are interesting. Why Procrustes and CKA can\u2019t discriminate the untrained network? Is it possible that they do discriminate it, but because we are only measuring dissimilarity to the master, we can\u2019t see it?\n9.\tWhy do you compare groups and not individual networks?\n10.\t\u201cThe relative relevance of Attractor B\u201d \u2013 number of trials? Amplitude?\n11.\tThe Untrained network is a good control to have, as it illustrates how statistics of input can dominate dynamics and similarity measures.\n12.\t Violin plots could be more informative than bars in the various plots.\n13.\tFigure 3: the expectations cartoon is confusing. The grey and purple have a specific order, despite the text saying they are expected to be the same. Furthermore, the order in the actual plots is opposite.\n14.\tThe DSA paper uses dimensionality reduction and classification to quantify distances between networks. It could be useful to visualize all networks of figure 3 using multi dimensional scaling as in that paper. For instance, that might help understand whether pretrained networks are really similar to untrained networks (in CKA and Procrustes), or are simply equally distant from the master.\n15.\tFigure 4 \u2013 why are the values at 0 accuracy difference so distant from zero? Shouldn\u2019t this include networks that are almost identical?\n16.\tLines 429-431: are purple and green in the figure?\n17.\tLine 431: the pattern is inverted for zero percent. What about the decline from 70% training to 100% training?\n18.\tLine 467 \u201cThis means that all training groups produce roughly the same dynamics\u201d \u2013 can this conclusion be reached when only comparing models to master, and not one to another?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper demonstrates that Dynamical Similarity Analysis (DSA), a recently introduced metric for comparing dynamical systems, outperforms related techniques such as Procrustes and CKA. DSA operates by projecting system trajectories into a high-dimensional space, where the vector fields governing the dynamics become approximately linear. It then aligns the two vector fields through an orthogonal transformation. The authors show that DSA consistently outperforms CKA and Procrustes across all tested tasks and suggest that DSA may be uniquely capable of capturing the computational processes underlying the tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper is timely and addresses an important challenge in neuroscience and AI: comparing dynamic trajectories across neural systems. The paper makes a nice contribution by empirically evaluating several widely used techniques for comparing dynamical trajectories and identifying the most effective one as DSA. The comparisons appear to me to be done fairly and objectively."
            },
            "weaknesses": {
                "value": "The paper is at times quite hard to read, in particular when the authors are describing the tasks. The paper would benefit from a streamlining of the explanations of the experiments. Also (as I'll argue below), the title is a bit misleading, and should be changed to reflect the actual contributions of the paper."
            },
            "questions": {
                "value": "- The title suggests that you have shown/proved that DSA is unique among all similarity metrics in its ability to capture how computations evolve. You have not shown this. In reality, you have shown that DSA is better at two other metrics on a range of tasks. This is still an important contribution, but the title should be toned down to reflect the actual contributions of the paper. \n- Is the appendix missing? There is a reference to it (L152), but it doesn't appear to go anywhere. \n- L118: \"... the momentum of traces in instead of...\" I'm not sure what this is trying to say."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors investigate how three metrics, Dynamical Similarity Analysis (DSA), Procrustes distance, and linear Centered Kernel Alignment (CKA), quantify diffferences in RNN dynamics. They attempt to provide benchmarks, which illustrate favorable properties of DSA over the other two."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "I think the intent and framing of the paper in the introduction is well written. The topic is interesting. Unfortunately, I have serious concerns about the execution of the project as outlined below."
            },
            "weaknesses": {
                "value": "This paper does not present a new technique or method, but is rather aiming to deepen our understanding of existing metrics. I really like this idea, but in my mind it means that the paper needs to be impeccably written and ideally involves some concrete mathematical results or guarantees about the methods being compared. I think the paper does not succeed on these terms and therefore does not meet the bar for publication at ICLR at its current level of polish and detail.\n\nLet me summarize major weaknesses briefly:\n\n* There are many places where the authors claim that there is an \"expected\" result that aligns with what DSA shows, but why these things are \"expected\" is not clearly described. I suspect that what one \"expects\" in many of these cases is debatable.\n* The test cases shown are bespoke. It is not clear whether any of this generalizes to a broader variety of settings. There is also a relatively small number of tasks considered. I think they consider roughly 2-3 tasks, most being variants of Driscoll et al's study. In comparison, [Klabunde et al.'s recent benchmark](https://arxiv.org/abs/2408.00531) considers six different tests across six different datasets.\n* There is a relatively small number of metrics considered. The authors consider three (DSA, CKA, and Procrustes). In comparison,  Klabunde et al's study linked above contains 23 similarity measures.\n* Related to the point above, Procrustes and CKA were never meant to be applied to dynamical time series so the comparison seems a little unfair and expected that DSA comes out \"ahead\" in certain respects. At the same time, the authors do not include Diffeomorphic vector field alignment as a comparison to DSA (even though they do cite it). Additionally, I would point the authors to stochastic shape distances as a viable metric for comparing dynamical flow fields: [Lipshutz et al. (2024)](https://openreview.net/forum?id=Fykvxdv2I8)\n    * For these reasons, the claim that \"DSA **uniquely** captures\" anything seems unjustified! I would only say that a method *uniquely* captures something if I had a mathematical proof that no other approach could work.\n\n\nBelow I unpack some of these weaknesses further with a bit more specificity:\n\n* Regarding Fig 2C, there is no clear motivation why we would want a metric to respond \"ratio-like\" when we combine attractors. Furthermore, DSA only has marginally better linear R^2 (0.99 vs 0.97 or 0.96), yet this is somehow treated as a \"win\" for DSA over these other measures. \n* Regarding Figure 3, the authors use the term \"normative predictions\" multiple times in relation to Driscoll et al.'s modeling work. I strongly encourage the authors to rephrase this. A normative model has a very specific meaning in theoretical neuroscience -- it involves predicting an attribute of a network on the basis of some functional or evolutionary principle (efficient coding is a classic example, see for e.g. [Mlynarski & Hermundstad, 2021](https://www.nature.com/articles/s41593-021-00846-0)). Driscoll et al. never use the term \"normative model\" in their paper and it is confusing to see the term applied here.\n* Moreover, I am hestitant to treat the results of Driscoll et al. &mdash; which, while interesting, is only one empirical study of a very specific family of RNN tasks &mdash; as a foundational way to benchmark metrics on neural representations. The authors state at the conclusion of this section that \"DSA is the only metric with correctly identifies the compositional representation that we expect.\" But it is not well explained what I should \"expect\" to see, and I suspect that what one \"expects\" to see could be debatable. In any case, the panel corresponding to DSA in Fig 3G does not seem to do a good job distinguishing the final 3-4 categories (only the yellow box plot seems substantially higher than the rest).\n* The horizontal axis in Figure 2 is confusing. Epochs often refer to training epochs. The horizontal axis should be labeled \"noise\" or something similar.\n* How noise impacts the simulation in Figure 2 is unclear. The authors don't show, for example, trajectories of neural firing rates. Also why plot only three levels of noise rather than a more fine scale grid?"
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}