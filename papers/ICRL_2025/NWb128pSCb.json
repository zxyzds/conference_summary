{
    "id": "NWb128pSCb",
    "title": "Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective",
    "abstract": "Accurate interpretation and visualization of human instructions are crucial for \\textbf{T}ext-to-\\textbf{I}mage (\\textbf{T2I}) synthesis. Existing T2I models often struggle to capture semantic variations caused by different word orders. Current evaluations rely on indirect metrics, like text-image matching, where the limitations with uncommon or complex linguistic patterns may be obscured by focusing on frequent word combinations. To address these deficiencies, we introduce a novel metric called \\underline{PermuteEffect} and a benchmark named \\underline{PermuteBench}, designed to evaluate the causality between textual semantic variations and visual semantic variations in T2I synthesis. Our experiments revealed that the CogView-3-Plus and Ideogram 2 models performed the best, achieving a score of 0.2/1, where semantic variations concerning relations between objects were more weakly understood than those about attributes, scoring only 0.07/1. We found that cross-modal alignment in UNet or Transformers plays a critical role in understanding semantic variations, a factor previously overlooked by a focus on textual encoders. Our work establishes an effective evaluation framework that advances \nthe T2I synthesis community's exploration of human instruction understanding.",
    "keywords": [
        "text-to-image synthesis",
        "semantics",
        "evaluation",
        "metric"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=NWb128pSCb",
    "pdf_link": "https://openreview.net/pdf?id=NWb128pSCb",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a novel metric, SemVarEffect, and a corresponding benchmark, SemVarBench, designed to evaluate the causality between semantic variations in input text and output images for text-to-image (T2I) synthesis models. The authors identify a gap in current evaluation methods, which often rely on indirect metrics like text-image similarity, failing to adequately assess how models handle semantic changes due to word order. SemVarEffect measures the average causal effect of input semantic variations on output images, while SemVarBench provides a dataset with controlled linguistic permutations to test model performance. The experiments reveal that even state-of-the-art models struggle with semantic variations, indicating room for improvement in understanding and generating accurate visual representations from textual instructions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper directly addresses a critical issue in the evaluation of T2I models, which is the accurate interpretation of human instructions and evaluate the finegrained instruction following ability of the models."
            },
            "weaknesses": {
                "value": "1. The paper suffers from significant presentation issues, making the proposed method difficult to comprehend. The overall structure of the methodology section requires refinement:\n   - Figure 2, which is crucial to understanding the proposed method, is challenging to interpret.\n   - In Section 2.2, the term \"localized change\" needs clarification.\n   - The definition of \"integrated visual semantic variations\" is also unclear.\n   - Line 278 uses the phrase \"evoke vivid mental images,\" but its meaning is ambiguous in the given context.\n   - Table 2 includes numerous annotations, but offers no explanation of what the different scores represent.\n\n2. In my opinion, the method appears overly complex. The core idea is straightforward: we anticipate that semantic alignment of generated images should be strong if the inputs are semantically identical, and less aligned if the inputs differ semantically. Thus, a simple metric like S(I_a, I_pu) - S(I_a, I_pi) could be employed. What advantages does the proposed causal relationship provide over this simple baseline? I would appreciate deeper insight and explanations on this matter.\n\n3. Regarding model-based evaluation, there is no discussion on whether the MLLM used to compute the alignment score is effective for this task, nor is there any human evaluation. This omission raises concerns about the reliability and trustworthiness of the scores presented."
            },
            "questions": {
                "value": "See weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel metric called SemVarEffect and a benchmark called SemVarBench to evaluate text-to-image (T2I) models' ability to capture semantic variations caused by word order changes in input prompts. The key contributions are:\n- A causal inference framework to measure how semantic variations in text inputs affect visual semantic variations in generated images.\n- A carefully curated dataset of text triplets (anchor, permutation with changed meaning, permutation with unchanged meaning) to test T2I models.\n- Comprehensive evaluation of  state-of-the-art T2I models. This also includes fine-grained Analysis of the role of text encoders and cross-modal alignment in handling semantic variations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Strengths:\nNovel and important problem formulation: The paper addresses a critical gap in evaluating T2I models' language understanding capabilities.\n\nThe causal inference based framework provides a principled way to isolate the effect of semantic variations.\n\nSemVarBench is carefully curated to avoid easily predictable variations and focus on challenging semantic permutations, with comprehensive coverage of different type of variations. LLM generated results were checked."
            },
            "weaknesses": {
                "value": "The training dataset is generated using the same base SDXL model and filtered by MMLM, which could introduce self-distillation artifacts and bias, particularly affecting performance on categories with less data. \n\nThe paper suggests that fine-tuning improves alignment at the word level rather than semantic variants, but doesn't provide sufficient discussion or analysis to support this claim"
            },
            "questions": {
                "value": "How does the choice of filtering training data using MMLM potentially affect the evaluation results, particularly for categories with limited samples?\n\nWith the 4 metics you had mentioned, which one is the main metric that would make sense quickly for readers? It is a little hard to follow with different metrics presented at different parts, which I can imagine is a little confusing especially for readers who are looking for a single metric for benchmarking."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper primarily focuses on the impact of word order variations in text prompt on text-to-image synthesis. It proposes a metric to evaluate a model's semantic sensitivity to changes in word order. The main idea is to calculate the difference in visual semantics of generated images between semantic-variant and semantic-invariant text rearrangements. Additionally, a dataset of approximately 11k samples has been collected as an evaluation benchmark, and popular models have been assessed and analyzed based on this benchmark."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors find that existing text-to-image synthesis models struggle to capture semantic shifts caused by word order changes and proposed an evaluation metric and benchmark. \n- The SemVarEffect metric is based on the difference between semantic-variant and semantic-invariant text rearrangements, similar to the differential denoising principle in circuits, which I find quite novel. \n- The paper is well-organized and easy to understand."
            },
            "weaknesses": {
                "value": "- The definition of visual semantics: The primary contribution of this work is the metric to measure the impact of word order in text prompts on text-to-image synthesis. The basic principle is to alter the text and observe the resulting visual semantic changes in the generated image. However, within this metric, the \u201cvisual semantics\u201d of the generated image is defined through text-image similarity, which may not be entirely accurate. Visual semantics in an image typically refers to the content within the image itself, such as object categories, attributes, or scene information. In contrast, text-image similarity only measures the alignment of high-level features between the two modalities; it does not represent the actual visual semantics. Ideally, visual semantics should be defined independently of text. \n- The mechanism for ensuring that the generated data is semantic-variant or semantic-invariant during benchmark creation seems to be insufficiently specified, and why are some data extraction templates and rules created using LLMs while others are crafted manually? What challenges are involved in this?"
            },
            "questions": {
                "value": "- In section 4.1, If text-image similarity is obtained using MLLMs, could biases inherent in the MLLM itself (insensitivity to changes in word order) affect the evaluation results?\n- Text-to-image synthesis models have difficulty accurately reflecting semantic changes caused by variations in word order, possibly due to certain commonsense biases inherent in the training data. This could be seen as a form of \u201cbuilt-in bias.\u201d For example, as illustrated in Figure 1 of the paper, \u201cA cat chasing a mouse\u201d is a reasonable depiction, whereas the reordered phrase \u201cA mouse chasing a cat\u201d is less plausible, leading to poorer generation results. It might be helpful to add related experiments to validate this hypothesis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on the performance of Text to Image (T2I) models in handling semantic variations. It identifies that existing T2I models tend to overlook the relationships between words in the input text when faced with similar semantics or semantic substitutions, making it challenging to generate accurate images. At the same time, the paper introduces the SemVarEffect metric and SemVarBench to evaluate this issue."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- This paper offers an interesting and novel perspective on the current issues facing T2I models, and provides an effective metric and dataset to measure this issue.\n- The paper offers detailed and fair experiments to compare various T2I models.\n- The analysis section is thorough and carefully examines the causes of the semantic understanding issues in T2I models, providing a reliable path for future research."
            },
            "weaknesses": {
                "value": "- I noticed that some of the samples provided in the paper, such as \"A mouse chasing a cat,\" are somewhat unrealistic captions. However, this category is not emphasized in SemVarBench, as seen with sentences like \"A cat chasing a dog\" and \"A dog chasing a cat.\" These sentences also exhibit semantic variations, but I believe that current T2I models can handle these changes. Therefore, I think further discussion on this issue is needed.\n- I believe it is necessary to further discuss the relationship between this phenomenon and the training data in the discussion. In the problematic captions where semantic variations occur, a significant portion of the original captions may be included in the pre-training data. Therefore, similar to the discussion in paper [1], I suggest designing experiments to investigate whether T2I models can perform reverse inference of semantic variations.\n\n[1] Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, A. C., Korbak, T., \\& Evans, O. (2024). *The Reversal Curse: LLMs trained on \"A is B\" fail to learn \"B is A\"*. arXiv:2309.12288."
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}