{
    "id": "zOMa82W1HV",
    "title": "SQuBa: Speech Mamba Language Model with Querying-Attention for Efficient Summarization",
    "abstract": "Abstractive Speech Summarization (SSum) becomes increasingly difficult as the input speech length grows. To address this, we present SQuBa (Speech Querying Mamba Architecture), an end-to-end model designed explicitly for efficient speech summarization. SQuBa leverages a querying-attention Mamba projector to condense extended acoustic features into compact semantic tokens, which are subsequently summarized by the Mamba Large Language Model (LLM). The architecture\u2019s computational complexity scales linearly with input length, enabling efficient handling of longer inputs. A two-stage training framework, complemented by bootstrapped Direct Preference Optimization (DPO) fine-tuning, empowers SQuBa to generate concise and coherent summaries. Experimental results demonstrate that SQuBa delivers competitive performance while significantly improving inference speed, making it ideal for real-world applications such as podcast and meeting transcriptions.",
    "keywords": [
        "Summarisation",
        "Speech",
        "Mamba"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zOMa82W1HV",
    "pdf_link": "https://openreview.net/pdf?id=zOMa82W1HV",
    "comments": [
        {
            "summary": {
                "value": "The paper explores the use of Mamba-based multimodal LLMs to process long speech segments. The authors also apply DPO to enhance alignment during the instruction fine-tuning stage. Their experiments focus primarily on the speech summarization task, showing that the model can successfully process long speech."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is clearly structured and represents a valuable exploration of using LLMs for long speech processing."
            },
            "weaknesses": {
                "value": "- Although the paper explores long speech processing using LLMs, the motivation is not sufficiently compelling. Notably, LLMs excel at summarization, and long speech summarization could be effectively handled by combining an ASR model with a strong LLM. While ASR may introduce some errors, LLMs are generally robust enough to manage this task. Therefore, long speech summarization may not be the most suitable task for evaluating LLMs in handling extended long speech inputs.\n- The contribution is somewhat limited, as Mamba-based multimodal LLMs and DPO have both been explored in speech instruction tuning. This work primarily combines these two methods and tests them on the speech summarization task.\n- The experiments are insufficiently comprehensive; relying only on speech summarization does not robustly support the model\u2019s capability with long speech. Furthermore, the LLM used here is not particularly strong.\n\nOverall, this paper lacks novelty and persuasiveness and would benefit from further development."
            },
            "questions": {
                "value": "I'm curious about the performance of advanced models (e.g., Whisper v3 and Llama 3) in building a cascade pipeline. Since this work aims to explore LLMs for speech processing, and LLMs are effective in handling long text and summarization tasks, how would these models compare?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a sub-linear complexity speech summarization model by combining Q-former and pretrained Mamba. Segmented audios are processed by Whisper and compressed by Q-former with Mamba-processed query vectors, which are then fed to Mamba LLM to generate the summarization. A 3-stage training with different tasks is carried out, i.e. short-form ASR, long-form ASR, and summarization, accompanied by DPO. Empirical studies show better results and speed compared to cascaded models and a HuBERT+LLAMA E2E model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The model enables efficient speech summarization by combining pretrained Mamba and Q-former.\n* Empirical results are strong."
            },
            "weaknesses": {
                "value": "* Ablation studies are not sufficient to demonstrate the advantages of the proposed methods and to identify the impact of each component. Some design choices are yet to be well-motivated.\n* The method is more like replacing transformers in existing methods (esp. arxiv:2407.02005) with Mamba, which leads to doubt on the technical novelty.\n* Only one synthetic dataset is used.\n\nSee questions below for details."
            },
            "questions": {
                "value": "Q-former-like mechanism should be able to compress input of any length into fixed length. Hence I'm a bit confused about the decision to compress every 0.33s of audio into 2 query vectors. This differs from previous works including arxiv:2309.13963 and arxiv:2407.02005, which considered 30~100 vectors for every 30s of audio. In this way, the contextual information further than 0.33s will not be captured by the Q-former. I guess that the reason is to avoid the high-cost quadratic cross-attention but it will be better for the author to discuss that explicitly. Also, compressing a short sequence (only dozens of vectors, as each Whisper frame takes 25ms) into merely 2 vectors is rather simple and I doubt if the complicated Q-former will really outperform a much simpler one, e.g. pooling or convolution, which may also process information within such a short context well. More ablation studies will be necessary to justify this decision, by comparing with pooling or CNN, and comparing with different context lengths.\n\nThere are many other approaches to compress speech signals into \"token-like\" embeddings to be processed by LMs, e.g. HuBERT units, speech tokens, and neural audio codec, while Q-former is somehow similar to a kind of VQ, but with continuous features. Can you elaborate on the reasons why you chose Q-former? Do you think there is any specific advantage?\n\nI am particularly concerned with the unidirectional Mamba used in Q-Mamba, and I fail to find the motivation to apply Mamba to the sequence of query vectors. Trainable query vectors should be already capable of introducing positional information. Ablation studies (e.g. by removing this Mamba layer) should be necessary to justify this choice.\n\nI also have some questions regarding the use of DPO. What is the experiment without DPO in Table 4? Using supervised fine-tuning only?\n\nIf the issue w/o DPO is that the summary will be too detailed, has the author considered any other more straightforward solution, e.g. length penalty during generation, downsampling the input sequence, or upweighting EOS during training?\n\nIt is commonly believed that instruction fine-tuning leads to better alignment, but at the cost of flexibility and adaptability to specific downstream tasks in fine-tuning, while the authors use a instruction fine-tuned version of Mamba-2.8B as the base LLM. Is there any specific reason to use Mamba-2.8B-Zephyr instead of the original Mamba-2.8B model?\n\nWhat is the LLM used in the Cascaded model? The original Mamba-2.8B or the instruction fine-tuned one? Is it further fine-tuned to summarization?\n\nCan you elaborate more on the speedup of the model compared to the cascaded one? With both of them using Whisper and Mamba (though the inputs to Mamba are different), I'm curious about the source of the extra overhead in the cascaded pipeline. Also, it can be helpful to report the average input sequence length to the final LLM model as a reference to the expected computational costs.\n\nUsing only synthesized datasets is a weak point of the empirical evaluation, particularly when the labels are also synthetic. It can be necessary to also report the results on real datasets, e.g. SLUE-SUMM, and include more examples and human evaluations.\n\nIt can be interesting to report the ASR performance of the model after either of the two Alignment stages.\n\nIt will be better to also include the original transcript in Appendix C.\n\nIn Figure 3, is Whisper frozen in the Fine-tuning Stage?\n\nMinor issues:\nL210: Figure 4.1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents SQuBa, an end-to-end speech summarization model that combines a Mamba language model with a novel querying-attention mechanism for efficient processing of long speech inputs. The key contributions are:\n\n- A query-attention Mamba projector that compresses acoustic features into compact semantic tokens.\n- Extension of Mamba-based LLM for speech summarization with a two-stage training process including bootstrapped DPO.\n- Empirical demonstration of competitive performance with significantly faster inference speeds compared to transformer-based approaches.\n\nThe model achieves this through:\n\n- Using Whisper encoder for speech features.\n- Novel Q-Mamba projector for efficient feature compression.\n- Pre-trained Mamba LLM (2.8B parameters) for generation.\n- Two-stage training: speech alignment followed by summarization fine-tuning.\n- Bootstrapped DPO for improved summary quality."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Novel Architecture:\n\nInnovative combination of Mamba with querying-attention for speech processing.\n\nWell-motivated design choices for handling long-form speech.\n\nClear architectural improvements over existing approaches.\n\n- Strong Empirical Results:\n\nSignificant speed improvements (17x faster than cascaded baseline).\n\nCompetitive performance on standard metrics.\n\nComprehensive ablation studies validating design choices.\n\n- Technical Soundness:\n\nThorough theoretical foundation and clear mathematical formulation.\n\nWell-documented training process and implementation details.\n\nCareful experimental design with appropriate baselines."
            },
            "weaknesses": {
                "value": "Limited Dataset:\n- Uses synthetic speech data for fine-tuning\n- Could benefit from evaluation on more diverse real-world speech datasets\n- Lack of cross-lingual evaluation\n\nArchitectural Constraints:\n- Fixed 30-second chunks due to Whisper encoder limitations\n- Query length choices could use more theoretical justification\n- Potential information loss in compression not fully analyzed\n\nEvaluation Metrics:\n- Limited human evaluation or qualitative analysis\n- No discussion of failure cases or limitations"
            },
            "questions": {
                "value": "/"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper describes an approach to summarizing 6-minute-long audio recordings by combining the Whisper speech encoder with the Mamba LLM through a cross-attention based Mamba querying projector. The authors show that DPO improves ROUGE and METEOR metrics, and that the proposed model has a better ROUGE and METEOR score, and latency over the cascade model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper attempts to address an important challenge, i.e., summarization of longform audio through a cross-attention based temporal downsampling module. \n2. It applies the recently introduced DPO technique to speech summarization, and demonstrates improved ROUGE and METEOR scores from this."
            },
            "weaknesses": {
                "value": "1. I have serious concerns about the novelty of the proposed approach. From a modeling standpoint, the work is very similar to Shang et al. (2024). From a training method standpoint, the 2-stage fine-tuning approach involving speech recognition and speech summarization is well established in the field since Sharma et al. (2021), leaving only two differences: (a) having an ASR training stage over both short and long audio as opposed to just short audio, and (b) using DPO post hoc, another well established technique to improve ROUGE and METEOR numbers. All in all, it appears that there is little technical novelty in the paper.  \n\n2. Validating the proposed approach on a single relatively shortform audio dataset (upto 6 minutes) comprising synthetic audio is not very convincing. Furthermore, the work is done on a custom dataset whose LLM-generated summary labels have not been validated for correctness, either through automatic or human evaluations.  Since LLMs are known to hallucinate, it is hard to make a meaningful case using any numbers on this dataset. The authors should ideally consider evaluating on any real other dataset(s) with real audio. \n\nTo add more context, here is a paper [1] that used synthetic data for speech summarization but still reported a myriad of automatic and human evaluation metrics to validate that the data used was reasonable. Something similar to this might be more convincing than what is in the paper currently. \n\n3. The metrics used for speech summarization in this paper do not go far enough. It is well known that ROUGE and METEOR based evaluations for summarization are not all encompassing, and that the metrics have significant flaws. This again makes it hard to validate that the observed improvements correlate with summaries of higher quality. The authors could supplement these measures using human evaluations of coherence, consistency, factuality and relevance. \n\n4. Table 4 could be expanded to show the impact of the long audio transcription based alignment if any. \n\n5. The manner in which DPO is performed is not very convincing. The authors use the model generated responses as the non-preferred responses and the ground-truth summaries as the preferred responses. Do the authors validate that the model generated responses are in fact undesirable, and record metrics that demonstrate the same ?\n\n\n[1] J. -W. Jung, R. Sharma, W. Chen, B. Raj and S. Watanabe, \"AugSumm: Towards Generalizable Speech Summarization Using Synthetic Labels from Large Language Models,\" ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Seoul, Korea, Republic of, 2024, pp. 12071-12075, doi: 10.1109/ICASSP48485.2024.10447328."
            },
            "questions": {
                "value": "1. In Equation 1, what is h'(t) ? \n2. The \"ideal\" query length for speech transcription is likely not representative of representations necessary for speech summarization. Can the authors clarify why these ablations were done for the transcription task on Librispeech ?\n3. How does the speed of Scuba compare to that of the model by Shang et al ? \n4. The Whisper speech encoder is frozen, and it is not clear why this modeling choice was made."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an end-to-end abstractive summarization method that processes speech inputs directly. It utilizes a querying-attention Mamba projector to condense extended acoustic features into compact semantic tokens, which are subsequently fed into the Mamba Large Language Model (LLM). They further employ Direct Preference Optimization (DPO) fine-tuning to produce coherent summaries. Experiments on a TTS-synthesized speech summarization dataset demonstrate that this approach outperforms both a cascaded baseline and an end-to-end baseline."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Originality:\n1. The Mamba-based approach has not yet been utilized for speech summarization.\n\nQuality:\n1. Faster inference with better summarization performance against a cascaded and E2E baseline.\n\nClarity:\n1. The paper is mostly easy to follow.\n\nSignificance: Results could be significant to speech summarization community."
            },
            "weaknesses": {
                "value": "Clarity: \n1. The biggest weakness to me is the lack of clarity on the baseline models. They use Whisper large v2 as the ASR model in the cascaded system, but it has a limit of 30 seconds. How do they use it to get ASR output? Do they feed every 30-second window of audio as input? Further, do they finetune ASR or LLM or are they used in a zero-shot manner? What are the results in both these scenarios? If LLM is finetuned, do they also use DPO finetuning? The paper should provide more details about the E2E speech summarization baseline in the main text to make the paper self-contained. \n2. It\u2019s also unclear how the approach handles long speech sequences, which seems to be a central aspect of this work's novelty. The paper mentions chunking audio into 30-second segments, yet doesn\u2019t address how contextual continuity is managed between chunks. Prior studies on streaming ASR (e.g., https://arxiv.org/abs/2107.09428) indicate that chunk boundary in the middle of token can result in generation inaccuracies. Clarifying whether any overlap is applied between chunks and providing additional discussion on this topic would improve the paper's depth and accessibility.\n\nSoundness: \n1. The evaluation is limited to one dataset, a TTS-generated synthetic speech summarization dataset. Including publicly available human speech datasets, such as SLUE_TED (https://huggingface.co/datasets/asapp/slue-phase-2) or AMI (https://groups.inf.ed.ac.uk/ami/corpus/), would provide a more robust assessment and ensure that the approach is tested on natural human speech data.\n2. Additional details on the synthetic dataset would be valuable, including whether it consists of single-speaker audio, and its quality (e.g., WER for the TTS output as evaluated by a pre-trained ASR model or via human relevance judgment).\n3. Further analysis is needed to pinpoint where the model outperforms a cascaded baseline. Does this improvement stem primarily from avoiding error cascading (that can potentially be addressed by improving the ASR system), or does the model also capture non-phonemic audio signals that enhance summarization quality?\n\n Significance and Originality: I feel that Mamba-based approaches have been shown to be useful for various speech processing tasks (https://www.isca-archive.org/interspeech_2024/miyazaki24_interspeech.html) and have been shown to be particularly efficient for long-form speech processing. The paper has limited novelty since it additionally verifies an expected conclusion that Mamba-based architecture is also useful for another speech processing task, namely speech summarization. Further, the lack of clarity in setup of baseline models make me question the improvements claimed in the work."
            },
            "questions": {
                "value": "Please refer to weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Refer to weaknesses"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}