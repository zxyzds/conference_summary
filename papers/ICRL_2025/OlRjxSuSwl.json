{
    "id": "OlRjxSuSwl",
    "title": "Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models",
    "abstract": "Large language models (LLMs) have been widely adopted for training embedding and ranking models, with recent advancements significantly improving the performance of Information Retrieval systems. However, while instruction-following is a core capability of LLMs, their ability to handle detailed user instructions has not been thoroughly investigated in search models. This study evaluates the instruction-following capabilities of various retrieval models, including LLM-based dense retrieval and reranking models. We develop a specialized benchmark InFoSearch spanning six dimensions: Audience, Keyword, Format, Language, Length, and Source, and introduce novel metrics to assess the models' responsiveness to instructions. Our findings show that even fine-tuned retrieval models struggle with instruction-following, highlighting a key limitation in current systems and providing valuable insights for improving their instruction-aware capabilities in future research.",
    "keywords": [
        "LLM",
        "Instruction-Following",
        "Retrieval Model",
        "Benchmark"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We propose InFoSearch, a novel benchmark, along with an evaluation protocol to assess the depth of instruction-following capabilities in information retrieval (IR) models.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=OlRjxSuSwl",
    "pdf_link": "https://openreview.net/pdf?id=OlRjxSuSwl",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a new dataset and evaluation metrics to evaluate instruction-based information retrieval. Their dataset, InfoSearch proposes 6 different subsets of evaluation: Audience, Keyword, Format, Language, Length, and Source. \n\nThey build the dataset by collecting query and document pairs from existing IR datasets as well as Google. They then have GPT-4 extract keywords from the documents, generate an instruction via some specific style, and then re-write the document as needed. They also prompt GPT-4 to generate instruction reversals, which are the opposite. They then manually review them. \n\nThey propose two new metrics to evaluate instruction following (WISe and SICR) and show results on a wide range of models. They show that larger models do better, rerankers do better, list-wise models do better, and instruction-trained models do better. They show that in some dimensions, such as audience, are still difficult for retrievers."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I think this paper has a lot of strengths:\n- A needed dataset for instruction-based IR, covering a wide range of topics, including some models still struggle on\n- A wide set of models they evaluated, including larger models like GPT-4\n- Interesting analysis from the metrics and various comparisons"
            },
            "weaknesses": {
                "value": "Although I lean positively, there are a few weakness/area to improve\n\n1. I am still confused slightly on the dataset creation. For example, what decided how many instructions there were per query? In Table 1 I see somewhere between 2-3 per query.  The instructions are also fairly short (20 words, which is almost standard query-length), but this is not a dealbreaker.  See the questions for these.\n2. The two new introduced metrics are somewhat difficult to understand and I am not sure I fully agree with them. See the questions, from where I worked through some examples and see some ways of assigning scores that seem odd to me.\n3. Although I wouldn't have expected the authors to have done this since the deadline was so soon after the model's releases, there are some models that claim to do better on this, including ICL-based models (BGE-ICL, RARe) and zero-shot models (Promptriever). It would be interesting to see their performance.\n\nOverall, I do like the paper and would be open to increasing my score if some of these weaknesses/questions are addressed (mostly metrics based)."
            },
            "questions": {
                "value": "### Re: Weakness 1, dataset construction\n\na. How were the number of instructions decided/created?\n\nb. How was the number of document in the corpus decided? \n\nc. It seems like all documents are merged together from all subsets to create the corpus, how are we sure there are not false negatives in the other sets? A qualitative analysis or GPT-based analysis would be very valuable here.\n\nd. How were these documents gathered, just any that were from the original pairs (but then where did the remainder come from?)\n\n### Re: Weakness 2, metrics\n\nA core motivation from the paper of WISe is:\n> Intuitively, if the ranking of Pi improves in the instructed mode (Rins < Rori) and worsens in the reversely instructed mode\n(Rori < Rrev), this demonstrates the model\u2019s effective compliance with the provided instructions\n\na: Why is this true? I don't necessarily understand why the query is more relevant with the instruction, weren't these gathered from already relevant query and doc pairs? So I see no specific reason why the rank should increase with the instruction as compared to no-instruction.\n\nb: Arbitrary values: why does WISe not go to 0? Why use 20 as the number for the conditional?\n\nc: Doesn't WISe/SICR have the same problem as p-MRR in terms of rank of 1 for the original and instructed query?\n\nd: Working through some examples for WISe, that I found unusual:\n- If the rank is 1 for both, the score is 0 correct? (From Eq 1)\n- From r_{ori} of 20 to r_{ins} of 1, we have a score of 1.0. If you change r_{ins} to 2, now the score is 0.07? Perhaps I did the math wrong, but this seems like a pretty large gap\n- From r_{ori} of 20 to r_{ins} of 2 we have 0.07 but from r_{ori} of 21 to r_{ins} of 1, we have 0.01 (an even lower score, even though arguably the model improved more)\n- Overall, the N=20 distinction seems to introduce some weird disfluencies in the possible range of scores for this metric. Even the overall range seems somewhat odd, you can get scores from -1 to 1, but oddly distributed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper, \u201cBeyond Content Relevance: Evaluating Instruction Following in Retrieval Models,\u201d introduces **InfoSearch**, a benchmark that assesses how well retrieval models follow user instructions beyond simple content relevance. Traditional retrieval models have limitations in understanding and executing detailed user instructions, often focusing on keyword matching rather than instruction adherence. InfoSearch spans six key dimensions\u2014Audience, Keyword, Format, Language, Length, and Source\u2014representing user-specific document attributes. The study evaluates 16 retrieval models using this benchmark and introduces two novel metrics, **Weighted Instruction Sensitivity evaluation (WISe)** and **Strict Instruction Compliance Ratio (SICR)**, designed to capture the depth of instruction-following capabilities. The results reveal that current retrieval models, even those fine-tuned, often struggle with adhering to specific instructions, providing insights for advancing instruction-aware retrieval in future research."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The writing is easy to follow and the logic is clear.\n- InfoSearch covers six essential dimensions, moving beyond traditional content relevance. This multidimensional approach provides a holistic evaluation of models, accounting for varied user-specific needs that reflect real-world scenarios.\n- The introduction of WISe and SICR metrics adds significant value by capturing nuanced differences in instruction-following. These metrics are specifically designed to evaluate instruction adherence, overcoming limitations in traditional metrics like nDCG or pMRR."
            },
            "weaknesses": {
                "value": "- The benchmark\u2019s focus on instruction-following may overlook other critical aspects of retrieval quality, such as semantic understanding and contextual relevance. Overemphasizing instruction compliance may risk sidelining these equally important factors.\n- The InfoSearch benchmark relies heavily on synthetic data generation, primarily using LLMs like GPT-4 to create instruction-query pairs and rephrased documents. This reliance may introduce artificial biases in the data that do not accurately reflect real-world queries or instruction styles, potentially limiting the benchmark\u2019s validity for evaluating real-world retrieval tasks.\n- While the benchmark tests models on reversely instructed queries (negative instructions), the analysis does not deeply explore how models interpret and handle negation or exclusion conditions. Understanding these nuances would be valuable for practical applications where users often specify what not to include in results."
            },
            "questions": {
                "value": "- What is the computational feasibility of using WISe and SICR metrics in large-scale production environments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduce InfoSearch, a novel benchmark aims to evaluate retriever models' instruction following capabilities. The authors find that existing metrics like nDCG, Robustness@k, and pMRR are not sufficient for evaluating instruction following capabilities. To more rigorously evaluate this capability, this paper introduce 2 WISe and SICR. This paper conduct extensive analysis regarding different ranking method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper bridge the gap that no benchmarks are evaluating retriever models' instruction following capabilities \n2. This paper introduces 2 metrics for rigorously evaluating instruction following capabilities\n3. Extensive experimentations \n4. Interesting interpretation and analysis of the main result\n5. reverse mode is interesting!"
            },
            "weaknesses": {
                "value": "1. Sec 2.3 Evaluation Metric, the motivation for designing WISe and SICR is not clear. In L181 - L183, weakness of Robustness@k is not clearly explained. \n\n2. Sec 2.3 Evaluation Metric, since you are comparing with your metrics with pMRR and Robustness@k, it's good to have a comparison table showing your metric is better in handling cases that pMRR and Robustness@k fail, in order to convince your audience that your metrics are effective. Additionally, it would be helpful if you could compare and explain the difference between WISe and SICR. \n\n^ 2 follow-up, I saw Table 2 comparing all metrics, I think it might be good to have a table listing all strengths/weakness at the first place.\n\n3. In eq (1), $F(q) = f_{\\textit{penalty}}(R_{ori}, R_{ins})$ if $\\lnot R_{ins} < R_{ori} < R_{rev}$; whereas, eq (3) only gives $R_{ori} < R_{ins}$. More clarifications will be helpful.\n\n4. One of my major concern is $\\textbf{you do not explain anything about the dataset construction pipeline}$, which I will treat the core of this paper. For example, you lack details \n\n    4.1 How you select web pages?\n\n    4.2 Prompt for step 2? No reference at all. How can you check the quality?\n\n    4.3 Quality check for all GPT-generated responses. There's no details. If they are in the appendix please reference them in the main text. \n\n    4.4 Only saying $\\textbf{manual review}$ is NOT acceptable. Biographies of annotators? Inter-annotator agreement score?"
            },
            "questions": {
                "value": "1. One minor issue but it's prevalent throughout the paper: consider using ~\\citep rather than \\cite for in-text citation, e.g., L145\n\n2. L161, where's your table 2.2? \n\n3. L198, $\\textbf{P}^i$ is unclear, what $i$ refers to? \n\n4. L203, 2 periods \n\n5. eq(2), why you select 20 and 0.01, how you decide 2K? Treat it as normalization factor? \n\n6. L310-L311, for reranking model, can you explain more how it works? \n\n7. It might be super helpful if you can include your benchmark example in the main text. \n\n8. Do you consider using general-purpose LLM for doing point-wise reranking? Like llama-3.2, mixtral, qwen2, etc.?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors evaluate retrieval models' instruction-following capabilities and develop the InfoSearch benchmark covering several dimensions of documents such as audience, keyword, format, language, length, and source. They also introduce new metrics to assess the models\u2019 responsiveness to instructions.\n\nPros:\n- The proposed dataset is somewhat useful to the research community.\n- The exploration to document attributes for instruction-aware retrieval is interesting.\n- The experimental study is insightful.\n\nCons:\n- It would be great if the authors could introduce more on the motivation and application scenario for instruction following retrieval. Based on my understanding, if the instruction following functionality over document-level features can be implemented by introducing LLMs for understanding the query intent then rewriting the query with customized filters.  There is no need for an advanced retrieval model.\nAgain, some examples might help reviewers understand the problem of instruction following retrieval and confirm whether this is a true research problem.\n- Focusing only on the document-level instructions is not enough for evaluating instruction-followed retrieval systems.  Even for document-level instructions, some additional dimensions such as temporal and location, should also be considered.\n- For the experiments, I would expect to add some customized filters (such as language requirement) on existing retrieval models.\n- It is unfair to compare retrieval models with reranking models.  I think the most challenging part is the retrieval part, which may fail to retrieve relevant results without good instruction understanding."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Pros:\n- The proposed dataset is somewhat useful to the research community.\n- The exploration to document attributes for instruction-aware retrieval is interesting.\n- The experimental study is insightful."
            },
            "weaknesses": {
                "value": "Cons:\n- It would be great if the authors could introduce more on the motivation and application scenario for instruction following retrieval. Based on my understanding, if the instruction following functionality over document-level features can be implemented by introducing LLMs for understanding the query intent then rewriting the query with customized filters.  There is no need for an advanced retrieval model.\nAgain, some examples might help reviewers understand the problem of instruction following retrieval and confirm whether this is a true research problem.\n- Focusing only on the document-level instructions is not enough for evaluating instruction-followed retrieval systems.  Even for document-level instructions, some additional dimensions such as temporal and location, should also be considered.\n- For the experiments, I would expect to add some customized filters (such as language requirement) on existing retrieval models.\n- It is unfair to compare retrieval models with reranking models.  I think the most challenging part is the retrieval part, which may fail to retrieve relevant results without good instruction understanding."
            },
            "questions": {
                "value": "- What is the underlying retrieval model for the reranking models in Table 3? (How the initial results are prepared?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "na"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new dataset for evaluating instructable retrievers. The dataset is based on existing IR data and utilizes GPT-4 to generate instructions across six dimensions. Additionally, it rewrites the original documents to align with the new instructions and includes reverse instructions. The paper proposes two new metrics to evaluate models\u2019 sensitivity to instructions: (i) WISe, which rewards search results that comply with instructions and penalizes those that do not; and (ii) SICR, which considers both the relevance score predicted by the model and the ranking information. The paper evaluates different types of retrieval models, finding that the LLM-based listwise reranker performs the best, followed by the pointwise reranker and dense retriever."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Compared to previous data, the new data considers more dimensions, providing better coverage for evaluation.\n\n- This paper demonstrates the weaknesses of previous metrics for evaluating instruction following, such as Robustness@k and pMRR, and introduces two well-motivated alternative metrics.\n\n- This paper conducts an extensive evaluation of different retrieval methods, with insightful results and analysis."
            },
            "weaknesses": {
                "value": "- The proposed data is incremental compared to existing data. For example, InstructIR includes the dimension of Audience, and this paper essentially reuses their data generation pipeline, adding more dimensions.\n\n- Similar to InstructIR, both the instruction and the positive examples are generated by GPT-4. Although the authors mention they manually reviewed the quality, I am still concerned about the data\u2019s reliability: the positive document is rewritten by GPT-4 to align with the instructions. However, GPT-4 itself serves as a baseline model with only moderate performance in determining whether a document follows an instruction.\n\n- The proposed metric assumes there are n positive documents under the original mode, with only one remaining positive under the instruction mode. However, this assumption does not always hold in real-world tasks, which limits this metric\u2019s applicability to their dataset alone. For instance, we might have multiple positives under the instruction mode or find that some positives are not strictly positive under the default mode."
            },
            "questions": {
                "value": "SICR uses relevance scores, but listwise models do not output relevance scores. How do you define the score for these models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}