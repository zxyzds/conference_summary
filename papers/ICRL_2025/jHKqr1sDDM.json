{
    "id": "jHKqr1sDDM",
    "title": "IgBleng: Unifying 3D structures and sequences in antibody language models",
    "abstract": "Large language models (LLMs) trained on antibody sequences have shown significant potential in the rapidly advancing field of machine learning-assisted antibody engineering and drug discovery. However, current state-of-the-art antibody LLMs often overlook structural information, which could enable the model to more effectively learn the functional properties of antibodies by providing richer, more informative data. In response to this limitation, we introduce IgBlend, which integrates both the 3D coordinates of backbone atoms (C-alpha, N, and C) and antibody sequences. Our model is trained on a diverse dataset containing over 4 million unique structures and more than 200 million unique sequences, including heavy and light chains as well as nanobodies. We rigorously evaluate IgBlend using established benchmarks such as sequence recovery, complementarity-determining region (CDR) editing and inverse folding and demonstrate that IgBlend consistently outperforms current state-of-the-art models across all benchmarks. Furthermore, experimental validation shows that the model's log probabilities correlate well with measured binding affinities.",
    "keywords": [
        "Antibodies",
        "LLM",
        "structure",
        "multi-modal"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "We provide an antibody LLM that incorporates both structure and sequence and show that incorporating structure improves results",
    "creation_date": "2024-09-17",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=jHKqr1sDDM",
    "pdf_link": "https://openreview.net/pdf?id=jHKqr1sDDM",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces IgBlend, a new method which adds structural information to an \nantibody language model using a Geometric Vector Perceptron. The authors describe a \npre-training approach which combines multiple objectives for learning sequence and \nstructure-based tasks. \nCurrently, I would recommend to reject this paper for the following reasons: (1) The novelty \nof the approach is limited. There have already been several models proposed which \nintegrate structure into protein language models, including LMDesign which is cited by the \nauthors, and the model is very similar to ESM-IF which has already been adapted to \nantibodies (2) The comparison to other methods is not fair. In particular, other methods \nwere not designed to be run on single chains and/or Nanobodies. (3) The apparent \ninclusion of modelled structures in the test set means that the results cannot be trusted \nsince IgBlend may simply be learning bias in IgFold, especially for Nanobodies."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "\u2022 The paper is well-written and it is easy to understand the differences between the \nexperimental settings. \n\u2022 The comparison to sequence-only settings throughout demonstrates that this \nmodel operates effectively as a sequence model in the absence of structure. \n\u2022 The analysis of model-guided HER2 design was interesting and motivated the \ninclusion of structure. \n\u2022 The ablation of the different model inputs is helpful and informative."
            },
            "weaknesses": {
                "value": "\u2022 The AntiFold results are much worse than those quoted in the AntiFold paper. This \nis likely because AntiFold was trained using both chains whereas this paper tests it \non individual chains (including Nanobodies) which is not a fair comparison. \n\u2022 The test set seems to include a large number of IgFold-generated structures. If this \nis the only method trained on IgFold structures, then it is very likely to perform \nbetter by learning bias in IgFold. Previous approaches test only on high-quality \nexperimental structures. This point holds especially when comparing RMSDs to \nIgFold-predicted structures. \n\u2022 \u201cNotably, IgBlend is the first inverse folding model to achieve results on nanobodies \ncomparable to heavy chains\u201d - this claim is not well supported since it appears that \nall tested Nanobody structures were modelled. \n\u2022 The authors note the similarity to LM-Design in the introduction but do not provide a \ncomparison in the experiments. \n\u2022 The biggest advantage of this work compared to the extensive work on antibody \nlanguage modelling and inverse folding is the \u201cSeq + Struct Guided\u201d setting, \nhowever the benefit of this is not demonstrated in any experiments except \npretraining. Could this be used to generate better embeddings or in a realistic \nantibody design task?"
            },
            "questions": {
                "value": "\u2022 I assume the title IgBleng is a typo? \n\u2022 Is there some inherent benefit to language models which justifies the exclusion of \ntools such as AbMPNN?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors present IgBlend, a model that combines antibody sequences and to enhance antibody engineering. The model shows improved performance in several benchmarks and shows strong correlation with binding affinities in experimental validation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors presented an effective pre-training framework, unifying antibody sturctures and sequences."
            },
            "weaknesses": {
                "value": "1. The authors did not compare IgBlend with another similar method, LM-Design, despite mentioning it in the related work section.\n2. The whole structure information (\"Struct Guided\") was used in sequence recovery and CDR editing tasks, which may introduce potential data leakage.\n3. The sequence recovery and CDR editing tasks are quite similar, and the observed improvements appear marginal.\n4. This paper is not clearly written, and there is a typo in the title (IgBleng -> IgBlend)."
            },
            "questions": {
                "value": "Why did the authors choose GVP as the structure encoder instead of other models, such as MPNN?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents the IgBlend method, a pre-trained model for antibody modeling. The model can encode both sequence and structure information, and predict the masked sequence. The model can also be used in other tasks like inverse folding or CDR design."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "-\tThe authors studied an important research topic.\n-\tThe authors conducted experiments on many downstream tasks."
            },
            "weaknesses": {
                "value": "-\tI have concerns about the novelty because jointly modeling sequence and structure have already been widely studied in many scientific tasks, such as organic small molecules [1,2] and protein [3].\n\n-\tIn Table 2, for the sequence only settings, the IgBlend performance seems to be worse than baselines.\n\n-\tMinor: the caption of Fig.2 should below the figure.\n\n[1] Dual-view Molecular Pre-training, KDD 2023\n\n[2] One Transformer Can Understand Both 2D & 3D Molecular Data, ICLR 2023\n\n[3] Protein sequence and structure co-design with equivariant translation, ICLR 2023"
            },
            "questions": {
                "value": "-\tDoes authors have any plan to release the pretraining code, data, and checkpoints for reproducibility\n-\tAs there are three coordinates for each position (C-alpha, N, and C), how are they processed by GVP-GNN? How are they combined?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces IgBlend, a large language model (LLM) specifically designed for antibody engineering, combining 3D structural data with sequential data. IgBlend aims to address the limitations of existing antibody LLMs by incorporating 3D backbone coordinates (C-alpha, N, and C atoms) alongside traditional sequence information. The model leverages a large dataset comprising over 4 million unique structures and 200 million unique sequences, enabling it to perform tasks like sequence recovery, CDR editing, and inverse folding. Empirical evaluations show that IgBlend consistently outperforms state-of-the-art antibody models across several benchmarks, and its log probabilities correlate well with binding affinity."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed antibody language model has biomedical applications, specifically in antibody and therapeutic design, where such capabilities are highly impactful.\n\nThe model proposes an novel combination of structure and sequence information, which is theoretically a promising approach for antibody design tasks.\n\nThe methodological and architectural descriptions are mostly clear, and the integration of benchmark comparisons supports the claims effectively."
            },
            "weaknesses": {
                "value": "Lack of clarification of the benchmark dataset. Given the fact that the model has seen millions of training samples, I worry about the data leak in benchmarking and the author did not mention much about the benchmark construction.\n\nThe author compare the model on several CDR infilling tasks. As a language model, the utility in representation learning is unclear.\n\nThe HER2 H-CDR3 editing experiment shows a weak correlation (Spearman correlation: 0.24) between model scores and binding affinity, which is also close to the baseline AntiFold of 0.23.\n\nLack of open source code for the implementation and experimental results."
            },
            "questions": {
                "value": "What is the size of your language models?\n\nAntibody sequences and structures are very humongous especially in the constant regions. I wonder if it's worth to train on millions of examples where most of them are similar. In addition, the introduction of predicted antibody structures in the training might introduce biases, and limits the model generalization capabilities. I would like to see some ablations the training data diversity, quantity, and the inclusion of predicted structures.\n\nIt would be convincing to also compare the model in representation tasks, such as antibody related downstream tasks.\n\nIt's important to show the performance gain is from the algorithm instead the memorization of the training set. To this end, I suggest authors to construct a non-redundant test set (such as sequence identity of 50%) for the infilling tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}