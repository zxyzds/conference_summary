{
    "id": "MmOQY71YHw",
    "title": "MS$^3$M: Multi-Stage State Space Model for Motion Forecasting",
    "abstract": "Motion forecasting is a fundamental component of autonomous driving systems, as it predicts an agent's future trajectories based on its surrounding environment. Transformer architectures have dominated this domain due to their strong ability to model both temporal and spatial information. However, transformers often suffer from quadratic complexity with respect to input sequence length, limiting their ability to efficiently process scenarios involving numerous agents. Additionally, transformers typically rely on positional encodings to represent temporal or spatial relationships, a strategy that may not be as effective or intuitive as the inductive biases naturally embedded in convolutional architectures. To address these challenges, we leverage recent advancements in state space models (SSMs) and propose the Multi-Stage State Space Model (MS$^3$M). In MS$^3$M, the Temporal Mamba Model (TMM) is employed to capture fine-grained temporal information, while the Spatial Mamba Model efficiently handles spatial interactions. By injecting temporal and spatial inductive biases through Mamba\u2019s state-space model structure, the model's capacity is significantly improved. MS$^3$M also strikes an exceptional trade-off between accuracy and efficiency, which is achieved through convolutional computations and near-linear computational strategies in the Mamba architecture. Furthermore, a hierarchical query-based decoder is introduced, further enhancing model performance and efficiency. Extensive experimental results demonstrate that the proposed method achieves superior performance while maintaining low latency, which is crucial for practical real-time autonomous driving systems.",
    "keywords": [
        "Motion Forecasting",
        "Autonomous Driving",
        "State Space Model"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "This paper proposes a Mamba-based motion forecasting architecture that achieves superior performance with significantly lower latency.",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=MmOQY71YHw",
    "pdf_link": "https://openreview.net/pdf?id=MmOQY71YHw",
    "comments": [
        {
            "summary": {
                "value": "1) Introduce the Multi-Stage State Space Model (MS3M), to integrate a selective scanning mechanism into motion forecasting tasks.\n2) The paper factors spatial and temporal tasks to two different models:  incorporates a Temporal Mamba Model to\ncapture fine-grained temporal information and a Spatial Mamba Model to model spatial interactions"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The paper is well written and easy to understand.\n2) A good overview of part of the existing work is summarized to help setup the motivations of the paper.\n3) A good ablation analysis is presented to understand different parts of the contribution."
            },
            "weaknesses": {
                "value": "1) The paper is not comparing to a large portion of existing literature.\n2) The factored reasoning over time and space can be achieved easily in the transformer domain using factored attention for example without need for the complicated architectural changed proposed here. There is not comparison to such transformer baselines, which makes it difficult to validate the claims made in the paper. [1]\n3) Experimental section is weak with small number of results just on the Argoverse dataset.\n\n\n\n[1] Wayformer: Motion Forecasting via Simple & Efficient Attention Networks"
            },
            "questions": {
                "value": "1) Why no transformer based baselines have been included in the comparison?\n2) Why WOMD is not used to strengthen the results section?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces MS\u00b3M to predict vehicle trajectories for self-driving cars. The key innovation is using state space models (Mamba) instead of the usual transformer-based approaches that everyone's been using.\n\nThe system has three main parts:\n 1. A temporal model that understands how things move over time\n 2. A multi-stage setup that figures out how different objects interact in space\n 3. A decoder that efficiently processes everything to make predictions\n\nThe authors tested it on the Argoverse 2 dataset and showed it holds up well against existing approaches.\nBasically, they've shown you can get good results without relying on transformers, which might be useful for actual self-driving cars where speed and efficiency matter."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Novelty in using state space models for trajectory prediction\nShows you can match transformer performance with less compute\nMethod is simple to understand and implement\nSolves a real problem (making self-driving predictions faster)"
            },
            "weaknesses": {
                "value": "No code or detailed implementation provided - makes reproducibility impossible\n\nPaper shows results only on Argoverse 2 benchmark\nNo cross-dataset validation\nNo real-world testing results\nNo robustness tests\nLimited ablation studies"
            },
            "questions": {
                "value": "Could you provide a concrete example showing how the FPN processes input sequences of different lengths (e.g., 10 vs 20 timestamps)?\nWhat are the specific constraints on input sequence length, if any?\nPlease include a detailed diagram showing how temporal resolution changes at each FPN level, with specific dimensions and downsampling ratios.\n\nWhat is the exact mathematical formulation used to predict anchor points between stages?\nAre anchor points represented as discrete coordinates or continuous values?\nCould you provide pseudocode for the anchor point computation algorithm?\n\nPlease detail the specific padding/masking operations used to handle varying numbers of agents\nHow are different numbers of lanes normalized or processed in the model architecture?\nWhat is the maximum supported number of agents and lanes?\nCould you provide an architecture diagram showing how variable-length inputs are processed through the network?\n\nCould you provide cross-validation results on the nuScenes dataset, as it represents a different data distribution?\nPlease include robustness tests for varying weather conditions and occlusion scenarios"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "For ego-vehicle motion forecasting, the authors leverage recent advances in state space models (SSMs) and propose the Multi-Stage State Space Model (MS3M). In MS3M, the Temporal Mamba Model is employed to capture fine-grained temporal information, while the Spatial Mamba Model handles spatial interactions. Furthermore, a hierarchical query-based decoder is introduced.\n\nTo demonstrate the effect of the proposed approach, the authors conducted experiments on the large-scale relevant Argoverse2 dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The authors propose the Multi-Stage State Space Model (MS3M), a novel multi-stage architecture that integrates a selective scanning mechanism into motion forecasting task. It incorporates a Temporal Mamba Model to capture fine-grained temporal information and a Spatial Mamba Model to model spatial interactions.\n2. The authors also propose an original hierarchical query-based decoder.\n3. The authors conducted experiments with the relevant and sufficiently diverse Argoverse2 dataset."
            },
            "weaknesses": {
                "value": "1. A significant drawback of the article is the small number of experiments that would confirm the hypotheses on the quality and performance of the proposed approach compared to state-of-the-art approaches. In addition to Argoverse2 for the same task, there are benchmarks for motion forecasting based on Waymo Open Dataset, Lyft, Nuscenes, on which the quality of the proposed approach was not tested. Thus, I recommend the authors to conduct additional experimental validation of their approach on the mentioned datasets. This would raise important questions about the performance of the method and its generalizability to different data distributions.\n2. The positive effect and applicability of the proposed approach is questionable. In Table 1, the authors themselves show that the developed MS3M approach is slightly inferior to the QCNet transformer approach, and in Table 2, the latency ranges of these methods significantly overlap taking into account the variance. Thus, the authors' thesis that the proposed method \"achieves superior performance while significantly reducing model size and latency\" is questionable. I suggest that the authors clarify or revise their claims about superior performance and reduced latency in light of the overlapping ranges.\n3. The comparison of methods does not include approaches that appeared in 2024, such as UniTraj, JointMotion, etc. I recommend the authors to add an updated comparison to the paper and discuss what fundamental advantages MS3M may have over these recent methods.\n4. The text of the paper requires the elimination of typos and inaccuracies, for example, some formulas lack punctuation marks at the end (commas or dots)."
            },
            "questions": {
                "value": "1. Judging by the scheme of the proposed Multi-Stage State Space Model (MS3M), one of the input data is the trajectories of other agents, it would be interesting to see the dependence of the performance of the proposed method on the number of these agents.\n2. How would this dependence compare with the result of transformer models, for example QCNet?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents the Multi-Stage State Space Model (MS$^3$M), a new architecture that leverages recent advancements in state space models to improve motion forecasting in autonomous driving systems by effectively modeling spatial-temporal information. This approach aims to address the inefficiencies of traditional transformer models that suffer from quadratic complexity as the number of agents increases. The paper introduces the Temporal Mamba Model and Spatial Mamba Model to handle temporal information and spatial interactions, respectively. Additionally, a hierarchical query-based decoder further enhances both performance and efficiency by structuring scene processing. This paper includes experiments and ablation studies on the Argoverse 2 dataset to demonstrate the effectiveness of MS3M and the trade-off it achieves between latency and performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper provides a well-articulated introduction to the methods and thorough explanations of the experiments. It introduces the Mamba architecture to handle long sequences and complex spatial-temporal interactions in motion prediction, which represents an interesting and promising direction especially for lighter model with lower latency solution in autonomous driving."
            },
            "weaknesses": {
                "value": "1. It would be beneficial to show the model's performance on additional benchmarks, such as WOMD/NuScenes/Argoverse 1 to strengthen the conclusion that this approach achieves a good trade-off between efficiency and accuracy, given that the best performance of MS$^3$M is still behind the baseline model.\n2. A recent paper [1] also demonstrates significant benefits from the Mamba architecture, and the paper [2] that utilizes LM also shows low latency with reasonable accuracy. It would be interesting to have a deeper analysis of the contributions to model size and latency among different architectures such as CNN/transformer/Mamba.\n3. The caption of Table 3 L493 could be improved to explain the ablation details, making it more readable.\n\n[1] Zhang, Bozhou, Nan Song, and Li Zhang. \"Decoupling Motion Forecasting into Directional Intentions and Dynamic States.\" arXiv preprint arXiv:2410.05982 (2024).\n\n[2] Seff, Ari, et al. \"Motionlm: Multi-agent motion forecasting as language modeling.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023"
            },
            "questions": {
                "value": "Paper [1] also evaluates the latency of QCNet on the Argoverse dataset (Table 2&8), and the latency is much higher. Could this be due to different settings? \n\n[1] Zhou, Yang, et al. \"SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient Motion Prediction.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an motion prediction model based on the Mamba architecture, which consists of the Temporal Mamba Model and the Spatial Mamba Model that handle temporal and spatial dependencies respectively. By leveraging the Mamba architecture's near-linear computational complexity with respect to sequence length, the proposed method demonstrates efficiency advantages compared to QCNet, a transformer-based approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is relatively well-organized and easy to read.\n2. The paper introduces the concept of state space models into the scene encoding of motion prediction, which is a noteworthy innovation."
            },
            "weaknesses": {
                "value": "1. The authors mention in the contributions that 'By injecting inductive biases of temporal and spatial dependency through Mamba\u2019s state-space model structure, the model\u2019s capacity is significantly improved.' The authors should clarify for this improvement relative to which specific methods.\n2. Based on the test results from Argoverse 2, the proposed method shows a significant performance gap in various accuracy metrics compared to Transformer-based methods like QCNet. Although MS$^3$M has the best performance on minFDE$_1$, this is achieved by excluding some more advanced methods such as SEPT and LOF. Therefore, the experiments do not support the claim in the paper that \"Mamba was proposed as a more advanced foundation model\".\n3. The experimental results are not sufficiently convincing, as the authors only conducted experiments on the Argoverse 2. It is recommended to include experimental results on the Argoverse 1 or other datasets.\n4. In the test for inference time, the paper presents some unusual data, such as '16.56\u00b126.38.' I suggest that the authors need to explain why the variance for the MS$^3$M is so large."
            },
            "questions": {
                "value": "1. Among the compared methods, some SOTA approaches listed on the leaderboard, such as SEPT and LOF, are not included. Please clarify the criteria used for selecting the baselines.\n2. In the scene encoding, the tokens corresponding to lane should have a natural permutation invariance property. Why do the authors assert that 'the order of tokens is crucial'? Additionally, what is the specific advantage of using a state-space model, a typical temporal model, to handle the spatial structural relationships in traffic scenes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}