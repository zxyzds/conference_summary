{
    "id": "4RHdGVimNA",
    "title": "StagFormer:  A Staggered Transformer for Decoding Layers in Parallel",
    "abstract": "Standard decoding in a Transformer based language model is inherently sequential as we wait for a token\u2019s embedding to pass through all the layers in the network before starting the generation of the next token. In this work, we propose anew architecture StagFormer (Staggered Transformer), which staggered execution along the time axis and thereby enables parallelizing the decoding process along the depth of the model. We achieve this by breaking the dependency of the token representation at time step $i$ in layer $l$ upon the representations of tokens until time step $i$ from layer $l\u22121$. Instead, we stagger the execution and only allow a dependency on token representations until time step $i\u22121$. The later sections of the Transformer still get access to the \u201drich\u201d representations from the prior section but only from those token positions which are one time step behind. StagFormer allows for different sections of the model to be executed in parallel yielding up to 33% speedup in decoding while being quality neutral. We also explore many natural variants of this idea.  We present how weight-sharing across the different sections being staggered can be more practical in settings with limited memory. We show how one can approximate a recurrent model during inference using such weight-sharing. We explore the efficacy of using a bounded window attention to pass information from one section to another which helps drive further latency gains for some applications. We also explore demonstrate the scalability of the staggering idea over more than 2 sections of the Transformer.",
    "keywords": [
        "decoder only language models",
        "transformers",
        "staggered execution",
        "pipelining",
        "parallel decoding",
        "efficiency"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4RHdGVimNA",
    "pdf_link": "https://openreview.net/pdf?id=4RHdGVimNA",
    "comments": [
        {
            "summary": {
                "value": "This paper proposed a new architecture StagFormer, which stagger the time dependency between the lower and upper layers. The overall design seems a little non-intuitive, but has a lot of potential for throughput and performance. For example, parameter sharing or local cross-attention could yield better throughput."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- StagFormer architecture is interesting, and has very good potential for both performance and throughput.\n- The idea of parameter sharing and recurrent decoding looks good."
            },
            "weaknesses": {
                "value": "- I like the concept and potential of this paper, but I believe that this paper is not well-organized, and looks like unfinished work yet. For example, there is missing reference in L.267 (I guess this refers to Table 3), there are a few results for proof of concept.\n- Table 3 is showing few-shot results for gray, blue, red lines in Figure 4 (correct me if I\u2019m wrong.) I wonder why shared-weights StagFormer (blue) outperforms Baseline 2.8B params (red) in some benchmarks, even though it shows higher loss values.\n- What makes StagFormer 2.9B to outperform Baseline 2.8B params in Table 1? Is it due to cross-attention in upper layers? This looks somewhat interesting and also confusing because I thought the changed structure (using previous timestep\u2019s intermediate activations) could degrade performance a lot.\n- How did the authors measure the decoding time in Table 2? Running separate parameters in parallel is not trivial, I believe. Is it actual time or hypothetical time by assuming parallel execution of them?"
            },
            "questions": {
                "value": "- For KV-caches, the total KV caches are a little increased by the amount of one layer for cross-attention in upper layers, rights?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors present the Staggered Transformer (StagFormer) and its variants which relieve sequential dependancies in the decoding pipeline to enable higher levels of parallel execution.\n\nConsider a transformer with two stacks of layers, A (bottom half) and B (upper half). In vanilla transformers, the input token embedding is passed to stack A. Then, the output of stack A is passed to stack B. All layers apply self-attention on outputs of the previous layer.\n\nIn the baseline StagFormer (`Separate-Weights`), stack A is the same. However, stack B takes in the input token embedding rather than the output of stack A.\nTo supplement this, stack B applies cross-attention on the final outputs of stack A, up until the previous token. In other words, stack B cross-attends to the outputs of *all previous input tokens* from stack A, instead of directly inputting that of the *current* input token. This relieves the dependency of stack B on stack A, within a single decoding step, thus both A and B can be computed simultaneously.\n\nThe authors investigate many variants of this design:\n1. `Shared-Weights`: this is where stack A and stack B share the same model parameters (excluding the cross-attention layers which are unique to stack B).\n2. `Recurrent, Shared-Weights`: this is a unique decoding method for the `Shared-Weights` trained model. In `Shared-Weights` stack A and B are identical, except that stack B applies cross-attention to outputs from stack A. Essentially, the shared stack S (= A = B) is first forwarded without cross-attention, and then forwarded a second time *with* cross-attention, attending to outputs from the first forward pass. The `Recurrent` setting refers to that where the first forward pass is skipped, and cross-attention in the second pass attends to outputs of the \"second\" pass from the previous decoding step.\n3. `p > 2`: this is where more than two stacks are considered.\n\nWhen compared to vanilla transformers pretrained from scratch, StagFormers show various advantages, mainly:\n- `Shared-Weights 2x18L`: StagFormer outperforms the vanilla 18L baseline (with roughly same parameters) in both perplexity and average task performance. Using recurrent decoding (roughly matching 18L baseline computation), average task performance lies between the two. StagFormer underperforms the vanilla 36L baseline with roughly same computation in perplexity, but performs comparably on tasks.\n- `Separate-Weights 2x18L`: StagFormer outperforms the vanilla 36L baseline (with roughly same parameters and compute) in both perplexity and task performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The idea and architecture design are very novel\n1. The authors propose numerous variants which showcase the potential extension of the idea across various axes\u2013parallel execution, weight sharing, recurrent computation.\n1. The architecture shows clear advantages over vanilla transformers across its variants\n1. The writing is easy to follow and visual depiction of the architecture and its variants are superb."
            },
            "weaknesses": {
                "value": "1. **Memory  bottlenecks during decoding may hinder benefits of parallel execution, which is not discussed**: LM decoding is typically bottlenecked by memory rather than compute (see references below). When batch size x context length is small, memory access is dominated by model parameter access. Otherwise, memory access is dominated by KV cache access. While StagFormer can conceptually *parallelize* execution of layers, the associated memory access load cannot be parallelized. In fact, the cross-attention layer will add additional KV cache access overhead. These are critical to assessing the actual wallclock time benefits of decoding with StagFormers compared to vanilla transformers, but is not discussed.\n    1. Different variants of StagFormers will have different memory bottlenecks. Examples:\n        1. All variants: cross-attention is added in half of layers. Therefore, the overall KV cache access overhead will increase by 50% (relative to that of self-attention, used in all layers). This will have a larger effect on decoding time as batch size x sequence length becomes large.\n        1. `Separate-Weights`: both stacks can be executed in parallel, but the memory load is identical as the parameters of both stacks must be retrieved from memory. This means that wall-clock time should typically be identical to vanilla transformers, as decoding is bottlenecked by memory access. `Shared-Weights` can solve this issue.\n    1. **It is unclear which StagFormer variant is used in Table 2, raising questions on the performance vs latency comparison**: While Table 2 states that a \"comparable quality StagFormer\" is 33% faster than baseline transformer during decoding, the exact variant is unclear. Given the reasons above, it seems likely that this is the `Shared-Weights 2x18L` variant. While its average task performance is comparable to baseline 36L, its PPL is in the middle of that between vanilla 18L and 36L. It would be misleading to describe this variant as \"comparable quality\" to vanilla 36L.\n    1. **Missing comparison of performance vs latency across model variants**: Expanding on the point above, a comparison of prefill/decode time across model variants will provide a clear picture on the performance vs latency benefits of each model variant. This could take the form of a single table that lists the PPL, task performance, and prefill/decode time for each model. In the case of  `p > 2, Shared-Weight` variants, I believe this may actually reveal some advantages in terms of latency.\n    1. **The additional KV cache overhead of cross attention may slow down decoding for longer contexts**: Since KV cache overhead is quadratic to context length, the decode time advantages as shown in Table 2 will likely diminish with longer contexts, especially in batch decoding. Given the relatively short context length of 1024 tokens considered in this study, compared to modern LLMs with 8K+ context, measurement on longer contexts and larger batch sizes can help gauge the potential of the architecture.\n1. **Misleading task performance of `Recurrent` variant**: In Table 3 (for example), the performance of various tasks are identical between the `Shared-Weights 18L` model and its `Recurrent` counterpart. This is likely because the tasks are measured in a teacher-forcing setting, where the outputs of the prefill stage are used for evaluation. This does not represent the task performance of the `Recurrent` setting, as recurrence is only applied to decoding, as explained in Section 3.2.\n1. **Experimental results on model variants are hard to follow**: The organization of the results section could be improved to make the comparison between different model variants more clear.\n    1. Within tables, variations could be better indicated with separate columns, task names could be shortened for space, latency metrics could be included, etc.\n    1. Results on different variants are presented in multiple tables without a clear organization.\n1. **Incomplete writing**: \"(TODO)\" in Line 385, the reference error \"??\" in Line 267, and numerous typos suggest that this is an incomplete manuscript that is not ready for review.\n\nReferences on memory bottlenecks during inference\n- [Efficiently Scaling Transformer Inference](https://arxiv.org/abs/2211.05102)\n- [LLM Inference Unveiled: Survey and Roofline Model Insights](https://arxiv.org/abs/2402.16363v4)\n- [Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve](https://arxiv.org/abs/2403.02310)\n- [Block Transformer: Global-to-Local Language Modeling for Fast Inference](https://arxiv.org/abs/2406.02657)"
            },
            "questions": {
                "value": "1. Can you describe the architecture shape (vocab size, qkv heads, embedding dimensions) and its justification? The vocab size of 256K is quite high for models of this size.\n1. In Lines ~499-501, the authors mention that cross-attention is linear to input length instead of quadratic with window size 1. Isn't it linear with any fixed window size? Considering that the cost of attention mainly stems from KV cache IO during decoding, I think the constant factor with a window size as small as 128 makes the cost of cross-attention negligible compared to self-attention (especially when expanding to modern context lengths of 8K or more).\n    1. However, the *increase* in performance when going from full cross-attention (1024) to windowed attention with window size 512 and 128 is strange. Can the authors justify this increase in performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel Transformer architecture called StagFormer designed to improve the efficiency of decoding in Transformer-based language models by enabling the parallel execution of layers along the depth axis."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. StagFormer introduces a unique method to break the sequential dependency of layers in Transformers, enabling parallel execution.\n2. Experiments demonstrate significant latency reduction while maintaining or even exceeding the quality of a standard Transformer.\n3. The paper investigates different StagFormer variants, offering flexibility and adaptability to various scenarios and resource constraints.\n4. The paper effectively explains the StagFormer concept and its variants, supported by clear diagrams and algorithms."
            },
            "weaknesses": {
                "value": "1. Limited exploration of p > 2. While the paper explores StagFormer with more than two stacks, it acknowledges performance degradation and the need for further research in this area.\n2. The paper mentions the communication cost associated with parallel execution but doesn't offer concrete solutions to mitigate it.\n3. While the Pile dataset is comprehensive, evaluating on additional datasets would strengthen the generalizability of the findings.\n4. Comparing StagFormer with other methods for efficient Transformer inference, such as speculative decoding, would provide a more comprehensive perspective."
            },
            "questions": {
                "value": "1. How does varying the depth of individual stacks in StagFormer affect the trade-off between decoding speed and model quality?\n2. What factors determine the optimal number of stacks for a given application, balancing computational efficiency and performance?\n3. Could the staggering concept be extended to encoder-decoder Transformers, like those used in machine translation?\n4. How well could StagFormer be combined with other techniques, like quantization or knowledge distillation, to further enhance decoding efficiency?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel transformer architecture that effectively reduces the number of sequential steps (layers) during the decoding process by staggering the computation across different time-steps. This allows for improved parallelism during decoding individual sequences, providing speedups during inference."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- staggered computation leads to significant improvements in per-time-step decoding speeds while slightly improving performance\n- provides results and analysis of different variants of staggered transformers that further explores the architecture's efficacy"
            },
            "weaknesses": {
                "value": "- Biggest critique is that it lacks comparative analysis of staggering computation vs. simply increasing the width of the model and lowering the number of layers, as this increases per layer parallelism while decreasing the number of layers leading to a similar improvement in decoding speed.\n- This technique is possibly only useful for speeding up decoding when only a single sequence is being decoded. A non-staggered model could in theory process twice the batch size as it has half the parallelism (and hence half the per layer memory requirement) of a model staggered with p=2. \n- StagFormer is possibly slower to train (as inferred by its slower pre-filling speed)\n- Paper could be further refined (minor critique): \n    - Some references are configured incorrectly (Table ?? in page 5, \"TODO\" in page 8)\n    - Plots have unnecessary information (Figure 4 doesn't need texts like /1/summarize/train)"
            },
            "questions": {
                "value": "Addressing the weaknesses outlined above would improve the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}