{
    "id": "QOXrVMiHGK",
    "title": "PEARL: Parallel Speculative Decoding with Adaptive Draft Length",
    "abstract": "Speculative decoding (SD), where an extra draft model is employed to provide multiple **draft** tokens first and then the original target model verifies these tokens in parallel, has shown great power for LLM inference acceleration.\nHowever, existing SD methods suffer from the mutual waiting problem, i.e., the target model gets stuck when the draft model is *guessing* tokens, and vice versa. This problem is directly incurred by the asynchronous execution of the draft model and the target model, and is exacerbated due to the fixed draft length in speculative decoding.\nTo address these challenges, we propose a conceptually simple, flexible, and general framework to boost speculative decoding, namely \n**P**arallel sp**E**culative decoding with **A**daptive d**R**aft **L**ength (PEARL). \nSpecifically, PEARL proposes *pre-verify* to verify the first draft token in advance during the drafting phase, and *post-verify* to generate more draft tokens during the verification phase.\nPEARL parallels the drafting phase and the verification phase via applying the two strategies, and achieves adaptive draft length for different scenarios, which effectively alleviates the mutual waiting problem.\nMoreover, we theoretically demonstrate that the mean accepted tokens of PEARL is more than existing *draft-then-verify* works.\nExperiments on various text generation benchmarks demonstrate the effectiveness of our PEARL, leading to a superior speedup performance up to **4.43$\\times$** and **1.50$\\times$**, compared to auto-regressive decoding and vanilla speculative decoding, respectively.",
    "keywords": [
        "speculative decoding",
        "inference acceleration",
        "large language models"
    ],
    "primary_area": "generative models",
    "TLDR": "we introduce PEARL (Parallel spEculative decoding with Adaptive dRaft Length) to further reduce the inference latency of Large Language Models (LLMs).",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=QOXrVMiHGK",
    "pdf_link": "https://openreview.net/pdf?id=QOXrVMiHGK",
    "comments": [
        {
            "title": {
                "value": "Response to Reviewer LqAV (part 3/3)"
            },
            "comment": {
                "value": "Table R5. Comparisons of different SD methods on MGSM with Llama 2 7&70b. The highest speedups are bolden.\n\n|                     | english (GSM8K) | bengali  | german   | spanish  | french   | japanese | russian  | swahili  | tegulu   | thai     | chinese  | average  |\n| ------------------- | --------------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| AR                  | 1.00            | 1.00     | 1.00     | 1.00     | 1.00     | 1.00     | 1.00     | 1.00     | 1.00     | 1.00     | 1.00     | 1.00     |\n| SD                  | 2.48            | 2.69     | 2.77     | 2.64     | 2.71     | 2.71     | 2.72     | 2.81     | 2.65     | 2.71     | 2.78     | 2.70     |\n| Lookahead           | 1.23            | 1.34     | 1.51     | 1.50     | 1.48     | 1.29     | 1.43     | 1.60     | 1.28     | 1.23     | 1.48     | 1.39     |\n| ouroboros           | 1.60            | 1.75     | 1.88     | 1.69     | 1.80     | 1.95     | 1.65     | 1.68     | 2.45     | 1.92     | 1.81     | 1.84     |\n| assisted generation | 1.96            | 1.69     | 1.75     | 1.70     | 1.67     | 2.02     | 1.68     | 1.58     | 3.07     | 2.17     | 1.97     | 1.93     |\n| **PEARL**           | **3.82**        | **3.94** | **4.00** | **3.81** | **3.76** | **3.94** | **3.85** | **4.18** | **4.10** | **3.93** | **4.06** | **3.95** |\n\n> 4. Other questions\n\nPhew! Thanks for your time and detailed feedback! Thanks for your clear effort to think critically about how to improve our paper! We answer them respectively.\n\n> Q1: Is additional communication overhead costs incurred when running drafting and verification on separate accelerators? (e.g. logits transport from 2 devices for rejection sampling/verification)\n\nAs shown in Table R2, the additional communication overhead incurred by running drafting and verification on separate accelerators is negligible.\n\n> Q2: There are 5 baselines listed but Table 2 and Table 3 only report Auto-regressive and SPEED?\n\nWe conduct additional experiments accordingly in Tables R4 and R5.\n\n> Q3: What is the assisted generation baseline exactly? Does it adjust draft length depending on the number of tokens accepted in the previous iteration?\n\nAssisted generation [3] is an improved speculative decoding algorithm. It initially sets the window size as 5, and increases it by 2 if all draft tokens are accepted, or decreases it by 1 otherwise. In our experiments, this strategy cannot handle the dynamic draft length well, sometimes brings additional overhead, and incurs more severe mutual waiting problems.\n\n> Q4: What value of gamma was used for the baselines on each task? Was it fixed according to the optimal gamma values determined for PEARL?\n\nFor vanilla speculative decoding, we search the optimal $\\gamma$ and report its best performance. For other baselines, we either use the reported number in their original manuscript or using their official codes with default parameters for reproduction.\n\n> Q5: Why do Table 5 and Table 6 differ for HumanEval with gamma=5 for Llama2 7B&70B (40.72 in Table 5 and 30.34 in Table 6)?\n\nWe apologize for any confusion that may have arisen from the discrepancy between Tables 5 and 6. To clarify, the results presented in Table 5 correspond to the **CodeLlama** model, whereas the results in Table 6 correspond to the **Llama2** model. \n\n> Q6: Is there a cap on draft length given the fixed optimal verification window size? Something like 2x? Large gamma values aren\u2019t only detrimental to drafting phase but also incur additional computational cost in verification (verifying 4 tokens vs. 32 can be a significant difference) even for vanilla SD\n\nThe draft length corresponds to the maximal number of draft tokens that can be accepted at a specific speculative step. Theoretically, there is no fixed upper bound of the draft length, as the draft model can generate draft tokens as many as possible. In PEARL, the verification of a large draft length is split into many windows, and each window contains a fixed number of draft tokens. This chunked verification is parallelized with the drafting process, hence it does not incur additional verification time cost.\n\n> Q7: Table 9 should clarify that it\u2019s not inference speed time but is the number of model runs? Perhaps good to report the ratio on the side (PEARL has ?x more model runs than SD)\n\nThanks for your suggestions. We have added the statistics accordingly.\n\n> Other typos in the manuscript.\n\nThank you for pointing out these typos, we will update them together into the revised PDF.\n\n\nWe humbly hope our response has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work.\n\n[3] https://huggingface.co/blog/assisted-generation"
            }
        },
        {
            "title": {
                "value": "Response to Reviewer LqAV (part 2/3)"
            },
            "comment": {
                "value": "> 2. Additionally, given the fact that drafting and verification is assumed to occur on separate devices in parallel, it would be good to see a mention into the communication overhead of data transfer (i.e. logits for adjusted sampling during verification rejection) as well as a breakdown into the additional computational and power consumption in practice from added drafting and verification calls that PEARL conducts in comparison to SD.\n\nThank you for your insightful feedback. We measure the time cost of each component in one PEARL step with different sizes of model pairs in Table R2. For a more distinct comparison, we also provide the measurement of SD in Table R3. As shown in these 2 tables, the communication overhead within PEARL is negligible. \n\nTable R2. The time cost of each component in one PEARL step. The experiments are conducted on HumanEval. \n\n|               | llama 2 7&70b | codellama 7&34b | deepseek 1.3&33b |\n| ------------- | ------------- | --------------- | ---------------- |\n| communication | 0.2 ms        | 0.3 ms          | 0.2 ms           |\n| verify        | 1.7 ms        | 1.6 ms          | 1.7 ms           |\n| draft         | 105.1 ms      | 68.9 ms         | 65.1 ms          |\n| target        | 108.0 ms      | 71.1 ms         | 66.1 ms          |\n\nTable R3. The time cost of each component in one SD step. The experiments are conducted on HumanEval. \n\n|        | llama 2 7&70b | codellama 7&34b | deepseek 1.3&33b |\n| ------ | ------------- | --------------- | ---------------- |\n| verify | 2.7 ms        | 2.0 ms          | 2.1 ms           |\n| draft  | 107.5 ms      | 68.0 ms         | 65.7 ms          |\n| target | 115.4 ms      | 69.1 ms         | 67.4 ms          |\n\nFor the additional computations and power consumptions within PEARL, we first measure the computations of a single model forward with thop [1]. For Llama 2 7b, the computation is 6.607 GFLOPS per token, while the computation is 68.713 GFLOPS per token for Llama 2 70b. Combining the results in Table 9, PEARL will take more computations of about 41.75% than SD, as well as the power consumption. However, our experiments demonstrate that PEARL significantly outperforms SD with up to 1.5$\\times$ inference acceleration, which takes the duty of these additional computations and power consumptions.\n\n[1] https://github.com/ultralytics/thop\n\n> 3. While a variety of tasks (HumanEval, GSM8K & MGM, MT-bench) and baselines (SD, Ouroboros, Lookahead Decoding, Distillspec, Assisted Generation) were mentioned in the experiments, the variety of baselines were only used for the HumanEval code generation task while the remaining GSM8K & MGM and MT-bench tasks only used auto-regressive (AR) and SD baselines.\n\nWe update the experiments to include results for the additional baselines (Lookahead, Ouroboros, Assisted Generation with LLaMA 2 7&70B) on the MT-Bench and MGSM tasks in Tables R4 and R5. These results will be added to Tables 2 and 3. \nRegarding Lookahead and Ouroboros, it is important to note that these methods currently do not support the LLama 3.1 models, as their official codes only support transformers<=4.36.2 [2], but the Llama 3.1 models require transformers>=4.43. For DistillSpec, the source code has not been made publicly available, and therefore, we are unable to include it as a baseline in our experiments in a short period. We will actively try to communicate with the authors of DistillSpec and reproduce its performance in the future for a more detailed comparison.\n\n[2] https://github.com/hao-ai-lab/LookaheadDecoding/tree/main\n\n\n\nTable R4. Comparisons of different SD methods on MT-bench with Llama 2 7&70b. The highest speedups are bolden.\n\n|                   | writing  | roleplay | reasoning | math     | coding   | extraction | stem     | humanities | average  |\n| ----------------- | -------- | -------- | --------- | -------- | -------- | ---------- | -------- | ---------- | -------- |\n| AR                | 1.00     | 1.00     | 1.00      | 1.00     | 1.00     | 1.00       | 1.00     | 1.00       | 1.00     |\n| SD                | 1.70     | 1.73     | 1.96      | 2.00     | 1.93     | 2.14       | 1.87     | 1.81       | 1.89     |\n| Lookahead         | 1.31     | 1.24     | 1.50      | 1.51     | 1.38     | 1.40       | 1.29     | 1.27       | 1.36     |\n| assist generation | 1.41     | 1.40     | 1.39      | 1.64     | 1.74     | 1.92       | 1.57     | 1.47       | 1.55     |\n| ouroboros         | 1.42     | 1.35     | 1.40      | 1.61     | 1.35     | 1.67       | 1.44     | 1.36       | 1.45     |\n| **PEARL**         | **2.40** | **2.45** | **2.85**  | **2.79** | **2.67** | **2.92**   | **2.58** | **2.50**   | **2.64** |"
            }
        },
        {
            "title": {
                "value": "Response to Reviewer LqAV (part 1/3)"
            },
            "comment": {
                "value": "We thank the reviewer for the insightful and valuable comments. We respond to each comment as follows and sincerely hope that our rebuttal could properly address your concerns. If so, we would deeply appreciate it if you could **raise your score** (5: marginally below the acceptance threshold). If not, please let us know your further concerns, and we will continue actively responding to your comments and improving our submission.\n\n> 1. The main paper assumes execution scenarios where there are enough resources to run drafting and verification in parallel (e.g. multiple GPUs). However, in many instances, drafting and verification happen in co-located settings (e.g. single GPU) where it is more resource constrained. While the authors do make brief mentions in the main paper about these resource constrained scenarios, their discussion and strategies to address this are limited to a relatively short section in the appendix with not much details on their experimental results. It would be much more fitting to have this section filled with more detail and part of the main paper. For example, the strategy mentioned in the appendix involved copying the drafter model across multiple chips which incurs greater memory cost as well as potential communication cost in having to transport and sync intermediate attention KV caches.\n\nWe appreciate the reviewer's insightful comments. **In response, we will expand the discussion of resource-constrained scenarios in the main body of the paper, specifically in Section 4.5 (Case Studies).**  We also revise the PDF accordingly. For other weakness, we answer as follows:\n\n1. **Clarification of application scenarios.** The core idea of SD series methods is to leverage redundant computational resources for acceleration. Our PEARL is motivated by the mutual waiting problem, which indicates that SD methods cannot fully utilize the computational resources. Based on this idea, the main application scenarios of PEARL focus on multi-GPU settings, where adequate computational resources can be exploited. We also would like to emphasize that multi-GPU applications, such as high-throughput inference scenarios and edge computing, are highly relevant in practical deployments, where sufficient resources can be allocated for parallel drafting and verification. These settings are important for scaling the approach to large, complex models. \n2. **PEARL in resource-constrained scenarios.** While our experiments are conducted primarily on multi-GPU settings, we acknowledge that resource-constrained scenarios are very common, and we provide an adaptive method to deploy PEARL in such scenarios. Regarding the details mentioned in the appendix, we provide further clarification on the memory and communication costs. \n   1. (**Additional Memory Cost**) In the strategy provided in the appendix, PEARL requires loading only 1 additional copy of the draft model. Given that the scale of the draft model is typically smaller than 7B, this additional memory cost is lower than 14GB GPU memory (e.g., Llama 2 7B with bfloat 16). It can be further reduced to < 1B with EAGLE draft heads.\n   2. (**Additional KV cache Sync Cost**) Our strategy does incur additional KV cache sync costs to transport the KV cache. However, as the draft model is relatively small, the KV cache itself does not incur significant memory overhead. For example, with Llama 3.1 8B, batch size=1 and input length=1024, the size of the kv cache is $2\\times 2 \\times 32\\times 1\\times 8\\times 1024\\times 128 \\approx 0.13$ GB. With NVlink, the theoretical time cost for transporting the KV cache is $0.13/300\\approx 0.43$ ms, which is significantly lower than the computational time cost. Other techniques such as  KV cache compression/quantization can further reduce communication costs. **Besides, this time cost can be parallelized with the target model verification process.** We provide empirical results of the KV cache transport time cost in Table R1. Note that in our implementation, the kv cache is sequentially transported, leading to a significantly larger time cost. However, this time cost can still be ignored (<5% of total time cost). These results demonstrate the effectiveness of our approach in mitigating resource contention and supporting efficient multi-task execution in resource-constrained environments.\n\nTable R1. The empirical time cost of transporting KV cache with different input length. The experiments are conducted with Llama 3.1 8B on HumanEval. \n\n| input length        | 128    | 256    | 512    | 1024   |\n| ------------------- | ------ | ------ | ------ | ------ |\n| empirical time cost | 1.3 ms | 1.4 ms | 1.5 ms | 1.6 ms |"
            }
        },
        {
            "title": {
                "value": "Response to Reviewer dgRU (part 3/3)"
            },
            "comment": {
                "value": "Table R4. Inference speed ratio in TensorRT LLM (with TP) and Huggingface (without TP).\n\n| Framework                | batch size | input length | output length | Llama2 7B speed (tok/s) | Llama2 70B speed (tok/s) | speed ratio |\n| :----------------------- | :--------- | :----------- | :------------ | :---------------------- | :----------------------- | ----------- |\n| TRT LLM (with TP)        | 1          | 512          | 512           | 68                      | 31                       | 2.19        |\n| TRT LLM (with TP)        | 1          | 1024         | 1024          | 65                      | 30                       | 2.17        |\n| huggingface (without TP) | 1          | 512          | 512           | 49                      | 10                       | 4.9         |\n| huggingface (without TP) | 1          | 1024         | 1024          | 49                      | 10                       | 4.9         |\n\n[1] https://github.com/kvcache-ai/Mooncake\n\n[2] https://docs.vllm.ai/en/v0.5.5/models/spec_decode.html\n\n[3] https://github.com/dilab-zju/self-speculative-decoding/issues/21\n\n\n\n> 4. Furthermore, using PP instead of TP causes a higher activation memory footprint, reducing the effective batchsize the model can accommodate during decoding, effectively reducing the overall throughput.\n\nThanks for your constructive comments. \n\n1. **Clarification of TP and PP.** TP and PP are both effective ways for inference acceleration and can be combined for better efficiency.  Several LLM inference frameworks, such as TRTLLM [5] and VLLM [6], demonstrate successful implementations where TP and PP are used in tandem. In our implementation, we choose PP as basic parallelism due to its wider applications and simpler implementations.\n2. **The potential of integrating PEARL into the existing TP framework.** We also provide an idea of TP implementation as an alternative parallelism strategy in response to weakness 3. To clarify, the key idea of PEARL is to utilize the underutilized GPU computations due to the mutual waiting problem, and this does not conflict with tensor parallelism. Therefore, our PEARL can be integrated with the existing TP framework and we leave it as a future work.\n3. **The batchsize issue.** It is important to note that the speculative decoding (SD) framework performs suboptimally in high-batch settings, as outlined in recent findings from the VLLM blog [4]. SD leverages redundant compute capacity, but in high batch-size regimes, the amount of redundancy diminishes, leading to an overhead from token rejection that outweighs the benefits of SD. This issue is exacerbated as the number of rejected tokens increases, resulting in a significant slowdown. In contrast, PEARL, with its adaptive draft length mechanism, **reduces this overhead by adaptively adjusting the draft length**, allowing it to better accommodate larger batch sizes and improving efficiency compared to SD.\n\n[4] https://blog.vllm.ai/2024/10/17/spec-decode.html\n\n[5] https://github.com/NVIDIA/TensorRT-LLM\n\n[6] https://github.com/vllm-project/vllm\n\nWe humbly hope our response has addressed your concerns. If you have any additional concerns or comments that we may have missed in our responses, we would be most grateful for any further feedback from you to help us further enhance our work."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer dgRU (part 2/3)"
            },
            "comment": {
                "value": "> 2. Some parameters are underspecified in the tokens accepted per second table (Table). It is not clear under what lookahead length are the baseline numbers achieved, or if they are optimal.\n\nWe appreciate the reviewer's feedback regarding parameter specification in Table 7 (for your convenience, we quote Table 7 as Table R2 here). In the mean accepted tokens experiments, we select the optimal $\\gamma$ for each model pair. Below, we provide a detailed comparison across different $\\gamma$ values in Table R3. Our reported baseline results align closely with the most optimal parameters. \n\n\n\nTable R2. Comparison of mean average accepted tokens of vanilla SD methods and PEARL. We run experiments to search the optimal $\\gamma$ for SD and report their best results.\n\n|       | Codellama 7&34B   | Codellama 7&70B    | Deepseek 1.3&33B  | Deepseek 6.7&33B  |\n| ----- | ----------------- | ------------------ | ----------------- | ----------------- |\n| SD    | 5.27 ($\\gamma=6$) | 8.32 ($\\gamma=10$) | 7.23 ($\\gamma=8$) | 5.69 ($\\gamma=6$) |\n| PEARL | **27.95**         | **26.53**          | **29.65**         | **39.90**         |\n\n\n\nTable R3. Optimal $\\gamma$ values of SD for each model pair. (Unit: tokens / second)\n\n| codellama 7&34         | codellama 7&70          | deepseek 1.3&33        | deepseek 6.7&33        |\n| ---------------------- | ----------------------- | ---------------------- | ---------------------- |\n| 30.95 ($\\gamma=4$)     | 25.92 ($\\gamma=8$)      | 37.22 ($\\gamma=6$)     | 30.92 ($\\gamma=4$)     |\n| 31.85 ($\\gamma=5$)     | 26.02 ($\\gamma=9$)      | 38.53 ($\\gamma=7$)     | 31.74 ($\\gamma=5$)     |\n| **33.57 ($\\gamma=6$)** | **27.60 ($\\gamma=10$)** | **39.52 ($\\gamma=8$)** | **33.77 ($\\gamma=6$)** |\n| 33.52 ($\\gamma=7$)     | 27.23 ($\\gamma=11$)     | 39.38 ($\\gamma=9$)     | 33.55 ($\\gamma=7$)     |\n| 32.79 ($\\gamma=8$)     | 26.65 ($\\gamma=12$)     | 38.69 ($\\gamma=10$)    | 32.97 ($\\gamma=8$)     |\n\n\n\n> 3. The authors picked pipeline parallelism for their implementation. However, while this is a convenient setup for solving the \"resource contention\" challenge, this is an unreasonable setting and introduces much higher latency in the first place. In deploying a 70B target model with a 7B draft model, using tensor parallelism (TP) can reduce the latency of the target model by leveraging more parallelism in each layer. Therefore, this casts doubt on all the speedup that the authors reported as this causes both the baseline and their reported results to be slower than in a TP setup. Also, with TP, the proposed solution to resource contention in Appendix E would not apply and it is not clear whether the authors can show a similar speedup with their algorithm.\n\nThank you for your insightful feedback. We acknowledge the benefits of TP in reducing inference latency. We clarify how our PEARL adapts to TP setting and emphasize its contributions as follows:\n\n1. **Our primary application scenarios are those with sufficient resources**, including adequate GPU capacity, to deploy both the large and small models on separate devices. In this situation, enabling tensor parallelism for both the draft model and the target model is normal and brings no significant overhead. For example, deploy the draft model with 1 A100 GPU with TP=1 and the target model with 7 A100 GPUs with TP=7. We emphasize this scenario is very common and promising in the industry. For example, Mooncake uses prefill-decode separation techniques to further improve the serving efficiency [1]. \n2. While our proposed solution in Appendix E can help PEARL to apply in PP settings, **it remains possible to integrate PEARL with both PP and TP.** Take an instance of our toy example in Lines 849 and 857, in the first 12 micro-steps, theoretically, we can enable tensor parallelism for the target model with TP=3 on GPUs 0, 1, and 2; while in the last 8 micro-steps on GPUs 1, 2, 3. However, we acknowledge that the implementation of tensor parallelism in the speculative setting is challenging, even if the vllm team cannot support vanilla speculative decoding with TP [2]. Moreover, many SD methods also do not support TP implementation as well [3]. We believe that constructing a general TP framework for SD methods is essential, and we think this framework deserves its own publication.\n3. **We emphasize that the main contribution of our work is the discovery of the mutual waiting problem.** This problem is very common, and will even be exacerbated in TP settings. An observation listed in Table R4 shows that the running speed ratio $c$ between the draft model and the target model is even smaller in TP settings, which leads to a more severe mutual waiting problem. We believe this is a fundamental obstacle for existing SD frameworks to deploy in real-world applications, and our PEARL stands as a pioneer in work to eliminate the mutual waiting problem."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer dgRU (part 1/3)"
            },
            "comment": {
                "value": "We thank the reviewer for the insightful and valuable comments. We respond to each comment as follows and sincerely hope that our rebuttal could properly address your concerns. If so, we would deeply appreciate it if you could **raise your score** (3: reject). If not, please let us know your further concerns, and we will continue actively responding to your comments and improving our submission.\n\n> 1. The latency of verifying a 70B model is about generating 2.5 tokens on the 8B model. Based on the speedup that the authors provided, this parallel approach would not work beyond a lookahead length of 3, which is shown to be suboptimal empirically. Therefore it is not clear how much this post-verify step improves the performance.\n\nThank you for bringing up the insightful comment. We answer it in three folds:\n\n1. **Clarification of Definition of Draft Length and Window Size**: We apologize for any confusion regarding the definition of the draft length. As in Line 48 of the original manuscript, the draft length refers to the number of tokens generated by the draft model in a continuous execution. This definition indicates that theoretically there is no upper limit to the draft length, as the draft model may generate draft tokens as much as possible. However, the mentioned *lookahead length*, i.e., window size in Line 47, is fixed at any step. In our PEARL, we theoretically show in Section 5.4.1 that the optimal window size should be $c$, i.e., the inference speed ratio between the draft model and the target model, which aligns well with the empirical results in Table 5 of the original manuscript. \n2. **Motivation of the Post-verify Strategy:** the key idea of post-verify is to allow the draft model to generate more than $\\gamma$ tokens without waiting for the target model's verification. For example, given that \"the latency of verifying a 70B model is about generating 2.5 tokens on the 8B model and we set the window size as 3\" and if the optimal draft length is 12, post-verify fully exploits the drafting ability of the small model to continually generate 4 windows of draft tokens without been interrupted by the target model verification. In this case, the de facto draft length is adaptively adjusted to 12. \n3. **Effectiveness of the Post-verify Strategy:** As illustrated in Figure 2 (b) in the original manuscript, the optimal draft length dramatically changes in different steps. Using a fixed window size to generate draft tokens cannot fit this change, while our post-verify allows the draft model to generate more tokens when the optimal draft length is larger than $\\gamma$. Besides, we also performed an ablation study, as shown in Table 4 (for your convenience, we quote Table 4 as Table R1 here). The results also demonstrate that the absence of the post-verify strategy leads to a substantial reduction in the speedup, which further validates the effectiveness of our post-verify strategy.  \n\n\n\nTable R1. Ablation study of post-verify strategy on HumanEval and GSM8K datasets. PEARL *w/o post-verify* denotes PEARL without post-verify strategy.\n\n|                         | Humaneval        | Humaneval        | Humaneval         | GSM8K            |\n| ----------------------- | ---------------- | ---------------- | ----------------- | ---------------- |\n|                         | Codellama 7B&34B | Codellama 7B&70B | Deepseek 1.3B&33B | Llama 2 7B&70B   |\n| PEARL *w/o post-verify* | 1.64$\\times$     | 2.57$\\times$     | 2.37$\\times$      | 2.15$\\times$     |\n| PEARL                   | **2.35$\\times$** | **3.79$\\times$** | **3.48$\\times$**  | **2.87$\\times$** |"
            }
        },
        {
            "summary": {
                "value": "This paper aims to accelerate LLM decoding by building on top of the popular Speculative Decoding (SD) algorithm. It addresses a key bottleneck in SD, the mutual waiting problem: the draft model and target model often get stuck waiting for each other because of sequential execution of the two logic with fixed draft lengths.\n\nUnlike traditional SD, the proposed algorithm PEARL (Parallel spEculative decoding with Adaptive dRaft Length) generates variable draft lengths and supports asynchronous execution through two new operations: pre-verify and post-verify. The authors evaluated PEARL across various tasks (e.g., HumanEval, GSM8k), observing significant speedups in performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The motivation is clear, with strong supporting evidence, such as in Figure 2.\n* Effective visualization of the PEARL algorithm, as shown in Figure 3.\n* Reduction in manual parameter tuning for gamma, which previously required significant effort in SD (Section 4.1), even if the estimation remains somewhat approximate.\n* Extensive evaluation across various tasks, including code generation, reasoning, and multi-round dialogue."
            },
            "weaknesses": {
                "value": "* The window size for each chunk (gamma) appears to remain fixed, which means the draft length will still be determined by the multiplication of gamma. While this may be unavoidable in the current algorithm, the abstract and introduction suggest that the authors intend to eliminate the need for gamma entirely.\n* Pre-verify with only a single token might not be the most reliable method. See below for a question.\n* While Section 5 presents a study on various datasets, the paper does not include a detailed analysis, such as step-by-step profiling or the failure rate in pre-verification."
            },
            "questions": {
                "value": "* The range of 1.50x to 4.43x represents a significant gap. Is there any analysis explaining the reasons for such large differences?\n* Considering that pre-verifying a single token may not be the most accurate method for estimating difficulty, can the authors provide empirical evidence or analysis on how effectively the pre-verification of the first token compares to that of other tokens? For example, could there be potential pitfalls when using a very large gamma?\n* Could the authors illustrate a few examples of profiling PEARL on real data, similar to Figure 3, but using actual data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an interesting speculative decoding (SPD) paradigm, PEARL, that attempts to overlap the drafting and verification stages in the standard SPD framework, thereby mitigating the so-called mutual waiting issues. As a training-free SPD method, PERAL achieves a state-of-the-art acceleration ratio with different pairs of LLMs on different domains by switching between pre-verify and post-verify stages to adjust the draft length dynamically."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The idea is novel to some extent: while the community has noticed that the drafting stage is the bottleneck of the current SPD system and has proposed works to either dynamically adjust the draft length or decode draft tokens in parallel, attempts to pre-verify and post-verity are innovative to lower the proportion of drafting latency in the SPD process.\n2. The illustrations are self-explanatory in Figure 3. \n3. PERAL eliminates the need to tune the drafting window size according to Section 4.1, a desirable property for other SPD frameworks."
            },
            "weaknesses": {
                "value": "1. Missing baselines: while comparison with SPD methods that require training (Medusa, EAGLE, etc.) is not expected, there is still a line of works that focus on training-free SPD, such as Self-Speculative [1], Parallel Decoding [2] and REST [3]. Adding these should be able to strengthen this submission, but please do not focus on this during the discussion stage.\n\nOther than these, I don\u2019t find obvious weaknesses in this submission. Please refer to the question section regarding my main concern.\n\n[1]: Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft & verify: Lossless large language model acceleration via self-speculative decoding.\n[2]: Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodol\u00e0. Accelerating transformer inference for translation via parallel decoding.\n[3]: Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, and Di He. REST: retrieval-based speculative decoding."
            },
            "questions": {
                "value": "I find the following concerns and would like further clarification from the authors. \n\nThe core idea behind SPD is to utilize hardware computation redundancy; therefore, running forward passes on drafter and target models simultaneously has to bring additional latencies for both the drafting and verification stages. I am glad to see the paper presents theoretical and empirical analysis, but none of them discussed this. Profiling the latency overhead brought by overlapping two stages could strengthen this paper. (I noticed Appendix E, an engineering technique to get around the resource competition issues, but still, some analysis is expected; let\u2019s say we don\u2019t have a multi-GPU environment to implement the PP solution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper describes and addresses the problem of \"mutual waiting\" in Speculative decoding (SD) for LLM inference acceleration, where typically, the target model and draft model are mutually blocked by each other since verification can only occur after drafting is completed and vice-versa (draft-then-verify). The authors propose a novel framework to address this mutual waiting problem called \"Parallel Speculative Decoding with Adaptive Draft Length\" (PEARL) that coordinates the drafting and verification steps partially in parallel by using \u201cpre-verify\u201d and \u201cpost-verify\u201d strategies for verifying the first draft token in advance during drafting and generating more drafts during verification, respectively. This can be thought of as a shift from the \u201cdraft-then-verify\u201d sequential paradigm into a more parallelized \u201cdraft-and-verify\u201d paradigm. In addition to the \u201cpre-verify\u201d and \u201cpost-verify\u201d strategies, the authors explore the use of an adaptive draft length to further reduce the mutual-waiting scenario where suboptimal draft lengths are used, whether too short or too long. Using these proposed strategies, the authors are able to showcase relative speedups on top of vanilla SD on a variety of text generation tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Authors clearly present their ideas and propose their PEARL framework with solid working examples and motivations. In particular, the analysis into the mutually blocked asynchronous execution of drafting and verification as well as optimal draft length per decode step were helpful in understanding the potential headroom for a parallelized framework of SD.\n- Experiments with PEARL yield a solid speed up of 1.5x over vanilla speculative decoding on common text generation tasks as well as in comparison to other baselines for the HumanEval code generation task\n- The paper examined each of the component strategies of PEARL, pre-verify, post-verify, and adaptive length, independently via ablation studies and analysis to isolate and highlight the relative impacts and relationships between each strategy\n- Authors provide code implementation for reproducibility"
            },
            "weaknesses": {
                "value": "- The main paper assumes execution scenarios where there are enough resources to run drafting and verification in parallel (e.g. multiple GPUs). However, in many instances, drafting and verification happen in co-located settings (e.g. single GPU) where it is more resource constrained. While the authors do make brief mentions in the main paper about these resource constrained scenarios, their discussion and strategies to address this are limited to a relatively short section in the appendix with not much details on their experimental results. It would be much more fitting to have this section filled with more detail and part of the main paper. For example, the strategy mentioned in the appendix involved copying the drafter model across multiple chips which incurs greater memory cost as well as potential communication cost in having to transport and sync intermediate attention KV caches.\n- Additionally, given the fact that drafting and verification is assumed to occur on separate devices in parallel, it would be good to see a mention into the communication overhead of data transfer (i.e. logits for adjusted sampling during verification rejection) as well as a breakdown into the additional computational and power consumption in practice from added drafting and verification calls that PEARL conducts in comparison to SD\n- While a variety of tasks (HumanEval, GSM8K & MGM, MT-bench) and baselines (SD, Ouroboros, Lookahead Decoding, Distillspec, Assisted Generation) were mentioned in the experiments, the variety of baselines were only used for the HumanEval code generation task while the remaining GSM8K & MGM and MT-bench tasks only used auto-regressive (AR) and SD baselines."
            },
            "questions": {
                "value": "- Is additional communication overhead costs incurred when running drafting and verification on separate accelerators? (e.g. logits transport from 2 devices for rejection sampling/verification)\n- There are 5 baselines listed but Table 2 and Table 3 only report Auto-regressive and SPEED?\n- What is the assisted generation baseline exactly? Does it adjust draft length depending on the number of tokens accepted in the previous iteration?\n- What value of gamma was used for the baselines on each task? Was it fixed according to the optimal gamma values determined for PEARL?\n- Why do Table 5 and Table 6 differ for HumanEval with gamma=5 for Llama2 7B&70B (40.72 in Table 5 and 30.34 in Table 6)?\n- Is there a cap on draft length given the fixed optimal verification window size? Something like 2x? Large gamma values aren\u2019t only detrimental to drafting phase but also incur additional computational cost in verification (verifying 4 tokens vs. 32 can be a significant difference) even for vanilla SD\n- Table 9 should clarify that it\u2019s not inference speed time but is the number of model runs? Perhaps good to report the ratio on the side (PEARL has ?x more model runs than SD)\n- Line 034: remove \u201cthe\u201d in \u201cthe natural language\u201d\n- Line 046-047: rewrite to \u201cdraft tokens that the original large model (referred as the target model) then verifies in parallel\u2026\u201d\n- Line 161: \u201cgenerating\u201d not \u201cgeneration\u201d\n- Line 199: \u201cthat stucks the target model\u201d ? Do you mean \u201cblocks\u201d?\n- Line 240 remove \u201chave\u201d\n- Line 331 \u201cbeing\u201d not \u201cbeen\u201d\n- Line 352: speed up ratio relative to baseline auto-regressive?\n- Line 410: Pearl without post-verify as Pearl w/o \u201cpre-verify\u201d\n- Line 412: \u201cexhibits a more pronounced\u201d\n- Line 691: \u201creject some\u201d not \u201csomeone\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors aim to address two challenges: 1. The mutual waiting problem, which arises when the target model becomes idle while waiting for the draft model to generate tokens, and vice versa. The asynchronous execution of the draft and verification phases leads to inefficiencies. 2. Fixed draft model length.\n\nThe authors introduced two strategies to solve this issue: 1. Pre-verification: This strategy involves using the target model to verify the first draft token during the drafting phase. By doing this, PEARL can determine whether the drafted token will likely be accepted or rejected.\nIf the first draft token is verified and accepted, the draft model can generate additional tokens more confidently. Conversely, if the first token is likely rejected, the draft model can generate fewer tokens, thus saving computational resources and time. \n\n2. Post-verification: In this phase, the draft model generates additional draft tokens while the target model verifies the previously drafted tokens. This allows for a more continuous flow of token generation and verification. By enabling the draft model to produce more tokens during the verification phase, PEARL capitalizes on situations where the target model is actively processing the earlier drafts. This strategy ensures that the draft model is not idle while waiting for the target model to complete its verification, thus maximizing throughput."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is clearly written and provides several contributions: \n\n1. PEARL allows the drafting and verification phases to occur simultaneously\n\n2. Instead of using a fixed draft length, PEARL adapts the number of draft tokens generated based on the context and complexity of the task. This flexibility ensures that the draft model generates an appropriate number of tokens, reducing unnecessary computations and improving the acceptance rate of tokens by the target model. This adaptability helps to optimize the inference process for different scenarios.\n\n3. PEARL theoretically demonstrates that it can achieve a higher mean number of accepted tokens compared to existing draft-then-verify methods. This means that more of the generated tokens are useful, leading to better performance overall"
            },
            "weaknesses": {
                "value": "The author claims that they can continue doing draft generation while doing the verification. This raises many questions:\n\n1. The latency of verifying a 70B model is about generating 2.5 tokens on the 8B model. Based on the speedup that the authors provided, this parallel approach would not work beyond a lookahead length of 3, which is shown to be suboptimal empirically. Therefore it is not clear how much this post-verify step improves the performance. \n\n2. Some parameters are underspecified in the tokens accepted per second table (Table). It is not clear under what lookahead length are the baseline numbers achieved, or if they are optimal.\n\n3. The authors picked pipeline parallelism for their implementation. However, while this is a convenient setup for solving the \"resource contention\" challenge, this is an unreasonable setting and introduces much higher latency in the first place. In deploying a 70B target model with a 7B draft model, using tensor parallelism (TP) can reduce the latency of the target model by leveraging more parallelism in each layer. Therefore, this casts doubt on all the speedup that the authors reported as this causes both the baseline and their reported results to be slower than in a TP setup. Also, with TP, the proposed solution to resource contention in Appendix E would not apply and it is not clear whether the authors can show a similar speedup with their algorithm.\n\n4. Furthermore, using PP instead of TP causes a higher activation memory footprint, reducing the effective batchsize the model can accommodate during decoding, effectively reducing the overall throughput."
            },
            "questions": {
                "value": "My concerns are raised above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}