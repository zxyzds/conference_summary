{
    "id": "qtqvuBmhxU",
    "title": "MONICA: Benchmarking on Long-tailed Medical Image Classification",
    "abstract": "Long-tailed learning is considered to be an extremely challenging problem in data imbalance learning. It aims to train well-generalized models from a large number of images that follow a long-tailed class distribution. In the medical field, many diagnostic imaging exams such as dermoscopy and chest radiography yield a long-tailed distribution of complex clinical findings. Recently, long-tailed learning in medical image analysis has garnered significant attention. However, the field currently lacks a unified, strictly formulated, and comprehensive benchmark, which often leads to unfair comparisons and inconclusive results. To help the community improve the evaluation and advance, we build a unified, well-structured codebase called Medical OpeN-source Long-taIled ClassifiCAtion (MONICA), which implements over 30 methods developed in relevant fields and evaluated on 12 long-tailed medical datasets covering 6 medical domains. Our work provides valuable practical guidance and insights for the field, offering detailed analysis and discussion on the effectiveness of individual components within the inbuilt state-of-the-art methodologies. We hope this codebase serves as a comprehensive and reproducible benchmark, encouraging further advancements in long-tailed medical image learning. The codebase will be publicly available on GitHub.",
    "keywords": [
        "Long-tailed Learning",
        "Benchmark",
        "Medical Image Classification"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "Benchmarking on Long-tailed Medical Image Classification",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=qtqvuBmhxU",
    "pdf_link": "https://openreview.net/pdf?id=qtqvuBmhxU",
    "comments": [
        {
            "summary": {
                "value": "The authors introduce the problem of long-tailed medical image classification and challenges in the field. Then they develop MONICA which is a package to benchmark various methods ranging from different loss functions, augmentations, etc on the benchmark datasets across medical image classification tasks. They provide an overview of datasets and methods and experiment results on the datasets. The authors additionally share learnings and observations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- A good overview of the datasets curated for this work\n- important contribution of decoupling the codebase\n- A good overview of the method approaches\n- practically useful to AI researchers in medical imaging"
            },
            "weaknesses": {
                "value": "- It would help to expand the benchmark datasets and bring in a canonical set for a field such as Camlyon for Pathology, etc. WILDS (medical subset) is a great example of a dataset to bring in to this benchmarking codebase\n- Resnet-50 is used as a backbone but the community has generally moved on to more complex backbones such as ConvNext / Swin or foundation model backbones for different datasets. \n- Generally the community uses pretrained backbones rather than training the backbones from the scratch.\n- The same backbone is used for every task for fairness but generally a sweep over backbones would help since different modalities and tasks require different approaches\n- Top-1 accuracy is an in appropriate metric for model selection in imbalances settings and AUROC, AUPRC, F1 should be used\n- error bars are missing in experiments\n- More thorough error analysis\n- Clearer articulation of novel insights\n- Better connection to clinical relevance\n- More detailed ablation studies"
            },
            "questions": {
                "value": "- Are their any key trends that you'll observed across the board to narrow down the design space for the future across the general task space? The results are not convincing in any one direction across the board on tasks and methods\n- Do you'll think stronger backbones can help learn better features?\n- Did you'll consider trying complex augmentation techniques such as AugMix or even learned augmentations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a unified benchmark for long-tailed learning in the medical domain by integrating several existing datasets and implementing a complete pipeline from data loading to model training and evaluation. The authors claim that the benchmark supports over 30 methods for comparison and provides an analysis of their performances."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper attempts to provide a comprehensive benchmark for long-tailed medical image classification. The idea of integrating multiple existing methods and datasets into a unified platform could potentially be useful for researchers who want to compare various methodologies under a standardized framework."
            },
            "weaknesses": {
                "value": "1. Motivation.\u00a0The paper lacks sufficient justification for evaluating long-tailed problems specifically in medical imaging tasks. While the authors mention some motivations at the beginning, these arguments are not convincing. Is there a fundamental difference between long-tailed problems in medical imaging and those in conventional tasks? Would this difference necessitate different methodologies? Even if the data modalities and evaluation methods are distinct (e.g., balanced vs. imbalanced test sets), would this lead to fundamentally different approaches? The paper analyzes multiple methods based on this premise but fails to provide insightful conclusions, which further deepens my skepticism about the motivation.\n\n2. Dataset Contribution.\u00a0Although the paper claims to use 12 datasets, 7 of these come from MedMNIST, and several of them are derived from previous work. This reduces the originality of the dataset contribution. Furthermore, the split between multi-class and multi-label datasets is 9/3, respectively. It is worth noting that many existing studies have already utilized MedMNIST for long-tailed learning (https://scholar.google.com/scholar?cites=11226954386823169312&scipsc=1&q=long+tail). Given that 7 out of the 12 datasets in this paper are from MedMNIST, why should users choose MONICA over MedMNIST, which already has extensive use and coverage in the medical imaging field? Additionally, the experimental methods used for multi-class and multi-label datasets are almost entirely different, and the analysis of multi-label results is limited to a single vague statement that multi-label classification is more challenging. This gives the impression that multi-label datasets were included just for the sake of completeness, rather than being a key focus.\n\n3. Code Contribution.\u00a0The code is not provided in the appendix, nor is there an anonymous GitHub link, which means the authors' claims about the code cannot be verified. By comparison, the NeurIPS D&B track (single-blind review) usually includes dataset or code links, along with information about author affiliations, licenses, and ethics. Although such links may be added after acceptance, this suggests that such work may not be well-suited for ICLR's double-blind review process.\n\n    Additionally, the description of the code structure in Section 3.1 is not particularly informative. The modular design described is basic and lacks novel insights. A more impactful modular design, like in mmdetection, which breaks down components into backbone, neck, and bbox head, would have been more meaningful. As it stands, the description feels unnecessary.\n\n4. Experimental Analysis Lacks of Insights. Comments below:\n\n    - Despite using multiple datasets, the authors only provide a generic / systematic comparison of the methods without analyzing differences across domains. For example, there is no discussion about which methods are better suited for dermatology versus ophthalmology. Almost all discussion is very general, without any specific insights related to medical applications. This diminishes the value of using 12 datasets, as the conclusions drawn are not substantially different from what could be obtained from a single dataset.\n\n    - The analysis in Section 4.2 is poorly organized. There is no clear structure, with the discussion jumping from evaluation metrics (e.g., \"Curse of shot-based group evaluation\") to re-sampling methods, MixUp, two-stage training, and even self-supervised learning in a seemingly random fashion. Many claims are also not supported by data. The overall takeaway from the experimental section is unclear, and I did not gain any insights on how to design better models.\n\n    - In Section 4.1, there is inconsistency in the training strategies used: some methods use a unified training strategy, while others use the one specified in the original paper (e.g., SAM, Line 306), with no explanation for this discrepancy.\n\n    - There are issues with the tables, such as Table 2, where it is unclear what methods like ERM, cRT, and LWS represent, as they are not referenced properly. Additionally, Section 3.2.3 does not fully align with the table.\n\n    - The categorization of methods is confusing. The authors categorize methods into three types\u2014class re-sampling, information augmentation, and module improvement\u2014but later mention that re-sampling and MixUp are used in many methods, making the classification in Tables 2/3 somewhat meaningless.\n\n    - The discussion on self-supervised learning (Line 398) appears out of place, as it is not introduced earlier.\n\n    - Similarly, the mention of OOD detection (Line 421) is abrupt and lacks context.\n\n    - The section on using an imbalanced validation dataset for checkpoint selection is unclear about its purpose. The conclusion seems to be that GCL exhibits lower fluctuations, but the reasoning and implications are not well explained. Additionally, Figure 4 lacks labels for the x and y axes, making interpretation difficult.\n\n    - Line 475 suddenly states that multi-label classification is more challenging without providing adequate context or analysis.\n\n    - Line 504 claims that \"the most advanced long-tailed learning methods no longer focus on improving a single strategy,\" but this claim is not well-supported by the preceding analysis."
            },
            "questions": {
                "value": "See weakness sections. Some more questions below:\n\nCould you elaborate on why the results of self-supervised learning and OOD detection are relevant in this paper? They seem out of place given the main focus on long-tailed classification.\n\nWhy did the authors not include a domain-specific analysis (e.g., which methods work better for certain medical fields)? It seems like an important missed opportunity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduced Medical OpeN-source Long-taIled ClassifiCAtion (MONICA), a comprehensive benchmark for long-tailed medical image classification (LTMIC). It includes a unified, well-structured codebase integrating over 30 methods developed in relevant\nfields and 12 long-tailed medical datasets covering 6 medical domains."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Long-tailed learning is an extremely challenging problem, this work can serves as a comprehensive and reproducible benchmark, encouraging further advancements in long-tailed medical image learning.\n\nIt covers most of the strategies that deal with long-tailed problems, and also include 12 datasets from different application domains."
            },
            "weaknesses": {
                "value": "This work doesn't introduce any new datasets or methods. It is a collection of datasets (multi class or multi label) that are already publicly available without justifications as they are many other such kind of long tail datasets available. Also, they have changed some of the original datasets, it would not be useful if they don't share the modified datasets.\n\nThey only tried ResNet for the tasks, would be nicer to make comparisons with other models. Also the discussions on SSL models seem not supported by any data."
            },
            "questions": {
                "value": "Some of the datasets have been changes in terms of distributions, would you share the modified datasets or the code on making the changes?\n\nWhat are the performance of the SSL models?\n\nThere are some typos such as quotation markers etc, please correct."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "All public datasets, no concerns."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a framework and codebase for structured benchmarking of long-tailed (LT) learning methods on various medical image classification tasks. The benchmark, MONICA, implements over 30 LT learning methods, with comprehensive experiments assessing performance across 12 LT medical image classification datasets spanning 6 different modalities. The experiments analyze which methods, and categories of methods, provide the most benefit to LT medical image classification across tasks in a controlled environment."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This work addresses an important problem, namely the variability in dataset/hyperparameters/etc. when evaluating LT learning methods for medical image classification tasks. These variations in setting make head-to-head comparisons difficult, so MONICA serves to provide a \u201cfair playing ground\u201d for these LT learning methods.\n- The framework will become publicly available and should serve as an extensible resource going forward for LT medical image classification research.\n- The organization and presentation quality of the paper is strong, with helpful use of formatting (typesetting, color, etc.) and high-quality figures.\n- Experiments are very thorough, spanning many relevant methods, datasets, and tasks.\n- Discussion is thoughtful, going beyond simply displaying all benchmark results. The authors try to synthesize takeaways, provide caveats/limitations, assess out-of-distribution performance, and more."
            },
            "weaknesses": {
                "value": "- Writing can be improved throughout. See specific comments below for examples of awkward wording, inconsistent naming, grammatical errors, etc.\n- It is possible that choosing a fixed set of hyperparameters across methods unintentionally advantages certain methods. Ideally, one could argue that each method should be individually tuned on each task; however, I am aware that this would require a vast amount of resources and time, so I do not consider this a major limitation. More practical solutions to enhance the benchmark would be the following: (i) uncertainty estimates should be provided (e.g., bootstrapped confidence intervals or standard deviations over multiple runs), and (ii) multiple performance metrics should be provided (e.g., AUROC)."
            },
            "questions": {
                "value": "- Is it possible that the chosen hyperparameters used across all methods happen to be more advantageous for certain methods and suboptimal for others? In one sense, using the same set of hyperparameters across methods appears \u201cfair\u201d; however, it may actually be more fair to individually tune each method on each task. I recognize the difficulty of conducting fair comparisons in such a large-scale experimental setting, where it is costly to, e.g., run multiple trials of all experiments. I am not asking the authors to necessarily perform such experiments, but rather to consider this point and perhaps comment on it as a limitation/consideration.\n- Can the authors provide a summary of practical suggestions for which methods to use in a few sentences near the Conclusion?\n- I might suggest including the **rank** of each method on a given task in all tables. This would also enable you to *quantitatively* assess method performance across tasks (which method has the lowest average/median rank overall?). To make this work logistically (fit all columns in the table), you may need to reduce the precision to one decimal place, e.g.\n- The two paragraphs \u201cLTMIC improves out-of-distribution detection\u201d and \u201cUsing imbalanced validation dataset for checkpoint selection\u201d are not properly set up. For the former, what does it mean to use \u201cImageNet as OOD samples, with 1,000 randomly selected images\u201d? What exactly is the task, how is it formulated, and how are experiments conducted? Further, why do we care about this model behavior? For the former, Figure 4 and its findings are confusing \u2013 why exactly does this demonstrate \u201cstable convergence\u201d? My general advice: **Use the methods section to describe and prepare the reader to understand everything that appears in the results**. When I come to these results sections, I should already have an idea of what experiments you have performed.\n\n**Minor comments/questions:**\n- Avoid editorializing with value judgments: \u201cbenchmark is **meticulously** designed\u201d; \u201cwe\u2026 develop a\u2026 **well-structured** codebase\u201d; \u201cour work provides **valuable** practical guidance\u201d. Simply present your work and let the reader make these judgments!\n- \u201cdata imbalance learning\u201d is not a phrase I have heard. Perhaps \u201cimbalanced learning\u201d?\n- \u201cunified, strictly formulated, and comprehensive benchmark\u201d. Unsure what \u201cstrictly formulated\u201d means. Could simply say \u201cunified, comprehensive benchmark\u201d\n- \u201cwe build a\u2026 codebase\u2026, which implements over 30 methods\u2026 and evaluated on 12\u2026 datasets\u201d. It seems that \u201cevaluated on\u201d is the wrong tense; also, what is being evaluated?\n- This does not belong in an abstract: \u201cWe hope this codebase serves as a comprehensive and reproducible benchmark, encouraging further advancements in long-tailed medical image learning.\u201d\n- Often unnecessary inclusion of \u201cthe\u201d before concepts: \u201cThe deep learning techniques\u201d; \u201cthe collected image datasets\u201d; \u201cthe long-tailed imbalance\u201d\n- \u201cThe deep learning techniques have proven effective for most computer vision tasks benefiting from the grown-up dataset scale.\u201d Remove \u201cThe\u201d; what does \u201cgrown-up dataset scale\u201d mean? \u201cGrown-up\u201d is not the right adjective \u2013 be more concrete.\n- Refrain from claims like \u201calways result\u201d (line 57) \u2013 soften to \u201cusually\u201d or similar\n- Confused by this justification: \u201cit is vital to recognize these rare diseases in real-world practice, as they are relatively rare for doctors and may also lack diagnostic capacity.\u201d This reads as \u201cit is vital to recognize rare diseases because they are rare\u201d.\n- Line 65: can change \u201ccontributions, i.e.,\u201d -> \u201ccontributions:\u201d\n- Be consistent with capitalization/presentation of terms: \u201cRe-sampling\u201d vs \u201cre-sampling\u201d; \u201cModule improvement\u201d vs. \u201cModule Improvement\u201d; \u201cmnist\u201d vs. \u201cMNIST\u201d; \u201cmixup\u201d vs. \u201cMixUp\u201d; etc.\n- Line 82: \u201cwe are still curious to explore\u201d. Perhaps just \u201cwe aim to explore\u201d?\n- \u201cThe partition schemes are vita important\u201d. What does \u201cvita\u201d mean?\n- The last two paragraphs of the introduction are probably better off being formatted as bulleted or numbered lists. Also, it is unclear why these numbered lists are formatted differently: **1) xxx.** vs. (1) xxx.\n- \u201cof class $k$ where $\\rho$ denoted as imbalance ratio\u201d. The phrase \u201cdenoted as\u201d is awkward + need a comma after $k$\n- \u201ca common assumption in long-tailed learning is when the classes are sorted by cardinality in decreasing order\u201d I\u2019m not sure what this means or why this represents an \u201cassumption\u201d. I would just remove this sentence since it does not seem to be used later.\n- Line 144: \u201cis a long-tailed version constructed from\u201d. Need to say it is a version \u201cof\u201d something; alternatively, use a word other than \u201cversion\u201d like \u201cdataset\u201d\n- Inconsistent spacing/use of commas in numbers. \u201c10, 015\u201d -> \u201c10,015\u201d; \u201c3200 fundus images\u201d -> \u201c3,200 fundus images\u201d\n- Inconsistent spacing around commas and colons: \u201ctraining/off-site testing / on-site testin\u201d; \u201c7 : 1: 2\u201d; etc.\n- \u201cLiver Tumor Segmentation Benchmark\u201d -> \u201cthe Liver Tumor Segmentation Benchmark\u201d\n- I realize it is hard to categorize some methods into one bin but GCL loss going in Information Augmentation is interesting, particularly since all other losses fall under re-sampling. It seems to also have module improvement as well.\n- \u201cCausal classifier (Tang et al., 2020) resorted to causal inference for keeping the good and removing the bad momentum causal effects in long-tailed learning.\u201d The phrase \u201cresorted to\u201d is strange and has a negative connotation; also, what do \u201cgood\u201d and \u201cbad\u201d mean?\n- \u201cAll these designs are for the fairness and the practicality of the comparison on the benchmark.\u201d Too vague \u2013 in what specific way do these support fairness?\n- Table 3: Inconsistent \u201cAvg\u201d vs \u201cAvg.\u201d vs \u201cavg\u201d\n- Table 4: Consider using a line break occasionally (so one loss function occupies two rows). This would allow you to use a larger font size. Also, be consistent \u201cCrossEntropy\u201d vs \u201cCE\u201d?\n- \u201cassessing MixUp based solely on performance is not fair\u201d. Soften to \u201cmay not be fair\u201d\n- \u201cled to a significant performance decline, e.g,\u201d. Refrain from saying \u201csignificant\u201d without statistical significance test + change \u201ce.g,\u201d -> \u201ce.g.,\u201d\n- \u201cUse two-stage training as a general paradigm\u201d sounds like a command. Perhaps \u201cUsing\u201d?\n- Define \u201cSSL\u201d acronym at first use\n- \u201cModify classifier to reduce prediction bias\u201d -> \u201cClassifier modification to reduce prediction bias\u201d\n- \u201cIn Fig. 2, We visualize\u201d -> \u201cIn Fig. 2, we visualize\u201d\n- Table 5 indicates the meaning of asterisk, which is never used in the table.\n- \u201cmodels with larger parameters\u201d. The parameters are not \u201clarger\u201d \u2013 could say \u201cmore parameters\u201d or \u201ca larger parameter count\u201d perhaps."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}