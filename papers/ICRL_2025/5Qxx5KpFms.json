{
    "id": "5Qxx5KpFms",
    "title": "Breaking Neural Network Scaling Laws with Modularity",
    "abstract": "Modular neural networks outperform nonmodular neural networks on tasks ranging from visual question answering to robotics. These performance improvements are thought to be due to modular networks' superior ability to model the compositional and combinatorial structure of real-world problems. However, a theoretical explanation of how modularity improves generalizability, and how to leverage task modularity while training networks remains elusive. Using recent theoretical progress in explaining neural network generalization, we investigate how the amount of training data required to generalize on a task varies with the intrinsic dimensionality of a task's input. We show theoretically that when applied to modularly structured tasks, while nonmodular networks require an exponential number of samples with task dimensionality, modular networks' sample complexity is independent of task dimensionality: modular networks can generalize in high dimensions. We then develop a novel learning rule for modular networks to exploit this advantage and empirically show the improved generalization of the rule, both in- and out-of-distribution, on high-dimensional, modular tasks.",
    "keywords": [
        "scaling laws",
        "modularity",
        "neural network",
        "generalization",
        "compositionality",
        "combinatorial generalization"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We show theoretically that modular neural networks trained on modular tasks can generalize to high-dimensional tasks with a fixed number of training points; we propose a learning rule to exploit this advantage empirically.",
    "creation_date": "2024-09-14",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=5Qxx5KpFms",
    "pdf_link": "https://openreview.net/pdf?id=5Qxx5KpFms",
    "comments": [
        {
            "comment": {
                "value": "We really appreciate your positive feedback on our submission and your suggested directions for improvement.\n\n**Fixed compute budget**\n\nThis is an important question of great practical relevance.\nWe have not tried any experiments in which the compute budget for the randomly initialized vs. pre-initialized networks are fixed. In fact, in this work, we do not at all optimize our kernel-based modular learning rule for computational efficiency.\n\nHowever, we note that approximating kernel methods for computational efficiency is a well-established field of research which can reduce the computational cost of kernel methods down to linear time in the size of a dataset (such as using random Fourier features). We expect that combining these methods with our proposal can help make our method significantly more competitive with random initialization under a fixed compute budget.\n\n**Mismatch between architectural module count and task module count**\n\nThis is an insightful point.\n\nIn our experiments, in general, the number of architectural modules is always greater than the number of modules in the task: thus, *these do not need to match*. For example, in the case of Compositional CIFAR-10, we fix the number of architectural modules to 32 and vary the number of task modules from 1 to 8. Figure 10 shows an additional experiment demonstrating the effect of choosing different numbers of architectural modules on the sine wave regression task. We find that adding more architectural modules helps performance even beyond the number of task modules.\n\n**Relevance to modern architectures**\n\nWe appreciate this point as well. In section 5, we have expanded our discussion of how our results apply to modern architectures.\n\nIn terms of practical relevance, we believe our results show that when training modular architectures, more emphasis should be placed on the optimization methods used to train the modules. Naively applying gradient descent may not be sufficient for effective training of these architectures. Our results also suggest a potential theoretical basis for why modularity is so effective in practice: namely, modularity breaks up a large high-dimensional task into easier to learn low-dimensional tasks."
            }
        },
        {
            "comment": {
                "value": "Thank you for your detailed review and for highlighting areas where our work can be strengthened. We address your concerns point by point below.\n\n**Novelty of theoretical results**\n\nWe understand that projecting high-dimensional data into lower-dimensional subspaces is a well-known technique. However, our contribution lies in providing a *quantitative analysis* of how modular architectures affect sample complexity in neural networks.\n\n\nOur work is the first to derive explicit, non-asymptotic expressions quantifying how modular architectures can circumvent the exponential sample complexity associated with high-dimensional inputs. This provides a theoretical foundation for designing modular networks that are efficient in practice. \n\nWhile our analysis assumes linearity and specific modular structures, these simplifications are essential for analytical tractability. Future work can build upon our framework to explore more general, nonlinear architectures and investigate how modularity affects their generalization properties.\n\n**Scalability of the proposed algorithm**\n\nYou raise a valid concern regarding scalability.\n\nWe note that approximating kernel methods for computational efficiency is a well-established field of research which can reduce the computational cost of kernel methods down to linear time in the size of a dataset (such as using random Fourier features). We do not explore these methods in this work as it is orthogonal to our contributions; however, we believe combining these well-known methods with our proposal is a fruitful direction.\n\n**Potential ensemble effects**\n\nThis is an interesting point.\n\nIn our compositional CIFAR-10 experiments, however, the modules all have the same weights (they are weight tied). Thus, the improved performance of the modular architecture cannot be attributed to an ensemble effect in this case.\n\n**Overclaiming and empirical comparisons**\n\nWe apologize if any claims appeared overstated.\n\nWhile our theoretical model captures the general trends observed empirically, certain deviations occur due to factors not accounted for in the simplified model, such as optimization dynamics. We discuss these factors in Section 3.3.\n\nWe are happy to soften any specific claims in our submission to match our empirical results.\n\n**Comparison to benchmark results**\n\nWe emphasize that Jarvis et al. tests on Compositional MNIST while we test on Compositional CIFAR-10, a significantly harder task. Thus, their performance metrics cannot be compared with ours.\n\n**Confusion about test error increasing with larger datasets**\n\nWe understand the confusion.\n\nIn Figure 1, the increase in test error with more data reflects the double descent phenomenon (Belkin et al.). This is a now well-established phenomenon in which test error increases with the number of training points until the interpolation threshold is reached (training points = number of parameters). Beyond this point, the test error decreases with the number of training points; this is also clearly illustrated in Figure 1. We are happy to provide further references on this phenomenon if helpful.\n\n\n**Independence of $y_j$**\n\nIn equation 3, we wish to rescale the summation such that the left hand side has constant magnitude as the number of modules varies. As a heuristic, if we treat each of the terms in the summation as independent under the assumption that $x$ is a random variable, we then find that the variance of the summation scales with the number of modules $K$. Thus, it is natural to divide the summation by $\\sqrt{K}$ to normalize.\n\n**Equation 4 clarification**\n\nThat's correct: as mentioned in the text before and after equation 4, we make a linearizing assumption on the parameters of the model. We assume that the model output is a linear function of the model parameters (which now include both the module parameters and the module input projection). This is, in fact, the same assumption made in Section 3.\n\n**Equation 15 clarification**\n\nEquation 15 simply solves equation 14 for $\\theta$. Note that equation 14 is a linear equation in $\\theta$. It is well known that the minimum norm solution for a variable in a linear equation (if a solution exists) can be computed by the pseudoinverse (denoted by $\\dagger$ in our notation): the solution to $Y=X \\theta$ minimizing $||\\theta||$ is $\\theta = X^\\dagger Y$. We are happy to provide further references if helpful.\n\n**Mixture of Experts paper reference**\n\nThank you for pointing this out. We have fixed this in our latest revision."
            }
        },
        {
            "comment": {
                "value": "We really appreciate your thoughtful and positive feedback on our submission. We address the points you raised below:\n\n**How to extend to setting with hierarchical modules**\n\nWe think this is an excellent point: indeed, our paper only considers a relatively simple form of modularity in which we use a single layer of modules and the module outputs are linearly combined to form the model output. As we note in the discussion section, practical modular architectures are often more complex, involving some level of hierarchy. We believe our results are a step in the direction of more theoretical analysis of how modularity can benefit generalization more broadly.\n\n\nWe do note however, that our experimental results include the case of nonlinear module projections (in which the module inputs are nonlinear projections of the task inputs). This is a step towards generalizing our results to more practical settings.\n\n**Concerns on CIFAR-10 experiment**\n\nThis is a nice suggestion. As we note in Appendix E though, given the very large number of possible class permutations ($10^k$ where $k$ is the number of images), we expect that for large $k$, all class permutations in the test data will be unseen. However, in Table 3 we explicitly test accuracies in the case where test inputs have distinct class combinations and find similar results.\n\nFor this task, we opt to use accuracy (fixing training samples) instead of sample complexity (fixing accuracy) as our performance metric since it is more natural to treat the Compositional CIFAR-10 dataset as unlimited (given the combinatorial number of samples that can be drawn). \n\n**Comments on tuning**\n\nOur experiments involve a number of hyperparameters. Unless otherwise mentioned, we set the hyperparameters to the most natural choice without tuning them. For certain hyperparameters, we do sweep over different values, with the sweep range indicated in Appendix E. We are happy to clarify any specific hyperparameter choices if needed.\n\n**Experiment with no input projection**\n\nWe appreciate this interesting suggestion. We note however, that in our Compositional CIFAR-10 experiment, all modules have the same weights (they are weight-tied). Thus, if they all had the same inputs, their outputs would also be the same. The final model output is the concatenation of the module outputs which in this case would perform very poorly.\n\n**Comment on validation set**\n\nFor both the Compositional CIFAR-10 and sine wave regression tasks, we use a separate validation set. Note that for the sine wave regression task, each input is drawn completely independently from an infinite sized dataset; thus, any points not trained on can be treated as validation data.\n\n**Number of architectural modules**\n\nWe treat the number of architectural modules as a (potentially tunable) architectural characteristic. It is fixed (at 32) for the Compositional CIFAR-10 task and tuned over for the sine wave regression task. Figure 10 illustrates performance for different choices of number of architectural modules for the sine wave regression ask.\n\n**Number of epochs for Compositional CIFAR-10 task**\n\nIn this task, the number of possible training inputs can be treated as virtually infinite (for large $k$). Thus, we believe it is more practically reasonable to fix number of epochs as one rather than repeat training on the same input multiple times.\n\n**Comment on equation references**\n\nThank you for pointing this out. We have fixed this in our latest revision."
            }
        },
        {
            "comment": {
                "value": "Thank you for your thoughtful review and for highlighting areas where our paper can be improved. We appreciate your insights and address your concerns point by point below.\n\n**Confusion with notation and explanation of the feature matrix**\n\nWe apologize for the confusion caused by the notation in our theoretical model, especially regarding the feature matrix. In our model, the feature matrix arises from applying a feature mapping to the input data, transforming each input $x$ lying in $m$ dimensions into a higher-dimensional feature matrix $\\phi(x)$ lying in $d \\times P$ dimensions. Here, $d$ denotes the number of output dimensions of the model and $P$ denotes the number of features per output dimension. We do not make any particular assumptions about exactly how $\\phi$ is constructed (it can be arbitrarily nonlinear and complex)- the main property it must satisfy is that the features $\\phi(x)$ are distributed as a Gaussian.\n\nThe reason why we use a feature matrix instead of a flattened feature vector is that it allows us to easily account for non-scalar output sizes ($d > 1$) with a fixed number of parameters $P$. Another option to account for multi-dimensional outputs is to have $d \\times P$ parameters (one set of parameters per each output dimension) while the number of features is fixed at $P$. This is also valid; however, our choice is more consistent with some prior literature (e.g. Jacot et al.).\n\n\nWe are happy to revise any particular points of confusion in our revision.\n\n**Clarity of expected training and test loss formulas**\n\nWe clarify the forms in Theorem 1 as follows: the test loss consists of three terms. The first term is largest near the interpolation threshold ($dn \\approx p$) and decreases away from it- this causes a spike near the interpolation threshold. Intuitively, this corresponds to overfitting to noise in the training data. The second term is negative and corresponds to the reduction in loss caused by capturing information about the true model. In the overparameterized regime, it linearly grows in magnitude with the number of training points, and in the underparameterized regime it is constant with amount of training data (as information in more training points can't be captured by the model). The last term is a constant offset corresponding to the loss when the number of parameters approaches infinity.\n\nThe training loss is a product of two terms: the first corresponds to the information about the underlying target function not captured by the model and the second corresponds to the amount of training data available that can't be captured by the model. If we have more parameters than training data, then the training loss is zero since all the information can be captured. Otherwise, there will be information loss proportional to the excess training data times the remaining information about the target function.\n\nWe are happy to revise any particular points of confusion in our revision.\n\n**Does modular data alone help without a modular architecture?**\n\nThe reviewer raises an interesting question about whether it is possible to benefit from merely modular data without a modular architecture. Our experiments on Compositional CIFAR-10 (see Figure 4) suggest the answer is no: a non-modular architecture performs relatively worse on higher-dimensional modular tasks than a modular architecture. Of course, it is possible that the non-modular architecture has learned the modular structure of the task to some extent (just not as well as a modular architecture) and we think this is an interesting future direction to explore.\n\n**Definition of modularity used**\n\nWe appreciate the need for a clear definition of modularity.\n\nIn this paper, we define modularity as the decomposition of a task or function into distinct, independently operating components or modules. Each module processes a subset of the input dimensions and contributes to the final output in a compositional manner. This structure allows for specialized processing within modules and recombination to solve complex tasks. Our modular neural network architecture mirrors this by having separate subnetworks (modules) that handle specific input projections.\n\nWe also highlight that in other literature, modularity may be used in other, potentially different ways.\n\n**Dependency on correct number of modules**\n\nThis is an insightful question.\n\nIn our experiments, in general, the number of architectural modules is always greater than the number of modules in the task: thus, *these do not need to match*. For example, in the case of Compositional CIFAR-10, we fix the number of architectural modules to 32 and vary the number of task modules from 1 to 8. Figure 10 shows an additional experiment demonstrating the effect of choosing different numbers of architectural modules on the sine wave regression task. We find that adding more architectural modules helps performance even beyond the number of task modules."
            }
        },
        {
            "summary": {
                "value": "This paper (1) constructs a simplified theoretical model of generalization (focused on the case of linear regression from what I believe to be a set of not-strictly-defined features), (2) provides empirical demonstrations that, at least in broad strokes, a sine wave regression task follows the predictions made by that model, (3) argues that in cases where the simple problem under consideration is modular (here meaning that it has k modules of size b which interact, rather than a full set of P features), better sample complexity can be obtained by using an explicitly modular parameter structure, and (4) provides some empirical support for this generalization behavior on another set of (this time modular) sine wave regression tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper focuses on an empirical task that lets them validate their ideas, but is realistic in terms of scope \n- The paper engages with an interesting problem of the structure of weights that allow for better generalization"
            },
            "weaknesses": {
                "value": "- The paper's notation, particularly in the initial presentation of the theoretical model, was confusing and felt under-explained. In particular I was confused by feature matrix (how did it get constructed from the inputs? What assumptions are being made about it? Why is it a matrix to begin with rather than a feature vector), and this confusion made it hard to understand future claims made in the paper (especially since the central claim was about the effect on generalization of _input_ dimension, which is mediated by the function implied in this feature matrix)\n- The forms of the expected training and test loss could have been broken down in a clearer and more intuitive way, rather than simply being presented as not-very-comprehensible formulas \n- This paper assumes that the only way to benefit from the generalization behavior of modularity is to have explicitly modular structure; it would have been interesting if it had also engaged with whether modular data gives you generalization benefits without a parallel parameter structure (since in practice modern models seem to generalize well without the benefit of this)"
            },
            "questions": {
                "value": "- What is the explicit definition of modularity being used? This concept was referenced without ever being really explicitly defined in a general-but-still-technical sense (and attention was given as an example). I ended up being confused about whether the focus was on independently functioning parts of a network, or shared weights in a more general sense .\n- As mentioned in \"Weaknesses\": what assumptions are made in general about the structure of the feature matrix? It is indicated in the modular version of the model that arbitrary nonlinear transforms of the input are considered as valid feature matrices, but this isn't clarified for the first treatment of the model \n- Do the benefits cited require being correct about the number (k) of modules in the underlying data? How much do positive results depend on being correct in your choice of k relative to what is present in the underlying data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper shows that the sample complexity associated with training modular neural networks is independent (under certain conditions) of the input dimensionality and does not follow the same exponential increase with input dimension as in the case of monolithic or traditional neural networks (NNs). \n\nFirst, the authors present a derivation of training and test error in monolithic NNs when the task is non-modular. The task and the NN are modeled linearly based on features generated using the input. The authors then, empirically validate that NNs with different architectures (loosely) match the theoretical trends when varying the input dimension, number of samples and the number of parameters. Note that the task considered for this experiment is modular. \n\nThe authors then theoretically compute the training and generalization errors of modular NNs given that the underlying task is also modular with the same structure. Each module is associated with a small NN (modeled linearly) and an input projection mechanism that reduces the module input dimensionality. The task is also modeled in the same way where the parameters are randomly initialized. \n\nThe resulting closed form solution shows that the training error is independent of the input dimension and the test error under the condition of under-parametrization is also independent of the input dimension. This result hinges on the input projection associated with each module, where the dimensionality associated with the overall input is reduced. The authors then propose a method to initialize (or learn) the parameters associated with the input projections. Once initialized the modular NNs are trained end-to-end. \n\nEmpirical results show that modular NNs that are learned using the proposed initialization mechanism achieve significant improvements over monolithic NNs and modular NNs conventionally trained."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The theoretical derivations are sound and very well done, and the paper is well written. The authors did a good job showcasing how modular networks can be related to monolithic networks (based on certain linear assumptions).\n\nIntuitively, modeling of the data, and learning of parameters related to module input bottlenecks make sense. This is similar to learning the connectivity associated with module input or limiting the number of module inputs to avoid module collapse. \n\nThe experiments show a clear trend that the input projection mechanism results in better performance and sample complexity, as compared to monolithic NNs and end-to-end trained modular NNs."
            },
            "weaknesses": {
                "value": "The major weakness of the paper is the consideration of a single layer of modules and data generating system. In such a system the output is a linear composition of the module outputs. This may not be true for many real world systems where multiple such modular layers can exist in a hierarchy."
            },
            "questions": {
                "value": "Continuing with the previous weakness, the algorithm to learn or initialize the input projection parameters may not work in such a case as it is dependent on the initial module NN weights. \n\nThe generalization performance for the compositional CIFAR-10 experiment can be divided into input class permutations present in the training data vs. not present in the training data to further dissect the difference between monolithic NNs and modular NNs. The sample complexity experiments with compositional CIFAR-10 tasks are not present and should be added to further strengthen the claims. \n\nFor individual tasks considered, there appears to be a large amount of tuning of methodology to train the modular NNs and the module input projections. (Referring to appendix)\n\nHow would the solution to training and test errors change if the input projections from each module were removed, and the modules considered the input x in its entirety? This is consistent with the current mixture-of-exert (MoE) models. \n \nIs there a validation set used for the experiments or is the generalization performance reported from the last training iteration?\n\nDo the modular NNs treat the number of modules as a hyper-parameter and tune it to improve the performance. Or is it an architectural characteristic such as the width or depth in monolithic NNs that is fixed ? \n\nThe CIFAR-10 experiments are run only for a single epoch, will increasing the number of epochs result in better performance for networks? \n\n\nMinor: Equations are referred to the appendix when they are also present in the main part of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors present both theoretical and algorithmic results regarding the generalization ability of a particular form of modular architectures. They first highlight theoretical results on generalization results showing exponentially large sample complexity as a function of the input dimension. Then they present a particular class of modular architectures as a sum of experts and assume that the training data was generated by this same architecture. They show that thanks to each module making a low-dimensional projection before processing the data, the scaling behavior is better behaved. Then they present a kernel-style algorithm to initialize such an architecture, to be then fine-tuned by usual supervised learning and SGD. Finally, they show results on a toy 1-dim sine-wave regression task and on the recently introduced compositional MNIST task."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Much remains to be understood about the generalization behavior of neural nets, especially the types that have a modular architecture, so advances on the theory (in special cases) that are presented do seem useful."
            },
            "weaknesses": {
                "value": "(1) the theoretical results are not suprising: projecting the m-dim input to several b-dim low-dimensional representations unsurprisingly reduces the exponential badness from m to b. The theory is also of fairly limited scope, with lots of unreasonable assumptions (e.g., of linearity wrt parameters) that may not tell us as much as we would like for more general forms of modular architectures.\n\n(2) the proposed algorithm is unlikely to scale well in terms of computational efficiency beyond small-size problems and into frontier AI, given the use of kernel methods in the novel part of the method\n\n(3) the fact that all modules are initialized independently and using the same (randomized) procedure suggests that a significant part of the advantage could come from an ensemble effect (which always helps generalization)\n\n(4) the paper seems to overclaim in multiple places, e.g., suggesting that their results tracks empirical behavior of modern neural nets (even the empirical comparisons don't match the theory, e.g., fig 2 bottom right).\n\n(5) I did not find numerical comparisons against benchmark results from other papers, and when I look at Jarvis et al 2023, their figures show much lower errors. Hence the experimental results may not be that good after all."
            },
            "questions": {
                "value": "(1) I was confused by the results in figure 1, whereby test error INCREASES with larger datasets. This seems incompatible with empirical observartions and traditional statistical analyses of generalization.\n\n(2) I did not understand in what sense the y_j could be considered independent (and what is the random variable), after eq 3.\n\n(3) eqn 4 seems wrong: on the LHS y is a function of the linear projection U x, whereas on the RHS U and x only interact via the presumably non-linear function phi.\n\n(4) why should we expect eqn 15 to give the minimum norm solution? (i.e. why is it a solution and why is it minimum norm, from what class)\n\n(5) You should add the citation of the original mixture of expert papers (e.g. Jacobs et al 1991)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors study modular neural networks and their ability to learn functions that are themselves modular insofar as they have either a compositional nature or a modular composition in their statistics. The authors first provide a theorem outlining expected scaling laws for sample complaxitiy and trainign accuracy in a curated linear task setup. They then corroborate the predictions from this theorem with numerical experiments. Following this, they present a task setup with explicit ground-truth modular structure and prove a second theorem outlining similar sclaing propoerties for this novel tasks setup along with a particual modular NN architecture. The result is reduced scaling complexity for tusch tasks when NNs are appropriately modular. The authors then propose an initialization scheme for NNs which first learn module initialization in a self-supervised fashion using task statistics. Finally, the authors show that this approach behaves as expected on non-trivial tasks such as compositional CIFAR, and that the proposed method works well on other modular architectures beyond the one used for their theoretical results."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper presents an excellent set of theoretical results outlining expected scaling laws for modular networks when task have a modular structure. It also presents a practical initialization scheme for potential models that is based on self-supervised alignment with task statistics. Lastly, it validates predictions with non-trivial and relevant experiments with architectures that go beyond the ones used for theory, outlining the potential generality of the result."
            },
            "weaknesses": {
                "value": "The paper is sound. There are potntials for improvements in two key areas:\n\n1. In experiments, it is unclear if the generalization advanatage of the modular networks remain if one factors in the pre-training (i.e. learning module initialization). In other words, for the same total compute, would a monolithic model do as well as the modular one for which some of the compute budget went toward initialization? My apologies if I missed it, but this is a key point that would factor into real-world scaling laws.\n\n2. What happens if there is a missmatch between the task modularity and the NNm odule count? In experiments, the models could accurately recover tasks modularity when the number of modules is known. In contrast, in other tasks, this is not known a priori. How would the models behave in this mismatched environment? To be fair, the authors acknowledge this issue, but I wonder if some rapid experiments could outline expected behaviors in this case. Once again, apologies if this has been explored and I missed it.\n\n3. While this is no doubt very relevant for the field, it's overall impact with respect to modern architectures remains unclear. Some discussion about the use of such methods in modern settings such as in the prsence of attention could greatly enhance the scope of the result. This is not necessary however, as this is a good paper in the current scope."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}