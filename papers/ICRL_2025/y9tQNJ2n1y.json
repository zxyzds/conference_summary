{
    "id": "y9tQNJ2n1y",
    "title": "CASE-Bench: Context-Aware Safety Evaluation Benchmark for Large Language Models",
    "abstract": "Aligning large language models (LLMs) with human values is essential for their safe deployment and widespread adoption. Current LLM safety benchmarks often focus solely on the refusal of individual problematic queries, which overlooks the importance of the context where the query occurs and may cause undesired refusal of queries under safe contexts that diminish user experience. Addressing this gap, we introduce CASE-Bench, a Context-Aware Safety Evaluation Benchmark that integrates context into safety assessments of LLMs. CASE-Bench assigns distinct, formally described contexts to categorized queries based on Contextual Integrity theory. Additionally, in contrast to previous studies which mainly rely on majority voting from just a few annotators, we recruited a sufficient number of annotators necessary to ensure the detection of statistically significant differences among the experimental conditions based on power analysis. Our extensive analysis using CASE-Bench on various open-source and commercial LLMs reveals a substantial and significant influence of context on human judgments ($p<$0.0001 from a z-test), underscoring the necessity of context in safety evaluations. We also identify notable mismatches between human judgments and LLM responses, particularly in commercial models within safe contexts. Code and data used in the paper are available at https://anonymous.4open.science/r/CASEBench-D5DB.",
    "keywords": [
        "safety",
        "benchmark",
        "context",
        "large language model",
        "contextual integrity"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "A context-aware safety evaluation benchmark for safety evaluation that considers context.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=y9tQNJ2n1y",
    "pdf_link": "https://openreview.net/pdf?id=y9tQNJ2n1y",
    "comments": [
        {
            "summary": {
                "value": "- The paper develops a benchmark for evaluating LLMs\u2019 refusal accuracy for problematic queries. The key point of differentiation is that the authors focus on \u201ccontext.\u201d This is the idea that auxiliary information might change whether or not an LLM should refuse to answer a query.  \n- The paper evaluates a range of LLMs on this benchmark, and finds that LLMs are generally over-moderated.\n- They also conduct an extensive survey of individuals to measure how context influences perceptions regarding when LLMs should refuse to answer."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Overall the paper is extremely well-written and clear.\n- The core idea tackled by the paper\u2013understanding how context might (or should) affect answer/decline decisions by LLMs\u2013is interesting.\n- The study methodology is extremely rigorous. \n- The experiments are also well-done and interesting."
            },
            "weaknesses": {
                "value": "- The paper tries to ground in the formal framework offered by contextual integrity (CI) theory. Section 2 and 3.1 spend time discussing CI and mapping it to LLMs. But its not obvious why CI is helpful here or the extent to which CI is even used in the construction of the benchmark? As best I can tell, CI is explained in the prompt used to generate conversational contexts. But it isn\u2019t clear whether that\u2019s necessary, or whether GPT-4o is even using that information. And the paper also mentions that all contexts were revised by the authors themselves.\n- The primary contribution (from a data perspective) would seem to be the different contexts the authors define\u2013especially as the queries themselves come from existing work. The paper doesn\u2019t really discuss what these contexts look like, how they were crafted, or how often they repeat across the different queries?"
            },
            "questions": {
                "value": "- Do the authors have any information on the identities/demographics of their human annotators? Ignoring settings where LLMs should decline to respond because of legal liability, the choice of whether an LLM should respond/decline seems entirely subjective. Different individuals will have different assessments, because of their personal politics, religious beliefs, cultural backgrounds, and more. So largely, this benchmark would seem to measure acceptance/decline accuracies grounded against the specific human pool the authors surveyed. Having more information about this pool would be helpful!   \n- The underlying message of the paper is that context matters in determining whether an LLM should respond to a query. For instance, an LLM intended to train medical professionals should probably respond to medical questions. Why is it fair, then, to evaluate LLMs like GPT-4o-mini or Claude, which are intended to be used in the \u201clayperson chatbot\u201d context? I\u2019m not sure I see why merely providing a \u201ccontext\u201d to these chatbots should actually alter their behavior. Given that a prevailing concern with these models is that users might employ deception to elicit responses from the models that developers do not desire (i.e., via jailbreaking or prompt-engineering)\u2013shouldn\u2019t these LLMs also decline to respond when the context is provided? The argument that these LLMs should respond when context is provided presumes that users will always be truthful when providing information in the prompt. \n- Could the authors share more information on the contexts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper puts forth CASE-Bench, a Context-Aware Safety Evaluation Benchmark which integrates context into subsequent safety assessments of LLMs. The benchmark boasts 900 distinct contexts with queries from 45 different categories."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- extensive benchmark collection, with 900 distinct contexts with queries from 45 different categories.\n- the inheritance of contextual integrity theory is interesting and well motivated\n- over 47K human annotations from over 2K annotators, resulting in a high quality dataset"
            },
            "weaknesses": {
                "value": "- the data was created with gpt-4o, bringing into question the potential for commercial use\n- explanation of limitations is lacking, would be good to delve into potential areas for improvement or future work"
            },
            "questions": {
                "value": "1. How reproducible is the automatic content generation with an open source model? Have you tried this?\n2. Manual revision process appears to be expensive. Were there attempt to automate this? If so, with what level of success?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces CASE-Bench, a context-aware benchmark for evaluating LLM safety. The main innovation is incorporating context into safety assessments using Contextual Integrity theory. The study validates the significant impact of context on safety judgments through large-scale human annotation (2000+ annotators) and evaluates several open-source and commercial LLMs on this benchmark."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper identifies a key issue in current LLM safety evaluations: too much focus on rejecting single queries without considering context, which can lead to LLMs over-rejecting legitimate requests.\n2. The research methodology is solid, using Contextual Integrity theory to formalize context, power analysis to determine annotator numbers, and various statistical methods to verify result significance.\n3. The dataset creation process is thorough, with 900 query-context pairs, 21 annotators per task, and robust quality control measures."
            },
            "weaknesses": {
                "value": "1. I wonder how the authors address this potential issue: many current jailbreaking techniques [1,2] actually work by creating \"safe-looking\" contexts to trick LLMs into generating dangerous content. Could context-based safety evaluation create new security risks?\n2. Regarding metrics, the paper mainly uses basic measures like accuracy and recall. Have the authors considered adding more detailed analysis, like measuring the impact of different types of errors?\n3. From a practical standpoint, I'm curious about the authors' thoughts on how to integrate this context-based evaluation approach into real-world LLM safety mechanisms?\n\n[1] Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation\n[2] Quack: Automatic Jailbreaking Large Language Models via Role-playing"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the problem of understanding whether a prompt/prompt reply is problematic or not given a particular context. It proposes that not all responses are problematic and some responses are problematic only if they are in a particular context. As such, it creates a dataset on 45 different categories, all with positive and problematic context. The dataset is used to evaluate state of the art models and also alignment/agreement with human annotators."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "A thorough analysis of the influence of context in deciding whether a prompt/prompt response is problematic\nA dataset that may be useful for other research"
            },
            "weaknesses": {
                "value": "I'm personally not convinced that the context should dictate the perceived harm in a prompt/prompt response. A malicious actor could always fake a positive intent/context to get the information they want and create harm. As such, I find the premise/motivation of the work weak."
            },
            "questions": {
                "value": "1. In which use-cases you find the differentiation by context important/useful?\n2. How can malicious actors that fake positive intent could be detected?\n3. In the results in Fig. 3, if I understand correctly, some unsafe contexts lead some categories to rated by humans as safe. That's unexpected and counterintuitive (categories between 10-20 on the x axis). Do you have an explanation for this? Can you share some examples from this category (I couldn't find any)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}