{
    "id": "2KWZjdFwmh",
    "title": "StEVE: Adaptive Optimization in a Kronecker-Factored Eigenbasis",
    "abstract": "Adaptive optimization algorithms such as Adam see widespread use in Deep Learning. However, these methods rely on diagonal approximations of the preconditioner, losing much information about the curvature of the loss surface and potentially leading to prolonged training times. We introduce StEVE (Stochastic Eigenbasis-adaptive Variance Estimation), a novel optimization algorithm that estimates lower order moments in the Kronecker-Factored Eigenbasis (KFE). By combining the advantages of Adam over other adaptive methods with the curvature-aware transformations of methods like KFAC and EKFAC, StEVE leverages second-order information while remaining computationally efficient. Our experiments demonstrate that EVE achieves faster convergence both in step-count and in wall-clock time compared to Adam, EKFAC, and KFAC for a variety of deep neural network architectures.",
    "keywords": [
        "KFAC",
        "EKFAC",
        "Natural Gradient Descent",
        "Adam",
        "Optimization",
        "Stochastic Optimization"
    ],
    "primary_area": "optimization",
    "TLDR": "Adam in the Kronecker-Factored Eigenbasis of the Empirical Fisher is faster than Adam, KFAC, and EKFAC",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=2KWZjdFwmh",
    "pdf_link": "https://openreview.net/pdf?id=2KWZjdFwmh",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces StEVE, a novel deep learning optimizer that combines aspects of Adam and KFAC. Specifically, they modify EKFAC, which corrects the eigenvalues of the KFAC approximation, by adding Adam's bias-corrected first and second moment estimators. The authors show that StEVE achieves faster training to a target performance in both step count and wall-clock time compared to Adam, KFAC, and EKFAC, on three different deep learning problems on CIFAR-10, CIFAR-100, and Tiny ImageNet."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper presents a novel and interesting approach to incorporating an Adam-style update into the EKFAC optimizer. The resulting algorithm is clearly described in Algorithm 1 and is rather straightforward to implement. The paper also provides code for the new optimizer (as well as the experiments). It presents an extensive introduction and background section and thus an accessible explanation of the method.\n- Faster neural network training is a crucial research topic and any progress in this area is of great interest to the entire deep learning community.\n- The paper not only focuses on the number of steps but also considers the - practically much more relevant - wall-clock runtime."
            },
            "weaknesses": {
                "value": "The empirical evidence for StEVE is too weak to be convincing. As there are now hundreds of deep learning optimizers, the empirical burden of proof of superiority is quite high, especially for optimizers like StEVE who are mostly motivated by their empirical performance. I believe the currently provided experiments don't provide enough evidence to convince people to adopt it in practical applications, for the following reasons:\n\n- Most importantly, the hyperparameter selection seems to be performed in an opaque and potentially unfair way. Apparently, no hyperparameter tuning was performed, e.g., with all optimizers sharing the same learning rate. Yet, the selected learning rate differs between experiments (e.g. 0.001 for CIFAR-10 and 0.00005 for CIFAR-100). How was this chosen? I suspect that these choices work well for StEVE, but not the compared baseline. A more meaningful comparison would be to either tune the hyperparameters for each method on each test problem independently (using the same budget) or use fixed hyperparameters for all methods that are shared across all test problems. The latter would be a \"hyperparameter-free\" optimization and would require different baselines, e.g. Schedule-Free [1].\n- All experiments are done on small problems, with CIFAR-100 being the largest. Also, all are from the same data domain and task, namely image classification.\n- No learning rate schedule was used. I don't think a constant schedule is a very practical choice.\n- Overall, the baselines seem to be very weak, likely due to inefficient hyperparameter choices (see the first point).\n- The target performances seem rather impractical, e.g. only 44% on Tiny ImageNet and 46% on CIFAR-100. This is far from the performance that one can achieve on these datasets (with the used models) and thus not a performance practitioners care about. This is relevant because optimizers that can quickly achieve a low performance can be quite different from optimizers that achieve a more competitive performance quickly.\n\nWithout a more rigorous evaluation, I doubt that the method will have a significant impact. I suggest having a look at [2], which describes a protocol for comparing deep learning optimizers. Although running the full benchmark might be too computationally expensive, following some of the described practices could significantly strengthen the empirical evidence for StEVE and thus demonstrate its strength more convincingly.\n\n[1] Aaron Defazio, Xingyu Alice Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, Ashok Cutkosky; \"The Road Less Scheduled\"; arXiv 2024; <https://arxiv.org/abs/2405.15682>\n\n[2] George E. Dahl et al.; \"Benchmarking Neural Network Training Algorithms\"; arXiv 2023; <https://arxiv.org/abs/2306.07179>"
            },
            "questions": {
                "value": "- Why is KFAC so much slower per step compared to EKFAC? E.g. in Figure 1, both KFAC and EKFAC perform 100 epochs, yet KFAC requires roughly 3x the wall-clock time.\n- Could you add a short paragraph providing a complexity analysis of the computational and memory requirements of StEVE compared to Adam and EKFAC? In my understanding, it should be very similar to EKFAC in both time per step and memory, with the additional memory of a second EMA (for the first moment). Is this correct?\n- Line 19: Should it be \"StEVE\" instead of \"EVE\"?\n- Suggestion: Both Section 1 and Section 2 extensively describe existing work. Only at the bottom of page 4, do you start describing your own method. If you compress Sections 1 and 2, you have more space to present your method, which I think would strengthen your paper.\n- In the paragraph starting at line 79, I think it might be worth mentioning and discussing Shampoo [e.g. 3] and related methods. Shampoo recently won the AlgoPerf: Training Algorithms competition and seems to be a practically relevant non-diagonal method (with likely use in training Gemini models).\n- Line 88: George et al. should probably be a parencite or citep.\n- Line 91: It should probably be \"Due to the expensive nature of [] computing [the] KFE \".\n- Line 127: There should probably be a space before the citation.\n- In Adam's equation, I think there is something missing for the EMA of the second momentum. Either a second gradient after the element-wise multiplication or rather a square (since you mention squaring below).\n- Also just below the equation (line 148) you mention \"vector-multiplication of $\\epsilon$. Do you mean \"addition\"? I don't see where $\\epsilon$ is multiplied.\n- Is there a reason that Section 1 uses $\\mathbf{P}$ as the preconditioner (line 45) and in Section 2 you use $\\mathbf{A}$ (line 116) instead?\n- Line 197: I think $USU$ should also be bolded, since you use bold-face for matrices, no?\n- Line 198: Is this sentence missing a \"to\", i.e. \"which is to say converting the gradient [to] $\\mathbf{A}$'s Eigenbasis\"?\n- In Algorithm 1, you could highlight the differences between StEVE and EKFAC, e.g. by coloring lines that changed.\n- Line 270: There is a double \"against\".\n- Line 271: \"Epoch Count\" and \"Wall-Clock Time\" should probably both be lowercase.\n- The figures, and especially the legends are relatively small and thus hard to read.\n- In the figures, try using a consistent coloring/legend. For example, Adam is yellow in Figure 1 but in Figure 2 KFAC is yellow. This makes it hard to quickly compare across figures. The colors are also relatively similar (yellow, orange, red, pink) and thus hard to distinguish.\n- Is there a reason to not compare to KFAC and EKFAC for the ViT on CIFAR-100?\n\n[3] Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, Yoram Singer; \"Towards Practical Second Order Optimization for Deep Learning\"; OpenReview 2021; <https://openreview.net/forum?id=Sc8cY4Jpi3s>"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an optimization method for deep learning, which performs Adam-style adaptation in a Kronecker-factored eigenbasis. The proposed method is evaluated empirically against vanilla Adam as well as other Kronecker-factored optimizers."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper gives a good introduction to relevant prior work and the contributions are adequately positioned in the context of prior work.\n- The idea for the method is well-motivated, lifting the adaptive Adam scheme to a Kronecker-factored eigenbasis. To my knowledge, this idea has not been explored before and is original.\n- The method shows promising initial results in the experimental framework of the paper.\n- The paper is generally well-written and easy to follow."
            },
            "weaknesses": {
                "value": "The proposed method is a straight-forward combination of existing ideas. No supporting theory is provided. In my opinion, such a paper needs a very detailed and fair experimental comparison to warrant publication at ICLR. Unfortunately, the quality of the experiments is subpar. To mention just a few issues I see\n- Experiments are run with a single random seed.\n- All methods use the same learning rate and it is not explained where that learning rate value comes from. This is not adequate for an empirical comparison of different optimizers.\n- Experiments use a constant learning rate instead of established learning rate decay schedules."
            },
            "questions": {
                "value": "- How was the learning rate for the experiments chosen?\n- Why weren't the learning rates set individually for each competing method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The work proposes STEVE, a novel optimization method that combines the strengths of the Adam optimizer (cheap tracking and adaptation to diagonal second order properties) and EKFAC (amortized better approximation of full second order). This is achieved by applying Adam, not in original parameter space, but in the Kronecker-Factored Eigenbasis (KFE) i.e. the \u201cpreconditioning\u201d basis used by KFAC and EKFAC. Experiments on image classification tasks with ResNet-50 and ViT architectures show significantly faster optimization compared to Adam, both in number of epochs and wall-clock time."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- Originality: The optimizer developed in the paper is novel: an original combination of the strengths of EKFAC and Adam. \n\n- Significance: This is a significant and timely contribution in the context of a heightened interest in more efficient optimization methods for deep learning (e.g. [1,3]) . The development of better off-the-shelf optimizers suitable for training deep learning models is an essential component for driving progress in the field, as can be seen in the wide adoption of Adam. In spite of their theoretical superiority, non-diagonal second order methods have struggled to manifest practical superiority for training standard deep learning models over their simpler diagonal counterparts. That the proposed method manages to convincingly beat Adam on deep network training tasks, both in number of epochs and in wallclock time, is thus significant. It showcases the potential of the approach and warrants the attention of the community.\n\n- Clarity: Motivation, background, and the proposed method are clearly explained (except for minor glitches, see below). This is in part thanks to a clear algorithm box. I also appreciate that readily usable pytorch code is given in the supplementary for reproducibility. Experimental setup and methodology are also briefly but clearly explained.\n\n- Quality: The approach is well-motivated, appears sound and well implemented, and the presented experiments convincingly support the claim of superiority of the developed optimizer."
            },
            "weaknesses": {
                "value": "- Missing a more thorough recent related works discussion.\nRelated work pertaining to background is well covered, but the paper is missing a section discussing later advances in second order optimization methods for deep learning. See [1,2,3] and first question below for starting pointers.\n\n- The experimental analysis could have been pushed further: to include also training loss curves, and an evaluation and discussion of the relative sensitivity to hyperparameters. (see questions section for details).\n\n- Somewhat limited scope and scale of experimental evaluation. \nWhile I value the experimentation on 2 different deep architectures ResNet50 and ViT and 2 image datasets, a more extensive experimentation on a larger variety of tasks and datasets would help to more solidly establish the advantage of the approach. See e.g. deep net training benchmark [1]. \n\n- Paper would benefit from a little more polishing. \nSome (minor and easily fixable) clarity issues. See questions part for a list and suggested improvements."
            },
            "questions": {
                "value": "Q1:  I would like to draw your attention to concurrent work SOAP [3], which seems closely related as it also uses Adam inside a second order preconditioning approach, Shampoo [2]. This doesn\u2019t lower the originality of your proposal, being concurrent work that you likely couldn\u2019t know about at the time of submission. But given the relatedness of the approaches, I am interested to know how you would contrast them? What can you highlight as the differences / anticipated benefits & limitations of STEVE=EKFAC+Adam v.s. SOAP=Shampoo+Adam ?\nAlso, how do they compare in memory and compute complexity? \nThis discussion could become part of a fleshed out related works section. \n\nQ2:  Algo lines 16 and 17 eigendecomposition(...): what are the expectations over B and T? Can you provide more details on how these expectations are computed/estimated/tracked? (I suggest to also update the algo box to provide this additional level of detail, as well as main text l 283 \u201crunning averages\u201d)\n\nQ3: Training loss curves associated with your test accuracy curves.\nCan you include these (in supplementary if space is insufficient in main)\nDo the higher test accuracies also correspond to lower training losses? Please discuss.\n\nQ4: What are the test accuracy and training loss reached by all algos at the max number of iterations you used?\n\nQ5: Sensitivity to hyperparameters?\nDo you have evidence that your optimizer outperforming Adam does not require extensive fine-tuning of (additional?) hyper-parameters. E.g. how sensitive is it to recompute frequency?\nSimilarly you write l291 \u201cThe other methods did not converge at this learning rate\u201d, but would thay at other rates?\n\nFurther clarifying suggestions:\n- L 200 \u201cas the critically important eigenvalues \u2026 are not preserved by the approximation\u201d -> needs more explanation.\n- The explanation of EKFAC and in particular the KFE in paragraph line 202 is too dense. This is the algo that you build on, so please try to lighten expand and clarify.\n- BUG towards end of update equation for Adam\u2019s  $v_{t+1}$ line 139, missing a square?\n- Curves: please use more easily distinguishable colors than different shades of red! (given the chance, make them color-blind friendly, see e.g. https://davidmathlogic.com/colorblind, and/or use different line styles)\n- Figure 3 is missing KFAC and EKFAC. \n\n\nTypos and English fixes:\n- Abstract L19: \u201cEVE\u201d -> \u201cSTEVE\u201d\n- L 148: \u201cvector-multiplication of $\\epsilon$ are done element-wise\u201d. I see no vector multiplication of $\\epsilon$ ???\n- L 161: \u201cis taking\u201d -> \u201cis taken\u201d\n- L 175: \u201creduces\u201d -> \u201cwhich reduces\u201d\n- L 198: \u201cconverting\u201d -> \u201cchanging to\u201d\n- L 270: \u201cagainst against\u201d \n- L 283: \u201crunning averages\u201d, computed how exactly?\n- L 285: $\\alpha$ has never been defined. You should at least say what it is and does in KFAC/EKFAC.\n- L 291: \u201cThe other methods did not converge at this learning rate\u201d -> do you mean they did to reach the target accuracy? What about at other learning rates? Did you hyper-optimize over it, and how sensitive are the methods to it? \n- L 382: \u201cof the Fisher\u201d -> \u201cof the empirical Fisher\u201d\n- L 391: \u201cOther directions to take the work are to investigate the potential of the improvements that have been made over Adam in the KFE such as proper weight decay or Nesterov momentum.\u201d -> \u201cFuture work should also investigate the potential of using, in the KFE, other improvements that have been made over Adam, such as proper weight decay [ADD REFERENCE] or Nesterov momentum [ADD REFERENCES].\n\n\n\n[1] Benchmarking Neural Network Training Algorithms, Dahl et al. 2023 https://arxiv.org/abs/2306.07179\n\n[2] Shampoo: Preconditioned stochastic tensor optimization. V. Gupta, T. Koren, Y. Singer. ICML 2018\n\n[3] SOAP: Improving and Stabilizing Shampoo using Adam. Vyas et al. September 2024.\nhttps://arxiv.org/abs/2409.11321"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Adamize diagonal corrections to KFAC in a similar way to EKFAC"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper adopts and extends the style of thinking seen in EKFAC to apply diagonal corrections to KFAC."
            },
            "weaknesses": {
                "value": "The paper opts to use SGD as the base opt for KFAC and EKFAC. The official and unofficial codebases for KFAC allow and some actual suggest to use Adam as the base opt for KFAC and EKFAC. This is because it's well known that when we operate using Adam as the base opt that drives E/KFAC it works better. \n\nThe authors say: \n\n\"However, instead of using only the second moments, STEVE maintains bias-corrected exponential moving averages of both the first and second moments of the gradients in the KFE, estimated in the same manner as in Adam. By combining the benefits of the Kronecker-factored approximation with the\nadaptive moment estimation of Adam, STEVE aims to achieve faster convergence.\" \n\nWhile the opt in this pape is not exactly Adam as the base opt driving E/KFAC it is in similar vain, as such it would have been helpful to have ran experiments with SGD and Adam as the base opts for E/KFAC so we could see if there is a delta. \n\nAnother weakness is not comparing to Shampoo which is an alternative kronecker factorized optimizer that has become quite popular recently due to its strong performance at Google. Furthermore the same way this paper proposes Adamized diagonal 1st and 2nd moment corrections to KFAC, SOAP proposes this for Shampoo. As such this paper should really compare to those methods. \n\nFurthermore, PSGD Affine or Kronecker factorized has been shown to outperform E/KFAC as well as Shampoo/SOAP and should be compared as well for this paper to be complete. \n\nAnother weakness is the use of a ViT for cifar datasets. The images are too small for patches to make sense and so it generally doesn't do well. Something like Keller's modded-nanoGPT would be a good place to show the performance of the opt since it's been benchmarked against all the latest curvature informed optimizers."
            },
            "questions": {
                "value": "What is the memory and computational complexity of the proposed opt? \n\nHow frequently is the preconditioner updated? Shampoo updates every 100 iterations, PSGD updates every 10 iters. It would be good to see how often the precond must be updated and how it effects performance. \n\nVariance bars? A\n\nThe claim that the proposed opt significantly outperforms (40% reduction in all clock time) Adam in fig 1 seems not true based on wall clock time. It seems at the end of training Adam ends at a higher accuracy, and Adam actually matches StEVE only a few hundred seconds later. Since the authors do not show variance bars we have no way of knowing if this is a legit speedup. \n\nFurthermore, with the extra memory needed to train with StEVE one could easily boost batch size for Adam and see an improvement in performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}