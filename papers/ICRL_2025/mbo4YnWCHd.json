{
    "id": "mbo4YnWCHd",
    "title": "Non-negative Tensor Mixture Learning for Discrete Density Estimation",
    "abstract": "We present an expectation-maximization (EM) based unified framework for non-negative tensor decomposition that optimizes the Kullback-Leibler divergence. To avoid iterations in each M-step and learning rate tuning, we establish a general relationship between low-rank decomposition and many-body approximation. Using this connection, we exploit that the closed-form solution of the many-body approximation can be used to update all parameters simultaneously in the M-step. Our framework offers not only a unified methodology for a variety of low-rank structures, including CP, Tucker, and Train decompositions but also their combinations forming mixtures of tensors. The weights of each low-rank tensor in the mixture can be learned from the data, which eliminates the need to carefully choose a single low-rank structure in advance. We empirically demonstrate that our framework provides superior generalization for discrete density estimation compared to conventional tensor-based approaches.",
    "keywords": [
        "Tensor factorization",
        "EM algorithm",
        "Low-rank approximation"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "EM-based unified framework for non-negative tensor decomposition that can accommodate a mixture of low-rank tensors and adaptive noise terms",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mbo4YnWCHd",
    "pdf_link": "https://openreview.net/pdf?id=mbo4YnWCHd",
    "comments": [
        {
            "summary": {
                "value": "This work presents an expectation-maximization (EM) based unified framework for nonnegative tensor decomposition that optimizes the Kullback-Leibler divergence, and further establishes a general relationship between low-rank decomposition and many-body approximation. The proposed framework offers a unified methodology for a variety of low-rank structures, including CP,\nTucker, and Train decompositions, and their combinations. A series of experiments are carried out to illustrate the merits of the developed methodology."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. A unified methodology has been developed to deal with a variety of low-rank structures, including CP, Tucker, Train decompositions, and  their combinations, forming mixtures of low-rank tensors. \n2. A mixture of low-rank tensor modeling procedure is developed  to empirically demonstrates inferential robustness and improved generalization.\n3. Both theoretical analysis and numerical comparisons are provided to show the merits of the proposed methodology."
            },
            "weaknesses": {
                "value": "1. The convergence analysis seems overly coarse and somewhat redundant, as it only demonstrates that negative cross-entropy increases with iterations\u2014an expected result given the maximization objective. More importantly, the analysis should address how negative cross-entropy could converge to the true value.\n2. Please note that the computational cost of the proposed algorithm increases significantly with the size of the tensor. Kindly provide the specific order of computational complexity and compare it with the complexity orders of related algorithms.\n3. The compared baselines are not SOTA. Please consider some recent methods for numerical comparisons. \n4. Please provide a detailed description of the convergence conditions in Algorithm 1."
            },
            "questions": {
                "value": "Is there a specific general mathematical expression for the many-body approximation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to parsimoniously unify a variety of low-rank tensor decomposition structures and address discrete density estimation using a novel mixture of decompositions method. The method permits mixtures of CP, Tucker, and tensor-train decompositions to model non-negative data. The authors derive a computationally efficient expectation-maximization (EM) algorithm for performing inference in their model. The inference algorithm allows for learning of the mixture weights of the components, which are tensor decompositions. In addition to establishing a unifying framework for low-rank tensor decompositions, the authors demonstrate their method\u2019s effectiveness in estimating discrete mass functions by comparing a specific instance of their method, EMCPTrain, to a large body of existing ones. Their method achieves marginally better negative-log-likelihood per sample than existing methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The contribution of closed-form maximization updates for the specific EM algorithm is a strength. The computational efficiency, in some instances of the method (such as EMCPTrain), are useful. The unifying framework across low-rank tensor decompositions is parsimonious, and leveraging the parsimony of mixture models is a neat idea. I particularly like how the authors show that one may learn the weights of their mixture model, removing the need to choose between low-rank structures in advance of training."
            },
            "weaknesses": {
                "value": "The approach, while unifying in theory and a neat idea, yields very incremental empirical results at best. The empirical gains are modest. When more baselines are taken into account, as in Table 6, the gains are further reduced. The comparisons and evaluations are not well-organized, making the aggregate contribution difficult to evaluate across the many tables in Section 5 and Appendix C. There are some section of the paper that came across as unclear that first time I read it. In particular, the paper refers to \u201cmany-body approximation\u201d many times in the first two sections of the paper without a clear definition. A formal definition in Section 1 would help clarify how the paper aims to leverage the many-body approximation representation."
            },
            "questions": {
                "value": "In my experience, adding a noise (or constant tensor) term to a non-negative tensor decomposition can significantly improve model fit, and the empirical results shown in Table 2 mostly demonstrate this phenomena. How much of the empirical improvements in negative log-likelihood are from the adaptive noise term? I view the ability to seamlessly learn the noise term from the data as an advantage of this method, although I am concerned it is the only substantial advantage of this method in practice. \nWhen would it be useful to include a Tucker component in practice? EMTucker is computationally expensive, scaling as R^D. While EMTuckerN beats existing methods in Table 5, it generally performs worse than EMCPTrainN."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The work deals with a nonnegative tensor decomposition algorithm for discrete density estimation. The framework is based on expectation maximization algorithm from KL divergence minimization perspective. Unlike the existing KL divergence-based nonnegative tensor decomposition algorithms for this problem that used gradient-based iterative steps for parameter updates, the proposed approach leverages the insights from the many-body approximation problem. Consequently, the updates of the parameters in the M-step boils down to closed form expressions, reminiscent of the updates derived in (Huang & Sidiropoulos,2017; Yeredor & Haardt, 2019) ."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Strengths:\n\n1.\tThe proposed tensor decomposition method is a general framework that can handle different types of tensor decomposition models like CP, Tucker and tensor train, innovatively utilizing the many body approximation technique from the (Ghalamkari et al.,2023). The closed-form updates of the parameters in the M-step of the EM algorithm is also attractive as it avoid tuning of any hyperparameters, which is often practically inconvenient"
            },
            "weaknesses": {
                "value": "Weakness:\n\n1.\tThe paper organization has lot of room for improvement. From the start of the paper, a clear description of problem is missing. Section 4 introduces the problem statement, while section 3 starts describing the approach (many-body approximation). Then, it is confusing to connect and understand that some of the terms have been obtained from the previous step of the algorithm.\n\n2.\tThe problem formulation and the application of the solution itself is debatable. The tensor ${\\cal T}$ is an empirical distribution tensor which is of the dimension of the categorical features. The entire solution of discrete density learning depends on the accuracy of the empirical tensor, which is hard to make sure due to curse of dimensionality. \n\n3.\tThe problem is hardly scalable as it deals with the many-body approximation tensor ${\\cal M}$ whose dimension is much larger than that of even the empirical tensor ${\\cal T}$.\n\n4.\tThe experiments and major baselines are lacking. In discrete density estimation, there are works using second and third order marginals, that can mitigate the curse of dimensionality problem to an extent.\n\na.\tKargas, Nikos & Sidiropoulos, N.D. & Fu, Xiao. (2017). Tensors, Learning, and \u201cKolmogorov Extension\u201d for Finite-Alphabet Random Vectors. IEEE Transactions on Signal Processing. 66. 10.1109/TSP.2018.2862383.\n\nb.\tS. Ibrahim and X. Fu, \"Recovering Joint Probability of Discrete Random Variables From Pairwise Marginals,\" in IEEE Transactions on Signal Processing, vol. 69, pp. 4116-4131, 2021, doi: 10.1109/TSP.2021.3090960\n\nDiscussions and comparisons with these baselines would help readers understand the strength of the approach (if any)."
            },
            "questions": {
                "value": "Questions/Comments:\n\n1.\tWriting and Organization: It is important to introduce the low-rank tensor structures when discussing it in the introduction. \n\n2.\tIt is commented that \u201cit always converges regardless of the choice of the low-rank structure assumed in the model.\u201d EM convergence is hard to establish unless it is well initialized. Hence, it is not easy to claim convergence for the proposed method\n\n3.\tWhile there is a technique introduced to reduce the computational complexity of tensor train decomposition as presented in Section 4.2, computational complexity of Tucker remains the same which becomes dominant especially the case of K>1 mixture cases. The convergence speed curves should be presented with time in x axis to understand where it stands with respect to the baselines. Due to the computational complexity, it is also challenging to utilize the mixture model in practice. Once may need to choose the component of the mixtures (the type of tensor decomposition) , rather than allowing the model to learn by itself. A detailed discussion on the limitation of the approach (in the main section) would help here. \n\n4.\tIt is unclear how does the adaptive noise term guarantees the ``convexity\u201d and convergence as claimed in Section 4.4. What do you mean by convexity here? How do you learn the noise parameter here? Do you learn different noise parameter for different low-rank tensor models?\n\n5.\tExperiments are limited to showing negative log likelihood. While it shows the dynamics of the algorithm, it does not show how does the approach learn the ground-truth. Simulation studies would help understand how well the method learns the true discrete density. In real data experiments, missing value prediction should be a better approach to understand the applicability of the approach.\n\n6.\tMinor typos: \u201cdistribution underlying the data\u201d in Page 5, notation $\\eta^k$ is confusing with $\\eta$ raised to kth power."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}