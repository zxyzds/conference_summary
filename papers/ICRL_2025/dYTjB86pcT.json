{
    "id": "dYTjB86pcT",
    "title": "System Aware Unlearning Algorithms: Use Lesser, Forget Faster",
    "abstract": "Machine unlearning aims to provide privacy guarantees to users when they request deletion, such that an attacker who can compromise the system post-unlearning cannot recover private information about the deleted individuals. Previously proposed definitions of unlearning require the unlearning algorithm to exactly or approximately recover the hypothesis obtained by retraining-from-scratch on the remaining samples. While this definition has been the gold standard in machine unlearning, unfortunately, because it is designed for the worst-case attacker (that can recover the updated hypothesis and the remaining dataset),  developing rigorous, and memory or compute-efficient unlearning algorithms that satisfy this definition has been challenging. In this work, we propose a new definition of unlearning, called system aware unlearning, that takes into account the information that an attacker could recover by compromising the system (post-unlearning). We prove that system-aware unlearning generalizes commonly referred to definitions of unlearning by restricting what the attacker knows, and furthermore, may be easier to satisfy in scenarios where the system-information available to the attacker is limited, e.g. because the learning algorithm did not use the entire training dataset to begin with. Towards that end, we develop an exact system-aware-unlearning algorithm that is both memory and computation-time efficient for function classes that can be learned via sample compression. We then present an improvement over this for the special case of learning linear classifiers by using selective sampling for data compression, thus giving the first memory and time-efficient exact unlearning algorithm for linear classification. We analyze the tradeoffs between deletion capacity, accuracy, memory, and computation time for these algorithms.",
    "keywords": [
        "machine unlearning",
        "learning theory",
        "selective sampling for unlearning"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=dYTjB86pcT",
    "pdf_link": "https://openreview.net/pdf?id=dYTjB86pcT",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes system-aware unlearning which ensures that an observer cannot distinguish between a model trained on the initial dataset with unlearning applied and a model trained on a smaller dataset without the deleted points."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Sharding and sub-sampling can improve efficiency of unlearning algorithms"
            },
            "weaknesses": {
                "value": "The paper ignores Bourtoule et al. \n\nThe paper needs to properly analyze the threats against prior unlearning algorithms (inference attacks against sequence of released models, revealing what data is deleted between two releases)."
            },
            "questions": {
                "value": "A. The paper does not talk about the prior sharding methods (Bourtoule et al, Machine Unlearning). How does this work compare with that method (which is splitting data into shards, training one model on each shard, and retraining only the model that includes the to-be-deleted datapoint). I suggest the authors to comment on the fundamental differences between these works, and also comment on the followings.\n\n1. Differences in computational efficiency during unlearning\n2. Memory requirements for storing models/data\n3. Impact on model accuracy as deletion requests increase\n4. Scalability to large datasets or high-dimensional data \n\n\nB. Many follow-up papers of Bourtoule et al highlighted the possibility of an adversary observing multiple snapshots of the models (over time, as unlearning requests are processed). Can you analyze the proposed system-aware unlearning algorithm in presence of adversaries with continuous observations of models? In particular, I suggest the authors to respond to the following questions. \n\n1. How do the privacy guarantees of the proposed system-aware unlearning change under an adversary with continuous model observations?\n2. Can you provide a comparative analysis of information leakage between your approach and previous methods in this scenario?\n3. Are there potential modifications to your algorithm that could strengthen its resilience against such adversaries?\n4. How does the computational overhead of maintaining privacy change in this threat model?\n\nC. What are the limitations of the proposed method? Which threats remain unaddressed, and which types of algorithms are incompatible with this approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new, system-aware formulation of machine unlearning, which takes into account the information that an attacker could recover by compromising the system. The authors develop exact system aware unlearning algorithms based on sample compression learning algorithms and establish the computation time, memory requirements, deletion capacity, and excess risk guarantees."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The idea of incorporating the observer's perspective into the unlearning process is novel.\n2. The theoretical analysis is overall coherent.\n3. The paper is well-structured."
            },
            "weaknesses": {
                "value": "1. Lack of quantitative comparisons with other methods (theoretically or empirically).\n2. The writing of the intuition part in Section 2 is not clear enough. The reviewer is quite confused about the statements on lines 125-128.\n3. Lack of definitions for notations, e.g., $\\mathcal X, \\mathcal Y, \\mathcal Z, \\mathcal Z^*, f^*(\\cdot), \\Delta(\\mathcal F), \\mathcal J$, etc.\n4. The author is confused about the \u201ccore set deletion capacity $K$\u201d and the \"$K$ shards\" in Algorithm 1. Are the two $K$s the same parameter?\n5. Some writing issues: grammar mistakes and typos, e.g., \"we developed algorithms for exact system aware unlearning algorithms\" on line 526 and \"exact system aware\" on line 531.\n6. The definition stated in Sekhari et al. (2021b) and Guo et al. (2019) is generally referred to as \"*certified* machine unlearning\".\n7. Although this paper mainly focuses on the theoretical part, it would be better to include the experimental results in the main text."
            },
            "questions": {
                "value": "1. What is $e$ in $\\frac{K}{e}$ on line 275?\n2. What are $\\mathcal T_P$ and $\\mathcal T_f$ on line 6 of Algorithm 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "(epsilon, delta) unlearning definitions require the unlearned model to be indistinguishable from retraining from scratch on the remaining data.  In this work, the authors argue that this is too strict of a definition for unlearning.  They propose a new definition for unlearning, called system aware unlearning, that takes into account the information that an attacker could recover by compromising the system.  The authors develop system aware unlearning algorithms that are efficient for a class of functions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- \u201cFurthermore, we believe that accounting for the information that an attacker could have access to is an interesting direction to explore in privacy, beyond unlearning.\u201c I fully agree, this is an interesting aspect of unlearning that prior work has neglected. Definitions that try to take this kind of information to account is valuable to the research community."
            },
            "weaknesses": {
                "value": "I fold in my questions here:\n\n- \u201cThis presents significant challenges in the context of privacy regulations such as the\nEuropean Union\u2019s General Data Protection Regulation (2016) (GDPR), California Consumer Privacy Act (2018) (CCPA), and Canada\u2019s proposed Consumer Privacy Protection Act, all of which emphasize the \u201cright to be forgotten.\u201d As a result, there is a growing need for methods that enable the selective removal of specific training data from models that have already been trained, a process commonly referred to as machine unlearning (Cao & Yang, 2015).\u201c A general comment: It\u2019s still an open problem how these pieces of legislation should apply to ML models, and whether unlearning can satisfy these legal requirements.\n\n\n- \u201cThis is evidenced by a dire lack of exact/approximate unlearning algorithms beyond the simple cases of convex loss functions.\u201c There have been many unlearning algorithms for non-convex models, unless you mean a dire lack of unlearning algorithms that work? Although there are also exact unlearning algorithms for non-convex models such as SISA [1].\n\n- \u201cOur framework leverages the fact that many ML systems do not depend on the entirety of their training data equally\u201c Can you precisely define what you mean by this?\n\n- Nit typo: \u201cWe then present a general-purpose algorithm for exact system-aware unlearning using data sharding for function classes that can learned using sample compression,..\u201c\n\n- \u201cWe also provide an improved system-aware unlearning algorithm for the special case of linear classification thus providing the first memory and time-efficient exact unlearning algorithm for linear classification.\u201c This confuses me. The above comment complained that there\u2019s a lack of algorithms beyond simple convex settings and yet you provide another algorithm for the convex setting..?\n\n- I thought that the motivating example in line 118 is bit contrived, to be statistically indistinguishable all you need to do is change the order of operations such that A() samples a set C from S and *then* removes U from this set, rather than sampling C from S\\U.\n\n- *(System-Aware-(\u03b5, \u03b4)-Unlearning): For any S\\U there exists an S' that is distributionally indistinguishable when trained.* Does the existence of S\u2019 imply we can find it efficiently? I think not, and if not, how should we think about instantiating this definition in practice?\n\n- In Definition 3, taking S\u2019=S\\U recovers the original definition. I\u2019m struggling to understand the utility of allowing S\u2019 to be anything other than S\\U. That is, I\u2019m struggling to understand if this flexibility is useful in practice. Can you provide an illustrative example?\n\n- \u201cSince the attacker can only gain access to information stored by the system and used in the unlearned model, then we want to learn predictors that are dependent on a small number of samples.\u201c I struggle to understand this statement. For most parametric models, samples are not \u201cstored\u201d in the system (that is, they are stored, but through a complex learning process which is difficult/impossible to reverse engineer). Does this statement only apply to simple linear models? This idea of reducing the number of dependent samples seems to be core to the ideas underpinning section Section 3 and Section 4.\n\n- \u201cAlgorithm 2 Unlearning algorithm for linear classification using selective sampling\u201d. I struggle to understand if this is a real contribution to unlearning. The algorithm is simply reversing the learning process on the unlearned points through subtraction. It also applies to a very narrow case as you point out: \u201dThis monotonicity is a unique feature of the BBQSAMPLER. Other selective sampling algorithms, such as ones from Dekel et al. (2012) or Sekhari et al. (2023), use a query condition that depends on the labels y of previously seen points. Due to the noise in these y\u2019s, y-dependent query conditions are not monotonic; points that were queried can become unqueried. This makes it difficult and expensive to compute the core set after unlearning.\u201c and \u201c We note that since the BBQSAMPLER uses a y-independent query condition, it is suboptimal in terms of excess error before unlearning compared to algorithms from\u2026\u201c.\n\n- \u201cWhy is Algorithm 2 not a valid unlearning algorithm under the prior unlearning definition\n(Definition 1)? When a queried point is deleted, an unqueried point could become queried. Thus, under traditional notions of exact unlearning, during DELETIONUPDATE, not only would we have to remove the effect of U, but we would also have to add in any unqueried points that would have been queried if U never existed in S.\u201c I don\u2019t follow this counterfactual argument. According to Def 1, running Algorithm 2 on S\\U should be identical to running on S and then removing on U?\n\n- Can you comment on the similarities between Algorithm 1 and SISA [1]?\n\n[1] https://arxiv.org/abs/1912.03817"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper begins by discussing existing definitions of machine unlearning, highlighting their limitations through a specific case study. It then introduces a novel definition, System-Aware Unlearning, and provides a detailed explanation.\nContributions:\n1.\tThe paper analyzes the limitations of the existing unlearning definitions and introduces the concept of System-Aware Unlearning. \n2.\tThey propose a general-purpose unlearning algorithm utilizing data sharding under the framework of System-Aware Unlearning.\n3.\tAuthors also propose an unlearning algorithm for linear classification using selective sampling for the special case of linear classification."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe paper introduces a novel System-Aware Unlearning framework that advances the theoretical foundations of machine unlearning. This framework takes a more practical approach by considering the actual system security model and attacker capabilities. \n2.\tThis paper includes two algorithms: a general-purpose algorithm for exact system-aware unlearning using data sharding, and an improved system-aware unlearning algorithm for the special case of linear classification. \n3.\tA comprehensive theoretical analysis is provided that includes deletion capacity, model accuracy, memory requirements, and computational complexity."
            },
            "weaknesses": {
                "value": "1.\tThe necessity of \"system aware unlearning\" is not well justified. A critical assumption in the Introduction states, \"consider a learning algorithm that relies on only a fraction of its training dataset to generate its hypothesis and hence the ML system only stores this data. \" However, the paper fails to explain why models would be trained on partial datasets, or whether the data selection process itself requires unlearning due to potential information leakage.\n2.\tThe paper claims, \" Even if an observer/attacker has access to larger public data sets that might include parts of the data the system was trained on, in such a system we could expect privacy for data that the system does not use directly for building the model to be preserved.\" This statement is not true, as no privacy can be preserved if the entire training dataset is exposed to attackers.\n3.\tThe conclusion that models would \"not be statistically indistinguishable from each other\" on Page 3 does not hold when the model owner uses the complete training dataset S.\n4.\tUsing only one shard's core set for prediction would likely cause significant performance degradation compared to models trained on complete datasets. The paper lacks experimental validation of this approach.\n5.\tThe paper lacks an Evaluation section. Machine unlearning algorithms should be assessed across multiple dimensions, including unlearning efficiency, effectiveness, and model utility preservation. A comparative analysis with existing methods is also necessary.\n6.\tThe paper lacks a Related Work section. \n7.\tTechnical writing requires improvement, particularly in defining key concepts like ExcessRisk and parameters such as f*."
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}