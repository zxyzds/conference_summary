{
    "id": "JCiF03qnmi",
    "title": "How Does Critical Batch Size Scale in Pre-training?",
    "abstract": "Training large-scale models under given resource budgets requires the careful design of parallelism strategies. In particular, the efficiency notion of critical batch size (CBS), concerning the compromise between time and compute, marks the point beyond which greater data parallelism leads to diminishing returns. To operationalize it, we propose a measure of CBS and pre-train a series of auto-regressive language models, ranging from 85 million to 1.2 billion parameters, on the C4 dataset. Through extensive hyper-parameter sweeps and careful control of factors such as batch size, momentum, and learning rate along with its scheduling, we systematically investigate the impact of scale on CBS. Then we fit scaling laws with respect to model and data sizes to decouple their effects. Overall, our results demonstrate that CBS scales primarily with data size rather than model size, a finding we justify theoretically through the analysis of infinite-width limits of neural networks and infinite-dimensional least squares regression. Of independent interest, we highlight the importance of common hyper-parameter choices and strategies for studying large-scale pre-training beyond fixed training durations.",
    "keywords": [
        "Pre-training",
        "Language Models",
        "Data Parallelism",
        "Optimization"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=JCiF03qnmi",
    "pdf_link": "https://openreview.net/pdf?id=JCiF03qnmi",
    "comments": [
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "Thank you for your constructive feedback. We have addressed each of your points as follows:\n\n> The paper heavily relies on Chinchilla scaling theory\n\nPlease note that we do consider under-training and over-training compared to Chinchilla compute-optimal regimes (fig. 1 middle and fig. 6). This enables a more controlled comparison as we control for model size and data size separately, which further strengthens our claim.\n\n >  \"dynamic horizon\" schedule \n\nThank you for the suggestion. We do compare extensively using Sec 2.1, Appendix A. Especially our results of fig. 10 show that tuning the EWA decay rate high is needed for training longer. This plays a similar role in tuning down the learning rate when training longer, as suggested in [1].\n\n\n[1] Scaling Optimal LR Across Token Horizons Johan Bjorck, Alon Benhaim, Vishrav Chaudhary, Furu Wei, Xia Song.\n\n> Weight decay was disabled for the experiments.\n\nWe disabled weight decay as we found it did not change the critical batch size in preliminary experiments for our setup. If this is crucial for reviewers\u2019 judgment of our work we would be happy to redo some of the experiments with 1e-4 decoupled weight decay (as suggested in Wortsman 2023).\n\nWe also note that our setup is based on the Olmo codebase. A previous work [2] (Figure. 5) also conducted experiments on the Olmo codebase and only observed very small difference for different weight decay values $0, 10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}$ for autoregressive LM pre-training on C4. \n\n[2] Deconstructing What Makes a Good Optimizer for Language Models. Rosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, Sham Kakade\n\n\n> \"trajectory\" to be defined more clearly. e.g., is it the trajectory of the weights during training? Also, please cite a source for this statement.\n\nThanks for the comment. Yes, we mean the trajectory of the weights. We will cite the Master Theorem of Yang and Hu (2021) in the proof as well.\n\n> Which model after what training have these evaluation variance numbers been obtained?\n\nAs suggested in Sec 2.1, we use 151M models for most ablation experiments to decide hyper-parameters and evaluation configurations. We use C4 consistently throughout the paper as our training and evaluation data.\n\n\nPlease note that the scaling of epsilon in Adam itself is an open research question and due to budget constraints we found that it has minor effects and didn\u2019t extensively sweep over different other values. Note that we study models up to 1.2B, and [3] pointed out **For standard parameterization models with up to a billion parameters, the typical default value of 1e-8 is likely acceptable**\n\n[3] Scaling Exponents Across Parameterizations and Optimizers. Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A. Alemi, Roman Novak, Peter J. Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, Jeffrey Pennington.\n\n> Typos on R(M,t) and \\theta\n\nThank you for pointing these out. We fixed them and updated the draft."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "Thank you for your constructive feedback. We have addressed each of your points as follows:\n\n> Schedule free optimizer  \n\nPlease note that From Fig. 2, schedule-free optimizer is competitive for small batch sizes, this aligns with what the original paper found. Notably the original paper only consider a batch size of around 2^9. However, it\u2019s worse for large batch sizes even if we tuned the beta1 from 0.9, 0.95, 0.98. \n\n> Reproduction on constant LR + EWA.\n\nPlease note that a very large EWA decay rate would cause instability but may lead to better convergence as suggested in the plot you referred to. As our goal is to reach a certain target loss, we use different EWA values for better convergence during the last part of the training.\nWe included our implementation in the supplementary materials. Note that the performance gap also depends greatly on the setting and especially on training durations ana batch size.\n\n> Weight decay was disabled for the experiments.\n\nWe disabled weight decay as we found it did not change the critical batch size in preliminary experiments for our setup. If this is crucial for reviewers\u2019 judgment of our work we would be happy to redo some of the experiments with 1e-4 decoupled weight decay (as suggested in Wortsman 2023).\n\nWe also note that our setup is based on the Olmo codebase. A previous work [1] (Figure 5) also conducted experiments on the Olmo codebase and only observed very small difference for different weight decay values $0, 10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}$ for autoregressive LM pre-training on C4. \n\nPlease note that the Wortsman et al. (2024) paper you mentioned shows that it does not affect the optimal LR for large enough models with size more than 85M (Figure E.10). To understand whether disabling weight decay would need a smaller learning rate, we add new experiments to show that our default value 3.16e-3 is consistently the best if the EWA decay rate is well tuned in Fig. 10. \n\n[1] Deconstructing What Makes a Good Optimizer for Language Models. Rosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, Sham Kakade\n\n\n> How did you use EWA and reproduction\n\nPlease note that EWA is introduced in Sec 2.1 and we included all the possible values in Tab.3, as well as additional ablation studies in Fig. 10. Note that it\u2019s common practice to tune EWA gradually from small values to larger ones. Especially we found that similar to the learning rate, the effectiveness of EWA+constant in different decay rates depends on the training duration. \nPlease refer to our supplementary materials for reproduction. \n\n> Did you tune SFO with the same sweeps as Adam?\n\nCan you please clarify what do you mean by SFO? We didn\u2019t use this abbreviation in our paper.\n\n> Why are the warmup steps chosen between 0.15, 0.25 and 0.35?\n\nPlease note that choosing warmup proportional to data size is a common strategy that has been justified in previous work. In [2], they set the warmup period to be the minimum of the model size N and 20% of the total token budget. In the table 5 of [2], they also show that Durations of up to 4N achieve very similar and slightly better results. Therefore, we believe sweeping over 0.15, 0.25, 0.35 would be a reasonable design decision to achieve reasonable performance. \n\n[2] Resolving Discrepancies in Compute-Optimal Scaling of Language Models Tomer Porian, Mitchell Wortsman, Jenia Jitsev, Ludwig Schmidt, Yair Carmon.\n\n> Suggestion on colors.\n\nThank you and we choose colors to reflect the ablations on different scales like model size and data size. We use Fig. 6 for side-by-side comparison to further support the claim."
            }
        },
        {
            "title": {
                "value": "Response [4]"
            },
            "comment": {
                "value": "> How a pre-training practitioner could use the insights from this paper when scaling (model or data)?\n\nWe have include Sec 1.1 for clarifying the empirical takeaways. Especially, our empirical finding that CBS scales primarily with data size implies that when scaling up data, one can reduce serial training time through greater data parallelism due to the increase of CBS, without a loss in computational efficiency that can be measured by floating point operations (FLOPs)."
            }
        },
        {
            "title": {
                "value": "Response [3]"
            },
            "comment": {
                "value": "> Does Figure 5 have empirical backing regarding the predicted CBS and the formalism of staying within the 20% margin of steps for similar loss?\n\nOr if the decoupled prediction for batch sizes hold reliably for different scales or ablations such as architecture, datasets, hyperparameters?\n\nPlease note that in Sec 2.1 we have explained that we only take into account the best-performing run for each batch size. So the scaling laws hold if we have models well-tuned across hyper-parameters we consider in the paper. We only consider transformer-based auto-regressive language models on C4 as they are canonical settings that are practical and can be reproduced more easily.\n\n> How exactly are the tuning decisions undertaken especially in Section 3.3 (example, L371-372)?\n\nPlease note that we have mentioned that we scale the warmup tokens proportionally: in Fig. 1 we have compute-optimal setting 1x, and the 0.28x, 0.5x for under-training, 2x, 4x for over-training. If we set the warmup tokens for compute-optimal setting as $0.25C_{Chin}N$, then we have $0.07C_{Chin}N$, $0.125C_{Chin}N$ for over-training and $0.5C_{Chin}N$, $C_{Chin}N$ for over-training.  \n\n> In Theorem 1, is it expected or meant to be w2>w1? If so, then there is a typo and makes a significant difference to the interpretation\n\nNo, only the two widths are greater than $w$, there is no relation between the two widths\n\n> What is the significance of t in Theorem 1? Should we compare losses under similar tokens for two vastly different model scales, especially following Chinchilla?\n\nYes, this theorem specifically only holds for models with different widths trained for the same number of tokens (and thus does not hold for Chinchilla setup where tokens increase with increasing model scale). So $t$ refers to the fixed number of tokens.\n\n> What is the specification or how is H defined in Section 4.2?\n\nAs shown in the beginning of Sec 4.2, H is the covariance matrix of the Gaussian data distribution.\n\n> Is there a more intuitive summary for the proofs for Theorem 2 and 3 especially for readers (like me), not familiar with Zou et al. (2023)?\n\nPlease refer to Sec 1.2, our newly added section, for an overview of the theory:\n\n- Theoretically, maximal update parameterization suggests that, beyond a certain point, increasing the width of the neural network (while keeping data size fixed) does not further increase the critical batch size. In contrast, by analyzing a simple least-squares regression with mini-batch SGD, we provide a theoretical basis for how the critical batch size continues to scale with increasing data size. \n- In infinite width regimes, training dynamics and performance of the networks become effectively independent of the model size. Consequently, the critical batch size remains nearly invariant when scaling up the model size beyond this point, indicating that larger models do not require proportionally larger batch sizes to achieve optimal training efficiency.\n\n> How exactly does the hybrid evaluation (L968) provide more accurate information?\n\nThe evaluation is for making sure the recorded #steps required to reach a goal loss would not be overly estimated because of infrequent evaluations. So we adopt the strategy to evaluate the runs more frequently when near the end of training.\n\n> What or how are the hyperparameters (HPs) for parameter sweeps ranked or ordered (L977-979)? Does a different ordering yield potentially different results?\nWhat are the default values for other hyperparameters when tuning each of these in order?\nAssuming in this order that LR is considered to be the most important HP, why tune the least important HPs (in \u03b22 and \u03bb) for each model scale and not LR (Table 4)?\n\nNote that we have swept learning rate for various model scales as suggested by Tab.3.  We include the search details for completeness but find that ordering does not result in too many differences. For example, our newly added experiments in Fig. 10 shows that our default learning rate 3.16e-3 is consistently the best among others with different EWA decay rates. We highlight the default values in Tab.3 using bold fonts. \n\n> I may have missed it but could there be a reference for how the power law model fit is made for the values in the caption of Figure 7?\n\nThe scaling law details are included in Sec 3.2 and Sec 3.3. Appendix E also includes additional details and interpretations. \n\n> How these designs were arrived at.\n\nWe include quite a few additional experiments on understanding design choices like learning rate in the Appendix. Please let us know which parts are unclear and we can clarify further."
            }
        },
        {
            "title": {
                "value": "Response [2]"
            },
            "comment": {
                "value": "> Any reason explaining the worsening efficiency for a 1.2B model on larger batch sizes in Figure 1.b) (right)?\n\nIt\u2019s possible that constraining the data to be only 3.07B tokens would make 1.2B models harder to train using batch size $2^{13}$. \n\n> Why do we expect the rankings of different schedulers found on 151M scale would transfer to larger scales? Do we have a literature or empirical reference for it?\n\nWe do not claim that the results can transfer without proper hyper-parameter tuning. But we do consider a realistic scale, e.g. a batch size of millions of tokens, compared to previous works. Benchmarking schedulers helps justify the use of EWA can avoid setting training durations beforehand. \n\n\n> Could the paper have bit more insight into why certain model scales were picked for certain experiments, since it is not always the smallest (151M) model that was selected?\n\nIn experiments where model size is controlled, we use 302M models because that is the medium one so we can scale the data proportionally compared with compute-optimal training of 85M, 151M, 604M, and 1.2B models. This makes comparison more controlled and the model size is small enough for sweeping over hyper-parameters. \nFor Chinchilla settings and other settings where data size is controlled, we consider 5 different model sizes that can be scaled through certain modifications as shown in Tab. 2 due to computational constraints.\nFor understanding the impact of hyper-parameters, we use 151M models as they are more efficient to train.\n\n> For a study with even lesser bias or confounding factors, would it make sense to make conclusions on the critical batch size (CBS) or its scaling exponents without an EWA of model weights?\nIn a similar vein, can we see in Figure 2 what only constant LR schedules look like?\nIs cosine here with or without warmups?\n\nWe expect CBS to exist regardless of the learning rate schedule, as shown in previous work (Shallue et al., 2019; McCandlish et al., 2018). But as clarified in Sec 2.1, we hope to study CBS on the premise that other hyper-parameters are well-tuned. In other words, we don\u2019t want to arrive at an unreliable conclusion about CBS if important hyper-parameters are very sub-optimal. Moreover, cosine scheduling is also run with warm-up steps.\n\n\n> Why does Figure 2 (right) have different step lengths for the same batch size assuming similar compute budgets overall, given same model size?\nSimilar to above, why does Figure 9 all have different lengths?\n\nPlease note that we are not recording the final performance at the end of training but just the number of steps to reach the target loss indicated by the dashed line. Only the best-performing curves are shown so the lengths can vary depending on the settings.\n\n> Is the only notion of efficiency in paper as denoted by the overhead ratio metric as defined by the measure on batch size 256/512?\nFor every result seen as plot/table, does it mean there was an equivalent run made on a batch size 256/512?\n\nYes please refer to Fig. 13 for all the results by taking batch size 64 performance as the denominator. As shown in Fig.13, because B_{opt}=256 is in the linear scaling regime, we use it to determine the target loss throughout the paper.\n\n> Would Figure 3 result hold for larger models with bigger context sizes? (I am not asking for an experiment at such scales but a cheaper experiment that could serve as proof-by-contradiction)\nHow does the model size change when Figure 3 writes about a model of size 151M but with context size ranging from 512-4096?\nAre model sizes calculated without the embedding parameters?\n\nAs this is not our main focus of the paper but justifying the design decisions of using a fixed context length of 512 throughout the paper, we didn\u2019t consider long-context models and larger models. Model sizes are calculated with non-embedding parameters.\n\n> Could the 20% and the resulting 5 in Section 3.2 be more generalized in the formalism?\nCould the readers have a clearer reference/intuition for the 20% overhead?\n\nYes the overhead level can be generalized according to specific practical settings and we found that these does not affect the fitted scaling laws too much. For example, when having the following 10%, 50% overhead, we get data scaling laws for 302M models of similar form as\n\n10%: B* = 20.67 * D^(0.48)\n\n20%: B* = 22.91 * D^(0.47)\n\n50%: B* = 30.50 * D^(0.44)\n\n> Could L318-319 have more support?\n\nSure, we will add reference to Fig. 1 left, which shows that for 1.2B models, up to 2^11 batch sizes, linear scaling regimes still hold."
            }
        },
        {
            "title": {
                "value": "Response [1]"
            },
            "comment": {
                "value": "Thank you for your constructive feedback. We have addressed each of your points as follows:\n\n> different seeds, design choices\n\nThanks for the feedback. Due to computation constraints, we did not sweep over multiple seeds but focused mainly on different hyper-parameters. But from our variance measure experiments in Appendix D, our runs are pretty consistent across seeds. \nPlease note that we have included many results in the Appendix to study the effects of major hyper-parameters. \n\n> hyper-parameters are done with small-scale 151M models\n\nDue to computation constraints, we use small-scale 151M models to study some hyper-parameters including context lengths, momentum $\\beta_1$, scheduling and learning rate studies. However, for important ones that can lead to performance boost like EWA decay rate $\\tau$ and $\\beta_2$, we swept all the values as indicated in Tab.3.\n\n> A limited grid search over learning rate\n\nNote that we have swept learning rate for various model scales as suggested by Tab.3.  We found that under our constant+EWA strategy, learning rate does not affect the steps required to reach a target loss too much. In Appendix Fig. 10, we include more studies to understand the the impact of learning rate. We found that our default value 3.16e-3 is stable across the three regimes we considered. To improve model performance with a fixed learning rate, increasing the training duration may require a larger EWA decay rate for optimal results.\n\n> Connections between the outcome of the theoretical proofs and empirical results? Is there a way to connect the toy regression experiment with the scaling law exponent fits?\n\nPlease note that our theoretical proofs corresponds to two controlled settings we consider: (we included the following into Intro Sec 1.1, Sec 1.2 as well)\nEmpirically: \n\n1) If we scale up training duration D while keeping N fixed (Figure 1, middle), the critical batch size increases to a similar degree. \n\n2) However, we find that CBS remains nearly invariant when scaling up N while keeping D fixed (Figure 1, right), suggesting that CBS weakly depends on model size N but more strongly depends on data size D.\n\nCorresponding to the two cases above, theoretically: \n\n1) In infinite width regimes [Yang et al, 2021], training dynamics and performance of the networks become effectively independent of the model size. Consequently, the critical batch size remains nearly invariant when scaling up the model size beyond this point, indicating that larger models do not require proportionally larger batch sizes to achieve optimal training efficiency. \n\n2) Consider mini-batch SGD with $D$ samples in the least square problems under power-law source and capacity conditions. The CBS, which enables mini-batch SGD to achieve the minimal expected excess risk while ensuring the fastest possible serial runtime, is given by $B^*(D) = \\Theta(D^c)$, where the exponent $c\\ge 0$ is determined by the exponents of the source and capacity conditions. In the regime where the variance error tends to be dominant, we have $c>0$, indicating CBS grows with data size.\n\n> Discrepancy of batch size 256 or 512 as the batch size for calculating overhead ratios\n\nPlease note that we use $B_{\\text{opt}}=256$ to determine the target loss consistently, but report the relative number of steps with 512 batch size results as the denominator because we are only interested in large batch size regimes to build intuitions on CBS. We didn\u2019t claim they are the same.\n\n> Should likely be b > a in L1299. Typo on L366: perhaps should be \"They\" -> \"Then\". Discrepancy in Fig. 5 caption $C_{chin}$. Figure 10 mentions context length. Plot sizes.\n\nThanks for pointing these out. We have greatly revised the draft to fix those issues.\n\n> Figure 4.a doesn't mention which model width or depth.\n\nPlease refer to Tab. 5 for the concrete setup, as mentioned in the main text.\n\n> Recommended to refer sections/tables/figures in the Contributions list in Section 1.\n\nThank you for the suggestion. We added Sec 2.1 and 2.2 accordingly for better introduction of our contributions.\n\n> Vertical lines in Table 4 separating different model sizes. Parantheses in L1119-1122 would enable easier parsing for readers.\n\nThank you for the suggestion and we would include the changes in the next draft."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "Thank you for your constructive feedback. We have addressed each of your points as follows:\n\n> The experimental scope is limited to models up to 1.2B parameters trained on C4. However, given the careful experimental design and clear theoretical analysis, I do not believe that these impact the validity of the findings.\n\nThank you for the note. Due to computational constraints, we cannot extend our study to a larger scale. But we have carefully controlled for different factors in the settings we considered.\n\n> It would be helpful to have a dedicated section discussing the limitations of both the theoretical analysis and empirical findings (e.g., Gaussian data distribution).\n\nThanks for the suggestion. Please note that we do include the discussion of our setting is limited to C4 data in the Conclusion section. We would include the following: The theoretical analysis assumes Gaussian data distribution and infinite-width neural networks, which may not hold for real-world, non-Gaussian, high-dimensional data or finite models, potentially affecting the generalizability of CBS scaling laws. Empirical findings are restricted by limited hyperparameter sweeps and computational resources, which may prevent a full exploration of optimal configurations, particularly across varied tasks and larger architectures. \n\n> It would be nice to use instead of * (asterisk) in line 135. The color contrast in Figure 9 can be improved.\n\nThank you for the suggestions and we will include your feedback into the final version."
            }
        },
        {
            "title": {
                "value": "Response [2]"
            },
            "comment": {
                "value": "> For most of the paper, you sweep the batch size between  2^6  and  2^13 with the exception of Figure 3, where the batch size is between  2^16 and  2^22. Is this because in this Figure you count the number of tokens, so practically batch size * tokens per batch?\n\nPlease note that as we explain in Sec 2.1, the batch size should be multiplied by context length to get the number of tokens per batch. So the range is still calibrated and we have made the x-axis to be **Batch Size (#Tokens)** instead of just **Batch Size**.\n\n> Did you try checking how close your thus predicted CBS tracks the true CBS for \"unseen data points\", i.e. test its predictive power?\n\nDue to computation constraints, we were not able verify the forecasted CBS.\n\n> In the concluding remarks you write \"Our findings contribute insights into [...] particularly highlighting the role of hyper-parameters such as learning rate scheduling and optimizer settings\" (line 530). What \"optimizer settings\" are you referring to?\n\nWe are referring to Adam optimizers including $\\beta_1, \\beta_2$.\n\n> Nits and typos.\n\nThank you for your careful read and we have fixed all the typos. We updated the draft significantly to address the formatting issues. Please double check the current versions.\n\n> Doesn't scaling from 151M to 302M double the depth, and therefore Figure 1 involves multiple ways of scaling up models?\n\nPlease note that by default we use the scaling configurations in Tab. 2. So all 302M models except for Fig. 4 are scaled from 151M ones through enlarging depth instead of width."
            }
        },
        {
            "title": {
                "value": "Response [1]"
            },
            "comment": {
                "value": "Thank you for your constructive feedback. We have addressed each of your points as follows:\n\n> \u201cEWA consistently improves model training efficiency\u201d\n\nWe included our code in Supplementary Materials. In the README.md or the assets folder, the experiments for cosine decay and WSD for various training lengths T. We can see that this is indeed the optimal training length for both of them. Note that these curves are at extremely high batch sizes, that\u2019s why the decay indeed hurts as the noise induced by variance in the iterates is pretty small. We will still reword the statement to hold only in our setup, but we have done training length sweeps for cosine and WSD (for the 150m model), and we are comparing only after these sweeps.\nNote that we include the concrete sweeping settings in the Appendix. The decay phase is set to be last 0.1, 0.2, 0.3 of the training durations for all batch sizes. In the supplementary assets folder or README.md, we show that constant+EWA is competitive with Cosine and WSD with various decay ratios. fig. 9 (c) (d) (e) also shows that even if WSD reaches the target loss right at the end of training, it\u2019s still comparable with constant+EWA. \n\n> The lines start to diverge a bit more for larger batch sizes, so is it possible that this is more pronounced for other settings (e.g. larger models, more training data, etc.)?\n\nYes, this may be possible. However, based on Figure 2.3, we can expect that even if the curves diverge, that might happen beyond the critical batch size.\n\n> How the results and conclusions would change if one varies this parameter, e.g. to 10% or 50%\n\nPlease note that it\u2019s a design choice for choosing the overhead level depending on concrete training setups. The overhead level can be generalized according to specific practical settings and we found that these do not affect the fitted scaling laws too much. For example, when having the following 10%, 50% overhead, we get data scaling laws for 302M models of similar form as\n\n10%: B* = 20.67 * D^(0.48)\n\n20%: B* = 22.91 * D^(0.47)\n\n50%: B* = 30.50 * D^(0.44)\n\n> width vs depth scaling.\n\nPlease note that those settings are Chinchilla compute-optimal settings where the data size is scaled up proportionally with the model size. So large models are trained on more tokens. This means still increasing the scale via enlarging width or depth has similar CBS scaling behaviors, which we found in the next section to be mostly dependent on the data size.\n\n> In the legend of Figure 1b (left), what is the number in parenthesis? Is it the (relative) number of training samples?\n\nIt is the token size, denoted by how many times or proportions the Chinchilla tokens are used. We revised the plot to improve clarity.\n\n> Plot size, font size, white space.\n\nPlease refer to the current PDF. We have changed the formats of most plots substantially. \n\n> Linear scaling line as well as the 20% region and highlight the critical batch sizes, e.g. by a star marker?\n\nPlease refer to current Fig. 5 for an illustration. The corresponding definition is on the left. Also, Note that connecting diagonal lines of squares in each plot can lead to the linear scaling regime.\n\n> Fig. 2 plot formats.\n\nThank you for the reminder. We have updated the plot with new, more distinctive marker symbols to enhance visibility.\n\n> In line 110 you mention that \"traditional learning rate decay strategies typically require predefining the total training duration\" and you mention Defazio et al., (2024). However, isn't Schedule-Free a counter-example? Schedule-Free is also using a, at least to me, a relatively similar approach to your \"constant+EWA\" strategy, no (e.g. comparing the equation in line 158 to Eq. (5) by Defazio)? Could you elaborate on how your method differs from Schedule-Free?\n\nSorry for the confusion. What we mean is how to get rid of fixed training duration is an actively studied problem so we cite Defazio et al., 2024. We have revised the narrative for clarity.\nMathematically they are equivalent, overall ScheduleFree SGD can be understood as a accelarated SGD followed by weight averaging. This can be shown by denoting $y_t =(1-\\beta) z_t+\\beta x_t, z_{t+1}  =z_t-\\gamma g\\left(y_t\\right), x_{t+1} =\\left(1-c_{t+1}\\right) x_t+c_{t+1} z_{t+1} $. Here, $y_t$ denotes the current weights of the model (note that the gradient is evaluated on $y_t$ ), while $x_t$ are the weights that will be used for evaluation. By recursively expanding $x_t$ to get that $x_t$ is an exponential average of $y_t$ for a constant $c_t$. However, ScheduleFree SGD couples the coefficients of momentum and weight averaging, which we decouple."
            }
        },
        {
            "summary": {
                "value": "The paper investigates the scaling behavior of critical batch size in the pre-training of autoregressive language models.\nThey first define the critical batch size (CBS) as the point where increasing the batch size no longer leads to significant gains in computational efficiency (>20% overhead when doubling the batch size vs. a linear scaling). They then perform experiments to determine the CBS of autoregressive Transformer-based language models of varying scales, finding that CBS scales primarily with the size of the training dataset rather than model size. They provide theoretical support for this finding by studying infinite-width limits of neural networks and infinite-dimensional least squares regression problems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper provides an interesting finding that the critical batch size scales mostly with data set size, and is largely invariant to model size. This is a relevant and, to my knowledge, novel insight.\n- The paper considers models ranging from 85 million to 1.2 billion parameters and thus covers a reasonably large domain of models.\n- I really liked the highlighted practical takeaway blocks throughout the paper, which made it easy to understand, well-structured, and accessible."
            },
            "weaknesses": {
                "value": "Some of the takeaways seem to me a bit too bold or not backed by enough evidence for the given claim.\n\n- For example, in Section 2.2, they compare the efficiency of learning rate schedules across batch sizes by comparing the number of steps to achieve a given target validation loss. They conclude that \"EWA consistently improves model training efficiency. [...] while outperforming Cosine for large batch sizes [...] and even with appropriate learning rate decay, [Cosine] underperforms our constant+EWA strategy in large-batch settings.\" (line 188). However, looking at Figure 2b), we can see that the training duration of cosine was chosen inefficiently. It continues to decay well beyond the target validation loss, achieving the target at roughly 50% of its decay schedule. It is also clear to see for the WSD schedule, which hits the target loss shortly after starting its decay, i.e. it is still halfway in its decay phase when crossing the threshold loss. I believe making a statement like \"EWA consistently improves model training efficiency\" requires a more rigorous empirical analysis. For instance, Schedule-Free [1] suggests a similar running average strategy to improve efficiency, providing a much more comprehensive and rigorous analysis.\n- Similarly, Section 2.3 claims that different context lengths have similar CBS, based on a single (comparably small) model & dataset. It seems that the lines start to diverge a bit more for larger batch sizes, so is it possible that this is more pronounced for other settings (e.g. larger models, more training data, etc.)?\n- All claims are made for the definition of CBS using 20% overhead. The authors state that \"20% can be replaced by any other suitable measure of increase from linear scaling\". I wonder how the results and conclusions would change if one varies this parameter, e.g. to 10% or 50%. For example, what would a plot look like of the CBS per model or data size as a function of the overhead?\n\n[1] Aaron Defazio, Xingyu Alice Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, Ashok Cutkosky; \"The Road Less Scheduled\"; arXiv 2024; <https://arxiv.org/abs/2405.15682>"
            },
            "questions": {
                "value": "- A central takeaway of the paper is that \"CBS remains invariant when scaling up N [model size]\" (line 93). However, isn't this partially contrasted by the results in Section 2.4, where you write \"Figure 4 shows that increasing depth and width [and thus model size] can both slightly increase the CBS\" (line 264)?\n- I found the plots to sometimes be a bit confusing or less accessible. Some suggestions:\n  - Could you add the linear scaling line as well as the 20% region?\n  - Could you highlight the critical batch sizes, e.g. by a star marker?\n  - Nit: The font sizes are inconsistent between subplots, e.g. the x-axis labels between Figure 1a and the subplots of Figure 1b.\n  - In the legend of Figure 1b (left), what is the number in parenthesis? Is it the (relative) number of training samples?\n  - The lines are sometimes hard to read. E.g. in Figure 2a, the Constant+EWA line is very bright. Similarly the lines in Figure 2b. I also had trouble distinguishing the shades of blue, e.g. in Figure 1. But it does look pretty :)\n  - Nit: The white spaces in Figure 4 are a bit weird. There is only a small space between the subcaption of subplot (a) and much more white space between the subcaption (b) and its subplot.\n- In line 110 you mention that \"traditional learning rate decay strategies typically require predefining the total training duration\" and you mention Defazio et al., (2024). However, isn't Schedule-Free a counter-example? Schedule-Free is also using a, at least to me, a relatively similar approach to your \"constant+EWA\" strategy, no (e.g. comparing the equation in line 158 to Eq. (5) by Defazio)? Could you elaborate on how your method differs from Schedule-Free?\n- For most of the paper, you sweep the batch size between $2^6$ and $2^{13}$ with the exception of Figure 3, where the batch size is between $2^{16}$ and $2^{22}$. Is this because in this figure, you count the number of tokens, so practically batch size * tokens per batch?\n- You provide a scaling law fit of the CBS. Did you try checking how close your thus predicted CBS tracks the true CBS for \"unseen data points\", i.e. test its predictive power?\n- In the concluding remarks you write \"Our findings contribute insights into [...] particularly highlighting the role of hyper-parameters such as learning rate scheduling and optimizer settings\" (line 530). What \"optimizer settings\" are you referring to?\n- Nits:\n  - Line 120: I believe the \"Refer\" is superfluous.\n  - Line 157: There is probably a closing parenthesis missing after the second citation.\n  - Line 185: Typo for \"maximum\".\n  - Figure 3a doesn't need to be a subplot as there is no other subplot.\n  - Line 260: This sentence seems grammatically weird to me \"As the main result in Figure 1 only involves a single way for scaling up models [...]\". I might also have misunderstood the sentence. For example, doesn't scaling from 151M to 302M double the depth, and therefore Figure 1 involves multiple ways of scaling up models?\n  - Line 271: \"Compute-optimal\" should probably be lowercase.\n  - Line 288: Typo with \"achieved\" and \"batch size\".\n  - Line 351: \"chinchilla\" should probably be capitalized.\n  - Line 366: \"They\" -> \"Then\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies how the critical batch size (CBS) - the threshold beyond which increasing batch size causes diminishing return - scales with the model and data size for pre-training language models. The authors conduct careful experiments on models ranging from 82 million to 1.2 billion parameters and show that the CBS slightly scales with data size rather than the model size (remains invariant to model size). The authors also provide theoretical justifications for this phenomenon using infinite-dimensional least squares regression and the infinite-width limits of neural networks. These findings have important implications for designing efficient pre-training strategies."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well-written with clear organization. This work would be a valuable contribution to the ICLR community. I especially appreciated the \u201ckey takeaways summary\u201d after each section.\n- The experimental design is rigorous; for example, decoupling various hyperparameters makes the claims more convincing.\n- Detailed experimental procedures are provided in Appendix D.\n- The formalization of CBS (beyond [1] \u201cAn empirical model of large-batch training\u201d) would be helpful in the literature. The key findings that the CBS scales slightly with data size (and stays invariant to model size) are interesting. These insights have important implications for efficient pre-training strategies."
            },
            "weaknesses": {
                "value": "- The experimental scope is limited to models up to 1.2B parameters trained on C4, which may not fully capture scaling behaviors at larger scales (e.g., models with over 50B parameters). On a similar note, key ablation studies are primarily conducted on smaller models (with C4). However, given the careful experimental design and clear theoretical analysis, I do not believe that these impact the validity of the findings.\n- It would be helpful to have a dedicated section discussing the limitations of both the theoretical analysis and empirical findings (e.g., Gaussian data distribution).\n- However, I believe that the work has substantial limitations that would prevent me from recommending acceptance."
            },
            "questions": {
                "value": "- (Minor) It would be nice to use $\\times$ instead of * (asterisk) in line 135.\n- (Minor) The color contrast in Figure 9 can be improved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper revisits the role of batch size in scaling language models during pre-training. The core finding is that the critical batch size needs to scale alongside the model. However, both the critical batch size (CBS) and model scale (specifically, model width) show diminishing returns when scaled under a given training budget. The authors demonstrate and conclude that CBS scaling is dependent on data scale but invariant to model scale. Various hyperparameter ablations provide additional insights into relative pre-training performance efficiency."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is well-written and largely easy to follow.\n2. The related literature covers the important papers in the topic well.\n3. Formalizations and hypotheses are clearly outlined and help understand results better.\n4. Though personally I would like to reconsider its exact design and placement, the _Takeaway_ block was helpful while reading the paper first time.\n5. The model scales reported in experiments are adequate in applying the insights to large-scale pre-training.\n6. Formalizing the notion of critical batch size (CBS) through the 20% overhead assumption is novel and seemingly useful.\n7. Considering various hyperparameters ablated across selected model scales is important and welcome in a scaling-related paper."
            },
            "weaknesses": {
                "value": "1. Some lower scale experiment with repetitions over different seeds to show the robustness of the findings (laws, exponents) and insights (data dependence and model scale invariance).\n2. The work is mostly a benchmarking study with main contribution relying on the hypothesis constructed and how the experiment for it is setup, which therefore leaves more room for explaining some of the design choices, especially with model scale, hyperparameters (see, Questions below for examples).\n3. Section 3.3 mentions how different hyperparameters were adjusted _to achieve optimal performance_ (at 302M), however, the grid search (for ablations) were done on a different model scale (151M), and thus is unclear what were the different HP settings considered in fitting points for the power law.\n4. Theorem 1's proof by existence could do with more details and support, i.e., it is unclear how _training iterations t_ related to models of different width (thereby, scales), possibly different batch size scales and naturally different compute budgets under Chinchilla-optimality (see, Questions).\n5. Despite leveraging theoretical insights from muP, there is only a limited grid search over learning rate values over the smallest available scale while that is apparently used as-is for even the larger model trainings, i.e., hyperparameters are not suitably scaled (this is unclear overall).\n6. The outcome of the theoretical proofs and empirical results appear independent and do not knit the contributions as well.\n    * Is there a way to connect the toy regression experiment with the scaling law exponent fits?\n\n\nMinor issues/fixes:\n1. Discrepancy of batch size 256 or 512 as the batch size for calculating overhead ratios\n    * L52, Fig. 1,3,4,6,8 y-axes, L370, L888.\n2. Should likely be `b > a` in L1299.\n3. Typo on L366: perhaps should be \"They\" -> \"Then\"\n4. Discrepancy in Fig. 5 caption where $C_{chin}$ comes to be around 6 as per the equation given for $B^*$.\n5. Figure 4.a doesn't mention which model width or depth.\n6. Figure 10 mentions _context length_ but has nothing to show for it.\n\n\n\nNitpicks:\n1. Irregular and inconsistent plot sizes.\n2. Recommended to refer sections/tables/figures in the Contributions list in Section 1.\n3. Vertical lines in Table 4 separating different model sizes.\n4. Parantheses in L1119-1122 would enable easier parsing for readers."
            },
            "questions": {
                "value": "1. Could the paper have bit more insight into why certain model scales were picked for certain experiments, since it is not always the smallest (151M) model that was selected?\n2. Any reason explaining the worsening _efficiency_ for a 1.2B model on larger batch sizes in Figure 1.b) (right)?\n3. For a study with even lesser bias or confounding factors, would it make sense to make conclusions on the critical batch size (CBS) or its scaling exponents without an EWA of model weights? \n    * In a similar vein, can we see in Figure 2 what only constant LR schedules look like?\n    * Is cosine here with or without warmups?\n4. Why does Figure 2 (right) have different step lengths for the same batch size assuming similar compute budgets overall, given same model size? \n    * Similar to above, why does Figure 9 all have different lengths?\n5. Why do we expect the rankings of different schedulers found on 151M scale would transfer to larger scales? Do we have a literature or empirical reference for it?\n6. Is the only notion of _efficiency_ in paper as denoted by the overhead ratio metric as defined by the measure on batch size 256/512?\n    * For every result seen as plot/table, does it mean there was an equivalent run made on a batch size 256/512?\n7. Would Figure 3 result hold for larger models with bigger context sizes? (I am not asking for an experiment at such scales but a cheaper experiment that could serve as proof-by-contradiction)\n    * How does the model size change when Figure 3 writes about a model of size 151M but with context size ranging from 512-4096?\n    * Are model sizes calculated without the embedding parameters?\n8. Could the 20% and the resulting 5 in Section 3.2 be more generalized in the formalism?\n    * Could the readers have a clearer reference/intuition for the 20% overhead?\n9. Could L318-319 have more support?\n10. Does Figure 5 have empirical backing regarding the predicted CBS and the formalism of staying within the 20% margin of steps for similar loss?\n    * Or if the decoupled prediction for batch sizes hold _reliably_ for different scales or ablations such as architecture, datasets, hyperparameters?\n11. How exactly are the tuning decisions undertaken especially in Section 3.3 (example, L371-372)?\n12. In Theorem 1, is it expected or meant to be $w_2 > w_1$? If so, then there is a typo and makes a significant difference to the interpretation.\n13. What is the significance of _t_ in Theorem 1? Should we compare losses under similar tokens for two vastly different model scales, especially following Chinchilla?\n14. What is the specification or how is $\\mathbf{H}$ defined in Section 4.2?\n15. Is there a more intuitive summary for the proofs for Theorem 2 and 3 especially for readers (like me), not familiar with Zou et al. (2023)?\n16. How exactly does the _hybrid evaluation_ (L968) provide _more accurate_ information?\n17. What or how are the hyperparameters (HPs) for parameter sweeps ranked or ordered (L977-979)? Does a different ordering yield potentially different results?\n    * What are the default values for other hyperparameters when tuning each of these in order?\n    * Assuming in this order that LR is considered to be the most important HP, why tune the least important HPs (in $\\beta_2$ and $\\lambda$) for each model scale and not LR (Table 4)?\n18. I may have missed it but could there be a reference for how the power law model fit is made for the values in the caption of Figure 7?\n\n\nOverall, the paper reads nice and does touch on an important and often under-studied aspect in scaling literature.\nHowever, the paper writing and presentation raises some eyebrows with really big plots, white spaces used, and a theory which feels like a sub-paper than something that brings the story together.\nGiven the expensive space of empirical experiments, the authors did well to consider multiple design choices and study them. \nUnfortunately, that also opens up more questions on how these designs were arrived at and that clarity would be my primary criticism.\nOne other thing would be the lack of _a_ clear takeaway in terms of _how_ a pre-training practitioner could use the insights from this paper when scaling (model or data).\n\n\nScores may be increased depending on suitable responses to most of the points raised above.\nThank you for the paper, it was a nice read overall.\n\n\nPS: I was unaware of Zou et al. 2023 and have not verified the proof on Pages 22-24."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors reassess the notion of critical batch size (CBS) in the context of language model training, and investigate the impact of scale on CBS. They find that the CBS does not scale with model size in majority, but with data size, and highlight roles of optimizer choices, where they find exponential averaging with a constant LR to be performant, matching or outperforming cosine or WSD."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "First of all, I thank the authors for looking into this problem \u2014 I believe the question of CBS is both extremely relevant and understudied. The paper is written in a clear manner, the experiments are very extensive, and the authors additionally provide theoretical studies. The work could be a starting point for further studies, e.g. going beyond Chinchilla optimal points."
            },
            "weaknesses": {
                "value": "While I very much appreciate the topic and investigation of the paper, I unfortunately have to be very critical of the experimental evaluation. \n\n- First, the authors note in the Appendix that they disable weight decay of AdamW, without justification; this means effectively only studying Adam and not AdamW. However, weight decay is not only a major part of modern large scale training (see e.g. Chinchilla Fig A7 https://arxiv.org/pdf/2203.15556 or Wortsman et al. (2024) https://openreview.net/pdf?id=d8w0pmvXbZ), but it also strongly changes the training trajectory, where disabling it leads to a practical decrease of the effective learning rate even for a constant schedule; see e.g. Kosson et al. https://arxiv.org/pdf/2305.17212. Therefore, it is unclear how the findings would generalize to actual practical settings (proper baselines), and assuming the same results is misleading.\n- The authors find a constant LR + EWA to be extremely performant. I have tried to replicate their results with the same hyperparameters (including disabled WD) in the same setting of LLM pretraining and fail to obtain the same results. While EWA can give a slight boost, it is far from the cooldown loss; even more, using e.g. a decay of 0.99 or higher results in a similar curve to the one of 0.9999 in Fig. 8 (increase in loss, but much earlier in training) and then EWA is *much worse* than the original model. Since the authors have not provided code, it is unclear to me where this discrepancy is coming from \u2014 I repeat this point as a question below.\n- For most experiments, the schedule free optimizer performs surprisingly bad; if well tuned, I would think it should be at least as good or below the stable phase of WSD from my experience.\n\nThe combination of these points makes me concerned of some of the main empirical takeaways of the paper. To be clear, I believe the paper has much merit, and I do not want to reduce it to these concerns. However, I think addressing them requires either rerunning most experiments or major rewrites."
            },
            "questions": {
                "value": "I mention the main points in the section above, but repeat them here as questions (with some others):\n\n- Why did the authors choose to disable weight decay?\n- How did you use EWA in detail, or is there a specific implementation trick that was required?\n- Did you tune SFO with the same sweeps as Adam?\n- Why are the warmup steps chosen between 0.15, 0.25 and 0.35? These are quite large fractions, whereas in practice the warmup is often a very minor part of training (e.g. less than 5%).\n\nI would be very happy to engage in a discussion about these points. More broadly, I hope the authors see my comments as not only critical but encouraging.\n\nAnother minor comment: I think the choice of colors for the plots is not ideal, as the contrast is very low and it makes it hard to read and compare lines (especially for people with color vision deficiency)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies how a compute-optimal batch size, the critical batch\nsize (CBS) is influenced by scaling the amount of data or model\nparameters in a Transformer-based language modeling objective. Most\nimportantly, the authors show a correlation between the training\nduration and CBS, both empirically and theoretically. Importantly, the\nexperiments also show that model size does not have an effect on CBS.\nFinally, a secondary but interesting part of the experiments concerns\nallowing to change the training duration mid-training. It is shown\nthat a simple constant schedule combined with exponential weight\naveraging at the end of training outperforms both \"standard\" cosine\ndecay and the recently proposed warmup-stable-decay (WSD) schedule.\n\nThe empirical study also includes a controlled setting to study the\neffect of width- vs. depth-scaling on the CBS (the finding being that\nboth scaling methods influence the CBS similarly). Finally, the\nauthors additionally underline their empirical findings using\nNTK/tensor programs and Gaussian linear regression theory."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "For their experiments, the authors used C4, a de-facto standard NLP\nresearch dataset, which makes interpretation and reproduction of the\npaper of the paper easier.\n\nA particularly strong section is that of relevant work, in which\nvarious important related papers are summarized well.\n\nI also really enjoy the \"takeaway\" boxes as section summaries."
            },
            "weaknesses": {
                "value": "The paper heavily relies on Chinchilla scaling theory and\nTransformer-decoder training with the language modeling objective. It\ndoes not account for settings which use fewer training steps than\nChinchilla-optimal training and does not study other data sets or\nmodels. This means it is difficult to extrapolate the results to the\ngeneral domain of deep learning.\n\nI would personally have preferred use of a more standard \"dynamic\nhorizon\" schedule such as WSD for the experiments in order to improve\ninterpretability/comparability with regard to other research. That\nsaid, the finding regarding constant+EWA quality is important.\n\nWeight decay was disabled for the experiments, which is hardly done in\npractice."
            },
            "questions": {
                "value": "### Please address\n\nIn Proof of Theorem 1 (page 8, line 418-419), it is stated that \"the\ntrajectory of the network approaches a limit as width tends to \u221e\". I\nwould wish for what exactly is meant by \"trajectory\" to be defined\nmore clearly. E.g., is it the trajectory of the weights during\ntraining? Also, please cite a source for this statement.\n\nAppendix D, Evaluation data size and frequency (page 18, line 964ff):  \nWhich which model after what training have these evaluation variance\nnumbers been obtained?\n\n### Minor comments\n\nPage 3, line 158:  \nPlease describe the meaning of \u03b8.\n\nPage 8, line 416:  \n$R(M, t)$ denotes the loss of network $N$ at time $t$.  \n-> $R(M, t)$ denotes the loss of network $M$ at time $t$.\n\nIn the experiments, ideally, the optimizer's \u03b5 hyperparameter would\nalso have been scaled with the model size\n(https://arxiv.org/abs/2309.14322)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}