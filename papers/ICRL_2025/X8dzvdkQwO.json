{
    "id": "X8dzvdkQwO",
    "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
    "abstract": "In the era of large language models (LLMs), detecting pretraining data has been increasingly important due to concerns about fair evaluation and ethical risks. Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data. In this paper, we first explore the benefits of unseen data, which can be easily collected after the release of the LLM. We find that the perplexities of LLMs perform differently for members and non-members, after fine-tuning with a small amount of previously unseen data. In light of this, we introduce a novel and effective method termed Fine-tuned Score Deviation (FSD), which improves the performance of current scoring functions for pretraining data detection. In particular, we propose to measure the deviation distance of current scores after fine-tuning on a small amount of unseen data within the same domain. In effect, using a few unseen data can largely decrease the scores of all non-members, leading to a larger deviation distance than members. Extensive experiments demonstrate the effectiveness of our method, significantly improving the AUC score on common benchmark datasets across various models.",
    "keywords": [
        "Large language models",
        "Fine-tuning",
        "Pretraining data detection"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose Fine-tuned Score Deviation (FSD), a novel and effective approach that improves the detection capabilities of current methods for pretraining data detection.",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=X8dzvdkQwO",
    "pdf_link": "https://openreview.net/pdf?id=X8dzvdkQwO",
    "comments": [
        {
            "summary": {
                "value": "The paper addresses the problem of detecting prestrainign data from LLMs. They notice that when fine-tuning an LLM on a few non-members selected by a cut-off date, the perplexity on other non-members (also based on the same cut-off date) decrease. Based on this observation, the authors propose a new method (Fine-tuned Score Deviation; FSD) to improve prior membership inference attacks (MIA) by adjusting the scores based on this fine-tuning phenomenon. They conduct experiments on 4 cut-off based benchmarks: WikiMIA, BookMIA, ArXivTection and BookTection and show significant gains wrt the baselines. The specific questions they tackle are:\n\n1. Can FSD improve the performance of current MIA methods?\n2. How many samples does the method need for fine-tuning?\n3. Does FSD work across different model sizes?\n4. Does FSD work with members for fine-tuning, instead of non-members?\n5. Does FSD work for any fine-tuning method?"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The idea is simple and intriguing, enlarging the difference between members and non-members could boost any MIA method.\n2. The idea works for any MIA method."
            },
            "weaknesses": {
                "value": "The evaluation has major flaw. It only uses benchmarks that separates members and non-members based on cut-off dates. As Duan et al., 2024, Das et al., 2024, and Maini et al., 2024 show, this is fundamentally wrong because it introduces a temporal shift that bias the benchmark. These works have consistently proved the need to avoid evaluating MIA based on cut-off dates. This paper acknowledges these works on lines 458-459 and questions whether the results of this ICLR submission are influenced by this temporal shift. However, instead of following the recommendations from these papers for a correct evaluation, they simply remove timestamps. This is not enough as Duan et al., (2024) shows. Duan et al., shows that the cut-offs introduces changes in the n-grams. An IID split have a higher n-gram overlap between members and non-members than the temporal split. Therefore, even when removing the timestamps in the plain texts, the n-gram distribution is different, making the evaluated method classify temporal data rather than membership. Moreover, the authors acknowledge this possibility in lines 535-537 but they do not follow the recommendations of the papers mentioned above, which are to evaluate on for example Pythia with data from the Pile."
            },
            "questions": {
                "value": "* Please conduct your experiments on an IID setup (see the works of Duan et al., 2024, Das et al., 2024, and Maini et al., 2024 that you cite; eg: using Pythia use as members the training set and as non-members the dev and test sets) so that we can clearly see the effectiveness of your work.\n* It is possible that the results of Figure 4 are explained by the temporal bias of the evaluation. Training more on new data might make the method identify better the n-gram distribution of the documents after the cut-off. We can reject this hypothesis by evaluating on an IID setup."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new method for improving the accuracy of pretraining data detection in LLMs, called Fine-tuned Score Deviation. they shows that fine-tuning models on a small set of non-member data increases the deviation between score distributions of seen and unseen data. They validate this claim via experiments on WikiMIA, BookMIA, ArXivTection and BookTection datasets. FSD consistently improves the performance of existing detection methods based on scoring functions like perplexity and Min-k%."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Very interesting observation, and notable improvement in pretraining data detection accuracy. I think this paper has a clearly defined objective, and interesting empirical results.\n\nThe results are strong, showing substantial improvements in AUC and tpr at low fpr across the selected datasets and models. FSD helps improving existing score-based pretraining data detection methods."
            },
            "weaknesses": {
                "value": "1. Have you experimented on the MIMIR dataset? This dataset seems to be more challenging, and authors claim this is because seen and unseen examples are not from different temporal distributions. I am very interested to see how this method helps MIA on MIMIR dataset.\n\n2. They have not explained or given intuition about why finetuning increases the gap between distributions.\n\n3.  The method requires fine-tuning the LLM, which makes it more expensive compared to existing methods. If authors find some intuition about why finetuning has this impact, they may find less expensive methods."
            },
            "questions": {
                "value": "1. Do you have intuitions or theoretical understanding about why finetuning increases the gap between distributions?\n\n2. What is the role of the finetuning technique you use? Is there a dependency on LORA?\n\n3. How sensitive is FSD to using unseen data from a loosely related domain impact detection performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper tackles the issue of detecting pretraining data. It introduces a method where a pretrained model is fine-tuned with non-member data from the same domain. The goal is to increase the gap in scores like perplexity between member and non-member data, making it easier to spot training data."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Clear introduction to the problem and motivation.\n* The method is well explained and supported with intuitive examples, such as Figure 2.\n* Comparisons with other relevant baselines, and showing significant improvement over them.\n* Thorough experiments, including ablation studies that address additional research questions.\n* Creative use of non-member data."
            },
            "weaknesses": {
                "value": "* The paper assumes access to unseen data in the same domain but doesn\u2019t define 'domain' clearly. Could the authors explain how they handle differences between domains, especially if vocabularies differ a lot, and how they decide what data counts as 'same domain'?\n* *\"To the best of our knowledge, our method is the first to utilize some collected non-members in the task of pretraining data detection\"* - There\u2019s limited information on how to collect non-member data, which seems a key aspect of your method.\n* The approach assumes access to both unseen data and model probabilities for each token, compared to other black-box assumption methods."
            },
            "questions": {
                "value": "* Could you explain more about the datasets, like how they\u2019re labeled and how well they fit the models used? Is there a class imbalance in any of the datasets?\n* Should fine-tuning happen for each example $x$ (or each group of examples $x_i$ within the same domain)? If so, that might be a big drawback.\n* Fine-tuning can be delicate, and it\u2019s easy to go past the optimal point. An experiment on how sensitive this method is to fine-tuning parameters (e.g., number of epochs, LoRA rank) would be useful.\n* For a more diverse test set, what happens if we fine-tune once on a mix of domains?\n* If non-member data in the specific domain is not available, is there any alternative in the general frame of this method?\n\n\n\n**Minor suggestions:**\n* If terms like \"Data Contamination\" or \"Membership Inference Attack\" are central, it might help to introduce them earlier (e.g., in the Introduction or Background). If not, why are they in the Related Work section?\n* Adding an ethics statement could underscore the method's role in detecting sensitive information used in LLM training, addressing the ethical significance of such methodologies.\n* Consider renaming either the \u201cMethod\u201d section or subsection for clarity.\n* Start a distinct \u201cResults\u201d section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper found that fine-tuning significantly reduces the perplexity of non-member data, which inspired the author to propose a new data detection method, FSD. The method determines whether data was used in training by comparing its perplexity in the original model versus the fine-tuned model. Experimental results showed a significant improvement over baseline methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The improvement is very significant.\n2. The experiments and analysis are solid.\n3. The writing is good and easy to follow."
            },
            "weaknesses": {
                "value": "1. This method requires fine-tuning the models. However, many models do not support fine-tuning especially white-box models, making this approach impractical in reality.\n2. Fine-tuning every model you want to detect would be very costly.\n3. Meta-evaluation typically fine-tunes a model on the data that needs to be detected, in order to assess whether the detection method is effective. However, this method is similar to the meta-evaluation.\n4. The in-distribution non-member data is not easy to be acquired for fine-tuning."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}