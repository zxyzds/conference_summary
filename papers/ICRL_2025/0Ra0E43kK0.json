{
    "id": "0Ra0E43kK0",
    "title": "CaLMol: Disentangled Causal Graph LLM for Molecular Relational Learning",
    "abstract": "Molecular Relational Learning (MRL), focused on understanding interactions between molecular pairs, is essential for drug design by utilizing both structural properties and textual knowledge, such as expert documents. However, most existing MRL methods assume static molecular distributions, meaning the distributions remain consistent across training and testing stages. This assumption may lead to the exploitation of variant correlations between structures and texts regarding interactions, thereby failing in the ubiquitous scenarios involving new drug predictions. To bridge this gap, we investigate zero-shot MRL by leveraging invariant relationships between molecular texts and structures w.r.t interactions for new molecules, which is largely unexplored in the literature and is highly non-trivial with following challenges: 1) How to disentangle molecular structure components between each pair to intrinsically determine interactions and address potential structural distribution shift issues for new drugs? 2) How to align molecular structures with semantic textual information to achieve invariant molecular relation predictions for new drugs? To tackle these challenges, we propose a novel Causally Disentangled Invariant Graph Large Language Model (LLM) for Molecular Relational Learning (CaLMol), capable of exploiting invariant molecular relationships to predict interactions for new drugs. Specifically, we propose Causal Molecule Substructure Disentanglement to capture the invariant well-recognized substructure pair for a specific molecule interaction. Then, we propose Molecule Structure and Property aware LLM Alignment to use molecule (with invariant substructure)-textual property pair to align structure information to semantic information, and use them together to guide the interaction prediction. On this basis, LLM can also provide further explanations.\nExtensive experiments on qualitative and quantitative tasks including 8 datasets demonstrate that our proposed CaLMol achieves advanced performance on predicting molecule interactions involving new molecules.",
    "keywords": [
        "Molecular Relational Learning",
        "Large language Model",
        "Graph Neural Network",
        "Causal Learning"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=0Ra0E43kK0",
    "pdf_link": "https://openreview.net/pdf?id=0Ra0E43kK0",
    "comments": [
        {
            "summary": {
                "value": "This paper presents CaLMol, a model for molecular relational learning (MRL) that uses a combination of Graph Neural Networks (GNNs) and Large Language Models (LLMs) to predict drug-drug (DDI) and solute-solvent (SSI) interactions in a zero-shot setting. The model\u2019s innovative approach in leveraging causal disentanglement and aligning molecular structures with semantic information provides a promising direction."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper combines causal disentanglement and semantic alignment between GNN and LLM, allowing for a comprehensive understanding of molecular interactions.\n- By targeting unseen molecules, CaLMol addresses an important area in MRL, providing potential for applications involving new drugs or compounds.\n- The model is evaluated across multiple datasets, showing improvements in accuracy over several baselines, which demonstrates its effectiveness in specific zero-shot tasks.\n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "See Questions."
            },
            "questions": {
                "value": "- Could the authors provide additional analysis on the computational complexity of CaLMol? How about the comparison with these baselines in training time and inference time?\n- More detail about interpretability cases and analysis should be provided to support the advantage of CaLMol.\n- In Table 1, it is evident that the three datasets for DDI classification present a highly imbalanced binary classification task; however, the results shown for CaLMol in Table 2 perform poorly on AUC-ROC, which is a crucial metric for imbalanced data.\n- Given the model\u2019s dependency on selected datasets, how would the authors suggest extending the approach to larger and more diverse datasets? For example, Drug-Target Interaction (DTI) is also a significant task in drug discovery; demonstrating that CaLMol is useful in this task would enhance its practical significance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents CalMol, a molecular relationship learning framework based on large models and disentanglement. CalMol consists of two main parts: a causal substructure extraction module and a multimodal large model fusion module. The causal substructure extraction module learns the core substructures of molecules by decomposing the target molecule and studying the substructures in contact between pairs of molecules. The multimodal large model fusion module integrates natural language instructions with SMILES and graphical representations of molecules and core substructures into LLM for downstream tasks by constructing prompts. This work is based on MolTC, with the addition of a causal substructure extraction module. The authors evaluated CalMol on DDI (drug-drug interaction) and SSI (solute-solvent interaction) tasks, where CalMol achieved comparative performance on the DDI task and notable performance on the SSI task."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This work presents CalMol, a molecular relationship learning framework based on large models and disentanglement, which achieved comparative performance on the DDI task and notable performance on the SSI task. Extracting the causal substructures of molecules is an interesting topic."
            },
            "weaknesses": {
                "value": "1. The authors believe that existing methods rely on \"variant molecular structures\", which hinders their performance, but there is a lack of a clear definition of \"variant molecular structures\".\n2. For a molecule, the substructures that play a key role may vary when it binds with different molecules, i.e., the so-called core substructures are not fixed. Therefore, it is not rigorous enough to determine the core substructures of a molecule with just one set of relationships.\n3. Using a substructure of a molecule as its causal substructure is somewhat far-fetched, especially for larger molecules.\n4. The supervision signal and loss function used in the substructure learning stage are unclear.\n5. The authors propose to make the disentangled spurious part S approach a random distribution, but the rationale for doing so is not explained.\n6. There is a lack of necessary ablation experiments, such as whether the disentanglement module is effective and whether the several disentanglement losses are necessary."
            },
            "questions": {
                "value": "As stated in the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces CALMOL, a causally disentangled invariant graph large language model (LLM) tailored for molecular relational learning (MRL), with a particular focus on zero-shot scenarios requiring predictions of new molecular interactions. By integrating Graph Neural Networks (GNNs) with LLMs, CALMOL captures causal structural relationships and aligns molecular structures with semantic information, thereby improving predictions in drug design and molecular interaction studies. Overall, this paper is highly intriguing and meaningful, but there are several issues that require attention."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The starting point of this paper is interesting; exploring causal substructures with large models is indeed an engaging and meaningful topic.\n2. Generalization and Robustness: By leveraging invariant relationships across molecular structures and text, CALMOL effectively addresses distribution shifts between known and new drugs, thus enhancing generalization to unseen molecules. CALMOL maintains consistent performance across various dataset splits (Section 4.1)."
            },
            "weaknesses": {
                "value": "1. **Assumption on Molecular Distributions**: The paper claims that most existing MRL methods assume the same molecular distributions. However, I rarely encounter papers that explicitly make assumptions about molecular distributions, and the term \"molecular distributions\" is somewhat ambiguous, requiring further clarification. To substantiate this claim, I would recommend that the authors provide specific examples of existing MRL methods that make this assumption or clarify precisely what they mean by \"molecular distributions\" in this context.\n\n2.  **Effectiveness of Molecular Feature Extraction**: The model only uses SMILES information during the modality alignment process, yet SMILES is also provided in the input. This raises questions about the effectiveness and actual contribution of molecular graph feature extraction. I suggest the authors clarify the role and contribution of molecular graph feature extraction in their model, given that SMILES information is used in multiple stages. An ablation study or analysis showing the added value of graph feature extraction over using SMILES alone would be helpful in addressing this concern.\n\n3. **Novelty of the Method**: The method\u2019s novelty is questionable; the paper seems to merely link motif sets\u2019 causal motif extraction with LLMs in a fairly straightforward manner, without a clear motivation. Additionally, the paper claims that the LLM provides further interpretability, yet no relevant case study is provided in the experimental section to support this. I suggest that the authors provide a more detailed comparison with existing methods that combine causal motif extraction and LLMs, highlighting any specific innovations in their approach. Including a case study or examples demonstrating the enhanced interpretability claimed for their LLM-based approach would strengthen the paper.\n\n4. **Interpretability Challenges**: While CALMOL offers causal substructure explanations, the interpretability of predictions could be improved. Providing more detailed analyses or visual examples would better illustrate how causal substructure disentanglement directly impacts interaction predictions (Section 3.1). This could offer greater clarity on the added interpretability benefits of the model.\n\n5. **Dependency on LLMs**: Due to computational demands, CALMOL\u2019s reliance on large language models may limit its applicability in resource-constrained environments. Furthermore, the paper does not clearly demonstrate any significant advantage of LLMs in this domain. I suggest the authors provide a more detailed discussion of the computational requirements of their model, ideally comparing performance versus computational cost with non-LLM methods. Specific examples or analyses that demonstrate the unique advantages that LLMs bring to molecular relational learning tasks would also help to substantiate this aspect."
            },
            "questions": {
                "value": "1. Please provide specific examples of existing MRL methods that make this assumption about molecular distributions, or clarify precisely what is meant by \"molecular distributions\" in this context. Are the authors referring to \"element distribution\" or \"atom distribution\"? Providing this clarification will help address the concern more directly and substantiate the authors' claims.\n\n\n2. The model input includes both the molecular graph information and the SMILES representation; it seems an additional ablation study is needed to demonstrate the effectiveness of both modalities like MolCA .\n\n\n3. After obtaining the substructure based on causal theory, why is it necessary to input it into a large language model rather than making a direct prediction? Does this approach truly improve the final predictive results? Furthermore, while the manuscript mentions that llm could enhance interpretability, I could not find any experiments or examples to support this claim.\n\n\n4. With the introduction of a LLM, the model's complexity and resource consumption should be compared with that of conventional models to verify the necessity of incorporating LLMs, allowing for a more comprehensive evaluation.\n\n\n5. More llm-based model are needed as baseline to verify CALMOL's performance.\n\n\n\n[1] MolTC: Towards Molecular Relational Modeling In Language Models\uff1b\n\n[2] MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method to keep the invariant between molecular structures and semantic texts under a zero-shot scenario. The topic is interesting, and the experimental results look positive. Unfortunately, the paper is vague and lacks clarity both in the description of the technical approach and in the construction of the proposed datasets used for training."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The topic is valuable and interesting. Introducing functional substructures based on LLM makes it intuitive to predict potential molecular interactions."
            },
            "weaknesses": {
                "value": "1. How to introduce supervised signals to optimize the weights between motifs from different molecules is confusing, and it is suggested that the authors provide more details to clarify the principles of calculating the weights between motifs, and what the symbol \\hat{Y}_C, \\hat{Y}_S, \\hat{Y} stand for.\n\n2. The core idea of CalMol is similar to MolTC[1], the authors should clarify the key difference between them.\n\n3. The ablation study is limited, the authors should further discuss the contribution of the LLM backbone. Besides, the contribution of casual GNN is weak in the DDI prediction task, but it shows strong promotion on SSI prediction, the authors can discuss this phenomenon.\n\n[1] Fang, J., Zhang, S., Wu, C., Yang, Z., Liu, Z., Li, S., ... & Wang, X. (2024). Moltc: Towards molecular relational modeling in language models. arXiv preprint arXiv:2402.03781."
            },
            "questions": {
                "value": "see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}