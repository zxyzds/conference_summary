{
    "id": "btaxn8Xce6",
    "title": "Astral: training physics-informed neural networks with error majorants",
    "abstract": "The primal approach to physics-informed learning is a residual minimization. We argue that residual is, at best, an indirect measure of the error of approximate solution and propose to train with error majorant instead. Since error majorant provides a direct upper bound on error, one can reliably estimate how close PiNN is to the exact solution and stop the optimization process when the desired accuracy is reached. We call loss function associated with error majorant **Astral**: neur**A**l a po**ST**erio**R**i function**A**l **L**oss. To compare Astral and residual loss functions, we illustrate how error majorants can be derived for various PDEs and conduct experiments with diffusion equations (including anisotropic and in the L-shaped domain), convection-diffusion equation, temporal discretization of Maxwell's equation, magnetostatics and nonlinear elastoplasticity problems. The results indicate that Astral loss is competitive to the residual loss, typically leading to faster convergence and lower error (e.g., for Maxwell's equations, we observe an order of magnitude better relative error and training time). The main benefit of using Astral loss comes from its ability to estimate error, which is impossible with other loss functions. Our experiments indicate that the error estimate obtained with Astral loss is usually tight enough, e.g., for a highly anisotropic equation, on average, Astral overestimates error by a factor of $1.5$, and for convection-diffusion by a factor of $1.7$. We further demonstrate that Astral loss is better correlated with error than residual and is a more reliable predictor (in a statistical sense) of the error value. Moreover, unlike residual, the error indicator obtained from Astral loss has a superb spatial correlation with error. Backed with the empirical and theoretical results, we argue that one can productively use Astral loss to perform reliable error analysis and approximate PDE solutions with accuracy similar to standard residual-based techniques.",
    "keywords": [
        "a posteriori error analysis",
        "functional error estimate",
        "PiNN",
        "physics-informed neural network",
        "scientific computing",
        "uncertainty quantification",
        "PDE"
    ],
    "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)",
    "TLDR": "We propose a new loss function that allows to approximate PDE solution and to obtain reliable error estimate.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=btaxn8Xce6",
    "pdf_link": "https://openreview.net/pdf?id=btaxn8Xce6",
    "comments": [
        {
            "summary": {
                "value": "This paper starts from the observation that the loss minimization used for pinns does not generally guarantee that the distance L2 of the approximated solution from the true solution is minimized. To overcome this problem, the authors propose new losses for PINNs, which are shown to bound the L2 distance. The authors then provide empirical evidence that these losses achieve lower relative L2 errors, particularly as the equation becomes more physically complicated."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Bounding of new losses are clearly and consistently derived.\n- Clear and pedagogical presentation."
            },
            "weaknesses": {
                "value": "- The proposed solutions are specific to each equation, which limits the applicative scope of the results.\n- Empirical results are not sufficiently convincing. Only a few examples show an improvement in errors, but in most cases this remains of the same order of magnitude. It is therefore unclear whether these improvements are significant, or simply the result of statistical or numerical fluctuations.\n- As has already been shown in several series of works, what affects the lack of convergence of PINNs is the poor conditioning of the problem (see for example https://doi.org/10.1016/j.jcp.2021.110768 or https://arxiv.org/abs/2310.05801) and solutions have since been successfully proposed to correct this problem (see for example https://arxiv.org/abs/2302.13163 or https://arxiv.org/abs/2402.10680), with a significant impact both on the minimization of the classical loss, and on the minimization of L2 and even H1 errors.\n- Finally, questions of generalizing PINNs to different losses have already been explored in detail by Siddhartha Mishra's students. We refer you, for example, to the dissertations by Tim De Ryck (https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/674112/dissertation_deryck.pdf?sequence=1) or Roberto Molinaro (https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/646749/Thesis%2813%29.pdf?sequence=1).\n- At the beginning of section 2, a proper definition of error and residual would be suited."
            },
            "questions": {
                "value": "- Perhaps the improvements you've seen in your experiences come from improved conditioning of the problem. If so, this could be an interesting line of research, since as things stand, the proposed corrections require expensive computational corrections. So it would be interesting if you could provide an analysis of the NTK spectrum with classical loss and with your loss, following the work presented in https://doi.org/10.1016/j.jcp.2021.110768."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The manuscript proposes alternatives to the common, residual-based loss functions used in physics-informed neural networks. The authors argue that certain functional a posteriori error estimators better match the error (measured in energy norm) than the standard PINN loss formulation. Numerical examples for a number of different equations are presented comparing the standard residual/PINN approach to the one using a posteriori error estimators."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Moving away from strong formulations to first-order systems (the error majorants presented are not exactly first order systems, but relatively close) seems like a good idea.\n\n- Good error estimators are useful in practice."
            },
            "weaknesses": {
                "value": "- Novelty: Although the proposed loss functions are not exactly first-order system reformulations of the considered PDEs, they share a similar spirit -- no second derivatives are needed but instead auxiliary variables are introduced. However, first-order system formulations are not novel, not even for neural network based solution methods for PDEs, see for example the works of [Cai, arXiv:1911.02109] or [Schwab, arXiv:2409.20264]. So I believe it is crucial to understand if the advantages of the proposed loss functions are due to the reformulation as a first-order system or if they are specific to the proposed losses. Can the authors comment on that? The advantages of first-order systems that I have in mind are: better conditioning than PINN type losses (thinking in terms of FEM results), and the fact that only first derivatives need to be computed. \n\n- Numerical results: The presented results confirm an improvement over the PINN baseline (in terms of errors), but it is not drastic. The relative L2 errors even seem to be comparable for the majority of the considered equations. Thus the value of the error majorants may lie mostly in what they are designed for -- estimating the error (after training) and the real question is: Are first-order systems (or Astral loss) computationally more efficient than the standard PINN formulation. This question is not sufficiently addressed in the present manuscript, and would need the experiments to focus on runtime and a thorough evaluation of automatic differentiation and implementation tricks. (For instance, it is often more efficient to compute the spatial derivatives in forward mode, incorporate tricks like the Forward Laplacian framework [ arXiv:2307.08214]  etc.)\n\n- Recent work shows that PINNs are best optimized using second order methods. Improvement in accuracy can be drastic when changing from a stochastic first order method to a natural gradient or Gauss-Newton method see [M\u00fcller, arXiv:2302.13163] and [Rathore, arXiv:2402.01868]. It is important to take these recent developments into consideration for a thorough evaluation."
            },
            "questions": {
                "value": "- More of a remark than a question: In Section 2.1, the authors give two motivating examples. The first one (the sinusoidal solution) shows that the function can be L2 close to the solution while having a large residual. The authors use this as an argument against residual based loss functions. I don't think this is fair, the same phenomenon happens for Astral loss. Can the authors comment?\n\n- The standard PINN residual is, in fact, also an a posteriori error estimator for the error. Even for the error in the regularity spaces (H2 for Poisson etc). See the results in [Zeinhofer, arXiv:2311.00529]. Can the authors discuss why they prefer different a posteriori error estimators? I suppose it is because of unknown constants, but I would appreciate a clarification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes training physics-informed neural networks (PINNs) not via the usual residual-based loss, but via minimizing the so-called error majorant, a quantity that can be derived for certain classes of PDEs but that requires learning surrogate functions. The paper shows that the energy norm error correlates with the relative error of the PINN candidate function to a better degree than the residuals do. The results indicate that this approach of training PINNs results in smaller relative errors in many cases, and that further the runtime to achieve this error is smaller than for conventional training strategies."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The idea of training PINNs with the error majorant is a novel idea that, at least for some PDE classes, appears to be quite promising. The paper is generally well written and, for the most part, quite accessible. I particularly commend the intuitive motivation in Section 2.1, and the fact that a large number of PINNs (100) were trained for each setting, thus ruling out random effects. Some readers may consider the fact that the majorant must be derived/available for the considered PDE class as a shortcoming. In my experience with PINNs, in most forward problems, training often requires problem-specific adaptations (such as domain decomposition for large domains, collocation point weighting for stiff problems, etc.). I thus see the proposed approach as yet another such problem-specific adaption, but as an innovative one that leaves the established PINN paradigm of using the residual loss."
            },
            "weaknesses": {
                "value": "At some parts, the paper is not well written and somewhat unclear. For example, I think the authors switch between $\\phi$ in Section 3, which is a general notation for the solution of the PDE, and problem-specific notation ($\\phi$, $B$, $u$, etc.) in Section 4. In Section 5, the solution for diffusion is given as $u$, while in Section 4 it is given as $\\phi$ (if I understand correctly). In Section 2.3, the surrogate function (flux) is denoted as $\\tilde{F}$, while I think this corresponds to $\\omega$ in Section 3. This makes reading the paper quite taxing. More importantly, it is sometimes not clear which functions are given and which are learned; and of those that are learned, which are the solution to the PDE ($\\phi$) and which are surrogate functions ($\\omega$). I thus suggest to homogenize notation.\n\nSection 4 could be moved to the appendix, and the example in Section 2.3 or the general framework in Section 3 could be expanded. I think it must be more clear that surrogate functions need to be learned, which may be best accomplished by a schematic.\n\nThere are further considerations regarding the experimental section:\n- In Section 5, it is not clear how the variational approach works. A few lines explaining this should be included in the revision.\n- To judge the difficulty of the problems, I suggest to plot the solutions and PINN candidates for each of the considered problems.\n- Table 1 could use vertical bars between Residual / Astral / Variational to better separate the results.\n- Table 2 shows that the majorant is often a quite loose bound, contradicting the claims of the paper somewhat. Indeed, also for the mixed diffusion equation, the majorant is at least a factor of 3-4 of the natural energy error. It would be valuable to explain this phenomenon, and maybe also illustrate the error of the surrogate function required to approximate the natural energy error (e.g., the flux).\n- Figure 1 requires axis labels.\n\nFinally, I suggest that the paper undergoes proof reading by a native speaker, as some constructs read strangely and do not parse well."
            },
            "questions": {
                "value": "- What is the surrogate function $\\omega$ for the elastoplasticity example in Section 4.4?\n- How exactly does the variational approach work?\n- What do the surrogate functions required for evaluating the natural energy error look like? What is the behavior of their errors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}