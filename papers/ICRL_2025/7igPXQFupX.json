{
    "id": "7igPXQFupX",
    "title": "CoTFormer: A Chain of Thought Driven Architecture with Budget-Adaptive Computation Cost at Inference",
    "abstract": "Scaling language models to larger and deeper sizes has led to significant boosts in performance. Even though the size of these models limits their application in compute-constrained environments, the race to continually develop ever larger and deeper foundational models is underway. At the same time---regardless of the model size---task-specific techniques continue to play a pivotal role in achieving optimal downstream performance. One of these techniques, called Chain-of-Thought (CoT), is particularly interesting since, as we point out in this work, it resembles employing a deeper transformer through re-applying the model multiple times. However, a key subtlety in computing the attention of past tokens differentiates CoT from simply applying the model several times. Based on this insight, we propose CoTFormer, a novel architecture which closely mimics CoT at the token level, allowing us to obtain significantly improved accuracies close to much larger models. While applying CoT introduces additional computation costs, we compensate for it by leveraging CoTFormer's special compatibility with token-wise variable depth. Through a compute adaptive model---which automatically allocates the compute to tokens that need it most---we show that it is possible to reduce the computation cost significantly without any reduction in accuracy, and with further compute cost reductions possible while maintaining a competitive accuracy.",
    "keywords": [
        "language models",
        "adaptive compute",
        "chain of thought"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=7igPXQFupX",
    "pdf_link": "https://openreview.net/pdf?id=7igPXQFupX",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces CoTFormer, a novel transformer architecture that draws inspiration from chain-of-thought (CoT) reasoning. The key insight is recognizing that CoT differs from simple weight-tying in how attention operates across intermediary reasoning steps. The authors leverage this insight to develop an architecture that allows tokens to attend to representations from all previous \"thought\" steps, leading to improved performance compared to baseline approaches like Block Universal Transformers. Additionally, they propose an adaptive computation mechanism that allows dynamic allocation of computational resources at inference time."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "*The application of deploying LLMs to storage-constrained devices like mobile phones is relevant and timely.\n* The proposed CoTFormer architecture (Figure 1(c)) effectively translates the CoT principle into architectural design, showing clear improvements over baseline approaches while maintaining parameter efficiency through weight sharing (Section 3.1).\n* The architectural tweaks introduced in Section 3.3, particularly reserved layers and layer normalization after each repeat (LN-CoTFormer), prove crucial for achieving state-of-the-art results.\n* Addition of depth embedding (Section 4.2) shows notable improvements in the adaptive setting.\n* While not outperforming a FLOP-matched non-repeated transformer, the authors improve upon existing parameter-matched weight sharing baselines.\n* The proposed architecture and adaptive repetition method are described clearly."
            },
            "weaknesses": {
                "value": "* While Section 3.2 provides empirical evidence, the theoretical understanding of why CoTFormer works better could be deeper. Through analysis of attention patterns, we observe that tokens in later repeats tend to focus heavily on earlier representations that capture key contextual information, suggesting the model learns to leverage complementary features detected at different processing stages. This selective attention to informative past representations may help explain why CoTFormer outperforms the baseline Block Universal Transformer, where such cross-repeat attention patterns are not possible.\n* Could better connect to recent theoretical work on transformer expressivity discussed in Section 2.\n* The sequence lengths that are used for training (256) are quite short relative to the lengths that are used for training modern language models and are shorter relative to common LLM evals and typical chatbot conversations.\n* Performance gap between adaptive and fixed-depth CoTFormers under same compute budget (Section 5)\n* Training efficiency of deeper layers could be improved (e.g. increasing the gradient information during adaptive training), as shown by the analysis of router weights distribution (Figure 5)"
            },
            "questions": {
                "value": "* I\u2019m confused by Figure 1(c): it seems to indicate that the earlier token representations (i.e. the red rectangles) are reprocessed by the model to make new token representations. But Section 3.1 seems to contradict this and instead describes that these earlier representations are only used as context in attention.\n* It would be helpful to state the total parameter counts of the models used in each experiment, as well as the total number of training tokens in each experiment (either in a table or in the prose describing experimental setup).\n* 471: It would be helpful if the authors provided more details on their \u201cefficient implementation\u201d, and specifically how the authors are using a non-causal FlashAttention kernel to implement their  proposed method.= \n* How do position embeddings work with the added interleaved tokens? Are the interleaved tokens given the same position id as the original tokens they came from, do the position ids change between repetitions, or something else?\n* Do the authors have any intuitions as to how their method behaves as the width of the model changes? It appears to be held constant across all experiments.\n* 402: what does it mean to \u201cactivate a prefix of repeats\u201d? is this the fixed depth baseline that is referenced in Figure 4?\nHow does mixture of repeats work during, for example, batch size 1 transformer decoding, where there is only a single token being processed through the model?\n\n\nBelow are some thoughts that might be helpful but are not critical to give insight into ways that might improve the paper.\n* Consider analyzing attention patterns and strengthening theoretical connections to transformer expressivity research (building on 3's architecture analysis).\n* Explore sparse variants to improve scaling for longer sequences beyond 8192 (extending the computational analysis in 3.2).\n* Focus on improving training efficiency, particularly for deeper layers and adaptive computation (addressing limitations discussed in 5).\n* Develop specialized attention implementations for better computational performance (following the implementation discussion in 5).\n* Expand evaluation to include longer sequences and broader comparisons with other adaptive approaches (extending the experimental work in 4.3)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes CoTFormer, a novel Transformer-based model architecture for generative language models.\nLike Universal Transformers, a CoTFormer re-applies the same Transformer blocks for an adaptive number of times for generating each token.\nThe major difference is, after each repeat, the output tokens interleaved with input tokens are used as the new input for the next repeat;\nthis is inspired by chain-of-thought where each generated \"thought token\" can attend directly to all previous thought tokens.\nOther details are handled, such as compatibility with KV cache and batch processing during inference.\nEmpirical results show that a CoTFormer achieves lower perplexity or higher scores in some common benchmarks \nthan Block Universal Transformer with the same configuration, \nor a standard Transformer of the same size."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This work proposes a novel Transformer-based model architecture, and draws an interesting link to chain-of-thought.\nThe proposed CoTFormer is compatible with KV cache and batch processing, which is not the case for many other adaptive-computation architectures tailored to autoregressive generation.\n\n- Overall the writing is good, and things are mostly well explained.\n\n- The source code is provided."
            },
            "weaknesses": {
                "value": "My major concern is that the empirical evidence for the efficacy of CoTFormer, or its advantages over the standard Transformer architecture, is insufficient.\n\n- Generally speaking, the empirical results from the main text suggest that a CoTFormer with $n_{\\text{repeat}} \\ge 2$ slightly outperforms a standard Transformer of the same size in terms of perplexity, but underperforms a standard Transformer with twice as many layers, except in Table 2 where a CoTFormer with $n_{\\text{repeat}}$ as large as 5 (and other tweaks) achieves a perplexity that is lower by only 0.06. \nThe issue is that the inference cost (in terms of time or memory, or both) of a CoTFormer, with the total number of tokens growing linearly with $n_{\\text{repeat}}$, can possibly be larger than that of a standard Transformer with twice as many layers.\nThis raises the question of whether CoTFormer actually pushes forward the Pareto frontier of accuracy and cost; to support such a claim, it is necessary to compare CoTFormer's accuracy-cost curves with those of standard Transformers (not just Block Universal Transformer).\nWithout clear evidence of its advantages over standard Transformers, the additional complexity overhead to code and infrastructure might further hinders the adoption of CoTFormer in future research or applications.\n\n- The results of downstream performance in Appendix B have limited implications, as discussed by the authors in Line 725. \n    For example, all scores for MMLU are close to 25%, namely the accuracy of randomly picking option A/B/C/D.\n\n- The current work only contains end-to-end performance (perplexity or scores) on some common datasets and benchmarks.\n    There is no intermediate empirical result (except for Figure 5) or synthetic task, like those in the original paper of Universal Transformers (Dehghani et al., 2019), for truly understanding when, why and how CoTFormer works or fails.\nThe authors might consider visualizing the attention patterns of CoTFormer, or designing synthetic tasks that highlight CoTFormer's fundamental advantages over standard Transformers or Universal Transformers."
            },
            "questions": {
                "value": "- Typo in Line 114, \"similar the\" --> \"similar to the\"\n\n\n- Is it possible to convert a standard pre-trained Transformer to a CoTFormer via a post-training or fine-tuning phase, which can be much more efficient than pre-training a CoTFormer from scratch?\nI can't see an obvious way of doing this, since the behavior of a CoTFormer deviates significantly from that of a standard Transformer."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new model architecture called CoTFormer that improves the Block Universal Transformer by providing intermediary representations from previous repeats in the attention. Besides CoTFormer architecture, the paper also proposes a training approach called Mixture of Repeats that varies the number of model passes for individual tokens based on their difficulty. Results show that CoTFormer substantially improves accuracy and inference computation efficiency over Block Universal Transformer."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The CoTFormer architecture and Mixture-of-Repeats approach effectively improve the performance and efficiency of the Universal Transformer.\n2. The evaluation of downstream tasks illustrates the CoTFormer's potential to surpass the standard Transformer."
            },
            "weaknesses": {
                "value": "1. Possible misuse of technical terms: Chain-of-thought is a prompting technique. The process illustrated by Figure 1 (a) is called auto-regressive, which is orthogonal to CoT. Could the authors clarify how the CoTFormer model architecture model relates to the CoT prompting? Could CoT prompt be applied to CoTFormer model?\n2. The model architecture is not clearly explained. Specifically, the meaning of different colors in Figure 1 is vague. Why are there no yellow tokens in Figure 1 (b) and (c)? The figure can be more clear if the caption explains the reason for the absence of yellow tokens and the meaning of different numbers of tokens."
            },
            "questions": {
                "value": "1. Figure 2 shows the inference FLOPs vs. Perplexity. However, it cannot suggest better \"scaling properties of CoTFormers\" (quote Line 257 of the paper) because scaling properties should be suggested by the training FLOPs vs Perplexity following Kaplan et al.[1]. Could you provide the training FLOPs vs. Perplexity plot for Figure 2?\n2. Could you add the standard Transformer to Figure 2?\n3. The paper claims that \"The growth in computation cost is actually much less noticeable\". Could you provide the real measurement of computation cost in terms of memory footprint (Figures 2 and 3 only show FLOPs)? \n \n\n[1] Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. \"Scaling laws for neural language models.\" *arXiv preprint arXiv:2001.08361* (2020)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents CoTFormer, a novel Transformer architecture that leverages the Chain-of-Thought mechanism to enhance model performance while allowing for budget-adaptive computation at inference. CoTFormer enables intermediate tokens to be accessible, improving accuracy without significantly increasing computational costs. The authors further propose an adaptive training method that dynamically allocates computational resources based on the needs of individual tokens. Empirical results demonstrate that CoTFormer outperforms existing models, such as the Block Universal Transformer, while maintaining a smaller model size."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The architecture is novel and the authors made smart observations with respect to the CoT can access previous tokens.\n2. The problem is very practical and of high interest to the community, especially with limited computation resources\n3. The paper is overall well-written and organized."
            },
            "weaknesses": {
                "value": "1. The paper could have a more detailed discussion on the scalability of the architecture, with respect to larger models and higher sequence lengths, since the paper discusses that the attention computation is not the bottleneck.\n2. The paper studies the performance of CoTFormer on a particular dataset; would be interesting to see the performance on other datasets\n3. The paper could have benefited from a more thorough theoretical analysis of COTFormer, especially with the number of repeats compared to the block universal transformer"
            },
            "questions": {
                "value": "1. Is the compute budget a hyperparameter to tune to achieve an optimal balance between accuracy and computation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}