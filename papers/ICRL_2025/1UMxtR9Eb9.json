{
    "id": "1UMxtR9Eb9",
    "title": "Unifying Disentangled Representation Learning with Compositional Bias",
    "abstract": "Existing disentangled representation learning methods rely on inductive biases tailored for the specific factors of variation (e.g., attributes or objects).\nHowever, these biases are incompatible with other classes of factors, limiting their applicability for disentangling general factors of variation.\nIn this paper, we propose a unified framework for disentangled representation learning, accommodating both attribute and object disentanglement.\nTo this end, we reformulate disentangled representation learning as maximizing the compositionality of the latents.\nSpecifically, we randomly \\textit{mix} two latent representations from distinct images and maximize the likelihood of the resulting composite image.\nUnder this general framework, we demonstrate that adjusting the strategy for mixing between two latent representations allows us to capture either attributes or objects within a single framework.\nTo derive appropriate mixing strategies, we analyze the compositional structures of both attributes and objects, then incorporate these structures into their respective mixing strategies.\nOur evaluations show that our method achieves performance that matches or exceeds strong baselines in both attribute and object disentanglement.",
    "keywords": [
        "Unsupervised Representation Learning",
        "Disentangled Representation Learning",
        "Compositionality"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1UMxtR9Eb9",
    "pdf_link": "https://openreview.net/pdf?id=1UMxtR9Eb9",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates the learning of disentangled representations in particular the adaptation of existing frameworks to the learning of representations that can disentangle both attributes (e.g., color, texture, ...) and objects in a scene which authors claim prior work only tacked one of the other. The authors propose to leverage compositionality to learn disentangled representations. The setup includes pre-trained VAEs which provide representations that are then combined. The new representations serve as input to a diffusion-based decoder which is trained to reconstruct the composition of the original images. A pre-trained diffusion model is also used to enforce consistency between the input composite representations and the representation of the generated image. The method is tested for feature and object disentanglement on multiple synthetic datasets where is shows either superior or comparable performance to attribute or object disentanglement methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- presentation: the paper is polished, clear, and well-written\n- relevance of the topics: learning models that disentangle sources of information whether attributes or objects without any prior knowledge about the type of sources but rather that rely on general prior information about the data structure like compositionality to enforce disentanglement is of great nterest to the community."
            },
            "weaknesses": {
                "value": "- complexity of the proposed approach leads to limited applicability and impact: the proposed approach requires the use of pretrained diffusion models to operate (i.e., to maximize the likelihood of composite images) and requires access to composite images to train the model. \n- limited performance increase: while results show more consistent improvements for the **multi-seed** attribute disentanglement experiments, the gains are less consistent across metrics for the **single-seed** object disentanglement experiment.\n\n\nMinor:\n- theta should be a subscript in line 187\n- typo line 212, 281, 310\n- error in figure 1: z3 should be blue instead of orange\n- line 227: figure 1 above\n\n- Not sure I am getting lines 210-213"
            },
            "questions": {
                "value": "- can authors elaborate on why the maximum likelihood is needed despite already enforcing low reconstruction error ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper attempts to tackle attribute and object disentanglement through the same mechanism as opposed to separate treatment by prior methods. Building on diffusion based decoding approaches that maximize compositionality, this paper lays emphasis on composing/mixing strategy of latents for object/attributes."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Addresses both attribute and object disentanglement by developing appropriate mixing strategy for latents. This is helpful to steer the field towards disentangling different types of factors of variation - eg properties of object and object themselves.\n2. The paper gives an in depth analysis of the intricacies involved in optimizing for compositionality.\n3. The paper is well written for the most part. There are appropriate visualizations in method and experiments that complement the text."
            },
            "weaknesses": {
                "value": "The impact of paper can be more by showing results on real world data"
            },
            "questions": {
                "value": "Are there any further insights on the failure cases? Is it harder to compose attributes or objects?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Note: I am not an expert on disentangled representation learning and know little/none of the related work.\n\nThe paper proposes an approach to learn a generative model for learning disentangled representations by maximizing the compositionality of representations. By mixing the representations of two images (given some constraints to make sure the results latent representations are valid) and maximizing the likelihood of the resulting composite images the model learns representations that can be disentangled on the object and attribute level. Experiments on synthetic datasets show that the model performs well in disentangling factors across several datasets both on the object and attribute level."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper addresses the learning of disentangled representations for both objects and attributes and makes use of a standard generative model for learning them. By introducing specific mixing strategies to combine latent representations of different images under given constraints the model is able to learn disentangled representations under a fairly simple framework.\n\nThe evaluation shows that the model learns better disentangled representations than the given baselines."
            },
            "weaknesses": {
                "value": "It seems like the approach is only useable if the practitioner already knows the underlying factors they want to disentangle, as the latent mixing strategies take this knowledge under account. \nIt's also not clear to me if this would translate to real-world datasets with more complicated distributions.\nThe experiments show results for either object disentanglement or attribute disentanglement but no experiments for joint object and attribute disentanglement.\nAll experiments are done on rather simple synthetic datasets."
            },
            "questions": {
                "value": "How would this generalize to more complex datasets where the exact factors of disentanglement might not be known. Does this scale to lots of disentangled factors (dozens or hundreds) or would that make the mixing strategies too complicated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a framework for disentangled representation learning that targets both attribute\u2014and object-based disentanglement within a single model. The authors formulate disentangled representation learning as maximizing the compositionality of randomly mixed latent representations of distinct images. The method uses a pre-trained diffusion model as an image generator and introduces an additional compositional consistency loss to encourage the composite images to remain faithful to the composite latent. The authors claim that their method can obtain superior performance in standard disentanglement benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Strengths:**\n\n- The paper is relatively clear and easy to understand;\n\n- The general idea of enforcing compositional consistency across mixed latent representations is fairly neat, and could possibly be extended to more challenging scenarios;\n\n- The results seem to match or exceed some of the previous works on disentanglement benchmarks."
            },
            "weaknesses": {
                "value": "**Weaknesses:**\n\n- The approach relies on a pre-trained diffusion model to ensure composite image realism, but this doesn\u2019t guarantee alignment with the intended attribute or object combinations. As such, it is my understanding that this can compromise the interpretability and control of compositions in the general case, especially in more complex scenarios with subtle and/or hierarchical attribute/object relationships. \n- There are no guarantees that the latent representations are identifiable under the current model, and by implication, neither are the compositions;\n- The fixed mixing strategies, although appropriate for the simple cases studied, are quite rigid and likely would not adapt well to more complex scenarios in real data;\n- The scope of the evaluation is limited to toy settings which is somewhat outdated given the recent progress in generative modelling.\n- The writing is a little careless at times, there are numerous typos and/or grammatical issues some of which are mentioned below.\n\nIn my opinion, in its current state, this work largely sidesteps the key challenges in the area today, particularly the theoretical analysis of identifiability for latent representations and the development of scalable techniques that allow object-centric methods to be applied effectively in real-world settings. Therefore, I would encourage the authors to bolster their current contribution by tackling one of the two aforementioned challenges in the future.\n\n**Typo corrections:**\n\nline 34 \"theoretically prove\" \\\nline 46 \"a unique object\" \\\nline 70 \"and verify\" \\\nsection 2 heading change to \"Background\" \\\nline 77 \"incompatible with\" \\\nline 97 \"that render\" \\\nline 107 \"tailored specifically\" \\\nline 122 \"maximizing the likelihood\" \\\nline 122 \"disentangle attributes and objects\" \\\nline 147 \"to the type of\" \\\nline 163 \"While (Jung et al., 2024) rely\" \\\nline 165 sentence needs rewriting for clarity \\\nline 167 \"derive a specific\" \\\nline 177 \"of each factor\" \\\nline 177 \"derive a corresponding\" \\\nline 188 \"independent sampling of\" \\\nline 190 \"is equivalent\" \\\nline 197 \"always contains\" \\\nparagraph starting at line 206 could do with rewriting for clarity \\\nline 216 \"belong to the same\" \\\nline 259 \"While Jung et al. (2024) also maximize...\" \\\nline 295 \"to each factor of\" \\\nline 307 \"ensure reliable image generation\" \\\nline 310 \"from scratch\" \\\npage 6 footnote \"significantly\" \\\n\netc"
            },
            "questions": {
                "value": "- What challenges do the authors anticipate in applying this model to real-world, complex datasets, and how might they address these?\n- Could dynamic/learned mixing strategies replace fixed ones to improve flexibility in complex scenes? \n- Have the authors thought about under which conditions their method can provide identifiability guarantees?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes a framework to learn disentangled representations of either attributes (e.g., an object's color or orientation) or distinct objects within a scene. The frameworks begins by encoding a pair of images using a VAE encoder. The embeddings generated are $k$ vectors that eventually will be the disentangled representations. At this stage a mixer samples some vectors from  image 1 and some vectors from image 2 generating the representation of a ``new\u2019\u2019 composed image. These new representation are then noised and denoised thanks to a diffusion model before going through the decoding stage of the VAE.  The mixing component can be adjusted according to the desired inductive bias. For attribute disentanglement, the model enforces mutual exclusivity by ensuring each latent vector is sampled from only one of the two images. In contrast, for object disentanglement, this exclusivity constraint is removed, allowing, for instance, the first latent vector to be sampled from both images.\n\nThe objective function is composed of three terms: (1) a latent denoising objective using a diffusion decoder (as in Jung et al., 2024); (2) a term to maximize the likelihood of the composed image, implemented as a diffusion loss, where the diffusion model is pre-trained for each task and then frozen; and (3) a consistency objective, which ensures that the latent representation $z$ of a given image and the latent representation re-encoded after decoding the reconstructed image from $z$ remain close. For this last term, the authors found that using an NCE-like objective, where each representation should be close to its counterpart and distant from other batch representations, outperformed simply minimizing cosine similarity.\n\nThe proposed method is evaluated against various baselines, datasets, and metrics for both attribute and object disentanglement, showing improved performance across the board."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is easy to read. The proposed framework leverages and combines many techniques (such as diffusion models, SSL, optimal transport) in interesting way. The final framework is simple and from the reported results effective."
            },
            "weaknesses": {
                "value": "The main weaknesses of this paper are in the empirical evaluations. Specifically, some of the results reported do not match those previously published, a very common task used to assess object disentanglement  (unsupervised segmentation) is missing, none of the experiments are done on realistic or complex datasets (although recent state-of-art works do employ those kind of datasets). These are the main points to be discussed during the rebuttal. Fixing these could increase the soundness and the contribution scores, hence, also the final recommendation score. See below for more details on all these weaknesses.\n- Results reported in this work about other baselines do not seem to match the original results reported by the respective original papers on the same tasks and datasets. For example LDS property prediction in the original paper shows much better accuracy (80.23% on Shape, compared to the one reported in this work for LSD which is only 68.25%, for comparison the proposed method accuracy is 70.90%). For the properties \u201cmaterial\u201d and \u201cshape\u201d the differences is even higher. \n- State of art works on object disentanglement consistently use unsupervised segmentation to assess the usefulness of the generated representations, however, these tests are missing from the current work. This is an important task because it shows a concrete application of these type of representations (and for ease of comparison given that all recent works use both unsupervised segmentation as well as property prediction).\n- Both set of experiments (attributes and objects) lack of realistic or more complex datasets which state-of-the art have been using (in addition to some of the datasets used in this work). While it is not needed to have results on all of the following datasets, showing that the proposed method scales to the complexity of some of those datasets comparably to the state of art would make the contribution stronger. For example:\n    - For the attribute disentanglement FactorVAE uses CelebA.\n    - For Object centric Jung et al. 2024 use Super-CLEVR (multi-colored parts and textures), and MultiShapeNet (for realistic images), while other work such as Object Centric Slot Diffusion use the MOVi-C dataset (which contains complex objects and natural background),  MOVi-E datasets (which contains up to 23 objects per scene), FFHQ (high quality image of faces).\n\nOther minor evaluations weaknesses:\n- Attribute disentanglement results are reported with standard deviation (great!) but it is unclear on how many runs. Results for object disentanglement are provided without any standard deviation (but they should). \n\nMinor Writing Comments. This writing suggestions are not critical but they would improve clarity and readability of the paper. No need to discuss them in rebuttal but they do need to be fixed and could increase the presentation score.\n- I find the first part of the paper (until section 3) lacking important details that could easily be provided. For example:\n    - The abstract is very dry, there is no mention of which are the \u201cstrong baselines\u201d, nor which tasks this work was tested on, nor quantitative evaluation to show that the propose method \u201cmatches or exceeds\u201d baselines. Consider adding more information.\n    - From the abstract (and even the introduction and the beginning of section 3.1) it is not clear what \u201cmix\u201d, \u201ccompose\u201d, \u201ccomposition operator\u201d mean. It could be concatenation, averaging, summing\u2026 it will only become clear much later but It would be great to provide more details if not in the abstract (ideal) at least in the introduction.\n    - Still by the end of Section 2 there is no formal definition of \u201cattribute\u201d and \u201cobject\u201d.  The first example of attributes is at page 4. Having these definitions would help the reader understanding the work much better since the beginning of the paper. From the examples at page 4 it seems that nose is an attribute and face an object but it could easily be argued that actually nose is an object in itself, or that face is an attribute of a bigger objet (human body). Again this highlight the need for a formal definition of attributes and objects.\n- In Figure1 there is a concrete image example but it is not clear if it belongs to Attribute mixing or Object mixing. The \u201cthing\u201d being mixed is a cylinder and a ball so why is it linked both to attributes and objects? It would be clearer to provide an example for both. Note that everything becomes clearer once the whole paper has been read but the first time the reader reaches Figure 1 this could be a source of confusion.\n- At page 6 the authors say \u201cThis occurs because the encoder can collapse the posterior p\u03b8(z|x) into a single mode\u201c. I know if this is an issue with posterior collapse.  If the encoder collapses the posterior, then the first loss ($L_{diff}$) should become high hence preventing the collapse. The problem seems to be related to the fact that the learnt encoding is sufficiently different (hence not collapsed) to keep $L_{diff}$ while what the authors want is not just $\\hat{z} = z$ but also as different as possible with respect to other $z$s.\n- Typo (?): \u201cwe can without modifying the objective function, which will be introduced in next paragraph.\u201d It is not clear what is that \u201cwe can\u201d.\n- Typo: line 241 \u201can noised\u201d.\n- The following sentence is incomplete: \u201cwe adjust our image encoder to take VAE features as input\u201d. Please clarify which kind of adjustments?\n- \u201cWhen back-propagate the gradient through xc, we truncate the gradient at the last iteration of decoding\u201d. Why, it would be great to explain and motivate this choice.\n- Typo in Line 310: \u201cmodel on each training dataset from the scratch\u201d. Should be \u201cfrom scratch\u201d.\n- It would be great to explain how you understand which latent controls which factor. I believe there is a brief explanation in the appendix but it would be great if it could be explained in the main paper.\n- In table 3 and some part of the appendix the loss term $L_con$ is called $L_cycle$. Please update it so that it is consistent throughout the paper."
            },
            "questions": {
                "value": "Please address the main weaknesses listed above. These are the most critical ones, I find the paper interesting but these weaknesses do need to be tackled, specifically:\nA. Could you explain or correct the mismatch between your results and those previously reported?\nB. Could you provide results on unsupervised segmentation tasks using the three typical metrics: Adjusted rand index for foreground objects (FG-ARI), mean intersection over union (mIoU), and mean best overlap (mBO) (see Jung et al 2024 as an example).\nC. Could you provide results on at least a couple of the more complex datasets listed above (and for the tasks used in the state of art work mentioned).\n\nAdditionally these are more questions that are interesting to discuss.\n\nD. The authors state at various points in the manuscript that previous methods use inductive biases specific to either attributes or objects, making them unsuitable for both simultaneously. For instance, in the statements, \u201cExisting disentangled representation learning methods rely on inductive biases tailored for specific factors of variation (e.g., attributes or objects). However, these biases are incompatible with other classes of factors\u201d and \u201cUnlike previous methods, which introduce inductive biases tailored specifically to either attribute or object.\u201d\nHowever, the proposed method also requires a choice of mixing strategy tailored to either attributes or objects, which seems like an inductive bias itself, specific to one type of disentanglement. Could this advance choice also be considered a form of inductive bias that is specific to objects or attributes? Likewise, could state-of-the-art methods (e.g., Jung et al., 2024) also be modified to handle both attributes and objects? It\u2019s unclear to me to what extent prior methods are fundamentally \"unable\" to address both types of disentanglement, as opposed their experiments being focused on of the the two tasks but potentially adaptable to the other in a way similar to how this proposed method can be adapted via choosing an appropriate mixing strategy.\n\nE. In Section 2 the authors make the following comment \u201cin object-centric scenes, the same objects can appear in different spatial locations, complicating the definition of independence metrics for object representations\u201d. It would be great to show qualitatively in examples like Figure 2 what happens when the image contains 2 identical objects and one of them is added or removed from the image. Would the proposed framework work or would there be a confusion among those object. I say this in part out of curiosity and in part because in Figure 3 (right 3rd column for inserting) it seems the model is confusing two similar objects and is adding the one in the back rather then one in the front. Could you provide those qualitative examples (if not possible in the rebuttal then in a potential future version of the paper).\n\nF. I could not find any detail (even in the appendix) about w(t). Could you please provide details about this function for both attribute and object tasks.\n\nG. The authors mention that Jung et al. use a similar prior term but since they use the same diffusion model (as opposed to a pre-trained and frozen one) they are measuring $p(x^c|z^c)$ rather than $p(x^c)$.  I have two comments and questions about this:\n1. Even when using a frozen diffusion model, wouldn\u2019t the final decoded image be conditioned on $z^c$? \n2. Regardless, I think this would be a good choice to compare. How does the current framework compare quantitatively to a similar framework that uses the term from Jung et al? Using Jung et al. solution would simplify the framework and reduce the need for training an extra model. Could you provide a comparison between these two options?\n\nH. For the DCI metric the authors say \u201cwe perform PCA as post-processing on the representation before evaluation, following (Du et al., 2021; Yang et al., 2023)\u201d. While I appreciate that this has been done before I wonder if it is a fair evaluation of disentanglement when it is applied only to some methods. Shouldn\u2019t each vector $z_i$ be considered one of the \u201cdimensions\u201d. With PCA one is not measuring the disentanglement of each dimension but rather the disentanglement of a rotated version of the linear combination of the dimensions. This does not seem the same. Please help me understand why this makes sense and it is a fair evaluation, or if you agree with me that this is not a fair evaluation please compute and report the DCI score without PCA."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}