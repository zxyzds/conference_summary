{
    "id": "KZII3faAs2",
    "title": "AIMing for Explainability in GNNs",
    "abstract": "As machine learning models become increasingly complex and are deployed in critical domains such as healthcare, finance, and autonomous systems, the need for effective explainability has grown. Graph Neural Networks (GNNs), which excel in processing graph-structured data, have seen significant advancements, but explainability for GNNs is still in its early stages. Existing approaches fall under two broad categories: post-hoc explainers, which are evaluated using ground truth explanations for synthetic data, or models based on prototypes or graph kernels that claim inherent interpretability and do not evaluate any actual measures. These evaluation practices fundamentally restrict the utility of any discussions regarding explainability for GNNs. We propose a unified and comprehensive framework for measuring and evaluating explainability in GNNs that extends beyond synthetic data and ground truths, while also allowing for further model development and refinement based on derived explanations. The framework involves measures of Accuracy, Instance-level explanations, and Model-level explanations (AIM), inspired by the generic Co-12 conceptual properties of explanations quality (Nauta et al., 2023). We apply this framework to a suite of existing models, deriving ways to extract explanations from them and to highlight their strengths and weaknesses. Furthermore, based on this analysis using AIM, we develop a new model called XGKN that demonstrates improved explainability while performing on par with existing models. Our approach aims to advance the field of Explainable AI (XAI) for GNNs, offering more robust and practical solutions for understanding and interpreting complex models.",
    "keywords": [
        "Graph Neural Networks",
        "explainability",
        "graph kernels"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=KZII3faAs2",
    "pdf_link": "https://openreview.net/pdf?id=KZII3faAs2",
    "comments": [
        {
            "summary": {
                "value": "The authors propose a set of metrics for measuring explainability of GNNs inspired by the Nauta et al. co-12 properties. They also develop a variant of graph-kernel based GNNs enhancing explainability properties."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea of evaluating explainability from many different perspectives is (not novel but) sensible, and it's application to GNN explainability is potentially interesting.\n\nThe proposed approach seem to strike a good balance between different metrics."
            },
            "weaknesses": {
                "value": "My impression is that the authors made an attempt to adapt the co-12 properties from (Nauta et al., 2023), but partly failed to account for the specificities of of networked data and GNNs, and overlooked a number of formalization efforts already available in the literature.\n\nCompleteness (without): I(f(G\\f(h(G)) != c) the formula is wrong, I\nguess it should be I(f(G \\ h(G)) != c). Also, this metric exists and\nis called fidelity- (or sufficiency).\n\nConsistency: this is poorly formalized, as IoU(h(G),h(G)) is 1 by\nconstruction. You should clarify (and formally write) that this is\ncomputed by picking explanations from a distribution (I guess).\n\nContinuity: While this is sensible with low-level data (like pixels),\nit's not necessarily sensible with discrete structures like graphs,\nwhere the the ground truth could be a motif, and removing elements of\nthe motif could end up changing the label of the graph. Indeed,\nmeasures like explanation fidelity (especially robust fidelity+),\nencode the fact that removing elements that are part of the\nexplanation should have an impact on the prediction of the model (and\nthus, on the resulting post-hoc explanation).\n\nContrastivity: the definition doesn't seem to fit the description in Table 1\n\nFor model-level metrics, correctness seems to be closer to a\n(model-level) definition of robust fidelity+, but in this set of\nmetrics is somehow in contradiction with consistency in the\ninstance-level metrics. Also \n\nM3: Compactness - I am not sure this is about compactness, I would\nrather talk about non-redundancy.\n\nTalking about interpretable GNNs, the authors only focus on\nprototype-based and kernel-based GNNs. However, the literature on\ninterpretable GNNs is much richer. For instance, all methods based on\nattention are missing, e.g.:\n\nSiqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. ICML 2022.\n\nChris Lin, Gerald J Sun, Krishna C Bulusu, Jonathan R Dry, and Marylens Hernandez. Graph neural networks including sparse interpretability. arXiv, 2020.\n\nGiuseppe Serra and Mathias Niepert. Learning to explain graph neural networks. arXiv, 2022.\n\nThis limits the relevance of the experimental evaluation. Additionally, only two explainers are evaluated, while  plenty of (more recent) explainers have been proposed in the literature.\n\nFinally, the authors propose a SHAP-based approach to extract explanations from interpretable GNNs, which is fine. But post-hoc explainers can still be applied to interpretable GNNs, so it is not true that no  solution exists. The authors should show that their approach is better in extracting explanations from interpretable GNNs."
            },
            "questions": {
                "value": "Can you comment on my concerns about (some of) the properties you propose?\n\nCan you comment on my concerns about the insufficient treatment of interpretable GNNs?\n\nCan you comment on my concerns about the insufficient treatment of GNN explainers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors focus on the problem of GNN explainability;\n\n- Contribution 1 -- a framework for graph explainability, with 12 properties\n\n- Contribution 2 -- XGKN, a new explanable GNN model based on random walks"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper tackles an important problem of GNN explainability\n- The proposed method XGKN seems to provide some benefits against KerGNN (though see Questions)"
            },
            "weaknesses": {
                "value": "- It would be great if the authors put their work in the context of previous efforts, especially for C1\n- Some questions about experiments"
            },
            "questions": {
                "value": "1. In terms of contribution 1, there are previous efforts such as GraphFramEx that try to provide \nsome properties and eval metrics for graph explainability. Although I understand AIM is not \nexactly the same, it would be great if the authors can elaborate on the differences and build their\ninsights over previous work instead of starting from scratch\n\n2. Personally I found it rather hard to compare XGKN and KerGNN by cross ref Figure 2 and Table 3\nwith Figure 4 and Table 4 that are 3 pages apart. But my understanding is that:\n\n- Table 3 v Table 4 -- out of 6 benchmarks, 3 KerGNN is better and 3 XGKN is better -- it would be \ngreat if the authors can elaborate on why \"XGKN outperforms its predecessor, KerGNN, delivering superior results\"\n\n- In terms of times to extract explanations -- the different is quite small and probably fall in the \nregime of implementation details -- it would be great if the authors can elaborate on why this difference\nis fundamental, rather than some implication of implementation decisions.\n\n- I didn't find a good way to compare Figure 2 and Figure 4 clearly and understand it; given the authors still\nhave 1 page left, I would enrich 4.2.2 and really compare these two methods clearly, tease apart different\ndatasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The submission first proposes AIM metrics that cover Co-12 properties for evaluation of the Explainer of GNN Models. The authors then validate different existing GNN explainers with the proposed metrics. Moreover, the authors propose a new explainer approach based on Graph Kernel Networks (GKN). Basically, the idea of the proposed explainer is not very novel, that is a simple extention of existing GKN approaches. The thorough validation on different properties of explaination of an explainer of GNN is important."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1). The thorough validation on different properties of explaination of an explainer of GNN is important. Exisiting work does not discuss such different properties thoroughly. This is a good contribution of this submission.\n\n2). The problem is important for XAI, espeicially for applications in important AI for sicience tasks, like drug discovery."
            },
            "weaknesses": {
                "value": "1). the idea of the proposed explainer is not very novel. The paper listed two other GKN approaches. On top of that, the new stuffs in this paper is not significant. Meanwhile, the performance improvement on AIM metrics over two other GKN approaches is not significant. \n\n2). The authors do not consider the OOD issue of the metrics that are discussed in the following papers:\nCooperative explanations of graph neural networks. WSDM\u201923.\nTOWARDS ROBUST FIDELITY FOR EVALUATING EXPLAINABILITY OF GRAPH NEURAL NETWORKS.  ICLR\u201924.\n\n3). The motivation is not well-written on the improvement over existing GKN approaches."
            },
            "questions": {
                "value": "as mentioned in weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose novel metrics for evaluating GNN explainer models, as well as a new interpretable GNN model based on KerGNN, called XGKN. The work is innovative and has great potential in the field of graph neural networks (GNNs). The authors have conducted extensive experiments to assess both their proposed metrics and the performance of their new model. However, there are several concerns that need to be addressed to improve the clarity and presentation of the paper."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors tackle an important problem in GNN interpretability and propose both a new model and metrics to evaluate it. The concept of XGKN and the proposed evaluation metrics are novel and relevant. Furthermore, the experimental evaluation is thorough, providing substantial support for the contributions."
            },
            "weaknesses": {
                "value": "1. Introduction Structure and Clarity:\n   The logic in the introduction is somewhat unclear. The authors should clearly state their two main objectives: (1) to propose new evaluation metrics for GNN explainability, and (2) to introduce XGKN, a GNN model with inherent interpretability. After discussing the shortcomings of existing methods, the authors should introduce their model XGKN, explaining how it addresses the identified limitations. This should be followed by a clear explanation of the shortcomings of current evaluation metrics, leading to their motivation for proposing new metrics. Additionally, it would be helpful to move the related work to a separate section, so the introduction can focus more on the motivation and contributions. The current contribution section lacks clarity and should be more explicit about the novelty of the work.\n\n2. Writing and Formatting Issues:\n   There are numerous writing errors throughout the paper:\n   - The caption for the tables is not formatted correctly. The captions should be placed above the tables.\n   - In line 81, \"GKN\" should be \"XGKN,\" as it refers to the authors' proposed model.\n   - Line 140: A comma is missing after \"as an explanation\" and before \"for the prediction.\"\n   - Line 141: The correct notation should be \"IoU(G1, G2).\"\n   - Line 142: The symbol \"II\" is introduced without explanation, and should be defined clearly when first mentioned.\n   - Line 153: The formula should be written as \"II(f(G \\ h(G)) \u2260 c).\" \n   - Lines 156 and 159 use \"G'\" to represent different meanings. This inconsistency is confusing, and different symbols should be used to avoid ambiguity. Furthermore, \"G'\" appears again in line 374 without proper explanation.\n   - Lines 160 and 213: The \"G\" font is incorrect; it should be italicized to represent \"graph.\"\n   - Line 238: There is a missing \"a\" before \"GIN.\"\n\n3. Formula and Notation Issues:\n   - Line 160: The explanation for contrastivity is not clear. Contrastivity measures the distinguishability between explanations for different predictions. The authors should clarify their reasoning behind the current formula and explain how it relates to the concept of contrastivity as typically understood in the field.\n   - In lines 149 and 167, the definitions of A1 and A2 rely on the explanation of ground truth. However, for real-world datasets where ground truth is not available, it is unclear how this would be handled. The authors could propose alternative metrics for cases without ground truth or discuss how their method could be adapted for such scenarios.\n   - Line 377: The meaning of \"K\" in the formula is not explained and should be clarified.\n\n4. Figure and Table Organization:\n   Figures 4 and Table 4 should be combined with Figure 2 and Table 3, respectively. This would allow for better comparison between the performance of the proposed method and existing methods, making it easier to assess improvements.\n\n5. Overall Organization and Presentation:\n   While the paper presents innovative ideas, the structure and presentation need significant improvement. It is recommended to first introduce the proposed method (XGKN), followed by the proposed metrics. In the experiments section, the authors should use their proposed metrics to evaluate both existing methods and their own. Additionally, the limitations of existing metrics should be clearly stated, and the authors should explain how their proposed metrics address these limitations."
            },
            "questions": {
                "value": "See in Weeknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose the adoption of the Co-12 properties previously developed in explanations for images into the graph domain.\nThe authors also claim that explainability for graphs is a rather under-studied issue, and that lacks standardized evaluation metrics.\nAfter having defined those evaluation metrics, the authors propose a way to extract local explanations from previous explainable models and propose a new variant improving on self-explainability."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The overall quality of the writing is good\n- Challenging common assumptions in the literature, such as evaluation metrics, is useful for the community\n- The visualization of results across different metrics (Fig. 1) is nice"
            },
            "weaknesses": {
                "value": "The paper **substantially lacks references to previous works**, and **many claims central to the proposed contribution are contradicted by the literature**. For example:\n\n1. line 14-15: the claim is not supported, as plenty of approaches evaluate post-hoc methods with metrics not requiring ground truth [1,2,3]\n2. line 15-16: the contrasting approach to post-hoc methods is referred to as ante-hoc methods. The indicated ante-hoc methods (prototypes and graph kernels) constitute only a very limited perspective on the current state of the art in ante-hoc methods [1,4]. \n3. line 37: the claim is not supported, see [1,2,3,4]. XAI for GNNs is actually a very active field of research\n4. line 51: the claim is not supported, see my point 2 \n5. line 60-61: the claim is not supported, as plenty of metrics are available for graph explanations [1,2,3,4]\n\n[1] Explaining the Explainers in Graph Neural Networks: a Comparative Study. Longa et al. 2024. ACM Comput. Surv.\n\n[2] GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks. Amara et al. 2022. LoG\n\n[3] Evaluating explainability for graph neural networks. Agarwal et al. 2023. Sci Data\n\n[4] A Survey on Explainability of Graph Neural Networks. Kakkad et al. 2023. LoG"
            },
            "questions": {
                "value": "- Line 143 says \"Explainers do not have to be deterministic\". I wonder which kind of insights a human can extrapolate from a black-box when the explanation for a certain fixed output changes when the explanation is non-deterministic. Also, PGExplainer is indicated as a non-deterministic algorithm. However, once the explainer is trained, it is fully deterministic. Only the training of such an explainer is non-deterministic. This should be clarified in the text.\n\n- The authors are proposing to evaluate graph explanation with a suite of 12 evaluation metrics. Such a high number of metrics, in my opinion, makes the assessment of different explainers difficult, making comparison hard.\n\n- The proposed model is difficult to understand, and Figure 3 does not provide enough information. I would suggest annotating the figure with more detail and providing a more intuitive explanation.\n\n- Some important implementation details for ensuring reproducibility are lacking, like how the Elbow method for obtaining explanation threshold is implemented. I suggest providing more details on this"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}