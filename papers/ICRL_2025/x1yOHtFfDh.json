{
    "id": "x1yOHtFfDh",
    "title": "SportU: A Comprehensive Sports Understanding Benchmark for Multimodal Large Language Models",
    "abstract": "Multimodal Large Language Models (MLLMs) are advancing the ability to reason about complex sports scenarios by integrating textual and visual information. To comprehensively evaluate their capabilities, we introduce SPORTU, a benchmark designed to assess MLLMs across multi-level sports reasoning tasks. SPORTU comprises two key components: SPORTU-text, featuring 900 multiple-choice questions with human-annotated explanations for rule comprehension and strategy understanding. This component focuses on testing models' ability to reason about sports solely through question-answering (QA), without requiring visual inputs; SPORTU-video, consisting of 1,701 slow-motion video clips across 7 different sports and 12,048 QA pairs, designed to assess multi-level reasoning, from simple sports recognition to complex tasks like foul detection and rule application. We evaluate four prevalent LLMs mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting on the SPORTU-text part. We evaluate four LLMs using few-shot learning and chain-of-thought (CoT) prompting on SPORTU-text. GPT-4o achieves the highest accuracy of 71\\%, but still falls short of human-level performance, highlighting room for improvement in rule comprehension and reasoning. The evaluation for the SPORTU-video part includes 7 proprietary and 6 open-source MLLMs. Experiments show that models fall short on hard tasks that require deep reasoning and rule-based understanding. Claude-3.5-Sonnet performs the best with only 52.6\\% accuracy on the hard task, showing large room for improvement. We hope that SPORTU will serve as a critical step toward evaluating models' capabilities in sports understanding and reasoning. The dataset is available at \\url{https://anonymous.4open.science/r/ICLR_01-42D5/}",
    "keywords": [
        "Multimodal Large Language Models",
        "Sports Understanding",
        "Benchmark"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=x1yOHtFfDh",
    "pdf_link": "https://openreview.net/pdf?id=x1yOHtFfDh",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces SPORTU, a new benchmark designed to evaluate the capabilities of Multimodal Large Language Models (MLLMs) in sports understanding and reasoning. SPORTU consists of two components: SPORTU-text, focusing on text-based reasoning, and SPORTU-video, focusing on video-based reasoning. The authors evaluate various LLMs on both components, revealing limitations in complex reasoning tasks."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed dataset could be useful for the community.\n2. Both close and open-sourced models are evaluated.\n3. Metrics are studied with human verification."
            },
            "weaknesses": {
                "value": "1. The reviewer is concerned about the lack of diversity and coverage of the dataset because of the limited prompt templates and number of samples. \n\n2. Implementation could be possibly flawed. \n- The error in Figure 6 looks suspicious and makes the reviewer wonder whether the model is called correctly or not. \n- The reasoning prompt asks the model to first generate answer and then reasoning, which is not optimal since the model's final answer cannot benefit from the reasoning process.\n- It is known that LLM usually prefers its own answer so it is important to understand G-eval' quality with different LLMs as the rater.\n\nMinor:\nL821 typos of \"Section ??\""
            },
            "questions": {
                "value": "Please check weakness for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper provides a multimodal dataset (text and slow-motion video) for evaluating (multimodal) LLM capabilities in the sports domain."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- A multimodal new dataset for sports domain (with multiple sports) and well annotated by experts; the dataset should be helpful for the research communities\n\n- A well prompting capabilities to show the limitation of current LLM capabilities on the dataset. \n\n- Evaluating several reasonable public or private LLM models."
            },
            "weaknesses": {
                "value": "- Yet another vertical dataset for LLM\n- It's helpful but marginal to expand the technical depth for the community\n- not clearly identified what current models failed."
            },
            "questions": {
                "value": "- The video quality for the datasets\n- For each sports type, the video are biased to certain views or events?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In the AI+Sports area, existing works are limited to restricted kinds of sports, absence of explanations, or lack of reasoning on rules, and it proposes SPORTU consisting of SPORTU-text and SPORTU-video to boost understanding more sports with rules understanding. SPORTU-text evaluates models on rule comprehension and strategy understanding in the pure text domain and SPORTU-video evaluates models on understanding both video details and rules in the video domain.\nIt evaluates LLMs and MLLMs on SPORTU-text and SPORTU-video, revealing their limitations in complicated sports questions."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. It proposes SPORTU-text and SPORTU-video to boost understanding more sports with rules understanding in text and video domains.\n2. It analyzes the views, reasoning prompts, sport types, the error types, which are comprehensive.\n3. The writing is clear."
            },
            "weaknesses": {
                "value": "1. A benchmark aims to evaluate certain abilities and give some insights. The paper does not deeply discuss why the models have different performances and does not give advice on how to resolve the problem of understanding videos and reasoning on rules.\n\n2. Prompt strategy in LLM can also be tested on MLLM when evaluating on SPORTU video benchmark to see how the reasoning process influences MLLM.\n\n3. It's not very clear if the questions in this dataset can comprehensively detect the models' abilities to understand sports.\n\n4. The Pearson correlation between humans and the other metrics is low. Many are near 0."
            },
            "questions": {
                "value": "1. How do you split the SPORTU-text questions into rules-related, strategy-related, and scenario-related? What is your basis?\n\n2. What are the results on rule/strategy/scenario, respectively, on sportu-text?\n\n3. How is the error analysis in 5.1 conducted? Is there a definition for each error type?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper prensents SPORTU, a comprehensive Sports Understanding Benchmark that integrates both text-based and video-based tasks to evaluate models\u2019 sports reasoning and knowledge application capabilities. Based on this benchmark, this paper tests the capability of existing open-source or close-source  models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. As a sport domain understanding benchmark, the proposed  SPORTU combines text-based and video-based tasks to assess models' sports reasoning and knowledge application abilities.\n\n2. The evaluation setting is comprehensive including the direct prompting, chain-of-thought (CoT) prompting. In addition, few-shot promoting is also applied in SPORT-text evaluation."
            },
            "weaknesses": {
                "value": "1. Unclear motivation. The authors should clarify the differences between the proposed SPORTU and existing sport domain understanding benchmarks. Although discussions have been made in introduction and related work section together with Table 1, it is still unclear why the introduced features , for example, slow motion, multi-camera angles are important. More discussions and visualizations are needed.\n\n2. Missing details in dataset construction. There exist some unclear details in the dataset construction. For example, how to guarantee the multi-camera setting? Is it achieved simply by human annotator check? In addiation, the proposed SPORTU contains both the multi-choice and open-ended question, how are these two categories divided?\n\n3. More advanced evaluation methods should be applied. For example, ST-LLM [1], qwen-vl [2]\n\n4. The paper writing should be polished. Some references are missing, for example \"Section ??\" in Line 821. The quotation mark error in '\u201dWhy is it a foul in the video?\u201d' in Linee 482.\n\n[1] ST-LLM: Large Language Models Are Effective Temporal Learners\n[2] Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}