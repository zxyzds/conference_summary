{
    "id": "kWtP5ZOErR",
    "title": "EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search",
    "abstract": "The high computational costs of large language models (LLMs) have led to\na flurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\ndynamic, non-uniform compression methods, which adjust the compression\nlevels (e.g., sparsity) per-block or even per-layer in order to minimize accuracy\nloss, while guaranteeing a global compression threshold. Yet, current methods\nrely on heuristics for identifying the \u201cimportance\u201d of a given layer towards the\nloss, based on assumptions such as error monotonicity, i.e. that the end-to-end\nmodel compression error is proportional to the sum of layer-wise errors. In this\npaper, we revisit this area, and propose a new and general approach for dynamic\ncompression that is provably optimal in a given input range. We begin from\nthe motivating observation that, in general, error monotonicity does not hold for\nLLMs: compressed models with lower sum of per-layer errors can perform worse\nthan models with higher error sums. To address this, we propose a new general\nevolutionary framework for dynamic LLM compression called EvoPress, which\nhas provable convergence, low sample and evaluation complexity. We show that\nthese theoretical guarantees lead to highly competitive practical performance\nfor dynamic compression of Llama, Mistral and Phi models: via EvoPress, we\nset new state-of-the-art results for structural pruning (block/layer dropping),\nunstructured sparsity, as well as quantization with dynamic bitwidths.",
    "keywords": [
        "large language models",
        "compression",
        "evolutionary algorithms",
        "quantization",
        "pruning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose an optimal approach for heterogeneous compression of large language models via pruning, quantization, or layer dropping.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kWtP5ZOErR",
    "pdf_link": "https://openreview.net/pdf?id=kWtP5ZOErR",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a new general framework for dynamic LLM compression. Building on the observation that error monotonicity does not generally hold for LLM compression, the paper proposed evolutionary search approach. The proposed approach is provably convergent and has low sample and evaluation complexity. Experiments show that the proposed methods improve dynamic compression of Llama, Mistral, and Phi models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed method shows strong empirical performance. It is flexible, as it can be applied to unstructured pruning, structured pruning/layer dropping, and quantization. Theoretical analysis of the proposed methods is also provided,"
            },
            "weaknesses": {
                "value": "I think it would be better if the average and standard deviation of the performance metrics across different runs were reported, as it can demonstrate the significance of the improvement. But I understand it could be expensive to do and might not be possible for baseline methods."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a meta-heuristic for non-uniform model compression that is largely independent of specific model architectures and compression methods. The method is demonstrated on three compression approaches\u2014depth pruning, unstructured sparsity, and quantization\u2014with ample numerical results and a supporting analytical convergence analysis."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The strength of the approach is its simplicity and the fact that it is somewhat agnostic to model architecture and compression techniques.\nWhile it doesn\u2019t introduce entirely novel algorithms, its design effectively minimizes computational costs when working with larger models. Overall, the paper is quite well-written and presents compelling arguments for the proposed approach."
            },
            "weaknesses": {
                "value": "The manuscript does not, however, discuss much about the scalability of the approach in more complex scenarios, and to what extent the evolutionary search is useful compared to more basic derivative-free optimization techniques or the like.\nWhile presented as evolutionary, the algorithm leans heavily on an elitist, exploitation-focused strategy. This approach could limit exploration, potentially trapping the algorithm in local minima and risk of poor generalization in more complex compression scenarios.\n\nThis approach appears to be effective as long as the manifold remains relatively convex. However, with multi-modal compression techniques or non-convex landscapes, there are concerns about whether this computationally efficient method will generalize well beyond the controlled benchmarking environments, especially when handling mixed or complex compression strategies."
            },
            "questions": {
                "value": "1) The scheme is presented as an evolutionary search, but the update mechanism uses an elitist strategy (focused on exploitation) based on single-offspring selection. The number of mutations is also kept minimal, with one mutation shown to be nearly optimal, as demonstrated in SI B. In this regime, the scheme resembles a form of random coordinate descent or derivative-free optimization, with the added twist that perturbation occurs on a manifold with constant overall compression (referred to as switching). Could you clarify which properties of an evolutionary algorithm are essential to the results achieved, as opposed to a simpler perturbative approach?\n\n2) In the current setting, how much variability is there in the optimal compression profile found after one run of the evolutionary search? Does it appear the search get stuck in local minima in any of the benchmarks? \n\n3) Can you predict if the hypothesis that smooth dynamic model compression results in a smooth fitness landscape with few local optima still hold in multi-modal cases? \n\nAdditional Questions/comments:\n\n4) How is the step size in a single dimension determined (e.g., the 1M weights for unstructured sparsity)? Is the step size fixed or adaptive?\n\n5) In the selection process, only two stages are shown (see SI B.2). Was this approach tested with more stages, and if so, were additional stages found to be ineffective?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes an algorithm to perform dynamic compression (different layers can be compressed to various levels) of large language models. Previous works assign importance to each layer and assume end-to-end model compression error is proportional to the sum of layer-wise errors. This work observes that error monotonicity does not hold for LLM, and proposes an evolutionary search algorithm to achieve better compression quality.\n\nIn EvoPress, multiple heuristics, including mutation operation and the selection step are proposed to achieve efficient sampling for LLM. Experimental results show better compression quality compared to previous work on multiple applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "LLM compression is an important topic. This work proposes a new optimization algorithm, and better performance has been presented on multiple applications, including pruning, introducing sparsity, and quantization."
            },
            "weaknesses": {
                "value": "1. It would be good to introduce an algorithm overview figure to illustrate the steps in EvoPress. In addition, the algorithm contains mulitple heuristics to make the sampling efficient. It would be good to also summarize these heuristics in the figure.\n2. It's not clear whether the hyperparameters used in the heuristics (mutation operation & multi-step selection) can transfer well across tasks/datasets/models. Further illustration would be very useful."
            },
            "questions": {
                "value": "Please see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new evolutionary framework for dynamic LLM compression."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper proposes an evolutionary algorithm within an optimization framework, for sparse structure selection for LLMs."
            },
            "weaknesses": {
                "value": "1. The theoretical justification in Theorem 1 appears irrelevant to the algorithm proposed in the paper. Theorem 1 is established for linear fitness functions; however, it\u2019s difficult to see how this applies, as LLMs likely do not have linear fitness functions.\n\n2.  The comparison lacks systematic rigor. For instance, Table 2 presents results only at the sparsity level 70%. What about results at other sparsity levels?"
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}