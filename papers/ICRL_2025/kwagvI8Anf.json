{
    "id": "kwagvI8Anf",
    "title": "A Precompute-Then-Adapt Approach for Efficient Graph Condensation",
    "abstract": "Graph Neural Networks (GNNs) have shown great success in leveraging complex relationships in data but face significant computational challenges when dealing with large-scale graphs. To tackle this issue, graph condensation methods aim to compress large graphs into smaller, synthetic ones that can be efficiently used for GNN training. Recent approaches, particularly those based on trajectory matching, have achieved state-of-the-art (SOTA) performance in graph condensation tasks.  Trajectory-based techniques match the training behavior on a condensed graph closely with that on the original graph, typically by guiding the trajectory of model parameters during training. However, these methods require repetitive re-training of GNNs during the condensation process, making them impractical for large graphs due to their high computational cost, \\eg, taking up to 22 days to condense million-node graphs. In this paper, we propose a novel Precompute-then-Adapt graph condensation framework that overcomes this limitation by separating the condensation process into a one-time precomputation stage and a one-time adaptation learning stage. Remarkably, even with only the precomputation stage, which typically takes seconds, our method surpasses or matches SOTA results on 3 out of 7 benchmark datasets. Extensive experiments demonstrate that our approach achieves better or comparable accuracy while being 96\u00d7 to 2,455\u00d7 faster in condensation time compared to SOTA methods, significantly enhancing the practicality of GNNs for large-scale graph applications. Our code and data are available at \\url{https://anonymous.4open.science/r/GCPA-F6F9/}.",
    "keywords": [
        "graph",
        "condensation"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kwagvI8Anf",
    "pdf_link": "https://openreview.net/pdf?id=kwagvI8Anf",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces \"Precompute-then-Adapt,\" a fast and efficient framework for graph condensation in large-scale Graph Neural Networks (GNNs). Unlike traditional methods that require repeated, costly retraining, this approach divides the process into a one-time precomputation phase and a one-time adaptation stage. This novel framework achieves competitive or superior results on several benchmarks, reducing condensation time and making GNNs far more practical for large-scale applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1: Efficiency: The proposed\"Precompute-then-Adapt\" framework does significantly reduce the graph condensation process\u2019s time,  and it effectively solves the repeat training consumption on the large-scale graph for existing structure-free methods, which sounds rational to me.\n\nS2: High Performance with Simplified Methodology: The framework achieves expressive performance on several benchmark datasets with various condensation ratios, even with just the initial precomputation phase. Its one-time precompute strategy with both structure aggregation and semantic aggregation looks simple but efficient, making the proposed methodology practical.\n\nS3: Clarity: The paper is well-structured, with a clear and logical flow that makes it easy to read and follow. All the experiential results are clearly demonstrated."
            },
            "weaknesses": {
                "value": "I do have some concerned questions in terms of methodology and experiments:\n\nW1: Unlike methods such as GCOND-X and SFGC, which mimic the GNN's learning behavior, the proposed method appears independent of specific GNN backbones, focusing instead on operations at the graph data level without involving models (one-time precomputation is not related to trained GNNs). This raises a question: how does the approach ensure similar performance on the original test graph set when no explicit GNN model-based constraints are applied?\n\nW2: For the ablation study, it only shows w/o structure and w/o semantics, what about only with precompute features but without the feature adaption module and contrastive learning?\n\nW3: Also for the ablation study, what is w/o both mean? does that mean using random initialized features to input the feature adaption module?"
            },
            "questions": {
                "value": "See weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a simple but effective and efficiency method for directly sample and merge node features (after message passing). Then using the alignment loss to enhance such features. The results are quite strong."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The performance of this\u00a0work is comprehensive,\u00a0typically\u00a0including\u00a0the\u00a0PubMed\u00a0and\u00a0Products.\n2. The\u00a0effectiveness\u00a0and\u00a0efficiency\u00a0are\u00a0both\u00a0impressive."
            },
            "weaknesses": {
                "value": "1. The methods used by the authors are not particularly novel, especially the use of contrastive loss for feature adaptation.\n2. I find the structure of the condensed graph somewhat unclear; it appears that the condensed graph may lack structure. I suggest clarifying this aspect within the context.\n3. The deeper rationale behind this straightforward method isn\u2019t entirely apparent to me, yet it yields strong results. From my perspective, the pre-computation stage resembles the initialization of previous methods. I\u2019m curious about the adaptation stage, which seems to primarily refine features selected in the pre-computation phase. Is it possible that the entire adaptation stage could be ablated?"
            },
            "questions": {
                "value": "1. Please provide a deeper insight into your method, as I find this simple approach and its strong results intriguing. However, I remain unconvinced, as it seems to me like only a minor improvement on current initialization techniques, even prior to gradient-based or trajectory-based matching.\n2. You might consider starting with some\u00a0more ablation study,\u00a0current\u00a0one\u00a0is\u00a0quite\u00a0short."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel Precomputethen-Adapt graph condensation framework. The framework overcomes the efficiency limitation of the trajectory-matching condensation methods. By separating the condensation process into a one-time precomputation stage and a one-time adaptation learning stage, it achieves good experimental performance on the standard benchmark."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The approach this paper proposed reduces computational costs compared to previous gradient-matching or trajectory-matching methods.\n2. The experimental results demonstrate the effectiveness of the method to some extent."
            },
            "weaknesses": {
                "value": "1. The novelty of this paper is limited. On the one hand, the so-called one-time precomputation stage in this paper, utilizing the mean value of sampled data features, is similar to some feature-matching dataset distillation methods; moreover, the story of \"semantic\" alignment was also proposed in [1]. On the other hand, the one-time adaptation learning process also just utilizes the commonly used graph contrastive loss.\n2. The presentation of the paper is relatively weak. Basic concepts like Graph Condensation and Structure-Free Graph Condensation are repeatedly mentioned in both the introduction and related work sections, taking up a large portion of the main text. Additionally, the authors do not clearly explain the interrelationship between the two components.\n3. In the experiments, the performance of the previous state-of-the-art methods is inconsistent with and significantly different from the performance reported in related technical papers. Was the baseline method carefully reproduced? Moreover, the method has an excessive number of hyperparameters and configurations, but the authors do not provide the hyperparameter settings for each dataset and condensation ratio or a principal tuning method, which undermines the effectiveness of the proposed method.\n4. Since N' is the number of synthetic nodes, in eq 3, for i = 1, 2, . . . , N  ->  for i = 1, 2, . . . , N'.\n\n[1] Diversified Semantic Distribution Matching for Dataset Distillation. ACM MM 2024"
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}