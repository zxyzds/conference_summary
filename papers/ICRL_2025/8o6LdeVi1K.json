{
    "id": "8o6LdeVi1K",
    "title": "WAPITI: A Watermark for Finetuned Open-Source LLMs",
    "abstract": "Watermarking of large language models (LLMs) generation embeds an imperceptible statistical pattern within texts, making it algorithmically detectable. \nWatermarking is a promising method for addressing potential harm and biases from LLMs, as it enables traceability, accountability, and detection of manipulated content, helping to mitigate unintended consequences. \nHowever, for open-source models, watermarking faces two major challenges: \n(1) incompatibility with fine-tuned models\n(2) vulnerability to fine-tuning attacks.\nIn this work, we propose WAPITI, a new method that transfers watermarking from base models to fine-tuned models through parameter integration.\nTo the best of our knowledge, we are the first to embed watermarks into fine-tuned model parameters and preserve their fine-tuned capabilities. \nFurthermore, our approach offers an effective defense against fine-tuning attacks. \nWe test our method on various model architectures and watermarking strategies. \nResults demonstrate that our method can successfully inject watermarks and is highly compatible with fine-tuned models. \nAdditionally, we offer an in-depth analysis of how the strength of \nparameter editing influences the watermark strength and overall capabilities of the resulting models.",
    "keywords": [
        "Watermark",
        "Large Language Models",
        "Model Interventions"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We propose the first watermarking method for open-source fine-tuned models using parameter editing, and it preserves the model capabilities while ensuring high detectability across diverse models in real settings.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8o6LdeVi1K",
    "pdf_link": "https://openreview.net/pdf?id=8o6LdeVi1K",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces WAPITI, a watermarking method for fine-tuned open-source LLMs. It embeds watermarks directly into model parameters, ensuring robustness against fine-tuning without additional training. Experiments show that WAPITI maintains watermark detectability with minimal performance impact, supporting traceability in open-source AI models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper presents WAPITI, a watermarking method for fine-tuned, open-source LLMs that embeds watermarks directly in model parameters, aiming for robustness against fine-tuning. The approach is somewhat novel, addressing a recognized challenge in model traceability with a parameter-based watermarking solution that does not require additional training."
            },
            "weaknesses": {
                "value": "1.\tWhile the end watermarking algorithm is very simple, it relies on multiple approximations and heuristic observations of the experimental results. Such as the orthogonality between the parameter differences. This may undermine the theoretical rigor and precision of the proposed method.\n2.\tThe experimental validation appears somewhat limited, with relatively few comparisons to other state-of-the-art watermarking methods. This raises questions about the generalizability and robustness of WAPITI. Therefore, the overall contribution may be incremental, and broader validation would strengthen its significance."
            },
            "questions": {
                "value": "Please refer to the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors present a watermarking technique for open-weight LLMs that involves interpolating between the parameters of non-watermarked and watermarked models. Preserving the capabilities of open-source models is a challenge when embedding a watermark. The authors show a controllable way of injecting the watermark with limited loss in the model's capabilities. Their method entails training a distilled, watermarked base model and adjusting the parameters of a fine-tuned model along the path between the non-watermarked and watermarked base models. The authors assess their method's detectability and generation quality using two well-known watermarking techniques."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The method is relatively simple but provides a method of embedding a watermark with a controllable loss in text generation capabilities\n\n- The approach is motivated and presented clearly. \n\n- The experiments in the paper appear sound."
            },
            "weaknesses": {
                "value": "- The paper's main contribution is fairly limited. Equations 4-13 can be added to the appendix as they are relatively straightforward. The idea of interpolating between parameters to control the strength of a modification has been applied before (e.g., in LoRA [B]). \n\n- The paper is missing a threat model. Assume the user has access to the base model. Then they can invoke Algorithm 1, obtain $\\Delta \\theta_{Base}$ and undo the watermark. \n\n- The authors claim that distillation impacts the model's math capability for Llama-2-7B while their approach has a controllable trade-off. What parameters did the authors use for distillation, and do distillation parameters exist that have a lower impact on the model's (math) capabilities? \n\n- The authors' claim that they are the first to distil watermarks is confusing. As the authors themselves correctly state in the introduction, Gu et al. [A] can distil a watermark from one \"base\" model into another \"fine-tuned\" model. The authors state that they are the first to additionally achieve the preservation of the model's \"fine-tuned capabilities,\" but this property is not well defined and can be challenged. \n\n- I do not understand how the method is considered train-free if it has to invoke the watermark distillation algorithm as a subroutine (see Algorithm 1). \n\n- The authors do not evaluate the robustness of their approach. \n\n- The authors do not ablate over the effect of the hyperparameter $\\lambda$ on the watermark detectability and accuracy on MMLU or GSM8k, which I believe could strengthen the paper. \n\n------\n[A] Gu, Chenchen, et al. \"On the learnability of watermarks for language models.\", ICLR 2024\n\n[B] Hu, Edward J., et al. \"Lora: Low-rank adaptation of large language models.\", ICLR 2022"
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "details_of_ethics_concerns": {
                "value": "None"
            }
        },
        {
            "summary": {
                "value": "The paper addresses the challenge of watermarking the weights of fine-tuned large language models (LLMs). Traditional watermarking techniques often degrade the performance of fine-tuned models, prompting the need for a new approach. The authors propose a novel method that involves embedding the watermark into a base model and subsequently applying the weight delta between the base and the watermarked base model to the fine-tuned model. This technique preserves the quality of the fine-tuned model while ensuring the watermark remains detectable."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Watermarking open source LLMs is an important topic. The fact that finetuned LLMs are hard to watermark is an interesting observation. \nThe propose method makes it possible to watermark fine-tuned models just by one operation of the weights"
            },
            "weaknesses": {
                "value": "The experiments presented in the paper are insufficient and lack detailed explanation and evidence.\n\n- In Figure 2, the authors highlight a key weakness of watermarking fine-tuned models by demonstrating that training on watermarked mathematical data reduces performance. However, mathematics is notoriously difficult to watermark due to its low entropy, making this a cherry-picked example where failure is expected. The authors could have employed watermarks specifically designed for low-entropy text, as suggested in [1].\n- The approach of fine-tuning a non-watermarked model on watermarked mathematical data as a baseline seems counterintuitive. The authors should demonstrate that a pre-trained watermarked model, when fine-tuned on mathematical data, does not exhibit watermark detectability. This would provide a more convincing baseline. The authors only cite Gu et al. as evidence that fine-tuning a watermarked base model removes the watermark.  But [2] show that it is pretty resilient. \n- Section 3.2 in my opinon excessive to intuitively justify Equation 13.\n- Table 2 is lacking critical information. The p-values of 0.5 appear to be expected rather than computed, but it is crucial to show that the tests yield random p-values under the null hypothesis (H0) to confirm that the scores are accurately computed. The authors do not specify which scoring method they use: for Kirchenbauer, is it binomial or z-score based? Additionally, how many tokens are scored? Do the authors perform appropriate deduplication to get reliable p values?\n- The reason it is easier to distill with kgw-k1 than aar-k2 is not due to the method itself but rather the window size, as discussed in [2].\n\n[1] https://arxiv.org/abs/2305.15060\n[2] https://arxiv.org/abs/2402.14904"
            },
            "questions": {
                "value": "see weaknesses.\n\n- 1.3M samples necessary for distillation? what does sample mean? for what method, which window size etc?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses the watermarking issues associated with open-source large language models by proposing a novel parameter integration method that facilitates the migration of watermarks from the base model to the fine-tuned model. This approach effectively avoids the performance degradation and high computational costs typically associated with watermark distillation. Building upon the watermark distillation method outlined in Gu2024, the paper resolves the incompatibility issues with fine-tuning and the inability to withstand fine-tuning attacks. Initially, watermark distillation is applied to the base model to calculate the weight difference \u0394\u03b8. Subsequently, the base model is fine-tuned to obtain a fine-tuned model, and the weighted sum of the fine-tuned model's weights and \u0394\u03b8 results in the new fine-tuned distilled model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The core idea of WAPITI is to leverage the impact of watermarks on the model's output distribution. The paper demonstrates that watermarks induce similar alterations in the output distribution of both the base and fine-tuned models. By adding the watermark parameter vector from the base model to the fine-tuned model parameters, the output distribution of the fine-tuned model is similarly modified, enabling the transfer of the watermark.\n2. This paper introduces, for the first time, a parameter integration-based watermarking method that facilitates the migration of watermarks from the base model to the fine-tuned model, thereby avoiding the performance degradation and high computational costs associated with watermark distillation.\n3. The proposed method effectively maintains the fine-tuning capabilities while ensuring the presence of the watermark, thereby providing robust defense against fine-tuning attacks and enhancing the security of the watermark.\n4. The paper is well-structured, with a generally clear logical flow and clearly articulated viewpoints, effectively conveying the main content."
            },
            "weaknesses": {
                "value": "1. The issue of watermark distillation's inability to withstand fine-tuning, mentioned in the contributions of Chapter 1, has already been raised in Gu2024 and cannot be considered a primary contribution of this paper.\n2. This paper serves as an improvement on the watermark distillation scheme proposed by Gu2024, which somewhat diminishes its novelty; further research is needed to solidify its impact.\n3. In line 78, the paper emphasizes that WAPITI effectively resists fine-tuning attacks, yet this contribution is not mentioned in the summary, and the subsequent content lacks a comprehensive discussion on fine-tuning attacks.\n4. Table 1 is missing a checkmark for the Decoding-based Watermarks row, and the description for \"It undermines capabilities\" is unclear; using parentheses in the table for clarification is also inappropriate. Additionally, the paragraph referencing this table mentions higher computational costs, which should prompt the addition of corresponding comparisons in the table, such as differences in vulnerability, robustness, and efficiency.\n5. In Appendix E.2, the paper attempts to prove that even when models undergo fine-tuning attacks, the watermark detection rate and model usability decline synchronously to support the conclusion that WAPITI can resist fine-tuning attacks. However, the fine-tuning experiments in Gu2024 indicate that fine-tuning may remove the watermark without specifying whether model usability also declines synchronously. If usability in Gu2024\u2019s experiments similarly declines, then WAPITI does not demonstrate a clear advantage over Gu2024 in resisting fine-tuning attacks, necessitating additional comparative experiments to substantiate this work.\n6. Figure 2 lacks clear annotations and fails to adequately explain the content depicted; it is recommended to split this into two figures or use line charts for a more intuitive presentation of data trends.\n7. In line 415, it is stated that WAPITI is effective and efficient. However, the term \"efficient\" requires supporting execution time data; to substantiate this conclusion, time cost experiments for the WAPITI scheme under various models and watermark methods should be added.\n8. In section 4.3, line 365 mentions that Appendix F will analyze the selection of the hyperparameter \u03bb, yet only partial analysis regarding \u03bb is found in Appendix E.1, and it does not provide a detailed explanation of the selection method for the \u03bb hyperparameter.\n9. The appendix contains graphical and typographical errors, such as the identical experimental figure in section F.3 and E.1, the same images for Figures 2 and E.2 Figure 7, an incorrect reference to Figure 14 as Figure 6 in Appendix E.1, missing descriptions for the captions of Figures 9-14 in Appendix F, and incorrect writing of \"coefffcient.S\" in Appendix E.1"
            },
            "questions": {
                "value": "1. The paper does not provide a sufficient and detailed description of fine-tuning attacks related to Gu2024, which undermines the persuasiveness of its conclusions. For instance, while it mentions adding the KGW watermark to Llama-Math and Llama-QA, it neglects the Llama-chat and Pythia-chat models used in the main text, and it omits the AAR watermarking method. Additionally, it is unclear which dataset was used for fine-tuning Llama-Math and Llama-QA after watermarking, leading to a decline in fine-tuning capabilities.\n2. If the paper aims to assert the incompatibility of the Gu2024 watermark distillation model with fine-tuning, it should validate this claim across diverse datasets. The relative entropy space of mathematical datasets is lower, and the performance of GSM8K may not sufficiently support this argument.\n3. Generally, distilling larger models is often more effective due to their greater number of parameters and enhanced learning capacity, allowing them to capture richer features and complex patterns. Why does the paper choose to distill smaller models? What are the results of applying this approach to larger models?\n4. If the parameters of the fine-tuned base model differ significantly from the original model across certain dimensions, could this result in the watermark being ineffective or lead to the loss of watermark information?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}