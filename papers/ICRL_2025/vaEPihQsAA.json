{
    "id": "vaEPihQsAA",
    "title": "CyberHost: A One-stage Diffusion Framework for Audio-driven Talking Body Generation",
    "abstract": "Diffusion-based video generation technology has advanced significantly, catalyzing a proliferation of research in human animation. While breakthroughs have been made in driving human animation through various modalities for portraits, most of current solutions for human body animation still focus on video-driven methods, leaving audio-driven taking body generation relatively underexplored. In this paper, we introduce CyberHost, a one-stage audio-driven talking body generation framework that addresses common synthesis degradations in half-body animation, including hand integrity, identity consistency, and natural motion.\nCyberHost's key designs are twofold. Firstly, the Region Attention Module (RAM) maintains a set of learnable, implicit, identity-agnostic latent features and combines them with identity-specific local visual features to enhance the synthesis of critical local regions. Secondly, the Human-Prior-Guided Conditions introduce more human structural priors into the model, reducing uncertainty in generated motion patterns and thereby improving the stability of the generated videos.\nTo our knowledge, CyberHost is the first one-stage audio-driven human diffusion model capable of zero-shot video generation for the human body. Extensive experiments demonstrate that CyberHost surpasses previous works in both quantitative and qualitative aspects. CyberHost can also be extended to video-driven and audio-video hybrid-driven scenarios, achieving similarly satisfactory results.",
    "keywords": [
        "Audio-driven Human Animation.+Diffusion Model.+Generative Model.+Human Video Generation"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We propose a one-stage audio-driven talking body generation framework, CyberHost, designed to produce human videos that match the input audio with high expressiveness and realism.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vaEPihQsAA",
    "pdf_link": "https://openreview.net/pdf?id=vaEPihQsAA",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces CyberHost, an innovative one-stage audio-driven framework for generating talking body animations, addressing common issues such as hand integrity, identity consistency, and natural motion. Unlike multi-stage methods using intermediate representations like poses or meshes, CyberHost works end-to-end and supports zero-shot generation.\n\nKey innovations like Region Attention Module and the usage of Human-Prior-Guided Conditions are proposed to improve the generation quality of local human regions and to address the motion uncertainty problem.\n\nExperiments show CyberHost outperforms existing methods both qualitatively and quantitatively and works well in audio-driven, video-driven, and hybrid scenarios."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. CyberHost introduces the first one-stage approach for audio-driven talking body generation, avoiding the complexity and inefficiencies of multi-stage systems that rely on intermediate representations.\n\n2. The proposed Region Attention Module component effectively enhances critical areas such as hands and faces, improving the quality of local details and maintaining identity consistency.\n\n3. By integrating motion constraints and structural priors via human-prior-guided conditions, the model mitigates the challenge of motion uncertainty, resulting in more stable and natural body animations.\n\n4. The qualitative results in the supplementary materials are impressive. Also, compared to the previous state-of-the-art audio-driven half-body generation method, VLOGGER, CyberHost produces visibly superior results.\n\n5. The paper is well-written and clearly presents its objectives, methodology, and findings."
            },
            "weaknesses": {
                "value": "1. Detailed Failure Analysis: The paper would benefit from a discussion of failure cases or limitations where CyberHost struggles, such as specific types of input audio or complex poses. This would provide a more balanced view of the model's capabilities.\n\n2. Scalability to Full-Body Generation: The paper focuses on half-body animation, but it does not discuss how well the architecture scales to full-body animation or if there are significant challenges in extending the framework.\n\n3. Lack of User Study for Subjective Evaluation: The paper does not include user studies or subjective evaluations to gather feedback on the perceived naturalness and quality of the generated videos. Such evaluations would provide valuable insights into how well the model meets human expectations for lifelike animation."
            },
            "questions": {
                "value": "1. Will the dataset used for training and evaluation be made publicly available? This would be valuable for reproducibility and further research by other researchers.\n\n2. Failure Cases: What are the known limitations or specific scenarios where CyberHost struggles? Highlighting these would give a more complete picture of the model\u2019s strengths and areas for improvement.\n\n3. Full-Body Animation Scalability: Can the model be adapted for full-body animation, and if so, are there significant challenges or limitations to scaling up from half-body to full-body scenarios?\n\n4. User Study Inclusion: Could authors conduct user studies for subjective evaluations to gather human feedback on the perceived quality of the generated videos?\n\n5. DiffGesture Baseline: In the experiments section, the authors mentioned that they trained DiffGesture on the collected dataset, how did the authors get the SMPLX annotations for the collected dataset? It would also be good if the authors can quantitatively and qualitatively assess the generated SMPLX quality of the trained DiffGesture."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an end-to-end audio-driven human animation framework, which is designed to generate realistic and natural upper body human videos from a single image and control signals such as audio, ensuring hand integrity, identity consistency, and natural motion."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Cyberhost can generate cospeech videos with very natural motions and clear hand/body structures.\n2. It employs various control training methods, including codebook , hand clarity, pose-aligned reference, and also key point supervision. Experimental results indicate that these methods effectively enhance the clarity of hands and the correctness of body structures in the generated objects."
            },
            "weaknesses": {
                "value": "1. The generated videos exhibit insufficient facial clarity and detail, resulting in a noticeable discrepancy between the generated object and the characteristic features of the person in the reference image.\n2. Unlike the codebook in VQ-VAE, which is specifically used for the reconstruction of designated features, the codebook in Cyberhost lacks supervisory signals during training, making it unable to ensure that the codebook effectively guides the model to generate correct hand shapes and facial features.\n3. It would be good to visualize the ablation study for the two main contribution components: \u201cMotion codebook\u201d and \"ID Descriptor\"."
            },
            "questions": {
                "value": "1. There are issues with the injection of the codebook during inference, and the paper does not clearly explain how to accurately detect the hand position from the noisy latent space when the timestep corresponds to a higher noise level."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a one-stage audio-driven talking body video generation framework, addressing issues in half-body video generation such as blurred hand details, inconsistent identity, and unnatural motion. Specifically, it introduces a Region Attention Module (RAM) to enhance the quality of local regions. Additionally, it proposes a human-prior-guided condition to improve motion stability in generated videos. A new dataset was collected for experimentation, with results verifying the effectiveness of the proposed method and the improvements contributed by each component."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe proposed method demonstrates a certain degree of generalization, allowing it to adapt to multiple tasks, such as video-driven generation or multimodal-driven generation, while also enabling open-set generation.\n2.\tBased on the experimental results, the proposed method surpasses both the baseline and state-of-the-art methods across multiple metrics."
            },
            "weaknesses": {
                "value": "1.\tAlthough the proposed method achieves promising results overall, it introduces many components. As shown in Table 1, there are nine components, but the experiments lack in-depth analysis of these. For example, the impact of the size of the latent bank in RAM. The results of using alternatives in the Region Attention Module (RAM), such as not using spatial latents, were not examined. Additionally, the effect of not decoupling the latent bank into spatial and temporal latents\u2014instead using a single 3D latent bank\u2014was not investigated. Furthermore, it remains unclear what specific aspects of video information are captured by the spatial and temporal latents, lacking justification and explanation.\n3.\tThe use of the Laplacian operator to compute the hand clarity score requires justification, as the rationale behind this choice is not explicitly discussed. Additionally, the influence of the hand clarity score on the experimental results is not demonstrated in the experiments. It is essential to clarify whether this score is necessary and how it contributes to the overall performance of the proposed method.\n4.\tThe method [1] is also a one-stage audio-driven half-body video generation model, but this paper does not discuss or compare it.\n\n5.\tThe dataset used in [2] was not employed in experiments for comparison with previous methods. Additionally, the beat consistency metric [3] was not reported in the experiments.\n\n6.\tSome typos, such as in line 313 feference -> reference\n\nreference:\n\n[1] Liu X, Wu Q, Zhou H, Du Y, Wu W, Lin D, Liu Z. Audio-driven co-speech gesture video generation.\n\n[2] Qian S, Tu Z, Zhi Y, Liu W, Gao S. Speech drives templates: Co-speech gesture synthesis with learned templates.\n\n[3] Li R, Yang S, Ross DA, Kanazawa A. Ai choreographer: Music conditioned 3d dance generation with aist++"
            },
            "questions": {
                "value": "1.\tWhen using full-body keypoints instead of the body movement map for video-driven generation, is it necessary to further fine-tune the entire model?\n2.\tHow can hand pose templates be combined within the framework to achieve multimodal-driven generation? Does this process require fine-tuning the model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel and elegant one-stage audio-driven human diffusion model. The authors primarily focus on the most challenging problems of existing body animation models, which are details underfitting and motion uncertainty. To address details underfitting, the authors introduce a region attention module, and to tackle motion uncertainty, they design a series of human-prior-guided conditions. The paper is well-written and enjoyable to read. The final video results demonstrate high-quality rendering and natural motion driving."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper addresses two important and challenging problems in the body animation field, and the proposed approaches are novel and effective.\n2. The proposed method supports multi-modalities driving\n3. The driving results show really good rendering quality and natural motion fidelity.\n4. The paper is well-organized and well-written."
            },
            "weaknesses": {
                "value": "1. Some details are not provided:  \na) Which specific layers of Wav2Vec features were used? (line 191)  \nb) How to constrain the basis vectors of the latents bank to be orthogonal? (line 242)  \nc) There is a lack of loss description for regional mask predictor. (line 260)  \n2. Authors claim that the hand clarity score can enhance the model's robustness to blurry hands during training and enable control over the clarity of hand images during inference. They conducted ablations on hand clarity, but they did not demonstrate to what extent this score can control hand clarity during inference. I would like to know this result.\n3. The explanation of how the proposed 'Pose-aligned Reference Feature' works has not convinced me for two reasons:  \na) Although the ablation on pose-aligned ref shows a lower HKC score compared with Cyberhost, this method was proposed to solve the case of challenging initial poses, and the authors did not demonstrate its effectiveness in that scenario.  \nb) The authors claimed that the skeleton map provides topological structure information, which improves the quality of hand generation. However, they did not explain how this structural information actually contributes to generating higher-quality hand images.  \n4. Some spelling mistakes: 'feference' should be corrected to 'reference' in line 313."
            },
            "questions": {
                "value": "1. The authors claimed that the two-stage methods are mainly limited by the capability of the pose or mesh detectors, this limitation constrains the model's ability to capture subtle human nuances. I wonder if there exists, for example, a mesh detector that provides accurate and nuanced results. What are the advantages of a one-stage method compared to a two-stage method?\n2. The authors presented various driving results, including video-driven body reenactment and multimodal-driven video generation. Was the model retrained when performing these two types of driving cases? If not, why can the body movement map be directly replaced by a skeleton or hand pose template?\n3. Is the regional mask predictor embedded in all layers? Because different layers learn different kinds of features to serve different roles in the network. Therefore, I wonder about the effectiveness of predicting regional masks in all layers. Perhaps predicting the mask from the most effective layer could perform better.\n\nConsidering the good results and novelty, I would be very willing to raise my rating if my questions are answered."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces CyberHost, audio-driven human animation framework based on diffusion models. It addresses the less explored area of full-body human animation driven by audio signals, focusing on enhancing the generation quality of critical regions like the hands and face. The authors propose a Region Codebook Attention mechanism, along with a suite of human-prior-guided training strategies. The paper aims to bridge the gap in audio-driven human animation by improving hand clarity and overall natural motion."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors aimed to tackle two significant challenges in audio-driven body generation and achieved progress in:\n1. Improving the synthesis quality of critical regions (hands and face)\n2. Reducing motion uncertainty caused by weak correlations.\n\nSpecifically, this paper successfully addresses the challenge of generating high-quality hand and facial features using proposed modules including RAM.\n\nIn addition, comprehensive experiments were conducted. Comparisons were made to evaluate not only audio-to-body generation methods but also video-to-video and audio-to-face methods, demonstrating its expandability."
            },
            "weaknesses": {
                "value": "While most parts are understandable, some details and explanations are missing. The questions regarding the missing information are listed under \"Questions.\" Additionally, as the methods utilize many well-known architectures and frameworks while introducing several modules\u2014including the Latent Bank, Pose Encoder, Heatmap Estimator, and Mask Predictor\u2014some missing information limits the paper\u2019s reproducibility and clarity of the paper. If the concerns or questions listed on \"Questions\" are addressed, this paper would be worthy of a higher rating."
            },
            "questions": {
                "value": "1. In Section 3.2, how was the hand heatmap estimator trained? Was it trained jointly with Equation 6 during stage 1, stage 2, or was it pretrained former to Equation 6? Also, when training the hand heatmap estimator, were all weights shared across timesteps?\n\n2. Are the Pose Encoder in the Body Movement Map and the Pose-Aligned Reference Feature shared? If they are, why are the rectangular box and human pose encoded using a shared network? What are the advantages of using a shared network compared to using different networks that share the latent space? If they are not shared, they should not be described as using the same Pose Encoder or abbreviated as \"P.\"\n\n3. Were the diffusion models initialized with pretrained weights or trained from scratch? At first, it seemed they were being trained from scratch, but in Line 191, it states, \"we extend the 2D version to 3D by integrating the pretrained temporal module from AnimateDiff.\" Could you clarify how all the components were initialized?\n\n\nSimpler Questions\n\n4. What are the dimensions of L_spa and L_temp in Latent bank?\n\n5. Starting from Line 855, how will this review system be incorporated into practical applications and future research?\n\n6. Is Laplacian standard variance sufficient for \"Hand clarity score?\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Due to the subject beign human video generation, careful and responsible appoarch is required."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}