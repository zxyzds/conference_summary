{
    "id": "plflYGf23L",
    "title": "CABS: Conflict-Aware and Balanced Sparsification for Enhancing Model Merging",
    "abstract": "Model merging based on task vectors, i.e., the parameter differences between fine-tuned models and a shared base model, provides an efficient way to integrate multiple models without retraining. This approach can be used to combine task-specific models into a multitask model, improve generalization, or address model deficiencies. One of the significant challenges faced by model merging is the conflicts between task vectors. Existing works aim to mitigate these conflicts through sparsification; however, two issues observed in our experiments significantly limit their performance: $\\textit{high parameter overlap}$ and $\\textit{unbalanced weight distribution}$. To address these issues, we propose a simple yet effective framework called CABS (Conflict-Aware and Balanced Sparsification), consisting of $\\textbf{C}$onflict-$\\textbf{A}$ware Sparsification (CA) and $\\textbf{B}$alanced $\\textbf{S}$parsification (BS). CA can reduce parameter overlap by applying masks during sequential pruning, ensuring that each task vector retains distinct, non-overlapping parameters. BS leverages $n$:$m$ pruning to preserve critical weights while maintaining an even distribution across layers. Our comprehensive experiments demonstrate that CABS outperforms state-of-the-art methods across a range of diverse tasks and model sizes. Notably, in experiments with 7B-parameter language models, CABS surpasses the average performance of an \"ideal\" model, a virtual model that selects the highest score from individual fine-tuned models for each task (CABS: 76.50 vs. Ideal Model: 76.30 vs. Baseline: 76.02 vs. Fine-tuned Model: 75.86). Our results highlight the importance of addressing both high parameter overlap and unbalanced weight distribution to achieve robust and high-performance model merging.",
    "keywords": [
        "Model Merging",
        "Pruning Technique",
        "Task Vectors",
        "Language Models",
        "Conflict-Aware Sparsity (CA)",
        "Balanced Sparsity (BS)"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "We propose CABS, a method to address parameter overlap and weight imbalance in model merging, achieving superior results across various tasks and model sizes.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=plflYGf23L",
    "pdf_link": "https://openreview.net/pdf?id=plflYGf23L",
    "comments": [
        {
            "summary": {
                "value": "This paper presents CABS (Conflict-Aware and Balanced Sparsification), a framework designed to address two key challenges in model merging: parameter overlap and unbalanced weight distribution. The framework consists of two main components:\n\n- CA: A sequential pruning approach that reduces parameter overlap between task vectors\n- BS: An n:m pruning strategy that maintains balanced weight distribution\n\nThe authors evaluate CABS on both large-scale models (Mistral-7B) and smaller models (RoBERTa), demonstrating improvements over existing methods, particularly at high sparsity levels (0.75). The method achieves better performance than an \"ideal\" model baseline on some tasks, though improvements are modest."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Novel perspective on handling conflicts in model merging\n- Simple but effective solution\n- Practical value demonstrated in experiments- Clear problem formulation with empirical validation\n- Comprehensive ablation studies\n- Results generally support main claims\n- Reasonable experimental design and evaluation metrics\n- Well-organized structure with clear flow\n- Good visualization of key concepts (Fig. 1)\n- Detailed experimental results"
            },
            "weaknesses": {
                "value": "Theoretical Foundation:\n\n- No mathematical derivation or proofs\n- Lack of mechanism analysis\n- Missing theoretical basis for n:m ratio selection\n\nExperimental Design:\n\n- Limited model coverage (only Mistral and RoBERTa)\n- Missing statistical significance analysis for Small model like RoBERTa\n- Insufficient variance analysis across seeds\n- Incomplete baseline comparisons\n- Small performance improvements\n\n\nTechnical Limitations:\n\n- Only applicable to homogeneous models\n- Multi-task vector case not addressed\n- Insufficient analysis of computational overhead\n- Unknown performance on larger models (>7B)\n- Simple linear combination for merging"
            },
            "questions": {
                "value": "Theoretical:\n\n\n- Can you provide some theoretical proof or analysis for why reducing parameter overlap improves performance?\n\nTechnical:\n\n- How would the method extend to merging multiple (>3) task vectors?\n- What is the impact of different pruning orders on final performance?\n- How do you determine optimal n:m ratios for different model sizes?\n- Why choose simple linear combination when there are more sophisticated approaches available, such as Fisher Information-based merging, gradient-based adaptive weighting, or attention-based parameter fusion? Have you considered incorporating these methods to potentially improve the merging performance? \n\n\nExperimental:\n\n- Why weren't other major architectures (LLaMA, GPT) tested?\n- Can you provide complete computational and memory overhead analysis?\n- How does the method perform on cross-architecture merging?\n\n\nScalability:\n\n- How would the method perform on larger models (30B+)?\n- How does computational complexity scale with number of task vectors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a new model merging approach called CABS. CABS has two steps: 1) Conflict-Aware sparsification (CA), which essentially applies masks sequentially during weight pruning for different tasks to avoid parameter conflicts. 2) Balanced sparsification (BS), which leverages existing n:m pruning technique to maintain weight balancing. Experiment results on both encoder-only and decoder-only models demonstrate the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Model merging is a promising field in terms of compositing LLM capabilities without retraining."
            },
            "weaknesses": {
                "value": "The proposed method is very ad-hoc, and the improvement seems not that convincing. In particular:\n- CA is simple, but questionable. However, what if you exchange the pruning order to task B first and then task A? The resulting model will not be equivalent, right? Also, what if you have > 2 models to merge? The later tasks will almost aways has less effective weights to be pruned from.\n- CA can be combine with any pruning technique, I do not see why BS here is a particularly good pruning option. n:m pruning is a paper from 2021, and there are multiple state-of-the-art pruning techniques recently. While the paper empirically proves BS is better than basic magnitude based pruning, what about other advanced pruning techniques (e.g., https://arxiv.org/pdf/2305.11627). If CA + any pruning technique better than n:m pruning results in better performance, then we should not over-sell the importance of BS.\n- The improvements are small compared to competing methods, what are the confidence intervals of these results?\n- Sparsification is only a necessary for pruning-based model merging techniques. That said, I would like to see how the method compares against other categories of model merging techniques such as evolutionary model merge (https://arxiv.org/abs/2403.13187) and pack of llms (https://arxiv.org/abs/2404.11531)."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces model merging based on task vectors, aiming to address high parameter overlap and unbalanced weight distribution through n:m pruning and inverse masks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Adopting n:m pruning and inverse masks to sparsify task vectors is an interesting approach. \n* The method is simple and straightforward. \n* Analyzing the impact of different overlap rates on performance in Figure 4 is insightful.\n* Overall, this paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "* Since sequential pruning is utilized, I believe the task order could impact performance. For instance, extracting the mask for task A and then using its inverse mask for task B might yield different results compared to extracting the mask for task B and using its inverse mask for task A. Could the authors analyze this aspect?\n\n* Additionally, I am uncertain about the rationale behind the inverse mask strategy. When extracting the mask for task A and then using its inverse mask for task B, the inverse mask may remove important patterns for task B, as it is derived without consideration of task B. Could the authors provide a theoretical explanation and/or a more detailed analysis to justify this method?\n\n* Magnitude and random pruning are commonly regarded as baselines in network compression literature. What might happen if alternative pruning criteria, such as the geometric median criterion (https://arxiv.org/abs/1811.00250), were used instead?\n\n* The score improvements in Tables 2 and 3 appear quite marginal compared to the baselines, and the results are not particularly compelling. Additionally, I believe it would be helpful to include the score of the pretrained model before fine-tuning to better understand the influence of the base model (W_base\u200b in Algorithm 1).\n\n* It would be helpful to present results from merging 3\u20135 models and compare them with the baselines to demonstrate the scalability and generalizability of the method.\n\n* Could the authors provide guidelines on setting \u03bb_A,B\u200b (the weighting constants of masked task vectors)? Are these values sensitive, and do they significantly impact performance?\n\n* Would it be possible to extend the experiments to include a generation-based benchmark, such as IFEval (https://arxiv.org/abs/2311.07911)? I am curious about the model's generation capabilities and instruction-following ability after merging."
            },
            "questions": {
                "value": "Please refer to the above weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents CABS (Conflict-Aware and Balanced Sparsification), a framework for improving model merging through task vectors. The key contribution is identifying and addressing two critical issues in existing sparsification approaches: high parameter overlap and unbalanced weight distribution. The method introduces Conflict-Aware (CA) Sparsification to reduce parameter overlap and Balanced Sparsification (BS) using n:m pruning for better weight distribution. Experiments on Mistral-7B and RoBERTa-Base demonstrate consistent improvements over SOTA."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Comprehensive empirical validation across both encoder-based (RoBERTa) and decoder-based (Mistral-7B) architectures\n\n- Strong quantitative results, notably surpassing the \"ideal\" model baseline (76.50 vs 76.30)\n\n- Thorough ablation studies demonstrating the individual and combined effects of CA and BS components\n\n- Clear practical impact with minimal computational overhead and straightforward implementation"
            },
            "weaknesses": {
                "value": "- Limited exploration of sparsity levels below 0.25 and above 0.75\n\n- No investigation of the method's applicability to cross-architecture model merging\n\n- Lack of analysis on the impact of different task orderings in the sequential pruning process\n\n- Experiments focused primarily on English language tasks, leaving questions about multilingual applicability"
            },
            "questions": {
                "value": "- How sensitive is the sequential pruning order in CA to task difficulty or model performance? Would a different ordering strategy (e.g., based on task complexity) potentially yield better results?\n\n- Could CABS be extended to merge models across different architectures or pre-training objectives?\n\n- In cases where sparsity levels exceed 1 and overlap cannot be completely eliminated, how do you determine which overlapping parameters to prioritize?\n\n- What is the computational overhead of CABS compared to simpler merging approaches when scaling to larger models (>13B parameters)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}