{
    "id": "L6IgkJvcgV",
    "title": "OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes",
    "abstract": "Images generated by text-to-image (T2I) models often exhibit visual biases and stereotypes of concepts such as culture and profession. Existing quantitative measures of stereotypes are based on statistical parity that does not align with the sociological definition of stereotypes and, therefore, incorrectly categorizes biases as stereotypes. Instead of oversimplifying stereotypes as biases, we propose a quantitative measure of stereotypes that aligns with its sociological definition. We then propose OASIS to measure the stereotypes in a generated dataset and understand their origins within the T2I model. OASIS includes two scores to measure stereotypes from a generated image dataset: **(M1)** Stereotype Score to measure the distributional violation of stereotypical attributes, and **(M2)** WALS to measure spectral variance in the images along a stereotypical attribute. OASIS\nalso includes two methods to understand the origins of stereotypes in T2I models: **(U1)** StOP to discover attributes that the T2I model internally associates with a given concept, and **(U2)** SPI to quantify the emergence of stereotypical attributes in the latent space of the T2I model during image generation. Despite the considerable progress in image fidelity, using OASIS, we conclude that newer T2I models such as FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts and still generate images with widespread stereotypical attributes. Additionally, the quantity of stereotypes worsens for nationalities with lower Internet footprints.",
    "keywords": [
        "Stereotype Measurement",
        "Responsible AI",
        "Trustworthy AI",
        "Interpretability"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We propose a toolbox to quantify stereotypes in Text-to-Image models",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=L6IgkJvcgV",
    "pdf_link": "https://openreview.net/pdf?id=L6IgkJvcgV",
    "comments": [
        {
            "summary": {
                "value": "In this paper, the authors propose multiple new metrics to understand stereotyping in T2I models.  In particular, they propose a metric for stereotyping that compares the prevalence of a given attribute to a societally-anchored prior probability.  They go on to present numerous other analyses for diversity of images and measuring how differnt stereotypical concepts relate to demographics in latent space.  They show that generally newer models exhibit less stereotyping and that stereotypes arise mostly in the earlier stages of optimization."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Measuring sterotyping is an important and challenging problem for generative models.\n\n2. The diversity of metrics seem quite interesting to get a more complete picture of how bias and stereotypes arise in these models.\n\n3. The paper is fairly thorough in its testing."
            },
            "weaknesses": {
                "value": "1. The biggest weakness in my opinion is that the paper is quite hard to follow for many of the metrics. I believe this is due to making the description of the metrics unnecessarily complex, and I would encourage the authors to try to describe all of the concepts more simply.  In addition to making the paper harder to understand, it also obfuscates the insights and contributions.\n\n2. One of the papers claimed main contributions is measuring stereotyping relative to societal priors on the distribution.  However, as the authors state in the appendix, the prior is often unknown and thus falling back to 50% is a reasonable choice.  That said, this seems to limit the contribution over prior work.\n\n3. The paper does not validate their work with human judgements\n\n4. More minor: the paper only presents the evaluations but do not work on mitigations.\n\n5. The intro could be more clear and sensitive in how it characterizes what are stereotypes, why they matter, and how they define which stereotypes are important."
            },
            "questions": {
                "value": "S3.2 - WALS is quite confusingly described.  In particular (6) doesn't really make sense as an equation.  If E(D) is the SVD that is just estimating the dataset? Do you mean the rank or something else?  Even the motivation doesn't make much sense to me of why this is necessary.\n\n\"Existing bias definitions are not applicable for some attributes studied in Tab. 1. For example, in the case of wearing turban, a T2I model needs to depict 50% of the images of Iranian with turban to be unbiased according to Eq. (1), which incorrectly represents Iranian people.\" - This seems like an odd remark in that this is largely what is done here just conditioned on gender.  What do you see as the primary difference here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The proposed work has a novel contribution in an existing research gap. Defining, separating, and quantifying a measure for stereotypes and biases in Image generation models (from text prompt) is a strong case of Ethical Machine Learning. The authors built a foundation of definitions that allowed them to measure estimates of data and image stereotypes. Then they apply that on well-known Text to Image models to uncover the stereotypes risks of applying those models according to the proposed measures/estimates."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Addressing a research gap\nClear mythology\nApplication to existing models"
            },
            "weaknesses": {
                "value": "It would have been nice if the writeup included a detailed case study of qualitative analysis of cases of stereotypes"
            },
            "questions": {
                "value": "- In the abstract the authors said \"incorrectly categorizes biases as stereotypes\", do you mean the opposite (i.e. \"incorrectly categorizes stereotypes as biases\")?\n- How about using other measures of visual variations of the generated images (e.g. a similarity measure based on feature extraction (e.g. HoG))?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 10
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces OASIS, a toolbox for auditing visual stereotypes in text-to-image (T2I) models. OASIS offers novel methods for measuring stereotypes and analyzing their origins within T2I models. OASIS stereotype scores reveal that newer T2I models produce significant stereotypical biases, potentially limiting their applicability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well-written, clear, and easy to follow.\n- The paper addresses a complex and socially significant problem by evaluating biases and stereotypes in T2I models. Additionally, it provides new methods to trace the origins of these stereotypes in T2I models, offering valuable insights to help mitigate these biases.\n- The findings of this work highlight the need for greater inclusion and representation of underrepresented communities, particularly those from regions with limited internet presence, in the development of T2I models."
            },
            "weaknesses": {
                "value": "- This work focuses exclusively on generated images based on nationality, which raises several concerns. For instance, in racially diverse countries like the United States, how can an image indicate that someone is a U.S. citizen without relying on stereotypes and social biases? Additionally, in regions where people from neighboring countries may share similar facial features or cultures (such as North and South Korea), how useful can OASIS be in distinguishing these cases?\n- How can OASIS be useful beyond nationality domains? It is not clear in the paper how OASIS can be extended to evaluate other biases in T2I models, such as gender biases, racism, etc.\n- Overall, this is a solid piece of work; however, a deeper discussion of its limitations and the specific types of stereotypes for which OASIS may or may not be effective would be beneficial."
            },
            "questions": {
                "value": "Please, refer to the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "details_of_ethics_concerns": {
                "value": "This work labels people's images to their nationalities, which might sound offensive or unethical to some. Further review from the ethics experts would be necessary."
            }
        },
        {
            "summary": {
                "value": "The authors present OASIS, a set of 4 metrics/methods to analyze the presence of stereotypes in T2I models, how those stereotypes form, and the connections between them. The authors use 3 popular models for this analysis. In the paper, OASIS shows the performance of these models on various prompts (e.g. \"Show me a picture of a Mexican person\") and how certain stereotypes are strongly associated with the prompt, at rates greater than those stereotypes in the real world. The paper shows weaknesses in models in their attempts to prevent stereotypes such as revealing stereotypes when prompts or combined or in how they produce low-variance along the stereotype's dimension. The paper also shows how certain stereotypes are clustered together and at what stage in the image generation process these models form (which is almost always in the early stages). \n\nIn all, OASIS provides a useful toolkit for understanding stereotypes in T2I models and highlights the importance for further work in this very important area."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper was very well-written, and it was quite easy to follow. I greatly appreciated how the terminology was constantly linked to concrete examples. The general formatting was also intuitive. \n\nThe paper did a good job of thinking about the broader context of this work, thinking a bit more critically about what phrases like biases actually mean and what we should think about when studying these models. Also incorporating real-world census data was a neat idea. \n\nI liked the results about how the biases are being generated early in the image-generation process.\n\nThe formulation for the stereotype score was quite intuitive and seems easy enough to compute at scale when provided with accurate descriptors from these LLMs."
            },
            "weaknesses": {
                "value": "I feel that overall, there was a lack of bench-marking for the methods presented in this paper beyond providing a few visual examples (which are still useful!). There were a few parts of the paper where I felt this significantly \n\n1. Computing the stereotype score: How accurate is the probability computation? The intuition of the metrics' formulation is clear, but there is no validation that the score provides accurate results in practice. As the paper mentions, CLIP is a biased VLM on its own, so its not fair to assume that this score will work perfectly. A smaller dataset demonstrating the accuracy of the score could be quite useful. \n\n2. Spectral variety along a stereotype. There was a nice description of each method for measuring $\\delta A$ in the appendix, but its not clear which one is used in the paper and which one is more accurate in practice. \n\n3. The biases of models like CLIP are well-documented (e.g. Birhana, Prabhu, & Kahembwe, 2021). If CLIP is being used extensively to identify additional biases, I think it makes sense to add more validation about how said biases may or may not be affecting the performance of OASIS. \n\n4. I understand the SPI claims about how certain stereotypes are intrinsically associated with a concept. However, the paper only provides a handful of examples of where this is occurring. I would appreciate a larger-scale analysis that includes something like 100 contexts generated in some way (i.e. top 100 most common ethnicities), and the average step at which each stereotype attribute occurs. This would be much more convincing than what could be cherry-picked examples. \n\nI think that the breadth of the experiments/validation were also a bit limited. I understand that gaining access to real-world data for a ground truth distribution is not always feasible, but consistently focusing on broad nationalities feels like it lacks nuance. \n\nAnother broad critique is that certain figures were unclear. I'll bring up those missing details in the following sections"
            },
            "questions": {
                "value": "Is this definition of stereotypes correct? Does it not implicitly push the stereotypes in society that cause imbalances to exist? Relying on census data to define an expected rate of a given attribute is perpetuating the stereotypes that exist in today's society. For example, using the example provided in the paper, if there are less women than male doctors in the world, it doesn't necessarily mean that the model is creating a 'stereotype' by generating an equal number of woman and male doctor images. \n\nFor experimental details, what is each point in Figure 3? What does each point correspond to in terms of attribute? \n\nIn Figure 5, there is a claim that \"We observe that some models have lower stereotypes at the cost of lower attribute variance\". This suggests a trade-off can exist between these two, but there is not much correlation observed for any model across nationalities. I agree that both metrics are important to measure, but it would be great to get more clarification on if there indeed is a trade-off that occurs often. \n\nI would appreciate any additional discussion about the accuracy of the metrics as well and if they work as intended."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to measure and understand stereotypes in Text-to-Image models. \n\n* Measurement: it introduces metrics to quantify the presence of stereotypes in generated images by comparing them to real-world distributions, and also assess diversity along stereotypical attributes. \n\n* Track Origins:  The author attempts to identify hidden associations that T2I model makes with certain concepts and trace how stereotypical attributes emerge over the generation process"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper tackles the significant problem of stereotypes in T2I models, which has important implications for ethical AI use and social impact.\n2. The paper is clearly written and organized. The visual presentations are good. \n3. It proposes StOP and SPI that try to understand the origin of the stereotypes, which may offer further insights."
            },
            "weaknesses": {
                "value": "1. The authors emphasize that their method is innovative in focusing specifically on stereotypes rather than just bias in the abstract. However, this distinction is not fully explained in the main content, and the methodology still seems quite similar to bias detection to me.\n\n2. The way they identify stereotypes is by prompting LLMs to generate stereotypical attributes for a given concept, but this method is already common in other papers. For example, [1] prompts LLMs to generate stereotypical / anti-stereotypical sentences for certain concepts and then identifies the stereotypical / anti-stereotypical attributes from those sentences.\n\n3. Although the paper includes many formulas, I found that the core idea is actually quite simple, and the formulas only make it harder to follow. If I understand correctly, the main process is: identify a concept, use LLMs to generate stereotypical attributes, generate images, measure the appearance of these attributes, and compare to real-world distributions & similarities across groups. This could be communicated in just a few sentences, which would make the approach clearer.\n\n4. I am concerned with the idea of comparing to real-world distributions. For example, men are dominant in many professions even today. In society, an educated person can be aware of stereotypes to some extent, but telling the model that it\u2019s okay to learn patterns such as \u201cmen are more likely to be doctors\u201d because it reflects real-world distributions doesn\u2019t seem fully convincing to me.\n\n[1] chatgpt based data augmentation for improved parameter-efficient debiasing of llms"
            },
            "questions": {
                "value": "I would appreciate it if the authors addressed the questions mentioned in the weaknesses section, and I will raise the score if their responses are convincing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}