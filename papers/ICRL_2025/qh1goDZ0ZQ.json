{
    "id": "qh1goDZ0ZQ",
    "title": "Towards Efficient Mixture of Experts: A Holistic Study of Compression Techniques",
    "abstract": "Scaling large language models has driven remarkable advancements across various\ndomains, yet the continual increase in model size presents significant challenges\nfor real-world deployment. The Mixture of Experts (MoE) architecture offers a\npromising solution by dynamically selecting and activating only a subset of experts\nduring inference, thus substantially reducing computational costs while preserving\nhigh performance. Despite these benefits, MoE introduces new inefficiencies, such\nas excessive parameters and communication overhead. In this work, we present\na holistic study on compression techniques of Mixture of Experts to enhance\nboth efficiency and scalability. While recent efforts have focused on reducing the\nnumber of experts, these approaches still suffer from considerable communication\nand computational costs. To address this, we propose more aggressive strategies,\nsuch as Layer Drop, which removes entire MoE layers, and Block Drop, which\neliminates transformer blocks. Surprisingly, these aggressive structure pruning\ntechniques not only preserve model performance but also substantially improve\nefficiency. Additionally, beyond Expert Trimming, we also introduce Expert\nSlimming, which compresses individual experts to further boost performance and\ncan be seamlessly integrated with Expert Trimming. Extensive experimental results\ndemonstrate the effectiveness of our proposed methods \u2014 Layer Drop and Block\nDrop \u2014 along with the comprehensive recipe that integrates Expert Slimming and\nExpert Trimming, achieving a 6.05\u00d7 speedup with 77.1% reduced memory usage\nwhile maintaining over 92% of performance on Mixtral-8\u00d77B. Our code will be\nmade publicly available upon acceptance.",
    "keywords": [
        "Mixture of Experts",
        "Model Compression"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We conducted a detailed investigation into MoE compression, providing a systematic understanding of its efficiency challenges. Building on these insights, we propose a comprehensive approach to further enhance the efficiency.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=qh1goDZ0ZQ",
    "pdf_link": "https://openreview.net/pdf?id=qh1goDZ0ZQ",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a comprehensive study on compression techniques for Mixture of Experts (MoE) architectures to enhance efficiency and scalability in large language models. The authors propose Expert Trimming strategies like Layer Drop and Block Drop, which remove entire MoE layers and transformer blocks, and Expert Slimming to compress individual experts, all aimed at reducing computational costs and memory usage while preserving high performance. Experimental results demonstrate that these methods achieve a 6.05\u00d7 speedup and 77.1% reduced memory usage, maintaining over 92% of the performance on the Mixtral-8\u00d77B model, offering a promising solution for real-world deployment challenges."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Pros.\n1.\tExpert Trimming and Expert Slimming are introduced in the paper and are well-studied comprehensively. \n2.\tThe evaluation result of Expert Trimming analyzes the impact of different compression methods, including Expert Drop, Layer Drop, and Block Drop. The analysis provides the instructive meaning to choose the compression method of Expert Trimming.\n3.\tThe comprehensive compression performance comprised of Expert Trimming and Expert Slimming is provided."
            },
            "weaknesses": {
                "value": "Cons.\n1.\tThe novelty is a big concern. The proposed method is a combination of existing works.\n2.\tThe baselines of SOTA works are expected in the table to show the effectiveness of the proposed methods."
            },
            "questions": {
                "value": "See weeknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work investigates compression techniques for MoE, including Layer Drop and Block Drop, which remove MoE layers and transformer blocks, respectively, enhancing efficiency without compromising performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-written.\n2. The figures are easy to understand and visually appealing.\n3. The observations related to expert trimming are meaningful, and the trade-offs between expert drop, layer drop, and block drop are well-considered."
            },
            "weaknesses": {
                "value": "1. The approach presented is straightforward, combining expert/layer/block drop with quantization to compress the MoE model. While the authors claim to introduce two novel techniques, Layer Drop and Block Drop, these methods are not truly new [1].\n2. In line 371, the authors state, \"Mixtral-8\u00d77B maintains over 90% of the original performance\"; however, would training from scratch with a similarly compact architecture also achieve comparable accuracy? Authors could conduct an ablation study comparing their compressed model to a model trained from scratch with a similar architecture. \n3. Additionally, the authors propose simultaneously removing both the MoE and Norm layers, yet this strategy lacks sufficient novelty or justification. Authors may conduct ablation studies showing the impact of removing each layer type individually vs. together.\n4. There are also typos in lines 311 and 312: \u201cIn this section, we evaluate the effectiveness of Expert Trimming techniques.\" Additionally, there's an extra \"-\" in line 264.\n5. Regarding the results, Table 3 indicates that the combined use of Expert Trimming and Expert Slimming (quantization) provides advantages over using only AWQ. However, the performance improvement is marginal. Please discuss the practical significance of these marginal improvements.\n6. There is no clear conclusion about which expert trimming is the best. The authors can include a more discussion. This could help readers quickly understand the trade-offs between the different approaches in terms of performance, efficiency, and ease of implementation.\n\n[1] Reducing transformer depth on demand with structured dropout."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a pruning method for MoE LLMs including experts trimming: layer drop and block drop and Expert slimming: other parameter pruning or quantization strategy. Experiments on some LLM benchmarks are provided."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "LLMs efficiency is an very important topic for the deployment of large language models. The idea proposed in this paper is simple.\nThe paper is clearly written."
            },
            "weaknesses": {
                "value": "1. Some typos: \n(1) line 304: Expert Slimmingand -> Expert Slimming and\n(2) line 311: n this Section -> In this Section\n(3) line 311: Expert Trimmingtechniques -> Expert Trimming techniques\n(4) line 43:  tandard MoE -> Standard MoE\n(5) line264: efficiency. - -. efficiency.\n\n2. The primary issue seems to be the paper's contributions. Expert Trimming, as implemented, is essentially layer pruning for LLMs, whether using layer drop or block drop techniques. Additionally, expert slimming does not appear to be a novel contribution here. Given this, the paper's contributions feel somewhat limited in scope.\n\n3. Lack of comparison of important baselines. I believe there are many layer pruning baseline methods, It is important to give some comparisons in experiments."
            },
            "questions": {
                "value": "1. In line 292, \"we assess its importance score by evaluating the similarity between its inputs x_l and outputs y_l\", are x and y here similar as the x' and y' not x and y in layer drop?\n\n2. Regarding Figure 3, it\u2019s intriguing that the layer-wise expert drop consistently surpasses global strategies at high drop percentages. Could there be any underlying mechanism or explanation for why this approach maintains performance better?\n\n3. On the training pipeline for LLMs, the typical stages are: (1) pretraining, (2) supervised fine-tuning (SFT), and (3) alignment. Based on the authors' findings, in which of these stages might the proposed pruning method be most effectively implemented?\n\n4. This proposed expert or layer pruning method utilizes static pruning. Are there any reasons why dynamic pruning, which adjusts layers for different tokens, was not considered? or any comparisons?\n\n5. From Table 3, there appears to be a performance discrepancy between Mistral MoE and DeepSeek MoE. Specifically, block drop or layer drop methods seem to perform considerably worse on DeepSeek. Could there be an underlying reason for this difference? So I am very confuse of the authors' conclusion that \"in Layer Drop and Block Drop excel in speedup as illustrated in Figure 8, with Block Drop demonstrating both higher performance and greater speedup\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}