{
    "id": "teE4pl9ftK",
    "title": "Gradient-Free Generation for Hard-Constrained Systems",
    "abstract": "Generative models that satisfy hard constraints are crucial in scientific applications, e.g., numerical simulations, dynamical systems, and supply chain optimization, where physical laws or system requirements must be strictly respected. However, many existing constrained generative models, especially those developed for computer vision, rely heavily on gradient information, which is often sparse or computationally expensive in other fields, e.g., partial differential equations (PDEs). Accurately solving these problems numerically demands the generated solutions to comply with strict physical constraints, e.g., conservation laws. In this work, we introduce a novel framework for adapting pre-trained, unconstrained generative models to exactly satisfy constraints in a zero-shot manner, without requiring expensive gradient computations or fine-tuning. Our framework, ECI sampling, alternates between extrapolation (E), correction (C), and interpolation (I) stages during each iterative sampling step to ensure accurate integration of constraint information while preserving the validity of the generated outputs. We demonstrate the efficacy of our approach across various PDE systems, showing that ECI-guided generation strictly adheres to physical constraints and accurately captures complex distribution shifts induced by these constraints. Empirical results show that our framework consistently outperforms baseline approaches in both zero-shot constrained generative and regression tasks, and achieves competitive results without additional fine-tuning.",
    "keywords": [
        "Flow Matching",
        "Generative Model",
        "Constrained Generation",
        "Partial Differential Equations",
        "Conservation Laws"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "We propose ECI sampling, a gradient-free approach for guiding pre-trained generative models for hard-constrained generation.",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=teE4pl9ftK",
    "pdf_link": "https://openreview.net/pdf?id=teE4pl9ftK",
    "comments": [
        {
            "summary": {
                "value": "In this work, the authors propose ECI for gradient-free generation for constraint systems. ECI is zero-shot and doesn't need tuning the model. It works through alternating between extrapolation (E), correction (C), and interpolation (I) in each sample step. The empirical results show the model achieves competitive performance across various PDE systems compared with baseline models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The work is well-motivated, it's important in scientific applications to have constraint generation\n2. The proposed method is effective and does have constrained generation. Also, the method is zero-shot which is a big benefit for constraint sampling method. Although it's based on functional FM not FM in general\n3. The experimental results show advantages over other baseline models."
            },
            "weaknesses": {
                "value": "1. The writing is also a bit confusing. Actually comments from reviewer KM5e help me better understand the algorithm 2. \n2. The authors are recommended to better formulate the contribution of this work. In particular, the model works in functional space and applies projection to constraint spaces to guarantee hard-constraint met. \n3. The authors mention supply chain optimization in abstract and introduction as a hard-constrained system. However, it lacks experiments on such problems. It would be better to showcase some applications beyond PDE learning (which I believe is broad)."
            },
            "questions": {
                "value": "1. How does proposed ECI compared with FFM without ECI in experiments? It would better show the improvement from ECI in sampling. \n2. In section 3.3, the authors mention resampling noise during sampling. Can the authors explain why this helps better generative results?\n3. The model assumes a perfectly trained FFM, however there is inevitable error in the trained generative model. How does such error affect the performance? Will such error break the constraint in sampling?\n4. Empirical results are on low-resolution problems. Can the authors comment on how ECI work at larger scale?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces ECI sampling, a framework that adapts pre-trained, unconstrained generative models to exactly satisfy hard constraints in a zero-shot manner without requiring gradient computations or fine-tuning. This framework alternates between extrapolation, correction, and interpolation stages during each iterative sampling step to integrate constraint information while preserving the validity of generated outputs. Empirical results demonstrate that ECI sampling strictly adheres to physical constraints across various PDE systems and outperforms baseline approaches in zero-shot constrained generative and regression tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is easy to follow with high readability. The problem setting is well motivated and important.\n2. The paper provides quite comphrenseive numerical results with comparison to relevant benchmarks. The extension to regression setting is also quite impressive.\n3. The paper also provides a quite detailed ablation study on the choice of algorithm hyperparameters."
            },
            "weaknesses": {
                "value": "*1. Clarification of Problem Setup*\n\nThe problem setup is not sufficiently explained, which may lead to confusion. Although the authors repeatedly emphasize that ECI is intended for the generative modeling of constrained PDE solutions, it remains unclear what distribution of constrained solutions ECI aims to recover. Specifically, is it targeting a uniform distribution over the space of constrained solutions $\\mathcal{U}_{|\\mathcal{G}}$? The authors should invest more effort in formally clarifying the problem setup.\n\n*2. Discussion of ECI\u2019s Advantages and Comparisons*\n\nThe paper appears to lack a section that discusses the benefits and potential sources of ECI\u2019s superior performance, as well as comparisons with other existing approaches mentioned in the work. Including such a discussion would help readers better understand both the relevant literature and the proposed method.\n\n*3.Validation of PDE System Satisfaction*\n\nWhile ECI ensures the exact satisfaction of the constrained operator by applying a correction operator after each one-step extrapolation, it is unclear why the solution obtained at time 0 still satisfies the PDE system $\\mathcal{F}_{\\phi} u(x) = 0$. In Algorithm 2, the next-step solution is generated by forward noising starting from the predicted solution at time 1. Does this noising process preserve the satisfaction of the PDE system? Additionally, is the linear interpolation of two PDE solutions a valid PDE solution, implying that the PDE system is linear? Further explanations are needed to clarify these points.\n\n*4.Missing References on Gradient-Free Guidance*\n\nSome references on gradient-free guidance are missing, such as [1] and [2]. It appears that the proposed method is related to [2], involving functional FM and additional projection steps. Providing more clarification on these connections would be appreciated.\n\n[1]Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli S Sastry, Siddharth Gururani, Sageev Oore, and Yisong Yue. Symbolic music generation with nondifferentiable rule guided diffusion. arXiv preprint arXiv:2402.14285, 2024.\n\n[2]Chung, Hyungjin, et al. \"Diffusion posterior sampling for general noisy inverse problems.\" arXiv preprint arXiv:2209.14687 (2022)."
            },
            "questions": {
                "value": "1. This is the most critical question. How does ECI guarantee that the achieved solution satisfies the PDE system? In computer vision tasks where differences from ground truth images are permissible.  In this work, boundary conditions are guaranteed, but how does ECI ensure that the generated data truly represents solutions to the PDE? More specifically, why the linear interpolation between two PDE solutions is still a valid PDE solution?\n\n2. Can you elaborate more on the stochasticity of generated solutions, as is discussed in Sec 3.3. Does more stocahsticity mean the generated distribution is distribution-wise closer to the uniform distribution over the space of constrained PDE solutions?\n3. For the extrapolation step, will the algorithm performance be better if more a refined scheme is used? For example, mutiple step extrapolation when time $t$ is far from $1$ and one step when $t$ is close to $1$. From my understanding, the quality of predicted $\\hat u_1$ is crucial to the algorithm performance. An additional ablation study could be performed here.\n4. Can you comment on the computational efficiency of ECI compared with existing approaches?\n\nThe current score assumes a positive answer to the first question. My score will be adjusted accordingly based on the authors' response."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "title": {
                "value": "Rebuttal to 11X9"
            },
            "comment": {
                "value": "We thank your recognition of our work's **extensive evaluations** and your constructive suggestions to improve the presentation of our work. We apologize for some not-well-explained notations with our assumption that all reviewers were familiar with generative models, especially flow matching. We have included a more detailed mathematical introduction to flow matching in the common rebuttal session, and we will further elaborate on some additional details to address your questions and concerns.\n\n## Q1 Notations\n\nWe first clarify some notations in addition to Section 3.1. We use $u:\\mathcal{X}\\subseteq\\mathbb{R}^d\\to \\mathbb{R}$ to denote a continuous function as the solution. The subscript always indicates the time step in the flow. We follow the flow matching convention such that $t=0$ corresponds to the noise and $t=1$ corresponds to the target (data). As the space of continuous functions has an infinite dimensionality, we follow FFM [1] to adopt measure-theoretical terms of $\\mu_0,\\mu_1$ as probability measures over the continuous functions, instead of using the common $p_0,p_1$ in computer vision domains. All $p,q$ in our work are probability densities defined as the  Radon\u2013Nikodym derivative (see the FFM paper for technical details). Specifically, at each time step $t$, the noised data $u_t:=\\psi_t(u_0|u_1)$\u200b\u200b can be obtained with *interpolation*. We noted that **all of the above definitions are standard notations in flow matching context**.\n\nWe do agree Equation 2 is less well-defined as we postponed the definitions in the bulletin list later. We will **modify the order in our revised manuscript** to first introduce and define the notations to avoid potential confusion. We will also follow your suggestion to move Proposition 1 to the main text to make it more understandable.\n\n## Q2 Algorithm\n\nYour understanding of our Algorithm 2, the core algorithm for ECI sampling, is definitely correct. We originally put it in a more concise way mainly because **all notations are quite standard in diffusion or flow-based generative models**. For example, [2, 3] also adopts an iterative formulation for guiding diffusion models with a double loop. We will consider reformatting the algorithm to include more details following your outline.\n\nStill, we would like to mention that both extrapolation and interpolation stages for flow matching have clear definitions with respect to the probability path induced by the flow. Specifically, the linear interpolation corresponds to the *OT-path* (OT for optimal transport) in flow matching literature [4] with theoretical benefits of optimal transport. It is **not our whimsical assumption but theoretically a good option** in choosing the probability paths. \n\nAlso, due to the iterative nature of flow sampling, ECI sampling needs to advance the solver for the flow ODE (see common rebuttal session). Therefore, the last step of advancing the solver to a large $t'=t+1/N$\u200b is also necessary.\n\n## Q3 Other Remarks\n\n- Regarding $\\mathcal{F}_\\phi$. Your understanding is correct. We will clarify it.\n- Regarding the domain of $x$. We assume the normal domain of $\\mathcal{X}$ for $\\mathcal{F}_\\phi$.\n- We used the notation $\\mathcal{U_{|F}}$ to indicate the solution set is *restricted* by the constrained to a narrower subset.\n- We used $:=$ for definition (of new quantities). For the push-forward, we use the standard notation of $(\\psi_t)_*$\u200b, e.g., the second part of Section 3.1.\n- For the correction algorithms in Algorithm 3&4, they serve as concrete examples of the correction $C(u_1,\\mathcal{G})$ in Equation 3&4 and Algorithm 2.\n\nWe also thank your other suggestions regarding the notations and formatting for better conciseness and understandability. We will add more clarifications on notations, more mathematical backgrounds, and better presentations in our revised manuscript to make our work more accessible to a larger audience.\n\n\n\n[1] Kerrigan, Gavin, Giosue Migliorini, and Padhraic Smyth. \"Functional flow matching.\" *arXiv preprint arXiv:2305.17209* (2023).\n\n[2] Lugmayr, Andreas, et al. \"Repaint: Inpainting using denoising diffusion probabilistic models.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2022.\n\n[3] Kawar, Bahjat, et al. \"Denoising diffusion restoration models.\" *Advances in Neural Information Processing Systems* 35 (2022): 23593-23606.\n\n[4] Lipman, Yaron, et al. \"Flow matching for generative modeling.\" *arXiv preprint arXiv:2210.02747* (2022)."
            }
        },
        {
            "title": {
                "value": "Rebuttal to KM5e (Reference)"
            },
            "comment": {
                "value": "[1] Lugmayr, Andreas, et al. \"Repaint: Inpainting using denoising diffusion probabilistic models.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2022.\n\n[2] Kawar, Bahjat, et al. \"Denoising diffusion restoration models.\" *Advances in Neural Information Processing Systems* 35 (2022): 23593-23606.\n\n[3] Li, Zongyi, et al. \"Fourier neural operator for parametric partial differential equations.\" *arXiv preprint arXiv:2010.08895* (2020).\n\n[4] Li, Zongyi, et al. \"Anima Anandkumar, Physics-informed neural operator for learning partial differential equations.\" *arXiv preprint arXiv:2111.03794* (2021).\n\n[5] Herde, Maximilian, et al. \"Poseidon: Efficient Foundation Models for PDEs.\" *arXiv preprint arXiv:2405.19101* (2024).\n\n[6] Sun, Jingmin, et al. \"Towards a Foundation Model for Partial Differential Equation: Multi-Operator Learning and Extrapolation.\" *arXiv preprint arXiv:2404.12355* (2024).\n\n[7] Huang, Jiahe, et al. \"DiffusionPDE: Generative PDE-solving under partial observation.\" *arXiv preprint arXiv:2406.17763* (2024).\n\n[8] N\u00e9giar, Geoffrey, Michael W. Mahoney, and Aditi S. Krishnapriyan. \"Learning differentiable solvers for systems with hard constraints.\" *arXiv preprint arXiv:2207.08675* (2022)."
            }
        },
        {
            "title": {
                "value": "Rebuttal to KM5e"
            },
            "comment": {
                "value": "We thank your recognition of our work's **comprehensive experimental results** and **concise formulation**. We are happy to address your questions and concerns. Still, we noted the potential misinterpretation of our proposed approach as a regression model but not as a generative model. See the common rebuttal for more details, clarifications, and mathematical backgrounds.\n\n## Q1 Results in Figure 6-8\n\nFigure 6-8 demonstrates the generation statistics of point-wise mean and standard deviation, together with the ground truth statistics. Similar to Figure 1&2, a closer resemblance to the ground truth statistics indicates a better generation result. It is worth noting, though, that MMSE and SMSE only reflect the first two moments of the distribution but not higher-order information. Therefore, FPD scores should be preferred, in which the pre-trained PDE foundation model Poseidon is used to extract the hidden representations (see Appendix D.1). We have included the detailed experimental setups in Appendix B and we will add more explanations for Figure 6-8.\n\n## Q2 Experiments with different IC/BC\n\nThanks for the insightful suggestion. We are running experiments on the Stokes dataset with different IC/BC values to further demonstrate the generalizability and effectiveness of our approach. We will update the results once they are readily available. \n\n## Q3 Overall framework\n\nThe overall framework of our ECI sampling can be understood as a modification to the vanilla flow sampling process in Algorithm 1. With minimum modifications, we demonstrate the ECI sampling in Algorithm 2. Previous work in guiding diffusion models also adopts an iterative formulation [1, 2], which is in fact **standard in the context of diffusion or flow-based generation models**. We understand that the reviewer may not be familiar with the generative models and we have provided more details in the common rebuttal session.\n\nRegarding the datasets used in our work, we noted all datasets and their spatiotemporal resolution setups are directly **taken from previous papers** (see Appendix B for reference of the datasets in previous papers). Therefore, we believe such an experimental setup is **standard, consistent, and comparable with existing works**. To the best of our knowledge, **4D datasets in PDE systems do not exist so far**. It is also a **standard practice** to test on 2D and 3D datasets in the PDE domain, e.g., neural operator learning [3, 4], foundation models [5, 6], and existing constrained generation [7, 8]. Therefore, we believe we have already provided enough evidence of the superior performance of our proposed method.\n\nWe already included the dataset descriptions, specifications, and task formulation in Table 2&7, Appendix B&D.2, for which our ECI sampling serves as a unified constrained generation framework.\n\n## Q4 Improvement in the whole domain\n\nWe note that our ECI sampling was designed to offer **iterative control** in each flow sampling step to mitigate the artifacts between the constrained and unconstrained regions. Indeed, our generation result demonstrated **remarkable consistency in the whole spatiotemporal domain**, with hardly noticeable artifacts around the boundary in Figure 1, 6, 7, and 8. This is in sharp contrast with the naive projection algorithm like ProbConserv, which can be only applied to the final prediction without fine-grained control over each sampling step. In the close-up demonstration in Figure 5, noticeable artifacts can be found for ProbConserv and other gradient-based approaches.\n\nThe iterative control of ECI sampling that gradually pushes the intermediate noised data into generations with hard constraints can be further demonstrated in Figure 4. Note how the generation trajectory gradually exhibits more consistency between the constrained and unconstrained regions.\n\n## Q5 Zero-shot effectiveness\n\nWe emphasize that **all experiments are carried out in a zero-shot manner** except for the conditional FFM baseline. Specifically, an unconstrained FFM was first trained as the generative prior. Different approaches including our ECI sampling were then applied in a zero-shot manner (see Appendix C&D.2). In this way, our framework provides a **unified approach for different constraints** without finetuning or conditional training (e.g., IC and BC in the Stokes problem). The generative evaluation metrics also demonstrated the state-of-the-art performance across all zero-shot models (conditional FFM is not zero-shot and should be viewed as an upper bound, as we have discussed in Section 4.1)."
            }
        },
        {
            "title": {
                "value": "Rebuttal to all reviewers"
            },
            "comment": {
                "value": "Dear Reviewers,\n\nWe sincerely appreciate your reviews that help make our work more concrete and clear. We thank you for pointing out the strengths in our work. Sadly, we note a common **misunderstanding and misinterpretation among all the reviewers** and we would first like to clarify this. \n\n### **Our proposed ECI sampling framework is NOT a regression model** (e.g., FNO [1]), **nor a plugin for existing regression models** (e.g., BOON [2]). **Instead, ECI sampling is a generative framework for functional data based on flow matching.**\n\nWe believe all reviewers have misinterpreted this setting and have asked for a comparison with regression models or using regression metrics, which are not applicable to our problem setup. We do apologize for our assumption that this paper's audience is familiar enough with generative models, especially flow matching. We now add a more detailed introduction to flow matching here to make our paper more accessible to the audience in the SciML domains.\n\n## Flow Matching\n\nFlow matching [3] is a generative framework built on continuous normalizing flows. Flow matching tries to learn the time-dependent *vector field* $v_t:\\mathbb{R}^d\\times [0,1]\\to\\mathbb{R}^d$ that defines a continuous time-dependent diffeomorphism called the flow $\\psi_t:\\mathbb{R}^d\\times [0,1]\\to\\mathbb{R}^d$ via the following *flow ODE*:\n$$\n\\frac{\\partial}{\\partial t}\\psi_t(x_0)=v_t(\\psi_t(x_0)),\\quad x_0\\sim p_0(x)\n$$\nThe flow induces a probability path with the push-forward $p_t=(\\psi_t)_*p_0$ for generative modeling. [3] demonstrated the conditional vector field $u_t(x|x_1)$ can be calculated simulation-free while sharing the same gradient with the flow matching objective. In short, flow matching models try to learn the conditional vector field which defines the dynamics of the flow and the probability path.\n\n[3] further demonstrated the conditional probability path can be manually defined with linear interpolation between the noise and target (the *OT-path*). [4] further extended flow matching to Riemannian manifolds where the interpolation has a more intuitive interpretation of geodesic interpolation. Either way, it is clear that **in all flow-matching literature, extrapolation and interpolation have well-defined meanings with respect to the probability path induced by the flow**. \n\nBased on the above observation, we can clearly summarize the difference between regression models (neural operators) and our generative framework as follows:\n\n- ECI sampling, as a flow-based generative framework, can **output a distribution of solutions**. In contrast, any regression model (neural operator) can only output one deterministic prediction.\n- As a special case of the flow matching model, ECI sampling is **an iterative process that gradually *denoises*** the current noised samples into meaningful ones by solving the flow ODE above using the Euler method or any ODE solvers. See Algorithm 1 and Figure 4 for demonstration. In contrast, neural operators directly output the final prediction.\n- The **learning objective of flow matching is completely different** from neural operators. The former tries to minimize the vector field discrepancy whereas the latter directly predicts the final outcome.\n- The generative nature requires **generative evaluation metrics instead of regression metrics**. As both the output and the ground truth are a set of solutions instead of a single solution, distributional properties should be compared as generative metrics.\n\nWe further noted that the **iterative nature** of diffusion and flow-based models makes it **non-trivial to guide**, as intermediate steps are noises instead of final predictions. Indeed, we have included existing constrained generation and inverse problem works in computer vision in the related work. However, the continuous nature of flow matching makes their approaches not directly applicable.\n\nWe understand the reviewers may not be familiar with the generative domains. As our work lies in the intersection between scientific ML and generative models, we urge the reviewers to **take the contributions in generative modeling of our work \u2014 which is our major contribution \u2014 into consideration** to give a more comprehensive evaluation of this work.\n\nWarm regards,\n\nAuthors.\n\n[1] Li, Zongyi, et al. \"Fourier neural operator for parametric partial differential equations.\" *arXiv preprint arXiv:2010.08895* (2020).\n\n[2] Saad, Nadim, et al. \"Guiding continuous operator learning through physics-based boundary constraints.\" *arXiv preprint arXiv:2212.07477* (2022).\n\n[3] Lipman, Yaron, et al. \"Flow matching for generative modeling.\" *arXiv preprint arXiv:2210.02747* (2022).\n\n[4] Chen, Ricky TQ, and Yaron Lipman. \"Riemannian flow matching on general geometries.\" *arXiv preprint arXiv:2302.03660* (2023)."
            }
        },
        {
            "title": {
                "value": "Rebuttal to rhkG (Part IV)"
            },
            "comment": {
                "value": "## Q8 Generation Performance\n\nWe have noted the **misunderstanding of our contributions** of ECI sampling, especially in terms of the correction stage in Q5 & Q6, and we have also noted the reviewer's **concerns for poor performance were ungrounded** in Q2. Indeed, across all 2D and 3D generative datasets, ECI sampling achieves state-of-the-art performance on most evaluation metrics. It is unclear to us **why the reviewer emphasized solely the MMSE for the NS equation but deliberately ignored all of the other metrics and datasets** in which our ECI-sampling achieved the best results.\n\n\n\n# Other Questions\n\n## Q9 Conservation Laws\n\nIt is always possible to apply alternative correction algorithms for conversation laws, but **unnecessary**, as 1) this correction stage is not our contribution; and 2) even the simple projection algorithm works pretty well with our iterative framework.\n\n## Q10 Higher Dimensional Data\n\nWe reiterate that the correction stage is the least important stage in our algorithm, as even a simple projection algorithm can lead to decent results. Therefore, we believe our ECI sampling can be scaled to higher dimensional data while preserving its competitive performance. However, as we have mentioned in Q2, such data are, if not none, scarce for training a generative model. We believe our existing experiments across various 2D and 3D PDE systems already demonstrated the superior performance of ECI sampling.\n\n## Q11 Frechet Poseidon Distance\n\nAs we have mentioned at the very beginning of our rebuttal, our ECI sampling is still a **generative framework** for which generative evaluation metrics for distributional properties should be used instead of the normal regression metrics. In this sense, **the ground truth is a set of solutions** with no one-to-one correspondence, making it impossible to calculate the regression metrics.\n\nIn contrast, inspired by the *Frechet inception distance* (FID), a widely used metric in image generation domains, we proposed to use the foundation model Poseidon to give a **more comprehensive evaluation of the distributional property**, in which our model excels over baselines. See Appendix D.1 for details.\n\n[1] Saad, Nadim, et al. \"Guiding continuous operator learning through physics-based boundary constraints.\" *arXiv preprint arXiv:2212.07477* (2022).\n\n[2] N\u00e9giar, Geoffrey, Michael W. Mahoney, and Aditi S. Krishnapriyan. \"Learning differentiable solvers for systems with hard constraints.\" *arXiv preprint arXiv:2207.08675* (2022).\n\n[3] Li, Zongyi, et al. \"Anima Anandkumar, Physics-informed neural operator for learning partial differential equations.\" *arXiv preprint arXiv:2111.03794* (2021).\n\n[4] Mouli, S. Chandra, et al. \"Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs.\" *arXiv preprint arXiv:2403.10642* (2024).\n\n[5] Li, Zongyi, et al. \"Fourier neural operator for parametric partial differential equations.\" *arXiv preprint arXiv:2010.08895* (2020).\n\n[6] Li, Zongyi, et al. \"Anima Anandkumar, Physics-informed neural operator for learning partial differential equations.\" *arXiv preprint arXiv:2111.03794* (2021).\n\n[7] Herde, Maximilian, et al. \"Poseidon: Efficient Foundation Models for PDEs.\" *arXiv preprint arXiv:2405.19101* (2024).\n\n[8] Sun, Jingmin, et al. \"Towards a Foundation Model for Partial Differential Equation: Multi-Operator Learning and Extrapolation.\" *arXiv preprint arXiv:2404.12355* (2024).\n\n[9] Huang, Jiahe, et al. \"DiffusionPDE: Generative PDE-solving under partial observation.\" *arXiv preprint arXiv:2406.17763* (2024).\n\n[10] Kerrigan, Gavin, Giosue Migliorini, and Padhraic Smyth. \"Functional flow matching.\" *arXiv preprint arXiv:2305.17209* (2023).\n\n[11] Lugmayr, Andreas, et al. \"Repaint: Inpainting using denoising diffusion probabilistic models.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2022.\n\n[12] Kawar, Bahjat, et al. \"Denoising diffusion restoration models.\" *Advances in Neural Information Processing Systems* 35 (2022): 23593-23606."
            }
        },
        {
            "title": {
                "value": "Rebuttal to rhkG (Part III)"
            },
            "comment": {
                "value": "# Contribution Concerns\n\n## Q4 Concept of ECI\n\nWe respectfully disagree with the reviewer's claim that ECI has been \"*well studied*\". We have noted in the previous clarification of the reviewer's misunderstanding that **extrapolation and interpolation have unique meanings in flow-based models** and are **completely different from concepts in regression-based methods** like the projected gradient. \n\nIn flow matching, extrapolation and interpolation are geometric concepts related to the push-forward measures $\\mu\\_t=(\\psi\\_t)\\_* \\mu\\_0$ on the probability path $\\\\{\\mu_t,0\\le t\\le 1\\\\}$ [10] . Here $\\mu_0$ is the initial (fixed) noise measure, $\\psi_t$ is the flow (possibly defined with the learned vector field), and $(\\psi_t)_*$ denotes the push-forward. When conditional on the target solution $u_1$\u200b, the extrapolation along the conditional probability path can be written in a variational way in Equation 2. Similarly, interpolation are defined along the probability path. In this way, extrapolation is never a step to \"minimize the objective\", and interpolation has no correspondence with the projected gradient. Except for the correction stage which we do not claim as our contribution (see Q5), **we do not see any similarity between our ECI and the projected gradient, nor do we know any existing flow-based constrained generation model that uses a similar approach**. \n\nFurthermore, with an iterative sampling process, the intermediate generation is partially noises (see Figure 4), making the projected gradient infeasible, as it operates on the final prediction in regression models. This further distinguishes our framework from existing works.\n\n## Q5 Novelty of ECI\n\nThe reviewer has a **significant misunderstanding of our contributions**. We do NOT claim any contribution in the correction stage  \u2014 we even manually placed it in the Appendix as it is less important. Instead, the **iterative control framework** with the concepts of extrapolation and interpolation unique to flow-based models (see Q4) using the interleaved ECI steps is our major claim for a better generation result. Furthermore, we almost **deliberately designed the correction step to be as simple as possible** to solidly demonstrate that the **good performance of the final generation comes from our iterative control framework** instead of from the correction algorithms like the projected gradient. \n\nIndeed, **even with such a naive correction algorithm, our ECI sampling still achieves remarkable generation results with hardly noticeable artifacts around the constrained region**. This is in sharp contrast with the ProbConserv baseline \u2014 a projection-based approach that directly operates on the final prediction without fine-grained iterative control \u2014 which exhibits noticeable and sharp change around the constrained region (see Figure 5 for a close-up result). Furthermore, in the correction stage in Figure 4, we demonstrate how the artifacts around the constrained boundary are gradually mitigated with our iterative ECI framework.\n\n## Q6 Correction Step\n\nWe have already clarified in Q4 & Q5 that **the correction stage is never our contribution**. We also explain in Q5 why we deliberately designed the correction step as simple as possible. Regardless, ECI sampling still achieves state-of-the-art generation performance thanks to its **iterative control** over the flow sampling. New correction algorithms are always readily applicable to our framework but are not necessary. For experiments in our paper, we already demonstrated simple correction algorithms suffice.\n\n## Q7 Hard-Constraint Generation\n\nThe hard-constraint generation task has **crucial importance** in physical systems like PDE, supply chains, and time series, where certain constraints like conservation laws have clear physical meanings and should always be respected. Previously, many regression models already tried to address this problem [1-4]. Our ECI sampling further extends such settings to generative modeling. Therefore, we respectfully disagree with the \"*limited scope*\" suggested by the reviewer, as we have already demonstrated the **effectiveness of ECI sampling across various 2D and 3D PDE datasets** and the potentially **wide applications in diverse SciML domains**.\n\nWe reiterate that extrapolation and interpolation have different and unique meanings in diffusion and flow-based generative models and that our contributions lie in the **iterative control of the flow sampling process by interleaving ECI steps**. This is completely different from any existing works, especially regression-based approaches like the projected gradient. In fact, though the extrapolation and interpolation for flow models (or the forward and backward diffusion processes for diffusion models) have been well-defined, **guiding diffusion or flow-based models is non-trivial**, and many existing works in the computer vision domain all adopted a similar zero-shot formulation [11, 12]."
            }
        },
        {
            "title": {
                "value": "Rebuttal to rhkG (Part II)"
            },
            "comment": {
                "value": "# Theoretical Concerns\n\n## Q1 Relation to projected gradient\n\nWe have already included a large amount of related work in the original manuscript. In the second part of related work, we reviewed existing work in the PDE domain for zero-shot constrained generation for **diffusion models**. In the third part, we also included related work in the computer vision domains which can be adapted for PDE tasks. We also include constrained prediction (*NOT generation*) work (e.g., [1-4]), which is essentially a special case of the projected gradient. However, we emphasize that we focus on **flow-based generative models**, so these regression approaches are **fundamentally different** from our problem step and can only serve as the correction stage of our framework.\n\nWe respectfully disagree with the reviewer's claim that \"*many methods share many similarities with ECI in that they perform unconditional update steps, and then project back into the feasibility region*\" and the lack of novelty. Though constrained prediction for regression models does exist (e.g., [1-4], as we have already cited them), they failed in the context of guiding diffusion or flow-based models that rely on iterative sampling. To the best of our knowledge, **we do not know any existing work for diffusion or flow-based models that share a similar idea with ours**. We would appreciate it if the reviewer could give more concrete references.\n\n\n\n# Experimental Concerns\n\n## Q2 Experimental Setup\n\nWe understand that the reviewer may be also not familiar with PDE domains. We noted all datasets and their spatiotemporal resolution setups are directly **taken from previous papers** (see Appendix B for reference of the datasets in previous papers). Therefore, we believe such an experimental setup is **standard, consistent, and comparable with existing works**. The datasets of PDE systems used in our work have also been the standard test cases in various papers (e.g. [5, 6]), which in turn demonstrated they are not \"simple\" cases, as the reviewer tried to suggest.\n\nWe further point out that the Navier-Stokes equation is a 3D dataset with 2D spatial and 1D temporal resolutions. To the best of our knowledge, **4D datasets in PDE systems do not exist so far**. It is also a **standard practice** to test on 2D and 3D datasets in the PDE domain, e.g., neural operator learning [5, 6], foundation models [7, 8], and existing constrained generation [2, 9]. Therefore, we believe we have already provided enough evidence of the superior performance of our proposed method.\n\nWe also believe the reviewer's claim that \"*ECI performs poorly in complex settings*\" is **ungrounded** due to their misinterpretation of our evaluation metrics (also see Q3). The MSE of the point-wise mean between the ground truth and the generation only partially reflects the resemblance of distributions. The MSE of the point-wise standard deviation is another metric, in which our model **significantly outperforms existing approaches**. Furthermore, inspired by the *Frechet inception distance* (FID), a widely used metric in image generation domains, we proposed to use the foundation model Poseidon to give a more comprehensive evaluation of the distributional property, in which our model excels over baselines. See Appendix D.1 for details.\n\n## Q3 Variance in the solution\n\nWe have noted this is one of the **misinterpretations** regarding our evaluation metrics. We do not claim a higher variance is a sign of bad solutions, as the reviewer suggested. Instead, SMSE **measures the difference in the point-wise standard deviation** of the generation with respect to the ground truth solution set. Therefore, a smaller SMSE indicates **a closer resemblance of variance** to the ground truth solution distribution, thus a better generation result."
            }
        },
        {
            "title": {
                "value": "Rebuttal to rhkG (Part I)"
            },
            "comment": {
                "value": "Thank you for your appreciation of our work's **clear presentation**, **sufficient ablation studies**, and **effectiveness regarding hard constraints**. We will address your concerns as follows.\n\n# Significant misunderstanding and misinterpretation of our work\n\nWe would first like to point out a **significant misunderstanding and misinterpretation** of our work. We address the constrained generation of the flow matching framework. The guided flow matching model during the iterative sampling process is still a **generative model** whose output is a **distribution over solutions** instead of a single ground truth solution. Sadly, we noted the reviewer emphasized the projected gradient approaches which are **infeasible for flow-based generative models**. The reviewer also **misinterpreted our generative evaluation metrics** and **emphasized constrained models in regression models** like projected gradient, which was not our claimed contribution.\n\nWe further distinguish our **problem and experimental setup** from existing work, especially for the projected gradient-related methods, in the following aspects.\n\n- **Generative modeling**. Unlike projected gradient-related methods, our proposed framework is a generative framework that captures the distribution of solutions instead of a single solution.\n- **Challenge**. The challenge for guiding flow (or diffusion) based models lies in its iterative sampling process, as we have mentioned in the second part of related work and Table 1. The iterative nature makes it infeasible to employ normal projected gradient approaches.\n- **Gradient-free** guidance. Our proposed ECI-sampling is completely gradient-free. We demonstrated in our experimental results that our gradient-free approach still outperformed gradient-based methods while achieving significant speed-up.\n- **Evaluation** metrics. The generative nature requires different evaluation metrics that capture distributional properties instead of simple MSE to the ground truth. In this regard, we calculate the point-wise MSE of the mean and standard deviation between the ground truth distribution and the generation distribution. We further apply Poseidon, a PDE foundation model to evaluate the Frechet distance.\n\nWe further reiterate our **contributions**, which we believe may have also been misinterpreted by the reviewer, as follows:\n\n- We proposed ECI sampling for hard-constrained generation for pre-trained **flow-based generative models**. We emphasize *flow-based generative models*, which impose unique challenges due to their iterative sampling nature. Existing approaches for *constrained prediction* in regression models are infeasible or perform badly. To the best of our knowledge, we are the first to deal with the zero-shot flow-based constrained generation problems for functional data.\n- **The extrapolation and interpolation concepts are unique to flow-based models**, which are completely different from any non-iterative approaches like project gradient. Indeed, we do not claim contributions in the correction \u2014 we even manually placed it in the Appendix as it is less important. Instead, the **iterative control framework** unique to flow-based models using the interleaved ECI steps is our major claim for a better generation result.\n- Our approach guarantees the exact satisfaction of constraints **without the expensive gradient information**.\n- Our approach achieves **state-of-the-art generative performance** on most evaluation metrics. We emphasize *generative evaluation metrics*, as the reviewer has drastically misinterpreted our metrics as regression MSEs. Instead, distributional properties are calculated and the foundation model Poseidon is used to give the most comprehensive metric.\n\nWe understand the reviewer might **not be familiar with generative modeling** in the PDE domain, which, as we have mentioned in the first part of our related work, is a newly emerged realm for functional data. However, as we have already pointed out the reviewer's misunderstanding and have distinguished our problem setup and contributions from existing work, we urge the reviewer to give a more fair and comprehensive evaluation of our work."
            }
        },
        {
            "summary": {
                "value": "This paper tackles the problem of conditional generation of physical systems with physical constraints (such as physical conservation laws). They propose \u201ca framework for adapting pretrained zero-shot, unconstrainted generative models to satisfy exacts constraints\u201d in a computationally efficient manner. Subsequently, the apply the framework to Partial Differential Equation (PDE) systems and demonstrate empirical results on a variety of 1D and 2D physical problems."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* The method does seem to operate as advertised. The generated results have 0 constraint error, compared to > 0 constraint error of other soft-constrained methods. At the very least, the claim that ECI guarantees hard-constraint satisfaction is well supported\n* The ablation study that shows the effect of the resampling intervals and the mixing iterations is quite good.\n* Reporting both generation quality as well as runtime for various experiments helps objectively evaluate the proposed methodology.\n* Overall, the presentation of the paper if quite good. The problem statement and context are very clearly outlined at the start of the paper. The advantages of the methodology are plain described, and the research problem is clearly state and motivated within existing literature. Thorough and well written description of experiments and results. Nice figures though could benefit from more detailed captions describing what we are looking at."
            },
            "weaknesses": {
                "value": "__Theoretical Concerns__\n* While the authors do a great job of citing the existing context for SciML applications and flow-matching models, they do not cite the large existing bodies of work related to constraint satisfaction/constrained optimization. In particular, many methods share many similarities with ECI in that they perform unconditional update steps, and then project back into the feasibility region. As such, the method lacks theoretical motivation and context of existing works.\n\n__Experimental Concerns__\n* Many of the experiments performed are quite simple, with either low spatial resolution, low dimensionality, or a small number of time-steps. In order to validate that the correction step is not overly destructive, it would be beneficial to perform experiments with higher resolution, and a greater number of time steps. Additionally, there is a concern that ECI performs poorly in complex settings, as it is out-performed by one model or another on all 2D experiments (in terms of MMSE), and thus it would be good to see a 3D fluid setting to evaluate how ECI performs in higher-dimension problems.\n* Additionally, fluid flow can exhibit quite complex behaviour. It is not clear why an increased variance in the solutions is a sign of a bad solution, as opposed to the system exhibiting complex dynamics (such as turbulence). Thus, the justification for SMSE as an evaluation metric is not quite clear.\n\n__Contribution Concerns__\n* First, the concept of perform Extrapolate-Correct-Interpolate steps in order to solve constraint satisfaction is not a novel idea. In fact, and is well studied in constraint satisfaction settings. For example, projected gradient descent functions very similarly by first taking a step to minimize the objective (Extrapolate) and then taking a step to project back into the feasible region (Correct).\n* Second, as mentioned in the paper, the extrapolate step assumes a pre-trained generative model (and thus is not a novel contribution), the interpolate step simply adds noise to the sample (and thus no novel contribution contribution), so the only real contribution is the correction step. However, the correction step is extremely trivial. In the case of the boundary conditions, the method simply sets the boundary values to the desired constraint. In the case of conservation, the method subtracts the deviation from the conserved amount from all cells equally. Both approaches are extremely simple, and don\u2019t really capture realistic physics. We can construct pathological examples where this type of correction leads to systems which satisfy the constraints but are completely implausible and useless for any sort of SciML application. In fact, this correction step is more or less the most trivial way to project back into the feasibility region.\n* Furthermore, to address new problems, you need to implement your own correction steps, which may be difficult, computationally intensive or simply intractable for more interesting problems. For example, there is no treatment for conservation of a vector quantity (i.e. momentum, angular momentum), which would occur in many settings of interest.\n* Additionally, the paper deals with a pretty niche problem. Notably hard-constrained generation using flow-matching models for physical system simulation. However, when digging into the ECI framework, we note that the E and I steps come from pre-existing generative models, and the C step is incredibly simple. Thus, the overall contribution is very limited in scope of application, as being quite simple from a theoretical and practical point of view.\n* Overall, given a pre-trained flow matching model, ECI is a somewhat simple extension that allows for hard constraints to be satisfied. This is guaranteed by the correction step, which is quite simplistic. In particular, we would expect that such coase correction steps would lead to poor solution quality. In fact, from the experimental results, we can see that ECI performs well in 1D settings, but under performs compared to other models in the more complex 2D problems. In fact, we would expect it to perform even worse with high spatio-temporal resolution or in 3-dimensional problems.\n\n\nOverall, the paper is very well written and presented, but the ECI framework (in particular the correction step) do not present a significant theoretical or practical contribution."
            },
            "questions": {
                "value": "* Is it possible to address the conservation laws in a more principled way? For example, minimal local perturbations to achieve the desired conservation (rather than a global correction), or monitoring which cells are causing the violation and thus correct the violations in a more targeted manner?\n* Can we apply ECI to a 3D problem/higher resolution/more complex problem to see if the correction step is much worse? It would also be good to see an experiment at a much higher resolution and with many more time-steps.\n* Why are we using Frechet Poseidon Distance? For these problems, we know the physics and so would it not be better to simply compute the ground truth for evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes an ECI sampling framework that strictly satisfies hard constraints such as IC/BCs and conservation laws through extrapolation, correction, and interpolation stages. The method has gradient-free generation and zero-shot inference for parametric PDE and dynamical systems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. There are many experimental results, sufficient comparison with baselines, and complete test metrics.\n2. The analysis presented in line 260 is basically confirmed by experiments: when the physical constraints are strong or contain a lot of information, the generation variance is small, and conversely the variance is large.\n3. Compared with Boundary enforcing Operator Network (BOON), this work implements hard constraints more concisely. The ECI sampling framework satisfy physical laws and system requirements strictly."
            },
            "weaknesses": {
                "value": "1. Although the results in Table 3 is excellent, Figures 6-8 may not clearly reflect the advantages of ECI sampling, and further explanation of the figures and settings is needed.\n2. This method can achieve zero-shot performance and exact satisfaction of different IC/BCs. However, Figures 5-7 are shown with IC or BC fixed. It is necessary to supplement the visualization of final predictions under different ICs. For the Stokes problem in Appendix B.1, we recommend testing more k and w values to demonstrate the generalization and applicability of ECI sampling.\n3. There is a lack of an overall framework diagram, and the entire training and generating process is not clear or intuitive enough. The PDE problems considered in the experiment are limited to 2-3 dimensions. And those experimental descriptions, data formats and prediction tasks should have a clearer and unified presentation."
            },
            "questions": {
                "value": "For pre-trained operator networks, fusing BC/ICs to enforce hard constraints can indeed improve predictions at initial time and boundaries, but can ECI sampling directly correct or improve the prediction values in the domain? And how effective is this ability under zero-shot inference?\nIn addition, u_1 in line 2 of Algorithm 4 should be in brackets and needs to be corrected."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce a novel framework for adapting pre-trained, unconstrained\ngenerative models to exactly satisfy constraints of PDEs in a zero-shot manner.\nSampling which alternates between extrapolation (E), correction (C), and interpolation\n(I) stages during each iterative sampling step to ensure accurate integration of constraint information of PDEs\nis proposed \nand called ECI sampling.\nThe new method is evaluated against various existing approaches and for different PDEs with both value constraints \n(initial and boundary conditions) and conservation conditions.\nThe topic is interesting, but the paper requires a complete rewriting focusing on the main contribution and towards a sound mathematical notation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- interesting and relevant research question\n\n- extensive evaluation of the new ECI sampling method versus other zero-shot guidance models for both generative and regression tasks\nand experiments on various 1d/2D and one 3D PDE;\nhowever I am not an expert in all these PDE learning methods"
            },
            "weaknesses": {
                "value": "The writing of the paper is  unsatisfactory, everything boils down to Alg. 2 together with the\nbeginning of p. 5 (Extrapolation, Correction, Interpolation), \nbut the authors \ntry to explain something in a non understandable, fuzzy way.\nIn particular, often the notation is not introduced when it is needed.\nFor example in formula (2) it is not clear how $q$ is defined.\n\n The paper could be considerable shortened or better some parts from the appendix could be moved to the main part\nto make the methods better understandable.\\\\\nProp 1 is straightforward and holds just by construction; it could go into the main part \n\n Alg. 2  is obscured by notational\noverload. Basically what is happening in Algorithm 2, line 6 is the\nfollowing:\n\n- drawing $u_1\\sim p_\\theta(u_1|u_t^{m-1})$ amounts to computing\n$$u_1=u_t^{m-1}+ (1-t)v_{t,\\theta}(u_t^{m-1})$$ In particular\n$p_\\theta(u_1|u_t^{m-1})$ is a delta measure resulting from the one step\napplication of the vector field.\n- then compute $\\hat{u}_1:=C(u_1,\\mathcal{G})$ in order to project to\na function $\\hat{u}_1$ that satisfies the constraints\n\n- sample from $q(u_t^m|\\hat{u}_1)$ by sampling from $u_0\\sim\\mu_0$ and\ncalculating $u_t^m=(1-t)u_0+t\\hat{u}_1$. This means that one assumes that\n$q(u_t^m|\\hat{u}_1)$ is distributed as $(1-t)U_0+ \\hat{u}_1$ where\n$U_0\\sim \\mu_0$.\n\nIn line 7 the only difference is that in the last step one computes\n$u_{t+1\\backslash N}=(1-(t+1\\backslash N))u_0+(t+1\\backslash N)\\hat{u}_1$."
            },
            "questions": {
                "value": "Minor remarks:\n\n- p. 3: PDE family is $\\mathcal F_\\phi$, but then $\\mathcal U_{|\\mathcal F}$ without $\\phi$ \n(skip the $x$ in $u(x)$ in definition of $\\mathcal U$ since you mean the function and not the function value);\nthe family could be $\\mathcal F:=\\{\\mathcal F_\\phi: \\phi \\in \\Phi\\}$\n\n- in the definition of $\\mathcal U_{|\\mathcal G}$ I am missing the domain of $x$ in $\\mathcal F_\\phi u(x) = 0$\n\n- what is the reason for the clumsy notation, why not calling it just $\\mathcal U_{\\mathcal G}$ and $\\mathcal U_{\\mathcal F}$\n\n- there are errors in English, e.g. and of p. 3 ''denote'' instead of ''denotes''\n\n- when the authors use $:=$ (push-forward measure) and when not (most of all other definitions)\n\n- formula (2) is not readable since $q$ is not defined here; later the authors try to explain the formula\n\n- put important formulas on an extra line\n\n- While Alg 1 is superfluous, how Alg 3 and 4 are embedded in Alg 2"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}