{
    "id": "DwiwOcK1B7",
    "title": "Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization",
    "abstract": "Neural networks are often challenging to work with due to their large size and complexity. To address this, various methods aim to reduce model size by sparsifying or decomposing weight matrices, such as magnitude pruning and low-rank or block-diagonal factorization. In this work, we present Double Sparse Factorization (DSF), where we factorize each weight matrix into two sparse matrices. Although solving this problem exactly is computationally infeasible, we propose an efficient heuristic based on alternating minimization via ADMM that achieves state-of-the-art results, enabling unprecedented sparsification of neural networks. For instance, in a one-shot pruning setting, our method can reduce the size of the LLaMA2-13B model by 50% while maintaining better performance than the dense LLaMA2-7B model. We also compare favorably with Optimal Brain Compression, the state-of-the-art layer-wise pruning approach for convolutional neural networks. Furthermore, accuracy improvements of our method persist even after further model fine-tuning.",
    "keywords": [
        "sparse factorization",
        "pruning"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We factorize each matrix in neural network into two sparse matrices and achieve state-of-the-art sparsification results.",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=DwiwOcK1B7",
    "pdf_link": "https://openreview.net/pdf?id=DwiwOcK1B7",
    "comments": [
        {
            "title": {
                "value": "Author response part 2"
            },
            "comment": {
                "value": "> Table 1: Why are you not comparing to SparseGPT, am I missing something? In my experience, SparseGPT is a very strong baseline. Also, why do you omit Wanda for 30% density? Is Wanda using a \"finalization\" step as well, i.e., are you reconstructing the remaining weights after pruning? You get that more or less for free if you pass the calibration data through anyway.\n\nADMM by Boza is better than SparseGPT, so we feel that including it in Table 1 does not bring any value.\nWe omitted 30% density with Wanda, because it has bad results. But since this raised questions, we put it back. Wanda algorithm does not use finalization, it just selects weight to prune. But Wanda with finalization is just ADMM-1 algorithm in ADMM pruning paper and is generally worse than ADMM with pruning done gradually.\n\n> Section 5.4: I find the choice of hyperparameters for the retraining/fine-tuning quite arbitrary. You use a stepped schedule for most of the pretraining, then use a stepped learning rate schedule for retraining as wellf 70 total) epochs. [1] shows that if you properly choose the initial learning rate of a linear schedule, you can recover the accuracy drop of magnitude pruning in very few iterations. I am not sure if these results would withstand scrutiny. It would be good to use best practices here, i.e., for the convolutional networks you can definitely use a linear/cosine schedule for pretraining, and then choose the initial learning rate for linear-schedule-retraining adaptively, as in [1]. This will give much more realistic results.\n\nFirst of all, in the Imagenet experiment, we already used a linear schedule for fine-tuning (we also compared it to cyclical pruning to ensure that our baseline is high-quality).\n\nBut you are completely right about CIFAR.  \nHere, we switched to the linear learning rate schedule for finetuning, which leads to ~0.5% gains in almost all cases (and DSF is still better). Thank you for pointing this out. We keep the pretraining schedule as is since it produces high-quality dense results (slightly better than reported in the original resnet paper).\n\n>Line 131: I presume it should be \"the layer-wise pruning problem\".\n>In general, you do not seem to use the glossary package and define your DSF-acronym over and over again. ...\n\nThank you for pointing all of this out. DFS is just a typo. We also removed unnecessary DSF definitions.\n\n> In Line 465 you state that your method does not support gradual pruning with fine-tuning between pruning steps, could you elaborate why? I am not sure what I am missing here.\n\nFirst of all, we changed the wording from \"does not support\" to \"it is unclear how to do.\" Here is our reasoning why it is unclear:\nImagine that your target density is 25%. In regular pruning, you can first prune to 50% density, then finetune network, and then prune again to 25% density. This works because you can easily prune the already-pruned matrix. But if you apply DSF, how should you apply DSF again with a lower target density? You could multiply the factors to get a dense matrix and factorize that, but is it the correct way?\n\n\n> In Line 196, you first \"look into the projection problem\". I am not quite sure I understand correctly how that is not the entire problem? A proper solution to that is what you are looking for, isn't it?\n\nThe original layer-wise pruning problem is min ||XW - XAB||^2. The projection problem is min ||X - AB||^2, thus a simplified version.\n\n\nThank you again for your very good suggestions. We hope our response clarifies your concerns. If that\u2019s the case, we would greatly appreciate it if you would consider raising your score."
            }
        },
        {
            "title": {
                "value": "Author's response"
            },
            "comment": {
                "value": "Thank you very much for a helpful and insightful review.\n\nHere are the responses to your concerns; we split them over multiple comments.\n\n>Section 3.1 would greatly benefit from a more detailed explanation of the ADMM method. How are Z and U initialized? I understand that it is not your job to explain ADMM in detail, but I think that nevertheless the paper would greatly benefit from more detailed remarks - at least in the appendix. Since this method is not standard (at least in the pruning literature and to my knowledge), I think it would be helpful to make this more clear.\n\n>In two sentences (Line 149, 150) you basically explain how you find the sparsity mask. Why do you precondition? How exactly is the cubic schedule (I presume Zhu & Gupta?) implemented, over how many iterations, with which interval between the increases? I am trying my best to infer this from somewhere, but it is nowhere to be found? Either I am missing something or the paper is lacking a crucial part, namely how the sparsity mask is found.\n\nWe agree that the ADMM method for pruning is underexplored and not well known.\nWe expanded section 3.1 to provide a quick overview of ADMM method used by \"Fast and Effective Weight Update for Pruned Large Language Models\" (https://openreview.net/forum?id=1hcpXd9Jir). Mainly, we talk about what ADMM does, how layer-wise pruning maps onto ADMM, and how the pruning mask is found.\nWe decided not to put more details into an appendix because we believe that if the reader needs even more information, reading the original paper by Boza would be much more beneficial than reading the appendix in this paper.\n\n> In Line 258, you state that you are using the Wanda saliency map, I think it would be good to give the mathematical formulation to that, especially how you \"scale one of the factors back\".\n\nWe added this to section 4.3.\n\n> Lines 37-39: If you replace the dense weight matrix with a product of two sparse matrices, will your model not be much slower at inference than when replacing with just a sparse matrix? For Low-rank decomposition, you at least get two linear layers which are much smaller dense matrices, but in your case, you basically have two sparse matrices. While you argue in Line 162 that the total number of multiplications is equal, this is far from realizable on the existing hardware. In practice, you incur a non-trivial overhead. I would like to hear the authors' thoughts on this.\n\nWe added section 4.4 to discuss the DSF method's computational concerns. Part of our argument regarding speed can be summarized as follows:\n\na) In many cases (e.g., local LLM inference), the main concern is fitting the best model into a small memory footprint. In this case, a slight decrease in inference speed might be tolerable. \n\nb) There are cases (again, for example, local single batch LLM inference) where the main computational bottleneck is transferring weights from memory to the local cache. This is explored in Flash-LLM (https://arxiv.org/pdf/2309.10285).  \n\nc) Doing two sparse multiplications is not much slower (when using MKL on CPU, it is not slower) than doing one sparse multiplication with the same total number of nonzeros.\n\n> Line 50: \"our method is the first layer-wise pruning method in which the larger pruned model is better than the dense smaller model\" - Are you sure this is true? I feel like already the original SparseGPT paper gets fairly close and there have been a variety of improvements since then, e.g. using non-uniform layer-wise sparsity. Maybe this claim can or should be made more precise.\n\nSparseGPT was not better in terms of perplexity (5.63 for pruned Llama2-13B, 5.12 for dense Llama2-7B), but is better on zero-shot benchmarks. Outlier weighted sparsity (https://arxiv.org/pdf/2310.05175) might be better, but they do not report such results.\n\"Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models\" (https://openreview.net/forum?id=rgtrYVC9n4) reports better results in zero-shot benchmarks but does not report perplexity.\n\nThus, we are adjusting our claim to uniform layer-wise pruning and perplexity measure. \n\n> Missing ablations: The paper is fixing a lot of hyperparameters and making claims without ablations. That includes e.g. the selection of sparsity distribution between the matrices (Line 209) or the initialization for A and B (Lines 248-250), among others. Such ablations should be added to justify the choice of parameters.\n\nWe added even more ablations to the appendix."
            }
        },
        {
            "summary": {
                "value": "The paper proposes Double Sparse Factorization (DSF) of the weight matrices to prune them effectively. They formulate it as an alternating optimization and optimize using ADMM. The experiments show a clear benefit of the proposed method on Llama for a language task and resnet on image classification."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The idea is nice, the problem formulation is neat, and using ADMM for optimization is elegant. \n2. On pruning LLAMA, the method shows clear benefit over the compared methods. Image classification experiments are marginally better than previous methods."
            },
            "weaknesses": {
                "value": "1. The SVD comparison is unfair in my opinion. SVD is more suited for low-rank compression and it may not enforce sparsity. Using the sparsity ratio as the main criterion may not be ideal. Why not use FLOPs? As FLOPS directly relates to inference speed as opposed to sparsity ratio. I would suggest that the authors include a comparison based on FLOPs in addition to the sparsity ratio. This would provide a more comprehensive evaluation of computational efficiency across different compression methods, including SVD and sparse factorization approaches.\n2. ADMM optimization may be compute-intensive. Not much discussion about it unless I missed something. Could you provide an asymptotic time complexity analysis and/or empirical running time comparison of the ADMM? You may also discuss the trade-offs between computational cost and compression quality, as it would give readers a clearer understanding of practical applicability of the proposed method."
            },
            "questions": {
                "value": "1. Could you discuss how this is related to sparse coding?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces double sparse factorization (DSF) which combines matrix decomposition with pruning to yield compressed neural networks with better generalization performance than the baselines used for comparison. The authors demonstrate that their proposed algorithm, an extension of the alternating direction method of multipliers (ADMM) algorithm, is capable of achieving competitive results when compressing pretrained LLMs and CNNs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* While pruning of factors obtained from matrix decomposition is not a novel contribution per se (Le Magoarou & Gribonval, 2016), its application to pretrained model compression is novel as far as I know. In any case, this work clearly distinguishes itself from prior art by focusing on the model compression task, particularly in the context of LLMs. \n* The paper is well written. \n* The empirical results outperform strong, SOTA baselines in a variety of contexts for LLMs and CNNS. \n* The authors take care to consider some of the practical concerns of their method, such as masking overhead. The demonstration of the generalization of DSF with a shared fixed A factor mask is particularly compelling. \n* Compression and efficient inference is of particular importance as model sizes continue to grow and scale. As such, this work addresses a timely and important topic."
            },
            "weaknesses": {
                "value": "Overall, I am leaning towards accept. However, I have some significant concerns regarding the practical applicability of the proposed method. Fundamentally, we require compressed models that offer advantages in one or more of the following dimensions: memory overhead, latency, and/or throughput. For each of these dimensions, we can consider both training and inference. For the following discussion, let\u2019s consider an intermediate fully-connected layer from a decoder block in a  LLaMa 2-7B @ 50% sparsity. This layer\u2019s weight tensor is of shape (11008, 4096). \n\n* Fine-tuning (FT) / training memory overhead: During FT, the proposed method requires ~37% more memory to store the intermediate activations of X@A@B compared with X@W. Activations can account for a significant portion of the overall memory footprint during training and this should be acknowledged in the paper. \n* Mask overheads: Assuming a bit-mask compression strategy and no shared masks between A factors, we find a similar 37% increased overhead compared to single layer sparsity. With shared A factors this overhead drops to ~1%, assuming the mask is shared across all 36 decoder blocks. From this perspective, I find the fixed-mask variant of DSF to be the most practically interesting. \n* Indices instead of bitmasks: In the introduction, the authors suggest using indices to store the locations of non-zero elements. However, given that we require uint16 indices to represent all positions in this weight tensor, this would only be practical at sparsities >= 15/16 compared to bit-masking. Given that this is currently an unobtainable level of sparsity for LLMs and roughly the limit at which we are able to find performant CNNs I find the suggestion to use indices to store non-zero locations poorly motivated. \n* Latency and Throughput: This is the most challenging dimension to estimate. Although the FLOPs analysis suggests similar performance to OBC, this may be misleading considering the additional matmul operations required in the low-rank decomposition and subsequent increase in overall memory bandwidth required to store and load intermediate activations between subsequent matmul kernel calls. I would be more convinced of the practical application for DSF if the authors include a discussion on runtime latency. This could be supported by preliminary benchmarking using Neural Magic\u2019s DeepSparse Engine which would offer some empirical evidence of improved runtime properties. \n* 2:4 support: It\u2019s unclear if the proposed method can support 2:4 sparsity as this would require a fixed sparsity level of 50% for both factors. The authors found that a smaller level of sparsity (~33%) yields the best performance but this prohibits using 50% sparsity in both factors as required for 2:4. \n* Hyperparameter sensitivity: There are a number of specific sparsity values used in the experimental method (16% sparsity, 25% sparsity, etc.). How sensitive is DSF to these values? If DSF is applied to a new model family, is it required to perform a hyperparameter search to find the optimal sparsity level for the smaller factor? How were these sparsity levels found? Could the authors add the results of their hyperparameter sweep for these values, assuming this was how the values were determined. \n* Reliance on PPL: The authors claim that their method \u201cis the first layer-wise pruning method in which the larger pruned model is better than the dense smaller model.\u201d. I believe this claim requires more evidence to support, namely, downstream evaluation for the compressed LLMs on real-world tasks. I would be more willing to support this claim with empirical results from the pruned models on OpenLLM Leaderboard v1 or similar. Relying on perplexity alone has been shown to be misleading for compressed models [1]. \n* LLM fine-tuning: The fine-tuning results section would benefit from expanding its scope to include fine-tuning of the compressed LLMs. I would also be interested to see what the memory overhead looks like for DSF when naive masked sparsity is used. \n\nBased on the above it appears that the memory overhead with a fixed A factor mask is comparable to regular pruning. However, it appears likely that the latency for DSF will be worse than models pruned with other techniques (Wanda, etc.). It is unclear whether DSF can support 2:4 sparsity whereas other methods such as Wanda do support this format (albeit with high loss). If tuning is required per model to set the smaller factor sparsity, this may result in DSF being much more expensive to use on new models. I am willing to accept this paper as the generalization results are good and motivate future work exploring this direction. However, given that DSF is fundamentally motivated by network compression, a more holistic discussion of the above points would convince me to raise my score and, in my opinion, raise the impact of this work. \n\n[1] A. Jaiswal, Z. Gan, X. Du, B. Zhang, Z. Wang, and Y. Yang, \u201cCompressing LLMs: The Truth is Rarely Pure and Never Simple,\u201d Oct. 02, 2023, arXiv: arXiv:2310.01382. doi: 10.48550/arXiv.2310.01382."
            },
            "questions": {
                "value": "* Specifically which LLaMa model is used for reporting the results in Table 1? The authors refer to both LLaMa 1 and 2 in their experimental setup. \n* Does DSF provide latency/throughput benefits over dense or typical sparse networks (single layer sparsity) when using DeepSparse Engine? \n* Can DSF be extended to 2:4 sparsity? What is the trade-off with generalization performance? \n* How do the pruned LLMs compare when evaluated on OpenLLM v1 leaderboard evaluation tasks?\n* Missing results for Wanda at 70% sparsity: Why were these not included in Table 1? \n* What is the memory overhead when fine-tuning DSF LLMs in a naive way (i.e., with masked paramters intead of compressed representations)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes Double Sparse Factorization, a method that, instead of pruning the original weight matrix, factorizes it into the product of two matrices (similar to e.g. low-rank decomposition), which together satisfy the same sparsity constraint. To solve this problem, they use the ADMM method. The paper claims to improve upon existing pruning and layer-wise pruning approaches, and they back their claims with experiments on state-of-the-art language models and medium-sized vision models. In addition, they show that the superiority of their methods seems to prevail after retraining the pruned models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea is interesting, to the best of my knowledge relatively novel, and the experiments are quite convincing. Most of the paper is fairly easy to follow and the reader is not left with many questions. I appreciate that the authors provide results before and after retraining the pruned models, as this is often not done in other papers. The proposed method is interesting, however there are open questions that I will discuss below."
            },
            "weaknesses": {
                "value": "I have several concerns regarding the soundness, clarity, and contribution of this work, which I detail below. I hope these remarks are helpful for improving the paper and am open to discussing my evaluation.\n\n### Clarity\nWhile I think that the idea proposed in this paper might be promising, I sometimes had a hard time following the paper. I think the structure as well as details could be improved.\n- Section 3.1 would greatly benefit from a more detailed explanation of the ADMM method. How are Z and U initialized? I understand that it is not your job to explain ADMM in detail, but I think that nevertheless the paper would greatly benefit from more detailed remarks - at least in the appendix. Since this method is not standard (at least in the pruning literature and to my knowledge), I think it would be helpful to make this more clear.\n- In two sentences (Line 149, 150) you basically explain how you find the sparsity mask. Why do you precondition? How exactly is the cubic schedule (I presume Zhu & Gupta?) implemented, over how many iterations, with which interval between the increases? I am trying my best to infer this from somewhere, but it is nowhere to be found? Either I am missing something or the paper is lacking a crucial part, namely how the sparsity mask is found.\n- In Line 258, you state that you are using the Wanda saliency map, I think it would be good to give the mathematical formulation to that, especially how you \"scale one of the factors back\".\n\n\n### Soundness\n- Lines 37-39: If you replace the dense weight matrix with a product of two sparse matrices, will your model not be much slower at inference than when replacing with just a sparse matrix? For Low-rank decomposition, you at least get two linear layers which are much smaller dense matrices, but in your case, you basically have two sparse matrices. While you argue in Line 162 that the total number of multiplications is equal, this is far from realizable on the existing hardware. In practice, you incur a non-trivial overhead. I would like to hear the authors' thoughts on this.\n- Line 50: \"our method is the first layer-wise pruning method in which the larger pruned model is better than the dense smaller model\" - Are you sure this is true? I feel like already the original SparseGPT paper gets fairly close and there have been a variety of improvements since then, e.g. using non-uniform layer-wise sparsity. Maybe this claim can or should be made more precise.\n\n\n### Experimental Validation\n- Missing ablations: The paper is fixing a lot of hyperparameters and making claims without ablations. That includes e.g. the selection of sparsity distribution between the matrices (Line 209) or the initialization for A and B (Lines 248-250), among others. Such ablations should be added to justify the choice of parameters.\n- Table 1: Why are you not comparing to SparseGPT, am I missing something? In my experience, SparseGPT is a very strong baseline. Also, why do you omit Wanda for 30% density? Is Wanda using a \"finalization\" step as well, i.e., are you reconstructing the remaining weights after pruning? You get that more or less for free if you pass the calibration data through anyway.\n- Section 5.4: I find the choice of hyperparameters for the retraining/fine-tuning quite arbitrary. You use a stepped schedule for most of the pretraining, then use a stepped learning rate schedule for retraining as wellf 70 total) epochs. [1] shows that if you properly choose the initial learning rate of a linear schedule, you can recover the accuracy drop of magnitude pruning in very few iterations. I am not sure if these results would withstand scrutiny. It would be good to use best practices here, i.e., for the convolutional networks you can definitely use a linear/cosine schedule for pretraining, and then choose the initial learning rate for linear-schedule-retraining adaptively, as in [1]. This will give much more realistic results.\n\n\n### Minor Remarks\n- Line 131: I presume it should be \"**the** layer-wise pruning problem\".\n- In general, you do not seem to use the glossary package and define your DSF-acronym over and over again. That is a bit contrary to the purpose of an abbreviation. Also, you sometimes use DSF, and sometimes DFS (as in Double Factorization Sparse), see e.g. Line 315 or the caption in Line 686 where this happens in the same sentence.\n\n#### References\n[1] Zimmer, M., Spiegel, C., & Pokutta, S. (2021). How I Learned to Stop Worrying and Love Retraining. _arXiv preprint arXiv:2111.00843_. https://arxiv.org/abs/2111.00843"
            },
            "questions": {
                "value": "- In Line 465 you state that your method does not support gradual pruning with fine-tuning between pruning steps, could you elaborate why? I am not sure what I am missing here.\n- In Line 196, you first \"look into the projection problem\". I am not quite sure I understand correctly how that is not the entire problem? A proper solution to that is what you are looking for, isn't it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}