{
    "id": "JtGPIZpOrz",
    "title": "Multiagent Finetuning of Language Models",
    "abstract": "Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement. However, successive steps of self-improvement can reach a point of diminishing returns, as the diversity of generations decreases, limiting further performance gains. In this work, we propose a complementary approach towards self-improvement where finetuning is applied to a multiagent society of language models. A set of language models are initialized from the same base model and then are specialized by independently updating each model using data generated by the model under multiagent interaction with other models. By training each model on independent sets of data, we illustrate how this approach enables specialization across models and diversification over the set of models. As a result, our overall system is able to autonomously improve over many more rounds of fine-tuning than single-agent self-improvement methods. We quantitatively illustrate the efficacy of the approach across a wide suite of reasoning tasks.",
    "keywords": [
        "Language Models",
        "Multi-agent Interaction",
        "Self Improvement"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We illustrate how multiagent finetuning can improve language models",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=JtGPIZpOrz",
    "pdf_link": "https://openreview.net/pdf?id=JtGPIZpOrz",
    "comments": [
        {
            "summary": {
                "value": "This paper contributes to the field of self-improvement fine-tuning for LLMs by proposing a multi-agent cooperation approach. It replicates an LLM into multiple generation agents and corresponding critic agents. A multi-agent debate architecture is utilized to generate label responses for generation agents and critic agents. Each agent is then fine-tuned using its unique generated dataset. During the inference phase, the final result is generated following the multi-agent debate. The method demonstrates superiority over the baselines in a series of math-related language reasoning tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is easy to follow and the content is well-organized.\n2. The paper proposes a method for agent self-improvement fine-tuning based on multi-agent collaboration, allowing for multiple rounds of self-improvement fine-tuning, which could be a promising approach."
            },
            "weaknesses": {
                "value": "My primary concerns with this paper are centered around the experimental section.\n\n(Major)The first concern is regarding the selection of experimental datasets. The paper exclusively uses mathematical language reasoning tasks, and each task is not particularly challenging. Arithmetic is limited to arithmetic operations, GSM corresponds only to Grade School level difficulty, and MATH selects only the first three levels. If the tasks are not challenging enough, it may lead to questioning the need for incorporating multiple LLMs when a single LLM might be adequate. What\u2019s more, conducting generalization experiments solely within mathematical datasets may not demonstrate the unique advantages in terms of generalizability, given the highly similar nature of the tasks. Including a graduate-level mathematical reasoning set or challenging datasets from other fields would make the experiments more convincing.\n\n(Major)The second concern is that the performance improvement on datasets of modest difficulty is not significant, especially considering the introduction of multiple large models collaborating. It may be worthwhile to quantify other metrics beyond answer accuracy, for example, the KL divergence of each LLM from the original LLM distribution? Introducing multiple large models might allow agents to achieve better results without significant parameter changes.\n\n*Overall, I believe the idea of the paper is commendable. However, as a practice-oriented paper lacking in theoretical explanation, its experimental design is insufficient. Without addressing these major concerns in the experiments, I'm afraid this is the highest score I can give.*\n\n\uff08Minor\uff09The diversity of models: When reading the sentence \" each model to capture parts of a task of interest.\", I am very excited. However, the authors do not take any measures during the implementation to activate the heterogeneous characteristics among the generative agents. In other words, diversity is only brought about by the quantity of agents. The diversity cannot be theoretically guaranteed, nor can it be conceptualized and defined for specific agents after it appears. If the authors could enable different agents to collaboratively complete a complex task and truly capture the sub-tasks they are interested in or excel at, it might greatly enhance the contribution of this paper."
            },
            "questions": {
                "value": "In addition to my primary concerns, there are also the following questions: \n1. In this paper, is each individual agent fine-tuned using the SFT (a combination of query and label response data) approach? If yes, can this method be integrated with DPO (a combination of query and human feedback preferences data) and PPO (a combination of query and reward signal data) approaches? Please provide a detailed discussion. \n2. How is the Combination of datasets implemented in line 25 of Algorithm 1 pseudocode? What does the hyperparameter denote? The authors need to clearly explain how the two types of data are used respectively in the training of the critic agent and what the role of the weight hyperparameter is.\n3. The abstract mentions \u201cmultiagent society of language models\u201d, yet the experiments only utilized 3-5 agents (and the improvement with 5 agents does not seem significant). How do the authors view the relationship between the number of agents and performance improvement, as well as the issue of number versus resource consumption?\n4. The selection of baselines. From the setup of the baselines, this work seems more like an \u201cA+B+C\u201d approach. However, would it be fair to compare \u201cB+C\u201d with B alone, even if the A module is ablated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method to address a key limitation of LLMs: their dependency on static datasets, which limits their ability to self-improve. The authors propose a multiagent approach for LLM finetuning that enables iterative self-improvement through interactions with other models. By using distinct Actor (generation) and Critic (evaluation) agents, their approach improves feedback quality and response diversity, which results in better responses over iterations of finetuning. The authors empirically demonstrate their approach's effectiveness across 3 reasoning benchmarks (Arithmetic, MATH, GSM), showing performance gains of approximately 1-15% across 4 baselines. Additionally, the finetuned agents may demonstrate zero-shot generalization from MATH to the other two benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Significance: This paper presents a promising approach to LLM self-improvement and could offer a valuable contribution.\n- Clarity: Most of the Figures in the paper are clear and the paper is generally well-written."
            },
            "weaknesses": {
                "value": "There are several comments I would like the authors to address to make some details clearer and the paper more complete.\n\n**Major comments**\n1. Role Specialization: The paper introduces distinct roles for models (generation agents and critic agents). However, it would be helpful to clarify the specific objectives each role optimizes. Additionally, I suggest emphasizing that only two roles are used in this paper (generation and critic) to avoid confusion. \n2. Zero-shot Generalization: In Section 4.3 The authors claim zero-shot generalization on held-out benchmarks. I find the claims made in Section 4.3 not very convincing for three reasons:\n- (a) The authors only evaluate on 100 random samples. Could the authors elaborate on why this specific sample size was chosen, and why not more?  If it is computationally possible to evaluate more samples (e.g. 1000), this would make the evidence for zero-shot transfer more convincing.\n- (b) In Figure 5, the standard errors for performance (accuracy) are absent; could you please report these like was done in the Table 2\n- (c) The choice of finetuning on MATH and evaluating on the GSM dataset is unclear; could you explain the rationale? Why not report all cross-dataset generalizations, such as MATH \u2192 GSM or Arithmetic \u2192 GSM? If the results are consistent across all cross-evaluations, this would strengthen the findings.\n3. Computational Trade-offs: The limitations section mentions increased computational demands but does not quantify these trade-offs. Details on the additional costs or time requirements would help to evaluate the compute/performance balance.\n4. Limitations Discussion: The current discussion of limitations is very short. Future researchers building on this work would benefit from more detailed insights into any bottlenecks or constraints of the approach. Based on these more detailed suggestions for future research directions would be valuable.\n5. Response diversity: Looking at the slope, Figure 3 seems to indicate that all the benefits could be coming from the initial improvement in diversity. It seems important to mention the initial diversity for each ablation. \n6. Standard error: How exactly is the standard error in Table 2 computed?\n\n**Minor comments** (that did not affect my score)\n- Abstract: The sentence\u201c A set of language models are initialized from the same base model and then are specialized by independently updating each model using data generated by the model under multiagent interaction with other models\u201d is somewhat lengthy and could be simplified for readability.\n- Citation Formatting: Throughout the paper, `\\cite{}` is used in places where `\\citep{}` might be more appropriate (e.g., lines 137, 244, 448).\n- Figure: In Figure 4, what are the multiple dots in each box for each method? I\u2019m assuming that their order is also the order of performance across iteration but it\u2019s not obvious."
            },
            "questions": {
                "value": "See my questions in the \"Major comments\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the fine-tuning of multiple LLM agents within the framework of multi-agent debate. The authors propose a method where multiple LLMs are fine-tuned and organized into generation and critic agents to facilitate multi-agent debate. Experiment results indicate that the proposed method outperforms baseline methods, and the authors further demonstrate that fine-tuning multiple LLMs helps preserve diversity compared to fine-tuning a single LLM."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tJointly optimizing the LLM in the roles of generators and critics appears to be a robust method for enhancing the reasoning ability of LLMs.\n2.\tThe work shows that finetuning multiple LLMs on independent datasets derived from multi-agent debate can preserve diversity, which is a critical challenge for LLM finetuning.\n3.\tThe evaluation results show the strength of the proposed method."
            },
            "weaknesses": {
                "value": "1.\tThe title \u201cMultiagent Finetuning of Language Models\u201d may imply a broader scope than the paper addresses. Multi-agent applications of language models can indicate a much broader range of settings besides reasoning tasks and multi-agent debate, such as gaming and social simulation; however, this work focuses solely on multi-agent debate.\n2.\tThe terms  \u201cSingle Agent\u201d and \u201cMulti Agent\u201d is vague and unclear in this paper. For example, Sec 2.2 \u201cFine-tuning Single Agent\u201ddiscusses scenarios involving multiple agents rather than a true single-agent setting.\n3.\tThis work might not obey the standard training and evaluation procedure on GSM and MATH dataset, as only 500 examples are selected for training."
            },
            "questions": {
                "value": "1.\tLine 77: How does the proposed approach \u201cpromotes diversification within the society of models\u201d? \n2.\tWhat precisely does \"Single-Agent\" in \"Fine-tuning Single Agent\" refer to? Is it intended to indicate one generation agent or a single LLM? If the latter, a more fitting term might be \u201cFine-tuning Single LLM.\u201d\n3.\tHow is a fair comparison with baseline methods established?\n4.\tHow does varying the number of agents affect the performance of the proposed method?\n5.\tIt is pretty costly to train multiple LLMs, especially considering the inference-time compute and resources required to serve N LLMs. A straightforward possible strategy to avoid training multiple LLMs while also maintaining diversity is to include an unique identifier (e.g. an ID) or a special token in the input for each agent. How does this strategy compare to finetuning multiple LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}