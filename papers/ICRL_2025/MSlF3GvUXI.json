{
    "id": "MSlF3GvUXI",
    "title": "Structured-Initialization Learning",
    "abstract": "The emergence of large language models (LLMs) has revolutionized natural language processing, but their development and deployment face significant challenges in computational resources and environmental sustainability.\nTraditional self-supervised learning (SSL) paradigms requiring extensive computational infrastructure and exhibiting slow convergence rates, leading to increased energy consumption and longer training durations.\nWhile existing model fine-tuning techniques such as Low-Rank Adaptation (LoRA) are resource-intensive and fail to facilitate swift knowledge updates when integrating a mount of new data in model version iteration.\nTo mitigate these challenges, we introduce Sail, a novel method for accelerating the training of neural network models by leveraging knowledge from (publicly available) pre-trained models.\nOur approach comprises two key components: (1) a parameter transformation technique that adjusts the dimensions of pre-trained model parameters to match the target architecture, and (2) a proximal parameter integration and retraining strategy that efficiently combines transformed parameters to initialize new models.\nWe formalize the concept of Proximal Parameter and provide theoretical guarantees for its convergence advantages.\nOur approach achieves substantial reductions in training time and computational resources while maintaining or improving model performance on downstream tasks.\nThese results indicate that Sail provides a promising direction for the more efficient and accessible development of the deep learning community.\nOur code will be made publicly available.",
    "keywords": [
        "Efficient Learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=MSlF3GvUXI",
    "pdf_link": "https://openreview.net/pdf?id=MSlF3GvUXI",
    "comments": [
        {
            "summary": {
                "value": "This paper presents SAIL, a method to accelerate training by leveraging knowledge from pre-trained models using a structured initialization approach. It introduces a parameter transformation technique to adapt pre-trained model dimensions to the target architecture, alongside a proximal parameter integration strategy. Theoretical guarantees show the benefits of using transformed pre-trained parameters for faster convergence compared to random initialization as well as the guidance on obtaining the optimal parameters in the integration strategy. Experimental results across NLP and computer vision tasks confirm that SAIL reduces training time while improving model performance, supporting the efficacy and broad applicability of structured initialization for efficient large model training."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper offers strong motivation for addressing the challenges of efficient model initialization by harnessing pre-trained models, making a compelling case for its approach.\n2. Rigorous theoretical analysis and effective visualizations are used throughout, solidly supporting the paper\u2019s claims and enhancing interpretability.\n3. Extensive experiments demonstrate that SAIL significantly outperforms random initialization, highlighting its effectiveness in reducing training time and improving model performance."
            },
            "weaknesses": {
                "value": "1. A major concern is the limited comparison with related methods. This paper\u2019s approach aligns closely with areas like model reuse/expansion and model merging, both mentioned in the related work. Model expansion combined with proximal parameter integration or parameter transformation with model merging could potentially address the problem posed here. A comparison with methods from these areas would provide a more comprehensive evaluation of SAIL\u2019s efficiency.\n2. The paper lacks discussion on the gap between its linear model theory and real-world application, which is crucial to understanding SAIL's limitations. For instance, it would be beneficial to clarify practical computation of $\\gamma^*$ and address situations where the required data isn\u2019t available\u2014a common issue with open-source models that often lack access to original training data."
            },
            "questions": {
                "value": "1. How do the authors justify the claim in lines 268-269 based on Theorem 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Structured-Initialization Learning (SAIL), a method to accelerate training for large models by reusing parameters from pre-trained models. The approach includes transforming parameters to fit the target model and integrating them to form a more optimal starting point for training, reducing the need for random initialization."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper provides a solid theoretical analysis, effectively demonstrating how the proposed Proximal Parameter initialization leads to faster convergence. The authors present well-structured convergence theorems, lending strong support to the efficacy of SAIL in reducing training time and improving efficiency.\n2. The method is tested on both NLP and computer vision tasks, demonstrating applicability across different domains and model architectures."
            },
            "weaknesses": {
                "value": "1. **Limited Novelty in Leveraging Pre-trained Models for Initialization** The proposed method of reusing parameters from pre-trained models to accelerate the training of new models is similar to existing work [1-2].\n2. The motivation and system design of Figure 1 claims to use a pre-trained model such as LLM, however, the actual experiments are conducted by training the model from scratch on small-scale datasets in a controlled setup. It would be good to actually use the pre-trained models in hugging faces to create the SAIL. \n3. The explanation of how the parameter transformation is conducted is not clear enough, the authors mentioned the random projection and learnable methods, but a detailed investigation and experiments on which method is better are not included.\n\n\n**Reference**\n\n[1] Initializing Models with Larger Ones ICLR 2024\n[2] Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization"
            },
            "questions": {
                "value": "1. Could the author provide the training curves of the NLP experiments as shown in the CV experiments, it helps to verify if the proved fast convergence is also applicable to transformer training."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a novel way to combine the weights of multiple pre-trained LLMs into a smaller model. The problem to be addressed is important, as it offers opportunity to train a smaller specialized model faster, rather than train from scratch. \n\nThe paper proposed a set of theorems support it's claims, which offers theoretical guarantees."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper supports its claims with both theoretical and empirical results. The proposed theorems are proved in the appendix, which looks correct (without in-depth review)\n\n- This paper address an important problem of LLM reusing, which lacks efficient solutions before. The proposed method seems to be better than naive averaging weights.\n\n- This paper marks different theorems and labels in color, which helps readers to quickly locate and navigate."
            },
            "weaknesses": {
                "value": "- The field of knowledge distillation should be discussed in the related works section, which is highly related.\n\n- The use of colored block seems to be too extensive, which is not common in research papers. Though not an outstanding weakness point."
            },
            "questions": {
                "value": "- Why is there a spike at $\\gamma=0.5$? \n\n- Are you assuming all other network elements are the same, e.g. activation functions, vocabulary size?\n\n- Better to add discussion about knowledge distillation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}