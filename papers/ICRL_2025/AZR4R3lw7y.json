{
    "id": "AZR4R3lw7y",
    "title": "Boosting Multiple Views for pretrained-based Continual Learning",
    "abstract": "Recent research has shown that Random Projection (RP) can effectively improve\nthe performance of pre-trained models in Continual learning (CL). The authors\nhypothesized that using RP to map features onto a higher-dimensional space can\nmake them more linearly separable. In this work, we theoretically analyze the\nrole of RP and present its benefits for improving the model\u2019s generalization ability\nin each task and facilitating CL overall. Additionally, we take this result to the\nnext level by proposing a Multi-View Random Projection scheme for a stronger\nensemble classifier. In particular, we train a set of linear experts, among which\ndiversity is encouraged based on the principle of AdaBoost, which was initially very\nchallenging to apply to CL. Moreover, we employ a task-based adaptive backbone\nwith distinct prompts dedicated to each task for better representation learning. To\nproperly select these task-specific components and mitigate potential feature shifts\ncaused by misprediction, we introduce a simple yet effective technique called the\nself-improvement process. Experimentally, our method consistently outperforms\nstate-of-the-art baselines across a wide range of datasets.",
    "keywords": [
        "continual learning",
        "ViT-pretrained continual learning"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=AZR4R3lw7y",
    "pdf_link": "https://openreview.net/pdf?id=AZR4R3lw7y",
    "comments": [
        {
            "summary": {
                "value": "This work considers the random projection (RP) strategy for pre-trained models in CL. Motivated by the benefits of RP in high-dimensional space, a multi-view strategy for an efficient classifier is further proposed, in which the principle of Adaboost is adapted to \novercome inherent obstacles and applied for the first time in CL. In addition, a self-improvement process technique, although simple, also shows significant effectiveness in selecting proper task\u0002specific prompts. The experimental results demonstrate a positive impact of the proposed method in improving model quality while only applying to linear classifiers."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(1)The proposed method BoostCL performs better than existing CL baselines, including Hide-prompt and ranpac.\n(2)This work addressed the challenge when applied AdaBoost directly to CL.\n(3)A self-improvement process, a simple but effective strategy is designed to help select prompts more accurately when inference."
            },
            "weaknesses": {
                "value": "As shown in Table 1, the proposed method takes no advantage in anti-forgetting, FFM metric."
            },
            "questions": {
                "value": "Does some ablation study be conducted on the proposed modules, Prompt selection process and Self-improvement process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents BoostCL, a method for pretrained-based continual learning that leverages multiple views and random projection (RP) to enhance performance. The authors theoretically analyze the benefits of RP in higher-dimensional spaces and demonstrate its effectiveness through experimental results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe paper provides a novel theoretical analysis of the benefits of random projection in higher-dimensional spaces for continual learning.\n2.\tThe authors conduct a comprehensive set of experiments to evaluate the performance of their proposed method, BoostCL, across various tasks and datasets.\n3.\tThe paper demonstrates the practicality of the proposed method by applying it to real-world continual learning scenarios."
            },
            "weaknesses": {
                "value": "1. The paper mentions the motivation behind using random projection, but it lacks a comprehensive discussion on why this approach is superior to other potential methods for feature transformation.\n2. While the paper proposes a novel method for continual learning, it would be helpful to see a more in-depth comparison with other recent methods in the literature. Specifically, how does BoostCL differ from and improve upon existing approaches?\n3. The technical details of the proposed method are somewhat challenging to follow. The paper could benefit from a more clear and concise explanation of the algorithm and its components. \n4. Discussing the scalability of the proposed method to larger datasets or more complex models would be relevant.\n5. The author needs to further analyze the consumption of the proposed method compared to other methods in terms of computational resources and training time.\n6. The paper could benefit from a more organized and focused presentation of the material. The introduction could be more clearly tailored to motivate the problem and the proposed solution. Additionally, the paper includes several appendices that contain detailed proofs and additional experimental results. While these appendices provide valuable information, they could be more effectively integrated into the main text to enhance the clarity and readability of the paper."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This manuscript introduces \"BoostCL\", a novel approach to improve continual learning based on pre-trained models. The manuscript provides theoretical analysis to show that feature separability and model generalization ability benefit from random projection. Then a multi-view random projection is proposed to adapt AdaBoost for CL. A self-improvement process is also proposed for appropriately selecting prompts for each task sample. Empirical validation on multiple CL benchmarks is then conducted."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The manuscript provides theoretical analysis of Random Projection, demonstrating how RP improves feature separability in high-dimensional space. \n2. The manuscript is clear and well organized. The formulas are presented in an unambiguous manner."
            },
            "weaknesses": {
                "value": "A substantive assessment of the weaknesses of the paper. Focus on constructive and actionable insights on how the work could improve towards its stated goals. Be specific, avoid generic remarks. For example, if you believe the contribution lacks novelty, provide references and an explanation as evidence; if you believe experiments are insufficient, explain why and exactly what is missing, etc.\n\n1. Although RP and multi-view strategies are theoretically shown to improve feature separability, there is limited discussion on the theoretical upper bound of generalization error.\n2. The manuscript does not provide theoretical guidance on choosing the optimal projection dimension in RP, which can greatly impact model performance for different tasks.\n3. The proposed self-improvement process for prompt selection could increase the model\u2019s inference complexity, but this potential trade-off is not fully analyzed, especially in scenarios with many tasks.\n4. Minor spelling errors, like \u201csammple\u201d in line 250.\n5. More experiment regarding hyper-parameters like number of views, projection dimensions, and threshold settings for the voting mechanism should be conducted."
            },
            "questions": {
                "value": "1. How to effectively choose the best projection dimension to make appropriate trade-off between performance and computational overhead.\n2. How much gpu resource does the proposed method need, will the gpu memory usage increase rapidly by increasing projection dimension."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides some theoretical analysis on Random Projection (RP) which project features into a higher-dimensional space to improve their linear separability. Furthermore, this paper proposes a Multi-View Random Projection scheme to create a robust ensemble classifier motivated by AdaBoost. Then the proposed method is applied to prompt-based CL methods for a better performance. Experimental results demonstrate its efficacy in various continual learning settings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThis paper provides theoretical insights into Random Projection (RP), reinforcing its foundation in an intuitive manner.\n\n2.\tThe motivation for the proposed Multi-View Random Projection is clear and well-justified, demonstrating its potential to enhance performance.\n\n3.\tThe paper is well-organized and easy to understand, addressing most of my queries within the main text or the appendix."
            },
            "weaknesses": {
                "value": "**Major**\n\n1.\tThe necessity of the huge views should be clarified. As mentioned in the paper, the subsequent atomic views aim to capture the complement knowledge of previous views. However, the final classifier only leverages a part of these views rather than all of them, which may omit some complement knowledge.\n\n2.\tThe theoretical results are interesting, but the method is heuristic. For example, the design of the voting strategy and the self-improvement are quite complex. It seems that a lot of tricks are introduced directly to improve performance.\n\n3.\tThe design of the self-improvement process is not intuitive, which may accumulate errors in multiple-step prediction. Specifically, if an incorrect prompt is chosen in the first step, the model may become biased towards predicting the class associated with that wrong prompt, potentially leading to accumulated errors and hindering improvements in prompt prediction accuracy.\n\n4.\tThe experiments do not include comparisons with simpler ensemble methods, such as randomly sampling views or using all views for the ensemble. Such comparisons can provide more intuitive insights to demonstrate the effectiveness of the proposed method.\n\n**Minor**\n\n1.\tThere are too many notations which may limit the fluency and readability. For example, in line 177, there is no description for the notation $\\gamma (s,g,\\mathbb{R}^d)$ and no explanation for the symbolic $g$. So please simplify the notations as much as possible.\n2.\tTypo in Corollary 4.4 (line 250), \u201csammple\u201d should be \u201csample\u201d."
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}