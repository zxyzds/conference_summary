{
    "id": "yBlVlS2Fd9",
    "title": "WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling",
    "abstract": "Language models have been effectively applied to modeling natural signals, such as images, video, speech, and audio. A crucial component of these models is the codec tokenizer, which compresses high-dimensional natural signals into lower-dimensional discrete tokens. In this paper, we introduce WavTokenizer, which offers several advantages over previous SOTA acoustic codec models in the audio domain: 1) extreme compression. By compressing the layers of quantizers and the temporal dimension of the discrete codec, one-second audio of 24kHz sampling rate requires only a single quantizer with 40 or 75 tokens. 2) improved subjective quality. Despite the reduced number of tokens, WavTokenizer achieves state-of-the-art reconstruction quality with outstanding UTMOS scores and inherently contains richer semantic information. Specifically, we achieve these results by designing a broader VQ space, extended contextual windows, and improved attention networks, as well as introducing a powerful multi-scale discriminator and an inverse Fourier transform structure. We conducted extensive reconstruction experiments in the domains of speech, audio, and music. WavTokenizer exhibited strong performance across various objective and subjective metrics compared to state-of-the-art models. We also tested semantic information, VQ utilization, and adaptability to generative models. Comprehensive ablation studies confirm the necessity of each module in WavTokenizer. The demo is available at https://wavtokenizer.github.io/.",
    "keywords": [
        "speech representation",
        "discrete codec",
        "audio language model"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=yBlVlS2Fd9",
    "pdf_link": "https://openreview.net/pdf?id=yBlVlS2Fd9",
    "comments": [
        {
            "comment": {
                "value": "Thank you for your response. Indeed, it is not entirely fair to directly compare WavTokenizer with semantic tokens, and I misunderstood this point earlier. WavTokenizer has made contributions in the field of acoustic codec models, achieving impressive reconstruction performance even at very high compression rates, and showing promising results when applied to the LLMs. I also look forward to the author's further experimental results on TTS.\n\nAfter revisiting the WavTokenizer carefully. I recognize that, from the perspective of acoustic codec models, the low-bit-rate single-layer quantizer maybe represent a new paradigm. The large codebook space also provides some insights for the VQ field. Additionally, attention vocos and the longer context window which are demonstrated through ablation studies, are also effective designs. **As a result, I believe WavTokenizer will have the impact both in academia and industry.**\n\nHaving carefully read the paper, **I fully understand and respect all reviewers' comments**. The final decision rests with the ACs and PCs. Thank you."
            }
        },
        {
            "title": {
                "value": "Discussion with Public Comment"
            },
            "comment": {
                "value": "Firstly, we would like to express our sincere gratitude to the reviewer for giving us such a high score, and we greatly appreciate the attention WavTokenizer has received from public comment. This indicates that WavTokenizer has attracted a lot of attention.\n\nRegarding the two points you raised from the public comment, our responses are as follows:\n\n1. **[Codecs with 25Hz are common, so WavTokenizer is not important]**\n\nWe believe there may be some misunderstanding between semantic tokens and acoustic tokens. In the reconstruction paradigm of acoustic tokens, the typical sampling rate is usually 75Hz or higher, as seen in models like Encodec, HiFiCodec, SpeechTokenizer, DAC, etc. These models also require multi-layer designs and consequently a large number of tokens\u2014typically 300, 600, or even 900 tokens per second for modeling with large language models (LLMs). In contrast, WavTokenizer achieves high-quality audio reconstruction using only 40 or 75 acoustic tokens, which represents a significant contribution to the field of **acoustic codecs**. \n\nYou mentioned that many semantic tokens can operate with just 25 tokens, and that is correct since they primarily retain semantic information. However, this belongs to a different domain. **WavTokenizer focuses on the acoustic codec space, and we have demonstrated its substantial breakthroughs in handling acoustic tokens.** The key difference between acoustic tokens and semantic tokens is that the former preserves acoustic information, such as style and timbre, and can unify both music and audio. Furthermore, acoustic tokens enable LLMs to generate directly via softmax, rather than through semantic cascaded generation. This also raises a critical debate on whether LLMs should directly replace TTS in the generation process.\n\n2. **[TTS performance]** \n\nWe agree with your point that evaluating WavTokenizer on generative tasks is crucial. In Table 4, we show that the audio quality generated by WavTokenizer in TTS tasks significantly surpasses that of a nine-layer DAC, which demonstrates two key points: 1) Acoustic models with a single-layer quantizer show significant potential in downstream autoregressive generative models, and 2) speech synthesis models using large-codebook acoustic representations can synthesize high-quality audio. \n\nYou mentioned that we should further include WER and SIM metrics, and we agree that these could be useful additions. We will incorporate these results in the rebuttal. However, we believe that the absence of these metrics should not be used as a basis to question the professionalism of a reviewer who awarded a score of 10.\n\nIt is worth noting that although we have not specifically evaluated WER and SPK, several other teams have already used WavTokenizer representations to implement reliable generative models. We also encourage you to follow their findings for further insights.\n\nOnce again, we sincerely appreciate your attention."
            }
        },
        {
            "comment": {
                "value": "I am astonished by the 10-point rating. This paper appears to be a fine-tuned version of DAC. The range of 40 to 75 tokens is not a bottleneck for language model modeling, and multi-layer codecs within 25Hz are already quite common. The paper claims to benefit LLM modeling but lacks basic generation experiments. The TTS experiments do not include SIM and WER metrics. However, it demonstrates exceptionally excellent performance in unified audio compression, which still makes it a good paper. I believe a 6 to 8-point rating is reasonable."
            }
        },
        {
            "summary": {
                "value": "This paper introduces WavTokenizer, a codec tokenizer for audio that achieves extreme compression and superior reconstruction quality compared to previous state-of-the-art models. WavTokenizer requires only a single quantizer with 40-75 tokens per second for 24kHz audio, while still preserving high subjective quality and rich semantic content. Extensive experiments demonstrate WavTokenizer's effectiveness across speech, audio, and music, with competitive results in both objective and subjective metrics, and ablation studies confirm the impact of each component."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Achieves state-of-the-art performance using only a single codebook with 40 or 75 tokens, demonstrating remarkable efficiency.\n- Covers multiple domains, including audio, speech, and music.\n- Its single codebook capability supports efficient training of large language models (LLMs).\n- Provides a comprehensive analysis of different settings, along with detailed ablation studies that validate the impact of each component.\n- The paper is well-written and easy to follow, with a comprehensive analysis included."
            },
            "weaknesses": {
                "value": "Here\u2019s a refined version of the weaknesses:\n\n- The evaluation could be more thorough by incorporating existing benchmarks such as Codec-Superb and DASB to enable a more comprehensive comparison of the proposed method against existing models under standardized settings.\n- The model currently supports only a 24kHz sampling rate; I wonder if you anticipate any challenges in adapting WavTokenizer to different sampling rates. It would be valuable to study its performance across different sampling rates, such as lower (16kHz) and higher (44kHz or 48kHz) rates."
            },
            "questions": {
                "value": "refer to weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 10
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces WavTokenizer, a single-vq codec aimed at simplifying and improving the current audio language modeling approaches by replacing codec with multiple-vq. It claims to achieve competitive reconstruction quality while enhancing integration with language models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The overall approach is sound.  The paper is straightforward and easy to understand, with a clear structure that makes it accessible to readers. Additionally, the introduction provides a good overview of the context and background, helping readers to understand the problem."
            },
            "weaknesses": {
                "value": "The paper introduces WavTokenizer, a single-VQ codec designed to replace the current codec + LLM audio generation systems. However, there is no experimental evidence to support that WavTokenizer can effectively achieve this. Existing codec + LLM systems can generally be categorized into RVQ-style generation (e.g., VALL-E, MusicGen, Moshi) or semantic-to-acoustic approaches (e.g., SpearTTS, SeedTTS, MusicLM), all of which demonstrate strong performance.\n\nFirstly, there are no experiments showing that WavTokenizer combined with an LLM outperforms any of the existing systems in practical applications such as TTS or music generation. Secondly, when considering the codec itself, WavTokenizer's reconstruction quality is significantly worse than RVQ-based codecs, and its semantic performance falls short compared to semantic token like HuBERT or WavLM.\n\nGiven these shortcomings, it is difficult to see any tangible improvements or significant contributions that WavTokenizer offers to the current field of audio generation."
            },
            "questions": {
                "value": "1, Missing baselines: SemantiCodec[1], Single-Codec[2], Mimi (Moshi) [3].\n\n2, Weak Semantic Performance: Compared to the ARCH benchmark (https://huggingface.co/spaces/ALM/ARCH), the semantic performance of WavTokenizer is far worse than models like HuBERT base. Therefore, the claim of \"rich semantic information\" does not hold up well. Moreover, it is unclear why WavTokenizer's semantic capabilities were compared only against acoustic codec models, rather than semantic codecs. Why not compare against semantic codecs like SpeechTokenizer[4], SemantiCodec[1], or Mini[3]?\n\n3, Single-VQ Assumption: The entire premise of the paper is based on the assumption that using a single-VQ codec is better for audio language modeling compared to RVQ. But is this correct? The paper mentions that RVQ-based codecs like DAC require 900 tokens per second, which is true, but it fails to acknowledge that RVQ codecs typically have a temporal resolution of only 50Hz, and the most advanced models like Mini have even lower resolution at 12.5Hz. In practice, language models typically need to model at frequencies of 50Hz or less. So, what is the real advantage of a single-VQ codec with 40 or 75 tokens per second? Moshi and Mini have already demonstrated success in low-latency speech dialogue applications, but what about single-VQ? Is reconstructing all acoustic details truly beneficial for language modeling?  Does an LLM truly need to model timbre and detailed acoustic features, or is focusing solely on semantic content sufficient?  Therefore, you must at least demonstrate the effectiveness of single-VQ in practical applications, as stated in the weakness section, rather than constructing the paper solely based on this assumption.\n\n[1]: Liu H, Xu X, Yuan Y, et al. SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound[J]. arXiv preprint arXiv:2405.00233, 2024.\n\n[2]: Li H, Xue L, Guo H, et al. Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation[J]. arXiv preprint arXiv:2406.07422, 2024.\n\n[3]:D\u00e9fossez A, Mazar\u00e9 L, Orsini M, et al. Moshi: a speech-text foundation model for real-time dialogue[J].\n\n[4]:Zhang X, Zhang D, Li S, et al. Speechtokenizer: Unified speech tokenizer for speech large language models[J]. arXiv preprint arXiv:2308.16692, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces WavTokenizer, while preserving the classical acoustic codec model paradigm, achieves high-quality audio reconstruction using only 40 or 75 tokens per second. By proposing a larger codebook space, integrating attention mechanisms, and extending the context window, WavTokenizer demonstrates impressive results in audio reconstruction, semantic understanding, and downstream TTS tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- The acoustic codec representation model is a crucial technology in the current speech domain. WavTokenizer addresses one of the core challenges in the field by achieving high-quality audio reconstruction with only 40 or 75 tokens.\n- WavTokenizer introduces a novel single-layer quantizer concept, demonstrating its potential in TTS tasks and offering a promising single-layer solution for codec-LLM architectures.\n- From a methodological standpoint, WavTokenizer revisits vector quantization (VQ) in the speech domain and proposes a larger codebook space, a more powerful decoder (with attention mechanisms), and an extended context modeling window. These innovations appear to be effective.\n- The model achieves strong experimental results across reconstruction tasks, semantic understanding tasks, and downstream TTS tasks.\n- The open-sourcing of the complete training and inference code, along with model weights, will contribute to the development of the research community."
            },
            "weaknesses": {
                "value": "Overall, this work does not present significant weaknesses. However, the design of a highly powerful decoder in WavTokenizer raises some concerns. Specifically, I am concerned that the increased model parameters and the introduction of attention mechanisms may potentially slow down the codec's reconstruction speed?"
            },
            "questions": {
                "value": "- How was VQ utilization calculated in Figure 2(b) of the paper?\n- It seems that WavTokenizer is not inherently streamable. Can WavTokenizer be extended to support streaming encoding and decoding?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces WavTokenizer, a GAN-based neural audio codec that uses a single vector quantization (VQ) codebook, in contrast to previous methods that rely on multiple residual vector quantization (RVQ) codebooks. This results in a bitrate as low as 480 bits per second, and to recover quality, the authors propose replacing the time-domain decoder with a Fourier-based decoder, preceded by attention layers. The ablation study confirms these design choices, and a comprehensive evaluation, including both subjective and objective metrics, shows that WavTokenizer maintains competitive reconstruction quality with state-of-the-art models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper effectively motivates and addresses an important problem in neural audio codecs - quantizing audio into a single sequence of tokens rather than multiple sequences, which complicates modeling for downstream tasks.\n- It presents useful findings regarding decoder design, which are supported by the ablation study. In particular, a Fourier-based decoder combined with an attention layer yields better results, while a time-domain decoder with attention performs surprisingly worse."
            },
            "weaknesses": {
                "value": "While the motivation for scaling the single VQ codebook to many entries is clear, the paper falls short of achieving high codebook utilization when expanding beyond a size of 4096. What the authors list as contributions in VQ, such as k-means initialization and random restarts, are in fact well-established techniques in neural audio compression, and this paper doesn\u2019t offer any novel methods to improve codebook usage. This is somewhat disappointing, given that a key focus of the paper is to provide a single quantizer. More experimentation to scale the VQ codebook is needed, as the current contribution feels more incremental and may be better suited for a different venue."
            },
            "questions": {
                "value": "1. Did the authors try other techniques to improve VQ codebook utilization? The current approach closely mirrors EnCodec, but DAC demonstrates that low-dimensional code lookups and L2-normalization can significantly improve RVQ scalability. A useful reference to consider is [1], which shows effective scaling strategies in image reconstruction that could be also valuable for audio codecs. I recommend continuing work on the paper, as a single-quantizer audio codec has potential, but the paper\u2019s current scientific contribution is limited.\n\n\n2. What is the motivation for evaluating semantic representation? Neural audio codecs are expected to encode low-level acoustic features rather than abstract semantic concepts. Comparing audio codecs on semantic representation could be misleading, especially given the results e.g. all codecs score below 10% on the SLURP dataset, while self-supervised models like HuBERT achieve nearly 50% (not shown in this paper). This gap calls into question the statement on line 445 that \"WavTokenizer effectively captures rich semantic information\" which may be an overclaim. That said, audio codecs might have a significant impact on representation learning, as shown by EnCodecMAE [2], so it may be more appropriate to treat semantic representation as a downstream task. WavTokenizer's single codebook could be particularly useful for discrete targets in BERT-like setups.\n\n[1] Zhu, Lei, et al. \"Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%.\"\n\n[2] Pepino, Leonardo, Pablo Riera, and Luciana Ferrer. \"EnCodecMAE: Leveraging neural codecs for universal audio representation learning.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}