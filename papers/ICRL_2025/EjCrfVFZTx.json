{
    "id": "EjCrfVFZTx",
    "title": "Investigating the Effectiveness of HyperTuning via Gisting",
    "abstract": "Gisting (Mu et al., 2023) is a simple method for training models to compress information into fewer token representations using a modified attention mask, and can serve as an economical approach to training Transformer-based hypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks built on Llama-2 models that generates task-specific soft prefixes based on few-shot inputs. In experiments across P3, Super-NaturalInstructions and Symbol Tuning datasets, we show that HyperLlama models can effectively compress information from few-shot examples into soft prefixes. However, they still underperform multi-task fine-tuned language models with full attention over few-shot in-context examples. We also show that HyperLlama-generated soft prefixes can serve as better initializations for further prefix tuning. Overall, Gisting-based hypernetworks are economical and easy to implement, but have mixed empirical performance.",
    "keywords": [
        "hypernetworks",
        "llm",
        "parameter-efficient fine-tuning",
        "prefix tuning"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "We use Gisting to train Llama-2 to efficient generate parameters for model adaptation",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=EjCrfVFZTx",
    "pdf_link": "https://openreview.net/pdf?id=EjCrfVFZTx",
    "comments": [
        {
            "summary": {
                "value": "This paper introduce a set of Gisting based hyper network called HyperLlama for generating soft prefix tokens for downstream tasks. The prefix tokens acts similar to few shot example in in-context learning. The experiments show that their HyperLlama is effective in generating soft prefix tokens, but they underperformed compared to multi-task fine-tuned models with attention to in-context examples."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper is easy to read. \n2. The paper has a very detailed discussion on each experiment."
            },
            "weaknesses": {
                "value": "It seems that the paper has no technical contribution to the community. The introduced HyperLlama follows the setting of HyperTuning. \nThe paper introduced a set of new models (HyperLlama) for sure, but I do not think it is a valid technical contribution. Further, the introduced models generally underperform few-shot language models by large margins."
            },
            "questions": {
                "value": "In second paragraph of 3.2, the authors says they use QLoRA for hyper network training, but in the last paragraph of 3.3, the authors states they fine-tuned all model parameters and no LoRA is used. I am a bit confused."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "- This study introduces HyperLlama, a Gisting-based hypernetwork designed to generate soft prefixes for a frozen downstream Llama-2 model.\n- Through experiments on P3, S-NI, and Symbol Tuning datasets, they demonstrate that HyperLlama can compress few-shot example information into soft prefixes,\n- HyperLlama-generated soft prefixes also serve as strong initializations for further prefix tuning, supporting efficient fine-tuning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is written in a way that makes the field and methodology easy to understand."
            },
            "weaknesses": {
                "value": "The Discussion section describes the strengths and weaknesses of HyperLlama well, but these aspects are not fully addressed in the paper. For example, while it is mentioned that HyperLlama saves resources during inference, the authors should provide numerical evidence to support this.\n\nI believe that the Gist (Mu et al., 2023) and the HyperTuning (phang et al. 2023) have both made substantial contributions to this field, and I have some concerns that this paper primarily builds on these approaches by combining them in its experiments."
            },
            "questions": {
                "value": "(1) What are the advantages of this paper's approach compared to existing Gist methods? Additionally, I would like to understand the benefits of separating the model components.\n(2) As highlighted in the title, it would be helpful if the paper demonstrated the effectiveness of the method with numerical evidence compared to existing methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce HyperLlama, a Gisting-based hypernetwork designed to generate soft prefixes for a frozen downstream Llama-2 model. Their experiments across diverse datasets demonstrate that HyperLlama effectively compresses information from few-shot examples into soft prefixes, outperforming baselines that lack access to additional examples. Additionally, they show that HyperLlama-generated soft prefixes provide superior initializations for further prefix tuning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The introduction of HyperLlama leverages Gisting to generate soft prefixes, compressing task-specific information efficiently.\n\n2. Comprehensive experiments across multiple datasets demonstrate HyperLlama's strengths, especially in initializing soft prefixes for prefix tuning, showing improved performance over random initializations."
            },
            "weaknesses": {
                "value": "1. The paper lacks clarity in several areas, including the introduction of motivation, the description of the methodology, and the presentation of experimental results and figures.\n\n2. The experiments are limited to the Llama-2-7B model, and there are no evaluations on newer models (such as Llama-3) or larger-scale models (like 13B), which limits the generalizability of the findings.\n\n3. HyperLlama struggles with tasks that rely on precise output formats or highly contextual few-shot examples, affecting its generalizability.\n\n4. The method\u2019s effectiveness is contingent on Gisting\u2019s ability to compress information accurately, which may vary across different types of tasks."
            },
            "questions": {
                "value": "Just like the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduced a Gisting-based hypernetworks. This hypernetwork is built on Llama-2 and is able to generate task-specific soft prefixes based on few-shot inputs. To do so, the authors use two LLMs. One acts as the hypernetwork and the other one acts as the downstream network. To enable training, the hypernetwork is trained with QLoRA (plus additional embeddings) to produce Gist tokens to append to the downstream network."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1, The figures are good and especially helpful for understanding the main idea of this paper.\n\n2, This paper has detailed training details."
            },
            "weaknesses": {
                "value": "1, Table 1 has not been cited in the paper. What is its role in this paper?\n\n2, At training, the hypernetwork and the downstream network are quantized to 4-bit. Are they full-precision when used in the test stage? If it is, how to solve the gap between the quantized model and the full-precision model?\n\n3, To generate Gist tokens, two forwards with LLM are needed, which may incur high costs.\n\n4, Even though I am not an expert in the area, I still recommend the author reorganize this paper for clear expression."
            },
            "questions": {
                "value": "Please answer the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes a novel HyperLlama, which uses a hypernetwork to generate gist tokens to capture information from few-shot examples. The author provides sufficient experimental evidence that HyperLlama performs well on P3 and S-IN datasets, but performs not that good on Symbolic-tuning task. However, this work needs to strengthen its analysis of HyperLlama's performance deficiencies."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper proposes a novel method to generate gist tokens using hypernetwork, which can compress few shot examples of different tasks into gist tokens.\n2. The author's motivation for designing this hypernetwork is very clear, and sufficient experiments are provided to prove the good performance of gist token on P3 and S-IN datasets, and its insufficient performance on Symbolic-tuning task. \n3. The author has demonstrated through sufficient experiments that using the soft token generated by the hypernetwork as the initial token of prefix finetuning can improve the efficiency of prefix finetuning."
            },
            "weaknesses": {
                "value": "1. As noted in Section 3.3, \"Compression Hyperpretraining\" is essential for the downstream model (specifically the Llama 2 model in this study) to support gist tokens. However, it remains uncertain whether this additional pretraining step (\"Compression Hyperpretraining\"), following the standard pretraining, might adversely affect the model's performance on other tasks.\n2. As discussed in Section 5, HyperLlama does not exhibit superior performance on the Symbolic-turing task. The paper lacks an in-depth analysis of the factors contributing to this shortcoming. Specifically, it is not clear whether the hypernetwork is unable to extract all relevant information from the few-shot examples, or if the gist tokens are insufficient in storing the information from the few-shot examples.\n3. The study lacks an ablation analysis regarding the number of few-shot examples and the number of gist tokens. In other words, it is not explored whether increasing the number of few-shot examples, while keeping the number of gist tokens constant, would lead to an improvement in performance."
            },
            "questions": {
                "value": "Please refer to the Weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}