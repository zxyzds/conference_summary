{
    "id": "5BRFddsAai",
    "title": "HASARD: A Benchmark for Harnessing Safe Reinforcement Learning with Doom",
    "abstract": "The advancement of safe reinforcement learning (RL) faces numerous obstacles, including the lack of simulation environments, demanding computational requirements, and a lack of widely accepted benchmarks. To address these challenges, we introduce **HASARD** (A Benchmark for **HA**rnessing **SA**fe **R**einforcement Learning with **D**oom), tailored for egocentric pixel-based safe RL. HASARD features a suite of diverse and stochastic 3D environments. Unlike prior vision-based 3D task suites with simple navigation objectives, the environments require spatial comprehension, short-term planning, and active prediction to obtain high rewards while ensuring safety. The benchmark offers three difficulty levels to challenge advanced future methods while providing an easier training loop for more streamlined analysis. Accounting for the variety of potential safety protocols, HASARD supports both soft and hard safety constraints. An empirical evaluation of baseline methods highlights their limitations and demonstrates the benchmark's utility, emphasizing unique algorithmic challenges. The difficulty levels offer a built-in curriculum, enabling more efficient learning of safe policies at higher levels. HASARD utilizes heatmaps to visually trace and analyze agent navigation within the environment, offering an interpretive view of strategy development. Our work is the first benchmark to exclusively target vision-based embodied safe RL, offering a cost-effective and insightful way to explore the potential and boundaries of current and future safe RL methods. The environments, code, and baseline implementations will be open-sourced.",
    "keywords": [
        "benchmark",
        "game",
        "doom",
        "vizdoom",
        "3D",
        "safe RL",
        "reinforcement learning",
        "constraint",
        "difficulty level",
        "PPO",
        "Lagrange",
        "sample-factory",
        "vision",
        "AI safety"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "A Safe RL benchmark for vision-based learning in complex navigable 3D environments.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5BRFddsAai",
    "pdf_link": "https://openreview.net/pdf?id=5BRFddsAai",
    "comments": [
        {
            "comment": {
                "value": "**W3 & Q2. ViZDoom Lacks Realism**\n\nWhile we acknowledge that engines like Isaac Gym offer advanced physics simulation, our aim is to expand the scope not only beyond conventional 2D environments (listed as a strength by the reviewer), but also continuous control robotic manipulation tasks. Rather than focusing on realistic physics or lifelike environments, our emphasis is on fostering decision-making in visually intricate settings. Notably, many recent studies in safe RL continue to utilize simplified, unrealistic environments for experimental evaluation [2, 3], indicating their ongoing relevance. ViZDoom, in particular, remains widely adopted in recent research [4, 5, 6, 7]. Appendix F of the paper provides further rationale for our choice of ViZDoom.\n\nFigure 18b in Appendix G depicts the learning curves for Level 3 tasks. Training has not fully converged across multiple methods even after 5e8 environment iterations, underscoring the complexity of these environments and the high iteration count needed to approach peak performance. Despite the complexity of the high-dimensional pixel-based inputs, the framework was able to simulate 500M frames and perform 150K policy updates in a training time of ~3 hours on a single GPU. This complexity doesn\u2019t stem from advanced physics or intricate motor control but instead from demands like precise navigation, spatial awareness, depth perception, and predicting the movement of other entities. Combining these challenges with accurate physics and continuous motor control would make the problem unfeasible to solve within a reasonable timeframe on standard hardware. Built on top of ViZDoom, HASARD therefore offers a balanced approach, blending fast simulation with complex tasks that remain meaningfully challenging.\n\n\n**W4. Inaccessible Video**\n\nWe tried accessing the video hosted on [emalm](https://emalm.com/?v=dGLYX) from multiple devices and did not encounter any issues. To ensure the reviewer can view the demo, we have also re-uploaded the video to the [jumpshare](https://jumpshare.com/v/abSD9rLLXQDdkIXkQ3c7) platform. Please let us know if further adjustments are needed.\n\nWe thank the reviewer for their feedback! Please let us know if there is anything else we ought to address.\n\n[1] Wachi, Akifumi, Xun Shen, and Yanan Sui. \"A Survey of Constraint Formulations in Safe Reinforcement Learning.\" arXiv preprint arXiv:2402.02025 (2024).\n\n[2] Wachi, Akifumi, et al. \"Safe exploration in reinforcement learning: A generalized formulation and algorithms.\" Advances in Neural Information Processing Systems 36 (2024).\n\n[3] Den Hengst, Floris, et al. \"Planning for potential: efficient safe reinforcement learning.\" Machine Learning 111.6 (2022): 2255-2274.\n\n[4] Park, Junseok, et al. \"Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 1. 2024.\n\n[5] Kim, Seung Wook, et al. \"Neuralfield-ldm: Scene generation with hierarchical latent diffusion models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n\n[6] Zhai, Yunpeng, et al. \"Stabilizing Visual Reinforcement Learning via Asymmetric Interactive Cooperation.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[7] Valevski, Dani, et al. \"Diffusion models are real-time game engines.\" arXiv preprint arXiv:2408.14837 (2024)."
            }
        },
        {
            "comment": {
                "value": "**W1. Hard Constraints**\n\nThis concern is well-founded. We wish to clarify that our introduction of hard constraints is not presented as a novel contribution, nor is it central to our paper, as similar setups can indeed be implemented in other libraries. However, it is worth noting that simply decreasing the cost threshold to 0 is not our only modification. Additional task-specific adjustments have been made; for example, harsher penalties apply upon incurring costs, often terminating the episode abruptly. In *Armament Burden*, for instance, the agent loses all acquired weapons if it exceeds carrying capacity.\n\nOur primary focus here lies in the evaluation protocol introduced by HASARD, which emphasizes the integration and assessment of safety constraints in RL training. HASARD supports evaluations across diverse safety formulations [1], enabling fairer comparisons by aligning the safety problems that different algorithms are designed to address. If the reviewer yet nevertheless feels that the emphasis on hard constraints seems overstated or the claims appear unsubstantiated, we are open to adjust the writing to reflect this.\n\n\n**W2 & Q1. Different Safety Budgets**\n\nUnderstanding the trade-off between safety and reward is indeed crucial for evaluating how algorithms adapt to varying safety requirements. We addressed this in Appendix B.2 (referenced in Section 5.1), where we examined the effects of different cost budgets by testing PPOLag and PPOSaut\u00e9 on Level 1 of HASARD, with both higher and lower safety thresholds than the original setting. The results in Figure 14 nicely show that stricter safety bounds lead to lower rewards, with *PPOSaut\u00e9* exhibiting greater sensitivity than *PPOLag*. We are open to incorporating additional safety thresholds and other baselines if the reviewer believes this would strengthen our work.\n\nIf the reviewer feels further details on these results would strengthen the main text, we are open to incorporating the key findings on these trade-offs directly into the main paper. This could potentially replace the hard constraints results section, which may have been overstated (as discussed above). We welcome and would appreciate the reviewer\u2019s feedback on this matter."
            }
        },
        {
            "summary": {
                "value": "The paper presents HASARD, a benchmark tailored to vision-based safe reinforcement learning (RL) using egocentric, pixel-based inputs. Built on the ViZDoom platform, HASARD comprises six unique 3D environments across three difficulty levels, each designed to test safe RL in increasingly complex and dynamic scenarios. The benchmark allows for a range of agent objectives, from navigation to item collection and hazard avoidance, focusing explicitly on embodied safe RL with vision-based inputs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Diverse Scenarios: HASARD provides varied 3D environments with different objectives and challenges, such as item collection, navigating hazardous terrain, and avoiding neutral units. This variety enriches the learning and testing possibilities, ensuring that the benchmark assesses both task performance and safety considerations.\n\nStructured Curriculum: By offering three difficulty levels, HASARD presents a built-in curriculum for training RL agents, allowing gradual learning in increasingly challenging conditions. This approach is effective for developing robust agents that can generalize to new, more complex scenarios."
            },
            "weaknesses": {
                "value": "Outdated Baselines: All the baseline algorithms were published over two years ago, and the original implementations of these baselines do not support visual inputs. The lack of SOTA vision-input baselines, such as Lambda [1], Safe SLAC [2], and SafeDreamer [3], limits the benchmark\u2019s relevance in evaluating current state-of-the-art safe RL methods.\n\nSolvability by Existing Algorithms: Have the tasks introduced in this framework already been solved by existing algorithms? For instance, can PPO-PID successfully address these tasks? Are there settings within HASARD that current algorithms struggle to handle? By not including experiments with the latest baselines, it is unclear whether the HASARD benchmark will drive the development of new algorithms or simply reaffirm existing solutions.\n\nTask Complexity:\nWhat is the primary contribution of HASARD compared to existing safety benchmarks, such as Safety Gymnasium? Compared to Safety Gymnasium, HASARD primarily adds hard constraints and fast simulation. However, implementing hard constraints is relatively straightforward, merely requiring a single line of code to terminate the episode upon any unsafe action. As for fast simulation, HASARD achieves this by sacrificing simulation fidelity and simplifying the action space, which limits its meaningfulness as a contribution compared to Safety Gymnasium.\n\nMoreover, most tasks in HASARD revolve around avoiding hazardous obstacles, which has already been extensively addressed and solved in Safety Gymnasium by existing algorithms (e.g., [1-3]). Given HASARD's simplified dynamics and action space, it would need to introduce more complex tasks than those in Safety Gymnasium to stimulate the development of new algorithms. However, I did not observe any such complexity in the task design that would distinguish it from prior benchmarks.\n\n[1] CONSTRAINED POLICY OPTIMIZATION VIA BAYESIAN WORLD MODELS\n\n[2] Safe Reinforcement Learning From Pixels Using a Stochastic Latent Representation\n\n[3] SafeDreamer: Safe Reinforcement Learning with World Models"
            },
            "questions": {
                "value": "1. Which tasks in HASARD require memory capabilities, and which involve long-horizon decision-making? It would be helpful if the authors could clarify how the benchmark challenges an agent\u2019s memory and planning capabilities over extended time sequences.\n\n2.  Why did you choose ViZDoom to build this benchmark? Does this platform offer specific advantages? From my perspective, it seems that ViZDoom allows only minor modifications to its existing game structure and may lack the flexibility to define more complex, varied tasks. Why not consider using a truly open-world environment, such as MineDojo [4], which enables safer RL environments with more sophisticated task definitions? A platform like MineDojo could potentially support a broader range of scenarios and facilitate more diverse task creation.\n\n3. Additionally, I noticed that you used Omnisafe for algorithm benchmarking, but this wasn\u2019t mentioned in the paper. I have some questions regarding one of the baselines you implemented. In the P3O algorithm code (see here:https://github.com/PKU-Alignment/omnisafe/blob/main/omnisafe/algorithms/on_policy/penalty_function/p3o.py#L82), there is a term  J_c  in the loss function that appears to be independent of the network parameters. What effect does including J_c in the loss function have? I observed in your experimental results that P3O also fails to satisfy the constraints, which may be related to the J_c term. This raises some doubts about the effectiveness of this baseline.\n\n[4] MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "HASARD is a benchmark testing platform specifically designed for safe reinforcement learning, based on ViZDoom, providing a diverse range of 3D environments.\n\n1. The tasks on this platform require agents to pursue high rewards while considering safety strategies, moving beyond simple 2D navigation to incorporate complex elements such as spatial understanding.\n2. HASARD offers three difficulty levels and supports both soft and hard safety constraints, flexibly adapting to varying safety requirements.\n3. The platform integrates Sample-Factory, enabling high-speed simulation that allows agents to address real-world safety challenges while reducing computational costs.\n4. HASARD includes six environments based on ViZDoom and benchmarks various methods to demonstrate the limitations of existing technologies."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors tested six baseline algorithms on HASARD and provided an analysis of the results.\n2. The tasks move beyond simple 2D navigation to incorporate complex elements such as spatial understanding"
            },
            "weaknesses": {
                "value": "1. The reviewer believes that if the distinction between soft and hard constraints is merely based on whether the threshold is $0$, then other benchmarks share this characteristic, making this claim somewhat unsubstantiated.\n2. Although multiple methods were tested in the current experiments, there is a lack of analysis on performance under different safety budgets. It is recommended to include experiments with varying safety thresholds to better understand the trade-off between safety and reward for each algorithm.\n3. HASARD is based on the ViZDoom game engine, which, while computationally inexpensive, lacks detailed simulation of real-world physics.\n4. The anonymous video link provided by the authors is inaccessible."
            },
            "questions": {
                "value": "1. The article does not provide an in-depth analysis of performance under different safety budgets. Is there a plan to supplement the experiments with varying safety thresholds to comprehensively demonstrate the trade-offs between reward and safety for each algorithm? This would be very helpful in understanding the adaptability of different methods under various safety requirements.\n2. Considering the limitations of ViZDoom in simulating real-world physics, have the authors explored other engines with superior physical simulation capabilities (e.g., Isaac Gym)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new egocentric vision-based 3D simulated environment for benchmarking safe reinforcement learning. The benchmark is more realistic and challenging compared to common prior safe RL benchmark environments. In addition, the paper has evaluations for some safe RL algorithms on the proposed benchmark demonstrating its feasibility of use and the potential for building better approaches to perform more favorably on it."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well motivated and targets an important problem - that of building realistic and reliable RL benchmarks, and more specifically benchmarks for safe RL. This involves addressing challenges with the simple natural of prior benchmarks - both visually and in terms of higher dimensional action space and increased temporal horizons. \n\n- The proposed benchmark HASARD is built on top of an existing game engine VizDoom and is able to inherit all of its properties for re-use. The multiple levels in HASARD can be potentially helpful in evaluating different notions of safety in proposed safe RL algorithms. \n\n- The paper has detailed evaluations of several safe RL algorithms on HASARD indicating that the framework is feasible for training constrained RL policies. The evaluations reveal that simple algorithms based on PPO and constrained PPO can achieve non-trivial performance in the benchmark and also reasonable constraint satisfaction. It is good to see that these simple algorithms do not saturate the benchmark and there is still a lot of room for improvement."
            },
            "weaknesses": {
                "value": "- Unfortunately, while the paper is a decent attempt at building a safe RL benchmark, I am not convinced the safe RL community will be incentivized to use it. The main reason is that the notions of constraints in this benchmark are not directly tied to the very pragmatic safety considerations that need to be tackled in the real world - ranging from control systems to robotic deployments. \n\n- The benchmark feels a bit incremental compared to the already existing VizDoom framework that has been around for years. The modifications for the different levels and environments in this framework do not capture the notions of open-world generalization and realism the field is headed towards in terms of evaluating RL systems. In addition, a lot of prior safe RL works have bechmakred their systems on real-world systems like robotic navigation and manipulation, and I am not convinced that a modified VizDoom framework is likely to create a reasonable impact in the community. \n\n- The evaluations are all with variants of PPO and no other safe RL algorithms are tested. It is unclear why this is the case, since in my understanding the benchmark should not be tied to a particular type of algorithm"
            },
            "questions": {
                "value": "Please refer to the weaknesses above:\n\n- Unfortunately, while the paper is a decent attempt at building a safe RL benchmark, I am not convinced the safe RL community will be incentivized to use it. The main reason is that the notions of constraints in this benchmark are not directly tied to the very pragmatic safety considerations that need to be tackled in the real world - ranging from control systems to robotic deployments. Could the authors clarify how exactly they envision this benchmark to drive innovation in the safe RL community? And what sub-field of researchers would be likely to use it?\n\n- The benchmark feels a bit incremental compared to the already existing VizDoom framework that has been around for years. Can the authors clarify if the proposed modifications are non-trivial and if they can be broadly applied to potentially other frameworks like Minecraft and other games?\n\n- The evaluations are all with variants of PPO and no other safe RL algorithms are tested. It is unclear why this is the case, since in my understanding the benchmark should not be tied to a particular type of algorithm. Please clarify the evaluations and if there is any specific assumption on the type of safe RL algorithms that could be tested on the benchmark? \n\n- Can the authors make 1-1 comparisons with the proposed benchmark and the features of prior simulated and real world benchmarks that have been used by safe RL papers in  the past?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the HASARD, a benchmark designed for egocentric pixel-based safe RL in diverse and stochastic 3D environments. Unlike existing benchmarks, HASARD emphasizes spatial comprehension, short-term planning, and active prediction for high rewards while ensuring safety. It offers three difficulty levels, supporting both soft and hard safety constraints. The benchmark includes heatmaps for visual analysis, aiding in strategy development. By targeting vision-based embodied safe RL, HASARD addresses the need for benchmarks mirroring real-world complexities. The paper's contributions include the design of six novel ViZDoom environments with safety constraints, integration with Sample-Factory for rapid simulation and training. Evaluation of baseline methods within HASARD highlights challenges in balancing performance and safety under constraints."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper demonstrates several notable strengths across the dimensions of originality, quality, clarity, and significance: \n\n  \n\n1. **Originality**: It introduces HASARD, a benchmark specifically designed for vision-based embodied safe reinforcement learning (RL) in complex 3D environments. \n\n  \n\n2. **Quality**: Comprehensive design of 6 diverse environments with 3 difficulty levels each, offering a range of challenges. \n\n  \n\n3. **Clarity**: The paper is structured in a logical and coherent manner, facilitating the understanding of complex concepts. \n\n  \n\n4. **Significance**: The paper Addresses an important need in safe RL research for more realistic and challenging benchmarks. It enables systematic evaluation and comparison of safe RL algorithms in vision-based 3D settings."
            },
            "weaknesses": {
                "value": "While the paper makes valuable contributions, several areas could be improved: \n\n  \n\n1. The paper refers to ViZDoom as a 3D environment, but its pixelated, less detailed graphics compared to modern 3D games challenge this characterization. \n\n  \n\n2. **Narrow Range of Baselines**: Evaluations focus primarily on PPO-based algorithms. Incorporating approaches like model-based safe RL or constrained policy optimization (e.g., https://arxiv.org/abs/2210.07573) would enhance the assessment. \n\n  \n\n3. **Limited Visual Input Analysis**: Though vision-based learning is emphasized, the paper lacks analysis of how visual complexity influences performance. Exploring different visual conditions (lighting, distractors) and comparing raw pixels with simplified representations would highlight the unique challenges of vision-based safe RL, especially since the visual inputs in the environment appear less realistic. \n\n  \n\n4. **Action Space Limitation**: Only discrete action spaces are supported. It is unclear how continuous safe RL algorithms would be benchmarked. \n\n  \n\n5. **Real-World Relevance**: The connection between the benchmark tasks and real-world safe RL challenges needs clearer articulation. Providing examples of practical applications would strengthen motivation. \n\n   \n\nAddressing these points could strengthen the paper and increase the impact and utility of the HASARD benchmark for the safe RL research community."
            },
            "questions": {
                "value": "1. Is ViZDoom truly a 3D environment, considering its graphics appear pixelated and less detailed compared to modern 3D games? \n\n  \n\n2. Why are the baseline algorithms limited to PPO-based approaches? Could the paper include more diverse methods, such as model-based safe RL or constrained policy optimization (e.g., https://arxiv.org/abs/2210.07573)? \n\n  \n\n3. How can continuous safe RL algorithms be benchmarked when the paper only supports discrete action spaces?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}