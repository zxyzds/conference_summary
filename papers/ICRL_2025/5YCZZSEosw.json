{
    "id": "5YCZZSEosw",
    "title": "Let Large Language Models Find the Data to Train Themselves",
    "abstract": "The current iterative development process for large language models (LLMs) is heavily data-centric, relying on human researchers and engineers to manually analyze model performance and determine what data to acquire for further training. However, this human-supervised approach is costly and may fail to identify optimal training signals. Its scalability is further limited as models become increasingly capable and may eventually exceed human intelligence. To address these issues, we propose an automated framework that enables models to autonomously discover and strategically acquire the most valuable training data to enhance their performance. It establishes a self-improving framework where models can invoke APIs to crawl and/or generate tailored datasets from various resources and environments, and retrain themselves. The data selection decisions are shaped by reinforcement feedback signals that reward performance gains while penalizing computational overhead. This formulation incentivizes models to develop self-knowledge about their strengths and areas for improvement in order to efficiently select training data. Empirical results demonstrate that LLMs operating within our framework are able to autonomously and strategically acquire valuable training data to enhance their performance across a variety of skills in 1,000 diverse in-house test tasks and three public benchmarks.",
    "keywords": [
        "Self-improving",
        "Synthetic Data",
        "Large Language Models"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We pioneer the idea of automating the data search process for training LLMs, a task currently handled by expert human efforts, making a further step towards fully automated self-improving AI systems capable of continuous learning and adaptation.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5YCZZSEosw",
    "pdf_link": "https://openreview.net/pdf?id=5YCZZSEosw",
    "comments": [
        {
            "summary": {
                "value": "Currently, LLM developers manually analyze model errors and engineer training datasets (be it through human labels, or synthetically) to enhance model performance across pre-training, instruction tuning, and preference learning stages. This approach is costly, error-prone, heavily reliant on developer expertise, and lacks scalability. To address these limitations, this paper introduces the **Active Data Search (ADS)** framework, where an LLM assesses its own strengths and weaknesses for a given task and autonomously collects relevant training data to improve itself.\n\nAt the core of ADS is an LLM-driven optimizer that, when given a target task (grounded by a few guiding questions), evaluates the capabilities of the primary LLM (referred to as the policy LLM) for the task and dynamically calls different data retrieval and generation APIs to collect relevant data for improvement. This data can then be used to fine-tune the policy LLM for the target task or enhance it via in-context prompt augmentation (as implemented in this paper). Since LLMs typically are not designed for this autonomous data collection task, ADS first trains this optimizer offline with reinforcement learning to balance the reward and cost associated with obtaining new data.\n\nThe paper employs three simple data collection APIs: (a) information retrieval from Wikipedia, (b) demonstration generation powered by an LLM, and (c) question-answering using an LLM. Trained on the MagPie dataset, the optimizer effectively learns to use these APIs to acquire relevant data, leading to improved performance on an internal test set and on 3 additional public benchmarks (AlpacaEval, MT-Bench, and Arena-Hard) across two different LLMs, at times matching the performance of larger LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper tackles the important challenge of gathering optimal data to enhance LLM performance for a given task.\n2. The proposed framing, where LLMs become self-aware of their strengths and weaknesses to identify optimal data for self-improvement, is compelling. This framing has significant implications, not only for advancing the next generation of state-of-the-art LLMs but also for democratizing AI by empowering non-experts to tailor LLMs to their specific needs.\n3. The proposed ADS method of framing diverse data collection techniques as API/Tool calls and training an optimizer using DPO to dynamically select the most effective data collection API calls for a given task, is innovative and powerful.\n4. The paper\u2019s related work section is well-structured, effectively positioning the authors' contributions within the context of prior research.\n5. Experimental evaluations demonstrate substantial improvements over the baseline Qwen-2-7b-instruct model across several public benchmarks."
            },
            "weaknesses": {
                "value": "The core weaknesses of this paper can be organized along the following broad themes:\n\n---\n### Insufficient Experimental Validation\n---\nThe paper lacks comparisons with key baselines and prior work, making it challenging to fully assess the contributions of this approach. Specifically:\n\n1. **Dynamic API Selection Validation**: While the individual data collection APIs\u2014Information Retrieval, Demonstration Generation, and Question Generation\u2014are well-studied in prior work as Retrieval-Augmented Generation and its variants [1], Self-Instruct [2], and behavior cloning/distillation from teacher LLMs respectively, this work\u2019s primary novelty lies in an optimizer that dynamically selects among these APIs. However, the paper does not compare this dynamic selection against these established methods, leaving unclear whether it is indeed critical for improved performance. To clarify, a comparison is recommended in the following settings:\n\n    - *Inference-only Comparison*: Evaluate ADS against comparable-sized synthetic datasets generated by prior methods, as well as a larger dataset from these methods, given that they do not require additional compute for optimizer training.\n    - *Optimizer Training with Single APIs*: Train ADS-style optimizers with individual APIs (e.g., Information Retrieval only) on the MagPie dataset, to assess the relative impact of each API within ADS.\n\n&nbsp;\n\n2. **MagPie Fine-Tuned Baseline**: Unlike ADS, which is trained on the MagPie dataset, baseline LLMs are not. Since fine-tuning on MagPie has been shown to enhance performance on target benchmarks, this gives ADS an advantage over baseline models. A more balanced comparison would involve fine-tuning baseline LLMs on MagPie to assess if ADS's optimizer training and synthetic data generation offer distinct advantages over straightforward fine-tuning.\n\n&nbsp;\n\n---\n### Scalability Concerns & Scaling Experiments\n---\n3. **Generated Dataset Size and Limitations**: The paper studies the proposed method in an in-context policy improvement setting, where synthetic data is added directly to the prompt. This restrictive setting limits the amount of synthetic data that can be utilized, as it must fit within the model's maximum sequence length. Discussion on scenarios where generated data exceeds this length and experiments studying the impact of synthetic data scaling on the method\u2019s effectiveness would strengthen the paper.\n\n4. **Input Task-Specific Dataset Size**: The method currently relies on only five labeled instances from the target task to generate synthetic data, whereas real-world applications may allow access to more extensive task-specific data. Including a discussion on method scalability to larger datasets (e.g., prompt with all task data at once or using sampling and multi-prompting) and experiments exploring the impact of task-specific data scaling would be beneficial.\n\n5. **Method Extension and Efficacy for Fine-Tuning**: Since the method is limited to in-context policy improvements, its effectiveness in policy fine-tuning contexts remains untested. Additionally, the feasibility of using the current optimizer training algorithm for fine-tuning is uncertain, as retraining policies to obtain reward for each data generation step could be prohibitively costly. Discussion of this limitation and experiments in fine-tuning scenarios could further clarify the method's general applicability.\n\n&nbsp;\n\n---\n### Method Robustness Concerns\n---\n6. **Optimizer Generality**: It is unclear if the optimizer learns generalizable insights into model strengths and weaknesses or if its effectiveness is limited to specific tasks. Current training and testing datasets are closely related (e.g., MagPie -> AlpacaEval, etc.). Testing on unrelated benchmarks, like Big-Bench-Hard, GSM8k, or MMLU, would offer insights into the optimizer\u2019s generality.\n7. **Dependence on MagPie Dataset**: The extent to which the optimizer\u2019s performance depends on the choice of MagPie as the training dataset is unclear. Evaluating its performance when trained on different datasets would provide evidence of ADS's broader applicability.\n8. **Reliance on Task Clustering**: ADS clusters similar instances into tasks to generate synthetic data during both training and testing. This clustering may not always be feasible and could require manual intervention. Without this careful clustering, retrieving the relevant synthetic data for a test instance could become a challenge, raising questions about the method's performance if instances are grouped differently, such as randomly, during data generation.\n\n&nbsp;\n\n---\n### Limited analysis\n---\nSeveral important areas lack critical analysis in the paper:\n\n9. **Generated Synthetic Data**: The paper does not analyze the characteristics or quality of the synthetic data generated. A manual review of the generated data could provide valuable insights into its role in improved performance.\n\n10. **API Call Patterns**: There is no examination of the API calls made by the optimizer at test time. Analyzing the frequency and distribution of these calls could clarify the optimizer's behavior and the relative importance of each data source (API).\n\n11. **Weakness Reflection Rationales**: Although the paper claims that the optimizer uses LLM-driven reflections to assess model strengths and weaknesses, it lacks a qualitative or manual analysis of these reflections. This omission makes it challenging to gauge the accuracy and depth of these self-assessments.\n\n&nbsp;\n\nReferences:\n\n[1] Yu, Wenhao, et al. \"Generate rather than retrieve: Large language models are strong context generators.\" arXiv preprint arXiv:2209.10063 (2022).\n\n[2] Wang, Yizhong et al. \u201cSelf-Instruct: Aligning Language Models with Self-Generated Instructions.\u201d Annual Meeting of the Association for Computational Linguistics (2022)."
            },
            "questions": {
                "value": "Please refer to the *Weakness* section above for recommendations on experiments and analyses that could further strengthen the paper.\n\nAdditional questions and suggestions **that do not impact the score**:\n\n1. How are preference pairs constructed from sampled API trajectories for rejection sampling and DPO? Is a single preference pair created from the highest and lowest reward trajectories?\n2. Is the cost of API calls considered only when selecting an efficient trajectory from a trajectory tier (group) or do you also add a cost term to the overall reward?\n3. Are all prompts (individual APIs, optimizer, evaluation prompts) zero-shot? If not, how many demonstrations are provided for each?\n4. Consider expanding the discussion on other data collection techniques (APIs) that could be integrated into the framework.\n5. The \"internal test-set\" is not described in the paper. Without additional details about this internal test set, it is difficult to assess the quality of the improvements. Is it just the test partition of the MagPie dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces the Active Data Search (ADS) framework, which enables large language models (LLMs) to autonomously identify and acquire training data to improve their own performance with minimal human intervention. The authors propose an optimizer model that generates API calls for data collection from various external sources, such as web search engines, AI assistants, and annotation services. This framework leverages reinforcement learning to refine the optimizer, balancing task performance enhancement with computational cost. Experimental results, using models like Qwen-2-7B-Instruct and Gemma-2-9B-Instruct, show substantial performance gains in in-house tasks and public benchmarks, demonstrating ADS\u2019s effectiveness in enabling smaller models to achieve results comparable to larger ones."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper presents the ADS framework, which automates the identification and acquisition of training data, enhancing the self-improvement capabilities of LLMs. This direction holds promise for reducing reliance on human intervention, making model development more efficient and scalable."
            },
            "weaknesses": {
                "value": "While the paper offers an innovative approach, it has several areas that could be strengthened. The lack of comparison with diverse baselines limits the assessment of the ADS framework's true efficacy. Specifically, comparisons with \n(a) LLMs using basic RAG for retrieval,\n(b) untrained LLMs collecting data for policy model inference, \n(c) training the optimizer model with simple rule-based or manually collected data,\n(d) other naive automated data collection techniques would provide deeper insights into the relative advantages of ADS. \nIncluding these baselines would enhance the evaluation's comprehensiveness and validate the framework's practical significance against existing methods."
            },
            "questions": {
                "value": "1. Relationship with RAG: The paper lacks a thorough discussion on how the proposed ADS framework relates to or differs from Retrieval-Augmented Generation (RAG). Given that both approaches involve external data retrieval, it would be beneficial to address whether ADS extends, overlaps, or diverges from RAG in terms of methodology, application, or objectives. This comparison would help position the contribution within the broader context of retrieval-based techniques and clarify its novelty.\n\n2. Inconsistent Use of Llama in Experiments: The absence of Llama-based experiments in Figure 2, despite its inclusion as a comparison point in Table 2, raises questions about consistency. It would be helpful for the authors to explain why Llama models were not tested under the same conditions as other models, or why their results were only included in specific sections. This would ensure a clearer understanding of the comparative analysis and model choice rationale."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces an automated framework called ACTIVE DATA SEARCH (ADS), designed to enable large language models (LLMs) to autonomously discover and acquire valuable training data for self-improvement without the need for human supervision.\nThe authors propose using reinforcement feedback signals to guide the models in selecting optimal data, rewarding performance gains while penalizing computational overhead.\nThe framework is validated through extensive experiments on 1,000 diverse in-house test tasks and three public benchmarks, demonstrating significant performance improvements."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(1) The ADS framework is innovative as it leverages LLMs to autonomously enhance their training data, reducing the need for costly human intervention and addressing scalability issues. The use of reinforcement feedback signals to balance performance improvement and computational efficiency is a practical and smart approach.\n(2) Empirical results are robust, including 1,000 in-house test tasks and three public benchmarks, showing clear performance gains and generalization capabilities. The inclusion of detailed implementation protocols, such as API designs for data acquisition, is valuable for reproducibility.\n(3) The paper presents a method for iterative refinement, demonstrating consistent performance improvements with the optimizer refinement through reinforcement learning."
            },
            "weaknesses": {
                "value": "(1) The paper would benefit from more comprehensive ablation studies. Specifically, it would be insightful to understand the individual contributions of each proposed API and the iterative refinement process. For example, what is the impact of excluding one of the APIs, or how does the system perform without the iterative refinement?\n(2) The paper does not sufficiently discuss the potential limitations and failure cases of the ADS framework. Identifying scenarios where the framework might not perform well or discussing any observed limitations during the experiments would provide a more balanced view.\n(3) Comparisons with more diverse sets of baselines, especially in terms of data acquisition strategies, would strengthen the validation of the effectiveness of the proposed framework.\n(4) The exposition around reinforcement learning strategies could be expanded to aid comprehension, especially for readers less familiar with advanced reinforcement learning techniques."
            },
            "questions": {
                "value": "(1) Could you perform additional ablation studies to analyze the impact of each data acquisition API within the framework? Understanding the individual contributions of each API would highlight their importance and the overall robustness of the ADS framework.\n(2) Were there any particular scenarios or tasks where the ADS framework did not perform as expected? If so, what were these failure cases, and how were they addressed or mitigated in the study?\n(3) Could you provide further comparisons with alternative data acquisition methods beyond the ones mentioned in the paper?\n(4) How sensitive is the ADS framework to different policy model architectures and sizes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a self-improvement framework for LLMs that leverages external environments for data acquisition.\n\nDuring training of the optimizer model, given a set of tasks and instructions for each task, the optimizer model learns (through RL) to generate optimized API call trajectories to acquire relevant external data, by measuring how the acquired data help improve performance of the policy model through finetuning or in-context learning on the data. The RL also takes cost control into account.\n\nDuring test time, the optimizer model generates data acquisition calls for tasks and instructions unseen during training and the data acquired is used to improve the policy model for these tasks.\n\nThe experiments show that data acquisition significantly improves the policy model's performance on various tasks, including Alpaca Eval, Arena Hard, and MT bench, when relevant tasks and instructions are provided for training the optimizer model and with in-context learning of the policy model on the acquired data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality: Self-improving LLMs via data acquisition is a novel direction, to the best of my knowledge.\nQuality: The paper shows promising quality gains against the baselines.\nClarity: The framework and the complex process of training the optimizer model are clearly explained.\nSignificance: Self-improvement of LLMs is clearly an important direction. Acquiring data from an external environment will be critical."
            },
            "weaknesses": {
                "value": "Significance: The quality gains against a stronger baseline strategy that \"utilizes the Question Answering API for each  observed instruction in the target task\" are relatively minor. \n\nGeneralization: It's not clear how well the optimizer model generalizes across tasks. The experiments are set up so that the training tasks and instructions appear to be similar to those used during test time. The paper would be strong if it could demonstrate generalization across more distinct tasks."
            },
            "questions": {
                "value": "What are the important characteristics of the optimized trajectories that allow them to outperform the baseline trajectories? \n\nWhat roles do \"information retrieval\", \"demonstration generation\", and \"question answering\" each play? What if we remove one of the APIs? How does their respective quality affect the final results?\n\nThe comparison against the \"Question Answering for all instructions\" strategy is only conducted for the in-house test set. Does it hold across benchmarks? \n\nIn Algorithm 1, \"Split task instructions Q into observed set Qo and held-out set Qh\" is in the iteration loop (t), does it mean that the split is different across iterations?\n\nIn section 4.3 \"we employ the Self-Instruct approach to generate three new instructions for each task and use them as the observed instructions for that task\". How much overlap is there between the generated instructions vs. the original ones?\n\nIn section 6.2, \"In comparison to the approach that focuses solely on maximizing performance without considering costs, our method not only achieves a lower cost as expected but also demonstrates an improved win rate\". It's not clear why cost control also improves quality. Is this due to randomness or some other factor?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents the active data search (ADS) framework, allowing large language models to autonomously acquire training data without human supervision. ADS learns an \"optimizer\" via reinforcement learning to enable active data acquisition while minimizing costs. Experiments demonstrate that ADS leads to performance gains compared with the original model."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The method is well-motivated. Letting LLMs actively identify training data for themselves can potentially improve data efficiency and reduce human efforts in model training."
            },
            "weaknesses": {
                "value": "- **The true implementation of ADS is different from its description.** In Sec. 3.2, the authors claim that \"_we **train** the original $\\mathcal M^p$ on the tailored training dataset to update its knowledge and capacities_\". This claim is very misleading because according to Appendix C, the so-called \"training\" is simply adding the collected data to the prompt. The authors refer to this as \"in-context learning\" (ICL). However, in [1], ICL is introduced as an _inference_ approach in contrast to _training_. Thus, I believe the main selling point in the title, \"finding the data to _train_ the models\", is a $\\text{\\color{red}significant overclaim}$, which can be very misleading to the community.\n\n- **The paper does not compare ADS with any relevant baselines.** The authors include a few relevant papers in Sec. 2, e.g., [2,3]. The authors should compare the cost and quality of ADS with baselines to demonstrate its effectiveness. \n- **I'm concerned about the efficiency of the method.** Algorithm 1 requires $\\mathcal O(N|\\mathcal A||\\mathcal T|)$ evaluations of the policy model. No where in the paper do the author report the computation cost of the method. It's also not clear whether the method can scale up to more types of API calls and larger models in a truly practical scenario.\n- Minor issues:\n  - Line 50: This sentence is hard to read and grammatically  incorrect. Please consider the modification proposed by LLM: \"We achieve the above automatic process through development of an optimizer that generates these APIs sequentially based on the target task to solve.\"\n  - Line 58: `\\citet` $\\to$ `\\citep`\n  - Line 138: LLM-as-a judgment  $\\to$ LLM-as-a judge\n\n[1] Language Models are Few-Shot Learners\n\n[2] Rlaif: Scaling reinforcement learning from human feedback with ai feedback.\n\n[3] AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models"
            },
            "questions": {
                "value": "Please refer to the **weakness** part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}