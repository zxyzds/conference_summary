{
    "id": "t1nZzR7ico",
    "title": "Automatic Jailbreaking of Text-to-Image Generative AI Systems for Copyright Infringement",
    "abstract": "Recent AI systems have shown extremely powerful performance, even surpassing human performance, on various tasks such as information retrieval, language generation, and image generation based on large language models (LLMs). At the same time, there are diverse safety risks that can cause the generation of malicious contents by circumventing the alignment in LLMs, which are often referred to as jailbreaking. However, most of the previous works only focused on the text-based jailbreaking in LLMs, and the jailbreaking of the text-to-image (T2I) generation system has been relatively overlooked. In this paper, we first evaluate the safety of the commercial T2I generation systems, such as ChatGPT, Copilot, and Gemini, on copyright infringement with naive prompts. From this empirical study, we find that Copilot and Gemini block only 5\\% and 11.25\\% of the attacks with naive prompts, respectively, while ChatGPT blocks 96.25\\% of them. Then, we further propose a stronger automated jailbreaking pipeline for T2I generation systems, which produces prompts that bypass their safety guards. Our automated jailbreaking framework leverages an LLM optimizer to generate prompts to maximize degree of violation from the generated images without any weight updates or gradient computation. Surprisingly, our simple yet effective approach successfully jailbreaks the Copilot and ChatGPT with 0.0\\% and 6.25\\% block rate, respectively, making it generate copyrighted contents in 73.3\\% of the time. Finally, we explore various defense strategies, such as post-generation filtering and machine unlearning techniques, but found that they were inadequate, which suggests the necessity of stronger defense mechanisms.",
    "keywords": [
        "Copyright",
        "jailbreaking",
        "T2I model"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=t1nZzR7ico",
    "pdf_link": "https://openreview.net/pdf?id=t1nZzR7ico",
    "comments": [
        {
            "summary": {
                "value": "The authors study jailbreaking commercial text-to-image systems to produce copyright infringing outputs, and highlight the vulnerability of these systems. The authors benchmark 4 systems on a dataset of copyright images they labeled (70 images). The authors show that current t2i systems produce copyright infringing outputs readily, with the except of ChatGPT. The authors propose an automatic jailbreak pipeline to generate copyright infringing images, by prompting LLM to literately refine a jailbreak prompt. The author follow a setup close to OPRO (Yang et al.), using LLM as an optimizer and uses CLIP score as optimization feedback. Unlike prior work, a classifier is not required for the jailbreak method but just a target image. The jailbreak pipeline improves over naive prompting and prior work by reducing the block rate of copyright generation, and achieving higher copyright infringement evaluation based on the human study."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors propose a practical pipeline and demonstrates its efficacy. The ablation experiments show how the different components in the score contribute to the performance.\n2. The authors point out the lack of robustness against copyright generation of t2i systems, with additional experiments showing the same vulnerability in one concept erasure method. \n3. The authors discuss the societal implication of their work and motivates the topic of study well."
            },
            "weaknesses": {
                "value": "1.  The authors acknowledge that 'our approach has the limitation that the violation rate does not always reproduce the same due to the randomness of the commercial T2I systems' (line 485). Given the small dataset size (70 images), how many iterations were run for the experiments in the tables? Could the authors report block rates and evaluation scores based on averages across multiple iterations? \n2. The paper has a high number of formatting, stylistic, and wrong element reference issues. The paper would benefit greatly from another round of proofreading for writing quality and clarity. The list below is not exhaustive.\n\ntypo/wrong element reference:\n- Line 407: should be linking Table 5 instead of Table 9\n- Figure 5 is not present in the paper, even though it was referenced in main text\n- \"Charcater\" in Table 7\n- Line 661: \"There are 20 images in each category, as shown in Table 13.\" The art category has 10 samples, not 20.\n\nstyle/format:\n- Many sentences are awkwardly constructed and/or have grammatical errors. For example: \"Furthermore, not only generating the\ncontents, the contents are exceptionally similar to the original IP content as shown in Figure 3\"  (line 354). \"This work has been\ndeemed exemption by IRB (IRB-2x-3xx) in anonymous\" (line 903). \"We show that the majority of commercial T2I systems result in copyright violation\" (line 127). \"Gemini-pro blocks all human-included generation in the current version which may block content not due to its harmfulness\" (line 221).\n- Image was cut off on page 7\n- Large gap of spacing in the middle of page 8\n3. The authors have conducted a human study to understand whether study participants consider generated images to be have copyright infringement issues. However, there does not seem to be discussion around how study participants are trained towards differentiating copyright infringement and fair use. The human study provides more insight around participant perception of copyright infringement, than actual determination of violation determination. \n4. Commercials T2I models are updated continuously. Please include the release date or version of each T2I model. \n5. One of the paper's contribution is dataset, though the dataset was not included as an anonymized link for review."
            },
            "questions": {
                "value": "Q1: \"Identical violations\" was mentioned twice in the paper but not defined. In lines 359-360, authors state that \"Upon examining the images classified as identical violations, it was found that over 50% were deemed to be cases of copyright infringement in product and logo.\" If identical violations refer to generating nearly identical images, the number of over 50% being deemed as infringement seems low. \n\nQ2:  Lines 371-372: \"In Figure 4, 42.19% of the generated images correctly answer more than seven questions.\" What are the seven questions? \n\nQ3: How many iterations of experiments are run for each copyright content in the dataset, for each table? Since these systems have randomness, how is the variance accounted for?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper primarily studies the critical issue of copyright infringement in text-to-image (T2I) models. Initial analysis showed that popular T2I systems such as Midjourney, Copilot, and Gemini are highly vulnerable to copyright violations even when using simple jailbreak prompts. While ChatGPT had a block rate of around 84% on simple prompts, the authors crafted an Automated Prompt Generation Pipeline (APGP) which significantly reduced the ChatGPT\u2019s block rate to around 6%. To summarize the contributions: (1) the authors highlighted a serious safety issue: showing that state-of-the-art T2I models can be easily jailbroken using optimized prompts without needing access to model gradients, and 2) created an annotated dataset, VioT, for evaluating copyright violations in T2I models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The finding that these widely used T2I models can be jail-broken using a simple prompt optimization approach is valuable to the community for further research on designing defense mechanisms. \n\n2. The overall paper presentation is quite good, with the research problem well-articulated to the reader (Except Figure 5 on page 7 which has some small formatting issues). Further, each component of the proposed jailbreak attack has also been well-motivated.\n\n3. Evaluation on the proposed VioT dataset clearly shows the efficacy of the proposed jailbreak framework. It is nice to see evaluations using both human and automatic metrics, which strengthens the experimental evaluations."
            },
            "weaknesses": {
                "value": "1. There are two stages of optimization in APGP, first using the VLM to search for the seed prompt and then again revising the prompt based on some defined scores. The authors should provide a comparison of the latency of their approach against other jailbreak methods. It is essential to understand the computational overhead of the approach for practical applications.\n\n2. Based on, Figure 13 (Appendix A.1), the VioT dataset has only 70 images. I think the evaluation of the proposed framework on only 70 images is not very convincing to demonstrate its effectiveness. I request the authors to clarify if I am missing something.\n\n3. [Minor] Although the paper has ablations on each component, it would further strengthen the draft if the authors could include an ablation on the LLM used for optimizing the prompt, which has been currently set as GPT-3.5-Turbo."
            },
            "questions": {
                "value": "To summarize the weakness, my major concerns are regarding the overall latency for generating the jailbreak prompts (see Weakness 1) and the limited size of the evaluation dataset (see Weakness 2)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors perform a study about the tendency of SOTA T2I models to produce copyrighted content. They do so by:\n- building a copyright violation dataset for T2I models, with characters, logos, products and arts. \n- producing naive prompts and a jailbreaking procedure\n- analysing the successfulness of both attacks and defences in the generation of copyrighted content across SOTA models. Concluding defences are currently inadequate.\n- the automated jailbreaking procedure uses an ageintic approach which is interesting."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Originality: the work is not extremely novel since several papers exist that use LLMs to jailbreak or induce regurgitation of training data of other models (e.g. [1,2]), one of which does it for memorization uncovering. Similarly the optimization procedure that is proposed is not extremely novel. However I do not know other works that use MLLMs for this specific purpose. \n-  Clarity: The paper writing is clear\n- Quality: the methodology is good and the experiment quality is sufficient. \n- Significance: The problem is of obvious relevance to companies. The introduced dataset and jailbreaking procedure have the potential of being useful. \n\n[1] https://arxiv.org/html/2312.02119v3\n\n [2] https://arxiv.org/abs/2403.04801"
            },
            "weaknesses": {
                "value": "- The authors counterargue the idea of using a coopyright detection model, suggesting that since there's no open sourced one then it's not practical. Would it be possible to just use MLLMs themselves as filters of copyrighted contents? An LLM could probably guess a good list of entities that are copyrighted (or find them in a catalogue) given the prompt and then an MLLM can simply verify the presence or absence of the copyrighted content in the image."
            },
            "questions": {
                "value": "Minor observation, The figure after Figure 4 is obviously misplaced."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper evaluated the safety of the commercial T2I generation systems on copyright infringement with naive prompts. The paper also proposed a stronger automated jailbreaking pipeline for T2I generation systems, which produced prompts that bypass their safety guards."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Comprehensive evaluation results\n2. The paper also tested some simple defenses to mitigate their attack\n3. The automated prompt generation process to stress-test the VLM for copyright issue is an important research question."
            },
            "weaknesses": {
                "value": "1. L50, \"To the best of our knowledge, there is no work on quantitative evaluation of the copyright violation by the commercial T2I systems\". Can you talk about the relationship between your work and the Glaze tool [1]? The Glaze tool also aims to protect the copyrighted and private images created. \n2. Suggest to recreate Fig. 1 and consider combining Fig. 2 with Fig. 1. Fig. 2 seems to be the major selling point of the paper, while I cannot clearly tell from Fig. 1, which generated image is copyrighted versus safe to output to users. \n3. L193, a(n) automated\n4. Why is this tool copyright specific? I feel like the prompt generation pipeline is agnostic to the type of the attack? Maybe I missed something here. \n5. What is a potential solution for/defense against such type of prompt attack?\n\n[1] https://glaze.cs.uchicago.edu/what-is-glaze.html"
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors focus on assessing and challenging the copyright infringement safeguards in commercial T2I systems such as ChatGPT, Copilot, and Gemini. They create a dataset, termed VioT, comprising images of copyrighted content (characters, logos, products, and artworks) and devise an Automated Prompt Generation Pipeline (APGP) that uses language models to generate prompts that circumvent copyright safeguards. The study finds that, despite existing safety measures, models are vulnerable to producing unauthorized reproductions, demonstrating that only ChatGPT consistently blocks such prompts at a rate of 96.25%. The APGP method reduces ChatGPT\u2019s block rate to 6.25%, highlighting the need for more robust protection mechanisms.\n\nThe authors suggest that current defenses, like post-generation filtering and machine unlearning, are inadequate, indicating a critical need for improved defense strategies in T2I models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Copy infringement is an important problem.\n2. VioT dataset: Providing a dataset that the future methods can compare with is useful. 20 images in each of the 4 catergoreis were provided. \n3. Human Evaluation gives the approach credibility. Infact introducing metric for evaluation is also useful."
            },
            "weaknesses": {
                "value": "1. The presentation in the experiment section is not upto par with ICLR. The figures and text should be arranged properly.\n2. The idea is similar to treating VLM and LLM as two agents helping to jailbreak the T2I diffusion model. How is approach different from [1]. \n3. 1. VioT dataset: 20 images in each of the 4 catergoreis were provided. However I feel the number of images is small to text the validity of the approach. \n4. Lack of scoring function ablation details to understand each of its contribution. Why is there a linear addition? Is there no normalization of the values? Such details are very important to understand the scoring function. \n\nFurther details related to weakness are asked using questions below. \n\n\n\n1. Dong, Yingkai, et al. \"Jailbreaking Text-to-Image Models with LLM-Based Agents.\" arXiv preprint arXiv:2408.00523 (2024)."
            },
            "questions": {
                "value": "I really like the idea of using VLM and LLM as two agents to jailbreak T2I. However I have a few questions:\n1. I see that the LLM observes the instructions and score to optimize future instructions. Why is a single scalar score given to LLM. Why is scoring not given separately and the LLM is asked to improve each of the scores. Such analysis and baseline of the scoring function is important. Is there any ablation done for contribution of individual parts of the scoring function and how was the final contribution of each scoring function finally decided? \n2. Why is not a single VLM enough for our approach? What I mean is, what if we provide the score and the system prompt (to make an VLM act like an optimizer) and ask the VLM itself to generate descriptions such that improves the score ? Why is reason for not trying that and introducing a LLM separately ?\n3. Why the gradient based methods to optimizing prompts for jailbreaking LLM have not been tried for T2I models? (It could have helped in developing a better agentic framework)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper shows that commercial Text-to-Image (T2I) systems may be overlooking the risks of copyright infringement, even with basic prompts. While many systems have built-in filters to prevent such violations, the authors\u2019 APGP attack can bypass these protections easily. \n\nThe authors use a new approach with a self-generated QA score and a keyword penalty score in its language model optimizer\u2014 then no need for any complex weight updates or gradient calculations. \n\nTheir tests show that APGP-generated prompts led to copyright issues in 73.3% of cases, even in ChatGPT. Overall, their approach not only makes it easier and cheaper to identify vulnerabilities in T2I models but also helps copyright holders protect their intellectual property more effectively."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper tackles an interesting topic: the jailbreaking of Text-to-Image generative AI systems for copyright infringement. The overall narrative is both meaningful and engaging. The authors also achieved impressive attack results, even against GPT models.\n\nThe authors claim that their approach doesn\u2019t require any weight updates or gradient computations, which I find intriguing. They also designed several loss functions to enhance their attack, and it\u2019s clear that they got solid results with GPT models."
            },
            "weaknesses": {
                "value": "The overall story and topic are definitely interesting, especially given the impressive results the authors achieved. However, the paper itself isn\u2019t well-written; there are too many typos and unclear statements that need to be addressed. For example,\n\n1. Figure 1a doesn\u2019t seem necessary.  \n2. Figure 2 is hard to interpret; it\u2019s unclear why weight updates or gradient updates aren\u2019t needed, and how to update the instructions isn\u2019t explained well.  maybe it's an overclaim because you still need to update the instructions. \n3. There are too many symbols in the equations that aren\u2019t clearly defined. For instance:\n   - What does \"m\" refer to in line 210?\n   - In line 210, \"v\" is used to represent LLM, but then in line 231, \"v\" represents the encoder.\n   - What\u2019s the expression for \\( S_k \\)?\n   - \"f2\" isn\u2019t defined in line 301.\n   - There are reference errors for Figure B.2 in lines 344 and 351, and the image formatting at the bottom of page seven is clearly off.\n   - Additionally, there are reference errors for Table 9 on line 407.\n\nOn top of that, the ablation study in Figure 9 is incomplete. What happens if you remove the two score functions?"
            },
            "questions": {
                "value": "For the unlearning experiments, it seems that the attack is not directly targeting GPT, but rather the diffusion model with unlearning, right?  This may not be very convincing; maybe the unlearning algorithm is just bad on this model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}