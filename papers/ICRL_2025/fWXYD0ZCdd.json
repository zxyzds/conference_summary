{
    "id": "fWXYD0ZCdd",
    "title": "A New Look at Low-Rank Recurrent Neural Networks",
    "abstract": "Low-rank recurrent neural networks (RNNs) have recently gained prominence as a framework for understanding how neural systems solve complex cognitive tasks. However, training and interpreting these networks remains an important open problem. Here we address these challenges by adopting a view of low-rank RNNs as parametrizing a low-dimensional ordinary differential equation (ODE) using a set of nonlinear basis functions. This perspective, which arises from an approach known as the ``neural engineering framework'', reveals that low-rank RNNs are equivalent to neural ODEs with a single hidden layer. We show that training a low-rank RNN to implement a particular dynamical system can thus be formalized as least-squares regression in a random basis. This allows us to propose a new method for finding the smallest RNN capable of implementing a dynamical system using a variant of orthogonal matching pursuit. More generally, our perspective clarifies limits on the expressivity of low-rank RNNs, such as the fact that without inputs, a low-rank RNN with sigmoidal nonlinearity can only implement odd-symmetric functions. We delve further into the role of inputs in shaping network dynamics and show that RNNs can produce identical trajectories using  a wide variety of static or time-varying dynamics; this highlights the importance of perturbations for inferring dynamics from observed neural  trajectories. Finally, we highlight the usefulness of our  framework by comparing to RNNs trained using backprop-through-time on neuroscience-inspired tasks, showcasing that our method achieves faster and more accurate learning with smaller networks than gradient-based training.",
    "keywords": [
        "low-rank rnn",
        "computational neuroscience",
        "dynamical systems",
        "neural dynamics"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "We introduce an approach to train low-rank RNNs to implement a desired dynamical system, clarify limits on expressivity of such models, and offer insights into how input-driven RNNs produce trajectories observed in neuroscience tasks.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=fWXYD0ZCdd",
    "pdf_link": "https://openreview.net/pdf?id=fWXYD0ZCdd",
    "comments": [
        {
            "summary": {
                "value": "This paper advances the understanding of low-rank RNNs by introducing a new theoretical framework, an efficient training approach, and empirical validation. These contributions improve both the interpretability and representational capacity of low-rank RNNs, demonstrating their ability to embed complex nonlinear dynamical systems through a randomized basis. The authors emphasize the importance of inputs in capturing odd-symmetric functions, expanding on prior research regarding the universal function approximation properties of these models. Also, their method enables closed-form parameter learning through regression, which greatly reduces the number of parameters compared to traditional methods. Altogether, the paper positions low-rank RNNs as an efficient and interpretable alternative to conventional training techniques."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**Originality**: The paper introduces a new perspective on low-rank RNNs by modeling them as low-dimensional ODEs using nonlinear basis functions. This approach links low-rank RNNs to the NEF, which has mostly been used for feed-forward networks. The authors present a more interpretable and efficient training method, avoiding the limitations of traditional gradient-based training.\n\n**Significance**: The significance of the paper lies, particularly, in its potential to address the challenges of training and interpreting RNNs."
            },
            "weaknesses": {
                "value": "1) The proposed method is not clear enough. For instance:\n\n- **Rank r**. It is unclear how one can systematically determine the optimal rank r for low-rank RNNs in general. It might be easier achievable for benchmark systems with specific trajectories or dynamics, but it is not clear enough for more complicated cases or real-world datasets with higher dimensions. Could you provide guidelines or heuristics for selecting **the rank r** for different types of problems or datasets?\n\n- **Role of different activation functions**. Given the impact of different activation functions \u03d5 on network performance, what criteria should be used to select the most appropriate activation function for a given task? Are there specific tasks where certain functions consistently outperform others? Could the authors include a comparative analysis of different activation functions on a set of benchmark tasks, showing how performance varies across functions?\n\n- **Probability distribution of the parameters**. Choosing an appropriate probability distribution for the parameters that define the random basis is crucial, as it may directly impact the model's expressivity, learning efficiency, and the stability/convergence of the training process.  Could the authors conduct an ablation study comparing different probability distributions for parameter sampling? Or, is there a data-driven sampling scheme to generate the random basis (i.e., a method for adapting the sampling distribution based on the characteristics of the dataset or task, e.g., similar to https://arxiv.org/abs/2410.23467)?  \n\n2) The paper claims that the proposed method converges faster than traditional BPTT methods, but it lacks a detailed analysis of computational efficiency, including **training time**. Could the authors provide a more detailed analysis of the actual training times across different models and tasks?\n\n3) Regarding the Lorenz attractor, in *Figure SI-3*, the plots do not allow for a clear comparison between the true and fitted (reconstructed test) trajectories. Could you include additional plots for comparing the true and fitted **chaotic trajectories**, as well as the associated **time series** of the x, y, and z components?\n\n4) The paper mentions the **interpretability** of low-rank RNNs but does not delve deeply into it. Could the authors provide more explanations about the interpretability of their method compared to other methods?"
            },
            "questions": {
                "value": "What additional experiments could be conducted to further validate these findings, particularly in real-world applications?\n\nPlease see also \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors make use of Neural Engineering Framework to present a different perspective on how to embed a given dynamical system into a low-rank RNN. They put emphasis on neuron-specific inputs or bias to make the low-rank RNNs universal approximators. The paper claims concerns on the fixed-point analyse framework of RNNs which is one of the mainstream type of reverse-engineering method in computational neuroscience."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The provided perspective of latent space view is beautiful and shows connections between neural ODEs and low-rank RNNs.\n\nThe paper demonstrates clear examples of indistinguishable dynamical systems.\n\nProposed learning algorithm is fast and efficient."
            },
            "weaknesses": {
                "value": "First, the problem that is concerned is to embed a _given_ dynamical system to a low-rank RNN. However, this approach, as also acknowledged by the authors, limits the applications. I believe this restriction is big and should be discussed by authors more. Currently, I do not think it is addressed enough since it is one of the *main* issues of the paper. I think it would be better to discuss this earlier. It is clear that this method cannot be used to find the dynamics required for a novel task --which is arguably, one of the most important use-cases of RNNs. So it can be only used for the tasks that we already know (or at least have hypotheses) its dynamical system. I would expect from authors to have more (and more importantly, distinct) use-cases of their algorithm for science purposes.\n\nSecond, I do not believe comparing your algorithm with BPTT is fair, because you do not provide what BPTT provides (see above). Plus, I couldn't see the details of how you train the two tasks using BPTT. Is there a clear way of embedding a given dynamical system into the RNN using BPTT? If so, I think if you do this to compare your algorithm with BPTT, it would be better. If not, I think this can be put into supplementary results but I don't think it is important.\n\nThird, you state \"A low-rank RNN is therefore not a universal approximator unless it has inputs, or equivalently, different biases or offsets to each neuron.\" I think this is an odd point to make. Because,  1-) An RNN is only a universal approximator when it has inputs or different biases. 2-) A low-rank RNN is strictly a subset of RNN. And so therefore, if an RNN does not have a property under some conditions, and so low-rank RNNs are. So I do not understand the point made by authors. You emphasize this point in your abstract as well, saying, \"our perspective clarifies limits on the expressivity of low-rank RNNs\", which I think is misleading, because it is not a problem of low-rank RNNs but RNNs in general. \n\nFourth, I liked the Figure 4, but I don't think this analyses could only done using your perspective. I think the field already knows the point you make, but you make your point nicely so it is still important. So you say \"Our discussion on non-autonomous dynamics suggests, how similar trajectories can be observed in the presence of dynamical fixed points/ no fixed points at all.\" but does this analysis allowed by your algorithm or is this a distinct point made in your paper besides the provided algorithm? Moreover, you can transform non-autonomous RNN into an autonomous one. This is a very important point which should introduce philosophical questions on your point from the neuroscience point of view. \n\nI think before introducing the Eq 9, you can talk (very briefly) about least-squares regression. \n\nYou put an introduction to low-rank RNNs but not NEF. I think the paper would benefit a lot if you also put emphasis on NEF more (mathematically) to make your connection more clear.\n\nTypos (did not affect my score):\nLine 097, ReLu -> ReLU\nLine 242 phi(x)\nLine 715, \"we thus use present a general\"\nLine 777, \"(A) Bi-Stable Attractor ODE and (B) Line Attractor ODE.\" unusual capital usage\nLine 796, M is m1, m2, m2, and better to put space between them.\nLine 801, \"Following our discussion o the\"\nLine 812, \"Data Simulated from trajectories\", didn't understand the choice of capitals\nWhen you write Lorenz, you should use capital L."
            },
            "questions": {
                "value": "What would be a way of finding the smallest RNN using BPTT? Does your algorithm makes it faster, or it provides a theoretical way of doing it as well, can you put more discussions on this?\n\nYou wrote an algorithm section for finding the smallest RNN, can you also do it for your main algorithm as well? I think this would clarify a lot.\n\nUnder which conditions low-rank RNNs are equivalent to neural-ODEs? If the latent space has noise, are they still equivalent? I would expect a more rigorous way of stating this. \n\nYou say \"(although not shown here) ... fails\", I wondered how does it look like when it fails."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Rank-$R$ RNNs are universal approximations of arbitrary $R$-dimensional dynamical systems that are (typically) trained with back propagation. The NEF conversely approximates dynamical systems with known equations, using RNNs with low-rank structure, by solving a least-squares minimisation problem. In this contribution low-rank RNNs are fitted to known flow-fields by solving a NEF style optimisation problem, with some focus on obtaining networks with small amount of units."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The work aims to connect some existing lines of work. While I am of the opinion that much of this is was introduced in past work (see weaknesses below), writing out connections explicitly can be useful.\n- A new method is introduced for fitting small (in number of units) low-rank RNNs to neural data. \n- The paper reads well and the figures do a good job of illustrating the points made."
            },
            "weaknesses": {
                "value": "My main concern with this paper is that it presents multiple results as new, even though they might not be \u2014 sometimes with statements that appear to be incorrect. Given that the paper has \u201ca new look\u201d in the title, I expected more novel insights. I think some rewriting and rephrasing is important, after which this can paper could turn in a useful and nice overview of recent lines of work with some new ideas.\n\n--------\n\n1. Non-linear recurrent dynamics are commonly used in the NEF\n\n> \u2028(Line 073) while the NEF has been widely explored, most studies have focused on feed forward-networks, comparatively few studies focused on non-linear recurrent dynamics\n\nThis statement seems false, the NEF is used to embed (non-linear) low-D functions in high-D networks, which can be (and are often) straightforwardly made recurrent, for instance when used for integration / memory, or pattern generation [1]. Most of the online tutorials using the python implementation of NEF (nengo) also deal with time-varying dynamical systems, e.g., non-linear oscillators or the chaotic Lorenz attractor: https://www.nengo.ai/nengo/examples.html.\n\n--------\n\n2. On fitting low-rank RNNs to flow-fields with regression\n\n  - 2.1 The NEF can (and is) also straightforwardly used with rate neurons [1], at which point it becomes at least very close to your framework (it solves the same regression problem). While the NEF crowd typically doesn\u2019t explicitly call their models low-rank RNNs, is there any significant difference with your framework besides the naming?\n\n  - 2.2 In Ref [2] low-rank RNNs were also fit with linear regression to known flow-fields (Section 6). While the parameterisation is slightly different, the general idea is very much the same (and there also the connection to NEF was pointed out). \n--------\n\n3. On new insights into universality\n\nRef [2] also shows that low-rank RNNs with inputs / biases are universal approximations (including for exactly the same system as is used in this paper). It also includes many results relating to symmetry when $\\tanh$ is used. \n\n--------\n\n4. On new insight into input driven dynamics\n\nRef [3] Fig. 1, shows very similar example flow-fields to Fig. 4 in this paper, and used them to make the almost the same point about identifiability. What is exactly the new insight here compared to Ref [3]?\n\n--------\n\n5.  On Low-rank RNNs as neural ODEs\n\nThe fact that Eq. 7 is a neural ODE with one hidden layer was explicitly pointed out in previous work (e.g., in the discussion of [4]). The additional insight of the connection to neural ODEs, the literature of which largely focuses on adjoint methods, is in any case also not completely clear to me (some insights by using the adjoint method in relation to low-rank RNNs were obtained in Ref. [5]). I think the main (known) observation to be made here is that Eq. 7 is a universal approximation with one hidden layer.\n\n--------\n\n6. On non-linear oscillators\n\n> (\u2028Line 274) Note that this 2D system is highly nonlinear\u2014unlike recent work focused on oscillations in linear dynamical systems\n\nThis statement is misleading. A quick search should give one many studies that investigated oscillations / limit cycles in non-linear RNNs (including low-rank ones!). Ref [2] also derived a limit cycle oscillator in low-rank RNNs.\n\n\n--------\n\nRefs\n\n[1] Stewart, 2012. A Technical Overview of the Neural Engineering Framework\n\n[2] Beiran et al., 2021. Shaping Dynamics With Multiple Populations in Low-Rank Recurrent Networks\n\n[3] Galgali et al., 2023. Residual dynamics resolves recurrent contributions to neural computation\n\n[4] Pals et al., 2024. Inferring stochastic low-rank recurrent neural networks from neural data\n\n[5] Pellegrino et al., 2023. Low Tensor Rank Learning of Neural Dynamics"
            },
            "questions": {
                "value": "1. Is the introduced method for finding small RNNs guaranteed to converge to the smallest RNN? Or can it converge to a local optimum?\n\n--------\n\n2. \n> (Line 277) This target ODE is not radially odd-symmetric, so once again (although not shown here), embedding the system in a low-rank RNN fails if we do not include inputs.\n\n\u2028It is unclear to me how to reconcile the statement with ref [2], where a rank-2 RNN with $\\tanh$ units (without biases) was derived such that it implements a similar limit cycle. \u2028One answer could it be that (unlike stated in the text) the system actually is odd-symmetric? \u2028From a quick try, converting your system to back to cartesian coordinates using $z_1 = r\\cos(\\theta)$, $z_2 = r\\sin(\\theta)$, I get: $\\dot{z}_1 = az_1-z_2$, $\\dot{z}_2 = az_2+z_1$, with $a=\\frac{1-r^2}{r}$, which satisfies $F(-z_1,-z_2) = - F(z_1,z_2)$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a novel perspective by viewing low-rank RNNs as parameterized ordinary differential equations (ODEs) with nonlinear basis functions. Concretely, given a generic RNN $\\dot{x} =  -x +J\\phi(x) + I u(t)$ and $z = W\\phi(x)$ and assuming that $rank(J) = dim(z)$ (i.e. a low-rank RNN for low dim. output $J=MN^T$).  These dynamics can be equivalently represented as a neural ODE with a single hidden layer (of $dim(x)$) and output dynamics $\\dot{z} = -z + n^T \\phi(Mz_t + Iv_t)$. The authors then consider the problem to fit $\\dot{z} = g(z)$ to a *known* output dynamics $g(z)$. This view allows low-rank RNNs to be fitted using generalized linear regression on $n$ given a fixed set of nonlinear basis functions defined through $M$ and inputs $I,v$. The ability to fit certain dynamics is then limited by the expressivity of the basis functions. The authors propose a greedy method to select a suitable basis based on an orthogonal matching pursuit framework (from a predefined set of random basis functions obtained through randomly sampling M,I)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-written, sound, and easy to follow. The authors provide examples of how this interpretation/method can be used to, e.g., analyze the expressivity of low-rank RNNs and other properties. The proposed method is efficient and easy to apply to given dynamics, and the paper effectively demonstrates the utility of generating new insights on the expressivity of low-rank RNNs."
            },
            "weaknesses": {
                "value": "- **Training RNNS**: The manuscript claims to \"address the issues of RNN training,\" but the method effectively only \"maps\" known low dimensional dynamics to high dimensional recurrent neural activity. These are different problems, aren't they? (If I know the low dimensional dynamics, why would I ever use BPTT to embed it into higher dimensional space?). I find the comparison to Backpropagation Through Time (BPTT) somewhat strange. RNNs trained with BPTT effectively address a different problem: learning dynamics from data points. (at least, I assume this is how the RNNs in Fig 5 are trained, as no details are provided). Isn't it expected that they perform worse in your examples because they have to figure out the right dynamics from the data, while your method just fits to the known \"true\" dynamics? \n\n- **Novelty**: Both the neural ODE interpretation and fitting method using linear regression are not new (e.g., see Beiran et al. 2021, which also uses least squares regression to fit RNNs to known dynamics). While the authors do cite this related literature, these methodological similarities should be more clearly stated in the manuscript.\n\n- **Limitations**: The method does require a known dynamical system to fit i.e. $g(z)$. This is a strong assumption and limits the applicability of the method.  I am unfamiliar with the neuroscience literature, but this is rarely the case, no? The authors should clearly discuss this limitation in more detail (earlier in the manuscript) and add some motivation on relevance (add some more details about the NFE?)."
            },
            "questions": {
                "value": "- Can the authors provide motivation or introduction to the NFE framework?\n- Eq. 11 is missing the residual, no?\n- Regarding the identified weaknesses, I am particularly concerned about how RNN training is framed in the current manuscript, given that the approach appears to have limited applicability for training and is mainly suited for analysis (see weaknesses).\n\nOverall, I hence tend to reject this paper. Given that I am not an expert on the relevance of this contribution to the neuroscience topic, I will only vote for a weak rejection."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}