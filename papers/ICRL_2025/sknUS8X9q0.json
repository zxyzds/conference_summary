{
    "id": "sknUS8X9q0",
    "title": "SAGE: Scalable Ground Truth Evaluations for Large Sparse Autoencoders",
    "abstract": "A key challenge in interpretability is to decompose model activations into meaningful features. Sparse autoencoders (SAEs) have emerged as a promising tool for this task. However, a central problem in evaluating the quality of SAEs is the absence of ground truth features to serve as an evaluation gold standard. Current evaluation methods for SAEs are therefore confronted with a significant trade-off: SAEs can either leverage toy models or other proxies with predefined ground truth features; or they use extensive prior knowledge of realistic task circuits. The former limits the generalizability of the evaluation results, while the latter limits the range of models and tasks that can be used for evaluations. We introduce SAGE: Scalable Autoencoder Ground-truth Evaluation, an evaluation framework for SAEs that enables obtaining high-quality feature dictionaries for diverse tasks and feature distributions without relying on prior knowledge. Specifically, we lift previous limitations by showing that ground truth evaluations on realistic tasks can be automated and scaled. First, we show that we can automatically identify the cross-sections in the model where task-specific features are active. Second, we demonstrate that we can then compute the ground truth features at these cross-sections. Third, we introduce a novel reconstruction method which significantly reduces the amount of trained SAEs needed for the evaluation. This addresses scalability limitations in prior work and significantly simplifies the practical evaluations. We validate our results by evaluating SAEs on novel tasks on Pythia70M, GPT-2 Small, and Gemma-2-2B, thus demonstrating the scalability of our method to state-of-the-art open-source frontier models. These advancements pave the way for generalizable, large-scale evaluations of SAEs in interpretability research.",
    "keywords": [
        "Mechanistic interpretability",
        "Large language models",
        "Sparse autoencoders",
        "Sparse dictionary learning",
        "Unsupervised learning",
        "Interpretable AI"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "This paper introduces SAGE, a framework to scale realistic ground truth evaluations of sparse autoencoders to large, state-of-the-art language models such as Gemma-2-2B.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=sknUS8X9q0",
    "pdf_link": "https://openreview.net/pdf?id=sknUS8X9q0",
    "comments": [
        {
            "summary": {
                "value": "This work tries to scale up the feature evaluation problem for large sparse autoencoders (SAEs) in large language models, towards interpretable AI. The main contribution of the work is a new reconstruction approach for task-specific features with large training reduction of the SAEs. The overall method and experimental results look fine."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Studying intrinsic features that are interpretable in large language models is an interesting and important direction.\nThe  large sparse autoencoders (SAEs) and indirect object identification (IOI) look promising. This is a relatively new direction and the proposed method and findings make a good attempt in this direction. The proposed reconstruction method seems to be sound."
            },
            "weaknesses": {
                "value": "This is a paper difficult to read. It is mainly based on two prior works, \n\n[1] . \"Interpretability in the wild: a circuit for indirect object identification in GPT-2 small\", Wang et al. 2023, \nwhich defines the problem of indirect object identification (IOI) with a discovery of circuit; and\n\n[2]  \"Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control\", Makelov et al. \nthat gives a framework for supervised learning of sparse autoencoders (SAEs) for the indirect object identification (IOI) task.\n\nI am not very familiar with the IOI and SAE literature, which seems to be a recently proposed problem in large language models. [1] and [2] were written in a relatively clean and clear way for readers to quickly understand the motivation and problem definition.\n\nHowever, the paper in study is much harder to read than [1] and [2]. This may be also due to my unfamiliarity to this direction. This paper seems to make an assumption of having the readers with all the prior knowledge about IOI and SAEs by quickly throwing out the jargon words without much explanation."
            },
            "questions": {
                "value": "My main concern regarding this paper is its writing and presentation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Authors consider the question of evaluating sparse auto-encoder\nmodels, which are studied for interpreting large scale models. They\nstart from a recent article and build on top of it. The previous\narticle already proposed a strategy for evaluating SAE models using\nsupervised dictionary features, extracted for a given task and points\n(cross-sections) in a language model. Here, this work is extended by\nfirst identifying the cross-section automatically and then using a\ndifferent training strategy for the SAEs. This way, authors claim that\nthe evaluation can be done for large-scale LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ Interpretation of components in LLMs is a highly relevant topic.\n+ SAEs is an interesting direction that recently attracted attention.\n+ Automated identification of cross-sections is a step forward. The\n  previous work relied on prior knowledge and thus was limited on what\n  it could be applied. Using automated discovery of cross-sections,\n  this limitation is alleviated."
            },
            "weaknesses": {
                "value": "- overall, I think the problem definition and the description can be\n  better written. Author rely heavily on the previous work, i.e.,\n  Makelov et al., for their description. However, the current article\n  on its own is not trivial to understand.\n- It is unclear how the supervised dictionaries are extracted in this\n  work and whether this is something new. Makelov already discusses in\n  appendix A.6 the MSE estimation of the supervised\n  dictionaries. Furthermore, in that model $u$ vectors are extracted\n  using MSE. Here, it is unclear how $u$ vectors are\n  extracted. Equation 5 and 6 only describes the weighting of the\n  vectors.\n- Projection-based reconstruction with residual streams is not really\n  justified in my opinion. More specifically, this section makes me\n  question the goal of this work. In Makelov et al.'s work, authors\n  aim to evaluate SAEs, thus the cost of training SAEs is not really\n  an issue. If the goal here is the same, why is the training of SAEs\n  all of a sudden an issue? I am not sure if the motivation for\n  reducing the training cost and the development for the residual\n  stream is clear at all.\n- Certain parts of the article are put in the Appendix, e.g.,\n  A.5. This makes me wonder whether given the page limitations of\n  conferences, a journal would be a better venue for this work.\n- Discovery of the cross-sections is the main novelty in my opinion,\n  however, this is also a part that is not trivial to evaluate and\n  assess the accuracy. Authors should perhaps discuss this a bit\n  further. Is there a way to really show that this is indeed a good\n  method? \n\nMinor:\n- the last $x_{corr}$ on line 264 is most likely $x_{clean}$."
            },
            "questions": {
                "value": "- I suggest improving the writing, especially in the method section. \n- The motivation for accelerating training of SAEs need to be better\n  justified.\n- How can one assess the accuracy of the cross-section discovery?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an evaluation framework to evaluate sparse autoencoders called SAGE. The approach consists of attribute cross-sections, supervised feature-dictionary reconstruction and projection-based reconstruction.  The framework is evaluated on novel tasks on Pythia 70M, GPT-2 small and Gemma-2-2B."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is dealing with a problem that Is of interest to the ICLR community."
            },
            "weaknesses": {
                "value": "The main weakness of this paper Is it\u2019s presentation, it is very difficult to get through the manuscript and understand the introduced evaluation framework. One would expect to have a clear answer to the following three questions after reading the abstract and the intro: (1) what is done, (2) why it is done, and (3) how it is done. However, after reading the paper multiple times, the reviewer still struggles to find the responses to the above-mentioned questions.  For details, please see Questions box. \n\nThe current paper presentation makes it difficult to assess the papers\u2019 novelty, impact and significance of the results."
            },
            "questions": {
                "value": "Abstract has some contradictions in it, it seems that the paper is about an evaluation framework, however, at the same time it seems like there are some model improvements, e.g., \u2018introduce a novel reconstruction method\u2019. \n\nAbstract. The validation is also unclear, the authors say that they validate the results on  novel tasks on 3 datasets, however, it is unclear what is the conclusion of these validations. Could the authors clarify this?\n\nCould the authors clarify the novelty of the introduced approach?\n\nIntro 2nd paragraph \u2013 the connection between the interpretability of LLMs and SAEs is not clear \u2013 could the authors clarify the connection?\n\nIntro 3rd paragraph \u2013 it is not clear what circuit discovery has to do with evaluation of SAE, could the authors clarify this?\n\nIntro: Could you please clarify what is meant by large frontier model?\n\nIntro: What the authors mean by linearly represented features?\n\nIntro: The use of manual experimentation is unclear. \n\nIntro: What the authors mean by sublayer activations and residual stream SAE?\n\nPlease make all the legends in all figures bigger."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a scheme to automate SAE evaluations for large models/complex tasks and for situations where ground truth features are not known.\n\nThere are several claimed contributions:\n\nA novel (as far as I can tell) approach to identify cross sections by leveraging clean and corrupted examples w.r.t the attribute of interest.\n\n\u201cA novel projection-based reconstruction using residual stream SAE\u201d\n\nA modification to the supervised feature dictionary approach of Makelov (2024) in which they instead \u201cpropose to compute weights to minimize the MSE loss between the reconstruction and the activation:\u201d\n\nTaken together, these innovations allow SAE evaluations to be \u201cscaled,\u201d apparently meaning w.r.t tasks with a lack of ground truth features or increased model size."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The chosen task is important and highly relevant."
            },
            "weaknesses": {
                "value": "Philosophically, I don\u2019t understand the point of this paper. If I am reading it correctly, and it\u2019s possible I am not, the idea is to generate pseudo-ground truth features via \u201csupervised feature dictionary learning\u201d for cross-sections corresponding to features of interest (which are also identified automatically). Then it is claimed these features can be used to evaluate SAEs. But this seems to me to defy the whole point of SAE evaluation in the first place, which is to isolate network components which are interpretable to humans. \n\nThe presentation of the method is also generally pretty confusing because it\u2019s entangled with the presentation of background and related work. I also feel like it\u2019s too abstract and needs a lot more concrete examples and figures to be grounded effectively. \n\nThe experimental results are hard to read, both due to the small font and the chart-style presentation."
            },
            "questions": {
                "value": "It would be great if you could address weakness 1. I also think the presentation of the method and results should be made much less abstract, i.e. with some concrete examples."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}