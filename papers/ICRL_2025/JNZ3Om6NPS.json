{
    "id": "JNZ3Om6NPS",
    "title": "On inherent limitations of GPT/LLM \\\\ Architecture",
    "abstract": "This paper shows that reasoning/proving issues of $GPT/LLM$ are an inherent logical consequence of the architecture. Namely, they are due to a schema of its prediction mechanism of the next token in a sequence, and randomization involved in the process.\n \nAfter the natural formalization of the problem into a domain of finite graphs,  $G({\\omega})$, we prove the following general theorem:\n\nFor almost all proofs, any learning algorithm of inference, that uses randomization in $G({\\omega})$, and necessitates veracity of inference, is almost surely a literal learning.\n\nIn the context, \"literal learning\" stands for one which is either vacuous, i.e. $\\forall x~[P(x) \\implies Q(x)]$ where $P(x)$ is false for every $x$, or create a random inference from a false assumption (hallucination), or it essentially memorizes the inferences from training/synthetic data.\n\nA few corollaries follow. For instance, if its formulation is somewhat original, it is easy to notice the issue of solving mathematical problems with $LLMs$ in the case of even low-complexity tasks.  Since its solution is unlikely to be found in a holistic form in a training dataset, a correct proof is not to be expected. \n\nIt is because, in a rigorous context, $GPT$ has exponentially decreasing odds of finding a valid proof of the result unless it simply \u201crepeats\u201d a known proof, perhaps with trivial modifications. Another observation is that the degradation has an exponential rate by the length of a proof. In other words, an attempt to prove a complex enough statement virtually has no chance to be \nsuccessful.\n\nIn a novel rigorous context (i.e., when $GPT$-based architecture is looking to prove a new result, for instance, a hypothesis), that is virtually impossible even for a long enough fragment. The probability of success becomes infinitesimal quickly for either a fragment of possible proof or a weaker non-trivial statement. That also was empirically shown for data mixtures and confirmed experimentally.",
    "keywords": [
        "0-1 laws",
        "first-order logic",
        "probabilistic spaces",
        "finite graphs"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We prove, using the first-order logic, that GPT/LLM architecture has inherent limitations in theorem proving and problem solving",
    "creation_date": "2024-09-17",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=JNZ3Om6NPS",
    "pdf_link": "https://openreview.net/pdf?id=JNZ3Om6NPS",
    "comments": [
        {
            "summary": {
                "value": "The paper studies the reason why today\u2019s LLM models struggle with multi-hop reasoning tasks and produce hallucinations in the process. It presents an argument for why the fundamental nature of inference on the transformer architecture prevents them from being able to generate correct mathematical proofs or output false statements in other settings which require rigor.\nTo do so, the paper considers the setting of first-order predicate statements on random graphs.\nUnder this model, the paper claims that a GPT model has exponentially decreasing probability of finding the correct proof unless it has already seen the proof in the train set."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The paper aims to understand fundamental limitations of autoregressive next-token predictors which is an important problem today for ML."
            },
            "weaknesses": {
                "value": "-\tThe presentation in the paper severely lacks a level of formalism necessary a technical conference. The writing is at times vague and this makes it hard to verify or critique the main Theorems and Corollaries presented. Some of the issues with the formalism are listed in the questions section below.\n-\tTheorem 1 seems to be wrong (?) It is well-known that GPT/LLM models can generalize and produce novel proofs so the statement that for almost all proofs, the model is performing literal learning appears to be incorrect?\n-\tThe paper relies heavily on \u201crandomization in inference\u201d of GPT/LLM models. This randomization is not clearly quantified or defined anywhere which makes it hard to follow the argument.\n-\tMoreover, there are many post-training techniques one could employ to increase the chances of producing correct output. The paper\u2019s claim seems to potentially hold only in the limit where the size of reasoning chains tend to infinity and we wish to be correct over all possible novel statements on the graph."
            },
            "questions": {
                "value": "1. If auto-regressive next-token predictors are universal (Malach 2023),  how do you re-concile the limitation supposedly proved in this work?\n\n\nIssues with formalism:\n1. What is a GPT model? It needs to be formally defined.\n2. Where is Lemma 0? Why is it in the Appendix and not referred to in line 150?\n3. In line 150, the appropriate result which allows us to conclude that the limiting probability is either 0 or 1 (Fagin\u2019s 0-1 law) needs to be cited.\n4. How are we excluding the limiting probability being 0 without having described what G(w) is?\n5. What does it mean that \u201cour inference follows a literal graph representation from the original\u201d and why is it implied in line 154?\n6. What is p in line 155?\n7. What is your formal definition of \u201clearning model for inference\u201d?\n8. Do the limitations proposed go away under greedy decoding rather than temperature based sampling?\n9. In Lemma 1, what is the definition of a \u201cfault\u201d in G(w)?\n10. It is not made clear where exactly the randomness in GPT generations is being used. Note that randomness alone doesn\u2019t mean a non-zero probability of an error in a single step. We can have a probability distribution that always assigns probabilities of 0 or 1 for different tokens.\n\nTypos/formatting errors:\n- Lines 68-73 the paragraph is repeated"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the reasoning abilities of large language models (LLMs), examining their potential limitations in producing truly novel and rigorous proofs. The authors argue that while these models can mimic logical structures and provide plausible responses, they lack the capacity for original, rigorous proof generation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The problem of assessing reasoning capabilities of LLMs versus memorization is very relevant to the current machine learning community."
            },
            "weaknesses": {
                "value": "- The paper seems to be not polished: in the introduction there is a repeated paragraph (lines 69-73), several concepts (e.g. literal learning, first order model graph, generic 0-1 law, hallucination) should be defined formally or explained better, there is no literature review paragraph, the flow is hard to follow. Section 2.1 cannot be followed without reading the appendix, a re-organization of the flow would be helpful.\n- The formatting in Section 2.1 is not clear: Q.E.D in line 171 does not seem to be the end of the proof of Theorem 1, and it is not mentioned where the proof of Theorem 1 can be found. Same for lines 176 and 191. In Corollary 3 it is not clear where the statement ends. \n- Because the lack of formalism in the presentation, the results seem unconvincing in this version of the paper."
            },
            "questions": {
                "value": "- You denote a finite graph $G(\\omega)$, what is $\\omega$ exactly?\n- what is the difference between $C(e_0, e_t) $ and $e_0 \\sim e_t$?\n- Line 150: why do we have only these 2 possibilities for the probability of $G$ satisfying property $A$?\n- Line 153: it is not clear where to find Lemma 0. A pointer to the exact subsection of the appendix would be useful, or you can move it to the main (the main is shorter than the page limit).\n- Line 155: what is p? Line 157: what are $\\psi_i$?\n- It is not clear where the proofs of the results in Section 2.1 can be found.\n- Section 2.2.1: Can you explain more in details the experiments that you run (e.g. prompts, models used, number of experiments for each model-prompt, answers received, metric to validate correctness of answers).\n- Line 227: `similar results were obtained for other chatbots, e.g. Perplexity': can you be more specific on those?\n- Line 229: Can you explain better why the results from Trinh et al.,2024 on geometry problems are not a contraddiction to your framework?\n- Can you explain the code in page 6?\n- In the introduction and abstract the authors mention that the inability of LLMs of producing original proofs follows from their architecture. However, I do not see any discussion of e.g. the self-attention mechanism in the main contributions. Could you please expand on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The main manifest of the paper is that LLM such as GPT are inherently incapable in the problem of producing rigorous proofs, build on chain of reasoning, unless such a proof has been exactly observed in the training set. It is also mentioned that this fundamental impossibility is due to the attention mechanism, and/or due to the finite-temperature prediction of the architecture, meaning that the next output token is not always the most likely token. \n\nThe proof is based on ideas from first-order logic, which I am not familiar with, but if I understand correctly, goes something like this: Since the proof requires a chain of reasoning comprised of multiple connections, and since each such connection there is a probability that it will not be sampled correctly, then overall the probability of the LLM to output the correct proof decays exponentially with the length."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1) The question whether LLMs can produce rigorous proofs or not is fascinating, and at the center of scientific interest. \n2) The logic-based model is potentially an interesting model to address such questions."
            },
            "weaknesses": {
                "value": "1) There is an empirical evidence that LLMs have some capability in providing rigorous proofs, even if currently a weak one. The paper does not seem to address this evidence too much (except the Trinh et al 2024 paper on line 229), and explain why its reasoning is not applicable in those cases. Much more detailed comparison is required. \n\n2) Section 2.1 presents the first order-logic model, and the main theorems. I am not familiar with first-order logic (as said), but this model appear to be general, and not tightly connected to neither any specific machine learning algorithm, nor LLMs or GPT. Theorem 1 appears colloquial \u2013 what does it rigorously mean \u201cnecessitates veracity of inference\u201d? How is \u201cliteral learning\u201d is defined? To establish the statement targeted by the authors, I think that a much more detailed model is required. Afterwards, there are examples of inference problems the are not solvable by LLMs. These are just mentioned without any justification. \n\n2) Experiments are lacking: Section 2.2 pretenses a couple of \u201cquestion and answer\u201d examples, with a few models, and it is claim that \u201cCalude was the most pleasant and sensible\u201d. This part is not scientific. These are just simple experimentation withe models and general impression. \n\n3) Section 2.3: It is impossible for me to understand this example from the displayed code.\n\n4) The concluding paragraph following \u2013 \u201cThere is a widespread belief that because the training set\u201d \u2013 reads more like a personal opinion rather than a solid conclusion from the paper. \n\n5) Proofs: These are difficult to follow because there is no methodological structure of accurate definitions of terms, follows by a clear definition of the probabilistic model, before the proofs. For a quick read, there is no technical challenge in the proofs, and these are just a description in mathematical terms of the ideas that explained in the paper in words (except for the use of Borell-Cantelli lemmas to prove 0-1 laws)\n\n6) While it is occasionally mentioned that the attention mechanism of GPT is an inherent cause for its impossibility to provide novel rigorous proofs, this is not actually discussed in the paper."
            },
            "questions": {
                "value": "Here are a few minor and representation comments:\n\n1) The paragraphs staring in line 69 almost repeats the one starting in line 64.\n2) The sentence \u201cThe logical view approach is more effective in generalization by varying a domain obeying the generic 0-1 law.\u201d is an example to a vague sentence.\n3) Before Equation (1), \u201cand $e_t$ respectively\u201d \u2013 something is missing here. \n4) In line 155, what is $p$ ? It is never defined.\n5) Line 189 \u2013 \u201cGeneralization of the main result Theorem 5\u201d \u2013 where is Theorem 5 ? There are many such inconsistencies. \n6) Line 389 \u2013 Isn't it the other way round between possibilities 1 and 2 \u2013 the first is not a novel context and the second is ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents theoretical results demonstrating the limitations of GPT/LLM architecture for reasoning."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "It is a nice topic."
            },
            "weaknesses": {
                "value": "The presentation e.g. the theorem and proof on lines 167-171 is unconvincing."
            },
            "questions": {
                "value": "Is this a draft? It seems like the presentation is strange, at the very least quite unfamiliar in style to me."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}