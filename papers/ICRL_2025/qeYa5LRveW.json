{
    "id": "qeYa5LRveW",
    "title": "ACER: Automatic Language Model Context Extension via Retrieval",
    "abstract": "Long-context modeling is one of the critical capabilities of language AI for digesting and reasoning over complex information pieces. In practice, long-context capabilities are typically built into a pre-trained language model (LM) through a carefully designed context extension stage, with the goal of producing generalist long-context capabilities. In our preliminary experiments, however, we discovered that the current open-weight generalist long-context models are still lacking in practical long-context processing tasks. While this means perfectly effective long-context modeling demands task-specific data, the cost can be prohibitive. In this paper, we draw inspiration from how humans process a large body of information: a lossy retrieval stage ranks a large set of documents while the reader ends up reading deeply only the top candidates. We build an automatic data synthesis pipeline that mimics this process using short-context LMs. The short-context LMs are further tuned using these self-generated data to obtain task-specific long-context capabilities. Similar to how pre-training learns from imperfect data, we hypothesize and further demonstrate that the short-context model can bootstrap over the synthetic data, outperforming not only long-context generalist models but also the retrieval and read pipeline used to synthesize the training data.",
    "keywords": [
        "language",
        "retrieval",
        "long context understanding",
        "text generation"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=qeYa5LRveW",
    "pdf_link": "https://openreview.net/pdf?id=qeYa5LRveW",
    "comments": [
        {
            "summary": {
                "value": "This paper explores the challenge of building language models with strong long-context processing capabilities, which are essential for tasks that require digesting and reasoning over complex information. The authors note that current generalist long-context models still fall short in practical applications, and that building task-specific long-context models can be prohibitively costly. To address this, the paper proposes an approach inspired by how humans process information - using a short-context language model to perform a lossy retrieval and ranking of a large document set, then fine-tuning that short-context model on the synthetically generated data to obtain task-specific long-context capabilities."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The topic of long-context modeling is both compelling and critical, and this paper provides valuable new insights into addressing this task.\n\n- The proposed method is well-conceived and alleviates the need for extensive resources for human-annotated data. \n\n- The approach demonstrates the potential for practical application, making it a meaningful contribution to long-context modeling research."
            },
            "weaknesses": {
                "value": "- My primary concern with this paper is the limited evaluation. The experiments provide only a narrow comparison of long-context benchmarks, such as Infinibench [1] and LongBench [2]. Additionally, the paper misses several important approaches mentioned in paper [3] such as self-extend [4] and lm-infinite [5], but lacks a comparative analysis or discussion of these methods, which would strengthen the evaluation.\n\n- There is also a lack of case studies and in-depth analysis of the model\u2019s long-context modeling capabilities. Providing examples on synthetic data or offering a more detailed examination of attention patterns would help demonstrate that the model genuinely improves long-context handling, beyond the benchmark performance alone.\n\nReferences:\n\n[1] InfiniBench: Extending Long Context Evaluation Beyond 100K Tokens: Extending Long Context Evaluation Beyond 100K Tokens\n\n[2] Longbench: A bilingual, multitask benchmark for long context understanding\n\n[3] A controlled study on long context extension and generalization in llms\n\n[4] Llm maybe longlm: Self-extend llm context window without tuning\n\n[5] Lm-infinite: Simple on-the-fly length generalization for large language models"
            },
            "questions": {
                "value": "- The capabilities of Llama-3-8b-it can sometimes be limited, and it may not consistently follow human instructions well in certain cases. How does this limitation impact the performance of your proposed method? Would using a larger backbone model (e.g., Llama-3.1-70b-it) provide greater benefits for the proposed approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the limitations of current open-weight generalist long-context models in practical long-context processing tasks and proposes an innovative approach inspired by human information processing. By developing an automatic data synthesis pipeline that uses short-context language models to mimic human document retrieval and reading processes, the authors aim to enhance task-specific long-context capabilities. The study demonstrates that short-context LMs, when further tuned with self-generated data, can surpass both long-context generalist models and the foundational retrieval-read pipeline in effectiveness for tasks such as long-context retrieval augmented generation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-written.\n2. The methodology is clear and effective."
            },
            "weaknesses": {
                "value": "1. The evaluation was conducted solely on long-context RAG tasks, where improvement is natural given the methodology. However, it was not assessed on more general long-context evaluation sets, such as LV-Eval and Needle in a Haystack.\n2. The approach seems to be too simplistic and straightforward, lacking innovation and contribution.\n3. Experiments are conducted on only one size and one type of language model."
            },
            "questions": {
                "value": "Please kindly address the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new method for extending the long-context capabilities of language models, called ACER (Automatic Context Extension via Retrieval). ACER enhances the long-context processing abilities of a short-context language model through a two-stage automated process.\n\nExperimental results show that ACER outperforms other mainstream long-context models across multiple tasks and datasets, particularly excelling in long-context Retrieval Augmented Generation (RAG) tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "An effective pipeline for automatically generating RAG training data, achieving significant performance improvements even with just an 8B model as a data generator. The scores for downstream RAG tasks are also impressive."
            },
            "weaknesses": {
                "value": "1.\tMost comparisons in the experiments are with Long-Context Models. I believe additional RAG strategies should be included for comparison.\n\n2.\tIt would be helpful to see some comparative data statistics, such as a table showing Big Context length and CoT Answer length from Figure 1, as well as length comparisons in the experimental section.\n\n3.\tFigure 1 needs a higher-resolution version. Using a PDF image is recommended."
            },
            "questions": {
                "value": "Please check weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel two-stage approach for enhancing the long-context capabilities of large language models without the need for human-generated supervision data. The first stage involves automatic data synthesis where a retrieval model and a short-context generator work in tandem to produce synthetic long-context data. The second stage is a fine-tuning process where a large LM is trained on the synthetic data to acquire long-context understanding abilities. The paper presents experiments on long-context retrieval augmented generation and long-context reading comprehension tasks, demonstrating the effectiveness of ACER."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(1) The paper presents an effective approach to extend the capabilities of LMs into longer contexts by leveraging retrieval mechanisms and synthetic data generation. \n(2) The experimental results are positive, with ACER being tested on standard benchmarks like Natural Questions (NQ), TriviaQA (TQA), and NarrativeQA. The paper also provides a detailed comparison with other models, which strengthens the quality of the evaluation.\n(3) The paper is well-structured and easy to follow."
            },
            "weaknesses": {
                "value": "(1) While the paper demonstrates the effectiveness of ACER on certain tasks, it does not explore the generalizability of the approach across different domains or longcontext evaluations. Additional experiments in this regard could strengthen the paper's claims.\n(2) The idea is valid but lacks novelty. It is more like an engineering trick than a research idea.\n(3) The article lacks some more in-depth analysis and ablation on experimental settings, such as the impact of chunk size and the potential relationship with context length."
            },
            "questions": {
                "value": "\uff081\uff09Chunking strategies in long documents can disrupt the contextual flow and background information, resulting in some chunks containing incomplete data. This fragmentation may prompt LLMs to reference unrelated contexts or revert to their internal, parameterized knowledge, which can lead to inaccurate responses. How should ACER deal with this problem?\n\uff082\uff09Different question may need different (M) ranker text chunks, how can we set up a M for different problems?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}