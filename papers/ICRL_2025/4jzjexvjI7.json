{
    "id": "4jzjexvjI7",
    "title": "Regret measure in continuous time limit for a stochastic Multi-armed bandit problem",
    "abstract": "We study a class of stochastic multi-armed bandit problems with a risk-sensitive regret measure within a continuous limit setting. This problem is interesting when optimizing the expected reward is not the foremost objective, and the problem horizon is long. Through scaling the state parameters, including the number of pulls and cumulative reward for each arm we study the  bandit problem with infinite horizon, we delineate such risk using a Hamilton-Jacobi-Bellman equation with quadratic growth. Using this approach, we establish an explicit form of the optimal policy associated with the considered risk. As an application, we present examples where the results obtained in continuous time offer insights into the optimal policy for each case. Finally, numerical experiments confirm the theoretical results are presented.",
    "keywords": [
        "Stochastic multi-armed bandit",
        "Risk-sensitive regret",
        "Hamilton-Jacobi-Bellman equation",
        "Continuous time-limit"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4jzjexvjI7",
    "pdf_link": "https://openreview.net/pdf?id=4jzjexvjI7",
    "comments": [
        {
            "summary": {
                "value": "This paper aims to analyze multi-armed bandit problems using differential equations and introduces a new risk measure for the analysis."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "I am unable to provide a comprehensive scientific review of the paper, and thus I cannot identify specific strengths. Please refer to the weaknesses  below."
            },
            "weaknesses": {
                "value": "The paper has significant issues with presentation. Not only are there numerous grammatical errors, typos, and punctuation mistakes, but many sentences are incomplete and seem disconnected from the surrounding context. Additionally, the writing lacks a clear logical flow, making it difficult to follow the argument.\n\nFurthermore, it appears that the authors have not adhered to the official ICLR style guidelines.\n\nDue to these issues, I am unable to provide a more detailed review."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies a class of stochastic multi-armed bandit problems with a risk-sensitive regret measure within a continuous limit setting"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Considering continuous-time limit of regret measures in continuous time."
            },
            "weaknesses": {
                "value": "The presentation is not clear. \n\nThe paper's contribution and the significance of the problem are not clearly articulated in the Introduction and the main text. \n\nThe English in the paper could benefit from some further refinement or editing to enhance clarity and coherence."
            },
            "questions": {
                "value": "1. what is the main contribution of the paper?\n\n2. what is exactly the problem studied?\n\n3. why studying the continuous-time limit is relevant for bandit problems?\n\n4. How should we interpret the main result Theorem 1 and understand its practical relevance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper considers the traditional multi-armed bandit problem with a new risk measure. The authors continuize the time through rescaling and use PDE to find the optimal policy. In the meantime, the authors use some simulations to verify their results."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The way to convert the MAB problem to a PDE problem is interesting and meaningful. The work compares different concepts, like frequentist and Bayesian settings making it easy to understand the applicability of the method."
            },
            "weaknesses": {
                "value": "1. The writing needs to be improved. There are a lot of typos which make it hard to understand the paper. \n\n2. There are no real-world applications provided by the author regarding why this new risk measure is important, reducing the credibility and impact of the paper.\n\n3. The usage of MDP seems improper. In your setting, $\\nu$ seems to be fixed and only $s$ and $q$ are changing. However, there is no need to learn the transition kernel as if you choose an action $a$, corresponding $q$ will be increased by 1. Then, it reduces to learning the reward function which is the same as in traditional MAB literature and so people usually don't call it MDP. It's more reasonable to use your framework to consider the case that $\\nu$ is varying and say it's MDP.\n\n4. The notations are messy. For example, why $V_{i+1}$ only relies on $R_i$? And you use a very strong assumption but only hide it in the Lemma 1.\n\n5. The Theorem 1 is unclear. What is zero? Why do you use a bracket but link it to nothing?\n\n6. In your numerical study, how do you implement UCB and TS? Do you adjust their definitions of regrets to your new risk measure? If not, they are not comparable. Otherwise, it's better to mention how you set the baseline in detail."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}