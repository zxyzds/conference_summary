{
    "id": "0Yfjerm9Zp",
    "title": "Enhancing LLM Faithfulness in Rationale Generation via Dual-Reward Probabilistic Inference",
    "abstract": "As large language models (LLMs) are increasingly applied to complex reasoning tasks, achieving both accurate task performance and faithful explanations becomes crucial. However, LLMs often generate unfaithful explanations, partly because they do not consistently adhere closely to the provided context. Existing approaches address this problem either rely on superficial calibration, such as decomposed Chain-of-Thought prompting, or require costly retraining to improve model faithfulness. In this work, we propose a probabilistic inference paradigm that provides fine-grained and lookahead rewards to ensure that LLM-generated rationales are logically coherent and comprehensive. These rewards are derived from a domain-specific proposal distribution, allowing for optimised sequential Monte Carlo approximations. Our evaluations across three different reasoning tasks show that this method, which allows for controllable generation during inference, improves both accuracy and faithfulness of LLMs while keeping computational costs similar to those of existing decoding techniques. This method offers a promising path towards making LLMs more reliable for reasoning tasks without sacrificing performance or efficiency.",
    "keywords": [
        "interpretability",
        "faithfulness",
        "Large language model",
        "constrained generation"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We propose a probabilistic inference paradigm that provides fine-grained and lookahead rewards to ensure that LLM-generated rationales are accurate and faithful..",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=0Yfjerm9Zp",
    "pdf_link": "https://openreview.net/pdf?id=0Yfjerm9Zp",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes an approach to do faithful rationale generation in LLMs. It uses a steering-based approach to make the outputs more faithful to the reasoning of the llm in classification. The idea is to weight token logits using 2 kinds of reward models: A \"local\" one that tries to match tokens to those suggested by a domain-specific expert model and a \"lookahead\" one that does an MTCS type search and re-weights logits based on rewards from unrolled sequences.  \n\nExperiments are performed on a couple of QA type datasets, demonstrating that each method makes improvements in classification accuracy and faithfulness of rationales. Some qualitative analyses are also presented."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. MTCS type inference is a hot topic right now, and it is indeed an important frontier for LLMs to improve on.\n2. At a surface level, experimental results seem to show large gains.\n3. There"
            },
            "weaknesses": {
                "value": "Section 3 is pretty badly written, it is pretty hard to get the details of the approach. Instead of invoking irrelevant sophisticated-sounding terminology like \"Feynman-Kac\" formulas it would be better to describe the method in more detail. The math especially is confusing, see below.  \n\nThe paper seems to show some positive experimental results, but I am concerned about whether we are looking at a meaningful comparison. The proposed methods rely on domain experts. Looking at table 8 in the appendix these are generally models that have been fine-tuned for the task in some way (and not just on the validation sets as the main section claims, some have access to external datasets). So it shouldn't be that surprising that a method that is given access to an expert which has more signal will do better than the backbone pre-trained model. A fair comparison would have to be with an approach that does vanilla fine-tuning of LLama or mixtral model. \n\nIn terms of novelty: The authors have not really cited relevant work in the controlled decoding space:\n\nhttps://arxiv.org/abs/2310.17022\n\nhttps://sea-snell.github.io/ILQL_site/\n\nThese works already do something more sophisticated than just token reweighting by a reward score. So what is the novel contribution here? 2 possibities:\n1. Focusing on the faithfulness problem.\n2. The \"lookahead\" idea of the reward model. I dont recall having seen this before, but it feels like a simplification of a full-blown MCTS. I would also call this a poor man's version of ILQL.\n\nSo we are just left with #1 then, unless I missed something. And this is something I consider of limited novelty (more like an application for a particular problem, though one with interesting implications from the steering perspective)."
            },
            "questions": {
                "value": "1. what is \"t \\wedge T\" ?\n 2. sec 3.3, what is P(s_t) a posterior over?\n 3. In what sense is \\pi_t a \"potential\" function?\n 4. I cannot make any sense of eq 2. Is w \\in V the same as w_t?  why cant you simply remove the indicator function and write it as \\sum_w \\in C ? why is the indicator function in the deminator as well? is the intent to have a logit distribution that only puts mass on the tokens in C?\n 5. are the rollouts done on the backbone model or the expert model? have we considered /measured the inference time cost? this is an important consideration in a paper about mtcs type methods.\n\n6. Does q_\\phi simply reward completions of the output that have tokens in C?\n\n7. intro: \"in contrast, an expert model.....\" : this is an interesting claim (does seem plausible). is there a citation for evidence?\n\n8. line 140: tend to generate similar token....\": what does this mean?\n\n9. i am not up to date on the faithfulness literature, but the kind of interventations that the paper describe as standard ways of evaluation i.e. word inclusion and perturbation just seem to be likely to be noise-prone, leading to unreliable evals?\n\n10. GenExpert =?  lookahead?\n\n11. comment: the discussion between 329-342 helped understanding a bit and should be earlier in the paper.\n\n12. sec 5.2.2: the NLI example is a bad one i think. Submergible only means it is something that can be submerged. which doesnt automatically mean it is submerged."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The work aims to improve the faithfulness of the LLM-generated rationales for reasoning tasks. They propose an inference-based method where an LLM is guided to generate more faithful rationales by both local and global rewards. Both rewards are provided by additional expert models which are trained on the downstream tasks. Experiments demonstrate the effectiveness of the method in achieving higher accuracy and faithfulness."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Faithful rationales are important for explainability and model control, which makes this work well-motivated.\n2. The proposed method is training-free (although with reliance on trained expert models), making their method portable.\n3. A comprehensive set of experiments is conducted to showcase the effectiveness of their proposed method."
            },
            "weaknesses": {
                "value": "1. The method requires the model to generate the answer prior to the rationale, which provides no guarantee that the decision is made based on the rationale. The model could still suffer from inherent biases.\n2. The method is limited to reasoning tasks with constrained answer space, limiting its generalization to more open-ended tasks.\n3. The method is poorly introduced. It would be very helpful if the authors could explain what exactly Eq.1-3 are doing in plain words."
            },
            "questions": {
                "value": "Could this method generalize to the setting where the rationale is generated before the answer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes an inference-time method to improve the performance and faithfulness of general (instruction-tuned) large language models (LLMs). Specifically, the method uses expert models to provide fine-grained and lookahead rewards to search and reweight possible tokens or continuations proposed by the LLM. With the help of expert models trained on the target task or domain, the proposed method can improve both the accuracy and faithfulness of the zero-shot answers of two instruction-tuned models on three reasoning tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The direction this paper explored has been receiving increasing interest recently: improving the quality of LLM answers at inference time without modifying the model weights directly. The proposed method improves the zero-shot accuracy and faithfulness of two strong general instruction-tuned models (Llama-3-8B and Mistral-7b-Instruct-v0.3) on three reasoning tasks. The experiment showing the benefits of going beyond local/token-level rewards and taking into account the global/lookahead reward is interesting."
            },
            "weaknesses": {
                "value": "- There needs to be more details explaining the proposed method, the motivation of each part, the equations and variables, the relation to related work, and the implementation details. Specifically:\n  - Section 3.3: how does the Feynman-Kac Formulae model inspire the faithfulness-seaking search framework? The connection is not straightforward. The notation of eq 1 is ambiguous. What does posterior P_t(st) mean exactly? How is it used in the proposed method? Also, the equation itself needs more explanations on what it is computing and why in this way.\n  - Section 3.4 (Local constraint): line 179 I find it hard to follow the motivation. How \"certain attributes can be implicitly conveyed over longer spans rather than the individual token\" is connected to \"Instead, domain-specific experts tend to demonstrate better accuracy in knowledge-rich tasks.\"? If the domain expert has better accuracy why not just use the expert to predict the scores? Why bother to use them to improve the backbone LLM? In lines 180-181, it says \"we introduce a set of classification label words C from these expert models ...\", how is C constructed? What is the motivation behind token masking?\n  - Section 3.4 (Lookahead Reweight): Equation 3 is hard to understand without proper explanations. $m$ and $x_i$ are not explained in the texts. $s_{t+l}=s_{t-1}||w_t$ is more confusing: $s_{t+l}$ has $t+l$ tokens while $s_{t-1}||w_t$ has $t$ tokens. What does equality mean here?\n- Many experimental details are missing, and important experiments are missing.\n  - Missing baselines: the performance and faithfulness of the expert models alone. If the faithfulness or accuracy of the expert models are better than the backbone LLM, why do we even need to use the expert models to improve the backbone LLM?\n  - Evaluation details: how is the original model evaluated? If it is a zero-shot evaluation. What is the exact prompt and task format used? How to extract answers from the outputs to calculate the accuracy? The backbone LLMs are state-of-the-art instruction-tuned models. However, the task performance as well as the faithfulness are quite low, so the authors need to provide more details on the evaluation.\n  - What is the choice of hyperparameter n (number of rollouts) and how is it chosen?\n- The writing of the paper could be improved for better readability. First, the paper is not properly scoped. For example, in lines 16-18, it says \"... to ensure that LLM-generated rationales are logically coherent and comprehensive.\" However, there is no result discussing the logical coherence or comprehensiveness of answers in the paper. Another example is line 108: it says \"We firstly introduce the faithfulness definition in our context,\", but there is no clear definition in section 3.2."
            },
            "questions": {
                "value": "1. If the expert model is as big as the base model, how can the computational cost be similar to beam search?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "They tackle the rationale generation tasks in LLMs' reasoning process. Specifically, they propose a probabilistic inference paradigm that provides fine-grained and lookahead rewards to instruct LLMs to generate good rationale. The key problem addressed is that LLMs often produce unfaithful explanations, especially when they fail to incorporate essential contextual information. \n\n+ **Local Reward**:  this component ensures coherence with the immediate context, often by using a domain-specific expert model.\n+ **Global reward**: This assesses the plausibility of the current token in relation to desirable future attributes\n\nThe search algorithm, especially for lookahead reweight seems interesting.\n\nPlease forgive me if I misunderstand something. I spent much time for reading the paper but to be honest, I am not an expert in this area. I will available on the rebuttal time for author's response and will read their response. I am also open to other reviewers' opinions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces a novel probabilistic inference method with a dual-reward mechanism, combining local and global reward. This is a very novel solution. \n2. The paper is well-written. I am not an expert in this domain but I can get their core contributions. \n3. The experiment design is clear: they design the ablation study in Section 5.1 to justify the local and global rewards for the final performance. Although I suggest authors could do better by choosing more LLMs in different model size to better support their experimental design."
            },
            "weaknesses": {
                "value": "1. There are several related works that are missing or less discussed: \n        + Evaluating Human Alignment and Model Faithfulness of LLM Rationale\n        +  On Measuring Faithfulness or Self-consistency of Natural Language Explanations\n2. Figure 2 about the distribution of domain-specific words is unclear to me. \"showing that our method can respond more actively to those domain-specific words\" Why does this part matters to the experimental results."
            },
            "questions": {
                "value": "1. The overall experiments are conducted on LLaMA3. I think more backbone LLMs and other sizes of LLMs are needed to justify the proposed inference paradigm.\n2. More experiments on more related datasets is needed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}