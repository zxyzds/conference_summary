{
    "id": "OegBJMucyM",
    "title": "Pre-Memorization Train Accuracy Reliably Predicts Generalization in LLM Reasoning",
    "abstract": "When large language models (LLMs) are finetuned on reasoning tasks, they can either reduce their training loss by developing problem-solving abilities, or by simply memorizing target traces in the training data. Our work aims to better understand how this learning process shapes a model's ability to generalize. We observe that, while LLMs often perfectly memorize most target solution traces by the end of training, their predictions at intermediate checkpoints can provide valuable insights into their behavior at test time. Concretely, we introduce the concept of pre-memorization train accuracy: the accuracy of model samples for training queries prior to exactly reproducing reasoning traces in the training data. We find that the average pre-memorization train accuracy of the model is strongly predictive of its test performance, with coefficients of determination around or exceeding 0.9 across various models (Llama3-8B, Gemma2-9B), datasets (GSM8k, MATH), and training setups. Beyond this aggregate statistic, we find that the pre-memorization train accuracy of individual examples can predict the model\u2019s sensitivity to input perturbations for those examples, allowing us to identify examples for which the model fails to learn robust solutions. A natural application of this insight is in data curation. We find that prioritizing the collection of examples with low pre-memorization accuracy leads to 1.5-2x data efficiency compared to i.i.d. data scaling, and outperforms other standard data curation techniques.",
    "keywords": [
        "LLMs",
        "Generalization",
        "Memorization",
        "Reasoning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=OegBJMucyM",
    "pdf_link": "https://openreview.net/pdf?id=OegBJMucyM",
    "comments": [
        {
            "summary": {
                "value": "This paper studies how memorization impacts model generalization.  More precisely, the authors study how the pre memorization accuracy -- the accuracy before which an example is memorized by the model -- is particularly predictive of test performance.  The authors preform a series of experiments to quantify this phenomena, and show that it persists across different model scales and datasets."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall the experiments are convincing and the analysis is thorough.  The comparison to active learning metrics is particularly nice, although perhaps a slightly more principled set of metrics could have been chosen (e.g., an influence function that is a dot product of loss gradients)."
            },
            "weaknesses": {
                "value": "There are no obvious weakness in the paper.   Some experiments would have been nice to see:\n* in particular, having a more diverse set of model sizes would have been interesting.  Is the proposed metric still predictive for smaller models?  It seems that for small models, they may never end up memorizing the reasoning trace, and the notion of a pre-memorization accuracy does not exist.  Are there nevertheless proxy metrics available in those cases?  At what scale does the generalization emerge?\n* It would be interesting to consider a more diverse set of active learning metrics.  For example, can influence functions be used to map examples to train data (as proposed in [1)?\n\n[1] https://arxiv.org/abs/2308.03296"
            },
            "questions": {
                "value": "(see above)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the problem-solving abilities and memorization processes of LLMs in the context of mathematical reasoning tasks. By introducing the concept of pre-memorization train accuracy, the authors establish a more effective metric for predicting the model's final test accuracy. This metric tends to serve as an indicator of training robustness and generalization capabilities. Additionally, it can be utilized to collect training data in a curriculum setting, resulting in a 1.5 to 2 times improvement in sample efficiency."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper highlights the discrepancy between training accuracy and test accuracy, as well as the impact of different training parameters (learning rate) on the training process, providing motivation for the methods proposed in the study.\n2. The authors conduct experiments on Llama-3 8B and Gemma2 9B, aiming to demonstrate the generalizability of their method across different models.\n3. The proposed method shows improvements in predicting test set accuracy and in curriculum learning scenarios compared to other baseline methods."
            },
            "weaknesses": {
                "value": "1. Many details are not adequately presented, such as the selection for the threshold $ p $ in pre-memorization train accuracy, as well as the contamination issues in the synthetic training data for Figure 6.\n2. While the paper aims to investigate the model's generalization capabilities, all experiments utilize training and test sets from the same distribution, lacking out-of-distribution experiments.\n3. In the experiments predicting testset accuracy, the comparisons with other methods are not entirely fair. The gradient variance and distance from initialization methods rely on significantly less input information during prediction, while the ATC method primarily focuses on OOD scenarios."
            },
            "questions": {
                "value": "1. In L246-247, the authors mention that the value of $ p $ is dependent on the task and the pretrained model, but I could not find any explanation about the selection of $ p $ in the paper. How is the value of $ p $ determined, and is the correlation coefficient for predicting testset accuracy sensitive to this parameter?\n2. The authors claim that this training set accuracy metric based on the training process is superior, but they lack necessary ablation studies. For instance, how would the prediction performance change if only the training set accuracy after the first epoch(rather than first m epochs) is used?\n3. The experiments in Figure 5 utilize 6 epochs of training, and it is not surprising to reach such conclusions in a setting that is sufficiently overfitted. In practice, it is uncommon to choose such a high number of epochs. Can the same conclusions be drawn with a maximum of 3 epochs?\n4. In L413, it is mentioned that the data collection methods used in the paper can be applied to human data. Given that there are already some large-scale open-source datasets available, could the experiments be repeated on these datasets (e.g., Numina-CoT) to verify the reproducibility of the results?\n5. The experiments in Figure 6 are conducted on GSM8K and MATH at difficulty levels 1-3. Considering that these datasets are relatively easy, I am curious about the experimental results at difficulty levels 4-5 in MATH."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper seeks to understand generalisation in LLMs by introducing a metric that when applied to training examples correlate to test set performance for GSM8K and MATH. Moreover, this metric can used as proxy for example difficulty and in turn be used for curriculum learning and/or data curation."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper takes an interesting approach by using studying the CoT in GSM8K/MATH (traces as they call it) of a training example. \nBy focusing on the variance of traces while maintaining the correct answer, the authors have a very nice definition of overfitting."
            },
            "weaknesses": {
                "value": "There are not enough details on how `p` is chosen. \n\nBy inspecting Figure 1, one could argue that the procedure of \"calibrating\" a `p` is simply selecting a sort of projection onto the `x=y` line which then causes the calibration of pre-memorisation train accuracy to be correlated the test accuracy. \n\nThe different run curves w/ different learning rates are similar enough that calibrating `p` could simply cause the correlation to be high. \n\nMoreover, finding such `p`, we have created a list of examples which correlate high to the test set. This is a form of leakage. Specially if it needs to be calibrate for each model and task."
            },
            "questions": {
                "value": "* Can you elaborate further on what the impact of `p` calibration above?\n* What is the impact of varying `p` on the metric value and correlation. Perhaps, would we show that we can calibrate `p` on a hold-out set of the training-set and the experiments still hold\n\nIt would be great to understand the impact of this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on investigating how the learning process influences the reasoning ability of LLMs to generalize. Specifically, this paper introduces the concept of pre-memorization train accuracy, which is shown to have a positive correlation with test accuracy. The authors also leverage this concept to develop a novel data curation method. Extensive experiments validate the effectiveness of the proposed model compared with a set of baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed method itself is coherent and easy to follow. The authors conduct experiments on several benchmarks to validate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1.\tOverall, the motivation behind the paper is not clearly articulated. The authors introduce a new concept, termed per-memorization train accuracy, which is calculated by sampling from the model multiple times and averaging the correctness of the samples. In my view, this metric is almost equivalent to the difficulty level of the problems. When the model samples multiple times and fails to obtain correct answers, it indicates that the problem is difficult. Conversely, successful sampling suggests the problem is easier. Furthermore, the difficult level of problems is often provided in many datasets, such as MATH. The authors should discuss this straightforward metric in the main text and include comparisons in the experiments.\n2.\tThe paper provides insufficient explanation of why the proposed metric is expected to correlate positively with test accuracy. Additionally, it lacks theoretical justification to support the effectiveness of the proposed data curation method.\n3.\tThe authors employ only two LLMs in their experiments. To strengthen the evaluation, recently proposed dense LLMs, such as Mistral and Qwen, as well as sparse Mixture-of-Experts (MOE) models like Mixtral and DeepSeekMOE, should be included as the backbone models for comparation.\n4.\tReasoning is a general ability for complex problem-solving. Beyond mathematical reasoning, there are many other important reasoning tasks, such as logical reasoning, commonsense reasoning. To comprehensively evaluate the effectiveness of the proposed method, the authors should conduct experiments on logical reasoning tasks (such as LogiQA) and commonsense reasoning tasks (such as HellaSwag and Winogrande)."
            },
            "questions": {
                "value": "1.\tCould you involve a broader range of LLMs, encompassing both dense LLMs and sparse MOE models, to provide a more comprehensive demonstration of the proposed method\u2019s effectiveness? \n2.\tCould you involve a wider variety of reasoning task to offer a more holistic demonstration of the proposed method\u2019s effectiveness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}