{
    "id": "Feg9xrbFcn",
    "title": "Is $k \\times k$ Matrix Eigendecomposition Sufficient for Spectral Clustering?",
    "abstract": "Spectral clustering has been widely used in clustering tasks due to its effectiveness. However, its key step, eigendecomposition of an $n\\times n$ matrix, is computationally expensive for large-scale datasets. Recent works have proposed methods to reduce this complexity, such as Nystr\\\"om method approximation and landmark-based approaches. While these methods aim to maintain good clustering quality while performing eigendecomposition on smaller matrix. The minimum matrix size required for spectral decomposition in spectral clustering is $k\\times k$ (where $k$ is the number of clusters), as it needs to obtain $n\\times k$ k-dimensional spectral embedding features. However, no algorithm can achieve good clustering performance with only a $k\\times k$ matrix eigendecomposition currently. In this paper, we propose a novel distribution-based spectral clustering. Our method constructs an $n\\times k$ bipartite graph between n data points and k distributions, enabling the eigendecomposition of only a $k\\times k$ matrix while preserving clustering quality. We demonstrate that our approach can achieve efficient and effective spectral clustering through $k\\times k $matrix eigendecomposition.",
    "keywords": [
        "Spectral clustering",
        "Kernel mean embedding",
        "Matrix eigendecomposition"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Feg9xrbFcn",
    "pdf_link": "https://openreview.net/pdf?id=Feg9xrbFcn",
    "comments": [
        {
            "summary": {
                "value": "This paper delves into the computational challenges in spectral clustering, specifically the high cost affiliated with the eigen-decomposition of large Laplacian matrices (commonly n \u00d7 n, where n denotes the number of data points). The authors proposed a distribution-based method D-SPEC, which transforms the spectral clustering into the bipartite graph clustering with n \u00d7 k adjacency matrix. In this way, the cost of eigen-decomposition is reduced to O(nk^2). The authors conducted extensive experiments on synthetic and real datasets, demonstrating that D-SPEC not only sustains the efficacy of clustering but also exhibits robust scalability on datasets with millions of data points."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. The proposed D-SPEC is interesting.\n3. There are extensive experiments on synthetic and real datasets, which demonstrate the effectiveness and scalability of D-SPEC."
            },
            "weaknesses": {
                "value": "1. D-SPEC refines the n-by-n similarity matrix S (in Line 158) by filtering out the edges with weights smaller than $\\tau$ and obtains landmarks/anchors (subgraphs). However, there is a paradox in the current setting. First, calculating the similarity matrix S requires O(n^2) time complexity, which is the same as the eigen-decomposition of the Laplacian matrix (which can be solved with efficient algorithms like Lanczos in O(n^2)). Thus, the D-SPEC makes no sense in terms of computational efficiency. Conversely, if D-SPEC chooses to sample p samples to construct the similarity matrix S (Line 165), then it degenerates into the most common approach of choosing landmarks/anchors. So the authors should provide a more detailed explanation for this paradox.\n\n2. The number of landmarks/anchors (subgraphs) is indirectly determined by the parameter $\\tau$. Could the authors provide some insights on how the parameter $\\tau$ affects the number of landmarks/anchors and further impacts the clustering performance?\n\n3. The paper introduces \u201cdistribution-based\u201d as a core concept of the new method, but it does not provide a detailed explanation of what this means. Could you clarify how this method uses \u201cdistributions\u201d for clustering and how it fundamentally differs from traditional point-based methods? What theoretical basis supports this choice, and are there practical cases where \u201cdistribution-based\u201d clustering shows clear advantages over traditional methods?\n\n4. There seems to be a mistake in Theorem 3.2. I think the eigenvector $v$ according to zero eigenvalues is not the one defined in Line 661. Besides, the proof of Theorem 3.2 is not rigorous enough. For example, the optimal solution of eign-decomposition is not unique, so can it be guaranteed that the obtained eigenvector is as stated in Theorem 3.2?\n\n5. It would help to standardize the use of symbols throughout and to provide clear explanations when new symbols are introduced."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a superior and effective distribution-based spectral clustering (D-SPEC)\nalgorithm, to solve the fundamental limitations of spectral clustering w.r.t. efficiency and effectiveness. First, D-SPEC proposes a novel distribution-based spectral clustering not the point-based method, which can contain more information of the original graph. Second, D-SPEC constructs an n\u00d7k bipartite graph between n data points and k distributions, enabling the eigendecomposition of only a k\u00d7k matrix, which is suitable for large-scale datasets. Finally, D-SPEC proves theoretically that distribution-based spectral clustering can preserve graph information and is more robust to noise. Comprehensive experiments verify the superiority of the proposed model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.The proposed D-SPEC Enhances effectiveness of spectral clustering by transitioning from a traditional point-based perspective to a distribution-based perspective.\n2.The proposed D-SPEC Enhances the efficiency of spectral clustering by constructing a bipartite graph with n*k, which is a smaller matrix than the matrix with n*p, where p is the number of the sampling points).\n3.The theoretical studies are very interesting.Theoretical proof that D-SPEC preserves graph information, along with a noise tolerance bound, demonstrates the robustness of D-SPEC."
            },
            "weaknesses": {
                "value": "1.In Figure 1, is the leftmost figure is the example graph G that contains 3 subgraphs? an Annotation on the figure will make it clearer to readers.\n2.The article contains many unclear symbols and expressions, such as \n(1)In Eq.(1), G_i denotes the i-th subgraph, in the following, G_j denotes the j-th subgraph, but i and j have different value ranges in other parts of the article (like i=1,2,...,n). Authors should use unified letter representation to prevent readers from misunderstanding.\n(2)For the pseudo code of D-SPEC, in step 4, what is letter V? it was not defined in the previous text. As W is an n*k matrix, does the letter V denote n samples or k spectral embeddings (it is described in Theorem 3.4)? \n3.Figure 3 is not clear.  \n4.What is the size and other information of 5 large-scale datasets in Figure 4? And Figure 4 cannot show D-SPEC\u2019s efficiency.   \n5.Compared existed methods, D-SPEC only gives a better method to construct a n*k bipartite graph. I think its innovation is not enough to be published in this top conference."
            },
            "questions": {
                "value": "1.In Figure 1, is the leftmost figure is the example graph G that contains 3 subgraphs? an Annotation on the figure will make it clearer to readers.\n2.The article contains many unclear symbols and expressions, such as \n(3)In Eq.(1), G_i denotes the i-th subgraph, in the following, G_j denotes the j-th subgraph, but i and j have different value ranges in other parts of the article (like i=1,2,...,n). Authors should use unified letter representation to prevent readers from misunderstanding.\n(4)For the pseudo code of D-SPEC, in step 4, what is letter V? it was not defined in the previous text. As W is an n*k matrix, does the letter V denote n samples or k spectral embeddings (it is described in Theorem 3.4)? \n3.Figure 3 is not clear.  \n4.What is the size and other information of 5 large-scale datasets in Figure 4? And Figure 4 cannot show D-SPEC\u2019s efficiency.   \n5.Compared existed methods, D-SPEC only gives a better method to construct a n*k bipartite graph. I think its innovation is not enough to be published in this top conference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a distribution-based spectral clustering algorithm, termed D-SPEC, that reduces the computational demand of spectral clustering to the eigendecomposition of a k \u00d7 k matrix. By constructing an n \u00d7 k bipartite graph, the authors enable clustering through a smaller matrix, preserving information while reducing complexity. D-SPEC is shown to perform competitively with traditional spectral clustering methods, especially in handling noise and maintaining graph structure, and experiments confirm its robustness on both synthetic and real datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.The proposed distribution-based clustering approach achieves efficient clustering with a k \u00d7 k matrix eigendecomposition, a notable shift from traditional methods.\n2. Explanations of the bipartite graph structure and eigendecomposition process are well-structured.\n3. This approach is claimed that it has potential to enable large-scale spectral clustering, expanding practical applications of clustering for large datasets."
            },
            "weaknesses": {
                "value": "1.The approach assumes noise tolerance, but experimental validation of this property on noisy, complex graphs is limited.\n2.The D-SPEC approach shows similarities with landmark-based clustering methods, lacking clear differentiation.\n3. Limited testing on real-world, large-scale datasets reduces confidence in its applicability.\n4.The theoretical sections are occasionally verbose, with redundant content that could be streamlined."
            },
            "questions": {
                "value": "1.How does D-SPEC\u2019s noise tolerance compare to landmark-based or Nystr\u00f6m methods? A direct comparison would help clarify its advantages.\n2.Could a hybrid method, combining landmarks and distribution-based clustering, improve flexibility?\n3.How is the threshold parameter for bipartite graph construction determined? Is the method highly sensitive to this parameter?\n4.How does the method handle non-spherical or poorly-separated clusters?\n5.Does the approach suffer performance degradation in highly connected graphs?\n6.Can the authors provide empirical comparisons of computation time between D-SPEC and traditional spectral clustering on large datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses spectral clustering's typical scalability limitations on large datasets. It explores whether spectral clustering can be effectively performed using only a small square matrix eigendecomposition. The authors propose a distribution-based method (D-SPEC) that constructs a bipartite graph between data points and landmarks to enable this reduced computation. Some experiment results are provided."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The method is fast in some sense.\n* There are some theoretical guarantees"
            },
            "weaknesses": {
                "value": "* The research motivation is questionable. It assumes W must be fully connected. However, many graph-based clustering methods only use a sparse W (e.g., 10-NN). Eigendecomposition of a sparse matrix is cheap.\n* The method can be sensitive to the hyperparameters (e.g., psi)\n* The experiment results are not convincing, especially for the MNIST and CoverType datasets."
            },
            "questions": {
                "value": "* In Table 5, you listed the parameter search ranges. How did you choose the best in the ranges?\n* Why D-SPEC is so bad for the covertype dataset?\n* It is unsuitable to use k x k in the title because k is not defined yet."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}