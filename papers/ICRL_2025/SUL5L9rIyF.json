{
    "id": "SUL5L9rIyF",
    "title": "Distribution Corrected Estimation via Adversarial Density Weighted Regression",
    "abstract": "We propose a novel one-step supervised imitation learning (IL) framework called Adversarial Density Regression (ADR). This IL framework aims to correct the policy learned on unknown-quality to match the expert distribution by utilizing demonstrations, without relying on the Bellman operator. Specifically, ADR addresses several limitations in previous IL algorithms: First, most IL algorithms are based on the Bellman operator, which inevitably suffer from cumulative offsets from sub-optimal rewards during multi-step update processes. Additionally, off-policy training frameworks suffer from Out-of-Distribution (OOD) state-actions. Second, while conservative terms help solve the OOD issue, balancing the conservative term is difficult. To address these limitations, we fully integrate a refined one-step Distribution Corrected Estimation (DICE)-type supervised framework named ADR. Theoretically, we demonstrate that this adaptation can effectively correct the distribution of policies trained on unknown-quality datasets to align with the expert policy's distribution. Moreover, the difference between the empirical and the optimal value function is proportional to the lower bound of ADR's objective, indicating that minimizing ADR's objective is akin to approaching the optimal value. Experimentally, we validated the performance of ADR by conducting extensive evaluations. Specifically, ADR outperforms all of the selected IL algorithms on tasks from the Gym-Mujoco domain. Meanwhile, it achieves an 89.5% improvement over IQL when utilizing ground truth rewards on tasks from the Adroit and Kitchen domains.",
    "keywords": [
        "Imitation Learning",
        "Deep Reinforcement Learning"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=SUL5L9rIyF",
    "pdf_link": "https://openreview.net/pdf?id=SUL5L9rIyF",
    "comments": [
        {
            "title": {
                "value": "Reply to Reviewer jPp9's Weaknesses' section (part 2)"
            },
            "comment": {
                "value": "**wk.2** The reliance on auxiliary Variational Autoencoder (VAE) training for each task raises concerns regarding computational efficiency. Although the paper does not explicitly address the associated computational cost, this aspect warrants further investigation.\n\n- wk.2.1 1) We have optimized the VAE weighted objective function. Specifically, assuming the input matrix has dimensions of $b\\times m$, we simplified the objective function from the original objective $min_{\\pi} E_{D}[D_{KL}(\\pi||P^*)-D_{KL}(\\pi||\\hat P)]$(Equation 9) to $min_{\\pi}E_{D}[\\log\\frac{P^*}{\\hat P} ||\\pi(\\cdot|s)-a||^2]$ (Equation 10), thereby reducing the complexity from O(2(1+b)b*m) to O(b*m). We have compared the two in the supplementary page (Figure 6: (Left)), and it can be seen that Equation (10) requires less time compared to Equation (9) under the same training settings.\n- wk.2.2 Common density estimators include Gaussian, VAE (Variational Autoencoder), transformer, and diffusion-based density estimators. While diffusion models have stronger representation capabilities, they require multi-step sampling during the sampling process, resulting in higher time complexity compared to VAE and Gaussian models.\n\n\n\n  Furthermore, considering this issue from the perspective of reinforcement learning (RL), although Gaussian may require fewer parameters, VAE is more suitable than Gaussian as a density estimator in RL [3]. Therefore, our method performs well and has lower computational complexity compared to diffusion models. Additionally, our research is conducted in the context of Markov Decision Processes (MDPs), while diffusion models and transformers are generally sequence models used in non-MDP settings, which is hard to be implemented under MDP settings. For exmaple, given current examples {$s_0,a_0$}, we can't find {$s_{0-k},a_{0-k},\\cdots s_0,a_0$.} for sequence models.\n\n**wk.3.1** Theorem 4.2 seems to be an obvious reformulation of equation (8). The necessity of naming it as a theorem is unclear. For example, what's the main observation and conclusion from this theorem is unknown. **wk.3.2** The application of KL-divergence has several limitations. For instance, it requires that the compared distributions have exactly the same support, and it isn't a true distance metric because it's not symmetric. Nowadays, more promising alternatives like Wasserstein distances are gaining attention, with wide exploration in reinforcement learning applications.\n\n**re (wk.3.1)** Thanks for your suggestion. We have renamed Theorem 4.2 from \"density weight\" to \"weighted linear objective can implement adversarial density regression.\" We have chosen to name this conclusion Theorem 4.2 because the objective function in Theorem 4.2 (Equation 10) can significantly improve the training efficiency of Equation (9). Additionally, in the Appendix page Figure 6 (Left), we have compared Equation (10) and Equation (9) under the same experimental setup, and the results show that the formula in Theorem 4.2 performs more efficient. Therefore, we need a conclusion to justify the rationality of the high correlation between our Equations (10) and (9), ensuring that the theoretical motivation of our method is reasonable and that the real implementation is well-founded.\n\u00a0\n\n\n**wk.3.2** The application of KL-divergence has several limitations. For instance, it requires that the compared distributions have exactly the same support, and it isn't a true distance metric because it's not symmetric. Nowadays, more promising alternatives like Wasserstein distances are gaining attention, with wide exploration in reinforcement learning applications.\n\n**re (wk.3.2)** We acknowledge the limitations of KL divergence. However, we derive the KL-based objective into a density-weighted objective (Equation 10). In Equation 10, we base our approach on the density-weighted BC objective, which is inherently symmetric because ||a-b|| = ||b-a||.\nIn addition, we compared our method with the Optimal Transport-based approach [4], and the experimental results showed that our method has advantages on long-distance tasks such as Andtroit and Kitchen (Table 2 in our paper).\n\n# Reference\n\n[1] Jason Ma, etal. Versatile Offline Imitation from Observations and Examples via Regularized State-Occupancy Matching\n\n[2] anonymous2024trajectorylevel,Trajectory-level Data Generation with Better Alignment for Offline Imitation Learning\n\n[3] Wu, etal. Supported Policy Optimization for Offline Reinforcement Learning\n\n[4] Luo,etal. OPTIMAL TRANSPORT FOR OFFLINE IMITATION LEARNING"
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer jPp9's Weaknesses' section"
            },
            "comment": {
                "value": "**wk.1** Novelty: The main innovation in Density Weighted Regression (DWR) is the importance sampling term combined with a behavior cloning objective. However, 1.2) the novelty is unclear. 1.1) The justification relies on comparisons to traditional Behavior Cloning (BC) and DICE, but the connection to DICE seems weak. DICE addresses a reinforcement learning problem using its dual form, involving a Bellman flow constraint, while ADR focuses on an imitation learning problem. These approaches are based on different formulations.\n\n**re (wk.1.1)** In this paper, we have not referred to our algorithm as a derivative of DICE. Instead, we have termed our algorithm as a DICE-type (in line 20) supervised learning algorithm. Specifically,\n\n  **Why utilizing the word 'DICE type' in lines 20.** Our motivation for considering ADR as a DICE-type algorithm (in line 20) is:\nADR also leverages a demo dataset to refine the distribution learned by the policy on other datasets, which shares a similarity in the 'correction' with the DICE algorithm in that it incorporates a distance regularization term in the reward function to adjust the learned policy.\n\n  In addition, due to the limited word count in the abstract, we prefer to utilize concise language to summarize the main content of this paper.\n\n  **Why mentioned DICE in our paper.** Furthermore, the reason we chose DICE for comparison (lines 51~55. On the other hand...), is that DICE algorithms also use the experimental setup of \"demo + other data\" to train a policy that is close to the demo policy. This experimental setup is relatively close to our experimental setup. Meanwhile, some literatures [1] written from IL, and these literatures are very solid.\n\n  **DICE is RL or IL?** During our investigation of DICE-related literature, we encountered some ambiguities, as the experimental setup of DICE is close to imitation learning (IL) setups, and some literature initializes the introduction from an IL perspective [1]. However, some DICE algorithms use reward functions, which has a strong connection with reinforcement learning (RL). Therefore, in our paper, we use the term \"another approach\" (lines 52) instead of categorizing DICE definitively under IL, however, there having similar settings between DICE and IL. Correspondingly, In the preliminary section, we introduce IL and DICE separately. \n\n  It is also important to note that we are aware of some DICE algorithms involve modifications to the reward function, which differ significantly from our method. Furthermore, most DICE papesr conduct theoretical analysis from the perspective of occupancy, while our paper conducts theoretical analysis from the perspective of return boundary. Therefore, we choose to use the term \"DICE-type\" rather than \"DICE\".\n\n  Given the historical context of DICE-related literature, we believe that our current writing balances the relationship beween RL/RL/IL and DICE from both the literature and experimental setup perspectives. Additionally, we have included supplementary explanations on this matter in an Appendix page of the latest version of our manuscript.\n\n**re (wk.1.2)** At the beginning of this paper, we mentioned that our algorithm falls under the category of imitation learning algorithms, given this context.\n- Our method belongs to an imitation learning (IL) algorithm. During the update process, we do not use the Bellman operator, thus our method does not suffer from offsets accumulated under suboptimal rewards, which we believe is a crucial point. Additionally, some research [2] has also discussed that the reward shaping methods of some previous IL algorithms do not guarantee 100% optimal rewards.\n- Meanwhile, our method does not include a conservative term, which results in a lower dependence on parameters for each task when testing offline tasks. Specifically, our algorithm completed all experiments using only few sets of parameters. As we all know, RL algorithms are sensitive to parameters, and most IL algorithms update their policies based on the RL framework and the reward function obtained through imitation learning. Therefore, there are parameter tuning issues stemming from the RL framework.\n- Furthermore, our algorithm demonstrates potential in terms of effectiveness within the domain of simulation environments.\n\nIn the paper, we mentioned algorithms related to DICE and discussed them in the introduction. Furthermore, given that the term \"DICE-type\" has caused some confusion for the reviewers in understanding the paper, we are considering whether to make necessary modifications to this term, and we will provided additional explanations on the supplementary page latter. If the reviewers strongly recommend modifying this term, we will describe the learning mechanism of ADR in the abstract section using other more appropriate terms."
            }
        },
        {},
        {
            "title": {
                "value": "The reason for the early reply to Reviewer jPp9's Weaknesses' section"
            },
            "comment": {
                "value": "Thanks for your questions and suggestions. And we appreciate your initial recognition of our theoretical analysis and experimental results. We have carefully read your comments provided. Upon initial assessment, we believe that some of your questions may require extensive communication. Meanwhile, given that reviewers are generally quite busy, we have decided to submit an early response to address some of the your theoretical concerns, such as the relationship between this terminology'DICE' and our method e.g.\n\nFor some questions sourced from question section, such as those related to different density estimators, we will provide analysis, and we will appropriately supplement experiments latter to further analyze these aspects from an experimental perspective. (We will respond as soon as possible during the rebuttal period)\n\nThank you"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a one-step supervised imitation learning (IL) framework that addresses the challenge of training policies on suboptimal datasets while aligning them with expert demonstrations. The proposed method leverages two variational auto-encoders (VAEs), trained separately on the suboptimal data and expert demonstrations, to estimate the respective behavior distributions. These VAEs are then employed to calculate density weights for offline policy training. This training is performed using an adversarial density regression (ADR) approach. ADR minimizes the Kullback-Leibler (KL) divergence between the policy\u2019s behavior distribution and the expert demonstration distribution while simultaneously maximizing the KL divergence between the policy and the suboptimal dataset distribution. This formulation translates into a density-weighted regression between the policy\u2019s output and the actions observed in the suboptimal dataset. Notably, this approach circumvents the reliance on the Bellman operator and demonstrates competitive performance compared to other offline reinforcement learning (RL) methods. The proposed framework exhibits robustness to noisy demonstrations and mitigates the risk of out-of-distribution (OOD) generalization issues."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **Empirical Validation**: The paper demonstrates the efficacy of the proposed Adversarial Density Regression (ADR) method through comprehensive experimental results. ADR consistently outperforms baseline algorithms across diverse task domains, achieving higher accuracy while exhibiting robustness to noisy demonstrations and a reduced risk of out-of-distribution generalization.\n2. **Training Efficiency**: The one-step supervised learning paradigm employed by ADR effectively minimizes cumulative errors, resulting in stable training without tuning conservative terms. This streamlined approach contributes to the method\u2019s practical appeal.\n3. **Theoretical Analysis**: The authors provide theoretical for ADR\u2019s effectiveness. They prove that minimizing the ADR objective function is aligned with attaining an optimal policy, which forms a valid theoretical justification for the proposed method."
            },
            "weaknesses": {
                "value": "1. **Novelty**: The main innovation in Density Weighted Regression (DWR) is the importance sampling term combined with a behavior cloning objective. However, the novelty is unclear. The justification relies on comparisons to traditional Behavior Cloning (BC) and DICE, but the connection to DICE seems weak. DICE addresses a reinforcement learning problem using its dual form, involving a Bellman flow constraint, while ADR focuses on an imitation learning problem. These approaches are based on different formulations.\n2.  **Computational Overhead**: The reliance on auxiliary Variational Autoencoder (VAE) training for each task raises concerns regarding computational efficiency. Although the paper does not explicitly address the associated computational cost, this aspect warrants further investigation.\n3. **Technical Significance and Soundness.**\n- Theorem 4.2 seems to be an obvious reformulation of equation (8). The necessity of naming it as a theorem is unclear. For example, what's the main observation and conclusion from this theorem is unknown.\n- The application of KL-divergence has several limitations. For instance, it requires that the compared distributions have exactly the same support, and it isn't a true distance metric because it's not symmetric. Nowadays, more promising alternatives like Wasserstein distances are gaining attention, with wide exploration in reinforcement learning applications."
            },
            "questions": {
                "value": "1. Dataset Size: The paper could benefit from an analysis of the impact of the demonstration dataset size on ADR\u2019s performance. Could you please demonstrate the relationship between dataset size and model accuracy?\n2. Noisy Data Handling: It would be beneficial to investigate the robustness of ADR in the presence of noisy or low-quality suboptimal datasets. Could you please demonstrate the capability of your method to address noisy or low-quality suboptimal data issue?\n3. VAE Choice: A clear justification for selecting VAEs as the behavior distribution estimator would enhance the paper\u2019s clarity. Could you please discuss the advantages of VAEs compared to other potential estimators?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The author propose a simple yet effective one-step supervised IL framework termed Adversarial Density Regression (ADR), which leverages demonstrations to correct the policy distribution learned from datasets of unknown-quality toward expert distribution without relying on the Bellman operator."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The author introduces a substantial and well-validated advancement in IL with Adversarial Density Regression (ADR). ADR addresses key limitations in current IL methodologies by diverging from the typical reliance on multi-step Bellman updates and conservative RL policies, which can introduce cumulative errors and struggle with out-of-distribution (OOD) data. Experimentally, ADR surpass selected IL algorithms and achieves better performance than the offline RL algorithm IQL in the Android domain."
            },
            "weaknesses": {
                "value": "This paper introduces ADR and demonstrates strong results in the Gym-Mujoco, Kitchen, and Adroit domains. However, these environments primarily focus on continuous control tasks with static distributions. This choice does not fully capture the complexity of real-world applications, which often involve changing dynamics or high-dimensional, sparse observation spaces (e.g., partially observable or dynamic obstacle environments). Testing ADR on environments with dynamically shifting conditions, like robotic control with dynamic obstacles or changing goals, could provide more insights into its robustness and generalizability. ADR's density-weighted objective, VAE-based density estimation, and adversarial learning framework are likely to introduce significant computational overhead, this paper limited analysis of computational efficiency."
            },
            "questions": {
                "value": "Could the authors comment on how ADR would handle more substantial or structured noise?\nCould the authors provide insights into ADR\u2019s computational demands and compare them to other IL methods?\nCould the authors clarify the benefits of using adversarial learning here? For instance, how much does Adversarial Density\nEstimation (ADE) contribute to ADR\u2019s overall performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents an offline imitation learning (IL) algorithm, where the agent lacks access to environment rewards and cannot collect new samples. Instead, it relies on two datasets: (1) expert demonstrations and (2) sub-optimal demonstrations. The scarcity of expert data, a realistic scenario in IL, makes learning expert-level policies particularly challenging in this offline setting.\n\nThe authors design their approach with two objectives in mind:\n It avoids the caveats of offline learning with the Bellman operator like TD learning or Value Distribution Corrected Estimation (ValueDICE) learning which are known to suffer from bootstrapping errors, especially in the offline case and with small datasets where OOD evaluations are inevitable.\nIt utilizes both the expert and the sub-optimal demonstrations with the goal of learning a policy that is as close as possible to the expert policy.\n\nTo achieve these goals, they design a policy loss function (termed ADR) that starts with expert behavioral cloning KL-div loss but adds negative KL-div loss (maximizing the KL-div) against the sub-optimal policies.\n\nIn order to learn this objective the authors suggest to learn an intermediate model of the expert policy P* and the sub-optimal empirical policy P^ with Variational Autoencoders (VAE) and use the log-ratio P*/P^ as a (log) Importance sampling weight for the policy loss. This results in a simple two-steps optimization problem (1) learn VAEs, (2) learn policy and avoids the Bellman operator.\n\nIn order to compensate for the scarcity of expert demonstrations, they add to the VAE loss a discriminator loss (GAIL [1] like, termed ADE) that lets adding samples from the sub-optimal dataset at the expense of altering the expert policy estimation.\n\nThe paper then analyses some theoretical bounds on the expected learned policy and its overall performance (i.e. value function). \n\nIn terms of experiments, first, the authors demonstrate some similarity of the frequency of states in the learned policy with respect to the expert policy. Then they conduct experiments in 3 test benches (Mujoco, Androit, Kitchen) and show superiority of their method over algorithms like Behavioral Cloning (BC), CQL, IQL+Oracle, IQL+OTR, IQL+CLUE which are designed for similar (yet not always identical) settings (offline RL, potentially with reward and without sub-optimal datasets) .\n\nReferences:\n1. Ho, Jonathan, and Stefano Ermon. \"Generative adversarial imitation learning.\" Advances in neural information processing systems 29 (2016)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper clearly motivates the problem, reviews relevant methods, and identifies their limitations.\n\nThe proposed approach is straightforward and avoids the complexities of Bellman-based TD learning."
            },
            "weaknesses": {
                "value": "1. Is ADR an appropriate optimization objective?\n\nIt is questionable whether the ADR loss is an adequate objective function for this RL setting. First, intuitively when the sub-optimal policy gets closer (or converges) to the expert policy, ADR converges to a degenerated objective function. Next, even if the sub-optimal policies are sufficiently different from the expert policy, the ADR policy does not converge to the P* solution or even sufficiently close to the P* solution. For example in the discrete case, the ADR objective is:\n\npi = argmin sum_i pi_i \\log(\\frac{P^_i}{P*_i})\n\nLets take for example the case where P* = (0.6, 0.2, 0.2) and P^=(0.2, 0.2, 0.6) (no states in this case, only 3 actions). The ADR policy converges to pi = (1, 0, 0), since actions with similar probability  between P* and P^ do not add weight to the policy. This means that ADR tends to increase the actions with positive ratio (where P*>P^), reduce (up to zero) the actions with negative ratio (P^ > P*) and ignore the actions where P*~=P^ which fails to capture the expert policy.\n\nMoreover, ADR tends to yield policies with smaller action-support than the expert policy (more deterministic, where some expert actions are never taken) this can lead to potentially sub-optimal results as some actions may be crucial for functional policy. This trend of smaller support of ADR can be both observed in my toy example and in Fig 2 where we find that in both ANT and HALFCHEETAH ADR has smaller state support than the expert policy.\n\nProposition 5.2 tries to upper-bound the KL distance between the learned policy and the expert policy. As indicated in this analysis, when the distance between P* and P^ is small (delta) then the bound gets looser.\n\n2. Are P* and P^ really needed and does ADR really avoid bootstrapping and OOD errors?\n\nGiven that one wish to design the ADR policy, it is not clear why the learned P* and P^ models are required. In a sense, if data is sufficient we can sample the backward KL-div D-KL(P*,pi) and D-KL(P^,pi) from the data. Therefore, assuming that the reason for incorporating P* and P^ models is to compensate the data scarcity (i.e. using both the sub-optimal and the expert dataset to regress for the ADR policy), one must ask himself whether ADR really avoids the caveats of bootstrapping (in the sense of using an estimated quantity as part of the ground truth for another model) as the ground truth for ADR includes the estimated log-ratio log(P^/P*) which also means that we evaluate P* out-of-distribution over the sub-optimal dataset, which essentially requires another bandage (the ADE auxiliary loss).\n\n3. Value bounds: Do they provide insight?\n\nProposition 5.3 tries to lower-bound the value differences between the ADR policy and the expert policy. In general for any two policies p and q we have |V_p(s0) - V_q(s0)| <= 2R_max / (1 - \\gamma). For ADR, the upper bound contains several more multiplicative factors, however, since elements like \\Delata C and D_TV(d*|d^D) are constant terms (i.e. not diminishing to zero) at any case, this bound is too loose to provide concrete reasoning about the similarity between pi and P*.\n\n4. Is ADE a proper way to handle the demonstrations scarcity?\n\nADE is presented as a practical auxiliary loss to ADR that should mitigate the problem of small expert dataset at the expense of altering the P* network which does not represent anymore the estimation of the expert policy. I\u2019m not sure this is a sound solution for the need to compensate for small dataset as it basically lead to a deadly tradeoff where as we increase the weight of the auxiliary loss we better avoid OOD samples but on the same time we move away from our desired function (i.e. the expert policy). There are other alternatives that should be considered here, for example to estimate the KL-div between the expert and the sub-optimal empirical policy (as it is done in [1]), this lets you train both networks from samples from both D* and D^ without altering the structure/behavior of both networks.\n\nreferences:\n1. Freund, Gideon Joseph, Elad Sarafian, and Sarit Kraus. \"A coupled flow approach to imitation learning.\" International Conference on Machine Learning. PMLR, 2023."
            },
            "questions": {
                "value": "1. Regarding VAE for density estimation: \nThere appear to be issues in the formulation of the ELBO loss (Eq. 5), which should align with Eq. 4 in [1] (CLUE). Additionally, what motivates the choice of this conditional VAE structure (with actions as input/output and state as context) over simpler density estimators like normalizing flows? You do not leverage latent space information in the algorithm (as CLUE does), and evaluating \\log\u2061 P\u2217(a\u2223s) and \\log P^*(a|s) requires Monte Carlo sampling. \nCould you clarify this choice and how you approximate \\log \u2061P\u2217(a\u2223s) and \\log P^*(a|s)?\n\n2. Regarding theorem 4.2 (theorem D.1 in the appendix). Can you clarify the last move in Eq 18 in the appendix, which probabilistic model do you assume the policy follows?\n\nreferences:\n1. Liu, Jinxin, et al. \"Clue: Calibrated latent guidance for offline reinforcement learning.\" Conference on Robot Learning. PMLR, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Adversarial Density Regression (ADR), a novel supervised Imitation Learning (IL) framework learning from expert and suboptimal data. Different from prior works, it minimizes the KL divergence between the learner and the expert behavior while diverging from the sub-optimal behavior. The paper proves that such objective is equivalent to weighted behavior cloning with weights being the log probability ratio on the given state-action pairs of expert and sub-optimal policy's behavior density, and estimates the weight with a VAE density estimator. The author proves the convergence of ADR theoretically and demonstrate its effectiveness on several testbeds."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The work is well-written and easy to follow. Though there are many mathematical notations and derivations in this paper, they are clearly introduced in a proper order. In particular, every formula in Sec. 3 and 4 is properly and briefly explained (without too much text between each formula) and highlighted with bold fonts.\n\n2. The idea of this work is simple, intuitive but effective: it proposes a weighted behavior cloning method where the weights are probability ratio learned by VAE. The final formula of weights is straightforward, and using VAE to model distribution is a natural approach.\n\n3. The results are not only tested on many environments (including mujoco, kitchen and adroit) with proper ablations (including loss components, upper bound of ADR, training stability and OOD risky analysis), but also guaranteed by theoretical derivations."
            },
            "weaknesses": {
                "value": "1. The literature investigation could be improved. \n\na) While imitation learning can indeed be categorized as mentioned in the \"imitation learning\" part of the related work, the paragraph did not show sufficient connection between related work and this work (e.g. adding a sentence like \"our work falls into the category of offline LfD\" would be much better). \n\nb) The work claims that it is a DICE-type framework in its abstract, but there is no literature investigation for DICE at all (and as my next point stated, the summary for DICE is inaccurate). In fact, I feel that this work is very different from DICE because the foundamental idea of DICE is to match occupancies, while in this work \"occupancy\" is not involved; the VAE is essentially learning *policies*, not *occupancies*; they are each other's Lagrange dual variables. In fact, I suggest authors to check TAILO [5], which introduces a much simpler way (originated from DICE works such as SMODICE) for estimating *occupancy* ratio, and is also a streamlined one-step supervised framework with log probability ratio for weighted Behavior Cloning (BC) without any RL.\n \n\nc) The introduction for DICE is inaccurate. Not all DICE works consider KL-divergence - SMODICE [1] uses $\\chi^2$-divergence for many of its tested environments. There are also many DICE works that rely on Wasserstein distance, such as PW-DICE [2] and SoftDICE [3]. Also, the objective (Eq. 5 of their paper) of DemoDICE [4], which is one of your baseline, does not seem to fit in your Eq. 4; they have two KL terms instead of one with a linear reward term in the objective.\n\n2. The last step of Eq. 18, which turns $\\pi_\\theta(a|s)$ into $\\|\\|\\pi_\\theta(\\cdot|s)-a\\|\\|^2$, is based on the assumption of Gaussian policy which is not mentioned (consider discrete policies where actions cannot subtract). This constraint of the policy being Gaussian could be a potential limitation for the proposed method.\n\n**Minor Issues**\n\n1. There is no y-axis in Fig. 7 and Fig. 8, which makes the message from this figure unclear (usually judging from the figure the standard deviation is quite large, but this does not seem to be the case compared to the table in Appendix F).\n\n2. line 98, \"Latent\" in \".../Latent representations\" should be lower case.\n\n3. It would be better to add \"\u03bb=0\" on \"ablation of ADE and DWR\" part to help the readers understanding the ablation.\n\n**References**\n\n[1] Y. J. Ma et al. Smodice: Versatile offline imitation learning via state occupancy matching. In ICML, 2022.\n\n[2] K. Yan et al. Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching. In ICML, 2024.\n\n[3] M. Sun et al. SoftDICE for Imitation Learning: Rethinking Off-policy Distribution Matching. ArXiv, 2021.\n\n[4] G-H Kim et al. DemoDICE: Offline Imitation Learning with Supplementary Imperfect Demonstrations. In ICLR, 2022.\n\n[5] K. Yan et al. A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories. In NeurIPS, 2023."
            },
            "questions": {
                "value": "I have several questions:\n\n1. Why does this work choose VAE instead of normalizing flow or diffusion model for estimation of density weight? One does not need approximation in Eq. 17 for normalizing flow, and diffusion models are stronger generative models than VAE (for ratio estimation, see diffusion DPO [1, 2]). An ablation on this would be great.\n\n2. How does the ablation \"ADR without DWR\" work? From my understanding, DWR is the crucial step that retrieves the policy to learn.\n\n3. Around line 187, the authors claim \"Some off-policy offline frameworks... overly conservatism constrains the exploratory capacity of policies, limiting their ability to adapt and improve beyond the demonstrations provided.\" This is true. But the problem is, since your algorithm is an offline, it is necessary to overcome such issue since you will not explore beyond your dataset at all? Or are the authors arguing that they have a better inductive bias for OOD area than the common pessimisitic principle (in this case there are some more recent improvements such as MCQ [3])?\n\n**References**\n\n[1] B. Wallace et al. Diffusion Model Alignment Using Direct Preference Optimization. In CVPR, 2024.\n\n[2] K. Black et al. Training Diffusion Models with Reinforcement Learning. ArXiv, 2023. \n\n[3] J. Lyu et al. Mildly Conservative Q-Learning for Offline Reinforcement Learning. In NeurIPS, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}