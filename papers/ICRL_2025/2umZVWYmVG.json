{
    "id": "2umZVWYmVG",
    "title": "Assessing Large Language Models for Valid and Correct Code Reasoning",
    "abstract": "Frontier large language models (LLMs) consider reasoning as first-class citizens: they learn to refine their reasoning process and try different strategies during training. Thereby, when prompted, can think through problems and respond better with proper reasoning. For programming tasks, this makes code reasoning a must. In this paper, we propose the task of Code Execution Simulation (CES) as a proxy for evaluating the code reasoning capabilities of LLMs. CES defines the notions of valid or invalid reasoning process, which enables it to promptly (1) determine where the execution simulation diverges from ground truth for incorrect output predictions (essential to understanding limitations of LLMs in code reasoning) and (2) identify suspiciously correct output predictions (essential to understanding reasoning shortcuts, hallucinations, or potential data leakage). In addition to evaluating LLMs\u2019 execution reasoning on a program with a single test, CES measures their reasoning consistency across tests with the same or different prime path coverage. This enables it to evaluate the code reasoning of LLMs in a spectrum: strong, weak, and random. Our results show that LLMs, to a great extent (82.32%), follow a valid reasoning process (results in 30.79% correct and 51.53% incorrect output predictions). However, their reasoning is mostly random (55.59%) or weak (41.69%), which explains their weakness in programming tasksthat require flow- or path-sensitive program analysis to succeed.",
    "keywords": [
        "LLM Reasoning",
        "Code Execution Reasoning"
    ],
    "primary_area": "causal reasoning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=2umZVWYmVG",
    "pdf_link": "https://openreview.net/pdf?id=2umZVWYmVG",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces Code Execution Simulation (CES), a new benchmark aimed at advancing the evaluation of large language models (LLMs) in code reasoning, particularly for complex programming tasks. CES addresses limitations in existing techniques, which lack comprehensive flow sensitivity and diagnostic capability, by unifying output prediction with key intermediate state evaluations\u2014focusing on flow-sensitive execution paths. CES prompts LLMs at essential decision points (e.g., loop variables, conditions) and leverages adaptive in-context examples for clarity, providing a scalable framework that supports diagnosing reasoning divergence and consistency across varying test coverages. Evaluating thirteen models, including GPT-4 Turbo, Gemini-1.5 Pro, CodeLlama, DeepSeekCoder, Magicoder-S, SemCoder-S, and StarCoder2, on the HumanEval dataset of Python problems, the study finds that while LLMs generally exhibit a high rate of valid reasoning steps (82.32%), their reasoning quality remains predominantly random (55.59%) or weak (41.69%), often falling short in complex flow-sensitive tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "I think the paper is bringing out an important research question.\n\nThe general idea of expecting that LLMs can emulate code if we want to use them for more general software engineering tasks is an interesting one. I would encourage the authors to continue along this research direction."
            },
            "weaknesses": {
                "value": "Overall I think the paper is not ready for publication yet. The writing of the paper could be improved in places. \nFor example, in Equation 1 the notation is not clear. CES and GT should have something to differentiate from the variables in the equation. In Figure 7, the radar plot and legend are unclear. \n\nThe definition of prime paths being between any two program points requires justification. Could the authors justify this decision. I can imagine that the there are a lot more inputs and dependencies at some intermediate point. An alternative that seems natural would be to consider acyclic paths from the start of a program to some intermediate point. This way the inputs are clearly defined as the inputs to the program.  \n\nRQ4 is the most important part of the paper. However, the results are underwhelming currently. The fact that there is no correlation between an LLM correctly emulating a piece of code and the LLM doing well on the programming task for that same piece of code does not support the hypothesis of the paper. Are there other explanations for this observation?\n\nThough I agree with the authors that it would be better if we the LLMs could also emulate the code, I do think this is neither necessary nor sufficient to be able to find bugs, as an example. A lot of humans also find bugs by just pattern matching based on their experience. \n\nI would recommend that the authors explore programs outside of HumanEval, perhaps also exploring other programming languages (C/C++, for instance). The reason being that these programs and programming languages are \"too simple\" and might require detailed understanding of the program semantics. Perhaps using more complex C/C++ programs involving bitwise operations, pointer arithmetic, etc. and looking at  tasks requiring a more detailed semantic understanding of the program (such as finding security vulnerabilities) might be more conducive to proving the hypothesis of the paper."
            },
            "questions": {
                "value": "- Why not use paths that always start from the beginning of the program?\n\n- Are there other explanations for RQ4?\n\n- Why restrict to HumanEval programs? \n\n- Did you explore other programming languages other than Python?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a Code Execution Simulation (CES) task to evaluate how well current language models understand and are able to execute code. The tasks is simulated the execution of a program by following the execution trace and producing the intermediate states. They introduce two aspects that go beyond code execution results correctness: checking if the simulated execution trace deviates from the correct program execution trace, and identifying situations where the model gets the right answer through questionable means. They also investigate how consistently these models perform with different test cases covering different execution paths. They find that LLMs still struggle with execution, especially for tasks that are more control flow sensitive."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper provides a very thorough investigation of LLMs' capability for code execution. They not only provide a reasonable framework to define strong or weak code execution capability but also have detailed error analysis. They also investigate more than 10 models across small to large, closed to open. This will be a valuable resource for readers interested in the capabilities of current LLMs.\n- It is interesting to study the \"invalid reasoning path,\" which they define as incorrect intermediate output but correct end results or branch selection, etc. It shows how the model may not follow exactly how to execute the instructions for the current state, unlike a program, and then still get the final answer correct.\n- Many other insights are backed by results from many different models. For example, they also investigate the consistency of code execution ability across different test inputs that cover different paths and show that most LLMs are not consistent in the sense that while they can execute some test cases successfully, even with test cases going through the same path, they often may still get them wrong."
            },
            "weaknesses": {
                "value": "The paper thoroughly investigates many aspects related to the execution path, like strong vs weak reasoning etc. However, it is not clear if the impact of variable values is discussed. For example, it isn't clear how things like large intermediate integers or long lists would affect the CSE results."
            },
            "questions": {
                "value": "- It is said that valid reasoning is at 83.32% but still only has a low accuracy of 30.79% being correct. Isn't this a bit misleading for the reader before looking into the definition of valid reasoning? The valid reasoning looks like anything but the invalid reasoning, and the invalid reasoning is not defined by whether the intermediate prediction results are wrong. So the valid reasoning containing errors should not be a surprising thing, right?\n- Is there a typo in line 362 or line 20 about the number for valid reasoning? (83.32 vs 82.32)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The presented paper proposes a new method for assessing the capability of LLMs to predict intermediate states of program execution. The method picks specific/relevant parts of the program such as branching conditions and loops and prompts the LLM to predict the state at these lines during execution of the program with a specific input. The authors then analyze how well the LLM prediction aligns with the program state and use this to assess the capability of LLMs to correctly and consistently reason about program states and to diagnose at which point the LLM starts incorrect predictions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-written and nicely structured. The figures and tables are well formatted and legible.\n- The story is engaging and the tackled topic interesting.\n- The proposed method promises improvements of previous work regarding the ability to pinpoint errors made by LLMs during reasoning at a lower inference cost."
            },
            "weaknesses": {
                "value": "1) Choice of predicted values\n\nThe proposed method compares to previous work that assesses the internal state of the program at other positions. It is not clear why exactly the proposed positions introduced in Sec 3.1. (branch taking, predicates, return value) specifically are the mainly relevant positions. The main argument appears to be that these are the most relevant values to detect and diagnose inconsistencies, and predicting further values would confuse the model.\n\nIt is possible that such assertions hold for the given dataset (or even more general programs) but I did not find any evidence pointing in this direction.\n\n2) Confusing Definitions of valid/invalid reasoning\n\na) No correspondence to \"consistency\"\n\nThe authors mark any reasoning as invalid (Sec 3.3.) if an intermediate state (i.e. predicate) is incorrectly predicted but the consequence is predicted correctly (i.e. the branch-taking based on the predicate). This appears to not accurately capture whether the _reasoning_ was indeed wrong, since the intermediate state and consequence could still be _consistent_ (i.e. for \"if p or not p:\", it does not matter what is predicted for p to correctly predict that the branch is taken) and thus not represent a case of incorrect reasoning. It could or could not be that in the given dataset the introduced invalid reasoning always constitutes incorrect reasoning, but such evidence is missing from the paper.\n\nb) Incorrect outputs are valid reasoning\n\nThe definition of \"valid reasoning\" includes (by definition) all instances where the model outputs incorrect output. This naming is confusing at best, since I would not expect that incorrect instances can constitute valid reasoning. As already mentioned in 2) this is due to a lack of evaluation of _consistency_ which I would consider indicative of reasoning.\n\n3) Weak performance at CES may imply subpar benchmark design\n\nIn Sec 5.2 the authors mention many cases of \"suspiciously correct\" outputs based on natural language reasoning and inconsistent with the produced code reasoning. My interpretation of this would be that the presented code evaluation is potentially unnatural and confusing to the language model and thus artificially reduces performance, where free-form reasoning in natural language allows the models to correctly derive a result. Interesting counter-evidence for such an interpretation would be that models also often override correctly reasoned code states with incorrect (i.e. biased through function names) natural language reasoning results.\n\nSimilarly in Sec 5.5. weak correlation of models in CES and other related program understanding tasks do not necessarily imply that models are subpar reasoners, instead it could also imply that CES is not a format in which models can effectively express their code understanding. \n\n\nThe following are some smaller points that left me confused after reading:\n- In Sec 5.1. the authors mention that there is no control for the coverage of test cases on programs. This appears weird, it would be interesting to somehow establish a controlled experiment for different path coverage. The detailed Figure 6 partially makes up for this.\n- Figure 7 is very difficult for me to parse, especially the legend, but also the choice of chart format, respective the choices of grouping (it might make more sense to overlay models on triangular LO, CO, LC chart?)\n- Figure 8: The instruction to the model reads \"You are given a piece of Python code and its _output_\" while the model is clearly given _input_ below. I hope this is a typo, otherwise it might have implications for the presented numbers.\n- In Sec 5.2. \"Invalid Reasoning\" it reads \"[\u2026] LLMs with good performance in valid reasoning also make more invalid reasoning\". This seems contradictory since reasoning is either valid or invalid, and the sum of it should be constant - thus increasing one would necessarily decrease the other. Please do clarify what is meant by this statement."
            },
            "questions": {
                "value": "Please provide a short statement or clarification to the points raised above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Code Execution Simulation (CES), a framework for evaluating code reasoning capabilities in large language models (LLMs). CES measures LLMs\u2019 capacity to predict both output and intermediate program states, including loop variables, iterables, conditional predicates, and branches. Except for the prediction accuracy, CES can identify whether models have valid reasoning process and can determine their reasoning consistency by executing test cases with different prime path coverage. Through experiments on multiple LLMs, the authors find that while LLMs can achieve a high level of valid reasoning (82.32%), their reasoning strength remains inconsistent, often performing at random (55.59%) or weak (41.69%) levels."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper presents a novel framework, Code Execution Simulation (CES), to assess code reasoning in LLMs. Code execution capability is an important aspect to evaluate LLMs.\n- CES's design is diagnostic. By defining the notions of valid or invalid reasoning process, it can detect suspiciously correct output predictions under invalid reasoning.\n- CES uses a novel reasoning consistency metric to benchmark LLMs' code reasoning abilities as strong, weak and random, by executing multiple tests per program with same or different prime path coverage."
            },
            "weaknesses": {
                "value": "- The primary weakness in this work lies in using code execution simulation as a measure of a model\u2019s code reasoning abilities, which is debatable. While I agree with the authors' choice of evaluating reasoning validity (valid or invalid) and reasoning consistency as indicators of reasoning ability, as they reflect the model's understanding of code logic, the execution process itself requires substantial computational accuracy and strict adherence to instructions. For example, executing `a = b * c` demands multiplication skills, and executing `x = a[5]` requires precise indexing. The relationship between these computational abilities and code reasoning capabilities remains a research question. A model can easily compute `2 * 3`, yielding correct outputs in simple cases, but as inputs scale in complexity, the model's computational skills are challenged. However, this does not necessarily imply a lack of logical understanding or reasoning capability regarding the code\u2019s logic. Thus, code execution simulation is inherently complex, and the authors do not sufficiently discuss this in the paper.\n- The definition of the 'invalid reasoning process' is ambiguous. In Equation 4, the authors consider a compound property to be 'invalid' when it contains both correct and incorrect predictions. However, the example provided here involves the loop variable `o` and the loop iterable `zip(evens, odds)`. According to the definition given in Section 3.1, these two do not belong to the same property.\n- The authors found in Section 5.5 that CES seems to have no correlation with other coding tasks, but they did not analyze the reasons for this. Is it because CES or bug-related tasks cannot represent the model's code reasoning ability, or do they focus on different aspects of reasoning ability? The authors also did not use other code comprehension tasks, such as code summarization, etc.\n- It seems that there are several 'why' questions left unanswered in the evaluation. Why the predictions differed from ground-truth values? Why LLMs make suspiciously correct output predictions. The authors have relied solely on case analysis without providing quantitative data analysis."
            },
            "questions": {
                "value": "- Do the authors use the same prompts for different LLMs? How does the in-context learning examples affect the models' performances?\n- To what extent does CoT (Chain of Thought) contribute to the results? Considering the hallucination phenomenon in LLMs , could the authors perhaps sample the output multiple times to observe the model's pass@k results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}