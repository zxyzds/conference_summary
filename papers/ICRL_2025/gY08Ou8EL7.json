{
    "id": "gY08Ou8EL7",
    "title": "Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh",
    "abstract": "Generalizable reconstruction of an animatable human avatar from sparse inputs and corresponding high-quality rendering conditioned on a given pose faces two main challenges: First, generalizable methods, which are needed for fast reconstruction, avoid scene-specific optimization but instead rely on data priors and inductive biases extracted from training on large data. However, at reconstruction time, information is limited as only a small number of sparse inputs are available. Note, we operate on a small set of images showing a human in possibly different but not multi-view consistent poses. Second, rendering is preferably computationally efficient yet of high resolution.\nTo address both challenges we augment the recently proposed dual shape representation, which combines the benefits of a mesh and Gaussian points, in two ways. To improve reconstruction, we propose an iterative feedback update framework, which successively improves the canonical human shape representation during reconstruction. To achieve computationally efficient yet high-resolution rendering, we study a coupled-multi-resolution Gaussians-on-Mesh representation. We evaluate the proposed approach on the challenging THuman2.0 and AIST++ data. Our approach reconstructs an animatable  representation from sparse inputs in less than 1s, renders views with 95.1FPS at $1024 \\times 1024$, and achieves  PSNR/LPIPS*/FID of 24.59/111.26/51.42 on THuman2.0, outperforming the state-of-the-art in rendering quality.",
    "keywords": [
        "Generalizable human rendering",
        "error feedback",
        "dual representation"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=gY08Ou8EL7",
    "pdf_link": "https://openreview.net/pdf?id=gY08Ou8EL7",
    "comments": [
        {
            "summary": {
                "value": "This paper tackles the challenge of creating an animatable human avatar from just a few images. To make reconstruction faster and more accurate, they introduce an iterative feedback approach that gradually refines the results. For high-quality, efficient rendering, they also develop a multi-resolution Gaussians-on-Mesh technique, which balances detail and speed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The feedback mechanism in this generalizable method is notably innovative, refining predictions by addressing visible errors from input views, resulting in more accurate and detailed reconstructions within a second.\n- Achieving human avatar reconstruction from images in different poses is highly practical, offering greater flexibility than multi-view approaches.\n- The paper is well-written, well-structured, and clear."
            },
            "weaknesses": {
                "value": "- The model does not appear to have been trained on two datasets and can support both tasks simultaneously, limiting its ability to handle mixed input types effectively.\n- The method's heavy dependence on SMPL priors poses a risk; if the estimations are inaccurate, especially for loose clothing, it can lead to significant errors\n- The method shows significant performance degradation when applied across domains, particularly affecting the facial region, as in the video. Besides, the novel pose representation displays black hands in the video. \n- Minor issues: line 156 \"estimate\" to \"estimation\", line 234 translation -> scale?"
            },
            "questions": {
                "value": "- In Figure 1, the mesh appears to be a GT-mesh rather than an SMPL model. Is the GT mesh utilized in your approach\uff1f\n- Given that Gaussian splatting has a strong tendency to overfit, how is pose generalization achieved in your method? Are the displayed novel poses primarily from the dataset, or do they include truly unseen poses?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper tackles the problem of fast, sparse-view 3D articulated avatar modeling. Specifically, the paper proposes to improve Gaussian-on-Mesh method by using a dual-resolution mesh: the low-resolution mesh is used for efficient feed-forward computation, while the high-resolution mesh anchors Gaussians to represent fine details. Furthermore, the approach the sparse input images as cues to extract pixel-aligned features to iteratively improve the 3D avatar. After training, the network can reconstruct 3D avatars for a given identity in under 1-2 seconds.\n\nThe experimental results show quantitative and qualitative improvement over previous methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The presented method has the following strengths:\n- The method shows fast, efficient reconstruction, and this is always a very welcome property for 3D avatar modeling.\n- The method shows promising results on reconstructing out-of-distribution avatars.\n- The method gets rid of the Gaussian split/growing heuristics, which are sometimes annoying to tune/control.\n- The qualitative and quantitative results verify the effectiveness of their proposed design choice.\n\n3D avatar modeling, the problem tackled in this paper, is an important topic in vision/graphics due to its wide impact on many fields (entertainment, sports, virtual try-on, etc). Overall, the presented approach makes sense and the results look promising,"
            },
            "weaknesses": {
                "value": "The paper has the following weakness:\n- Missing comparisons to GoMAvatar: as mentioned in line 121-123, the presented method is derived from GoMAvatar. It therefore makes sense to compare against GoMAvatar. The iterative-feedback bears a close similarity to [1], which also utilizes pixel/texel align features to improve fidelity. It would be great if the paper could discuss about these relevant works.\n- The writing can be further improved: for example, the sentence in line 143-145: ```As a fix, to regularize the Gaussians and to ease the animation, pairing with parametric models like FLAME or SMPL helps``` may not be as straightforward as saying ```Prior work [cite] regularizes the Gaussians and enables animation using parametric models such as FLAME and SMPL```. Also, in line 180-181 ```given as additional input the target camera ...``` seems unnecessary, as line 175-179 already described the input. \n- Mesh topology limits the geometry that can be captured, as shown by the hairs in Figure 4 (also pointed out in Appendix C).\n- Missing memory profile: perhaps I have missed it, but I did not see discussions related to the memory/GPU vram consumptions for the proposed method. How much VRAM does it take to train/render the avatars? Does it work on commodity-level GPUs?\n- Potentially susceptible to body pose misalignment issues: when the body pose is imperfect/does not overlap with the correct body part, the feedback network will extract features from incorrect locations, and thus impact the reconstruction quality.\n\n[1] Drivable Volumetric Avatars using Texel-Aligned Features, SIGGRAPH 2022"
            },
            "questions": {
                "value": "Please address/discuss about the weakness mentioned above if they are not already in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a method for generating high-quality, animatable 3D human avatars from sparse images. It proposes a multi-resolution Gaussians-on-Mesh representation, where low-resolution mesh is used for efficient geometry and high-resolution Gaussians for high-quality avatars. This paper also introduces an iterative feedback update mechanism to refine the model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The method achieves fast 3D human avatar reconstruction from sparse inputs, making it suitable for real-time applications.\n2. The multi-resolution Gaussians-on-Mesh representation balances efficiency and rendering quality."
            },
            "weaknesses": {
                "value": "1. The experimental comparisons are incomplete without recent works[1,2,3] that report better metrics. A comparative analysis or discussion of technical differences is needed.\n2. The cross-domain generalization shows concerning facial artifacts despite the proposed iterative feedback. What causes these failures? If it's a data bias issue, why showcase this case? Consider discussing limitations and potential solutions (e.g., face-aware regularization).\n3. The ablation study focuses only on hyperparameters rather than analyzing the effectiveness of core components. Need evaluation of:\nIterative feedback mechanism; Coupled-multi-resolution representation; Module-wise contribution analysis.\n\n\n[1] 3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting, CVPR2024\n[2] GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh, CVPR2024\n[3] HUGS: Human Gaussian Splats, CVPR2024"
            },
            "questions": {
                "value": "Please refer to my weakness part. I also have some questions about the videos supplied.\n1. Why not show all the input images in Cross-domain generalization part? Without all input frames, it is hard to evaluate the performance. \n2. Please make the input number of images clear in Novel pose synthesis part. The novel pose performance is impressive if it is achieved by a single reference image."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a generalizable human rendering approach using a learned iterative feedback update mechanism. The method leverages a coupled multi-resolution Gaussian-on-mesh representation and demonstrates state-of-the-art results on benchmark datasets such as THuman 2.0 and AIST++."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe proposed iterative update net is a novel idea and is clearly motivated.\n2.\tThe writing is clear and easy to follow, with a well-structured introduction and related work section.\n3.\tExtensive experimentation on benchmark datasets shows clear performance improvements over previous methods (Table 1, Table 2, Table 3).\n4.\tThe authors provide a comprehensive ablation study, which helps justify the effectiveness of iterative step choice and subdivision."
            },
            "weaknesses": {
                "value": "1. In the related works section, the authors categorize human rendering into \"per-scene optimized\" and \"generalizable\" approaches. However, there is no discussion on \"large reconstruction model-based\" human rendering approaches [1,2]. It would be valuable to compare the proposed method\u2019s performance (in terms of inference time, training time, resolution and PSNR) with such large reconstruction models.\n\n2. The proposed method employs a multi-resolution Gaussian-on-mesh representation, but a direct comparison with traditional mesh-based human representations [3, 4, 5] is missing. It would strengthen the paper if the authors could discuss the advantages or trade-offs between these representations ((in terms of rendering quality, animation flexibility, FPS).\n\n3. The method is built on SMPL/SMPLX models, which might limit its effectiveness in rendering loose clothing, such as skirts or dresses (see  examples in https://zhaofuq.github.io/NeuralAM/). Including examples of such scenarios would enhance the evaluation of the method\u2019s generalizability.\n\n[1] Weng, Zhenzhen, et al. \"Single-view 3d human digitalization with large reconstruction models.\" arXiv preprint arXiv:2401.12175 (2024).\n\n[2] Chen, Jinnan, et al. \"Generalizable Human Gaussians from Single-View Image.\" arXiv preprint arXiv:2406.06050 (2024).\n\n[3] Liao, Tingting, et al. \"Tada! text to animatable digital avatars.\" 2024 International Conference on 3D Vision (3DV). IEEE, 2024.\n\n[4] Zhang, Xuanmeng, et al. \"Getavatar: Generative textured meshes for animatable human avatars.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[5] Liao, Tingting, et al. \"High-fidelity clothed avatar reconstruction from a single image.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "While the paper demonstrates promising results, I am curious about the method\u2019s generalizability across diverse scenarios. In particular, on the project page, it would be insightful to evaluate the animation quality of cross-domain examples (e.g., drastically different body types, dynamic poses, or motion sequences). Addressing these aspects would provide a clearer understanding of the robustness and adaptability of the proposed approach"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}