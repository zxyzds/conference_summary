{
    "id": "5xwx1Myosu",
    "title": "Expressivity of Neural Networks with Random Weights and Learned Biases",
    "abstract": "Landmark universal function approximation results for neural networks with trained weights and biases provided impetus for the ubiquitous use of neural networks as learning models in neuroscience and Artificial Intelligence (AI). Recent work has pushed the bounds of universal approximation by showing that arbitrary functions can similarly be learned by tuning smaller subsets of parameters, for example the output weights, within randomly initialized networks. Despite the role of biases in shaping a neural network's response to changes in context, as demonstrated by a wealth of neuroscience literature and in-context learning in AI, it is an open question whether universal approximation results can be shown when only biases are learned. The current paper answers this question. We provide theoretical and numerical evidence demonstrating that feedforward neural networks with fixed random weights can approximate any continuous function on compact sets. We further show an analogous result for the approximation of dynamical systems with recurrent neural networks. Our results are relevant to neuroscience, where they demonstrate the potential for behaviourally relevant changes in dynamics without modifying synaptic weights, as well as for AI, where they shed light on multi-task methods, like bias fine-tuning, and in-context learning.",
    "keywords": [
        "random neural networks",
        "recurrent neural networks",
        "plasticity",
        "deep learning",
        "neuroscience",
        "multi-task learning"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5xwx1Myosu",
    "pdf_link": "https://openreview.net/pdf?id=5xwx1Myosu",
    "comments": [
        {
            "summary": {
                "value": "Previous work has investigated the expressivity of feed-forward neural networks (FNNs) when only subsets of parameters are trained (ie. only the output layer, normalization parameters, etc\u2026). In the same vein, the authors introduce a method of training feed-forward neural networks by randomly sampling fixed weights and subsequently learning only the biases, termed bias learning. They provide theoretical and empirical evidence that demonstrates that FNNs trained through bias learning can approximate any continuous function on compact sets - meaning that they are theoretically as expressive as fully-trained FNNs.\n\nThey start with a theoretical treatment of bias learning where they carefully define their terms and introduce their theorems. A simplified version of the rigorous proof is as follows: 1) Train a fully connected network (N1) where the weights are constrained to lie in some fixed range. 2) Create a new network (N2) by randomly sampling the hidden neuron weights from the fixed range in 1. 3) After sufficient sampling, there exists a subnetwork of neurons in N2 that is \u2018identical\u2019 to the neurons in N1. 4) By training the biases, the outputs of neurons outside of this subnetwork can be removed, leaving N1 from N2 bias training. They provide a similar proof for recurrent neural networks (RNNs).\nNext, the authors provide empirical evidence supporting their theory and explore the expressivity of bias-learned networks. They do this in multiple ways, including performing multi-task learning with bias learning in 7 tasks (MNIST, KMNIST, Fashion MNIST, etc.), comparing bias learning and mask learning in FNNs, and applying bias learning on an RNN trained on both an autonomous and non-autonomous dynamical system. The main takeaways are as follows: 1) multi-task bias learning leads to emergence of task-specific functional organization revealed by clusters of activation patterns measured by task variance, 2) compared to mask learning, bias learning had less sparse solutions and higher unit variance values, 3) bias learning in RNNs can succeed in time-series forecasting of non-linear dynamical systems with high enough gains."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Overall, there is strength in its novelty of proving that bias learning in neural networks can have high expressivity that performs almost as well as a fully-trained network. This is significant because bias learning trains fewer parameters than a full network.\n- Nature of bias learning is more behaviorally relevant in the context of tonic inputs, intrinsic cell parameters, threshold adaptation, and intrinsic excitability\n- The theoretical proofs are very thorough, and backed up by numerical proofs."
            },
            "weaknesses": {
                "value": "- In response to bias learning having fewer parameters to learn, no data was shown on training time\n- Little background was given on mask learning (the mask learning section was also super short - felt less developed relative to other parts of the paper). This is important because two of their highlights in the results relate to mask learning.\n- Makes claims (i.e. lines 253 - 259, lines 418-420) that could have been easily backed up by data, but were not.\n- Figure 1 color scheme is weird\n- Practically, not sure how exciting this is (i.e. other models can do what this model does - it's just that they use a different approach)\n- The work seems highly related, in spirit, to neural tangent kernel approaches and other methods that consider wide NNs, but no references to that work were made."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors show that both feedforward and recurrent neural networks can act as universal function approximators for functions and dynamical systems respectively, even when only biases are learned. They propose an alternative proof for the theorem that masking is all you need (Strong Lottery Ticket Hypothesis), and extend that and the bias result to RNNs approximating dynamical systems. They authors demonstrate their results using simple simulations, and discuss relevance to AI and neuroscience."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors connect well to the neuroscience and AI/ML literature and explain the proofs in an intuitive manner. \nThe extension to RNNs and dynamical systems is also commendable as these often receive reduced attention in the ML community.\nThe issue with the \"gain\" g in the weight distribution is well brought out."
            },
            "weaknesses": {
                "value": "The section 3.3.2 on the Lorenz system is not clearly written and the architecture and external input to the network are not clear. \n\nAt first glance, the result seems to be a simple extension of the masking theorem of Malach et al 2020.  The difference with that proof should be made clear."
            },
            "questions": {
                "value": "In Section 3.3.2 - The authors write RNN, but then say that the recurrent state is given? Also what is provided as an external input? Is it the recurrent state? The difference between the 'standard' and the 'self-sustained' networks is not clear. To me the self-sustained way is the standard, and if somehow the recurrent state is provided (at each time step?), then the network is just acting as a feedforward network. Then in this case, I suspect that to actually use the RNN (usual self-sustained way) to learn the dynamics, the authors would need a lot more units.\n\nThe authors have not explained how (positive & negative) biases may arise in neuroscience if not by synaptic weights. As they mention threhsold changes etc. change the neural gain (and possible have a strongly non-linear effect). What about the role of inhibition and other brain areas switching parts of the network on and off?\n\nThe authors should bring out the differences between their proof and the Malach et al proof."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this work, the authors prove a universal approximation result for the expressivity of neural networks with frozen weights but trainable biases. In particular, they build upon well-known universal approximation results of feedforward and recurrent architectures, and show via a simple mask learning-like argument that sufficiently large networks with randomly chosen weights can be constructed to approximate any function (feedforward) or finite-time trajectory (recurrent). They conduct experiments comparing fully trainable architectures to bias-learning variants, demonstrating that bias-only learning can achieve reasonable performance on some simple tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The main expressivity results shown are well-explained and seem mathematically tight. Considering that these results made use of a reduction to mask learning problems, the authors also do a good job discussing the relationship between their findings and those of the mask learning literature."
            },
            "weaknesses": {
                "value": "A crucial aspect of this work with regards to its practical relevance is how large a bias-trained network needs to be to achieve similar performance to a fully trained network. Surely the scaling is better than the extreme network expansions constructed for the existence proofs, but how much better? The authors allude to performance as a function of trainable parameter count scaling similarly to fully trained networks, and thus only needing quadratic scaling in layer width, but they only evidence this explicitly with comparisons to mask learning networks, which in my view are also less expressive than standard networks (for fixed number of parameters). I would like to see a more detailed investigation of this question. For example, could the authors extrapolate from the MNIST experiments (Fig. 1a) whether the required scaling is indeed quadratic? I imagine this scaling would also depend significantly on the task difficulty and the frozen weight initialization. \n\nOverall, the tasks the authors used to demonstrate efficacy of bias-only learning seemed restrictively simple, by the standards of both the machine learning and computational neuroscience literature. In particular, for the RNN experiments, only simple 1D pattern generation tasks were considered. I would be interested in seeing how biased-trained RNNs perform on simple \"cognitive\" tasks often used to assess task-trained RNNs in the computational neuroscience literature (e.g., interval timing, delayed match-to-sample). For example, I imagine that any task that requires the construction of many stable fixed points could be quite difficult for bias-learning RNNs, and might require prohibitive scaling of network size compared to fully-trained counterparts (e.g., N-bit flip flop)."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Note: I\u2019m not an expert in the field of pruning and universality.\nThe authors show that random neural networks with trained biases are universal approximators. This is shown both for feedforward and recurrent networks. The authors use an argument that is similar to masking, and use negative biases to implement the masking. Numerical simulations show that several benchmarks reach similar performance when training biases or training weights as well."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The questions of pruning, masking and universality are all important questions for neuroscience and machine learning. In neuroscience, it is known that many cell-autonomous adaptation mechanisms exist (e.g., spike threshold adaptation), and the wide distributions of firing rates hint that these properties could be a form of long-term plasticity as well.\nProving universality results has opened the door for more application-oriented research in the past. The combination of mathematical proofs with systematic simulations and a wide literature review is a strength."
            },
            "weaknesses": {
                "value": "My main concern is the scaling of network size, which seems exponential and is also missing from the main text. The results of Malach et al 2020 suggest that masking is weaker than weight-pruning, unless the size of the network is exponential.\nIf I understand correctly, I expect a scaling of (R/eps)^n. Because there are n independent event with probability (R/eps). Line 944 (appendix) states a scaling, which by approximating log(1+z)=z is indeed (1/eps)^(n^2).\nGiven this large scaling, and the results of Malach et al that with polynomial scaling neuron-pruning is weak, it seems strange that bias learning is as strong as weight learning. Indeed \u2013 the actual numerical results do not show comparable performance. As the tasks become harder, the gap widens. Also when controlling for the number of learned parameters, bias learning is still weaker."
            },
            "questions": {
                "value": "1.\tDefinition 2: bounded by gamma?\n2.\tLine 186 large hidden layer width. Can you provide a rough estimate? What is the scaling? I assume it is roughly (R/eps)^n. In Malach et al, the size was polynomial. If I understand correctly, line 944 gives such scaling, and by approximating log(1+z)=z it is indeed (1/eps)^(n^2).\n3.\tFig E2. It is hard to compare what happens from 5000 parameters or so. Perhaps a logarithmic or ratio plot would help.\n4.\tFig E2 \u2013 fully trained was still better than bias-only, even when controlling for parameter number. Do you know why this is? Is this related to the main concern raised above?\n5.\tLine 304. Correlation between TV and bias. Was this computed for every unit, and then averaged within each cluster?\n6.\tLine 304 If the theory is aligned with training, then the number of units should be much higher than simply the square of fully trained network. If Figure E2 suggests otherwise, then why expect the mechanism of the proof to hold? Further \u2013 what are the values of biases? Are some of them extremely low \u2013 effectively shutting down neurons?\n7.\tCorrelation values between mask and bias \u2013 what is the correlation between different realizations of the training process?\n8.\tLine 461 \u2013 similar scaling in RNN and FNN. Figure E2C shows that the fully trained saturates at 64^2 parameters. The bias trained network is shown up to 64^2 parameters, so we can\u2019t see whether fully-trained RNNs with less than 64^2 parameters behaves similarly to FNN.\n9.\tLine 462 stability in larger windows. Perhaps I didn\u2019t understand this, but is this really stability or simply more test sets? Because the network is fed the true dynamics, is there a meaning to larger windows?\n10.\tFigure 4C \u2013 How does the fully trained network generalize in this scenario?\n11.\tLine 481 \u2013 I think some discussion of scaling should go into the main text, even if the proof is in the appendix.\n12.\tLine 483 \u2013 task-selective clusters similar to fully trained. This was not shown. Specifically, quantification of how task-selective are bias-trained vs. fully-trained networks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}