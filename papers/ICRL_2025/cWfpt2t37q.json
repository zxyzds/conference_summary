{
    "id": "cWfpt2t37q",
    "title": "From Risk to Uncertainty: Generating Predictive Uncertainty Measures via Bayesian Estimation",
    "abstract": "There are various measures of predictive uncertainty in the literature, but their relationships to each other remain unclear. This paper uses a decomposition of statistical pointwise risk into components associated with different sources of predictive uncertainty: namely, aleatoric uncertainty (inherent data variability) and epistemic uncertainty (model-related uncertainty). Together with Bayesian methods applied as approximations, we build a framework that allows one to generate different predictive uncertainty measures.\n\nWe validate measures, derived from our framework on image datasets by evaluating its performance in detecting out-of-distribution and misclassified instances using the AUROC metric. The experimental results confirm that the measures derived from our framework are useful for the considered downstream tasks.",
    "keywords": [
        "Uncertainty quantification",
        "Bayesian methods",
        "Statistics"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "We show, how one can get predictive uncertainty measures using Bayesian approximations of statistical risk.",
    "creation_date": "2024-09-17",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cWfpt2t37q",
    "pdf_link": "https://openreview.net/pdf?id=cWfpt2t37q",
    "comments": [
        {
            "title": {
                "value": "Author's response"
            },
            "comment": {
                "value": "Thank you for your thoughtful feedback and for taking the time to review our work. We appreciate your comments and overall positive evaluation!\n\n---\n\n> \u201cwe see how the EU at the predictive level is not quantifiable any more, since it gets washed away by taking the expectation E_\\theta ..\u201d\n\nThis is indeed a very interesting and important remark!\n\nIndeed, providing a point estimate, instead of considering all possible models from the model posterior, will \u201cvanish out\u201d all the epistemic uncertainty.\n\nThis vanishing out is indeed present in some of the instances within our framework, namely MI, RMI, etc., and those, with constant predictions (e.g. R_exc(3,2) and R_exc(2, 3)). However, some instances, namely EPBD, do not suffer from this drawback. Therefore, within our framework, there are measures that avoid this integration, potentially capturing epistemic uncertainty more carefully. \nHowever, in our experiments, it is not that concrete, that it is always better.\n\nBut nevertheless, we agree that this is a very important conceptual thing that we need to incorporate, and we will do it in a revision.\n\n\n__Literature on credal sets__\n\nWe thank the reviewer for pointing this out. We will add the discussion of credal based sets uncertainty in related work and we will acknowledge the papers you mentioned. Thank you!\n\n\n__On misprint__\n\nThank you for pointing this out! You are right, we will fix it.\n\n---\n\nIf there are any additional areas where we could provide further clarity, strengthen our results, or improve the paper to better meet your expectations\u2014and possibly enhance your evaluation\u2014we would be most grateful for your suggestions!"
            }
        },
        {
            "comment": {
                "value": "__Questions__\n\n> Previous work by Hofman et al [1] seems to be related and at least partially overlapping, but not cited. For parts of Table 1 it seems Hofman et al. give a similar result also based on proper scoring rules. From this it seems that a general framework for uncertainty measures already exists. Can the authors clarify how their proposed framework extends beyond this?\n\nAs mentioned above, due to the double-blind review policy, we are limited in how much detail we can provide about this specific paper without risking a violation of anonymity. \n\nThe paper you referenced is not the first introducing generalization of uncertainty measures based on proper scoring rules. Please note, that the authors of that paper are informed on the existence of our paper, and we recently contacted them.\n\nWe hope you understand our constraints due to the review process, and we are happy to provide further clarification within those limits."
            }
        },
        {
            "title": {
                "value": "Response to Additional Feedback [2/2]"
            },
            "comment": {
                "value": "> The notation of e.g. R_Exc^{(1, 3)} is hard to keep track of as a reader. Would it be acceptable to rename \u201cExcess\u201d risk to epistemic risk, or Bayes risk to Aleatoric Risk? Perhaps the indices may also be substituted with abbreviations so the connection to the approximations is clear?\n\nWe agree that using more intuitive names could help make the paper easier to read. However, we chose to use the conventional terms \"Excess Risk\" and \"Bayes Risk\" from statistics to stay consistent with established terminology.\n\nRegarding your suggestion to rename \"Excess Risk\" to \"Epistemic Risk\" and \"Bayes Risk\" to \"Aleatoric Risk,\" we considered this option. We felt that keeping the traditional names would avoid confusion for readers familiar with the standard definitions in statistics.\n\nAs for substituting the indices with abbreviations to make the connection to the approximations clearer, we appreciate the idea. We did think about using abbreviations, but with so many possible combinations, it became challenging to create unique and meaningful names for all of them. We were concerned that this might make the notation even more complicated.\n\nWe appreciate your suggestion and are committed to improving the clarity of our work. Thank you for helping us make our paper better."
            }
        },
        {
            "title": {
                "value": "Response to Additional Feedback [1/2]"
            },
            "comment": {
                "value": "> In Section 6.4 it would be good to clearly point to where we can find evidence for which claim (table, row and column). This mainly applies to Lines 477-482. I cannot find the results that show R_exc(3,1) is better than both energy-based methods for misclassification detection, nor that R_Exc(3,1) is preferred over energy based measures for Soft-OOD.\n\nThank you for pointing out the need to clearly indicate where the evidence for our claims can be found. We appreciate your feedback and apologize for any confusion. Here are the specific locations in the paper where you can find the supporting results.\n\n*R_exc(3,1) is better than both energy-based methods for misclassification detection:*\n\n- Table 10, line 1366 (R_exc(3,1)) vs lines 1371-1372 (energies). All datasets.\n- Table 13, line 1480 (R_exc(3,1)) vs lines 1501-1502 (energies). TinyImageNet. Here the results are close (R_exc(3,1) is in the middle of two energy estimates).\n\n*R_Exc(3,1) is preferred over energy based measures for Soft-OOD:*\n\n- Table 3, lines 442 (R_exc(3,1)) vs lines 446-447 (energies). ImageNet-O.\n- Table 5, lines 1203-1204 (R_exc(3,1)) vs lines 1209-1210 (energies). CIFAR10C-[1, 2]\n- Table 9, lines 1339-1340 (R_exc(3,1)) vs line 1349 (energies). ImageNet-O.\n\n> Line 456 seems overstated \u201call instances of excess risk should perform worse [than Bayes/Total risk on misclassification detection]\u201d. This is true if there is more (separable) aleatoric uncertainty than epistemic uncertainty, but this is not true in general. For example in low data or high dimensional problems, this might not hold. Adding the word \u201ctypically\u201d or \u201cusually\u201d could be sufficient.\n\nYou are correct; our statement on Line 456 may be overstated. We considered the scenario where there is sufficient data, but as you pointed out, in situations with limited data or high-dimensional problems, this might not hold true. Excess risk does not always perform worse than Bayes or total risk in misclassification detection under all circumstances. We agree with your suggestion and will adjust the wording by adding \"typically\" or \"usually\" to reflect this nuance.\n\n> Lines 104-107 are repetitive with Lines 36-38.\n\nWe agree that these lines convey similar information. Our intention was to recap the key points to reinforce understanding for the reader. However, we understand your concern about redundancy. If you feel that this repetition is unnecessary, we are happy to remove one of these sections to improve the clarity of the paper.\n\n> The whole paper assumes Deep Ensembles as the BNN, but different behavior may be observed with different models. It could be good to show behavior with MC-Dropout or Flipout.\n\nYou are absolutely correct that different models may exhibit different behaviors, and exploring them could provide additional depth to our study.\nWe chose to focus on Deep Ensembles because they typically offer strong performance and are straightforward to train. Deep Ensembles are well-regarded for their effectiveness in uncertainty quantification, which aligns with the goals of our paper.\n\nHowever, we agree that evaluating other methods like MC-Dropout or Flipout could enhance our work. We will make our best effort to incorporate additional results using these approaches before the end of the rebuttal period.\n\n\n> Table 5 mentioned on Line 475 is actually in the appendix, but the text presents it as part of the main body.\n\nWe agree with the reviewer. We will fix it, thank you!\n\n> In the tables it should be clear whether the plus-minus indicates standard deviation, variance or standard error. If appropriate, it may be helpful to indicate the which parts of the tables relate to conclusions drawn (highlighting in bold may be useful).\n\nAgain, we agree with the reviewer and we will incorporate proposed changes. Thank you."
            }
        },
        {
            "comment": {
                "value": "__4. \u201cIt is unclear why the energy-based models are discussed, as they seem to have little relevance. Please clarify the relevance or remove this section\u201d__\n\nThank you for expressing your concern about the discussion of energy-based models (EBMs) in our paper. We appreciate the opportunity to clarify their relevance.\n\nIn our framework, EBMs naturally appear as a special case of Bayesian approximation. Specifically, the Excess Risk we analyze corresponds to the difference between energies in a Bayesian model induced by different potentials\u2014that is, the energy induced by the averaged potential versus the average energy generated by individual potentials. This connection is significant because EBMs have been previously used in out-of-distribution detection, although not within a Bayesian context.\n\nWe find it interesting that EBMs appear organically in our framework, and we believe this is a valuable finding to share. EBMs are a popular class of models for OOD detection. We have cited a key paper in this area [5], and we think that including another influential work [6] would strengthen this section.\n\nWe hope this explanation clarifies why we discuss EBMs in our paper. Please let us know if you have any further questions or if anything remains unclear.\n\n\n[5] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. \"Energy-based Out-of-Distribution Detection.\" Advances in Neural Information Processing Systems, 33:21464\u201321475, 2020.\n\n[6] Grathwohl, W., Wang, K. C., Jacobsen, J. H., Duvenaud, D., Norouzi, M., & Swersky, K. \"Your classifier is secretly an energy based model and you should treat it like one.\" International Conference on Learning Representations, 2020."
            }
        },
        {
            "comment": {
                "value": "__3. Concern on the definition of AU and EU. \u201cConsider for a source [3], Section 2.4.1.\u201d__\n\nThank you for bringing up this point and referencing the paper [3]. We appreciate your insight and believe this highlights the ongoing discussion about the definitions of uncertainty in the field.\n\nWhile there is general agreement that aleatoric uncertainty (AU) is irreducible and solely a property of the data, and that epistemic uncertainty (EU) is reducible, the specific details of these definitions can vary. In our work, we follow the definitions provided in [4], which we have cited and acknowledged.\n\nSpecifically, Section 2.3 of [4] states:\n> \"Aleatoric uncertainty refers to the irreducible part of the uncertainty, which is due to the nondeterministic nature of the sought input/output dependency, that is, to the stochastic dependency between instances x and outcomes y, as expressed by the conditional probability.\"\n\nAccording to this definition, AU focuses on the inherent randomness in the output given the input and does not explicitly include \"uncertainty in the inputs,\" as you mentioned.\n\nFurthermore, the same section defines EU as:\n> \"Model uncertainty and approximation uncertainty, on the other hand, are subsumed under the notion of epistemic uncertainty, that is, uncertainty due to a lack of knowledge about the perfect predictor, for example caused by uncertainty about the parameters of a model.\"\n\nBased on these definitions, we believe our approach aligns with established literature. We would appreciate it if you could clarify what you mean by using a \"more consistent and complete definition of aleatoric and epistemic uncertainty as a starting point,\" so we can address your concern more effectively.\n\nRegarding \"model misspecification\" as a source of epistemic uncertainty, we totally agree, and we want to emphasize that it is included in our consideration. In our framework, this is reflected in the definition Excess Risk. If the model is misspecified, the Excess Risk cannot be fully eliminated, thereby accounting for this source of uncertainty.\n\nRegarding aleatoric uncertainty, you noted that it includes \"uncertainty in the inputs or an otherwise stochastic relationship between the features and the labels.\" We agree with this perspective. In line 34 of our paper, we intended to convey the latter part of your statement, and we will revise the wording to make this clearer.\n\nAs for the \"uncertainty in the inputs,\" this is an insightful remark. In our current framework, we assume the common setup where we have access to the true inputs $x$ without noise. Considering scenarios where only noisy instances $\\tilde{x}$ are available is indeed interesting and could extend our work to new problem settings. While this is less typical in the literature, it presents a valuable direction for future research. We will include a corresponding remark in the text.\n\nThank you again for your thoughtful feedback. We hope this addresses your concern, and we are open to further discussion to clarify any remaining issues.\n\n---\n\n[4] Eyke H\u00fcllermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. Machine Learning, 110:457\u2013506, 2021."
            }
        },
        {
            "comment": {
                "value": "__2. \u201cthe usefulness of the proposed generalization\u201d__\n\nThank you for your feedback regarding the usefulness of our proposed generalization. We understand that you have concerns about how our generalization contributes to the field.\n\nIn the Strengths section of your review, you mentioned:\n> - \"By offering a better theoretical understanding of the commonly used uncertainty measures, we may progress the field of disentanglement.\"\n> - \"I think the paper provides a nice overview of different uncertainty measures and how they relate.\"\n\nWe appreciate these positive remarks and believe they highlight the usefulness of our generalization. Our paper introduces a unified framework that allows for the generation of different uncertainty measures, providing deeper theoretical insights into how these measures are connected.\n\nCould you please clarify what kind of strong evaluation or hypotheses you expect to follow from our generalization? Understanding your expectations would help us address your concerns more effectively.\n\n> \u201cOnly experiments 6.2 and 6.3 come with a hypothesis, but those hypotheses generally apply to uncertainty quantification and not the proposed generalization\u201d.\n\nWe would like to clarify that all our experimental questions (which can be viewed as hypotheses) arise from our proposed generalization. For example, in Section 6.1, we explore which choice of the function G is better, and we provide an answer to this question. Without our general framework, it would be challenging to formulate and address this question.\n\nSimilarly, the questions in Sections 6.2 and 6.3 are closely related to our generalization framework because we associate aleatoric uncertainty with Bayes risk and epistemic uncertainty with Excess risk. While it's true that one could define estimates of aleatoric and epistemic uncertainty in various ways, our framework offers a systematic approach to examine these concepts. We believe the relevance of these questions is important, as they help deepen the understanding of when Excess risk or Bayes risk is more useful, especially in the context of soft-OOD and hard-OOD data.\n\nFurthermore, the question in Section 6.4 is also relevant, as the connection to Energy-Based Models (EBMs) naturally arises from our framework as a special case of approximation.\n\nWe hope this explanation clarifies how our experimental questions and hypotheses are connected to our proposed generalization. Please let us know if this addresses your concern or if there are specific aspects you would like us to elaborate on."
            }
        },
        {
            "title": {
                "value": "Author's response"
            },
            "comment": {
                "value": "We thank the reviewer for his/her comprehensive review.\n\n\n__1. Relation to the prior work.__\n\nWe appreciate the reviewer's feedback regarding the relation to prior work.\n\n> \u201cparts of this are not entirely novel and the paper is not clear about this\u201d.\n\nCould reviewer please specify which parts he/she finds unclear? We would like to address any confusion and ensure our contributions are clearly presented.\n\n__Discussion on Schweighofer et al. (2023a)__\n\nWe would like to emphasize that we have cited Schweighofer et al. (2023a) and properly acknowledged their work in our paper. Their study builds on the critique raised by Wimmer et al. [1] (also cited in our work) and derives the EPKL measure, which, to the best of our knowledge, was first introduced by Malinin and Gales [2] three years earlier.\n\nWe believe there are key distinctions between our paper and the one by Schweighofer et al.:\n1. __Scope of Measures Considered:__ Schweighofer et al. focus specifically on \"information-theoretical\" measures, which in our framework correspond to a special case when using the Log Score. Our framework is more general and can generate a broader class of uncertainty measures.\n2. __Starting Point of Analysis:__ Their paper starts from the classical decomposition of predictive uncertainty, which already depends on model estimates (Equation 1 in their paper). In contrast, our work begins with the decomposition of risk, which depends on unknown quantities. This provides a different perspective and foundational approach.\n3. __Generation of New Uncertainty Measures:__ Their paper does not generalize or provide methods to create new uncertainty measures. Our framework, however, offers ways to derive new measures beyond those previously considered.\n\nWe kindly ask the reviewer to let us know if there are specific aspects that are not clear regarding our relation to prior work. We are more than willing to provide additional explanations or clarify any points to improve the paper.\n\n---\n\n[1] Lisa Wimmer, Yusuf Sale, Paul Hofman, Bernd Bischl, and Eyke H\u00fcllermeier. \"Quantifying Aleatoric and Epistemic Uncertainty in Machine Learning: Are Conditional Entropy and Mutual Information Appropriate Measures?\" In Uncertainty in Artificial Intelligence, pp. 2282\u20132292. PMLR, 2023.\n\n[2] Andrey Malinin and Mark Gales. \"Uncertainty Estimation in Autoregressive Structured Prediction.\" In International Conference on Learning Representations, 2021.\n\n__Discussion on Hofman et al. (2024)__\n\nThis paper indeed provides a similar derivation of uncertainty measures as ours. However, we can mention that the work that is was made available online 2 months after the first version of our paper was released. For this reason, we believe it would not be appropriate to cite it at this stage. Due to the double-blind review process, we cannot provide specific details about our preprint. \nPlease note, that the authors of that paper are informed on the existence of our paper, and we recently contacted them. We definitely will acknowledge their work as concurrent in the camera-ready version of our paper (if accepted).\n\nTo ensure fairness and adhere to the review guidelines, we have written a private note to the Area Chair (AC) or Program Chair (PC) regarding this matter.\n\n---\n\nReviewer mentioned: \n> \u201cThis disagrees with the problem statement on Line 45 which suggests that it is not clear how these measures relate to each other.\u201d\n\nWe would like to clarify that there is no contradiction. The paper by Schweighofer et al. (2023a) considers only Log Score-based measures, such as Mutual Information, EPKL, and Entropy. However, other measures like Variation Ratios and Mean Standard Deviations discussed in [3] (cited in Line 45) are not covered in their paper.\n\nOur paper shows that all these measures, including those not addressed by Schweighofer et al., are special cases within our general framework, derived using proper scoring rules and specific Bayesian approximations.\nWe hope this explanation clarifies the statement in Line 45. Please let us know if you agree that there is no contradiction.\n\n[3] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In International conference on machine learning, pp. 1183\u20131192. PMLR, 2017.\n\n---\n\n> \u201cConsider explicitly showing which parts of the generalization are novel, and which relationships are established (for example by [1] and [2]).\u201d\n\nThank you for this helpful suggestion.\n\nRegarding Schweighofer et al. (2023a) and Malinin and Gales (2021), we have acknowledged their introduction of EPKL. We will ensure that our paper clearly delineates our original contributions, such as the general framework that unifies a broader class of uncertainty measures.\n\nFor the work by Hofman et al., please refer to our earlier comment."
            }
        },
        {
            "comment": {
                "value": "__Questions__\n\n> Does the loss function to quantify uncertainty and the training loss function need to be the same?\n\nThank you for this very insightful question; we appreciate the reviewer's attention to this detail.\n\nInitially, we believed that matching the loss function for training and uncertainty quantification might be beneficial. We observed that this approach tends to perform slightly better on average. However, a more detailed analysis showed that this is not always the case. Sometimes using the same loss function for both training and uncertainty estimation leads to improvements, but other times it may not perform as well.\n\nWe also noticed that in very specific situations\u2014such as a particular combination of dataset, model architecture, and choice of G (for example, with the CIFAR100 dataset, training with the Brier score, and using the Spherical loss for uncertainty quantification)\u2014there were occasional drops in performance for a few random seeds. However, these instances were rare, and we did not observe consistent issues across our experiments.\n\nIn summary, while using the same loss function for both training and uncertainty quantification can sometimes offer advantages, it is not universally better. The effectiveness may vary depending on the specific conditions, and we recommend considering the particular context when choosing the loss functions."
            }
        },
        {
            "comment": {
                "value": "__5. Lack of training details__\n\n>\u201cWhat loss function was used to train the models?\u201d\n\nThank you for pointing this out. We agree that the details about the loss functions could be clearer. We used different loss functions generated by different instances of G. The specific loss function used for training is mentioned in the captions of the relevant tables.\nFor example:\n- In Tables starting from Table 5, we explicitly state the loss function in the caption.\n- In Table 2, we mention in the text (lines 392-393) that we used the same G for both the loss function and the evaluation. This means that, for instance, in Table 2 (Log), we used Cross-Entropy as the loss function and employed log-score-based uncertainty measures.\n- For Tables 3 and 4, we followed the same approach, as noted in their captions.\n\nWe are open to suggestions on how we can improve clarity in this regard. Would it help if we explicitly emphasized in Appendix H that the loss functions are specified in the table captions? Please let us know if this would address your concern.\n\n> \"How were the ensembles trained? Were they trained as completely independent networks, or did they share components?\"\n\nThe ensembles were trained as completely independent networks without sharing any components. We will explicitly mention this in both the experimental description section and in Appendix H.\n\nWould this clarification resolve your concern?"
            }
        },
        {
            "comment": {
                "value": "__4. Lack of Baseline Comparisons__\n\nThe reviewer says:\n\n> \u2018They [authors] also mention:\n\"Instead, we aim to verify whether different uncertainty estimates are indeed related to specific types of uncertainty.\"\nHowever, the paper fails to provide comparisons to modern estimates of uncertainty such as [3, 4], which could help contextualize their findings and strengthen the evaluation.\u2019\n\n\nWe appreciate your feedback and would like to clarify our approach. Our main goal was to check if different uncertainty estimates are related to specific types of uncertainty, namely aleatoric and epistemic uncertainty. To do this, we conducted two experiments in different downstream tasks, each designed to present a specific type of uncertainty. We then examined whether our estimates could capture these uncertainties.\n\nSince our focus was on exploring the uncertainty measures within our framework, rather than benchmarking all known uncertainty measures, we did not see the need to implement or compare with other methods. We believe this approach is appropriate for the scope of our theoretical contribution.\n\nWe are grateful for the references you provided to the recent preprints [3] and [4], which we were not aware of. We are happy to include them in the related work section of our paper.\n\nHowever, we believe that including these methods in our experimental comparison may not be directly relevant. For example, [4] proposes a computationally intensive approach where a diffusion model generates the parameters of a neural network. This method may be impractical for many real-world applications due to its high computational cost. Additionally, they used unconventional baselines in their experiments, making it difficult to assess how their approach scales to the datasets we used.\nRegarding [3], their work focuses on reinforcement learning tasks, not on computer vision classification, which is the domain of our experiments. Therefore, a direct comparison may not be appropriate.\n\nWe would like to emphasize that the main contribution of our paper is theoretical, as you kindly acknowledged in your review under the Strengths section. Our aim is to provide a unifying theoretical framework for uncertainty measures, and we believe our experiments support this goal."
            }
        },
        {
            "comment": {
                "value": "__3. Assumption of Well-Approximated Data Distributions__\n\nThank you for raising this important concern.\n\nYou mentioned: \n> \u201cThe framework relies on the decomposition of total risk into Bayes risk and excess risk, which assumes that the underlying true data distribution can be well approximated. In practice, this assumption may not hold in many real-world scenarios, especially with complex, high-dimensional data.\u201d\n\nWe would like to clarify that the decomposition into Bayes risk and Excess risk is a general concept that does not depend on specific assumptions, except for the existence of the true conditional distribution $p(y \\mid x)$. This distribution can be arbitrarily complex, depending on the problem at hand.\n\nIn supervised learning tasks like classification and regression, our goal is to approximate $p(y \\mid x)$ with an estimated distribution $\\hat{p}(y \\mid x)$, which is often modeled using neural networks in modern deep learning. Despite the complexity of the true distribution $p(y \\mid x)$, neural networks have shown practical success in approximating it effectively.\n\nWe are not entirely sure whether your concern about \"complex, high-dimensional data\" refers to the input features $x$ or the labels $y$. In either case, neural networks have been effective for both scenarios:\n- __High-Dimensional Inputs (x):__ Neural networks handle high-resolution images and other complex data types well.\n- __Complex Labels (y):__ They also perform successfully in tasks with many class labels, such as ImageNet classification with 1,000 classes.\n\nTherefore:\n\n- __Neural Networks as Approximators:__ Neural networks are suitable tools for approximating the true distribution $p(y \\mid x)$, even in complex, high-dimensional settings.\n- __General Decomposition:__ The decomposition into Bayes risk and Excess risk is general and does not rely on the assumption that p(y \\mid x) can be well approximated.\n\nWe hope this addresses your concern. If we have misunderstood your point or if there are specific aspects you would like us to elaborate on, we would appreciate further clarification."
            }
        },
        {
            "comment": {
                "value": "__2. Redundant Conclusions__\n\n> __\u201cGiven the lack of novelty in experimental design (see 1)...\u201d__\n\nThank you for your feedback. We would like to clarify our understanding of your concern. In your first point, you mentioned a lack of experimental evaluation rather than a lack of novelty in experimental design. Regarding our experimental design, we believe we addressed an interesting new problem by considering both soft-OOD (out-of-distribution) and hard-OOD data types. This approach allowed us to show that Excess risk is more suitable for soft-OOD data, while challenges may arise in hard-OOD scenarios. To the best of our knowledge, it was not explicitly evaluated in the previous work.\n\n> __\u201cthe conclusions drawn in the experimental section are similar to those in previous works [1, 2]\u201d__\n\nWe are a bit unclear about which specific conclusions you are referring to. The two papers you mentioned seem to address different issues:\n- __Wimmer et al.__ critique Mutual Information with respect to a set of introduced axioms.\n- __Schweighofer et al.__ introduce a new uncertainty quantification measure (EPKL) that satisfies more axioms.\n\nIt appears there are no shared conclusions between these papers and ours. Could you please specify which conclusions you believe are similar?\n\nMoreover, if there are shared conclusions, we view this as a positive sign. Consistency with previous work can reinforce the validity of our findings. Since our paper aims to generalize previous studies, it's reassuring that our conclusions align rather than contradict earlier research.\n\nCould you please clarify your concern? Why are consistent conclusions in a work that generalizes prior studies considered a drawback? Are there specific conclusions you believe overlap significantly?"
            }
        },
        {
            "title": {
                "value": "Author's response"
            },
            "comment": {
                "value": "We would like to thank the reviewer for their thoughtful feedback.\n\nWe would like to kindly clarify one point in your summary: while we conducted experiments on out-of-distribution detection, we also considered misclassification detection as a downstream task in our experimental evaluation.\n\n__1. Limited Scope of Experimental Section__\n\n> __\u201cexperimental section only considers image classification tasks, which have been extensively researched in the uncertainty community\u201d__\n\nWe appreciate your concern about the scope of our experimental evaluation. It's true that image classification is a well-explored area in uncertainty quantification research. However, despite extensive studies, there is still no \"silver bullet\" that perfectly addresses uncertainty quantification in this domain. This is why many different uncertainty measures have been proposed, each with varying degrees of success. What has been missing is a clear understanding of how these measures relate to each other.\n\nAs you kindly noted in your \"Strength\" section: \n> \u201cThe paper presents a solid theoretical contribution by unifying different predictive uncertainty measures\u201d...\u201dIt connects well-known uncertainty quantification methods (e.g., Mutual Information, Expected Pairwise KL Divergence) with a common theoretical foundation, which adds clarity and depth to the topic.\u201d\n\nWe believe our primary contribution is this unifying theoretical framework. By showing that various existing measures are special cases of a general approach, we provide new insights that add clarity to the field.\n\nIn the experimental part, we aim to showcase some of the important properties of the derived uncertainty measures. To achieve this, we focused on the image domain to ensure consistency and clarity of the results. Many recognized papers in machine learning research, particularly in uncertainty quantification, validate their ideas using a single domain like image classification. The papers you cited in your review [1, 2] also focus solely on this domain. While we understand the importance of broader experimental validation, we believe our focus is appropriate given the theoretical nature of our contribution.\n\nGiven that you acknowledged our \"solid theoretical contribution,\" we are a bit confused about why our contribution was evaluated as poor (1). Could you please clarify this point so we can better understand your concerns and improve our work?\n\n---\n\n[1] Lisa Wimmer, Yusuf Sale, Paul Hofman, Bernd Bischl, and Eyke H\u00fcllermeier. Quantifying aleatoric and epistemic uncertainty in machine learning: Are conditional entropy and mutual information appropriate measures? In Uncertainty in Artificial Intelligence, pp. 2282\u20132292. PMLR, 2023.\n\n[2] Kajetan Schweighofer, Lukas Aichberger, Mykyta Ielanskyi, and Sepp Hochreiter. Introducing an improved information-theoretic measure of predictive uncertainty. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023a."
            }
        },
        {
            "summary": {
                "value": "The authors introduce a framework for quantifying uncertainty by decomposing it into aleatoric and epistemic. The authors use Bayesian methods to approximate and derive a unified framework for generating uncertainty measures. The framework is validated through experiments on classification tasks for image datasets, specifically on out-of-distribution detection."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper is well written and clear.\n2. The paper presents a solid theoretical contribution by unifying different predictive uncertainty measures under a Bayesian risk decomposition framework. It connects well-known uncertainty quantification methods (e.g., Mutual Information, Expected Pairwise KL Divergence) with a common theoretical foundation, which adds clarity and depth to the topic."
            },
            "weaknesses": {
                "value": "### 1. Limited Scope of Experimental Section\n\nThe paper's experimental section only considers image classification tasks, which have been extensively researched in the uncertainty community. It could be strengthened by:\n\n   - **Broadening the task domain**: The paper could provide results on regression tasks to bolster the experimental validation.\n   - **Exploring state-of-the-art methods**: Consideration of more cutting-edge methods, such as generative models for uncertainty quantification, could enhance the experimental insights.\n\n### 2. Redundant Conclusions\n\nGiven the lack of novelty in experimental design (see 1), the conclusions drawn in the experimental section are similar to those in previous works [1, 2], which limits the novelty of the paper\u2019s contributions to the literature.\n\n### 3. Assumption of Well-Approximated Data Distributions\n\nThe framework relies on the decomposition of total risk into Bayes risk and excess risk, which assumes that the underlying true data distribution can be well approximated. In practice, this assumption may not hold in many real-world scenarios, especially with complex, high-dimensional data.\n\n### 4. Lack of Baseline Comparisons\n\nThe paper appears to lack comparison to strong baselines. While the authors state:\n\n> \"We emphasize that the goal of our experimental evaluation is not to provide new state-of-the-art measures or compete with other known approaches for uncertainty quantification.\"\n\nThey also mention:\n\n> \"Instead, we aim to verify whether different uncertainty estimates are indeed related to specific types of uncertainty.\"\n\nHowever, the paper fails to provide comparisons to modern estimates of uncertainty such as [3, 4], which could help contextualize their findings and strengthen the evaluation.\n\n### 5. Lack of training details\n\nThe authors appear to have omitted important training details in Appendix H. Specifically, it is unclear:\n\n   - What loss function was used to train the models?\n   - How were the ensembles trained? Were they trained as completely independent networks, or did they share components?\n   \nProviding these details is crucial for reproducibility and understanding how the proposed framework was implemented. \n\n[1] Lisa Wimmer, Yusuf Sale, Paul Hofman, Bernd Bischl, and Eyke H\u00fcllermeier. Quantifying aleatoric and epistemic uncertainty in machine learning: Are conditional entropy and mutual information appropriate measures? In Uncertainty in Artificial Intelligence, pp. 2282\u20132292. PMLR, 2023.\n\n[2] Kajetan Schweighofer, Lukas Aichberger, Mykyta Ielanskyi, and Sepp Hochreiter. Introducing an improved information-theoretic measure of predictive uncertainty. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023a.\n\n[3] Berry, Lucas, and David Meger. \"Escaping the sample trap: Fast and accurate epistemic uncertainty estimation with pairwise-distance estimators.\" arXiv preprint arXiv:2308.13498 (2023).\n\n[4] Chan, Matthew A., Maria J. Molina, and Christopher A. Metzler. \"Hyper-Diffusion: Estimating Epistemic and Aleatoric Uncertainty with a Single Model.\" arXiv preprint arXiv:2402.03478 (2024)."
            },
            "questions": {
                "value": "Does the loss function to quantify uncertainty and the training loss function need to be the same?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes a generalized framework from which common measures of aleatoric and epistemic uncertainty can be obtained. This is done through Proper Scoring rules and the notion of risk. The framework assumes Bayes (aleatoric) and Excess (epistemic) risk, and explains that the risk may be estimated differently with different ways to approximate the true function and the learned function. Following this they observe which proper scoring rule tends to perform well, and when aleatoric/epistemic uncertainty performs well for a certain task. Lastly, they briefly describe energy based measures."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The paper clearly outlines its contribution. By offering a better theoretical understanding of the commonly used uncertainty measures we may progress the field of disentanglement. \n- The paper covers a lot of ground in a limited span, discussing risk, proper scoring rules, different ways for risk estimation, and a substantial amount of experimental results. Overall I think the paper provides a nice overview of different uncertainty measures and how they relate."
            },
            "weaknesses": {
                "value": "- While the paper unifies various results into a shared formulation, parts of this are not entirely novel and the paper is not clear about this. For example, Schweighofer et al. (2023a)[2] already describe EPKL as a deviation from Mutual Information that does not assume the BMA to be the true model. They also discuss the relation to the reverse Mutual Information. This disagrees with the problem statement on Line 45 which suggests that it is not clear how these measures relate to each other. Similarly, results similar to Table 1 are already given by Hofman et al. [1] (see questions). Consider explicitly showing which parts of the generalization are novel, and which relationships are established (for example by [1] and [2]). \n- The current work fails to assert the usefulness of the proposed generalization. I would expect a strong evaluation to create hypotheses that follow from the generalization, and validate those experimentally. Only experiments 6.2 and 6.3 come with a hypothesis, but those hypotheses generally apply to uncertainty quantification and not the proposed generalization. This severely limits the impact of the paper. \n- At various locations the authors discuss that aleatoric and/or epistemic uncertainty are vaguely defined, but they do not try to maintain a precise definition following the literature. For example, they argue that epistemic uncertainty is the lack of knowledge of the right model parameters, but this would ignore model misspecification, which is accepted as a source of epistemic uncertainty. Similarly, on Line 36 authors say that aleatoric uncertainty is ambiguity in the label distribution, which ignores uncertainty in the inputs or an otherwise stochastic relationship between the features and the labels. I encourage the authors to attempt to use a more consistent and complete definition of aleatoric and epistemic uncertainty as a starting point. This is particularly relevant because the Pointwise Risk perspective seems to try to give an alternative precise definition. Consider for a source [3], Section 2.4.1.\n- It is unclear why the energy-based models are discussed, as they seem to have little relevance. Please clarify the relevance or remove this section. \n\n[1] Hofman, Paul, Yusuf Sale, and Eyke H\u00fcllermeier. \"Quantifying Aleatoric and Epistemic Uncertainty with Proper Scoring Rules.\" arXiv preprint arXiv:2404.12215 (2024).\n\n[2] Schweighofer, Kajetan, et al. \"Introducing an improved information-theoretic measure of predictive uncertainty.\" arXiv preprint arXiv:2311.08309 (2023).\n\n[3] Gawlikowski, Jakob, et al. \"A survey of uncertainty in deep neural networks.\" Artificial Intelligence Review 56.Suppl 1 (2023): 1513-1589.\n\n\n## Additional Feedback\n\n- In Section 6.4 it would be good to clearly point to where we can find evidence for which claim (table, row and column). This mainly applies to Lines 477-482. I cannot find the results that show R_exc(3,1) is better than both energy-based methods for misclassification detection, nor that R_Exc(3,1) is preferred over energy based measures for Soft-OOD. \n- Line 456 seems overstated \u201call instances of excess risk should perform worse [than Bayes/Total risk on misclassification detection]\u201d. This is true if there is more (separable) aleatoric uncertainty than epistemic uncertainty, but this is not true in general. For example in low data or high dimensional problems, this might not hold. Adding the word \u201ctypically\u201d or \u201cusually\u201d could be sufficient. \n- Lines 104-107 are repetitive with Lines 36-38. \n- The whole paper assumes Deep Ensembles as the BNN, but different behavior may be observed with different models. It could be good to show behavior with MC-Dropout or Flipout.\n- Table 5 mentioned on Line 475 is actually in the appendix, but the text presents it as part of the main body. \n- In the tables it should be clear whether the plus-minus indicates standard deviation, variance or standard error. If appropriate, it may be helpful to indicate the which parts of the tables relate to conclusions drawn (highlighting in bold may be useful). \n- The notation of e.g. R_Exc^{(1, 3)} is hard to keep track of as a reader. Would it be acceptable to rename \u201cExcess\u201d risk to epistemic risk, or Bayes risk to Aleatoric Risk? Perhaps the indices may also be substituted with abbreviations so the connection to the approximations is clear?"
            },
            "questions": {
                "value": "- Previous work by Hofman et al [1] seems to be related and at least partially overlapping, but not cited. For parts of Table 1 it seems Hofman et al. give a similar result also based on proper scoring rules. From this it seems that a general framework for uncertainty measures already exists. Can the authors clarify how their proposed framework extends beyond this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a family of uncertainty measures based on point-wise risks which allows to decompose the uncertainty into aleatoric (Bayes risk) and epistemic (Excess risk) parts. Plugging-in proper scoring rules into the derived uncertainty measures, they obtain expressions for computing the Bayes and Excess risk for the specific case of proper scoring rules. Finally, the authors compare three different methods for estimating the risks based on the Bayesian formulation, which they allows to derive many of the existing uncertainty measures as special cases."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Many parts of the work have appeared in some form before, but combining them is a novel idea. For example, generating the uncertainty measure from the point-wise risks has been explored in [1] and [2] as the authors note, but also in [3]. Proper scoring rules were explored in the context of the uncertainty quantification in [4]. Finally, the various Bayesian risk estimation techniques have existed in the literature for a long time. However, the authors do a good job connecting these pieces and showing how various uncertainty measures naturally arise and correspond to different ways of estimating the risk for a proper scoring rule. The authors supplement this with a deeper investigation of the relationships between the estimates and exploring which estimates are better under specific conditions. \n\n[1] Kotelevskii, Nikita, et al. \"Nonparametric uncertainty quantification for single deterministic neural network.\" Advances in Neural Information Processing Systems 35 (2022): 36308-36323.\n[2] Lahlou, Salem, et al. \"Deup: Direct epistemic uncertainty prediction.\" Transactions on Machine Learning (2021).\n[3] Liu, Jeremiah, et al. \"Accurate uncertainty estimation and decomposition in ensemble learning.\" Advances in neural information processing systems 32 (2019).\n[4] Gruber, Sebastian, and Florian Buettner. \"Better uncertainty calibration via proper scores for classification and beyond.\" Advances in Neural Information Processing Systems 35 (2022): 8618-8632."
            },
            "weaknesses": {
                "value": "While the work as a whole appears sound, I have a few concerns:\n1. How are the proposed risks affected by the model misspecification, e.g., prior misspecification of $p(\\theta)$? I think since the method is Bayesian in nature, it's important to outline the assumptions and highlight possible shortcomings.  \n2. No simulations. Assuming, we know the true $\\eta$, it would be interesting to see how the different estimators of the risk fare under (1) various distribution shapes (2) misspecification (of prior) (3) error from approximate posterior (if full posterior is not available). \n3. Experimental section is narrowly focused and not well-motivated. The authors devote their experiments to testing whether the aleotoric and epistemic uncertainties are captured well. For this, the authors compute the proposed Total, Bayes, and Excess risks and classify the samples into out-of-distribution and misclassified based on these values. They then test the accuracy of this classification. Since Excess risk is connected to the epistemic uncertainty that is connected to the out-of-distribution classification, the out-of-distribution classification accuracy must be better based on the Excess risk. Similarly for the Bayes risk and the accuracy of identifying misclassification, both of which are connected to the aleatoric uncertainty. First, these experiments hinge on the assumption that indeed the misclassification and out-of-distribution are related to the aleatoric and epistemic uncertainties -- a largely untestable assumption. Second, from the tables, I see very little difference between using the Total, Bayes, and Excess risks across all datasets and tasks: the differences while being statistically significant appear almost negligible, which raises questions whether the suggested measures and/or experiments are meaningful. \n4. No comparison with other UQ measures. The authors do not compare their proposed UQ measures with the existing approaches, which leaves the question of why they should be used in practice. \n5. (Minor gripe)  Limited practical utility. It appears to compute any of the proposed uncertainty measures, one needs an access to the posterior $p(\\theta|X,Y)$. Ultimately, this limits the set of methods to which the proposed approach could be applied."
            },
            "questions": {
                "value": "1. In Section 4.1 and Appendix E, the authors arrive at the conclusion that choosing the best estimate is often impossible in practice. namely, in Section 4.1, the authors say that exact relationship of the central estimate, $r(\\bar\\eta)$ with the other ones is not known so it's not clear which choice of the estimate for Bayes risk is best; while in Appendix E, the authors conclude that knowing which estimate to choose for the Excess is impossible to know apriori. Similarly, there is little guidance on the choice of the proper scoring rules. Overall, this leaves me with a 4 (proper scoring rules) * 9 (estimates) possible ways to get the Excess risk. Similarly, for the Bayes risk, we have another 12 possible choices. Are there any general or maybe domain-specific advice on how to choose the best UQ measure from among the proposed ones? Is it possible to have results on the error rate of each estimate for some common distribution families? In Appendix E, the authors note that none of the estimated yield a lower / upper on the Excess risk, which raises a question if it's possible to derive such an estimated bound? \n2. I think there are important points missing from the experiments description (Section 6). For example, how are the UQ measures computed given the outputs of the deep ensemble model? And more generally, given a model that only allows access samples from the posterior, are we resorting to a regular Monte Carlo? \n3. In Section 6, what is meant by 5 groups of ensembles? Do all of them have the same architecture? I think the wording here is confusing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors use a decomposition of statistical pointwise risk into components, associated with aleatoric and epistemic uncertainties. Together with Bayesian methods, applied as an approximation, they build a framework that allows to generate different predictive uncertainty measures. Experiments are shown that support the theoretical claims."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This is a nice and interesting paper that addresses a cogent problem in the literature, that is, the existence of many aleatoric and epistemic uncertainty measures based on the total risk decomposition, and how they're related to one another."
            },
            "weaknesses": {
                "value": "Taking a Bayesian route has a fundamental drawback, that was highlighted e.g. by Hullermeier and Waegeman (2021, already cited in the paper) and Caprio et al. (2024, Credal Bayesian Deep Learning). Consider the *posterior predictive distribution*,\n\n$$p(\\tilde y \\mid \\tilde x , D)=\\int_\\Theta p(\\tilde y \\mid \\tilde x, \\theta) p(\\theta \\mid D) \\text{d}\\theta = \\mathbb{E}_{\\theta \\sim p(\\theta \\mid D)} [p(\\tilde y \\mid \\tilde x, \\theta)],$$\n\nwhere $p(\\tilde y \\mid \\tilde x, \\theta)$ is the model distribution, and $p(\\theta \\mid D)$ is the parameter posterior. Posterior predictive $p(\\tilde y \\mid \\tilde x , D)$ tells us \"how likely\" output $\\tilde y$ is to be the \"correct one\" for input $\\tilde x$, given the knowledge encapsulated in the data $D$ we collected, which enters the computation via the posterior probability $p(\\theta \\mid D)$. Oftentimes, scholars claim that, in a Bayesian setting, the distribution on the parameters $\\theta$ captures (or is linked to) the epistemic uncertainty (EU) faced by the agent. This is a somehow agreeable premise, akin to a second-order distribution reasoning. If we accept this assertion, though, we see how EU at the predictive level is not quantifiable any more, since it gets washed away by taking the expectation $\\mathbb{E}_{\\theta \\sim p(\\theta \\mid D)} [\\cdot ]$. \n\nThis conceptual problem is relevant for this paper, especially because it is related to the ideas of Bayesian averaging of risk and Central label. I'd like the authors to add a discussion on this matter."
            },
            "questions": {
                "value": "See Weaknesses. The authors should cite https://arxiv.org/abs/2302.09656 and https://link.springer.com/chapter/10.1007/978-3-031-57963-9_1#:~:text=In%20their%20seminal%201990%20paper,bound%20to%20hold%20with%20equality when discussing about the mentioned problem (and possibly in the related work section). They may also cite other approaches that may be considered in the future, such as credal sets ones, studied by Yusuf Sale, Eyke H\u00fclleremeier, Paul Hofman, Michele Caprio, Viktor Bengs, Sebastien Desterke, Fabio Cuzzolin, Thierry Denoeux, Alessio Benavoli, and Cassio de Campos.\n\nAlso, I think there's a typo in line 198: shouldn't it be $\\eta_{\\theta \\mid D_{tr}}$ instead of $\\eta_{\\theta}\\mid D_{tr}$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}