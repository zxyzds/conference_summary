{
    "id": "Dci14asFPV",
    "title": "DPD-LoRA: Dynamic Prompt-Driven Low-Rank Adaptation for Improved Generalization",
    "abstract": "Fine-tuning large models presents technical challenges such as catastrophic forgetting and parameter inefficiency. Low-rank Adaptation (LoRA) and Propmt Learning can help address some of these challenges by providing more compact and flexible representations. However, Low-rank approximation is susceptible to outliers and relies on the assumption of a global low-rank structure, which can be suboptimal. Additionally, Prompt learning can overfit to specific downstream tasks, reducing its effectiveness when adapting to new tasks. In this paper, we introduce $\\textbf{Dynamic Prompt-Driven Low-Rank Adaptation (DPD-LoRA)}$, a novel framework that seamlessly integrates task-specific guidance using hierarchical prompt tokens and parameter-efficient adaptation. Unlike traditional methods, task-aware prompts in the DPD-LoRA dynamically influences low-rank updates in the model's parameters, thus enabling robust adaptation and generalization across diverse tasks and mitigating the forgetting issues. We further improve the learning capabilities of the model by breaking down the standard LoRA into multiple low-rank sub-matrices, without adding additional parameters. Further, we use an adaptive loss function to guarantee alignment with the distribution of the pre-trained model. Specifically, we introduce a self-regulated mechanism to improve stability, and a soft-gated selection mechanism to decide when to activate adaptation modules to improve performance on unseen categories. Extensive experiments on 11 benchmark datasets demonstrate that DPD-LoRA significantly outperforms state-of-the-art methods in both accuracy and generalization, offering a comprehensive solution to the challenges of fine-tuning large-scale models.",
    "keywords": [
        "Vision-Language Models",
        "PEFT",
        "Prompt Learning"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We propose DPD-LoRA, a method that uses learned prompt tokens to guide enhanced low-rank feature spaces, achieving improved performance in both adaptation and generalization for downstream tasks.",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Dci14asFPV",
    "pdf_link": "https://openreview.net/pdf?id=Dci14asFPV",
    "comments": [
        {
            "title": {
                "value": "Addressing Reviewers' Shared Questions (Concerns were frequently raised across multiple reviews)"
            },
            "comment": {
                "value": "We appreciate all reviewers' time and insightful comments. Given the relatively short rebuttal window, we have addressed as many concerns as possible, and we are more than pleased to address any remaining issues.\n\n**1. Hyper-Parameter Settings and Inconsistency in $\\alpha$, $\\beta$, $\\gamma$**\n\n1. We acknowledge the confusion caused by our notation, as many reviewers thought we have too many hyper-parameters and questioned why there are so many. In fact, our $\\alpha$, $\\beta$, and $\\gamma$ refer to the same weights and have the same values (0.1 for layer $(l-1)$ and 0.9 for layer $l$, as shown in our Table 4(a)). We initially chose different alphabetical symbols to distinguish between prompt-token side annotations and LoRA side annotations. To prevent confusion, we will unify them into the same representation.\n\n2. All hyper-parameters (including the number of sub-LoRA matrices $m$ and rank $r$) are provided in Appendix Table 4(a).\n\n3. For different loss weights $\\lambda$, we empirically defined them to ensure that each loss is within a similar magnitude. \n\n**2. Why We Decompose Plain LoRA into LoRSS**\n\nA straightforward answer is that we found under the same parameter budget (e.g., $3 \\times r = 3$ sub-LoRA setting vs. $r = 12$ plain LoRA setting), LoRSS consistently outperforms the plain setting in both Base and Novel evaluations.\n\nThis LoRSS idea is inspired by MoE-LoRA, but our approach is more parameter-efficient regarding learnable parameters. We decompose the LoRA matrix into sub-LoRA matrices under the same parameter budget, whereas MoE-LoRA duplicates the LoRA matrix into several LoRA matrices. For example, if we have $n$ sub-LoRA matrices with a fixed rank $r$ and $W \\in \\mathbb{R}^{d \\times k}$, MoE-LoRA's parameters increase to $n \\times (d \\times r + r \\times k)$, whereas our parameters remain at $(d \\times r + r \\times k)$. Another difference is that MoE uses a network to select the importance of matrices $A/B$, while we employ a single learnable parameter (the scaling factor) for each sub-LoRA matrix, which is more efficient. Finally, our downstream tasks are entirely different, highlighting the distinct applicability of our method. From our observations, under the same parameters (e.g., $3 \\times r = 3$ sub-LoRA setting vs. $r = 12$ plain LoRA setting), LoRSS always outperforms the plain setting.\n\n\n**3. Concerns About  Memory and Cost Efficiency**\n\n1. As shown in the appendix (page 18), we provide our algorithm. Our method follows a two-step training strategy, which does not require much memory cost. The duplication is implemented as a cached tensor only.\n\n2. Moreover, one reviewer asked if more efficiency metrics could be provided. We acknowledge that varying dataset sizes and different GPU architectures can make direct comparisons challenging due to discrepancies in training time and resource consumption; Our initial focus was on parameter counts (Table 4(b)) as a very intuitive measure because they remain fixed across various datasets and GPU architectures. However, to address these concerns, we have conducted additional experiments under consistent conditions to measure FLOPs, FPS, and training time per epoch. These metrics are provided below, along with comparisons to previous methods, to support our efficiency claims:\n\n| Method           | Params   | % CLIP | Base  | Novel | HM    | FPS (batch 4) | GFLOPs | Time (1 epoch) |\n|-------------------|----------|-----------------|-------|-------|-------|----------------|--------|------------------------|\n| CoOp             | 2048     | 0.002           | 82.69 | 63.22 | 71.66 | 104.5| 162.5  | ~32s                   |\n| CoCoOp           | 35360    | 0.03            | 80.47 | 71.69 | 75.83 |  53.3   | 162.5  | ~47s                   |\n| MaPLe            | 3.55 M   | 2.85            | 82.28 | 75.14 | 78.55 | 175.58         | 167    | ~28s                   |\n| ALIGN            | 3.58 M   | 2.87            | 83.38 | 75.51 | 79.25 | 72.6           | 314.6  | ~42s                 |\n| PrompSRC         | 31488    | 0.02            | 84.26 | 76.10 | 79.97 | 149.86         | 281.21 | ~27s                   |\n| **DPD-LoRA\u2020**    | **1.92 M** | **1.54**       | **84.80** | **76.80** | **80.60** | **82.51**   | **334.03** | **~40s**                   |\n| DPD-LoRA         | 4.72 M   | 3.79            | 85.67 | 76.91 | 81.05 | 81.57          | 334    | ~42s                   |\n\nOne more thing we hope reviewers may note is that even though our method has slightly higher GFLOPs due to additinal LoRA/LoRSS computations, **our convergence speed is much faster than any previous methods. Our method showcases accelerated convergence and favorable early-stage performance. Specifically, our method reaches better performance in just 7 epochs, which is 65% fewer epochs than the 20 epochs required by previous SOTA\u2014a reduction of over 65% in training time (as shown in Figure 1b and Figure 5).**"
            }
        },
        {
            "title": {
                "value": "Response to Reviewer 1iPg (2) with additonal experiments results"
            },
            "comment": {
                "value": "**Q:Mising efficiency-related metrics; Including a comparison of these metrics to baseline methods would further support the efficiency claims.**\n\nWe agree that providing specific efficiency metrics such as training time per epoch, and FLOPs would substantiate our claims of resource efficiency. However, it is relatively hard to provide solid efficiency-related metrics due to different GPUs and datasets, as this phenomenon is observed in previous papers. Our initial focus was on parameter counts (Table 4(b)) as a very intuitive measure because they remain fixed across various datasets and GPU architectures.\n\nWe acknowledge that varying dataset sizes and different GPU architectures can make direct comparisons challenging due to discrepancies in training time and resource consumption. However, to address your concerns, we have conducted additional experiments under consistent conditions to measure FLOPs, FPS, and training time per epoch. These metrics are provided below, along with comparisons to baseline methods, to support our efficiency claims:\n\n| Method           | Params   | % CLIP | Base  | Novel | HM    | FPS (batch 4) | GFLOPs | Training (1 epoch) |\n|-------------------|----------|-----------------|-------|-------|-------|----------------|--------|------------------------|\n| CoOp             | 2048     | 0.002           | 82.69 | 63.22 | 71.66 | 104.5| 162.5  | ~32s                   |\n| CoCoOp           | 35360    | 0.03            | 80.47 | 71.69 | 75.83 |  53.3   | 162.5  | ~47s                   |\n| MaPLe            | 3.55 M   | 2.85            | 82.28 | 75.14 | 78.55 | 175.58         | 167    | ~28s                   |\n| ALIGN            | 3.58 M   | 2.87            | 83.38 | 75.51 | 79.25 | 72.6           | 314.6  | ~42s                 |\n| PrompSRC         | 31488    | 0.02            | 84.26 | 76.10 | 79.97 | 149.86         | 281.21 | ~27s                   |\n| **DPD-LoRA\u2020**    | **1.92 M** | **1.54**       | **84.80** | **76.80** | **80.60** | **82.51**   | **334.03** | **~40s**                   |\n| DPD-LoRA         | 4.72 M   | 3.79            | 85.67 | 76.91 | 81.05 | 81.57          | 334    | ~42s                   |\n\nOne more thing you may note is that even though our method has slightly higher GFLOPs due to additinal LoRA/LoRSS computations, **our convergence speed is much faster than any previous methods. Our method showcases accelerated convergence and favorable early-stage performance. Specifically, our method reaches better performance in just 7 epochs, which is 65% fewer epochs than the 20 epochs required by previous SOTA\u2014a reduction of over 65% in training time (as shown in Figure 1b and Figure 5).**\n\n\n\nWe appreciate your feedback and are willing to reformat the table or provide additional explanations to enhance clarity if necessary."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer 1iPg (1)"
            },
            "comment": {
                "value": "We thank the Reviewer 1iPgfor many insightful comments. We answer the questions in what follows. Please let us know if further clarification is needed.\n\n**Q:Typos; LLaVa to LLaVA, PLoRA to DPD-LoRA**\n\nThank to Reviewer 1iPg for bring this to our attention, we will definitely revise these typos.\n\n**Q:Confusion; without any additional models prior; the abbreviation of \u201cPEFT\u201d**\n\nBy \"without any additional model priors,\" we mean that no other models are included except the pre-trained CLIP. This is a common practice in the PEFT field. Specifically, many methods import large models to learn stronger textual or visual representations. We have explicitly stated this in the related work section(line 154-156).\n\nRegarding the abbreviation \"PEFT,\" we mention it only once in the introduction (Lines 43-44). We explicitly use \"parameter-efficient fine-tuning\" for LoRA/adapter-like methods (Line 126) and refer to \"prompt learning\" as \"prompt-based efficient fine-tuning\" (Line 141). Therefore, we believe there should be no confusion on this point.\n\n**Q:The related work section lacks references to significant LoRA extensions(e.g. DoRA, SVFT, PISSA, and LoRA-XS);\nThe comparative experiments do not include related LoRA methods(e.g. DoRA and VeRA)**\n\nFirst, we acknowledge that our related work section can be improved by including more references to significant LoRA extensions such as DoRA, SVFT, PISSA, and LoRA-XS. We will update the manuscript to reflect the progress in this area of research.  \n\nHowever, conducting comparative experiments with these methods is beyond the scope of our current work. Our primary contribution is demonstrating that prompt tokens can provide additional task-specific guidance to LoRA. To our knowledge, we are the first to show that this approach is feasible. Integrating and comparing with other LoRA methods is an excellent suggestion for future work. Additionally, since plain LoRA works effectively in our experiments (as shown in Table 5), we anticipate that other LoRA-like methods would also perform similarly.\n\n**Q:The ablation study section only presents the individual performance of each component without evaluating the performance of their combinations.**\n\nThere may be a misunderstanding regarding our ablation study presented in Table 3. In this table, each row represents the performance of the model with components added cumulatively. That is, each component is included in addition to all the previous ones.\n\nThis cumulative approach is a conventional format used in many papers, as well as in our baseline [1,2], to evaluate the impact of each component both individually and in combination with others. As you suggested, the third row effectively represents the performance with two components combined, and the fourth row shows the combination of three components.**Thus, our ablation study already evaluates different combinations of components as per the recommendation of Reviewer 1iPg .**\n\n1.Khattak, M. U., Rasheed, H., Maaz, M., Khan, S., & Khan, F. S. (2023). Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 19113-19122).\n\n2.Khattak, M. U., Wasim, S. T., Naseer, M., Khan, S., Yang, M. H., & Khan, F. S. (2023). Self-regulating prompts: Foundational model adaptation without forgetting. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 15190-15200)."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer zPfS"
            },
            "comment": {
                "value": "We thank the Reviewer zPfS for many insightful comments. We answer the questions in what follows. Please let us know if further clarification is needed.\n\n**Q:This can seem a bit cluttered and redundant; a more concise summary and consolidation of related content would be beneficial.**\n\nThank Reviewer zPfS for insightful suggestion. Reviewer pw2m mentioned this as well, and we will revise the paper to focus on the main points. We want to emphasize that our main contributions are twofold: first, we are the first to prove that prompt learning can additionally provide task-specific guidance to LoRA (even in plain LoRA settings, as shown in Table 5 in the appendix); second, the proposed gating mechanism strengthens their connection.\n\n**Q:Sec 3.2; the explanation seems to treat prompts and LoRA separately, although the appendix provides a detailed explanation of their combined effect. As this is a crucial part of the paper, more clarity and detail in the main body of the text would be necessary.**\n\nThey are actually not separate. Our intention in Section 3.2 was to demonstrate the combined effect of prompts and LoRA within the Transformer architecture. Specifically, we introduce learnable prompts in both the textual and visual branches, which are then incorporated into the input sequences. These prompted inputs interact with the MHA mechanism as shown in Equation (2). Then we adapt the standard LoRA formulation to this prompted framework in Equation (3), which is directly influenced by the prompted inputs $X$. This equation illustrates that the output h is a result of both the original weights and the prompt-guided LoRA adjustment, highlighting their interconnected roles. However, we acknowledge that we might merge more details from the appendix. Thank Reviewer zPfS for pointing this out.\n\n\n**Q:The title of Section 3.3; The order of introduction and the content should correspond to the title.**\n\nThank Reviewer zPfS for bringing this to our attention; we will align the order of introduction and section contents.\n\n**Q:Analysis of the synergistic effects between LoRA and prompt learning**\n\nWe include this part in our Appendix Section D, and the interpretation based on mathematical derivation is also included in Section D5.\n\n**Q:Different weights allocation methods are used: \u03b1 and 1-\u03b1 for prompt tokens, and \u03b2 and \u03b3 for LoRA layers.**\n\nWe thank you for pointing out this issue that many reviewers care about. We acknowledge it causes confusion that our \n $\\alpha,\\beta,\\gamma$  are the same and have the same values (0.1 for $(l-1)$ and 0.9 for $l$, as you can tell from our Table 4(a)). The reason we chose different alphabetical symbols is that we wanted to separate prompt-token side annotations and LoRA side annotations. We will change them to the same representation to prevent confusion.\n\n**Q:Hyper-parameters configurations; An explanation for these fixed values would be necessary**\n\n* To ensure a fair comparison, we followed previous methods[1,2] for all prompt token settings, including deep prompt tokens, N_CTX, and learning rates.\n\n* Regarding the LoRA component, we initially experimented with rank settings commonly used in conventional LoRA[3], specifically r = 1, 4, 8. Higher ranks such as r = 32, 64 were not considered due to our aim to minimize the number of parameters. We observed that performance improved with increasing ranks in the set r = {1, 4, 8}. We then incrementally increased the rank until r = 12, where we found the performance to be better than at r = 13, prompting us to stop at r = 12.\n\n* For the quantity $m$, we have very limited choices when we have fixed rank. With r = 12 from our previous experiments, we tested all divisor combinations of r and m (i.e., r \u00d7 m = 12 \u00d7 1; 6 \u00d7 2; 2 \u00d7 6 ; 4 \u00d7 3; 3 \u00d7 4). Based on these experiments performance, we set the sub-LoRA matrices with r = 4 and m = 3 across all benchmarks.\n\n* Finally, for different $\\lambda$, we empirically defined them to ensure that each loss is within a similar magnitude.\n\n1.Khattak, M. U., Rasheed, H., Maaz, M., Khan, S., & Khan, F. S. (2023). Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 19113-19122).\n\n2.Khattak, M. U., Wasim, S. T., Naseer, M., Khan, S., Yang, M. H., & Khan, F. S. (2023). Self-regulating prompts: Foundational model adaptation without forgetting. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 15190-15200).\n\n3.Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer pw2m"
            },
            "comment": {
                "value": "We thank the Reviewer pw2m for many insightful comments. We answer the questions in what follows. Please let us know if further clarification is needed.\n\n**Q:I strongly suggest the authors consider identifying and focusing on the core components of their method.**\n\nThank Reviewer pw2m for insightful suggestion. Our proposed methods mainly focus on two things: first, prompts can additionally provide task-specific guidance to LoRA (even in plain LoRA settings, as shown in Table 5 in the appendix); second, the proposed gating mechanism strengthens their connection.\n\n**Q:It is unclear how Eqn (4) is optimized. Are $si$ and $AiBi$ learned simultaneously?**\n\nYes, $AiBi$ and $si$ are learned simultaneously. We provide our algorithm on page 18 of the appendix. \n\n**Q:How many sub-LoRAs are used, and why is it imperative to decompose a single LoRA into multiple sub-LoRAs essential? Do the learnable and share any functional overlap?**\n\nWe provided all hyperparameters in Table 4(a); we use a fixed 3 sub-LoRA matrices in all evaluations.\n\nThis LoRSS idea is inspired by MoE-LoRA [1], but our approach is more parameter-efficient in terms of learnable parameters. We decompose the LoRA matrix into sub-LoRA matrices under the same parameter budget, while MoE-LoRA duplicates the LoRA matrix into several LoRA matrices. For example, if we have n sub-LoRA matrices with a  fixed rank r and $W \\in \\mathbb{R}^{d \\times k}$ their parameters increase to $n\u00d7(d\u00d7r+r\u00d7k)$, whereas our parameters remain at $(d\u00d7r+r\u00d7k)$. Another difference is that MoE uses a Network to select the importance of matrices A/B. In contrast, we employ a single learnable parameter (the scaling factor) for each sub-LoRA matrix, which is clearly more efficient. Finally, our downstream tasks are completely different, highlighting the distinct applicability of our method. From our observation, we found that under the same parameters (i.e., 3*r=3 sub-LoRA sitting V.S. r=12 plain LoRA sitting), LoRSS always outperforms the plain setting.\n\nThe scaling factors $si$ and the gating $G(\\cdot)$ are totally different. The $si$ are the weights of different LoRA sub-matrices, while the gating provides a confidence score to the total sum of all LoRA sub-matrices. If we rewrite our euqation 6,7, you can see the scaling factor $s_i$ of LoRA matrices $A_i$ and $B_i$ are independent of the gating prediction:\n\n$$\n\\Delta W = \\left( \\beta \\sum_{i=1}^{m} \\left( s_{i}^{(l)} \\times A_{i}^{(l)} B_{i}^{(l)} \\right) + \\gamma \\sum_{i=1}^{m} \\left( s_{i}^{(l-1)} \\times A_{i}^{(l-1)} B_{i}^{(l-1)} \\right) \\right) \\times G(P_l)\n$$\n \nwhere the $\\Delta W$ is the final updated matrix and will be added like in normal LoRA.\n\n**Q:Why doesn't the weighting form in Eqn (6) match that in Eqn (5)? This discrepancy should be clarified.**\n\nWe thank Reviewer pw2m for pointing out this issue that many reviewers care about. There is actually a misunderstanding that our \n $\\alpha,\\beta,\\gamma$  are the same thing and have the same value (0.1 for $(l-1)$ and 0.9 for $l$, as you can tell from our Table 4(a)). The reason we chose different alphabetical symbols is that we want to separate prompt-token side annotations and LoRA side annotations. We will change them to the same representation to prevent confusion.\n\n**Q:DPD-LoRA requires storing LoRAs per layer (Eqn (4)) and also duplicates each encoder in both branches while retaining unprompted inputs, which appears to impose a substantial memory cost.**\n\nIn fact, if you look at our Tables 4(a) and (b), where we provide computational complexity among different prompting methods, we only add a few parameters (or even fewer than previous methods!), and our proposed methods show better performance. As previously mentioned in our appendix algorithm, we actually follow a two-step training strategy which does not require much memory cost.\n\n\n1.Tongxu Luo, Jiahe Lei, Fangyu Lei, Weihao Liu, Shizhu He, Jun Zhao, and Kang Liu. Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models. arXiv preprint arXiv:2402.12851, 2024.\n\n2.Zhang, Q., Chen, M., Bukharin, A., Karampatziakis, N., He, P., Cheng, Y., ... & Zhao, T. (2023). AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer Tgia"
            },
            "comment": {
                "value": "We thank the reviewer for many insightful comments. We answer the questions in what follows. Please let us know if further clarification is needed.\n\n\n**Q:The motivation for using prompts to guide LoRA learning is not entirely intuitive.**\n\nAs we states in introduction. Prompt learning does not contribute to attention weights(lines 78-80), while solely using LoRA cannot provide task-specific guidance due to updating only pre-trained weights (lines 45-47). Therefore, we propose a new method that can integrate task-specific guidance directly into the adaptation mechanism.\n\n**Q:The authors should clarify why applying a weight to each in the LoRA layer solely through gating prompt tokens is expected to be effective.**\n\nGating is commonly utilized in various deep learning tasks. In our case, it acts like a dynamic weight predictor; it assigns weights between [0,1] to help prevent updating unreliable LoRA matrices. Intuitively, you can consider the output of gating as a confidence score. Meanwhile, we have proven that even without gating (or the confidence score), our proposed method is valid (see $1^{st}$ column of ablation study in Table 3).\n\n**Q:The explanation of the Gating function requires clarification and potentially overlap in function.**\n1. Actually, The LoRA matrices $A_i$ and $B_i$ are independent of the gating prediction. As explained in the previous question, our gating mechanism takes the prompt as input and then predicts weights/confidence scores for these LoRA matrices. The scaling factors $s_i$ and the gating are totally different. The $s_i$ are the weights of different LoRA sub-matrices, while the gating provides a confidence score to the total sum of all LoRA sub-matrices. Thus, as show in (eq 4), the prompt token differs across layers, which makes the weights of different layers differ. Therefore, $G(p_l)!=G(p_{l-1})$. \n\n**Q:Additionally, it is unclear how interacts with the Hierarchical Interaction\u2014does it apply weighting to $A_{i}B_{i}$ at layer $l-1$ as well?**\n\nHierarchical Interaction and LoRSS are actually affected by the gating in every layers, exactly as shown in our Equations (6) and (7). For your convenience, we rewrite it here:\n\n$$\n\\Delta W = \\left( \\beta \\sum_{i=1}^{m} \\left( s_{i}^{(l)} \\times A_{i}^{(l)} B_{i}^{(l)} \\right) + \\gamma \\sum_{i=1}^{m} \\left( s_{i}^{(l-1)} \\times A_{i}^{(l-1)} B_{i}^{(l-1)} \\right) \\right) \\times G(P_l)\n$$\n \nwhere the $\\Delta W$ is the final updated matrix and will be added like in normal LoRA. We thank you for pointing this out, and we will add additional annotations to this equation.\n\n\n**Q:Given the complexity of the proposed method and its multiple components, the current ablation study feels insufficient. For example, what is the rationale for decomposing a single LoRA into multiple sub-LoRAs?**\n\nThere are actually two additional tables in the appendix you might have overlooked. Table 4(a) provides the full setting, while Table 5 offers an additional ablation study. As you can see from Table 5, which uses only plain LoRA, it actually performs worse than our proposed LoRSS. The reason is simple: we found that under the same parameters (i.e., 3*R=3 sub-LoRA sitting V.S. R=12 plain LoRA sitting), LoRSS always outperforms the plain setting.\n\nSince Reviewer pw2m asked a similar question, and we elaborated on the question in shared response **\"Why We Decompose Plain LoRA into LoRSS\"** If you are interested, please refer to our answers there.\n\n**Q:How do authors define hyper-parameters**\n\nWe thank you for pointing out this issue that many reviewers care about. There is actually a misunderstanding that our \n $\\alpha,\\beta,\\gamma$  are the same thing and have the same value (0.1 for $(l-1)$; 0.9 for $l$, as you can tell from our Table 4(a)). The reason we chose different alphabetical symbols is that we want to separate prompt-token side annotations and LoRA side annotations. We will change them to the same representation to prevent confusion. For different $\\lambda$, we empirically defined them to ensure that each loss is within a similar magnitude.\n\n**Q:How does the addition of orthogonal regularization prevent overfitting? More details on this would clarify the choice and its benefits.**\n\nOrthogonal regularization is commonly used in LoRA research, such as in [1]. It can prevent redundancy: orthogonality ensures that the rows (or columns) of the matrices are linearly independent. This reduces redundancy in the learned features, allowing the model to capture more diverse and informative representations.\n\n\n\n1.Zhang, Q., Chen, M., Bukharin, A., Karampatziakis, N., He, P., Cheng, Y., ... & Zhao, T. (2023). AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512."
            }
        },
        {
            "summary": {
                "value": "This paper presents a dynamic prompt-guided LoRA approach that integrates several key modules: Hierarchical Interaction, a Prompt-Conditioned Gating Mechanism (PCGM), and a Self-Regularized Lower-Rank Subspace. The proposed method is evaluated on 11 benchmark datasets, demonstrating its effectiveness."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The integration of prompts with LoRA represents an innovative exploration in this domain.\n+ The authors conducted extensive experiments to substantiate the performance improvements of the proposed algorithm."
            },
            "weaknesses": {
                "value": "+ The motivation for using prompts to guide LoRA learning is not entirely intuitive. The authors should clarify why applying a weight to each $A_i B_i$\u200b in the LoRA layer solely through gating prompt tokens is expected to be effective.\n\n+ The explanation of the Gating function requires clarification. Does $G(P)$ apply a weight before each $iA_i B_i$? How does this differ from directly learning $S_i$, and could it potentially overlap in function? Additionally, it is unclear how $G(P)$ interacts with the Hierarchical Interaction\u2014does it apply weighting to $A_i B_i$ at layer $l\u22121$ as well?\n\n+ Given the complexity of the proposed method and its multiple components, the current ablation study feels insufficient. For example, what is the rationale for decomposing a single LoRA into multiple sub-LoRAs? How are hyperparameters $\\alpha$, $\\beta$, $\\gamma$, $\\lambda_1$, $\\lambda_2$\u200b, and $\\lambda_3$ set, and what is their impact on the final performance?\n\n+ How does the addition of orthogonal regularization prevent overfitting? More details on this would clarify the choice and its benefits."
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes the DPD-LoRA algorithm, which integrates prompt learning to guide the LoRA learning distribution. By incorporating modules such as Hierarchical Interaction, the Prompt-Conditioned Gating Mechanism (PCGM), and the Self-Regularized Lower-Rank Subspace (LoRSS), the proposed DPD-LoRA achieves strong performance across 11 benchmark datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-structured and relatively easy to understand.\n- Detailed experiments provide convincing evidence of the effectiveness of the proposed algorithm."
            },
            "weaknesses": {
                "value": "- Overall, the proposed algorithm involves numerous modules. I strongly suggest the authors consider identifying and focusing on the core components of their method.\n- It is unclear how Eqn (4) is optimized. Are $s_i$ and $A_iB_i$ learned simultaneously? How many sub-LoRAs $m$ are used, and why is it imperative to decompose a single LoRA into multiple sub-LoRAs essential? Do the learnable $S_i$ and $G(P)$ share any functional overlap?\n- Why doesn't the weighting form in Eqn (6) match that in Eqn (5) (e.g., setting $\\gamma=1-\\beta$) ? This discrepancy should be clarified.\n- In Eqn (8), why does the orthogonal regularization prevent overfitting and encourage diversity in the learned LoRA? If this assertion is based on findings from other studies, supporting citations would strengthen the claim.\n- I would like to see a memory cost comparison between the DPD-LoRA and SOTA methods. DPD-LoRA requires storing $m$ LoRAs per layer (Eqn (4)) and also duplicates each encoder in both branches while retaining unprompted inputs, which appears to impose a substantial memory cost."
            },
            "questions": {
                "value": "See weakness 2-5"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "DPD-LoRA uses task-specific prompts to dynamically influence the low-rank updates of model parameters, enhancing the model's adaptability across diverse tasks and mitigating forgetting issues. By decomposing the standard low-rank adaptation into multiple low-rank sub-matrices, the method retains flexibility without adding additional parameters, thus improving the model\u2019s learning capacity. An adaptive loss function is introduced to ensure alignment between the adapted distribution and the pre-trained model, thereby enhancing learning effectiveness and stability. A self-regulating mechanism is used to further improve model stability, along with a soft-gating mechanism to determine when to activate adaptation modules, ensuring improved performance on new categories."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(1) The proposed methods are relatively comprehensive, using several points to improve existing problems.\n\n(2) The writing is clear and easy to understand."
            },
            "weaknesses": {
                "value": "(1): In line 041, \u201cLLaVa\u201d should be revised to \u201cLLaVA\u201d for consistent terminology throughout the document, avoiding unnecessary visual inconsistency.\n\n(2): The related work section lacks references to significant LoRA extensions, such as DoRA, SVFT, PISSA, and LoRA-XS. It is recommended to include these studies and discuss how the proposed method compares to or builds upon these prior approaches. Specifically, it would be helpful to highlight the innovations of this work and the advantages it has over these extensions.\n\n(3): The method incorporates a distillation-like Self-Constrain Loss, but there is no evaluation of training time, GPU resource consumption, or other efficiency-related metrics. Providing specific efficiency metrics, such as training time per epoch, peak GPU memory usage, and FLOPs, would substantiate the claims of being resource-efficient. Including a comparison of these metrics to baseline methods would further support the efficiency claims.\n\n(4): The ablation study section only presents the individual performance of each component without evaluating the performance of their combinations. Adding experiments that evaluate different component combinations (e.g., two, three, and all four components) would provide a more comprehensive view of the model's performance. Including a table or figure showing these combinations or using an approach like forward selection to systematically evaluate the synergies between components would be very informative.\n\n(5): The comparative experiments do not include related LoRA methods, such as DoRA and VeRA. Including comparisons with these methods would more clearly demonstrate the advantages of the proposed approach. It is suggested to add a specific experiment or table comparing the proposed method to DoRA, VeRA, and other relevant LoRA variants on key metrics or datasets to provide a clearer demonstration of its benefits."
            },
            "questions": {
                "value": "(1): The phrase \u201cwithout any additional models prior\u201d in lines 110-111 is somewhat ambiguous. Typically, Parameter-Efficient Fine-Tuning (PEFT) builds on pre-trained models, so it is recommended to clarify whether this refers to the absence of model priors or additional model parameters.\n\n(2): The abbreviation \u201cPEFT\u201d is used for both Prompt-based Efficient Fine-Tuning and Parameter-Efficient Fine-Tuning, which may lead to confusion. It is advisable to select distinct abbreviations to improve clarity.\n\n(3): The term \u201cPLoRA\u201d in line 378 is confusing, as its specific reference is unclear. Further definition or clarification of this acronym is recommended for improved reader comprehension."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This manuscript introduces DPD-LoRA, a novel framework that aims to improve the generalization capability of large models by integrating dynamic prompt-driven low-rank adaptation. This method combines hierarchical prompt tokens and parameter-efficient adaptation to incorporate task-specific guidance, demonstrating superior performance over existing techniques across multiple benchmark datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposal of the DPD-LoRA framework is innovative as it integrates prompt learning and low-rank adaptation to enhance the model's generalization capabilities. The introduction of adaptive loss functions and soft-gated selection mechanisms (PCGM) adds to the novelty of the approach.\n2. The method designed by the authors has been applied to three different tasks: Base-to-novel class generalization, Cross-dataset evaluation, and Few-shot learning, showing promising results across the board, which speaks to the effectiveness of the approach.\n3. The authors have conducted extensive experiments on 11 benchmark datasets, which helps to substantiate the effectiveness of the proposed method.\n4. The overall structure of the paper is relatively clear, with proper introductions to various techniques, facilitating the reader's understanding of the content."
            },
            "weaknesses": {
                "value": "1. The paper employs a variety of techniques and methods, including prompt learning, LoRA, gating mechanisms, and loss design, with five points listed in the INTRODUCTION under contributions and five in the METHOD section. This can seem a bit cluttered and redundant; a more concise summary and consolidation of related content would be beneficial.\n\n2. Section 3.2 is titled \"PROMPT LEARNING WITH LOW RANK ADAPTATION IN TRANSFORMERS,\" yet the explanation seems to treat prompts and LoRA separately, although the appendix provides a detailed explanation of their combined effect. As this is a crucial part of the paper, more clarity and detail in the main body of the text would be necessary.\n\n3. The title of Section 3.3 is \"HIERARCHICAL INTERACTION AND EXPANDED SUBSPACES,\" but the content first introduces expanded subspaces and then hierarchical interactions. The order of introduction and the content should correspond to the title.\n\n4. The paper slightly lacks in-depth analysis of the synergistic effects between LoRA and prompt learning. Although an ablation study is conducted, showing experimental results under different conditions, a deeper analysis of how these components interact and contribute to performance improvements is needed, especially considering this is the core and key of the paper."
            },
            "questions": {
                "value": "1. In the hierarchical interaction section, both prompt tokens and LoRA layers establish connections between the current layer and the previous one to prevent information loss across layers. However, different weight allocation methods are used: \u03b1 and 1-\u03b1 for prompt tokens, and \u03b2 and \u03b3 for LoRA layers. It would be beneficial to explain the rationale and necessity for using different methods when their purposes are aligned.\n2. The paper sets a considerable number of hyperparameters, including learning rates, weight factors, deep prompt tokens, etc., and uses a fixed rank r and quantity m for LoRSS configurations across three different tasks. The paper does not seem to discuss the rationale behind these settings or how different ranks and quantities might impact the results. An explanation for these fixed values would be necessary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}