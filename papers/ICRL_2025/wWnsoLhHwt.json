{
    "id": "wWnsoLhHwt",
    "title": "Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct",
    "abstract": "It has been reported that LLMs can recognize their own writing. As this has potential implications for AI safety, yet is relatively understudied, we investigate the phenomenon, seeking to establish whether it robustly occurs at the behavioral level, how the observed behavior is achieved, and whether it can be controlled. First, we find that the RLHF\u2019d Llama3-8b\u2013Instruct chat model - but not the base Llama3-8b model - can reliably distinguish its own outputs from those of humans, and present evidence that the chat model is likely using its experience with its own outputs, acquired during post-training, to succeed at the writing recognition task. Second, we identify a vector in the residual stream of the model that is differentially activated when the model makes a correct self-written-text recognition judgment, show that the vector activates in response to information relevant to self-authorship, present evidence that the vector is related to the concept of \u201cself\u201d in the model, and demonstrate that the vector is causally related to the model\u2019s ability to perceive and assert self-authorship. Finally, we show that the vector can be used to control both the model\u2019s behavior and its perception, steering the model to claim or disclaim authorship by applying the vector to the model\u2019s output as it generates it, and steering the model to believe or disbelieve it wrote arbitrary texts by applying the vector to them as the model reads them.",
    "keywords": [
        "LLM",
        "Interpretability",
        "AI",
        "Activation Steering",
        "Representation Engineering",
        "Control"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wWnsoLhHwt",
    "pdf_link": "https://openreview.net/pdf?id=wWnsoLhHwt",
    "comments": [
        {
            "summary": {
                "value": "The paper investigates the self-recognition ability of large language models (LLMs), focusing on the Llama3-8b-Instruct model. The authors explore whether the model can reliably distinguish its own outputs from those of humans and other models. The study highlights the implications of self-recognition for AI safety, suggesting that such ability might be related to situational awareness, potentially influencing how an AI system reacts in training versus deployment contexts. The authors use a combination of behavioral experiments and model inspections to identify a specific vector in the residual stream responsible for this self-recognition. Additionally, they demonstrate that manipulating this vector can alter the model\u2019s output to claim or disclaim authorship of text."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- the exploration of a \"self-recognition\" vector in the residual stream is innovative and provides new insights into how LLMs process self-generated text.\n- the experiments are comprehensive, employing multiple datasets, paradigms, and control measures. The use of both paired and individual presentation paradigms adds depth to the investigation.\n- the findings have important implications for AI safety, as the ability to control model behavior through vector manipulation could influence future approaches to securing LLMs against misuse.\n- the paper is detailed and generally clear, with extensive appendices supporting the main findings."
            },
            "weaknesses": {
                "value": "- some sections, particularly those on the technical details of vector activation and steering, are dense so simplifying these descriptions or providing more diagrams could improve comprehension\n- while the appendices provide valuable information, some essential points might be better included in the main body to avoid over-reliance on supplementary material.\n- although the work is thorough for Llama3-8b-Instruct, it would be beneficial to discuss whether these findings might extend to larger or more diverse models."
            },
            "questions": {
                "value": "1. Could the authors clarify if the \"self-recognition\" vector can influence the model\u2019s responses to unseen, completely out-of-domain prompts?\n2. Is there potential for the identified vector to generalize to models beyond Llama3-8b-Instruct, or is it highly specific to this architecture?\n3. How might the ability to manipulate self-recognition vectors be used responsibly to mitigate risks, and what safeguards could be put in place?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors explore the capability to recognize text as being self generated in LLaMA-3-8b models.\nThey find that LLaMA-3-8b-instruct (but not the base model) can distinguish texts created by it from texts created by humans, but not from texts created by other similar language models.\nThen they create a \u201cself-recognition\u201d vector that corresponds to this capability. They evaluate it in various ways, showing that this vector indeed explains how the model makes a decision about whether a given text was written by it or not."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Authors make a convincing claim that capabilities related to self-recognition arise during the post training process.\nThe analysis of the \u201cself-recognition\u201d vector is meticulous.\nThe text is easy to follow."
            },
            "weaknesses": {
                "value": "According to lines 94-95, you include the source text and the instructions in the questions. This has two significant downsides:\nFirst, it decreases safety relevance. For example, in the introduction you mention the risk of collusion when the model recognizes it is talking to itself. But in such scenarios the model won\u2019t have the full context (e.g. it will not know the other instance\u2019s system prompt).\nSecond, it\u2019s much harder to tell what is the mechanism behind self-recognition. You argue in 3.1.2 that perplexity doesn\u2019t matter, but you don\u2019t make a convincing case that the model doesn\u2019t use this type of reasoning at all (it would be hard to make such a case).\n\nIt seems likely that the vector you found is just something like \u201cthis looks like a text from an RLHFed model\u201d. RLHFed models tend to speak in a different way than humans. For example, a text with N tokens generated by an RLHFed LLM will usually have more characters than a text with N tokens written by a human. Base models are similar to humans in this regard. You found that LLaMA can\u2019t distinguish their text from texts generated by other RLHFed models, but can distinguish from humans and from the base model, so this is consistent. It seems also consistent with your other findings, e.g. around line 349 or 453. You could try to refute the simple version of this hypothesis by verifying the accuracy of a simple classifier (e.g. Naive Bayes over a bag of words) trained to distinguish human and LLaMA text.\n\nOverall, differences between texts generated by humans and RLHFed models are much easier to spot than differences between texts generated by different RLHFed models. I think that as long as the models can\u2019t distinguish themselves from other LLMs, it\u2019s pretty hard to make a convincing claim that they have a real self-recognition ability.\n\nMinor things:\nLine 151, \u201cIn all but the SAD dataset \u2026\u201d - I don\u2019t see anything SAD-specific on 1a\nFigure 1a: the font is much too small. Also what is \u201cLLaMA\u201d on the plot?\nTable 1: why compare only to human text, not other LLMs?\nSection 3.2 could use a summary of findings.\nFigure 4: a better caption, what is left and what is right?\nTable 3 in Appendix 1 is unclear"
            },
            "questions": {
                "value": "Have you tried the setup where the source text and instructions are not shown to the model? If yes, what is the performance there?\n\nIn line 159 you mention that you trim all texts \u201cto a set length\u201d. Is this length in tokens or in characters/words?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates the ability of LLMs to recognize their own writing, focusing specifically on Llama3-8b-Instruct. The authors make three main contributions:\n1.Demonstrate that the RLHF'd chat model can reliably distinguish its own outputs from human writing, while the base model cannot\n2.Identify and characterize a specific vector in the model's residual stream that relates to self-recognition\n3.Show this vector can be used to control the model's behavior regarding authorship claims"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.This paper conduct thorough and controlled experiments using multiple datasets with different characteristics including cnn, xsum, dolly and sad.\n2.Clear ablation studies with statistical analysis demonstrate causal relations.\n3.Successfully isolated a specific vector in the residual stream using contrastive pairs method and provide evidence of vector\u2019s causal role through steering experiments.\n4.Identify the correlations between vector activation and confidence."
            },
            "weaknesses": {
                "value": "1.The paper\u2019s experiments are mainly limited to one model family(LLama3) with a relative small LLM(8B).\n2.The paper cross referenced many figures in the appendix which is hard to read.\n3.More discussion needed on practical applications for AI Safety and Model Alignment.\n4.More details on methods and statistical analysis need to be added."
            },
            "questions": {
                "value": "1.Have you investigated whether similar vectors exist in other model architectures? This would help establish the generality of your findings.\n2.It\u2019s better to describe how the similar vector is derived in details.\n3.How stable is the identified vector across different fine-tuning runs? This would have implications for the reliability of using it as a control mechanism.\n4.Could you elaborate on how the vector's properties change across model scales? This might provide insights into how self-recognition capabilities emerge during training."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper dives into the mechanistic explanation of 'self-recognition' for the LLM-generated texts, using LLaMA3-8b-instruct as a case. In the paper, the authors show that LLaMA3-8b-instruct recognizes its generated text from others, such as humans and other LLMs with high performances across two tasks and four datasets. By comparing with the LLaMA3-8b-base model, the authors point out that RLHF enables such self-recognition ability in LLMs. To investigate how self-recognition is represented and computed inside the model, the authors use steering each layer's activations to observe the effect on the output. By zeroing out each layer separately, the authors get causal evidence that layer 16 is the most intensive for representing such 'self-recognition' ability in LLaMA3-8b-instruct. Finally, by 'coloring' the texts based on the steering vectors, the model can interpret the output texts in its own way, showing a valid representation of the vectors."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The authors choose two different task scenarios (paired and individual paradigms) as well as four datasets to investigate the question. The authors also did a comprehensive sanity check to ensure the stability and contribution of the result, such as testing the model before and after RLHF, correlating with perplexity, and normalizing the length effect and positional bias in LLM.\n\n- The authors investigated the computation and representation of 'self-recognition' in the model. By identifying the layers and extracting vectors, the authors show the correlational and causal relationship between the model computation in certain layers and the ability to recognize the self-generated texts. The authors also show the representation can indeed be used to change the style of a text, which reveals the solidity of the representation they find.\n\n- The writing is generally clear and satisfying."
            },
            "weaknesses": {
                "value": "- From a perspective of cognitive science, I still wonder what makes the 'style' of language that LLMs speak and humans different. The authors did a lot of work to find out the valid representation of such an ability to recognize self-generated texts. But what makes the style different is still unclear. If such representation could map on specific features of the style (length for example, or tone, some special word frequency, etc.). It may make sense to ask humans to do the same task (their 'self' is humans) and to see the performance. Probably this can be a good point to make about how LLMs and humans process and understand the language differently.\n\n- The caption of each figure can be more detailed. For example, in Figures 3 and 5, there are multiple sub-figures but I cannot gain any information to distinguish them only from the figure and caption. It could be more reader-friendly to add details in the caption."
            },
            "questions": {
                "value": "- One question I found interesting is when the authors choose to steer the activations, why a very big multiplication on the embedding would result in less effect (for example, in Figure 4, 15 or 16 layer, as the multiplicator grows, the effect grows as well. But when it comes to 14, it is weaker instead, and even turn negative)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}