{
    "id": "pzmbxkCBiq",
    "title": "Understanding Likelihood Over-optimisation in Direct Alignment Algorithms",
    "abstract": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation (DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives to online Reinforcement Learning from Human Feedback (RLHF) algorithms such as Proximal Policy Optimisation (PPO) for aligning language models to human preferences, without the need for explicit reward modelling. These methods generally aim to increase the likelihood of generating better (preferred) completions while discouraging worse (non-preferred) ones, while staying close to the original model's behaviour. In this work, we explore the relationship between completion likelihood and model performance in state-of-the-art DAAs, and identify a critical issue of likelihood over-optimisation. Contrary to expectations, we find that higher likelihood of better completions and larger margins between better and worse completion likelihoods do not necessarily lead to better performance, and may even degrade it. Our analysis reveals that while higher likelihood correlates with better memorisation of factual knowledge patterns, a slightly lower completion likelihood tends to improve output diversity, thus leading to better generalisation to unseen scenarios. Moreover, we identify two key indicators that signal when over-optimised output diversity begins to harm performance: ***Decreasing Entropy over Top-k Tokens*** and ***Diminishing Top-k Probability Mass***. Our experimental results validate that these indicators are reliable signs of declining performance under different regularisations, helping prevent over-optimisation and improve alignment with human preferences.",
    "keywords": [
        "Preference Learning",
        "Large Language Model",
        "Direct Alignment Algorithm"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "In this work, we identify a critical issue of likelihood over-optimisation in state-of-the-art DAA methods and explore the relationship between completion likelihood and model performance.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=pzmbxkCBiq",
    "pdf_link": "https://openreview.net/pdf?id=pzmbxkCBiq",
    "comments": [
        {
            "summary": {
                "value": "* The paper studies how completion likelihood affects model performance in Direct Alignment Algorithms (DAAs) like DPO, IPO and Hinge loss, using 7B and 35B models.\n* Key finding shows that higher likelihood of better completions and larger margins between better/worse completions don't necessarily improve performance. Higher likelihood helps with factual recall but can hurt diversity.\n* Authors propose two metrics for detecting likelihood over-optimization: Decreasing Entropy over Top-k Tokens and Diminishing Top-k Probability Mass. These metrics help prevent over-optimization while maintaining good performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The work provides comprehensive experiments across multiple dimensions including likelihood, diversity and performance. The ablation studies are systematic and well-documented.\n* The paper challenges common assumptions about likelihood optimization in DAAs. The trade-off between memorization and generalization is clearly demonstrated with empirical evidence.\n* The proposed metrics for detecting over-optimization are concrete and actionable. The findings provide clear guidance for practitioners working on DAA training."
            },
            "weaknesses": {
                "value": "* The paper only studies single epoch training when DPO typically needs 2-3 epochs for best performance. This important limitation is not well justified or analyzed.\n* The BINARIZEDPREF dataset lacks crucial details about its construction and preference collection. If preferences come from GPT-4 rather than humans, the generalization of findings is questionable.\n* The primary evaluation uses GPT-3.5-turbo as baseline which feels dated. Testing against stronger models like LLaMA-3 would strengthen the findings."
            },
            "questions": {
                "value": "* Could you provide more information about BINARIZEDPREF construction, including how preferences were collected and validated? The source of preferences is particularly important.\n* Why was single-epoch training chosen when DPO typically needs more epochs? Do these patterns persist in multi-epoch training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a detailed analysis of contemporary offline alignment methods, focusing on DPO, IPO, and Hinge. A comprehensive set of metrics, gathered during the alignment process for both a proprietary 7B model and the Cohere Command R 35B model, is employed to investigate the issue of over-optimization. Furthermore, the paper proposes using the entropy of the top-k tokens during generation for DPO and IPO, as well as the aggregate mass of the top-k tokens for Hinge, as indicators of over-optimization."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Evaluation details are comprehensive.\n- This work is useful for the community as it could be used to determine offline metrics to identify the best model at various steps."
            },
            "weaknesses": {
                "value": "- Over-optimization itself is already well-studied in [1]. Some results, like Figure 1, seem to be consequences of the high KL divergence. I think NLL for y_w and KL are highly correlated metrics. It would be useful to see a figure with KL on the x-axis and NLL(y_w) on the y-axis.\n- While Figure 1 supports the claim that there is no correlation between NLL(y_w) and win rate in general, Figure 2 shows that, for the given method and hyperparameters, the best step can be determined by observing NLL(y_w). More precisely, I would suggest tracking the difference between NLL from the previous and current checkpoints. If this value becomes larger than a certain threshold, one can stop training and thereby obtain the strongest checkpoint. Therefore, from the reported I would say that tracking NLL(y_w) could be sufficient to detect over-optimization across all methods (instead of proposed methodology in Section 4.4, which depends on method).\n- The paper presents an extensive amount of metrics during the training procedure; however, there is no clear criterion to detect over-optimization. First of all, the proposed methods in Section 4.4 are highly dependent on alignment methods, and if one were to use other popular methods (like KTO, ORPO, etc.), it is not clear which metric would serve as a flag for over-optimization. Additionally, it is unclear if these results remain the same for different models (see next weakness point).\n- The choice of models is in question. Results obtained from a closed-source 7B model may not generalize to widely used open-weight models with similar sizes like LLAMA, Gemma, or Mistral. Additionally, this has a negative impact on the reproducibility of the experiments.\n\n[1] Scaling Laws for Reward Model Over-optimization in Direct Alignment Algorithms (Rafailov et al.)"
            },
            "questions": {
                "value": "Lower entropy usually indicates that a model is overfitted. Could this fact indicate that the UltraFeedback dataset is not sufficiently diverse, and therefore the model could memorize patterns that are not useful in general? Does the issue of over-optimization hold across different datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides a experimental analysis of the behaviors of margin-based alignment approaches, especially, they focus on the so-call over-optimisation issue. They claim that they are the first to explore the relationship between completion likelihood and performance in alignment algorithms. Their empirical finding is that, likelihood does not have a striong positive correlation with model performance, and it might be affected by diversity. Besides, they identify two indicators of overly generatinve diverse outputs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "## Originality\n- Though the claim may not be novel, the systematic experiments are original and interesting.\n- The reviewer appreciates the statistical metrics.\n\n## Clarity\n- This paper is well-written and densely organised.\n- The figures are clearly plotted.\n\n## Significance\n- This paper may be able to clarify a controversial point, whether DAA should directly increase(decrease) the likelihood of accepted(rejected) completion? which the reviewer believes is a important question."
            },
            "weaknesses": {
                "value": "## Major\n- The two indicators are so trivial, that people already know this before. Thus these two findings might not be counted as contributions.\n- The experiments are restricted. Some DAAs with alternate objective (CPO, SimPO, ...) (such as length normalization) are not tested, making the claim less solid.\n\n## Minor\n- The curves are a bit confusing, especially Figure 4, making it hard to come to the authors' conclusion."
            },
            "questions": {
                "value": "- In Figure 3, why the DPO/IPO 7B ultrafeedback model cannot gain much improvement compared with initial checkpoint?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors explored the effect of increasing the probability of chosen sequences on the overoptimization of direct alignment algorithms (DAA). They concluded that an increased gap between chosen and rejected sequences could lead to overoptimization and linked overoptimization with reduced diversity of samples."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The initial problem of overoptimization is important, and solving it is crucial for the field of alignment.\n- Understanding signals of overoptimization is helpful and allows for faster evaluation. Instead of waiting for evaluation with LLMs, we can detect overoptimization through reduced entropy and decreasing mass in top-k tokens."
            },
            "weaknesses": {
                "value": "- While the authors provided implementation details, it is unclear why they used a 7B model with closed weights when a wide range of open-source models of this size is available.\n- My main concern is the limited novelty of the obtained results. From what I see in Figure 2, while the gap between chosen and rejected sequences is increasing for small $\\beta$, the likelihood of chosen sequences is decreasing. This aligns with the observations of [1], as probability mass moves towards out-of-distribution (OOD) examples (and to avoid overoptimization, we should prevent leakage of mass to OOD sequences). Therefore, the findings do not seem to present new insights. When demonstrating that increased probabilities for chosen sequences can also lead to overoptimization, it is important to explore the probabilities associated with OOD examples. From this perspective, the main novelty of the paper lies in understanding the mechanism of detecting overoptimization via reduced diversity and aligning these observations with the impact on performance across various tasks, but this aspect is poorly explored.\n- The presentation of results is difficult to read. The captions of large figures (2, 3, 4) do not contain useful information for understanding, making some metrics hard to interpret. Additionally, the analysis of these results is far from the figures themselves.\n\n[1] Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms, Rafailov et. al."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}