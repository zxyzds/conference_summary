{
    "id": "S7F7IMGX4O",
    "title": "Mora: Enabling Generalist Video Generation via A Multi-Agent Framework",
    "abstract": "Text-to-video generation has made significant strides, but replicating the capabilities of advanced systems like OpenAI\u2019s Sora remains challenging due to their closed-source nature. Existing open-source methods struggle to achieve comparable performance, often hindered by ineffective agent collaboration and inadequate training data quality. In this paper, we introduce Mora, a novel multi-agent framework that leverages existing open-source modules to replicate Sora\u2019s functionalities. We address these fundamental limitations by proposing three key techniques: (1) multi-agent fine-tuning with a self-modulation factor to enhance inter-agent coordination, (2) a data-free training strategy that uses large models to synthesize training data, and (3) a human-in-the-loop mechanism combined with multimodal large language models for data filtering to ensure high-quality training datasets. Our comprehensive experiments on six video generation tasks demonstrate that Mora achieves performance comparable to Sora on VBench \\cite{huang2024vbench}, outperforming existing open-source methods across various tasks. Specifically, in the text-to-video generation task, Mora achieved a Video Quality score of 0.800, surpassing Sora\u2019s 0.797 and outperforming all other baseline models across six key metrics. Additionally, in the image-to-video generation task, Mora achieved a perfect Dynamic Degree score of 1.00, demonstrating exceptional capability in enhancing motion realism and achieving higher Imaging Quality than Sora. These results highlight the potential of collaborative multi-agent systems and human-in-the-loop mechanisms in advancing text-to-video generation.",
    "keywords": [
        "Video Generation",
        "Multi-agent",
        "Adaption Training"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "Mora, a multi-agent framework, outperforms open-source text-to-video methods by using self-modulation, data-free training, and human-in-the-loop filtering, achieving results comparable to OpenAI's Sora on video generation tasks.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=S7F7IMGX4O",
    "pdf_link": "https://openreview.net/pdf?id=S7F7IMGX4O",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces Mora, a novel multi-agent  video generation framework, which leverages existing open-source modules to replicate Sora\u2019s functionalities. Mora enhances inter-agent coordination through multi-agent fine-tuning with a self-modulation factor and uses a data-free training strategy to synthesize high-quality training data. It also incorporates a human-in-the-loop mechanism for data filtering. Evaluation shows Mora achieves comparable performance to Sora."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(1) Comprehensive system: The proposed Mora is capable of handling a wide range of visual generation and editing tasks, including Text-to-Image Generation, Image-to-Image Generation, Image-to-Video Generation, and Video Connection. \n\n(2) Clarity of Writing: The writing is clear and easy to follow, making the paper accessible to a broader audience.\n\n(3) Code Availability: Although the training code is not provided, the authors have shared part of their code in the supplementary materials, promoting transparency and potential reproducibility of their results."
            },
            "weaknesses": {
                "value": "(1)\u00a0Limited\u00a0Novelty:\u00a0The\u00a0paper's\u00a0primary\u00a0contribution\u00a0lies\u00a0in\u00a0utilizing\u00a0a\u00a0language\u00a0model\u00a0as\u00a0an\u00a0agent\u00a0to\u00a0call\u00a0various\u00a0pre-existing \u00a0visual\u00a0generation\u00a0models\u00a0for\u00a0video\u00a0generation.\u00a0However,\u00a0neither\u00a0the\u00a0agent\u00a0system\u00a0nor\u00a0the\u00a0video\u00a0generation\u00a0models\u00a0are\u00a0novel\u00a0contributions\u00a0from\u00a0the\u00a0authors.\u00a0Although\u00a0the\u00a0authors\u00a0propose\u00a0some\u00a0additional\u00a0modules\u00a0to\u00a0optimize\u00a0the\u00a0system,\u00a0these\u00a0present\u00a0several\u00a0issues,\u00a0as\u00a0outlined\u00a0below.\n\n(2)\u00a0Concerns\u00a0with\u00a0Self-Modulated\u00a0Fine-Tuning\u00a0Algorithm:\u00a0In\u00a0Section\u00a03.2,\u00a0the\u00a0authors\u00a0introduce\u00a0a\u00a0self-modulated\u00a0fine-tuning\u00a0 algorithm\u00a0that\u00a0optimizes\u00a0both\u00a0the\u00a0language\u00a0model\u00a0(A1)\u00a0and\u00a0visual\u00a0generation\u00a0models\u00a0(A2-A5)\u00a0end-to-end\u00a0 using\u00a0a\u00a0visual\u00a0generation\u00a0loss.\u00a0This\u00a0concept\u00a0is\u00a0unconventional\u00a0and\u00a0lacks\u00a0sufficient\u00a0supporting\u00a0research.\u00a0The\u00a0authors\u00a0do\u00a0not\u00a0provide\u00a0adequate\u00a0citations\u00a0to\u00a0justify\u00a0this\u00a0algorithm.\u00a0Additionally:\n\n(2.1)\u00a0The\u00a0process\u00a0is\u00a0inherently\u00a0difficult\u00a0to\u00a0optimize.\u00a0The\u00a0authors\u00a0mention\u00a0using\u00a0only\u00a096\u00a0samples\u00a0for\u00a0training\u00a0in\u00a0Section\u00a04.2,\u00a0which\u00a0is\u00a0unconvincing\u00a0for\u00a0such\u00a0a\u00a0complex\u00a0task.\n\n(2.2)\u00a0Without\u00a0releasing\u00a0the\u00a0training\u00a0code,\u00a0it\u00a0is\u00a0hard\u00a0to\u00a0assess\u00a0the\u00a0effectiveness\u00a0of\u00a0their\u00a0methodology\u00a0based\u00a0solely\u00a0on\u00a0theoretical\u00a0descriptions\u00a0and\u00a0hyperparameter\u00a0choices.\n\n(2.3)\u00a0The\u00a0visual\u00a0results\u00a0(Figures\u00a09-12)\u00a0 show\u00a0significant\u00a0artifacts,\u00a0such\u00a0as\u00a0blurriness\u00a0and\u00a0object\u00a0deformation,\u00a0which\u00a0raise\u00a0doubts\u00a0about\u00a0the\u00a0efficacy\u00a0of\u00a0the\u00a0self-modulated\u00a0fine-tuning\u00a0algorithm.\n\n(3)\u00a0Data-Free\u00a0Training\u00a0Strategy\u00a0and\u00a0Distillation\u00a0Concerns:\u00a0The\u00a0authors\u00a0propose\u00a0a\u00a0data-free\u00a0training\u00a0strategy\u00a0using\u00a0large\u00a0models\u00a0to\u00a0synthesize\u00a0training\u00a0data,\u00a0which\u00a0resembles\u00a0a\u00a0distillation\u00a0approach.\u00a0However,\u00a0this\u00a0strategy\u00a0may\u00a0cause\u00a0the\u00a0generated\u00a0results\u00a0to\u00a0overfit\u00a0to\u00a0the\u00a0large\u00a0models.\u00a0This\u00a0calls\u00a0into\u00a0question\u00a0the\u00a0added\u00a0value\u00a0of\u00a0using\u00a0a\u00a0multi-agent\u00a0 approach,\u00a0which\u00a0typically\u00a0comes\u00a0with\u00a0higher\u00a0inference\u00a0costs.\u00a0The\u00a0authors\u00a0should\u00a0provide\u00a0a\u00a0fair\u00a0comparison\u00a0to\u00a0demonstrate\u00a0that\u00a0training\u00a0Mora\u00a0with\u00a0this\u00a0strategy\u00a0yields\u00a0better\u00a0performance\u00a0 than\u00a0directly\u00a0distilling\u00a0a\u00a0smaller\u00a0model.\u00a0Additionally,\u00a0quantitative\u00a0analysis\u00a0of\u00a0agent\u00a0success\u00a0rates\u00a0and\u00a0inference\u00a0efficiency\u00a0would\u00a0strengthen\u00a0the\u00a0claims\u00a0made\u00a0about\u00a0the\u00a0multi-agent\u00a0system\u2019s\u00a0benefits.\n\n(4)\u00a0Problem\u00a0Definition\u00a0in\u00a0Section\u00a03.1:\u00a0The\u00a0authors\u00a0claim\u00a0that\u00a0their\u00a0objective\u00a0is\u00a0to\u00a0maximize\u00a0quality\u00a0metrics\u00a0while\u00a0ensuring\u00a0diversity\u00a0in\u00a0the\u00a0generated\u00a0videos.\u00a0However,\u00a0quality\u00a0metrics\u00a0alone\u00a0are\u00a0not\u00a0the\u00a0ultimate\u00a0goal\u00a0for\u00a0video\u00a0generation.\u00a0Beating\u00a0existing\u00a0benchmarks\u00a0such\u00a0as\u00a0SORA\u00a0does\u00a0not\u00a0inherently\u00a0demonstrate\u00a0the\u00a0model's\u00a0superiority.\u00a0Furthermore,\u00a0in\u00a0an\u00a0agent-based\u00a0system,\u00a0other\u00a0factors\u00a0such\u00a0as\u00a0module\u00a0collaboration\u00a0speed,\u00a0accuracy,\u00a0and\u00a0robustness\u00a0must\u00a0be\u00a0considered."
            },
            "questions": {
                "value": "\uff081\uff09Optimization Challenges with the Self-Modulated Fine-Tuning Algorithm:\n\n\uff081.1\uff09The self-modulated fine-tuning algorithm in Section 3.2 is unconventional and lacks supporting citations. Could the authors provide more references or evidence to justify this approach?\n\n\uff081.2\uff09Given the complexity of optimizing such a system, training with only 96 samples seems inadequate. Can the authors elaborate on how this sample size is sufficient, or provide results from larger-scale experiments?\n\n\uff081.3) The visual results show significant artifacts like blurriness and object deformation. How do the authors plan to address these issues to improve visual quality?\n\n\n\uff082\uff09Effectiveness of Data-Free Training Strategy:\n\n\uff082.1\uff09The data-free training strategy resembles a distillation approach and may result in overfitting to large models. Can the authors show comparative experiments to demonstrate that the proposed multi-agent approach offers clear advantages over simply distilling a smaller model?\n\n\uff082.2\uff09Additionally, can the authors provide a quantitative analysis of agent success rates and inference efficiency to better justify the benefits of using the multi-agent system?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new generalist framework named Mora that synergies Standard Operating Procedures for video generation. The proposed Mora can be used to tackle a lot  of video-related tasks. Specifically,  Mora consists of five Agents: Prompt Selection and Generation Agent, Text-to-Image Generation Agent, Image-to-Image Generation Agent, Image-to-Video Generation Agen, and Video Connection Agent. Experimental results show the effectiveness of the proposed method, and Mora can surpass the capabilities of existing leading models in several respects."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- A new generalist Agent-based framework named Mora is proposed.\n- The proposed Mora can perform a lot of video-related tasks and shows remarkable results.\n- Experimental results indicate the effectiveness of the proposed method, and Mora can surpass the capabilities of existing leading models in several respects."
            },
            "weaknesses": {
                "value": "- This paper contains a lot of existing techniques involving Prompt enhancement, Image generation, Image editing, Video generation, and Video connection. The main technical innovations should be clarified. Explaining how the system's architecture or the interaction between components differs from or improves upon existing approaches may help readers to understand.\n- Since many components are involved, Mora's inference speed and computational requirements to existing state-of-the-art models for each of the video-related tasks mentioned should be discussed. Additionally, is there any optimizations implemented to improve efficiency, given the multi-component nature of the system.\n- Will there be accumulated errors between multiple components? Providing empirical results showing how errors may or may not accumulate across different components and tasks is necessary. Or are there any techniques implemented to mitigate potential error accumulation between components?"
            },
            "questions": {
                "value": "Please refer to Weaknesses for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a multi-agent framework named Mora to enable text-to-video generation by leveraging open-source models such as Llama and Stable Diffusion. The main contributions include a self-modulation factor for inter-agent coordination, a data-free training strategy and a human-in-the-loop data filtering. The authors compare the performances of Mora with Sora on six video generation tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors break down the process of video generation into several steps and utilize off-the-shelf models to implement the generation process.\n2. The authors clearly describe the whole pipeline.\n3. Data systhesis and filtering strategy are used to improve the quality of training."
            },
            "weaknesses": {
                "value": "1. There are no demo videos provided by the authors, no gif files or no project website. It is hard to evaluate the quality of generated video with some sampled frames. \n2. The key component of the self-modulated training is the modulation factor. Is it able to visualized this factor?\n3. How would the performances be if the agents A1 to A5 were replaced with different models?"
            },
            "questions": {
                "value": "How to explain that a multi-step generation can perform better than an end-to-end generation? For example, why a combination of open-sourced text-generation model, text-to-image model, image-to-image model and image-to-video model can outperform an end-to-end open-source text-to-video generation model and replicate a close-source model's functionalities."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}