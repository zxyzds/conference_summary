{
    "id": "9QYJu1cGfE",
    "title": "Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models",
    "abstract": "Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted towards the development of large motion models. \nDespite some progress, current state-of-the-art works remain far from achieving truly generalist models, largely due to the lack of large-scale, high-quality motion data. \nTo address this, we present MotionBase, the first million-level motion benchmark, offering 15 times the data volume of the previous largest dataset and featuring multimodal data with hierarchically detailed descriptions.\nBy leveraging this vast dataset, our large motion model demonstrates strong performance across a broad range of motions, including unseen ones.\nThrough systematic investigation, we underscore the importance of scaling both data and model size, with synthetic data and pseudo labels playing a crucial role in mitigating data acquisition costs.\nMoreover, our research reveals the limitations of existing evaluation metrics, particularly in handling out-of-domain text instructions --- an issue that has long been overlooked.\nIn addition to these, we introduce a novel 2D lookup-free tokenizer for motion quantization, which preserves motion information and expands codebook capacity, further enhancing the representative ability of large motion models.\nThe release of MotionBase and the insights gained from this study are expected to pave the way for the development of more powerful and versatile motion generation models.\nOur code and database will be released at \\url{https://anonymous.4open.science/r/MotionBase}.",
    "keywords": [
        "human motion generation",
        "large motion model",
        "large language model"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=9QYJu1cGfE",
    "pdf_link": "https://openreview.net/pdf?id=9QYJu1cGfE",
    "comments": [
        {
            "summary": {
                "value": "Paper introduce motionbase, a motion generation benchmark trained on large amount of data with focus on motion generation with LLMs."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The work is well motivated in terms:\n Showing the gap of prior work and lack of domain generlization \nShowing limitation of prior metrics \nA new motion codebook \n\n\nThe new dataset is quite large in comparison with prior ones, which is a valuable addition to the community. It comes with a good set of text descriptions. \n\nEvaluation on multiple datasets and multiple models with strong baselines.\nAnswer to important questions like the need of scale and model size impact on the task \nDiscussion on OOD behaviour \nAblation of motion quantization"
            },
            "weaknesses": {
                "value": "I do not see much of concerns about the work, more of questions."
            },
            "questions": {
                "value": "\u2013 Questions:\nHow did the author verify the correctness/accuracy of the pose estimation\nWhat do authors think about properties of a new metric?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors introduce MotionBase, a large-scale human motion generation benchmark featuring over one million motion sequences, a fifteen-fold increase over previous datasets, with multimodal data and detailed text descriptions. The authors demonstrate that scaling both data and model size significantly improves motion model performance, particularly with synthetic data and pseudo labels to reduce data acquisition costs. The authors also propose a novel 2D lookup-free motion quantization approach to enhance motion information retention and expand codebook capacity. Experimental results on various datasets validate the efficacy of their approach, with notable performance on out-of-domain data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper introduces MotionBase, a large-scale dataset comprising over one million human motion sequences, designed to support more comprehensive training and evaluation of motion generation models.\n\n2. The paper identifies key factors influencing the effectiveness of large motion models, underscoring the importance of scaling both data and model size.\n\n3. This paper proposes a 2D lookup-free motion quantization method that enhances motion representation while retaining essential information, thereby contributing to improved model performance."
            },
            "weaknesses": {
                "value": "1. While MotionBase is introduced as a benchmark with the potential to enhance motion model performance, the paper lacks a thorough comparative analysis across varied methods to demonstrate MotionBase's influence on model efficacy. Additional baselines and a broader selection of models trained on MotionBase would more robustly substantiate its claimed advantages.\n\n2. The paper does not include visual comparisons of motions generated by models trained on the baseline Motion-X dataset versus those trained on the proposed MotionBase dataset.\n\n3. Including the ground truth R-Precision and FID scores in relevant tables would strengthen the presentation and transparency of the results.\n\n4. The paper would benefit from dynamic visualizations within the qualitative analysis of the motions in the proposed datasets, which could provide a clearer and more engaging illustration of the dataset's scope and quality."
            },
            "questions": {
                "value": "1. It will be interesting to see the scalability of different architectures. Have the authors explored fine-tuning existing methods, such as MotionGPT[1] or MoMask[2], with larger parameter settings on the MotionBase dataset? \n\n2. Regarding the automated evaluation metrics referenced by the authors, it is also noteworthy that the R-precision scores are relatively low on the proposed large-scale MotionBase dataset, potentially weakening the benchmarking results. Implementing text-motion retrieval models like TMR[3] may provide a more accurate evaluation of model performance.\n\n[1] MotionGPT: Human Motion as a Foreign Language.\n\n[2] MoMask: Generative Masked Modeling of 3D Human Motions.\n\n[3] TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper is to answer the research question of \"can a large motion model be a promising direction for motion generation?\", and the designed a data colletion pipeline which collects multi-modal information including RGB, depth and bounding box with multi-person.\nIn addition the paper introduces a method to expand the codebook capacity, named lookup-free approach for motion tokenization, for better motion representation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "This paper presents the first large-scale dataset specifically designed for motion generation, featuring richly multi-modal data accompanied by hierarchical text descriptions. MotionBase, the dataset introduced, is expected to be highly beneficial for the advancement of future research in motion generation and to serve as a valuable resource for the computer vision community. The dataset offers researchers access to an extensive collection of motion data, enabling more robust analysis and development of large motion model."
            },
            "weaknesses": {
                "value": "I have minor concerns on this paper.\n\nThe layout of the paper is somewhat challenging for readers. It contains numerous messages and analyses, requiring readers to scroll up and down frequently to locate referenced tables. Additionally, due to page limitations, many explanations are placed in the Appendix. Tables and figures are positioned mid-page without aligning well with the paragraph height, disrupting the flow.\n\nThis paper was the first to introduce the concepts of partitioning body parts and 2D quantization, making it a valuable reference. (Pi, H., Peng, S., Yang, M., Zhou, X., & Bao, H. (2023). Hierarchical generation of human-object interactions with diffusion probabilistic models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15061-15073.)\n\nMinor Issues and Typos:\nAppendix D: \"quantitative results\" should be \"qualitative results.\"\nFigure 4: It may improve clarity to add a y-axis label."
            },
            "questions": {
                "value": "In Table 4, what is the ratio between synthetic, static, and real data? It can be brefiely explained in table caption.\n\nI have the concern of the quality of occlusion cases or blurred images. How the authors recognize the motion is blurred or occluded?\nIn multi-person settings, the occlusion might be very common.\n\nSince this is a dataset paper, I expect the more detailed explanation and instructions of the benchmark will be released once the paper upon the paper acceptance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper collects a new large-scale text-motion dataset called MotionBase and then finetunes LLMs with different sizes. Additionally, for better scaling, the authors follow the video domain to train a new LFQ tokenizer with a large vocab size."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors try to scale the tokenizer vocab size and the model size.\n2. The authors collect a new large-scale text-motion dataset."
            },
            "weaknesses": {
                "value": "1. The biggest server weakness is containing the one-frame pose data into the database. The Agora, mscoco, muco_3dhp, and other more datasets are used for 3d pose estimation, and they even occupy a large portion of the whole database, which may lead to static motion generation. \n2. The motion quality has not been validated. Neither the estimated motions nor the texts generated by LLM have been checked manually or by any algorithm. The video collection process is not clarified clearly. A lot of web videos are long and contain various camera shots. Which film shot boundary detection algorithm are you using? And how many frames do you insert into LLM to get the text? More details need to be added. \n3. The experiments with static data ablation study are not fair. Does the validation set contain static data and synthetic data?"
            },
            "questions": {
                "value": "1. The FID in Table 6 is so wired. The FID of reconstruction is 1.76 while the generation FID in Table 3 is 0.166. This is impossible from my understanding. I suspect that the reconstruction result is not good enough. The original MPJPE calculation will subtract the root movement. If you calculate MPJPE similarly, the high reconstruction FID means the translations are not accurate. \n2. What do the authors get from scaling experiments? Did the author see any hope for emerging? The shown examples are common cases, that can be also observed in other motion generation work.\n3. Did the supervised label contain only motion tokens or both text and motion tokens? \n4. Did the author try zero-shot text testing? For example, could the largest model do some texts like \"The old man with a broken leg is walking forward slowly with a crane\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper claims to propose a large motion model with a very large motion database. However, the motion quality is not well evaluated. Besides, the authors propose a motion quantization method, which is borrowed from LFQ (Mentzer et al., 2023). The authors claim a good generation quality of the generated results, which is not provided in the demo."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The writing of this work is a bit fancy. \n\n- The statistics of the dataset are clear."
            },
            "weaknesses": {
                "value": "There are several fundamental concerns about this work. Each of these is fatal. \n\n1. **The motion collection process.** This process contains several issues. \n    - This work does not evaluate the quality of the video mocap data quality. To my knowledge, even the quality of the latest Motion-X++ suffers significant jittering and foot sliding. How can your method escape from this? **(my main concern)**\n    - If the quality of the ground truth is not good enough, how can you generate good motion? Therefore, the result in L395-402 is not solid and convincing. **(my main concern)** I suggest authors read the blog [1] written by a well-known graphics scientist, Daniel Holden. \n    - The limited contribution of the dataset. The video data comes from InternViD and WebVid and the data collection process is from motion-x and other methods. The dataset contribution is limited. \n\n2. **The text annotation.**\n    - The annotation quality of the text by Gemini-1.5-pro is not well evaluated. In my practice, it always contains some answers like \"sorry...\". The results should be corrected by researchers one by one. Has the >1M data been checked? \n    - The proposed contribution of hierarchical text is not discussed well. Has it been used in the model training? If I miss, please point it out. If this annotation is not used, what is the motivation for this hierarchical text contribution? Will it make the result more fine-grained? It is quite unclear. **(my main concern)** \n\n3. **Limited technical/evaluation contribution.** The LFQ is proposed by the original paper. The authors did not have a new understanding over this. Besides, the H2VQ proposed in Humantomato (ICML-24) is also missing for discussion or comparison. \n\n4. This work does not include any demo video, which is unacceptable in the animation community. The FID in Table 5 is extremely large, which strengthens my concerns about the motion quality. \n\n5. **Motivation.** The motivation for introducing LLM is not clear. The method misses a basic baseline of a transformer (like in T2M-GPT, CVPR-23) for comparison. Besides, it is also not clear whether the usage of pre-trained parameters of LLMs or not. Whether the fine-tuning method is LoRA or not is also not well discussed. Therefore, it is not technically sound. **This is my strong concern.**\n\n[1]: Daniel Holden, https://theorangeduck.com/page/animation-quality."
            },
            "questions": {
                "value": "- The vocabulary of the LLM and motion codebooks are different. How do authors handle this issue? What is the efficiency of the LLM-based motion generation method? Please compare with the fastest motion generation method, MotionLCM (ECCV-24).\n\n- **I would like to know why authors should cite [1].**\n\n[1]: Zheng et al., Steve-eye: Equipping llm-based embodied agents with visual perception in open worlds."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper contains a dataset of human subjects. The human RGB videos are also included. Besides, some of the data comes from other datasets, which should be clear on the usage of commercial or not."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}