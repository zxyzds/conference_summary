{
    "id": "1Njl73JKjB",
    "title": "Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control",
    "abstract": "Disentangling model activations into human-interpretable features is a central\nproblem in interpretability. Sparse autoencoders (SAEs) have recently attracted\nmuch attention as a scalable unsupervised approach to this problem. However, our\nimprecise understanding of ground-truth features in realistic scenarios makes it\ndifficult to measure the success of SAEs. To address this challenge, we propose\nto evaluate SAEs on specific tasks by comparing them to supervised\nfeature dictionaries computed with knowledge of the concepts relevant to the\ntask. \n\nSpecifically, we suggest that it is possible to (1) compute supervised sparse\nfeature dictionaries that disentangle model computations for a specific task;\n(2) use them to evaluate and contextualize the degree of disentanglement and\ncontrol offered by SAE latents on this task. Importantly, we can do this in a\nway that is agnostic to whether the SAEs have learned the exact ground-truth\nfeatures or a different but similarly useful representation.\n\nAs a case study, we apply this framework to the indirect object identification\n(IOI) task using GPT-2 Small, with SAEs trained on either the IOI or OpenWebText\ndatasets. We find that SAEs capture interpretable features for the IOI task, and\nthat more recent SAE variants such as Gated SAEs and Top-K SAEs are competitive\nwith supervised features in terms of disentanglement and control over the model.\nWe also exhibit, through this setup and toy models, some qualitative phenomena\nin SAE training illustrating feature splitting and the role of feature\nmagnitudes in solutions preferred by SAEs.",
    "keywords": [
        "mechanistic interpretability",
        "sparse autoencoders",
        "evaluations"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We compute and validate *supervised* sparse feature dictionaries on the IOI task, and then compare SAEs against them",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1Njl73JKjB",
    "pdf_link": "https://openreview.net/pdf?id=1Njl73JKjB",
    "comments": [
        {
            "summary": {
                "value": "The authors propose a (principled) method allowing to create supervised dictionaries for space features, which allow for evaluating the degree of disentanglement of SAEs. The developed method is then applied to SAEs and LLMs, witnessing not only interpretable later variables, but also providing possibility of editing attributes. Metrics of sufficiency, necessity and control are used for this."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "In general the manuscript is well written and \"strong\".\n\nThe question which is really to ask is how much this finding is relevant for the literature. Here I should say my expertise is perhaps too limited to provide a proper judgment."
            },
            "weaknesses": {
                "value": "Though this has most probably costed a lot of work, the empirical validation of the proposed methodology is rather scarce. Whether the constructed dictionaries would also function for other tasks/semantics is not clear.\n\nThe mathematics are well explained, in clear and simple way.\n\nSome (not that many) parts of the manuscript I had to read several times, e.g., the title under Figure 1 or the paragraph on interpretability at the end of Section 3.2. But in general formulas aid much understanding."
            },
            "questions": {
                "value": "In the title of Figure 1, could you make a clear connection of the text with precise parts of visuals, to facilitate the understanding?\n\nTaking into account the size of the paragraph on related work, it should be possible to describe the related work without much terminology and thus shifting it closer to the beginning of the manuscript. This would allow the reader to better position the framework with respect to the state of the art."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the Sparse autoencoders to capture interpretable features for the IOItask and the expeirment results show that the proposed approach achieves the best performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "There are several weaknesses:\n\n1. The motivation of this section should be enhanced.\n\n2. The English language should be improved.\n\n3. The main idea seems not very novel. This paper should provide a strong motivation.\n\n4. The experiment can be further improved by providing more results and analysis."
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on evaluating sparse autoencoders (SAEs) for their ability to recover known ground-truth features learned by a model. To do so, the authors first train a supervised dictionary on the indirect object identification (IOI) task, for which model computation is already relatively known due to prior interpretability and circuit discovery work. Both IOI task-specific SAEs and full-distribution SAEs are trained and evaluated with respect to the supervised dictionaries to understand if SAEs allow for the same level of approximation, control, and interpretability as the supervised dictionaries. Results reveal that more recent SAE architectures improve these capabilities and task-specific SAEs are much more effective than full-distribution SAEs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The writing and paper structure are very clear and easy-to-follow.\n- The focus of this paper is highly relevant and interesting. The problem of finding a ground truth with which to evaluate interpretability and explainability methods has remained an issue for decades, and this work works towards solving this problem by exploring using human-generated groundtruths that have been backed up by prior work.\n- The experiments are well-defined, inutitive, and easy to understand.\n- I believe the results are interesting and useful - they reveal that task-specific SAEs are more useful in practice than full-distribution SAEs, hinting that data quality is of utmost importance when training SAEs. Further, this suggests that human priors may be useful when developing interpretability methods/SAEs."
            },
            "weaknesses": {
                "value": "- I find the \u201cWhy use these attributes?\u201d paragraph in Section 3.1 to be confusing. If prior work had not proposed the IO, S, and Pos attributes, how would one go about defining the supervised dictionary? If the evaluation pipeline described in this paper were to be used for a different task, I\u2019m not sure whether this section would be generalizable. In particular, when there are many choices of attributes, what is the manner of choosing between them without using another interpretability method, which would then prevent the need of using an SAE in the first place?\n- It would have been significantly more convincing to me if the authors had considered more than one task in their evaluation. At the moment, it\u2019s unclear to me how the proposed methodology and results from this work could be applied to future works that want to evaluate trained SAEs.\n- The section on interpretability (section 4.4) is also a bit confusing to me - I would find it very helpful if the authors provided interpretations of the SAE latents, and a visualization of how these features could then be used to explain the LLM\u2019s computation on a single example. Some examples of /possible/ interpretations are provided in Appendix 7.13-7.14, but if I understand correctly these are not the actual labels of the SAE features.\n- It is my understanding that the authors wish to propose the use od supervised dictionaries an evaluatory baselines for SAEs. However, in practice, this paper reads more as an exploration of whether SAEs can recover the IOI task. While the authors discuss the limitations of hardcoding the attributes to compare SAEs against and only considering a single task and model, I believe these drawbacks fundamentally limit the work in its general utility."
            },
            "questions": {
                "value": "- In section 4.3, if I understand correctly, the SAE latents are found by simply optimizing/searching for the features that perform the task (move one latent to the specified counterfactual latent). This seems a bit roundabout to me - wouldn\u2019t this propose that you need to know the features you are looking for in order to label SAE features? How would one do this searching or interpretation without access to the counterfactual activations? Wouldn\u2019t it be more realistic to interpret or label each SAE feature and then use the features that are labelled to be relevant to the task at hand?\n- Please see the above weaknesses!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a way to compute sparse feature dictionaries that disentangle model computations similar to an SAE fashion using a supervised method; as well as introduces several ways to evaluate these feature dictionaries from different perspective such as necessity, sufficiency and controls of the feature dictionaries towards a specific task. The author applies the work to the indirect object identification (IOI) using GPT-2 Small, and compare the feature dictionaries obtained by their method, vs some other recent SAE variants such as Gated SAE adn Top-K SAE."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Thorough study in the case of IOI. The use of IOI in the study mainly because we know the \"feature\" that more or less directly affect the outputs. This minimizes the possibilities of the cases that \"SAE features\" may not coincide with the features that human understands. This was also implied in Section 4.2.\n- Extensive study on the paper that includes a lot of results in the appendix.\n- Some proposed methods such as Necessity and sufficiency, to the control method proposed can be applied to a more general case, as described in Section 4. \n- This paper also addresses a lot of the details on small nuances in evaluation of SAEs. This includes the discussion in Session 4.2, and the paragraph on \"Preventing...\" in Section 4.3."
            },
            "weaknesses": {
                "value": "**Overall**\n\n- As this paper is a case study to IOI, it is somewhat restrictive. I don't think it is necessarily a weakness of the paper though as I don't think anything can be done to address this \"weakness\". It could be viewed as a stepping stone for future work as well.\n- Presentation and Readability are the main weakness of this work. For example, experiment results in Section 3 includes a lot in section 4 such as description of FullDistribution SAE, Task SAE, TopK, Gated SAE etc. There are crucial formulae and methods that should be in the main text, but instead they are in the Appendix. For instance, the necessity formula in Section 7.4 ~line 878 in the appendix.\n- The section on \"Interpretability\" such as in Section 3.2, Figure 5 and Section 4.4 are out of place. The author even mentioned that \"we stress that this experiment is of a rather exploratory and qualitative nature\". Putting these sections in the appendix would be much more coherent.\n- Section 6 on discussions and limitations is too short. There should be more discussions on the experiment results. Possibilities of applying similar techniques to a general settings (as IOI provides the \"known features\"). Including a discussion on future work may be useful.\n\n**Detailed issues**\n\n- On Sufficiency in \"Figure 3. Left\" - as the experiment is to test whether the reconstructions are sufficient, we would hope to compare the logit difference of the original and the reconstruction. The method in Figure 3 shows the ratio of the *average* logit difference with and without the intervention. This may not be the best because the averaging may hide the changes in logit difference with the intervention for each example. A simpler method like the average changes (absolutized) of the logit difference _may_ work. The authors can also opt to include a brief discussion on this so that it does not seem like they were hiding something. For example, showing the distribution of  absolute changes of the logit difference in a histogram, or some statistics on it.\n- Necessity. The experiment is to test whether the reconstructions are necessary. This means that we want to show that without the reconstructions, the model cannot do so well, resulting a drop in model performance - i.e. logit difference. Thus there are three quantities\n1) The reconstruction $\\hat{a}(p)$\n2) The proposed quantity; average plus SAE error term\n3) The average.\n\nShowing necessity should be showing the difference between 1) and 2) are large. However, in the main text of the paper, it opts to show the difference between 2) and 3) only. A crucial formula and description of the necessity score directly addressing this problem is in the appendix (Section 7.4, around line 878), which in my opinion, should be in the main text.\n- \"Probing accuracy\" in \"Control\": Seems out of place? Is it referenced somewhere else in the paper? Also no results were shown? In the absence of results, this section on probing accuracy does not seem to achieve the goal of the section: \"measures the degree to which the supervised feature dictionaries disentangle the different attributes\". This is because the probe is linear, which itself can \"disentagle\" the attributes. I think it would be better to either remove this section (put this in the appendix), or show some experiment results with discussions on the way to disentangle the attributes.\n\n**Minor**\n- Section 4.3 Expressing... line ~361. edit 3 --> (3) or Equation 3.\n- Results (line ~424). objective 4 --> (4) or Equation 4.\n- missing parenthesis at line 434 (resp. $a(p_t)$ **)** by their...\n- broken references in appendix (line 883, 935, 1849)"
            },
            "questions": {
                "value": "- Equation 2. Reason for the \"bias\" term is $E[a(p)]$? Does this mean $E[u_{IO}] + E[u_S] + E[u_{POS}] \\approx 0$ if we take the expected value on $a(p)$ and $\\hat{a}(p)$? If it is by design, can we make a comment on this design? What is a brief explanation behind this design?\n- What is F and A in F1 Score? In the text, it seems F=the set of examples activating a specific SAE latent \"f\". A=binary attribute of a prompt. It seems that the F1 Score is applied **on each SAE latent**, as described in Section 4. How do we get \"A\" in this case? How do we know which \"binary\" attribute of a prompt that the latent f corresponds to? Can we give a more detailed explanation in the text? It would be nice to include some examples of F and A (specifically A)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}