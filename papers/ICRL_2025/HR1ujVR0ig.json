{
    "id": "HR1ujVR0ig",
    "title": "Learning Generalizable Skills from Offline Multi-Task Data for Multi-Agent Cooperation",
    "abstract": "Learning cooperative multi-agent policy from offline multi-task data that can generalize to unseen tasks with varying numbers of agents and targets is an attractive problem in many scenarios.\nAlthough aggregating general behavior patterns among multiple tasks as skills to improve policy transfer is a promising approach,\ntwo primary challenges hinder the further advancement of skill learning in offline multi-task MARL. \nFirstly, extracting general cooperative behaviors from various action sequences as common skills lack bringing cooperative temporal knowledge into them.\nSecondly, existing works only involve common skills and can not adaptively choose independent knowledge as task-specific skills in each task for fine-grained action execution.\nTo address these challenges, we propose an approach named Hierarchical and Separate Skill Discovering (HiSSD) for generalizable offline multi-task MARL through skill learning.\nHiSSD leverages a hierarchical framework that jointly learns common and task-specific skills.\nThe common skills learn cooperative temporal knowledge and enable in-sample exploration for offline multi-task MARL.\nThe task-specific skills represent the priors of each task and achieve a task-guided fine-grained action execution.\nTo verify the advancement of our method, we conduct experiments on multi-agent MuJoCo and SMAC benchmarks.\nAfter training policy using HiSSD on offline multi-task data, the empirical results show that HiSSD assigns effective cooperative behaviors and obtains superior performance in unseen tasks.",
    "keywords": [
        "Reinforcement Learning",
        "Multi-Agent Reinforcemenr Learning"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "Learning generalizable multi-agent policy from offline multi-task data by improved skill learning",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=HR1ujVR0ig",
    "pdf_link": "https://openreview.net/pdf?id=HR1ujVR0ig",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a new method called Hierarchical and Separate Skill Discovering (HiSSD) for learning generalizable multi-agent policies from offline multi-task data. The key idea is to jointly learn common skills that capture cooperative behaviors across tasks, as well as task-specific skills for fine-grained action execution. The method uses a hierarchical framework with a high-level planner that learns common skills and a low-level controller that learns task-specific skills. Experiments on SMAC and Multi-Agent MuJoCo benchmarks show improved generalization to unseen tasks compared to baselines. Overall, this paper presents a novel and effective approach to an important problem in multi-agent RL. With some additional analysis and clarifications, it would make a good contribution to the field."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Novel approach to learning generalizable multi-agent policies by jointly learning common and task-specific skills.\n\n2. Theoretical analysis and derivation of objectives for skill learning.\n\n3. Comprehensive experiments on multiple benchmarks showing improved generalization.\n\n4. Addresses an important problem in multi-agent RL of transferring to tasks with varying numbers of agents.\n\n5. The authors have shared comprehensive details of implementation along with codebase, which further bolsters the experimental efforts."
            },
            "weaknesses": {
                "value": "1. Ablation study can be further expanded to have a comprehensive analysis of different components. Comparison to some recent related works like HyGen is missing."
            },
            "questions": {
                "value": "1. There is a very recent work that tackles the same problem from a different methodology perspective found here : \"Variational Offline Multi-agent Skill Discovery\" by Chen et. al. I would like to know the authors' thoughts on how their methodology is different from this work, and if there are scenarios where one or the other might work well/worse."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes HiSSD to leverage a hierarchical framework to jointly learn the common and task-specific skills in offline MARL settings. Experiments in SMAC and multi-agent MuJoCo show its effectiveness and generalizability to different tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe division into common and task-specific skills provides a balance between generalization and task adaptation, addressing the limitations of previous methods that only focused on common skills.\n2.\tThe proposed objective to learn common skills for exploration and prediction is new.\n3.\tHiSSD enables multi-agent policy transfer across varying tasks, outperforming existing multi-task MARL baselines on two benchmarks"
            },
            "weaknesses": {
                "value": "1.\tHiSSD\u2019s hierarchical design and multi-component training can lead to instability. I think the authors should reveal more about the training details, hyperparameters selection, and tuning methods. \n2.\tIt can be better to present an algorithm of the whole training process and the execution process for understanding. \n3.\tIn the ablation studies, the HiSSD variants (e.g., w/o Planning and Half-Negative) often show strong performance compared to HiSSD with minor differences or even surpassing HiSSD. \n4.\tThe visualization experiments do not show clear evidence about the latent skills. For example, the task-specific skills in the same task seem closer in Figure 3."
            },
            "questions": {
                "value": "1.\tFrom Equation (3), does $q(s_{t+1}\\mid c_t^{1:K})$ has any meaning in real? How does HiSSD model the sequence $c_t^{1:K}$?\n2.\tIf the negative samples are drawn from limited seen tasks when doing task-specific skill learning, how can we ensure a correct skill when facing unseen task distribution?\n3.\tWhy are the two baselines, ITD3-CQL and ITD3-BC, not compared in the SMAC experiments?\n4.\tHow is the performance if we only adopt a common skill without using the task-specific skills?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a hierarchical framework, HiSSD, for jointly learning common and task-specific skills from offline multi-task datasets. The common skills represent the general cooperation patterns, while task-specific skills achieve a task-guided fine-grained action execution. The algorithm is broken up into two phases. In the first phase, the high-level planner discovers common skills. In the second stage, the low-level controller learns task-specific skills."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is well written, with thorough experiments, and a variety of tasks across two environments.\n2. In order to achieve better performances in cooperative multi-agent environments, the paper brings cooperative temporal knowledge into common skills, which is reasonable."
            },
            "weaknesses": {
                "value": "1. According to the paper, HiSSD is superior to previous skill-based methods like ODIS because it brings cooperative temporal knowledge into common skills and discovers task-specific skills that can distinguish each task\u2019s specific knowledge. However, HiSSD fails to outperform ODIS in some tasks.\n2. In MAMuJoCo, the paper didn't use ODIS as a baseline, which has a good performance in SMAC.\n3. The process of learning skills is similar to imitation learning, so it failed to perform well in unseen tasks, indicating low-level generalization ability. For example, in Table 1, the win rates for 10m12m and 13m15m are lower than 10% with the medium-quality dataset.\n4. In the passage, some verbs sound a bit odd. For example, 'contain' in the sentence 'Equipping offline multi-task MARL with skill learning to improve policy transfer still contains issues.'\n5. One concern for the paper is to evaluate HiSSD in maps that are more difficult (like the Stalker-Zealot task set in ODIS), to validate its effectiveness."
            },
            "questions": {
                "value": "- In Table 3, I wonder why presents BC-best's results, rather than ODIS. ODIS seems to be the baseline with the best performances.\n- In Figure 3, Large-scale tasks (i.e., 10m and 12m) overlap with each other but small-scale tasks don't. The paper said this is because 10m and 12m are similar. However, 3m, 4m, and 5m are also similar. I would appreciate some further discussion about this.\n- In Equation 6, the weight $\\alpha$ serves as the trade-off between guiding to space with high-reward and space that the execution policy ought to have a correct imitation. I wonder why the paper didn't conduct a sensitive analysis for this parameter like $\\beta$ in Equation 11, as shown in Appendix E.2.\n- The task-specific skills learn task-specific knowledge of source tasks, why do these skills play an important role in unseen tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Hierarchical and Separate Skill Discovering (HiSSD), a method for learning generalizable skills in offline multi-task multi-agent reinforcement learning. HiSSD's hierarchical framework simultaneously learns common skills (cooperative behaviors across tasks) and task-specific skills (unique task characteristics). It uses an objective function combining reward maximization and next-state prediction for common skills, and contrastive learning for task-specific skills. HiSSD trains on offline data and enables decentralized execution with local information during testing."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper introduces Hierarchical and Separate Skill Discovering (HiSSD), a method for learning generalizable skills in offline multi-task multi-agent reinforcement learning. HiSSD's key strength is its hierarchical framework that simultaneously learns common and task-specific skills, overcoming existing limitations. It enables fine-grained action execution and trains using only offline data. During testing, HiSSD allows decentralized execution using local information and learned skills without centralized control. Experiments on SMAC and MAMuJoCo benchmarks showed HiSSD's performance improvements, especially in unseen tasks."
            },
            "weaknesses": {
                "value": "While the paper claims to integrate cooperative temporal knowledge into the process of learning common skills, it fails to adequately explain the clear causal relationship or mechanism between these two concepts. There appears to be a logical leap in assuming that simply performing global state prediction and value estimation directly leads to the learning of cooperative knowledge. Furthermore, the paper lacks a clear definition of what exactly \"cooperative temporal knowledge\" means and how it is measured and evaluated. This ambiguity makes it difficult to verify whether this knowledge has been learned."
            },
            "questions": {
                "value": "* Can you explain how performing global state prediction and value estimation directly leads to learning cooperative knowledge?\n* HiSSD claims to simultaneously learn common skills and task-specific skills, but how did you verify how distinct these two types of skills actually are? Was there any overlap or interference between the two skills?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an algorithm for multi-task multi-agent skill discovery from offline data, which is a quite meaningful and challenging research direction. The novelty (compared with ODIS) mainly lies in jointly learning common and task-specific skills."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(a) This paper targets at a challenging problem setup, i.e., recovering generalizable multi-agent skills from multi-task offline data.\n\n(b) The evaluation results are sufficient and show consistent improvements over the SOTA algorithm -- ODIS."
            },
            "weaknesses": {
                "value": "(a) The algorithm framework, including both the multiple components to learn (Fig. 1) and the objective design (Eqs. 4, 7, 11), is quite convoluted compared to ODIS, which could potentially bring training difficulty.\n\n(b) The writing needs to be significantly improved, especially for methodology part in Section 3. \n\n(c) It's not clear why the common skills can integrate cooperative temporal knowledge and enable an offline exploration.\n\n(d) Since all tasks share the same space for the local skills z^{1:K}_t, why z can embed task-specific knowledge?\n\n(e) Multi-agent skills should provide abstractions at both temporal-level and subgroup-level. That is, each multi-agent skill should embed a (short) control sequence of a subgroup of agents. How is this aspect shown in the algorithm design?\n\n(f) Thanks for releasing the code package. But it seems that the authors only made minimal changes to the readme file provided by ODIS. Please provide a detailed instruction (in the readme file) on how to reproduce all the reported results in this paper. Also, the released yaml files do not include the ones for MAMuJoCo."
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}