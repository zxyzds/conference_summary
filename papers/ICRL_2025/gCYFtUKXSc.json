{
    "id": "gCYFtUKXSc",
    "title": "Avoid Being a Shortcut Learner through Library-Based Re-Learning",
    "abstract": "Replay-based methods provide a promising solution to address catastrophic forgetting issue in continual learning. They try to retain previous knowledge by using a small amount of data from previous tasks stored in a fix-sized buffer. In this work, we invoke the information bottleneck principles and reveal some fundamental limitations of those methods on their effectiveness in capturing the truly important features from the prior tasks by relying on the buffer data selected according to the model's performance on known tasks. Since future tasks are not accessible during model training and buffer construction, the trained model and the buffer data tend to be biased towards making accurate predictions on the labels of known tasks. However, when new task samples are introduced along with labels, the biased model and the buffer data become less effective in differentiating samples of the old tasks from those of the new ones. Inspired by the way humans learn over time, we propose a novel relearning technique that makes use of additional past data, referred to as the library, to test how much information the model loses after learning the new task. We then realign the model towards those forgotten samples by training on a carefully selected small subset samples from the library for a few epochs with comparable computational cost as existing replay-based models. The experimental results on multiple real-world datasets demonstrate that the proposed relearning process can improve the performance of the state-of-the-art continual learning methods by a large margin.",
    "keywords": [
        "Continual learning",
        "Data-Efficient Learning",
        "Information Theory"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=gCYFtUKXSc",
    "pdf_link": "https://openreview.net/pdf?id=gCYFtUKXSc",
    "comments": [
        {
            "summary": {
                "value": "The paper tackles the problem Class Incremental Learning (CIL), focusing on the shortcut learning problem. To do so, the authors give a theoretical perspective justifying the existence of short learning in Continual Leaning and propose to leverage a large pool of data called library. The library is then used to detect potential shortcut learning issues when learning the current task and allows for relearning. The authors conduct experiments on various relevant CIL datasets and show a considerable improvement compared to state-of-the-art methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The information bottleneck perspective of the shortcut learning problem is appreciate\n- The performances are compelling\n- The core idea is easy to follow\n- The code is shared\n- The paper is easy to follow"
            },
            "weaknesses": {
                "value": "## Weaknesses\n - l 44. \"the replay buffer is fundamentally limited given how it is constructed because the model may be biased when selecting the buffer without accessing the future task data\". The replay buffer construction can vary from one paper to the other. No method is specified here, hindering the validity of such argument. In addition, the most common memory building strategy remains the reservoir sampling, which does not rely on the model performances at all, making this argument completely invalid in this context. The authors should be more precise in this statement.\n- The entire introduction, while making various claims about the existing strategies, contains only one single reference.\n- l52 \"Most previous work attributes the inability to retain the previous task data performance as the forgetting issue\". This is the actual definition of forgetting.\n- l53. The described case of wrong learning or shortcut learning in the case of Continual Learning is not new, see [1, 2]. Such work should be considered for comparison. \n- Work on learning representation with mutual information between task have been proposed before and are not mentioned in the paper [3, 4]. The usage of contrastive learning as a mutual information proxy is not new, see also [5].\n- l262. What about regularization strategies to mitigate shortcut learning?\n- l 311 \"Furthermore, just increasing replay-buffer size is not feasible as the computation cost of training on the replay-buffer will increase with the size of replay-buffer.\" I disagree with this statement. The batch size of data extracted from the buffer will still be the same, so the computation overhead does not depend on the buffer size. However, the space complexity will indeed increase.\n- Following on previous point, I do not understand how the authors justify the usage of library while denying large memory size. To me the usage of library is completely similar to using an infinite-size memory buffer and the proposed methods should be compared with larger memory size. Similarly, is computing the difficulty score for each sample in the library computationally intensive?\n- The authors claims an improved computation efficiency compared to large memory buffer, but such information is not provided in the paper.\n- How do you choose the library size for a given dataset? \n- While the paper claims to solve shortcut learning in this context, I do not see muuch evidence of that in the paper.\n- Why not compare to [4] ? It is cited in the paper and also addresses shortcut learning. A comparison of various CAM with compared methods, on top of performances, would be required to prove that the proposed method solves shortcut learning.\n### citations\n[1] Wang, Maorong, Nicolas Michel, Ling Xiao, and Toshihiko Yamasaki. \"Improving Plasticity in Online Continual Learning via Collaborative Learning.\" In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 23460-23469. 2024.\n\n[2] Yujie Wei, Jiaxin Ye, Zhizhong Huang, Junping Zhang, and\nHongming Shan. Online prototype learning for online continual learning. In ICCV, 2023.\n\n[3] Gu, Y., Yang, X., Wei, K., & Deng, C. (2022). Not just selection, but exploration: Online class-incremental continual learning via dual view consistency. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (pp. 7442-7451).\n\n[4] Guo, Y., Liu, B., & Zhao, D. (2022, June). Online continual learning through mutual information maximization. In _International conference on machine learning_ (pp. 8109-8126). PMLR.\n\n[5] Mai, Z., Li, R., Kim, H., & Sanner, S. (2021). Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_ (pp. 3589-3599)."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes a strategy for Continual Learning (CL) methods based on rehearsal and the prediction of the Task-id. In this approach, a large \"library\" of past data expands the replay buffer, allowing the model to test itself on previously learned information. When the model identifies forgotten information or shortcuts (patterns learned for one task that may not generalize to others) the model undergoes \"relearning\" by training on carefully selected data from the library. The method incorporates a contrastive learning mechanism for the feature extractor and it subsequently updates the classification heads for each task using this sub-sample of the library, referred to as the library buffer."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The manuscript is well-written and provides comprehensive analyses, covering several essential aspects such as computational time costs, detailed ablation studies, details on task splitting, and a robust comparison with a diverse set of competitive baselines. \nBoth Figure 2 and Algorithm 1 make the comprehension of the framework easy, and I appreciate that the methods' performances are averaged over multiple runs, with standard deviations included."
            },
            "weaknesses": {
                "value": "My main concern about this work is that in a scenario where the literature is increasingly moving towards replay-free approaches (see SLCA[1]), this work appears heavily dependent on retaining *at least* twice the amount of data typically used with a replay buffer.\u00a0\u00a0\n\n    [1] Zhang, Gengwei, et al. \"Slca: Slow learner with classifier alignment for continual learning on a pre-trained model.\"\u00a0Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\u00a0\n\nAdditionally, the library, sized at 5k or 10k examples, is nearly as large as the data from an entire task. This goes beyond the typical concept of a replay buffer, as it resembles replaying almost the entire previous task. While I understand that the model is not trained on the whole library, this approach still raises concerns related to privacy and data storage constraints in continual learning settings\n\n\n**Minor:**\n\nThere is a lack of consistency in the notation used for the official names of architectures and methods, and sometimes they are incorrect. For example, \"Resnet\" should be written as \"ResNet,\" and \"Der++\" is sometimes referenced this way and other times as \"DER++,\" which is the correct version.\n\nThere's a typo in line 538: relay-based should be \"replay-based\"."
            },
            "questions": {
                "value": "**Questions and points to address**\n- To address my main concern regarding fair comparisons, I believe a truly fair evaluation of different baselines\u2019 performance could be achieved by setting both buffer-only and relearning methods to use a total data storage size of $m$. For the relearning methods, this storage could be split into $0.5m$ for the replay buffer and $0.5m$ for the library, (or at least the library buffer) creating a more equitable baseline. This approach would ensure that both methods operate within the same overall memory constraint, avoiding any potential bias from additional memory or data availability for the relearning model. \n\n    Could the authors conduct an experiment where both the buffer-only method (e.g., DER++) and the relearning method (e.g., DER++ + RL) are restricted to the same total storage size? Given time constraints, focusing this setup on a single-method comparison would be sufficient to provide insight. \n\n- How is the threshold $\\lambda$ (which determines when to perform relearning based on the quiz mechanism) set for the main Tables \n experiments? The results appear to differ from those presented in Table 3.  I think this value should be explicitly reported for each presented experiment. Additionally, I suggest expanding the explanation of the quiz mechanism in the manuscript, as it currently occupies a relatively small section.\n\n- The buffer and library sizes for the methods listed in Tables 1 and 2 are not very clear, except for the upper part of Table 2. To enhance comprehension of the results, it would be beneficial to include the buffer/library sizes consistently throughout both tables. \n\n- Could the authors clarify which augmentations are applied and specify how many augmented versions of the same input are generated during training? This detail would help in understanding the contrastive learning process more thoroughly. \n\n- I recommend moving the explanation of Figure 1, in particular the section **Empirical evidence for relearning** earlier in the text. By introducing this explanation sooner, the motivations behind the study will become clearer to the reader. A well-placed and timely discussion of the figure can help contextualize the objectives, allowing readers to better progress through the manuscript. This adjustment could enhance the overall flow and comprehension of the work. \n\n - I have a last question regarding the reported accuracy results for the competing methods. Are these results specifically for methods that do not involve relearning in either Task Incremental Learning (Task-IL) or Class Incremental Learning (Class-IL)? It seems from the text that the results may pertain to Class-IL; however, your method appears to incorporate some form of Task-ID knowledge or estimates. \nClarifying this distinction is crucial, as it will help the reader better understand the validity of the comparisons being made and how these different methods are evaluated against one another. This clarification will enhance the rigour of your analysis and provide valuable context for interpreting the results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No Ethics Concerns"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a relearning technique for CL. It select relearning samples considering each sample's learning difficulty. The experimental result table shows improvement. \n\nHowever, I have some concerns, please see weaknesses."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper proposes a relearning technique for CL. It select relearning samples considering each sample's learning difficulty. The experimental result table shows improvement."
            },
            "weaknesses": {
                "value": "1. Please provide more visualization analysis, such as analysis on Task-Recency Bias, Last layer Bias, Feature Drift, etc. \n\n2. In the selection process for the library buffer, Except theoretical analysis, please provide some visualizations to show the different effect brought by selecting different samples to relearning.\n\n3. Figure 3 is not clear. please highlight your novel relearning process in Figure 3. \n\n4. Please compare with more advanced continual learning methods and strategies, such as Mutual Learning/Knowledge Distillation in CL.\n[1] Wang, M., Michel, N., Xiao, L. and Yamasaki, T., 2024. Improving Plasticity in Online Continual Learning via Collaborative Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 23460-23469).\n[2] Michel, N., Wang, M., Xiao, L. and Yamasaki, T., Rethinking Momentum Knowledge Distillation in Online Continual Learning. In Forty-first International Conference on Machine Learning.\n\n5. I am wondering is your relearning process compatible with Mutual Learning/Knowledge Distillation strategies? Please conduct experiments on on the above Refs. [1-2]. By doing so, I believe this paper will be more convincing."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The author proposes *Relearn*, a relearning method based on *library* to address the shortcut learning problem in CL. *Library* stores the historical information of past tasks. The model is tested with a subset of *library* frequently throughout the training, and relearn the forgotten samples in the library."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The performance is attractive.\n2. The paper is easy to follow.\n3. The proposed method makes sense intuitively."
            },
            "weaknesses": {
                "value": "(Fatal)\n1. The author claimed they are trying to solve the shortcut learning problem in the title and introduction. However, there is no evidence in the paper that the shortcut learning problem is addressed with the provided evaluation. In the evaluation, only accuracy is included. To ground the claim soundly, I believe it is necessary to include evaluations of the gradient attention map (or other metrics directly related to shortcut learning, like Shapley value, etc.) in both final results and ablation studies\n\n(Major)\n1. Since there are existing works (For example, OnPro in the citation) also aims to solve the shortcut learning problem. Despite the difference in the settings (OnPro is online CL), adapting OnPro to the current setting and making a comparison of accuracies/attention maps should be beneficial to the manuscript. \n2. The comparison might be unfair because the Library size (5k) is much larger than the buffer size (200 and 2k) in the evaluation. For fair comparison, having the same information of past tasks (storing the same amount of historical data) is important for fair comparison. Some results of 5k memory size for CIFAR-100-10T are available in [1]\n\n(Minor)\n1. There is an inconsistency of Notations in Table 1. (CF100 and CIFAR100) Also, it is suggested to have the same decimal digits in the experimental results.\n2. It is suggested that more information about the computation time be included. For example, the proposed method can be compared with other baselines. \n3. Stroing a lot of data in the library might be a problem for the CL setting.\n\n(Summary)\n\nThe proposed method is interesting and makes sense intuitively. However, the weakness outweighs the strength, and I would like to give a reject.\n\nReference:\n\n[1] DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning. ECCV 2022"
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}