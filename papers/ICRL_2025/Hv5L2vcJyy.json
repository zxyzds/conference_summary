{
    "id": "Hv5L2vcJyy",
    "title": "Elementary: Pattern-aware Evidence Discovery with Large Language Models",
    "abstract": "The remarkable success of rationale generation provokes precise Evidence Discovery, which aims to identify a small subset of the inputs sufficient to support a given claim. However, existing general extraction methods still fall short in quantifying the support of evidence and ensuring its completeness. This paper introduces a heuristic search framework, Elementary, which formulates the Evidence Discovery as a multi-step prompt construction process. Specifically, we offer a clear perspective that the LLMs prompted with \\emph{according to}, without fine-tuning on domain-specific knowledge, can serve as an excellent reward function to assess sufficiency. Based on this, Elementary explores various potential reasoning patterns and uses future expected rewards, including independent and pattern-aware rewards, to find the optimal prompt as evidence. Experiments on three common task datasets demonstrate that the proposed framework significantly outperforms previous approaches, additional analysis further validates that Elementary has advantages in extracting complex evidence.",
    "keywords": [
        "Evidence Discovery",
        "Large Language Model"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Hv5L2vcJyy",
    "pdf_link": "https://openreview.net/pdf?id=Hv5L2vcJyy",
    "comments": [
        {
            "summary": {
                "value": "The paper presents Elementary, a heuristic search framework for evidence discovery that leverages large language models through multi-step prompts to identify supportive evidence for given claims. This work aims to tackle the problem that existing work lacks in identifying the supportiveness of the retrieved evidence. Therefore, they introduce a framework that includes \u201cpattern-aware\u201d and \u201cindependent\u201d reward mechanisms to enhance evidence sufficiency and completeness without domain-specific fine-tuning. Their framework was evaluated on two gemma models with HoVer, PubMedQA, and CovidET datasets. Their results show that Elementary outperforms baseline methods by capturing complex evidence patterns across diverse domains."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Their Elementary framework formulates evidence discovery as a multi-step prompt construction process using LLMs. This innovative approach combines heuristic search methods with LLM-guided pattern exploration, which is unconventional compared to standard single-step or retrieval-based techniques.   \nIntroducing the \u201caccording to\u201d prompt and forcing the LLM to decode the given claim and calculate the log probability as the score for assessing evidence sufficiency is interesting. They designed an unsupervised value function considering both sentence and paragraph-level evidence dependence rewards."
            },
            "weaknesses": {
                "value": "Base model generalizability: The author did their experiments on gemma models with two different parameter sizes. The value function\u2019s result on the same test instance could be similar since both models are trained on similar lexical. Have the authors considered evaluating their framework with other models of similar size to show the generalizability across different base models?   \nComputational cost: This framework requires multiple computations between the evidence and the claim checked. Have the author compared the computational cost between the baseline method and their framework?"
            },
            "questions": {
                "value": "1. The notation for **a** and **s** is a bit confusing to me. Does the **s** represent the potentially relevant context in the paragraph/document, and **a** stands for the single sentence from **s**?\n2. Since the author mentioned, \"All decoding/sampling settings were kept default.\"-- Are there any variances in the predicted results in different runs?  \nSuggestion: Please consider including more details in the figure and table captions. Figure 3's caption is very clear and easy to understand."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a formulation and an heuristic search framework to rank evidence from context provided to an LLM for generation using the \"according to\" prompting mechanism."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Investing on better mechanism to rank evidence, passages and context for LLMs to answer prompts is an area of research that is relevant and will eventually provide better benchmarks and better use of LLMs.\n\nSection 5 and its analysis posed as questions is good an informative."
            },
            "weaknesses": {
                "value": "- The paper is difficult to follow and does not set the context right for the reader. For example, in the abstract, it is unclear what \"according to\" in cursive means without reading the paper. This happens in lines 52 (where there is at least a citation) and 61 again.\n\n- it is very unclear why the baselines in Table 1 are the right ones? Please motivate. See my comments about citing relevant literature in RAG, maybe that would help motivating whether these are the right baselines in the future version of the paper.\n\n- The authors claim that elementary consistently outperforms various evaluated baselines, but it is unclear if the results are statistically significant and whether the improvements are relevant to the reranking of passages/context. Also, the impact of with or without according to is unclear based on table 2, which repots the log probability of each claim is not fully clear to me.\n\n- I suggest to provide more information in the captions of the tables in the paper. They break the flow of reading the paper, and how they connect with the tasks and datasets. Note that the calculation of metrics is never explained. Similarly, why top-3 and top-5 are the right settings for evaluation is never explained.\n\n-The choice of LLM (Gemma-2b-it) is not well motivated. Why this one, why not another one? The paper later discusses this in Section 5.4, but the writing is unclear as it is not preparing the reader for that in line 294\n\nSee my questions below as support for above comments.\n\nminor: - when you say \"typically, we employ LLMs...\" cite a paper that shows that this is typical."
            },
            "questions": {
                "value": "I'm surprised to see that in the task description and formulation the context and the evidence is not directly connected with retrieval augmented generation which has become a very active area of research in recent years;  the authors chose to to only cite 2014 papers for this (note that I am not saying they should not be cited, just complement with recent work where this can be related). You can start by citing Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks - Part of Advances in Neural Information Processing Systems 33 (NeurIPS 2020) / Retrieval-Augmented Generation for Large Language Models: A Survey\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang - Why not connecting with this?\n\nThe task of evidence discovery where an evidence set is extracted from the context sounds quite similar to the retrieval step in RAG using an embedding model that ranks passages by similarity that are then provided as context to an LLM for generation. I understand that the methodology and claims are different, but I wonder if that would be a good way to connect with the readers familiar with RAG, and even experiment with datasets recently used here. Why not connecting with this?\n\nSimilarly, quantifying the support of an input for a target claim sounds very similar to the idea of citations and source attribution in RAG. Why not connecting with this?\n\nIn line 160, I don't understand how you are \"forcing\" the LLM to decode the given claim. Providing an \"according-to\" prompt will not alleviate hallucinations, it will just alleviate them. You even show that in Section 5.5. This, again, shows that the paper needs to improve in its writing. I suggest to not use the word \"forcing\", maybe \"guiding\"\n\nhave you tried variations of the wording for \"according to the given context, we can infer that <Claim>\"? there are multiple ways to write this, and this may impact the final outcome."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce a new framework for Evindent Discovery overcoming the previous baseline regarding the completeness of the evidence. They take advantage of heuristic search which provides independent and pattern-aware rewards. Their pipeline is based on a multi-step prompt construction process that includes \"according to\" to ensure grounding the LLM in the context and sufficiency."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The use of a heuristic reward function to filter different pathways regarding selecting evidence seems interesting and similar to a look-ahead strategy to prune irrelevant pathways."
            },
            "weaknesses": {
                "value": "- I am not sure how your works compare to chain-of-thought/evidence and what advantages it brings w.r.t CoT. It would be interesting if you could include CoT for comparison purposes or at least clarify if it is relevant or not. For this reason, I think your contribution is limited. \n- Section 5.4 doesn't have a significant contribution as you only indicated instruction fine-tune models are better in instruction following which is obvious.\n- According to Table 3 and Fig. 3, the drop in the EM score from 2-hop to 3-hop is significant which points out regarding the pattern recognition it is not very powerful when the number of supporting evidences is more than 2. Therefore, although it is better than baselines, but it still seems weak to say it is pattern-aware.\n\n* The EM score for the FEVER-1 in Table 3 is a duplication of the F1 score. The same thing happens in Table 4.\n* Please highlight the best scores in your tables to make it easy for comparison.\n* TYPO: in line 117, you need to put a space after \"methods\"."
            },
            "questions": {
                "value": "I have no question."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}