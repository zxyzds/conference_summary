{
    "id": "1qgZXeMTTU",
    "title": "Coreset Spectral Clustering",
    "abstract": "Coresets have become an invaluable tool for solving $k$-means and kernel $k$-means clustering problems on large datasets with small numbers of clusters. On the other hand, spectral clustering works well on sparse graphs and has recently been extended to scale efficiently to large numbers of clusters. We exploit the connection between kernel $k$-means and the normalised cut problem to combine the benefits of both. Our main result is a coreset spectral clustering algorithm for graphs that clusters a coreset graph to infer a good labelling of the original graph. We prove that an $\\alpha$-approximation for the normalised cut problem on the coreset graph is an $O(\\alpha)$-approximation on the original. We also improve the running time of the state-of-the-art coreset algorithm for kernel $k$-means on sparse kernels, from $\\tilde{O}(nk)$ to $\\tilde{O}(n d_{avg})$, where $d_{avg}$ is the average number of non-zero entries in each row of the $n\\times n$ kernel matrix. Our experiments confirm our coreset algorithm is asymptotically faster on large real-world graphs with many clusters, and show that our clustering algorithm overcomes the main challenge faced by coreset kernel $k$-means on sparse kernels which is getting stuck in local optima.",
    "keywords": [
        "spectral clustering",
        "kernel k-means",
        "coresets"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "We combine the benefits of spectral clustering and coresets",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1qgZXeMTTU",
    "pdf_link": "https://openreview.net/pdf?id=1qgZXeMTTU",
    "comments": [
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "We thank the reviewer for their time and feedback, and we respond to their concerns below.\n\nOn the first question, as we performed our experiments it became clear that coreset kernel clustering struggles with sparse kernels. This was to be expected as Dhillon et al. (2007) reported this phenomenon for kernel $k$-means. They show that the process of ensuring the kernel matrix is positive definite (by adding a multiple of $D^{-1}$ to $K$), makes it more difficult for datapoints to move between clusters. Spectral clustering avoids this phenomenon completely as it recovers the optimal solution of a relaxed problem which is invariant under this shift. \n\nFor the second question, yes. We construct a similarity graph based on the Euclidean distance between the input instances.  \n\nRegarding Figures 5, 6, and 7, we experimentally evaluate two versions of CSC: the ordinary CSC algorithm, and a fast version which uses the spectral clustering variant proposed by Macgregor (2023). We find that both CSC algorithms perform better than the coreset kernel $k$-means baseline in terms of ARI and at least one of our variants meets the ARI of standard spectral clustering with a suitably large coreset. The fast CSC variant offers a significant speedup at the cost of slightly lower ARI. This speedup becomes more visible as the number of clusters increases.\n\nOn your point about Figure 7, if the partition of coreset kernel $k$-means doesn't change from one iteration to the next (as was often the case) then the algorithm terminates early. Based on the work of Dhillon et al. (2007), this is to be expected for sparse graphs. Note that the baseline performs poorly in terms of ARI in this instance."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "We thank the reviewer for their time and feedback, and we respond to their concerns below.\n\n\nFor the first question, our results hold regardless of the distribution of datapoints. This is due to the nature of the underlying coreset guarantee for kernel $k$-means: for any dataset, a coreset preserves the objective of every set of centers with high probability. Most coreset algorithms achieve this by performing some sort of importance sampling to make sure they cover imbalanced or uneven clusters sufficiently.\n\n\nTo answer the second question, we agree that the performance of a clustering algorithm depends on the chosen parameter when constructing a similarity graph and, rather than a downside of our technique, most clustering algorithms would face this scenario. However, there have been many empirical studies on choosing the right parameters for typical datasets, and we notice that the value of $k$ between $200$ and $500$ as the number of neighbours suffices for our experiments."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "We thank the reviewer for their time and positive review, and we respond to their comments below.\n\nRegarding the comments on $d_{avg}$ vs $k$, we notice that most graphs occurring in practice, including internet graphs, biological networks, and several social networks, are power-law graphs with very low average degrees. However, the number of clusters of these graphs could be quite high as shown in the following examples from our first experiment: \n\n- The Friendster graph has an average degree of 28 while the number of ground truth (overlapping) communities is 5,000.\n- The LiveJournal graph has an average degree of 9 and 5,000 ground truth communities.\n- The wiki-topcats graph has an average degree of 14 and 17,000 ground truth communities.\n\nHence, for real-world graphs, it is typical that the number of clusters dominates the average degree. The actual running time of our algorithm is $\\widetilde{O}(n\\cdot \\min (k, d_{avg}))$, which is always asymptotically better than the method of Jiang et al. We will make it clearer in the abstract of the next version of the paper.\n\nFinally, we agree that adding an overview of clustering methods in empirical research would\nenrich our discussion, and we couldn't do so due to the page limit. We will add more such discussions in the related work section in the next version of our paper."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "We thank the reviewer for their time and insightful comments, and we respond to their mentioned points below.\n\nFirst of all, we think it is a very interesting idea to apply other sampling techniques, including the Metropolis-Hasting algorithm, to improve our result. However, applying these algorithms would require us to significantly expand our analysis and proof. We will leave this for further work.\n\nFor your comments on the RatioCut, we believe that a similar result should hold for the RatioCut as well, and we chose to use NCut since it's arguably more commonly used in spectral clustering literature. We can report our result with respect to RatioCut in the final version if the reviewer thinks it is necessary. \n\nRegarding your last question, ARI is commonly used to compare the performance of clustering algorithms that optimise different objectives as long as ground-truth labels are available. For label-free objectives such as NCUT and RatioCut, there may not even be a uniquely defined optimal clustering. Even though the algorithms we test do not optimise ARI directly, ARI allows us to evaluate them fairly."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "We thank the reviewer for their time and feedback, and we respond to points below.\n\nOn the first question, the actual running time of our algorithm is $\\widetilde{O}(n\\cdot \\min (k, d_{avg}))$, which is always asymptotically better than the method of Jiang et al. We will make it clearer in the abstract of the next version of the paper.\n\nOn the second question, we highlight that our technical contribution is to develop the relationship between the approximation guarantee from cluster centers in (coreset) kernel space and the one for graph partition.  To achieve this, we employ the techniques from kernel $k$-means and spectral clustering, the two fields which had been studied separately in the past."
            }
        },
        {
            "summary": {
                "value": "The paper develops new tools in coreset construction merging ideas from two different problems: one is the coresets for k-means and kernel k-means clsutering and the other is spectral clustering, hence the name Coreset Spectral Clustering of the paper.\n\nThe main result is to give an approximation algorithm for the problem of normalized cut based on coresets. Specifically, they can approximately solve the problem on the coreset graph and prove that this is enough to get a reasonable approximation on the original input graph. The authors also perform experiments and demonstrate that their approach leads to asymptotically faster results on large real-world graphs with many clusters beating prior coreset kernel k-means approaches for sparse kernels.\n\nThe second result of the paper is to speed up the running time of the current state-of-the-art coreset algorithm for the problem of kernel k-means on sparse kernels, where the speed up depends on the average degree of the graph."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "-nice idea to rely on kernel sparsity that yields the first coreset construction for kernel spaces and leads to speed up which is especially useful for large graphs with many clusters.\n\n-the two main protagonists here which are spectral clustering and kernel k-means are studied often separately, and I view this approach of merging ideas/techniques interesting.\n\n-the coreset spectral clustering algorithm is interesting and gives a clean result statement: an \\alpha-approximation of the normalized cut problem on the coreset graph will  in fact give an O(\\alpha)-approximation of the normalized cut problem on the original graph. To me this is a very useful and interesting statement as it can be used as a black box and lead to practical results as well."
            },
            "weaknesses": {
                "value": "-novely: while the paper draws inspiration and combines cleverly prior works on normalized cut, kernel clustering and coresets, I wanted to point out that the current paper seems to heavily rely on ideas and techniques that were developed before. Of course the authors had to cleverly combine them in order to get the clean statement as their main result. I also read parts of the technical proofs in the appendix, and I believe that in terms of techniques the paper is a bit weak. Perhaps the authors could elaborate on what crucial ideas in terms of techniques were the novel aspects of this work. Specifically the analysis of Jiang et al. seems to be doing the heavy lifting in many parts of the paper, and conditioned on that  paper, I believe  the current technical contribution appears to be slightly less solid. This is my only concern about the paper, otherwise I do like the paper."
            },
            "questions": {
                "value": "-The authors say their speed up is from nk to nd where d is the average degree in some sense. In the abstract this is a bit confusing: but is this necessarily a speedup; what if k is relatively small but the average degree in the kernel matrix leads to more non-zero entries? Perhaps this is good to clarify early on as you do later in the main body cause the reader might be confused.\n\n-While reading the paper, many ideas used in the analysis are actually coming from (and are cited) from the prior work by Jiang et al. I was curious if the authors could elaborate on what ideas were the novel part of the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper leverages the equivalence between kernel kmeans and spectral\nclustering to improve spectral clustering. As a secondary result they also\nimprove coreset construction for sparse matrices.\n\nThe equivalence between kernel kmeans and spectral clustering is well known.\nIt is, therefore, natural to expect improvements in kernel kmeans\nalgorithms to produce better spectral clustering. Results along this line\nwere recently described by Jiang24, performing kernel kmeans on\nweighted sampled data (coreset).\n\nThe paper argues that improving coresets do not necessarily lead to\nimproved spectral clustering because the kernel kmeans typically gets\nstuck in a local minimum. By contrast, spectral clustering computes\nan approximation to the global optimum, and does not gets stuck in local\nminima.\n\nUsing this key observation the authors propose a novel framework of going\nback and forth between \nthe graph and the points in high dimensional space that are represented\nby the coreset. This improves the speed, but not the quality of the clustering \n(as measured by NCUT). The paper shows that the reduction in quality\nis linear."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is very nicely written. It describes a result that appear interesting\nin theory and useful in practice."
            },
            "weaknesses": {
                "value": "An important side result is the fast construction of a coreset that can be used\nfor kernel k-means clustering. The improvement comes from a fast\n$D^2$ sampling technique. I believe that there are other, competitive\nfast sampling techniques and I was missing a comparison.\nHere is an example:\n\nChib and Greenberg, 1995, Understanding the Metropolis-Hastings algorithm,\nThe American Statistician.\n\n\nIn addition please see the questions below."
            },
            "questions": {
                "value": "The result: Why are the derivation and experiments discussing only NCUT?\nThe equivalence of Dhillon04 was extended in Dhillon07 to other criteria,\nin particular RatioCut. It should also apply to the newer stochastic box model.\n\n\nExperimental results: why is there no comparison of the NCUT values\nthat were obtained in the experiments? The current evaluation is\nin terms of ARI, but this is not what the algorithms attempt to maximize."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 10
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an algorithm coreset spectral clustering algorithm for k-means clustering. This is done by first converting the input graph into a k-means problem instance, constructing and $\\epsilon$-coreset for this instance, then solving the spectral clustering problem on the reduced graph. A second contribution is an algorithm for fast $D^2$-sampling utilized in coreset construction, which results in an coreset construction algorithm with running time $\\widetilde{O}(n d_{avg})$."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The contribution of the paper is solid, with the main idea being combining the approaches of coreset construction and spectral clustering. The utilization of sparsity to improve the running time of clustering algorithm is also well-executed.\n- The presentation is overall excellent, with all of the contributions stated clearly. Schemes and easy-to-read pseudocode are very helpful with understanding the approach.\n- The experimental section is detailed and well-organised."
            },
            "weaknesses": {
                "value": "It is somewhat unclear how often it is desired to solve spectral clustering on sparse data, or whether settings of interest have $d_{avg} < k$. I would like the authors to add an overview on how clustering methods are used in the empirical research, for example social network analysis, in the introduction or related work."
            },
            "questions": {
                "value": "Please address the concern that I raised in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper tackles the challenges of clustering large, sparse datasets, where traditional spectral clustering methods can be computationally demanding. While spectral clustering is widely used for identifying non-linear cluster boundaries, its dependence on dense similarity matrices restricts scalability, particularly when dealing with numerous clusters. The authors introduce Coreset Spectral Clustering (CSC), a method that merges the efficiency of coreset sampling with the accuracy of spectral clustering, achieving a substantial speedup while maintaining clustering precision."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- CSC is optimized for sparse graphs, where the sparsity structure significantly reduces both computation and memory usage. By using a small, representative subset of data (the coreset), CSC scales well with data size and can handle graphs with millions of nodes and thousands of clusters. This scalability makes CSC suitable for large datasets in social networks, biological clustering, and sensor network analysis, where traditional methods would struggle.\n\n- Standard spectral clustering can become infeasible with large, dense similarity matrices due to the high demands on computation and memory. CSC addresses this by working with a sparse kernel matrix and clustering only on the coreset, significantly reducing matrix size and computational cost. This efficiency enables CSC to process large datasets on standard hardware, which would otherwise require extensive resources for traditional spectral clustering.\n\n- A smaller coreset speeds up computation, while a larger coreset captures more nuances in the data structure. This adaptability is useful for applications with specific accuracy or runtime needs, making CSC versatile across different types of data and clustering goals."
            },
            "weaknesses": {
                "value": "- The accuracy of CSC\u2019s clustering largely depends on the representativeness of the coreset. To achieve high-quality clusters, the coreset need to accurately capture key structural and distributional aspects of the dataset. In datasets with uneven distributions or subtle data patterns, it could be difficult to create a coreset that fully represents the original data, and even minor inaccuracies could impact clustering results.\n\n- CSC relies on an initial similarity or nearest-neighbor graph, and parameters such as the number of neighbors (k) or distance threshold (\u03f5) can significantly affect clustering performance. Choosing suboptimal values for these parameters may lead to an inaccurate initial graph structure, impacting the quality of the final clusters."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a refined algorithm of constructing a coreset for kernel k-means problem. They improves the time complexity from $\\tilde{O}(nk)$ [Jiang et al. ML' 24] to $\\tilde{O}(nd_{avg})$, where $d_{avg}$ is the average number of neighbors of a single vertex on the graph defined by the given similarity matrix. They also showed how to use their technique to improve spectral clustering and obtain a approximate solution for normalized cut problem. The experiments are designed to support their theoretical results."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed technique of constructing a coreset is quite useful when k is large and the similarity is sufficiently sparse."
            },
            "weaknesses": {
                "value": "1. Limited contribution. The proposed method highly depends on the former work [Jiang et al. ML' 24]. And their claimed improvements seems trivial. Theorem 1 is also easy to obtain. \n2. This paper assumes that the similarity matrix is sparse, which means a vertex has only few neighbors. So when a vertex is sampled, only its neighbors ($d_{avg}$ neighbors on average) need to update the their distance to the sampled set. Therefore, the time complexity of $\\tilde{O}(nd_{avg})$ is straightforward.\n3. The experimental on the Appendix A seems not ideal. For example, in Figure 5,6,7, the proposed method does not obtain the best ARI; Figure 7 also shows that the green baseline is actually faster. And there is no explanation for that."
            },
            "questions": {
                "value": "1. In the experimental part, the ARI performance of yours is much better than the green baseline (which is the method of [Jiang et al. ML' 24]). But I think your result is mainly based on the green baseline and you improve their running time. So it makes sense that your method is faster. But why your ARI is so much better than the green baseline?\n2. In the experimental part, you mention that you use the nearest neighbor graphs of MNIST. How to construct such a graph on MNIST? Is it a nearest neighbor graphs based on Euclidean distance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}