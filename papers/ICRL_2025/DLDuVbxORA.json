{
    "id": "DLDuVbxORA",
    "title": "OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition",
    "abstract": "The recent paradigm shift to large-scale foundation models has brought about a new era for deep learning that, while has found great success in practice, has also been plagued by prohibitively expensive costs in terms of high memory consumption and compute. To mitigate these issues, there has been a concerted effort in post-hoc neural network pruning techniques that do not require costly retraining. Despite the considerable progress being made, existing methods often exhibit a steady drop in model performance as the compression increases. In this paper, we present a novel approach to compressing large transformers, coined OATS, that compresses the model weights by approximating each weight as the sum of a sparse matrix and a low-rank matrix. Prior to the decomposition, the weights are first scaled by the second moment of their input embeddings, so as to ensure the preservation of  outlier features recently observed in large transformer models. Without retraining, OATS achieves state-of-the-art performance when compressing large language models, such as Llama-3 and Phi-3, and vision transformers, such as Google's ViT and DINOv2, by up to $60\\\\%$, all while speeding up the model's inference on a CPU by up to $1.37\\times$ compared to prior pruning methods.",
    "keywords": [
        "network pruning",
        "low-rank",
        "compression",
        "sparsification",
        "large language models",
        "outlier features"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=DLDuVbxORA",
    "pdf_link": "https://openreview.net/pdf?id=DLDuVbxORA",
    "comments": [
        {
            "title": {
                "value": "Question about CPU throughput tests presented in Section 3.4"
            },
            "comment": {
                "value": "I hope this message finds you well.\n\nI have a question regarding the CPU throughput tests presented in Section 3.4 of your paper. Specifically, I was wondering if any hardware acceleration techniques, such as compression or other methods, were used during these tests. If not, in scenarios where the total data volume (SUV matrix > Weight matrix) is larger, what strategies would you suggest for improving throughput?\n\nThank you for your time, and I look forward to your insights.\n\nBest regards"
            }
        },
        {
            "summary": {
                "value": "The paper introduces OATS (Outlier-Aware Pruning Through Sparse and Low-Rank Decomposition), a method designed to compress large transformer models without the need for retraining. The central concept involves representing weight matrices as the sum of a sparse and low-rank matrix.\n\nThe authors propose an iterative alternating thresholding technique to compute the joint sparse and low-rank decomposition of a matrix. They also focus on preserving outliers by scaling weights according to input embeddings prior to decomposition. The results indicate that OATS performs effectively on both language and vision tasks, outperforming other pruning methods proposed for transformers across various compression levels and tasks. Additionally, OATS offers speed improvements on the CPU."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The concept of compressing a model as a sum of a sparse and low-rank matrix is very promising. Unlike most prior methods, which focus on one approach, OATS leverages both to potentially enhance performance.\n\n- OATS is retraining-free, which is crucial for practical applications where even a single backpropagation pass can be computationally prohibitive.\n\n- The framework has been tested on state-of-the-art models like Llama and ViT, demonstrating competitive performance.\n\n- The Alternating Thresholding technique in OATS heuristically finds an effective combination of low-rank and sparse components and accommodates different sparsity patterns.\n\n- By scaling the weight matrix $W$ with a diagonal rescaling matrix $D$, OATS emphasizes outliers, enabling outlier-aware compression that avoids performance degradation."
            },
            "weaknesses": {
                "value": "One concern is that the method relies on multiple calls to truncated SVD, which can be computationally intensive. Specifically, finding the top-$r$ singular values of an $m \\times n$ matrix has a time complexity of $O(mnr)$. Given that compression speed is a significant factor for practical applications, it would be helpful if the authors could clarify the time complexity and wall-clock time spent on the compression process of the overall algorithm. This would offer a more concrete understanding of its practicality."
            },
            "questions": {
                "value": "One potential extension of this work could involve incorporating quantization into the proposed framework. Although this addition may be a long shot, integrating quantization could make the model a unified approach to transformer compression. Could the authors provide insights on whether quantization can be integrated with their current framework, or if not, what are the main challenges?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces OATS, a novel compression technique for large-scale transformers that combines sparse and low-rank approximations to reduce memory and compute costs without requiring costly retraining. OATS scales model weights by the second moment of input embeddings to preserve essential outlier features in transformers, ensuring model performance is maintained during compression. This approach addresses the typical performance degradation seen with increasing compression in existing pruning methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "+ Low-rankness plus sparsity is a good fit for compressing LLMs without retraining.\n+ By separating the model into a sparse and a low-rank part, the approximation error can be theoretically reduced as the two parts can compensate for each other. \n+ This paper provides measurements for practical speedups on CPUs with existing sparse computation frameworks."
            },
            "weaknesses": {
                "value": "- The novelty is limited. The combination of low-rankness and sparsity is an old topic that has been explored for many years [R1, R2]. Applying the well-established approximation techniques to decompose/compress the large matrices in LLMs has little technical contribution. Besides, compressing DNN models using low-rank and sparse decomposition has already been well explored in [R3]. This paper just scales it to larger models and matrices. Authors are encouraged to specify the unique difference from existing approaches and why this difference is also unique for LLMs.\n- The proposed Truncated SVD and Threshold strategies to achieve low-rankness and sparsity are too trivial. It is unknown how to decide the rank and number of zeroes. Besides, the order of applying SVD and thresholding has a significant impact on the approximation errors. Authors are encouraged to clearly explain why using such decomposition strategies.\n- This paper claims \"outlier information\" in this paper. However, I have not seen any analysis or explanation for the \"outlier information,\" and the proposed solution is not related to the \"outlier information.\" Instead, this paper seems to directly apply the pruning approaches proposed in Wanda. The authors are encouraged to provide explanations of why the diagonal matrix is related to \"outlier information\" and why it is good for compression.\n- Many works have been proposed to compress LLMs with low-rankness and sparsity [R4-R6]. The authors have not presented the main differences among them and the unique contributions that stand out from those works.\n- Even though theoretical analysis may not have a practical guarantee of accuracy, the authors are encouraged to provide.\n- The paper presentation could be improved, especially the math equations. \n\n[R1] Sparse and Low-Rank Matrix Decompositions, Forty-Seventh Annual Allerton Conference, 2009.\n\n[R2] Godec: Randomized low-rank & sparse matrix decomposition in noisy case. ICML 2011.\n\n[R3] On compressing deep models by low rank and sparse decomposition, CVPR 2017.\n\n[R4] Slope: Double-pruned sparse plus lazy low-rank adapter pretraining of llms\n\n[R5] LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation\n\n[R6] SLiM - One-shot Quantized Sparse Plus Low-rank Approximation of LLMs"
            },
            "questions": {
                "value": "See the Weakness. Additionally, why use inverse transformation to reach the compressed weight? What is the actual speedup when using N:M sparsity on GPUs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a relatively novel model compression technique aimed at Transformer architectures, in which weight matrices are approximated as a sum of a sparse and a low-rank matrix. To control for the (previously documented) outlier feature problem, weights are also scaled by the second moment of their input."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The method proposed by this paper is well-explained and well-justified. The actual practical algorithm is easy to follow. \n\nReal-time speedup is shown in the CPU setting.\n\nThe experiments cover a range of model sizes (3.8-14B parameters). I especially liked seeing results on fairly small models, since those may be harder to compress. \n\nSection 5 is an interesting way to look at the problem, which I think can lead to interesting further work in the direction of interpretability."
            },
            "weaknesses": {
                "value": "The choice of the rank ratio parameter could have been better explored (in particular, looking at multiple architectures/tasks). \n\nTypos:\nLine 18: \u201capproximating each weight\u201d -> \u201capproximating each weight matrix\u201d\nLine 142: \u201cthe activations are calculated through a calibration set that is propagated through the compressed layers\u201d - should be uncompressed?"
            },
            "questions": {
                "value": "It would be nice to see a bigger hyperparameter selection section with more architerctures considered."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the author presents a novel approach to compressing large transformers, coined OATS, that utilizes the second moment information in the input embeddings to decompose the model weights into a sum of sparse and low-rank matrices. The author also conducts a lot of experiments showing that OATS is able to consistently outperform prior state-of-the-art across multiple benchmarks and compression rates while also improving on speed-up."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.This paper propose a novel method for large transformers compression that utilizes the second moment of the input embeddings to approximate the model\u2019s weight matrices as a sum of a sparse matrix and a low-rank matrix.\n\n2.Extensive experiments on recent large language models demonstrate the effectiveness of the proposed method, which also generalizes well to vision Transformers."
            },
            "weaknesses": {
                "value": "Please see the questions."
            },
            "questions": {
                "value": "1. The author has clarified the differences between Wanda and OATS. However, an ablation study would further illustrate the impact of the low-rank term on performance. How much does the low-rank term contribute to the performance boost compared to Wanda alone?\n2. The hardware speedup on GPU with N:M sparsity patterns can be shown and discussed in Section3.4.\n3. The paper lacks details on how the sparsity pattern is defined for a matrix composed of a sparse matrix S plus a dense matrix LLL, especially within the context of N:M sparsity and structured pruning. Providing more detailed explanations would enhance clarity. \n4. The baseline for models are all originally designed for LLMs. It would be valuable to compare against pruning metrics specifically designed for ViTs. This would present a stronger baseline to effectively showcase the proposed method\u2019s advantages.\n5. The motivation behind the choice of outlier information for the sparse term S is not entirely clear. Given that there are various methods for selecting the sparse components, such as using magnitude-based or gradient-based criteria, it would be helpful to know if the authors experimented with these or other selection methods.\n6. Given that most pruning studies report results on 70B-parameter models, has the author tested their method on larger language models, such as Llama2-70B or the newer Llama3-70B?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}