{
    "id": "dML3XGvWmy",
    "title": "G\u00f6del Agent: A Self-Referential Framework Helps for Recursively Self-Improvement",
    "abstract": "The rapid advancement of large language models (LLMs) has significantly enhanced the capabilities of AI-driven agents across various tasks. However, existing agentic systems, whether based on fixed pipeline algorithms or pre-defined meta-learning frameworks, cannot search the whole agent design space due to the restriction of human-designed components, and thus might miss the globally optimal agent design. In this paper, we introduce G\u00f6del Agent, a self-evolving framework inspired by the G\u00f6del machine, enabling agents to recursively improve themselves without relying on predefined routines or fixed optimization algorithms. G\u00f6del Agent leverages LLMs to dynamically modify its own logic and behavior, guided solely by high-level objectives through prompting. Experimental results on mathematical reasoning and complex agent tasks demonstrate that implementation of G\u00f6del Agent can achieve continuous self-improvement, surpassing manually crafted agents in performance, efficiency, and generalizability.",
    "keywords": [
        "Agent",
        "Large Language Model",
        "Reasoning",
        "Self-Improvement"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "We introduce G\u00f6del Agent, a self-referential framework, enabling agents to recursively improve themselves without relying on predefined routines or fixed optimization algorithms.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=dML3XGvWmy",
    "pdf_link": "https://openreview.net/pdf?id=dML3XGvWmy",
    "comments": [
        {
            "title": {
                "value": "Official Response to Reviewer J1mU by Authors -- Part Two"
            },
            "comment": {
                "value": "> Section 3 is notably small and lacks clarity on the method\u2019s implementation. It would benefit from additional details, particularly on how self-improvement is enacted within the Godel Agent and how evaluations are conducted.\n\nThe core idea behind G\u00f6del Agent is to create a self-referential system that can recursively modify its own components for continuous self-improvement. The initial setup focuses on enabling this recursive self-modification capability. Once the system is initialized, the LLM autonomously decides which modules to modify based on feedback from its current performance.\n\nRegarding the self-improvement process, the LLM monitors the rewards from its current policy as well as its internal state information. Based on this feedback, the LLM generates action instructions that can modify various components of the agent, such as its policies, tools, or even its decision-making logic. This recursive process allows G\u00f6del Agent to iteratively optimize itself over time.\n\nAs for evaluations, after G\u00f6del Agent has optimized its policy, the resulting algorithm is tested on the target task\u2019s test set. We use standard evaluation metrics such as accuracy or F1 score to assess the performance. \n\n> Lines 215-233 imply that evaluations rely on the environments provided. If this involves using the test set for evaluation, it represents a significant limitation of the method.\n> \n\nThe agent does not have access to the test data. In fact, during the evolving process, we only utilize validation data, as detailed in Appendix B. To ensure a fair comparison with baseline methods, the selection of validation data follows the approach used in the paper[2]. We use the scores on the validation set as feedback from the environment to guide the G\u00f6del Agent in optimizing its policies.\n\n> How the solutions in Godel Agent are evaluated (referenced in line 6 of the pseudocode) needs clearer explanation.\n> \n\nThe evaluation of solutions in G\u00f6del Agent is conducted using an environment-provided utility function. Specifically, after G\u00f6del Agent optimizes its policy, the resulting policy (which represents a task-specific algorithm) is tested on the test set of the target task. The utility function assesses the policy\u2019s effectiveness by calculating standard performance metrics such as accuracy or F1 score.\n\n> It would be beneficial to compare the cost of the Godel Agent with other methods, such as the CoT, both in terms of time and financial resources. The authors might want to reevaluate their claims and revise the paper to provide a clearer presentation of the proposed method.\n> \n\nThank you for your suggestion.  Our computational cost can be broken down into two parts:\n\n1. **Self-Improvement Phase**: G\u00f6del Agent incurs additional computational costs during the self-improvement phase, as detailed in Appendix D. Compared to approaches like ADAS, our method significantly reduces these costs by optimizing the agent's capabilities autonomously. While CoT-based methods do not require this self-improvement phase, they rely heavily on human experts for design, which involves a substantial time and financial investment. One of the main motivations for G\u00f6del Agent is to replace this manual effort with computational self-optimization, thereby reducing the need for human intervention.\n2. **Testing Phase**: The testing cost varies across different methods. For instance, methods like CoT-SC and debate require multiple queries and thus have significantly higher computational costs compared to standard CoT. In contrast, G\u00f6del Agent's testing costs generally fall between those of CoT and CoT-SC. Notably, in some cases, G\u00f6del Agent evolves strategies that reduce or even eliminate the need for LLM queries, achieving much lower computational costs.\n\n[1] Goedel Machines: Self-Referential Universal Problem Solvers Making Provably Optimal Self-Improvements\n\n[2] Automated Design of Agentic Systems"
            }
        },
        {
            "title": {
                "value": "Official Response to Reviewer  J1mU by Authors -- Part One"
            },
            "comment": {
                "value": "Thanks for your careful and valuable comments. We will explain your concerns point by point.\n\n> There are several ambitious claims within the paper; for example, the abstract suggests that the method can explore the entire space to find the optimal solution. This is a bold statement and does not seem to be substantiated by the findings presented.\n> \n\nIn theory, G\u00f6del Agent is designed to explore the solution space and find the optimal solution within the computational resources available, as demonstrated in the G\u00f6del machine paper[1]. However, due to practical limitations, such as computational resource constraints and the inherent instability of LLM decision-making, it is possible that the method may not always achieve the theoretical optimal in real-world applications.\n\nWe appreciate your critique, and we will revise the paper to express this point more clearly and cautiously. Specifically, we will emphasize that while G\u00f6del Agent can theoretically explore and optimize solutions within the available resources, practical challenges may affect the extent to which this optimality is achieved in real-world scenarios.\n\n> The central claim about the capability to create \"different agents\" from an initial agent mostly involves modifying the conditioning of the autoregressive order through prompt engineering.\n> \n\nIt seems there might be a misunderstanding regarding the core mechanism of G\u00f6del Agent. To clarify, we do not create \"different agents\" through prompt engineering. Rather, there is a single G\u00f6del Agent that continuously refines and improves itself over multiple iterations.\n\nThe key difference is that G\u00f6del Agent does not simply adjust its behavior through prompt modifications. Instead, it engages in a recursive process where the agent modifies its own underlying code to optimize its capabilities. For example, the initial agent $a_0$ uses its current state to improve itself, resulting in $a_1$. Then, $a_1$ uses its enhanced capabilities to further refine itself into $a_2$, and so on. This self-referential and recursive improvement process goes beyond trivial prompt adjustments\u2014it fundamentally changes the agent\u2019s optimization abilities over time.\n\n> Figure 1 shows some learnable components, yet the framework itself does not appear to incorporate learnable elements.\n> \n\nTo clarify, G\u00f6del Agent\u2019s framework indeed incorporates learnable elements. The action space, tools, and policies are not static but are dynamically modifiable by the agent. G\u00f6del Agent can autonomously refine its action space, create and adjust its tools, and optimize its policies based on feedback during its self-improvement process. These components are effectively learnable because they can be adapted and optimized over time as the agent evolves.\n\n> Line 50 claims \"to eliminate the human design prior,\" which is challenging given that LLMs inherently contain human priors from being trained on human-generated text.\n> \n\nYou are absolutely correct that LLMs inherently carry human priors because they are trained on human-generated text. However, in line 50, our intention was to convey that we aim to reduce the human priors specifically in the design of the agent\u2019s logic, strategies, and decision-making processes. While LLMs themselves are inevitably influenced by their training data, our goal is to allow the agent to operate autonomously, minimizing manual human intervention in its policy design. In other words, while the underlying LLM contains human priors, the agent\u2019s higher-level decision-making and optimization processes are designed to evolve independently, free from predefined human-designed rules or biases. We acknowledge that complete elimination of human influence is not feasible due to the nature of LLMs, but our focus is on reducing human biases in the agent\u2019s self-improvement framework.\n\n> Discussions on self-awareness of LLMs are speculative and might be better left out of a scholarly paper focused on introducing a new method.\n> \n\nThank you for your thoughtful comment and suggestion. We apologize for any confusion caused by our use of the term \"self-awareness.\" In this context, we intended to convey that G\u00f6del Agent has the capability to introspect and read its own code and files, not to imply any philosophical sense of consciousness or awareness. We have emphasized this point more clearly in the revised version of the paper. If you find that the current explanation is still not sufficiently clear, would using the term of *self-introspection* be more appropriate?"
            }
        },
        {
            "title": {
                "value": "Official Response to Reviewer xqBT by Authors"
            },
            "comment": {
                "value": "Thanks for your insightful comments. We are happy to discuss with you about your points.\n\n> 1.  Why LLM-based agent is a valid approach? No LLM is perfect. All LLMs make mistakes. What is reliable information? What is ground truth? Then why should we rely on LLMs to make decisions? For software engineering, no LLM can achieve a pass@1 of 100%. To make it worse, the popular evaluation method with several tests, is not perfect, so the evaluation results are not fully reliable.\n> \n> 1. How to make LLM-based agent a valid approach? LLMs may help generate plausible solutions. But there should be a mechanism to filter out or fix incorrect solutions.\n> \n> How to improve an LLM-based agent? It seems this boils down to waiting for stronger LLMs, which is likely out of the scope of the paper, though.\n> \n> 1. There are papers discussing / criticizing the reasoning / planning capacity of LLMs, which is the foundation of LLM-based agent (including those based on the G\u00f6del machine), e.g. 1) LLMs can\u2019t plan, but can help planning in LLM-modulo frameworks, in ICML, 2024. 2) GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models. arXiv 2024\n> \n> Without sound capabilities for reasoning / planning, how to build LLM-based agents, in particular, the G\u00f6del Agent?\n> \n> It appears that empirical improvements are not enough to answer the above questions.\n> \n\nWe fully acknowledge that LLMs have limitations, make mistakes, and have significant room for improvement. However, it is also undeniable that LLMs have achieved remarkable results across many tasks and continue to improve. In our work, we propose utilizing LLMs to enable G\u00f6del Agent\u2019s self-improvement, focusing on leveraging their strengths rather than dwelling on their imperfections.\n\nG\u00f6del Agent\u2019s approach doesn\u2019t rely solely on LLMs for decision-making. Instead, it incorporates LLMs as part of a recursive, self-improvement process where the agent can autonomously refine its policies and improve its performance over time. This self-referential mechanism allows G\u00f6del to evolve beyond the limitations of the initial LLM and address its shortcomings through continuous self-modification. \n\nWe also want to emphasize that our framework is not tied exclusively to LLMs as decision tools. If alternative methods for decision-making exist, our framework is flexible enough to incorporate them, which makes it adaptable to different settings and tools.\n\nWe ask that you focus on evaluating the innovative contributions of our framework and the novel approach it offers for agent self-improvement. While we are open to further discussion of LLMs' limitations, we kindly request that you reconsider the evaluation of our paper based on the broader contributions of our approach, as it provides a unique perspective on agent self-improvement. Thank you again for your feedback."
            }
        },
        {
            "title": {
                "value": "Official Response to Reviewer xqBT by Authors -- Part Two"
            },
            "comment": {
                "value": "> Based on Appendix C, the best policy found by Godel Agent for MMLU is the same as CoT-SC, why are the two numbers reported in Table 1 different? Similarly for the best policy for GPQA, why is it different from CoT? If this is due to extended few-shot examples or specific instructions, then it seems more like a prompt engineering problem, suggesting we're not using optimal prompts.\n> \n\nYou are correct that the best policies found by G\u00f6del Agent for MMLU and GPQA are similar in methodology to CoT-SC and CoT, respectively. The differences in the reported performance in Table 1 indeed stem from G\u00f6del Agent\u2019s use of extended few-shot examples and specific instructions.\n\nHowever, we do not view this as a limitation. On the contrary, this highlights G\u00f6del Agent\u2019s capability to autonomously optimize prompts, which includes fine-tuning few-shot examples and refining specific instructions to achieve better performance. If a human were to manually adjust prompts to optimize results, it would be a time-consuming and labor-intensive process. G\u00f6del Agent, on the other hand, integrates this prompt optimization seamlessly into its self-improvement process. Thus, G\u00f6del Agent is not only optimizing policies from a methodological standpoint but also refining prompts in a way that enhances performance, as demonstrated by our experimental results.\n\n> The reason why the method is evaluated on MGSM instead GSM8K is not well-justified, for my understanding, multi-lingual is not a target of the framework. Moreover, all current benchmark, even regular CoT has high accuracy, this together with limitation, I am questioning the generalizability of the framework on harder tasks, like MATH, MATH500,omni-MATH and SWEBench.\n> \n\nFrom a theoretical perspective, our framework is task-agnostic\u2014provided that the environment can provide feedback, G\u00f6del Agent can adaptively improve itself, making it applicable to both controlled and more complex real-world tasks, including embodied environments. However, we fully agree that experiments on more complex, real-world tasks would provide stronger evidence of the framework\u2019s scalability. However, we chose the current set of tasks for the following reasons:\n\n1. The baseline methods we are comparing against have only been tested on these controlled tasks, so using the same tasks ensures a fair and consistent comparison.\n2. As this is the first proposal of such a framework, we wanted to demonstrate its feasibility and core capabilities on representative tasks before extending it to more complex scenarios. More complex tasks might distract from highlighting the novel aspects of our framework at this stage as they need more engineering work.\n\n> For the related work, some recent recursive self-improvement work is not discussed.\n> \n\nThank you for pointing out these recent works. Below, we compare these approaches to our G\u00f6del Agent and we have added them in our current version:\n\n1. Glore proposes SORMs to improve LLM reasoning through global and local refinements, using synthetic data to predict correctness of solutions. It focuses on identifying when and where to refine reasoning steps but does not involve modifying the model's architecture or self-improvement mechanisms like G\u00f6del Agent.\n2. V-star trains a verifier to evaluate both correct and incorrect self-generated solutions, iteratively improving reasoning accuracy. Unlike G\u00f6del Agent, which autonomously modifies its own architecture, V-star uses external verifiers to guide the refinement of reasoning.\n3. RISE enables recursive self-improvement by fine-tuning models to introspect and correct previous mistakes in multiple iterations. G\u00f6del Agent differs by incorporating a recursive mechanism that includes altering the architecture, not just refining responses.\n4. SCoRe uses reinforcement learning to improve self-correction in LLMs by learning from self-generated correction traces, avoiding issues like behavior collapse.\n\n> How to safeguard the current workflow if the model is allowed to modify the policy and interaction code directly, and the policy will be used to generate further responses. How can we prevent runtime memory issues and ensure that both the code implementation and the final generation are not harmful?\n> \n\nEnsuring the safety of G\u00f6del Agent during self-modification is indeed a top priority. To prevent potential misuse of resources, we recommend running G\u00f6del Agent within a sandboxed environment where system permissions and resource access are tightly controlled. We have added an Ethics Statement at the end of the paper. Thank you for your suggestion!\n\nWe hope that our responses have addressed your questions and concerns. We also hope that you find our approach exciting, as it represents a fundamental departure from previous methods. If you have any further questions, please feel free to reach out, and we will be happy to address them. Thank you once again for your valuable feedback."
            }
        },
        {
            "title": {
                "value": "Official Response to Reviewer xqBT by Authors -- Part One"
            },
            "comment": {
                "value": "Thanks for your careful and valuable comments. We will explain your concerns point by point.\n\n> The motivation for adding four additional tools is not well-justified; there is no evidence supporting that these are the correct tools to add or that they comprehensively cover all needs. It's counterintuitive because if we claim the model has the ability to refine its policy and develop new policies, why do we need to provide tool guidance? Additionally, it's notable in Appendix C that although there is variability in the best policy, this policy seems can be decomposed into a combination of the listed tools, which suggests this work is not fully distinguished from meta-learning optimized agents where humans manually define the high-level policy, and the agent learn a strategy to combine or rank the high-level strategy.\n> \n\nThe motivation for initially providing G\u00f6del Agent with four tools (Chain-of-Thought, error handling, code execution, and API calls) stems from the current limitations of existing LLMs. While G\u00f6del Agent is designed to autonomously refine its strategies and develop new tools, the capabilities of current LLMs are not yet sufficient to handle more complex tasks without additional support. The inclusion of these tools serves to enhance the agent\u2019s initial performance and practicality, given these constraints.\n\nIt\u2019s important to note that these tools are not hard-coded limitations. G\u00f6del Agent has the flexibility to modify, extend, or even replace these tools as it optimizes itself. The tools provided are simply starting points to bridge current limitations, allowing G\u00f6del to function effectively in its current environment. Over time, G\u00f6del can autonomously evolve beyond the initial toolset as its capabilities improve.\n\nRegarding the concern that our approach is similar to meta-learning agents where humans define high-level strategies, we would like to emphasize a key distinction: G\u00f6del Agent is not limited to combining predefined strategies. For instance, in the \"Game of 24\" task (Section 5.3), G\u00f6del Agent developed a completely new search-based algorithm that achieved 100% accuracy. This represents a qualitative leap beyond what Meta Agent Search can achieve, as the latter is constrained by predefined search spaces and prompt-based strategies.\n\n> The current framework heavily relies on the model's capability to generate and refine policies, as well as generate responses given a policy and query. However, this would not improve the model's ability to acquire new knowledge if such kind of off-line dataset available.\n> \n\nThe performance of G\u00f6del Agent is indeed dependent on the capabilities of the underlying LLM to recognize that modifying its own code and structure will directly impact its performance.\n\nHowever, G\u00f6del Agent does have the potential to surpass the limitations of the base LLM. If the LLM is advanced enough to understand how its own parameters influence the agent\u2019s overall performance, G\u00f6del could theoretically go further by generating code to fine-tune or even replace the underlying LLM. This self-modification could enable the agent to evolve beyond the capabilities of the original LLM, effectively enhancing its performance beyond its initial constraints.\n\n> The current refinement requires an oracle to progress the performance (U in line 6 from Algorithm 1). For the current Table 1, is the number of oracle utility function calls the same across all methods, and is the number of model queries the same for different settings?\n> \n\nRegarding the use of the oracle utility function, it is true that our method relies on an environment-provided utility function for optimization. During this phase, G\u00f6del Agent autonomously decides when to call the utility function to obtain rewards on the validation set, similar to how human experts adjust their methods based on validation feedback. However, once the policy optimization is complete, the final policy uses the utility function in the same way as other methods, ensuring consistency across approaches in the evaluation phase.\n\nRegarding the number of model queries, it is indeed different across various methods. This variation is inherent due to the nature of each approach. For instance, CoT only requires a single query, while methods like CoT-SC and debate require multiple queries to achieve their results. As the framework we propose focuses on self-improvement, our primary concern is evaluating the effectiveness of each method rather than strictly equalizing the number of queries."
            }
        },
        {
            "title": {
                "value": "Official Response to Reviewer waxH by Authors -- Part Three"
            },
            "comment": {
                "value": "> Can this approach scale to more complex tasks that require long-term planning or interaction with unpredictable environments?\n> \n\nWe believe that G\u00f6del Agent\u2019s approach can indeed scale to more complex tasks that require long-term planning and interaction with unpredictable environments. The key factor is the availability of feedback or rewards from the environment. As long as the environment can provide such feedback, G\u00f6del Agent is capable of optimizing its strategies to adapt to the given task.\n\n> How does the agent's performance depend on the underlying LLM's capabilities? Can it surpass those limitations?\n> \n\nThe performance of G\u00f6del Agent is indeed dependent on the capabilities of the underlying LLM, particularly in terms of its decision-making, planning, and its ability to understand that it is part of G\u00f6del Agent. The agent relies on the LLM to recognize that modifying its own code and structure will directly impact its performance.\n\nRegarding the second part of your question, G\u00f6del Agent does have the potential to surpass the limitations of the base LLM. If the LLM is advanced enough to understand how its own parameters influence the agent\u2019s overall performance, it could theoretically go further by generating code to fine-tune or even replace the underlying LLM. This self-modification could enable the agent to evolve beyond the capabilities of the original LLM, effectively enhancing its performance beyond its initial constraints.\n\nWe hope that our responses have addressed your questions and concerns. If you have any further questions, please feel free to reach out, and we will be happy to address them. Thank you once again for your valuable feedback."
            }
        },
        {
            "title": {
                "value": "Official Response to Reviewer waxH by Authors -- Part Two"
            },
            "comment": {
                "value": "> More detailed comparison to ADAS is needed in related work (Automated Design of Agentic Systems) What is Meta Agent Search and why does this Godel agent perform better?\n> \n\nThank you for the opportunity to clarify the comparison between our method and Meta Agent Search (MAS). In MAS, a fixed meta-agent is manually designed to create task-specific agents by interacting with the environment. However, this meta-agent remains static throughout the process, meaning the final optimized results are inherently constrained by the initial design and capabilities of the meta-agent.\n\nIn contrast, our approach, inspired by the G\u00f6del machine, moves beyond the meta-learning paradigm by endowing the agent with self-referential capabilities. This allows the agent to evolve not only its task-specific part but also its own optimization part. As a result, both the agent's ability to optimize and its performance on downstream tasks improve simultaneously, leading to more efficient and effective solutions.\n\nA clear example of this qualitative improvement can be seen in the \"Game of 24\" task as shown in Appendix C.2. G\u00f6del Agent was able to design a search-based algorithm that solves the task with 100% accuracy, whereas MAS is limited to searching within predefined spaces and designing prompt-based methods. This shows that G\u00f6del Agent can generate solutions that are not only more flexible but also fundamentally more capable than those produced by a fixed meta-agent.\n\n> The method is somewhat vague and not clear what\u2019s going on, and what this has to do with self reference/recursion. Is this different than just tasking an LLM with modifying its own agent code?\n> \n\nIn G\u00f6del Agent, the concepts of self-reference and recursion refer to the agent's ability to autonomously modify its own code to improve both its optimization process and task-solving capabilities. The key difference lies in the fact that after each self-modification, G\u00f6del Agent's optimization abilities themselves are enhanced. This creates a loop where improved optimization leads to more effective self-modification, which is recursively applied to the agent itself rather than an external entity.\n\nThis process is fundamentally different from simply tasking an LLM with modifying its own agent code, as seen in methods like ADAS. In those cases, the LLM\u2019s optimization capability remains static, meaning the changes are limited to task-specific adjustments without fundamentally enhancing the LLM's underlying optimization skills.\n\n> How does G\u00f6del Agent ensure safety and prevent harmful behaviors during self-modification? How do you put constraints on it such that it doesn't blow all of your GPT credits or hog all the GPUs on your system?\n> \n\nEnsuring the safety of G\u00f6del Agent during self-modification is indeed a top priority. To prevent potential misuse of resources, we recommend running G\u00f6del Agent within a sandboxed environment where system permissions and resource access are tightly controlled. We have added an Ethics Statement at the end of the paper. Thank you for your suggestion!\n\n> What mechanisms are in place to handle errors or prevent the agent from degrading its performance over time?\n> \n\nCurrently, G\u00f6del Agent is equipped with an error feedback mechanism: when it performs an action that is invalid or non-functional, the system returns an error message. G\u00f6del Agent uses this feedback to adjust its behavior and refine its strategies.\n\nRegarding performance degradation, we have chosen not to implement additional manual corrections. Instead, we rely on G\u00f6del Agent\u2019s ability to reflect on its actions and autonomously adjust its strategies over time. While it is technically possible to manually evaluate the strategy at each iteration and prevent modifications if performance declines, we have opted against this approach to minimize human intervention. \n\nMoreover, we believe that allowing the agent to occasionally make mistakes can be beneficial. By experiencing negative outcomes, G\u00f6del Agent can learn which modifications are counterproductive, which can inform better decision-making in the future."
            }
        },
        {
            "title": {
                "value": "Official Response to Reviewer waxH by Authors -- Part One"
            },
            "comment": {
                "value": "Thanks for your careful and valuable comments. We will explain your concerns point by point.\n\n> Experiments seem limited in scope\u2014mostly on controlled tasks; might not scale to complex real-world applications or embodied tasks\n\nThank you for your valuable feedback. From a theoretical perspective, our framework is task-agnostic\u2014provided that the environment can provide feedback, G\u00f6del Agent can adaptively improve itself, making it applicable to both controlled and more complex real-world tasks, including embodied environments. However, we fully agree that experiments on more complex, real-world tasks would provide stronger evidence of the framework\u2019s scalability. However, we chose the current set of tasks for the following reasons:\n1. The baseline methods we are comparing against have only been tested on these controlled tasks, so using the same tasks ensures a fair and consistent comparison.\n2. As this is the first proposal of such a framework, we wanted to demonstrate its feasibility and core capabilities on representative tasks before extending it to more complex scenarios. More complex tasks might distract from highlighting the novel aspects of our framework at this stage as they need more engineering work.\n\n> The final policies that are returned by the method don't seem very complicated or different from a basic human designed template. The agent designs discovered in the ADAS paper seem much more complex and creative. What explains this difference?\n\nADAS operates within a predefined search space, limited by human-designed methods, such as CoT and Debate. These algorithms are combined to create a solution, but the underlying structure is based on existing human knowledge. In contrast, G\u00f6del Agent is not bound by such human priors. It is free to explore and design strategies that go beyond predefined templates, as demonstrated in the \"Game of 24\" task in Appendix C.2. In this task, G\u00f6del Agent independently chose to use a search-based algorithm, breaking away from the limitations of prompt-based methods, as detailed in Section 5.3. While the policies G\u00f6del Agent generates may not always seem highly complex in every instance, its full autonomy dictates that it has a higher ceiling and will get stronger as LLM grows.\n\n> Given the limited evaluations, the room for self-improvement seems limited, can we really get much better than the base model for mathematical reasoning with fancy agent pipelines and prompts? A better showcase of the method would be on openended or embodied tasks vs text based reasoning\n\nWe agree that the full potential of G\u00f6del Agent will be best demonstrated in more complex, real-world scenarios, such as open-ended or embodied tasks. We are excited about the possibility of applying our framework to these types of tasks in the future. Currently, our experiments have focused on text-based reasoning tasks to establish the feasibility and advanced capabilities of G\u00f6del Agent in a controlled setting. While we recognize that this scope is limited, it was an essential first step in proving the underlying framework's effectiveness.\n\n> What\u2019s the upper limit of self improvement, does it saturate? *Whats the improvement in the 6 iterations\n\nThank you for your thoughtful question. The concept of recursive self-improvement does suggest that the process could continue indefinitely, as each iteration improves the optimizer, which in turn enhances the agent's ability to improve itself. However, in practice, we believe that there is an upper limit to this process. On the one hand, the core of G\u00f6del Agent is based on a LLM, and the LLM\u2019s understanding is inherently limited. If the agent becomes too complex, the LLM may no longer be able to comprehend or process its own changes, at which point the self-improvement process would stagnate. On the other hand, LLMs have some degree of randomness and are prone to errors, meaning that the optimization process may encounter hurdles that prevent indefinite improvement.\n\nFigure 4 in our paper shows an example of how the strategy changes as the number of iteration steps increases."
            }
        },
        {},
        {
            "title": {
                "value": "Official Response to Reviewer ALKV by Authors -- Part Two"
            },
            "comment": {
                "value": "> Justify the choice of having baselines that use less inference compute. Provide a best-of-n baseline or argue why this is not a good comparison.\n\nThank you for your question. It seems there may be a misunderstanding about the computation cost of our method. During the optimization iterations, G\u00f6del Agent does not generate a new strategy or test it at every step. Instead, it selectively optimizes its policy at certain steps and evaluates it based on feedback from the validation set. The specific steps where this happens are determined autonomously by the agent.\n\nIn terms of inference computation, the cost is similar to that of the baselines because we only test the final optimized policy once. Therefore, the additional computational cost is primarily associated with the optimization process, where G\u00f6del Agent uses this compute to replace the human labor typically involved in manually designing and refining agent strategies. Moreover, for each task, only a one-time optimization process is required, and the generated policy can always be applied on that task, so we consider this consumption acceptable.\n\nThe baseline methods, on the other hand, are manually designed by human experts, which consumes significant time and resources. Thus, while G\u00f6del Agent uses more compute during optimization, it eliminates the need for manual design, which is far more resource-intensive.\n\n> Clarify the comparison to Meta Agent Search and wheather your method produces qualitatively better solutions.\n\nThank you for the opportunity to clarify the comparison between our method and Meta Agent Search (MAS). In MAS, a fixed meta-agent is manually designed to create task-specific agents by interacting with the environment. However, this meta-agent remains static throughout the process, meaning the final optimized results are inherently constrained by the initial design and capabilities of the meta-agent.\n\nIn contrast, our approach, inspired by the G\u00f6del machine, moves beyond the meta-learning paradigm by endowing the agent with self-referential capabilities. This allows the agent to evolve not only its task-specific part but also its own optimization part. As a result, both the agent's ability to optimize and its performance on downstream tasks improve simultaneously, leading to more efficient and effective solutions.\n\nA clear example of this qualitative improvement can be seen in the \"Game of 24\" task as shown in Appendix C.2. G\u00f6del Agent was able to design a search-based algorithm that solves the task with 100% accuracy, whereas MAS is limited to searching within predefined spaces and designing prompt-based methods. This shows that it can generate solutions that are not only more flexible but also fundamentally more capable than those produced by a fixed meta-agent.\n\n> Justify the choice to use GPT-3.5 for the policy models. If possible, provide some results using GPT-4 for the policies.\n\nWe chose to use GPT-4 only during the learning phase because it offers superior reasoning capabilities, which are essential for the self-improvement process where the agent generates and refines its strategies. This phase involves replacing human labor with autonomous optimization, and GPT-4's enhanced reasoning helps G\u00f6del Agent improve its policy efficiently.\n\nFor task execution, we opted for GPT-3.5 for all methods because, in real-world applications, executing policies often requires frequent API calls, and GPT-3.5 is significantly more cost-effective for this purpose. \n\nThe policy generated by G\u00f6del Agent is fixed after the learning phase, much like CoT, where the learned policy remains unchanged during execution. Therefore, the decision to use GPT-3.5 for execution maintains fairness in comparison.\n\n> The paper often uses the term \"self-awareness\" without qualification. I'm concerned that anthropomorphization leads to a less scientific discussions and I would recommend the authors try to rewrite the paper to remove phrases like \"Our Godel Agent achieves self-awareness\".\n\nThank you for your thoughtful comment and suggestion. We apologize for any confusion caused by our use of the term \"self-awareness.\" In this context, we intended to convey that G\u00f6del Agent has the capability to introspect and read its own code and files, not to imply any philosophical sense of consciousness or awareness. We have emphasized this point more clearly in the revised version of the paper. If you find that the current explanation is still not sufficiently clear, would using the term of *self-introspection* be more appropriate?\n\nWe hope that our responses have addressed your questions. We also hope that you find our approach exciting, as it represents a fundamental departure from previous methods. If you have any further questions, please feel free to reach out, and we will be happy to address them. Thank you once again for your valuable feedback.\n\n[1] Automated Design of Agentic Systems"
            }
        },
        {
            "title": {
                "value": "Official Response to Reviewer ALKV by Authors -- Part One"
            },
            "comment": {
                "value": "Thanks for your careful and valuable comments. We will explain your concerns point by point.\n\n> Does the learning agent ever see the data the final policy will be evaluated on?\n\nThe agent does not have access to the test data. In fact, during the evolving process, we only utilize validation data, as detailed in Appendix B. To ensure a fair comparison with baseline methods, the selection of valid data follows the approach used in the paper[1]. We use the scores on the valid set as feedback from the environment to guide the G\u00f6del Agent in optimizing its policies.\n\n> For how many iterations do you run the Godel agent and how did you determine this hyperparameter?\n\nAs stated in line 310 of the paper, we conducted 6 independent self-improvement cycles for each task, with a maximum of 30 iterations per cycle. However, based on our experimental observations, G\u00f6del Agent often stops iterating before reaching this limit. The choice of 30 iterations as a hyperparameter was determined empirically through multiple self-improvement trials. Specifically, in 50 experiments conducted on MGSM, we observed that in 42 cases, G\u00f6del Agent terminated early, deeming its current policy sufficiently optimized before reaching 30 iterations.\n\n> In general, how did you tune prompt variations and hyperparameters for this agent? How did you prevent overfitting to the test data?\n\nThank you for this question. Due to G\u00f6del Agent's capability for unrestricted self-modification, there was no need for manual tuning of prompts or other hyperparameters. During the self-improvement process, the agent autonomously optimizes these aspects. This self-optimization ability is a key advantage of our approach compared to others. Consequently, the agent does not overfit the test data, as all adjustments are driven internally by the agent based solely on its interactions with the validation set.\n\n> Could you please clarify the action space of the agent and how the agent learns about the possible interactions? Is it only given the prompt in Appendix A or is it also given few-shot samples for example?\n\nThe action space of G\u00f6del Agent evolves continuously as it undergoes self-optimization. Initially, as described in Section 3 of the paper, the action space includes: (1) reading its own code, (2) modifying its own code, (3) evaluating its current policy using a utility function, and (4) recursively entering the next level of self-improvement. These actions were deliberately designed to align with the core capabilities of G\u00f6del Agent.\n\nDuring the self-improvement process, G\u00f6del Agent is free to create new actions and tools, which means its action space is not fixed but rather expands dynamically based on its needs. For example, in our experiments, G\u00f6del Agent autonomously developed tools such as numerical parsers, calculators, and even assistant agents.\n\nRegarding how the agent learns about possible interactions, this is facilitated through the initial prompt provided in Appendix A. The prompt sets up the foundational understanding, encouraging the agent to analyze, explore, and construct new actions. However, we deliberately avoided using few-shot because they could introduce human priors and influence agent\u2019s behavior. Our approach is to minimize human intervention.\n\n> How does the agent execute the policy? Is this happening automatically each iteration, or does the agent manually chose to exit the policy? Is this different during iteration and during evaluation?\n\nThank you for this insightful question. The execution of the policy is not automatic at every iteration. Instead, testing the policy using the utility function is treated as one of the actions available to the agent. During the iterative self-improvement phase, G\u00f6del Agent autonomously decides whether or not to test its current policy based on its assessment. It does not execute the policy at every step but rather chooses when it is optimal to do so.\n\nIn contrast, during the final evaluation phase, the process is different: we manually invoke the utility function to test the agent\u2019s final policy. This ensures that the evaluation is consistent and unbiased. Thus, the approach during iteration\u2014where the agent makes autonomous decisions\u2014is distinct from the controlled, manual process used during the final evaluation.\n\n> Clarify the evaluation protocol\n\nAs mentioned earlier, each task involves a utility function that helps evaluate the agent\u2019s policy. During the iterative process, G\u00f6del Agent decides autonomously when to invoke this utility function to test its current policy. In the final evaluation phase, we manually test the final policy using the same utility function.\n\nA key distinction between iteration and evaluation is the data used: during the iterative process, the utility function is applied to the **valida data** to guide the agent's self-improvement. In contrast, during the evaluation phase, we use **test data** to assess the performance of the agent\u2019s final policy."
            }
        },
        {},
        {
            "summary": {
                "value": "The paper proposes a self-improving language model agent (\"Godel agent\") and evaluates it on a variety of benchmarks. The results suggest that the self-improving agent achieves better performance than hand-designed agent scaffolds or meta-learned agents, while also being cheaper to run than the latter."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The proposed method is conceptually simple and clear and provides and intriguing possibility of building self-improving agents with current LLM technology.\n* The experiments include multiple benchmarks, a number of baselines and generally suggest strong results.\n  * The authors run ablations of different components of their agent and find all of them improve performance.\n* The qualitative analysis of the results is insightful and provides a good understanding of the strategies the self-improving agent implements."
            },
            "weaknesses": {
                "value": "**Results**\nI'm concerned about the presentation of the experiments and the **fairness of the empirical evaluation**. My key concerns are that the Godel agent uses significantly more inference compute than any of the other methods and that the description of the evaluation protocol is not sufficient to determine if the evalaution is fair.\n\nHere is my understanding of the evaluation setup (please correct me if I'm wrong):\n* The Godel agent is run for N iterations on a validation set of samples. In each iteration is produces a new policy and a new learning function.\n* After N iteration the policy is evaluated on held-out test problems and this result is reported.\n\nThis process gives the Godel agent significantly more inference compute than any of the hand-designed agent systems, which makes the comparison unfair. The only fair comparison in Table 1 is to Meta Agent Search which seems to typically be comparable in performance. Based on these results, I am not convinced the the Godel agent actually improves performance in a fair comparison.\n\nThis concern could be addressed by letting the baseline be a best-of-N method that applies N somewhat random perturbations to the policy and chooses the best of them. For example, this could be implemented by using GPT-4 to generate N different prompt variations and choose the best according to performance on the validation data.\n\nRelatedly, according to Appendix B the Godel agent uses GPT-4 for the learning algorithm but all methods only use GPT-3.5 for the policy. This makes the comparison additionally skewed in favor of the Godel agent which has access to GPT-4 in contrast to the other methods. A fair comparison would use GPT-4 for both the learning algorithm and the policy.\n\n\n**Presentation**\n\nI also have some concerns about the presentation and framing of self-improving agents in the paper. The paper often uses the term \"self-awareness\" without qualification. I'm concerned that anthropomorphization leads to a less scientific discussions and I would recommend the authors try to rewrite the paper to remove phrases like \"Our Godel Agent achieves self-awareness\".\n\nMoreover, potential risks from self-improving AI are not appropriately discussed. There is a significant literature on risks from self-improving AIs and this is a key concern in the AGI safety community. I think this literature should be acknowledged in the paper and there should be a more complete discussion of broader impacts of this technique than currently the case."
            },
            "questions": {
                "value": "**Clarification questions**: Please answer the following questions to help me better evaluate the paper.\n\n* Does the learning agent ever see the data the final policy will be evaluated on?\n* For how many iterations do you run the Godel agent and how did you determine this hyperparameter?\n* In general, how did you tune prompt variations and hyperparameters for this agent? How did you prevent overfitting to the test data?\n* Could you please clarify the action space of the agent and how the agent learns about the possible interactions? Is it only given the prompt in Appendix A or is it also given few-shot samples for example?\n* How does the agent execute the policy? Is this happening automatically each iteration, or does the agent manually chose to exit the policy? Is this different during iteration and during evaluation?\n\n**Concerns**: Addressing the following concerns would make me reconsider my score.\n* Clarify the evaluation protocol.\n* Justify the choice of having baselines that use less inference compute. Provide a best-of-n baseline or argue why this is not a good comparison.\n* Clarify the comparison to Meta Agent Search and wheather your method produces qualitatively better solutions.\n* Justify the choice to use GPT-3.5 for the policy models. If possible, provide some results using GPT-4 for the policies.\n\n**Overall assessment**\nI'm intrigued by the premise of the paper, but quite concerned about the evaluation and reproducibility. If the authors can address my concerns and answer my questions, I'm willing to reconsider my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "* A big limitation of LLM agents is that there are handcrafted, self-improving agents are a promising direction to make this more autonomous\n* self referential framework that enables agents to recursively improve themselves without predefined routines\n* Agent can alter its own code and runtime memroy\n\nThe paper introduces G\u00f6del Agent, a self-referential framework that enables agents to recursively improve themselves without predefined routines or fixed optimization algorithms.\nInspired by the G\u00f6del machine, G\u00f6del Agent allows agents to modify their own code and logic using large language models (LLMs), guided only by high-level objectives.\nThe agent uses techniques like monkey patching to read and alter its runtime memory, achieving self-awareness and self-modification.\nExperiments across coding, science, and math tasks show that G\u00f6del Agent outperforms manually crafted agents in performance, efficiency, and generalizability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Seems like a promising and open-ended approach for removing humans in the loop of agent pipeline building\n* ADAS Meta Agent is a strong similar baseline and Godel outperforms it\n* Demonstrated effective self-improvement across multiple domains including reasoning"
            },
            "weaknesses": {
                "value": "* Experiments seem limited in scope\u2014mostly on controlled tasks; might not scale to complex real-world applications or embodied tasks\n* The final policies that are returned by the method don't seem very complicated or different from a basic human designed template. The agent designs discovered in the ADAS paper seem much more complex and creative. What explains this difference? \n* Given the limited evaluations, the room for self-improvement seems limited, can we really get much better than the base model for mathematical reasoning with fancy agent pipelines and prompts? A better showcase of the method would be on openended or embodied tasks vs text based reasoning\n* What\u2019s the upper limit of self improvement, does it saturate?\n   *Whats the improvement in the 6 iterations\n* More detailed comparison to ADAS is needed in related work (Automated Design of Agentic Systems)\n   * What is Meta Agent Search and why does this Godel agent perform better?\n      * This needs to be clearer in the paper\n* The method is somewhat vague and not clear what\u2019s going on, and what this has to do with self reference/recursion. Is this different than just tasking an LLM with modifying its own agent code?"
            },
            "questions": {
                "value": "How does G\u00f6del Agent ensure safety and prevent harmful behaviors during self-modification? How do you put constraints on it such that it doesn't blow all of your GPT credits or hog all the GPUs on your system?\n\nWhat mechanisms are in place to handle errors or prevent the agent from degrading its performance over time?\n\nCan this approach scale to more complex tasks that require long-term planning or interaction with unpredictable environments?\n\nHow does the agent's performance depend on the underlying LLM's capabilities? Can it surpass those limitations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new self-evolving framework, Godel Agent, which leverages LLMs to dynamically modify self-generated logic and behavior. Empirical experiments show that this framework enables agents to recursively improve themselves without predefined routines or fixed optimization algorithms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The workflow proposed in this paper is interesting; the core idea of recursive improvement is not implemented by improving the response itself step-by-step, but through updating the policy in an indirect way.\n\n2. The paper is well written, and easy to follow.\n\n3. The illustrations in figure 1 and examples given in Appendix C are clear and informative."
            },
            "weaknesses": {
                "value": "1. The motivation for adding four additional tools is not well-justified; there is no evidence supporting that these are the correct tools to add or that they comprehensively cover all needs. It's counterintuitive because if we claim the model has the ability to refine its policy and develop new policies, why do we need to provide tool guidance? Additionally, it's notable in Appendix C that although there is variability in the best policy, this policy seems can be decomposed into a combination of the listed tools, which suggests this work is not fully distinguished from meta-learning optimized agents where humans manually define the high-level strategy, and the agent learn a strategy to combine or rank the high-level strategy.\n\n2. The current framework heavily relies on the model's capability to generate and refine policies, as well as generate responses given a policy and query. However, this would not improve the model's ability to acquire new knowledge if such kind of off-line dataset available.\n\n3. The current refinement requires an oracle to progress the performance (U in line 6 from Algorithm 1). For the current Table 1, is the number of oracle utility function calls the same across all methods, and is the number of model queries the same for different settings?"
            },
            "questions": {
                "value": "1. Based on Appendix C, the best policy found by Godel Agent for MMLU is the same as CoT-SC, why are the two numbers reported in Table 1 different? Similarly for the best policy for GPQA, why is it different from CoT? If this is due to extended few-shot examples or specific instructions, then it seems more like a prompt engineering problem, suggesting we're not using optimal prompts.\n\n2. The reason why the method is evaluated on MGSM instead GSM8K is not well-justified, for my understanding, multi-lingual is not a target of the framework. Moreover, all current benchmark, even regular CoT has high accuracy, this together with limitation, I am questioning the generalizability of the framework on harder tasks, like MATH (https://github.com/hendrycks/math), it is reported that MATH data is contanmintated in some training dataset, so it would be safer to evaluate on MATH500 (https://huggingface.co/datasets/qq8933/MATH500), and omni-MATH (https://omni-math.github.io/). Additionally, the current evaluation would benefit from other agentic tasks, including code generation tasks, like SWEBench (https://www.swebench.com/).\n\n3. For the related work, some recent recursive self-improvement work is not discussed\n[1] Havrilla, Alex, et al. \"Glore: When, where, and how to improve llm reasoning via global and local refinements.\" arXiv preprint arXiv:2402.10963 (2024).\n[2] Hosseini, Arian, et al. \"V-star: Training verifiers for self-taught reasoners.\" arXiv preprint arXiv:2402.06457 (2024).\n[3] Qu, Yuxiao, et al. \"Recursive introspection: Teaching language model agents how to self-improve.\" arXiv preprint arXiv:2407.18219 (2024).\n[4] Kumar, Aviral, et al. \"Training language models to self-correct via reinforcement learning.\" arXiv preprint arXiv:2409.12917 (2024).\n\n4. How to safeguard the current workflow if the model is allowed to modify the policy and interaction code directly, and the policy will be used to generate further responses. How can we prevent runtime memory issues and ensure that both the code implementation and the final generation are not harmful?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a self-evolving framework, G\u00f6del Agent, inspired by the G\u00f6del machine, to enable recursive self-improvements of agents, without relying on predefined routines or fixed optimization algorithms.\nThe authors conduct experiments to validate the proposed approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) A framework\nThe authors propose a self-evolving framework, G\u00f6del Agent, inspired by the G\u00f6del machine, to enable recursive self-improvements of agents, without relying on predefined routines or fixed optimization algorithms.\n\n2) Experiments\nThe authors conduct experiments to validate the proposed approach."
            },
            "weaknesses": {
                "value": "1. Why LLM-based agent is a valid approach? No LLM is perfect. All LLMs make mistakes. What is reliable information? What is ground truth? Then why should we rely on LLMs to make decisions? For software engineering, no LLM can achieve a pass@1 of 100%. To make it worse, the popular evaluation method with several tests, is not perfect, so the evaluation results are not fully reliable.\n\n2.\nHow to make LLM-based agent a valid approach? LLMs may help generate plausible solutions. But there should be a mechanism to filter out or fix incorrect solutions.\n\nThe current submission does not provide such a mechanism.\n\nHow to improve an LLM-based agent? \nIt seems this boils down to waiting for stronger LLMs, which is likely out of the scope of the paper, though.\n\n3.\nThere are papers discussing / criticizing the reasoning / planning capacity of LLMs, which is the foundation of LLM-based agent (including those based on the G\u00f6del machine), e.g.\n\nLLMs can\u2019t plan, but can help planning in LLM-modulo frameworks, in ICML, 2024.\n\nGSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models. arXiv 2024\n\nWithout sound capabilities for reasoning / planning, how to build LLM-based agents, in particular, the G\u00f6del Agent?\n\nIt appears that empirical improvements are not enough to answer the above questions."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents the Godel Agent, a prompt engineering framework inspired by the Godel machine, claiming to be a self-improvement framework. The authors evaluate Godel Agent in 4 tasks, and the game of 24."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The research topic is interesting and important. The paper provides an extensive evaluation."
            },
            "weaknesses": {
                "value": "1. There are several ambitious claims within the paper; for example, the abstract suggests that the method can explore the entire space to find the optimal solution. This is a bold statement and does not seem to be substantiated by the findings presented.\n2. The central claim about the capability to create \"different agents\" from an initial agent mostly involves modifying the conditioning of the autoregressive order through prompt engineering.\n3. Figure 1 shows some learnable components, yet the framework itself does not appear to incorporate learnable elements.\n4. Line 50 claims \"to eliminate the human design prior,\" which is challenging given that LLMs inherently contain human priors from being trained on human-generated text.\n5. Discussions on self-awareness of LLMs are speculative and might be better left out of a scholarly paper focused on introducing a new method.\n6. Section 3 is notably small and lacks clarity on the method\u2019s implementation. It would benefit from additional details, particularly on how self-improvement is enacted within the Godel Agent and how evaluations are conducted.\n7. Lines 215-233 imply that evaluations rely on the environments provided. If this involves using the test set for evaluation, it represents a significant limitation of the method.\n8. How the solutions in Godel Agent are evaluated (referenced in line 6 of the pseudocode) needs clearer explanation.\n9. It would be beneficial to compare the cost of the Godel Agent with other methods, such as the CoT, both in terms of time and financial resources.\nThe authors might want to reevaluate their claims and revise the paper to provide a clearer presentation of the proposed method."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}