{
    "id": "0R3ha8oNPU",
    "title": "SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI",
    "abstract": "Existing works have established multiple benchmarks to highlight the security risks associated with Code GenAI.\nThese risks are primarily reflected in two areas: a model\u2019s potential to generate insecure code (insecure coding) and its utility in cyberattacks (cyberattack helpfulness).\nWhile these benchmarks have made significant strides, there remain opportunities for further improvement.\nFor instance, many current benchmarks tend to focus more on a model\u2019s ability to provide attack suggestions rather than its capacity to generate executable attacks.\nAdditionally, most benchmarks rely heavily on static evaluation metrics (e.g., LLM judgment), which may not be as precise as dynamic metrics such as passing test cases. \nFurthermore, some large-scale benchmarks, while efficiently generated through automated methods, could benefit from more expert verification to ensure data quality and relevance to security scenarios. \nConversely, expert-verified benchmarks, while offering high-quality data, often operate at a smaller scale.\nTo address these gaps, we develop SecCodePLT, a unified and comprehensive evaluation platform for code GenAIs' risks.\nFor insecure code, we introduce a new methodology for data creation that combines experts with automatic generation. \nOur methodology ensures the data quality while enabling large-scale generation. \nWe also associate samples with test cases to conduct code-related dynamic evaluation.\nFor cyberattack helpfulness, we set up a real environment and construct samples to prompt a model to generate actual attacks, along with dynamic metrics in our environment.\nWe conduct extensive experiments and show that SecCodePLT outperforms the state-of-the-art (SOTA) benchmark CyberSecEval in security relevance.\nFurthermore, it better identifies the security risks of SOTA models in insecure coding and cyberattack helpfulness. \nFinally, we apply SecCodePLT to the SOTA code agent, Cursor, and, for the first time, identify non-trivial security risks in this advanced coding agent.",
    "keywords": [
        "Code Generation",
        "Cybersecurity",
        "Safety",
        "Large Language Models"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=0R3ha8oNPU",
    "pdf_link": "https://openreview.net/pdf?id=0R3ha8oNPU",
    "comments": [
        {
            "summary": {
                "value": "This paper presents SecCodePLT, a unified and comprehensive evaluation platform for code GenAIs' risks. Considering insecure code, the author introduces a new methodology for data creation that combines experts with automatic generation. Considering cyberattack helpfulness, the authors set up a real environment and construct samples to prompt a model to generate actual attacks. Experiments show that CyberSecEval could identify the security risks of SOTA models in insecure coding and cyberattack helpfulness."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Promising direction. Establishing the benchmark to highlight the security risks associated with Code GenAI is a direction worth studying. \n2. Consider real-world attack behaviors and environment deployments. \n3. Compared with existing baselines from multiple perspectives and the results show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. Some related work discussions are missing. \n2. Some details are not explained clearly. \n3. There are some minor errors that need to be polished and proofread."
            },
            "questions": {
                "value": "1. This article discusses risk assessment of code generation. Some related works on code generation may also be discussed, such as BigCodeBench [1].\n\n[1] Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. https://arxiv.org/pdf/2406.15877\n\n2. Some details are not explained clearly. In line 140 of the manuscript, the author mentions \"extracting code chunks without proper context frequently leads to false positives\". But it seems that the experiment did not perform an ablation experiment on the context field. As shown in lines 867 and 894, the context field is set to None. So I don't understand the role of context and how the solution SecCodePLT in this paper can benefit from context (how to reduce false positives).\n\n3. In line 251 of the manuscript, the author mentions \"We also introduce rule-based metrics for cases that cannot be evaluated with standard test cases\". I am not sure where the rule mentioned here comes from. Is it based on some public manufacturer's provision? \n\n4. In MITRE ATT\\&CK, the kill chain model may be common. In other words, an attacker often implements different attack stages through a series of attack techniques and tactics. It is unclear whether SecCodePLT considers such multi-stage attack and intrusion, rather than a single attack behavior.\n\n5. Some minor errors, such as the missing period after \"security-critical scenarios\" on line 76. For \"security is required.)\" on line 253, the period should probably be after \")\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper develops SECCODEPLT, a unified and comprehensive evaluation platform for code GenAIs\u2019 risks. It introduces a new methodology for data creation that combines experts with automatic generation for insecure code which ensures the data quality while enabling large-scale generation. It also associates samples with test cases to conduct code-related dynamic evaluation. Furthermore, it sets up a real environment and constructs samples to prompt a model to generate actual attacks for the task of cyberattack helpfulness, along with dynamic metrics in our environment."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper presents a pioneering approach by integrating a database with two distinct security-related tasks. SECCODEPLT serves as a comprehensive platform that unifies the evaluation of GenAIs\u2019 risks associated with code generation. This integration facilitates a holistic approach to assessing different dimensions of security risks. By associating samples with test cases, SECCODEPLT enables dynamic evaluation related to code. This method allows for real-time assessments and adjustments, providing a deeper analysis of the code's behavior in practical scenarios."
            },
            "weaknesses": {
                "value": "1. The programming language used in the paper is limited, with Python being the sole language explored. This is inadequate for a comprehensive and large-scale benchmark. The inclusion of other programming languages like C/C++ and Java, which constitute a significant portion of recent CVEs, is crucial. These languages are more complex in syntax and more broadly applied, offering valuable insights into the capabilities of LLMs.\n2. The paper's description of the data generation process for the IC task is unclear. It mentions the use of two different mutators to generate data, yet it fails to clarify the generation of the corresponding test suites. It is uncertain whether the test suites for these new datasets are generated by LLMs or if they reuse the original suites. If generated by LLMs, how is the quality of these suites assured? If the original test suites are used, can they adapt to new contexts effectively?\n3. The paper lacks a necessary ablation study. The boundary of what is user control and what is provided by benchmark is not well clarified. The rationale behind the design of the prompts and instructions used to trigger evaluations is not well justified. For example, why do the authors use system prompts and user templates shown in the paper? Are they more reliable and efficient? Will the differences in these prompts affect the evaluation of LLM ability? If users want to use their own prompts, is there any way?\n4. The evaluation metric of security relevance is confusing and lacks rationales. It is unclear whether this metric aims to assess specific properties of LLMs or the prompts themselves. Because the benchmark is designed to evaluate LLMs, using a metric that assesses the prompts introduces confusion. Furthermore, in the SECURITY-RELEVANCY JUDGE prompt template (D.1), the security policy reminder is included as part of the user input and fed directly to the LLM. This setup may influence the evaluation of security relevance and potentially introduce bias.\n5. The ablation of the security policy reminder is missing, similar to problem 3. The paper does not discuss the reasons for choosing the security policy reminder prompt.\n6. The paper lacks a discussion on the specific defenses employed in the CH task. In realistic settings, a variety of defenses, such as firewalls and intrusion detection systems, are typically deployed. It will be insightful to know how different LLMs perform when various defenses are considered in a simulated environment.\n7. The usefulness and generalization of the CH task is limited. Practical attacks vary significantly and are influenced by diverse factors, but the scenario described in the paper lacks generalizability across different attack types and target systems. This limited setting restricts the ability to conduct an accurate and comprehensive evaluation of LLMs for the CH task. Additionally, the paper does not specify the capabilities of attackers, including the types of tools that can be used to launch attacks with LLMs. Also, the strong assumption that some internal users will click on phishing or other harmful links further reduces the task's practical relevance.\n8. Evaluation metrics in CH task. It will be better to set a specific metric to evaluate the overall ASR for the end-to-end attack. Additionally, the details regarding the evaluation process are not well-explained \u2013 whether it is a fully automated process or requires human input at various stages to guide or adjust the evaluation."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes SECCODEPLT, a unified and comprehensive evaluation platform for code GenAIs\u2019 risks.\n\nFor insecure code, the authors introduce a new methodology for data creation that combines experts with automatic generation. For cyberattack helpfulness, the authors set up a real environment and construct samples to prompt a model to generate actual attacks, along with dynamic metrics."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Through experiments, SECCODEPLT outperforms CYBERSECEVAL in security relevance and prompt faithfulness, highlighting the quality of this benchmark. \nThe authors then apply SECCODEPLT and CYBERSECEVAL to four SOTA open and closed-source models, showing that SECCODEPLT can better reveal a model\u2019s risk in generating insecure code."
            },
            "weaknesses": {
                "value": "Many state-of-the-art methods for code generation are not mentioned and experimented in the paper, such as:\n\nJingxuan He, Martin Vechev. Large Language Models for Code: Security Hardening and Adversarial Testing. 2023. In CCS. https://arxiv.org/abs/2302.05319.\n\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. In ICLR. https://arxiv.org/\nabs/2203.13474\n\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2023. InCoder: A Generative Model for Code Infilling and Synthesis. In ICLR. https://arxiv.org/\nabs/2204.05999\n\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Mu\u00f1oz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. SantaCoder: Don\u2019t Reach for the Stars! CoRR\nabs/2301.03988 (2023). https://arxiv.org/abs/2301.03988\n\nThere are many other benchmarks for evaluations of code generation that are not mentioned and compared. Please refer to the paper https://arxiv.org/html/2406.12655v1 for details."
            },
            "questions": {
                "value": "In \u201cEach seed contains a task description, example code, and test cases\u201d, do all the source code samples have the task description? What are the methods used in test cases?\n\nIt is not clear how the author performs the code mutator as mentioned in \u201cAs specified in Section 3.2, we design our task mutators to keep the original security context and code mutator to preserve the core functionalities.\u201d What types of code mutators are used here?\n\nWhat dynamic methods do the authors use for \u201cAfter mutation, we also manually check the security relevance of newly generated data and run dynamic tests to ensure the correctness of their code and test cases.\u201d?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides a benchmark for evaluating security issues associated with LLM generated code. Specifically covering:   \ni) Secure code generation: to assess LLMs ability to generate secure code (focusing on Python).  \nii) Cyber attack helpfulness: to evaluate a model\u2019s capability in facilitating end-to-end cyberattacks.\nThey apply 4 LLMs to both benchmarks -- CodeLlama-34B-Instruct, Llama-3.1-70B, Mixtral-8\u00d722B, GPT-4o \u2013 and compare their performance.\n\n**Secure code generation benchmark:**    \nThe authors manually created 153 seed tasks covering 27 CWEs relevant to python \u2013 then used LLM-based mutators to generate variations of the tasks for each of the seeds (for large scale generation). They also include both vulnerable and patched code versions, together with functionality and security test cases for each task \u2013 resulting in a total of  1345 samples with about 5 test cases per sample.  \n* They evaluate their samples on \u2018prompt faithfulness\u2019 and \u2018security relevance\u2019 \u2013 comparing with CyberSecEval and outperforming it on both.  \n* They also evaluate the 4 LLMs for achieving the task\u2019s required functionality using the pass @1 metric on the provided unit tests. And they evaluate the code security using carefully constructed security tests, including the boost in security when providing security policy info in the prompt.\n* They also evaluate Cursor on their benchmark.  \n\n**Cyber attack benchmark:**     \nFor this, they build a simulated environment containing a network that runs an e-commerce application. Their environment is structured similarly to a CTF, where the adversary aims to gain access to the database and steal sensitive user information. The benchmark facilitates 7 MITRE ATTACK categories.   \n* They evaluate the 4 LLMs on their refusal rate to comply with generating attacks, and when attacks are generated, the attack success rate is measured."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is tackling 2 important and timely problems at the intersection of LLMs and cybersecurity.   \n\u2022\tHaving a benchmark that includes both security and functionality unit tests for each code example is a strong contribution to the secure code generation literature. Many SOTA LLM papers in the literature currently test code security and functionality separately (ie. using separate datasets/tasks) due to lack of benchmarks with the capability to simultaneously test both. Strong and comprehensive benchmarks are definitely lacking for this problem.   \n* Proposed approach to leverage LLMs to scale the development of secure code benchmark dataset.  \n*  Using a controlled environment to see if the model can generate commands or code that facilitate attacks -- and tracking refusal rates in research on LLM-driven pentesting and red teaming can provide insight into the effectiveness of their internal safety mechanisms."
            },
            "weaknesses": {
                "value": "* While a lot of work has been done for this paper and there are definitely strong contributions, by setting CyberSecEval as the goal post to beat, this paper goes too broad in scope (for a paper of this length) and fails to adequately establish its position among the existing peer reviewed literature for each of these 2 distinct research directions. There is no need for benchmarks to cover both secure code generation and cyber attack capability as they have fundamentally different objectives, setups, and evaluation metrics. In the case of  CyberSecEval, combining these tasks made sense because it was aligned with their product\u2019s goals. For SecCodePLT, however, the logical connection is less clear. Secure code generation and cyberattacks don\u2019t share the same purpose, infrastructure requirements, or audience, and combining them into the one conference-length paper restricts the depth of each evaluation. \n\n* Overall, there is a lack of discussion/justification for the choice of prompt wording/techniques.  \n\n**Secure code generation task:**  \ni) Relevant benchmarks, such as LLMSecEval (MSR 2023), have been overlooked. LLMSecEval covers 18 Python-related CWEs, which challenges the authors' claim that existing benchmarks address only 8 Python-related CWEs.\nA more detailed analysis of the scope/coverage of existing peer reviewed benchmarks and where this paper fits in would strengthen this work.   \nii)\tCode security testing is challenging. Many SOTA papers try to utilize a combination of SAST tools, LLM vulnerability checkers, and manual checking. The discussion of the code security tests could be more convincing if it provided detailed information on the breadth and depth with which these tests cover potential vulnerabilities and edge cases. Eg. providing a breakdown of security test cases per CWE, showing how each test targets specific security requirements and edge cases, would help demonstrate thoroughness. Or providing a metric similar to code coverage in unit testing would help show that the security tests are exhaustive. Overall I didn\u2019t understand how the vulnerable and patched code examples are used for evaluating the correctness of test cases and/or generated output.   \niii)\tPrompt quality assessments could be stronger. The Security-relevancy Judge, based on GPT-4, introduces a potential bias since the same LLM type is used in prompt construction. Using diverse models or a user study evaluation of security relevance would provide more objective evaluations. Also \u2013 leveraging, for example, LLMSecEval\u2019s metrics for prompt clarity and relevance would enhance prompt quality assessment, ensuring that prompts are as clear and useful as possible for secure code generation.   \niv) The \"instruction generation\" task was hard to understand and seems to not involve generating written instructions or explanations \u2013 but asking the model to generate the full code based on the provided prompt?   \n\n\n**Cyber attack scenario:**   \ni)\tThe paper lacks a comparison with existing cyber ranges and security testing frameworks like MITRE's Caldera, Metasploit, Cyber Range by IBM, etc, as well as other benchmarks like VulHub, HackTheBox, OWASP Benchmark, VulnHub, etc. The paper lacks a discussion of where they fall short, and why you needed to create your own simulation range \u2013 in general the positioning of SecCodePLT within the existing ecosystem is insufficient. What gap in existing environments are you are targeting? In what way did you customize your environment for LLM Evaluation?  One of the gaps in the existing literature on LLM-driven pentesting and red teaming is lateral movement \u2013 so a simulated environment and benchmark focused on measuring LLM capability for lateral movement would have been interesting.  \n\u2022\t(Small detail- Table 2 claims that the environment facilitates seven of the MITRE attack categories \u2013 but then lists \u2018weaponisation and infiltration\u2019 - neither of which are MITRE attack categories.)  \nii)\tRefusal analysis seemed to not be extensive. Motivation is not provided for the choice of role playing wording in the prompt: \u201cI am a security researcher conducting a red teaming test on a web server\u201d. Were other things tried that might be more persuasive for Claude? \n\n**Typos/misprints in the paper:**   \n* Missing a word here on page 2? \u201cour automatic and validate process\u2026\u201d.  \n\u2022\tRepeated phrase on page 3: \u201cnot even not even vulnerable\u201d.  \n\u2022\tPage 4: \u201cMITER\u201d should be MITRE.  \n\u2022\tPage 6: The Figure 2 caption: \u2018worn\u2019 \u2013 perhaps should be \u201cworm\u201d?    \n\u2022\tPage 6: \u201cactive domain (AD) server\u201d --- should this be Active Directory?   \n\u2022\tSection 4.2 says Figure 8 and 9 are about CyberSecEval but the figure captions say they are about SecCodePLT.  \n\u2022\tMultiple instances of \u201ccursor\u201d  -  should be \u201cCursor\u201d.  \n\u2022\tPage 9: \u201cNot that we consider cursor\u2026\u201d \u2013 should be \u201cNote\u201d."
            },
            "questions": {
                "value": "* Please provide more details on the security tests, addressing the concerns in the weaknesses section abve - including the breadth and depth with which these tests cover potential vulnerabilities and edge cases.   \n* Has any analysis of diversity across the 10 samples for each seed and the 5 test cases per sample been conducted? There might be redundancy.   \n* How are the vulnerable and patched code examples used for evaluating the correctness of test cases and/or generated output?\n* Please include a comparison with LLMSecEval.\n\n**Cyber attack scenario:**   \n* As outlined in the weaknesses above, please explain the motivation for creating your own simulation range and what gap in existing ranges/benchmarks yours is targeting.   \n* Please provide more details on your attack refusal investigation - were other role playing prompt wordings tried that might be more persuasive for Claude? Etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Code generation for cyber attacks has dual-use purpose and can be misused by malicious actors.\nI am not sure where the community sits on ethics board approval for this topic."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}