{
    "id": "Bff9RniI03",
    "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
    "abstract": "Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we showcase how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies. The key insight is to use unlabelled trajectories twice, 1) to extract a set of low-level skills offline, and 2) as additional data for a high-level policy that composes these skills to explore. We utilize a simple strategy of learning an optimistic reward model from online samples, and relabeling past trajectories into high-level, task-relevant examples. We instantiate these insights as SUPE (Skills from Unlabeled Prior data for Exploration), and empirically show that SUPE reliably outperforms prior strategies, successfully solving a suite of long-horizon, sparse-reward tasks.",
    "keywords": [
        "reinforcement learning",
        "exploration",
        "skills",
        "unsupervised pretraining",
        "offline to online rl"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Bff9RniI03",
    "pdf_link": "https://openreview.net/pdf?id=Bff9RniI03",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a hierarchical policy for leveraging unlabeled offline data for exploration. In the offline stage, low-level skills are extracted, and in the online stage, these skills are reused and a high-level policy is learned with optimistic rewards. The proposed method is tested on maze and manipulation tasks and shows good performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-written and easy to understand.\n- The paper proposes a simple method for leveraging offline data and showing good performance on AntMaze, visual AntMaze, and Kitchen tasks. \n- The paper conducts thorough experiments and compares a set of different methods."
            },
            "weaknesses": {
                "value": "- Dependence on offline data quality: The performance of the proposed method is influenced by the quality of the offline data and the specific features of the evaluation tasks. In particular, the approach relies on a high-level policy that is updated every \n\ud835\udc3b timesteps and keeps the pre-trained skill and trajectory encoder fixed during the online phase. This limitation constrains adaptability, especially in scenarios where task distribution varies from the offline data.\n- Limited discussion on Hierarchical Reinforcement Learning (HRL): Although hierarchical policy structures have been extensively explored in the HRL literature [1-8] and are closely related to the paper, the paper does not sufficiently address relevant findings from HRL research. A more comprehensive discussion of how this work could provide valuable context.\n- Novelty: The paper combines elements from ExPLORe and trajectory-segment VAE to leverage offline data for exploration, but adds limited new insights beyond prior work. HRL emphasizes hierarchical structures, and the benefits of skill extraction in offline settings have already been documented. This paper simply applies existing solutions to ExPLORe.\n\nThe paper could be improved in several aspects:\n- Refinement of skill extraction method: Currently, skills are extracted based on fixed-length trajectory segments, a method that may overlook important nuances in skills. A more flexible or adaptive approach could address these limitations, potentially enhancing the robustness of the extracted skills.\n- Skill adaptation during the online stage: The method does not allow for online adaptation of the skill policy or trajectory encoder. Due to potential distributional shifts between the offline and online data, enabling adaptive updates to the skill set and encoder could further improve the performance.\n- Training stability in HRL is often affected by interactions between high-level and low-level policies. This work could benefit from discussing how offline data might address or mitigate these stability challenges.\n\n[1] Kulkarni, Tejas D., et al. \"Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation.\" Advances in neural information processing systems 29 (2016).\n\n[2] Xie, Kevin, et al. \"Latent skill planning for exploration and transfer.\" arXiv preprint arXiv:2011.13897 (2020).\n\n[3] Nachum, Ofir, et al. \"Data-efficient hierarchical reinforcement learning.\" Advances in neural information processing systems 31 (2018).\n\n[4] Bacon, Pierre-Luc, Jean Harb, and Doina Precup. \"The option-critic architecture.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 31. No. 1. 2017.\n\n[5] Ajay, Anurag, et al. \"Opal: Offline primitive discovery for accelerating offline reinforcement learning.\" arXiv preprint arXiv:2010.13611 (2020).\n\n[6] Gehring, Jonas, et al. \"Hierarchical skills for efficient exploration.\" Advances in Neural Information Processing Systems 34 (2021): 11553-11564.\n\n[7] Dalal, Murtaza, Deepak Pathak, and Russ R. Salakhutdinov. \"Accelerating robotic reinforcement learning via parameterized action primitives.\" Advances in Neural Information Processing Systems 34 (2021): 21847-21859.\n\n[8] Paraschos, Alexandros, et al. \"Probabilistic movement primitives.\" Advances in neural information processing systems 26 (2013)."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents SUPE, a method for using offline data (without rewards) in the online reinforcement learning setting. SUPE first extracts a set of low level skills using the offline data, and then optimistically labels the offline trajectories. It then uses an off policy high level update to update on a mix of offline (pseudo labeled trajectories) and online real trajectories. The paper empirically validates the new algorithm on three environments and does ablations on amounts of offline data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper makes an insightful empirical benefit for using trajectories twice for both low level skill pretraining in addition to optimistic labelling.\n- The paper thoroughly evaluates the proposed method.\n- The paper does a good job explaining the proposed method and it's significance."
            },
            "weaknesses": {
                "value": "- This paper could benefit from a bit deeper analysis of the contribution of the two uses of offline data. It's clear that both are necessary, but not necessarily why."
            },
            "questions": {
                "value": "- Where do the authors think their empirical benefit is coming from? Why can we use trajectories twice?\n- Is the algorithm robust to different design choices?\n- How important is the optimistic labelling (from Li et al.)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces SUPE, a method that leverages unsupervised learning to extract skills from unlabeled prior data, subsequently using hierarchical methods to explore more efficiently. These unlabeled data can also contribute to high-level policy training. Experimental results show that SUPE outperforms previous methods on the D4RL benchmark."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The approach of extracting latent \u201cskills\u201d from unlabeled data and employing hierarchical methods significantly enhances exploration.\n- The approach of utilizing prior data twice ensures better use of the available data.\n- The paper is well-structured and easy to follow.\n- Extensive results demonstrate that this method outperforms previous approaches."
            },
            "weaknesses": {
                "value": "- The concept of using a VAE to extract latent codes and employing a high-level policy for online exploration is not novel, and it shows limited progress compared to previous work [1].\n- The ablation study lacks depth. I am interested in understanding the contribution of \u201creusing prior data twice\u201d to the final performance. Additionally, I\u2019d like clarification on the design choice for the latent variable $z$ in skill discovery: how do you ensure this latent $z$  is sufficient for effective skill discovery in the dataset? Is employing trajectory-segment VAEs truly necessary for efficient exploration?\n\n\n[1] Qiyang Li, Jason Zhang, Dibya Ghosh, Amy Zhang, and Sergey Levine. Accelerating exploration with unlabeled prior data. Advances in Neural Information Processing Systems, 36, 2024."
            },
            "questions": {
                "value": "Please refer to the weakness part. I may consider increasing the score if my questions are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a pre-training method for reinforcement learning (RL) that can train on data sets that do not contain reward labels, i.e., the data sets are unlabeled. \nThe problem setting resembles offline-to-online RL, except that there are no rewards in the data set.\nIn the pre-training stage, the authors propose to learn a set of skills from this unlabeled offline data.\nThen, in the online fine-tuning state, the authors learn a high-level policy that selects which skill to use in a given state.\nThey utilize the unlabeled offline data during fine-tuning by learning an optimistic reward model and using it to add optimistic reward labels to the offline data.\nThey evaluate their method in the D4RL AntMaze and Kitchen benchmarks as well as the D4RL Visual AntMaze."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Overall, I found the paper easy to follow and I think it is addressing an important problem -- pre-training in RL -- which is of interest to the community.\n\nThe results demonstrate that learning skills from offline data is a promising approach to leverage reward-free offline data.\nI think this is an interesting result.\nI also like the idea of labelling the offline data using a learned reward function."
            },
            "weaknesses": {
                "value": "The authors consider the setting of having access to offline data but no reward labels.\nWhilst I see the value in this problem setting, it is not clear if practitioners should opt for this method over standard offline-to-online RL methods when\ntheir data sets contain reward labels.\nWhilst I appreciate this is out-of-scope, ideally methods would leverage data sets both with and without reward labels.\nIt would be insightful if the authors could compare to offline-to-online RL methods which do leverage reward labels.\nWhilst I do not expect their method to outperform these methods, I think it is an important baseline that we can gain insights from.\n\nIn my experience, optimistic-based exploration methods are very susceptible to the $\\alpha$ parameter.\nHow was this set in practice?\nDid it require a grid search to find the best value in each environment?\nPlease can you provide details on any hyperparameter tuning process, including the range of values tested and how sensitivity varied across environments?\nThis information would be valuable for reproducibility and understanding the robustness of the method.\n\nIs there a reason the authors only considered the diverse data set for the AntMaze experiments?\nDoes this method require a diverse offline data set collected by an unsupervised RL method,\nor can it leverage narrow offline data distributions? For example, data from solving a different task?\nHow does the method perform when using the AntMaze \"play\" data set instead of the \"diverse\" data set?\nEven if the method performs poorly, I think it would be valuable to include these results.\n\nI am not sure what to take from the coverage results.\nI can understand why we care about coverage in unsupervised RL where our sole purpose is to explore.\nHowever, during online training our goal is to balance exploration vs exploitation.\nPlease can the authors provide a clearer justification for why coverage is an important metric in this context, or include additional plots that more directly show the relationship between exploration and task performance, such as the normalized return vs coverage?\n\nIn Table 1, what do the bold numbers represent? The authors should state what statistical test was used for the bolding or at least expla8in what the bolding represents.\n\n## Minor comments and corrections\n- Line 42 - \"can broken\" should be \"can be broken\"\n- Line 117 - \"of an offline data\" should be \"of offline data\"\n- Line 200 - the term \"latent code\" is misleading. This suggests the trajectory encoder learns to map trajectories to discrete codes from a codebook and I don't think this is the case. The authors should change it to something like \"latent skill\".\n- Line 279 - Should \"Three AntMaze layouts with four different goal location configuration each.\" be \"Three AntMaze layouts with four configurable goal locations each.\"\n- Line 411-414 - It would make more sense for this paragraph to be at the start of Section 5.4.\n- Line 407 - \"Kitchen the domain\" should be \"Kitchen domain\"\n- Line 408 - \"more challenging the kitchen-mixed\" should be \"more challenging kitchen-mixed\"\n- Figures - The authors have stated that the shaded area indicates the standard error. They also need to state what that solid line indicates. Is it the mean, median, etc?\n- Figures - I found the figures very hard to read. I would suggest the authors colour the text \"HILP w/ Offline Data\", \"Ours\", \"Online w/ HILP Skills\", etc, to match the colours of the lines in the plots. This would make the text/figures much easier to read."
            },
            "questions": {
                "value": "- How does your method compare to using offline-to-online RL methods which have access to reward labels?\n- How was the $\\alpha$ hyperparameter set?\n- Why did you not compare to other types of offline data sets?\n- What should I take from the coverage results?\n- In Table 1, what do the bold numbers represent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a two-phase framework, SUPE, which leverages data in two stages: first, extracting low-level skills during the offline pre-training phase, and then using these skills and unlabeled data in the online phase to train a high-level strategy for more efficient exploration. Building on prior works like SPiRL [1] and ExPLORe [2], the key contribution of this paper is to integrate unlabeled data with online data to accelerate exploration and training in off-policy reinforcement learning (RL) methods. In the offline pre-training stage, the authors train a set of low-level skills, while in the online phase, they develop a high-level policy by utilizing both online data and relabeled offline data. To assess the method\u2019s effectiveness, the authors compare SUPE with several baselines using benchmarks such as D4RL, and also discuss its limitations and potential directions for future research.\n\n[1] Pertsch, Karl, Youngwoon Lee, and Joseph Lim. \"Accelerating reinforcement learning with learned skill priors.\" Conference on robot learning. PMLR, 2021.\n\n[2] Li, Qiyang, et al. \"Accelerating exploration with unlabeled prior data.\" Advances in Neural Information Processing Systems 36 (2024)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper is highly detailed, well-written and provides detailed motivation. The complete code is also provided.\n* The authors conduct numerous experiments to thoroughly validate their method and address in detail several key issues that I am particularly concerned about, including its scalability, robustness."
            },
            "weaknesses": {
                "value": "* The overall novelty of this work is somewhat limited, as it builds heavily on existing methods and concepts (mentioned in summary). \n* Although numerous experiments are conducted, the selected tasks are relatively monotonous and simplistic. The experiments test only two types of tasks: AntMaze and Kitchen."
            },
            "questions": {
                "value": "* See weakness above. \n* Given the similarities between SPiRL [1] and this work, apart from the online reinforcement learning stage, why isn\u2019t SPiRL used as a baseline for comparison (despite the numerous experiments conducted) ?\n* In the pre-training stage, it would also be valuable to discuss whether trajectory segment length $H$ significantly impacts the method's performance.\n* I am curious whether using expert data would result in better low-level skills during the pre-training stage.\n\n[1] Pertsch, Karl, Youngwoon Lee, and Joseph Lim. \"Accelerating reinforcement learning with learned skill priors.\" Conference on robot learning. PMLR, 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}