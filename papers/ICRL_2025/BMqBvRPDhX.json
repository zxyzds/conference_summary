{
    "id": "BMqBvRPDhX",
    "title": "Enhancing Logits Distillation with Plug&Play Kendall's $\\tau$ Ranking Loss",
    "abstract": "Knowledge distillation typically employs the Kullback-Leibler (KL) divergence to constrain the output of the student model to precisely match the soft labels provided by the teacher model. However, the optimization process of KL divergence is challenging for the student and prone to suboptimal points. Also, we demonstrate that the gradients provided by KL divergence depend on channel scale and thus tend to overlook low-probability channels. The mismatch in low-probability channels also results in the neglect of inter-class relationship information, making it difficult for the student to further enhance performance. To address this issue, we propose an auxiliary ranking loss based on Kendall\u2019s $\\tau$ Coefficient, which can be plug-and-play in any logit-based distillation method, providing inter-class relationship information and balancing the attention to low-probability channels. We show that the proposed ranking loss is less affected by channel scale, and its optimization objective is consistent with that of KL divergence. Extensive experiments on CIFAR-100, ImageNet, and COCO datasets, as well as various CNN and ViT teacher-student architecture combinations, demonstrate that the proposed ranking loss can be plug-and-play on various baselines and enhance their performance.",
    "keywords": [
        "Knowledge Distillation",
        "Kendall's tau Coefficient",
        "Ranking Loss"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "Plug & Play Ranking Loss for Logits Distillation",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=BMqBvRPDhX",
    "pdf_link": "https://openreview.net/pdf?id=BMqBvRPDhX",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes an enhancement to the knowledge distillation process by introducing a plug-and-play ranking loss based on Kendall\u2019s \u03c4 Coefficient, which aims to mitigate the limitations of Kullback-Leibler (KL) divergence. The proposed ranking loss addresses issues like the neglect of low-probability channels and the inability of KL divergence to fully capture inter-class relationships. Extensive experiments on CIFAR-100, ImageNet, and COCO datasets demonstrate the effectiveness of the approach, showing consistent improvements when applied to various teacher-student architecture combinations in CNN and Vision Transformer (ViT) models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Novelty and contribution: The use of Kendall\u2019s \u03c4 ranking loss in the context of knowledge distillation appears to be novel and provides a promising way to complement traditional KL divergence-based losses. The ranking-based approach helps the student model better capture inter-class relationships.\n2. Plug-and-Play nature: The ranking loss is designed to be plug-and-play, which increases its practicality. It can be easily integrated into existing logit-based distillation frameworks without modifying the underlying architecture.\n3. Intensive experiments: The paper provides a wide range of experiments on different datasets and architecture combinations, demonstrating the robustness and generalizability of the proposed ranking loss.\n4. Addressing suboptimal points: The paper provides convincing arguments about how ranking loss helps in avoiding suboptimal solutions often seen in KL divergence optimization. The experimental results back up these claims, particularly in the analysis of accuracy and loss curves."
            },
            "weaknesses": {
                "value": "1. limited ablation study on hyperparameters. The author only discuss the effect of hyper-parameter k in the ranking loss. there is limited analysis of how sensitive the model is to different values of \u03b1, \u03b2, and \u03b3 in the overall loss function. \n2. Relation with other different distillation loss is not clear. The paper gives some explanations on why ranking loss works through its gradient form. However, I think since this loss is not used for its own. The author should discuss its relation with KD loss. KD constrains the logits after the softmax, however, ranking loss gives the constraint before the softmax, is it really necessary? I am not convinced by this.\n3. Some of the derivations involving the ranking loss (e.g., differentiable form of Kendall\u2019s \u03c4 coefficient) are challenging to follow due to their dense notation and lack of intermediate steps. Please consider adding more explanation or flowchart to increase the readablity."
            },
            "questions": {
                "value": "1. For the experiments involving different values of k and the comparison of different ranking loss forms, have you considered the effect of different initializations of the student model? The stability and sensitivity of the results with respect to different initial conditions could provide additional insights.\n2. RKD is another important loss for distillation. Have you ever tried to combine with RKD (relation knowledge distillation)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper highlights the issues with using KL divergence in knowledge distillation and introduces a ranking loss based on Kendall's \u03c4, which can be integrated into existing methods, enhances low-probability channel focus, and maintains inter-class relationships. Experimental results across various datasets and model architectures demonstrate that this approach consistently enhances performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed method is designed for straightforward integration into existing logit-based distillation frameworks, increasing its relevance and utility.\n2. Multiple experiments conducted on a variety of datasets and architectures provide evidence of the proposed approach's effectiveness"
            },
            "weaknesses": {
                "value": "1. The KL divergence optimization is a relatively common scheme for the logit distillation task. Could the authors elaborate on the main novelty of this integration?\n2. More ablation experiments and analysis are required for discussion; please see the Questions."
            },
            "questions": {
                "value": "1. What strategies could be implemented to minimize the computational overhead associated with the proposed ranking loss?\n2. This article mentioned that the proposed method balances the model\u2019s attention to larger and smaller-valued channels. Could the ranking loss also offer advantages in scenarios with class imbalance?\n3. Are there any adverse effects when combining the proposed method with others? Could you provide relevant ablation experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This study presents an auxiliary ranking loss based on Kendall\u2019s Tao Coefficient to improve knowledge distillation. The proposed ranking loss addresses the issue of KL divergence\u2019s neglect of low-probability channels by incorporating inter-class relationship information and enhancing focus on low-probability channels. It can be integrated into any logit-based distillation method and demonstrates consistent optimization objectives with KL divergence. Experiments on three datasets across various CNN and ViT teacher-student combinations show that the ranking loss effectively improves performance across multiple baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper proposes a plug-and-play ranking loss to address the suboptimization issues in knowledge distillation optimization.\n2. This paper demonstrates that Kullback-Leibler divergence is influenced by channel scale."
            },
            "weaknesses": {
                "value": "1. The paper claims that the proposed ranking loss primarily addresses KL divergence's tendency to overlook low-probability channels. However, based on the proposed formula, the main objective appears to be enforcing ranking consistency between the teacher and student models, with no clear indication of increased emphasis on information from smaller channels. It is recommended that the author explain this aspect.\n2. In the experimental section, it is recommended to include visualization experiments to highlight the primary contribution\u2014improved attention to low-probability channels. \n3. Since LSKD shows superior performance in Tables 1 and 2, further explanation of this result is advised."
            },
            "questions": {
                "value": "Please refer to the Strengths and Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the problem of knowledge distillation by highlighting the limitations of traditional KL divergence. The proposed method introduces an auxiliary loss based on Kendall\u2019s \u03c4 Coefficient, which enhances the learning of inter-class relationships and low-probability channels. Experiments conducted on three image classification datasets demonstrate the effectiveness of this approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed method is straightforward and can be seamlessly integrated with logits-based knowledge distillation techniques.\n- Experiments are conducted using both CNNs and ViTs across three different datasets. The ablation studies offer valuable insights into the proposed method."
            },
            "weaknesses": {
                "value": "- Some claims lack adequate justification. For instance, it remains unclear how the proposed method resolves the suboptimal problem depicted in Figure 1. Including visual comparisons of logits with and without the ranking loss could enhance clarity and understanding.\n\n- The proposed method includes multiple hyperparameters; however, the observed performance improvements are limited. Furthermore, the proposed method is evaluated against several straightforward baseline methods for knowledge distillation."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}