{
    "id": "8dzKkeWUUb",
    "title": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding",
    "abstract": "Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery.\nDespite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with specialized scientific tasks.\nTo develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instruction-following capabilities for domain-specific tasks.\nIn this process, we identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions. \nWe address these challenges through a meticulous pipeline, including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation.\nApplying this strategy, we present a suite of LLMs: SciLitLLM, specialized in scientific literature understanding.\nThese models demonstrate promising performance on scientific literature understanding benchmarks.\n(1) We present an effective framework that integrates CPT and SFT to adapt LLMs to scientific literature understanding, which can also be easily adapted to other domains.\n(2) We propose an LLM-based synthesis method to generate diverse and high-quality scientific instructions, resulting in a new instruction set -- SciLitIns -- for less-represented scientific domains. \n(3) SciLitLLM achieves promising performance in scientific literature understanding benchmarks.",
    "keywords": [
        "Large Language Model",
        "Pre-training",
        "Supervised Fine-tuning",
        "Scientific Literature Understanding"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8dzKkeWUUb",
    "pdf_link": "https://openreview.net/pdf?id=8dzKkeWUUb",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a method to improve scientific instruction following through post-training Qwen models. The main contribution is to collect science textbook data and improved SFT data mix. Their evaluation on a recent SciRiff dataset shows improvement."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper introduces a strong pipeline in collecting textbook data and SFT data. This is aligned with most recent LM papers, showcasing the importance of data in the success of LM training. \n- Improved result in science instruction following"
            },
            "weaknesses": {
                "value": "The authors don't seem to be planning to release their textbook dataset. This raises the question of data contamination in evaluating the proposed model."
            },
            "questions": {
                "value": "Are you planning to release the textbook datasets? \nCan you elaborate on the pipeline on what type of textbooks impact and improve task performance in SciRiff? \nCan you provide contamination studies in textbook datasets and the test/eval cases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)",
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I expect more explanation about the textbook datasets rather than mentioning \"in-house\" textbooks."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a synthetic data generation pipeline for training\nLMs for scientific literature understanding tasks. The main\ncontributions include:\n\n1.  a pipeline for curating continued pre-training corpus based on\n    textbooks and research papers (including document parsing,\n    formatting, and filtering),\n\n2.  another pipeline for creating instruction fine-tuning data for\n    scientific literature understanding tasks via prompting GPT-4o.\n\n3.  the authors show that finetuning Qwen-2.5 model (7B and 14B) on the\n    CPT and SFT data can improve the performance on scientific\n    literature understanding tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.  Overall the paper is nicely written and the pipeline is nicely\n    presented.\n\n2.  The curated dataset and models could be helpful."
            },
            "weaknesses": {
                "value": "1.  The primary contribution of the paper seems to be focused on the\n    dataset construction; overall the method resembles similar works for\n    synthetic data generation and there are some limitations:\n\n    1.  The PDF processing pipeline can be improved. Scientific PDF are\n        known to contain complex layout and structures, and previous\n        work have identified that using simple PDF parsers can lead to\n        suboptimal training results (S2ORC, Lo and Wang, and ). However,\n        the author primarily uses a simple PyPDF parser (\\\"Converting\n        these documents using tools like PyPDF2 often introduces\n        formatting and syntax errors, which degrade the quality of the\n        corpus (line 246)\\\"). I'd suggest the authors investigate the\n        text quality issues and check other libraries like papermage, Lo\n        et al.\n\n2.  The results are not very strong. I'd imagine a domain-specifically\n    distilled model can have a substance gain in performance compared to\n    GPT-4o, especially the instruction fine-tuning dataset is generated\n    via GPT-4o (line 331); however, as shown in table 3, the trained\n    models (SciLitLLM-7B and SciLitLLM-14B) are on par with GPT-4o. Also\n    the experimental design and presentation could be improved (see my\n    suggestions in questions)."
            },
            "questions": {
                "value": "1.  Are there special reasons why we should group the results based on\n    the 10B model size? (table 2/3). I think it's more reasonable to\n    organize based on pre-trained only/with instruction tuning/with\n    domain specific tuning?\n\n2.  Also in this paper there is only fine-tuning results on the Qwen\n    model family but not others. It would be interesting to compare the\n    fine-tuning effects on llama or other model families.\n\n    1.  the author compared the performance with the SciTulu model,\n        which is trained based on Llama-2 families. I don't think it's a\n        fair comparison in table 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new strategy to improve LLM for scientific literature understanding, which includes continual pretraining and supervised fine-tuning. The paper uses Llama3-8B to correct parsing errors and filters the dataset with Llama3-70B. The paper continues pretraining the Qwen2.5.The paper designs a three-step pipeline to generate diverse scientific contexts and corresponding QA pairs. The paper then incorporates heuristic deduplication and LLM-based filtering for the instructions. The proposed framework seems to improve the performance compared to SciRIFF. The paper also includes an ablation study."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper proposes a new framework that includes continual pretraining and supervised fine-tuning. The proposed framework and dataset can be very useful for other LLMs specialized in scientific understanding. The released dataset seems to be very comprehensive compared to the existing dataset.\n2. The paper shows that with the new framework, the paper can further improve the performance of general LLM. The framework is especially useful in small models. Additionally, the paper includes an ablation study for CPT, SFT, and instruction quality filters. \n3. The paper provides the code and its model. In the appendix, the paper shows the improvement of format & grammar correction, CPT quality filter, SFT details, benchmark details, and detailed performance on SciAssess."
            },
            "weaknesses": {
                "value": "1. The creation of the CPT and SFT datasets seems to rely on LLMs. The paper can randomly sample a small subset of the created dataset to check its quality with humans to show the effectiveness of the proposed framework.\n2. The experiment and ablation study is not comprehensive. The paper fails to show that the model is fine-tuned to other existing scientific understanding datasets, such as the Dolma dataset (https://allenai.github.io/dolma/). The paper did include SciTulu-7B dataset; however, scitulu is based on LLama2-tb. The paper should also show the model's performance only finetuned with SciLitIns. The analysis in the experiment is also rather simple. The paper needs to provide some explanation instead of just repeating the results in the table.\n3. Some details are not very clear. What is the score used in Tables 2 and 3? Many additional evaluation results and analysis are put in the Appendix. Authors should move some of them to the main paper."
            },
            "questions": {
                "value": "What is the difference between the new CPT dataset and the Dolma Dataset (https://allenai.github.io/dolma/)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces SciLitLLM, a specialized language model for scientific literature understanding, built using a hybrid approach combining continual pre-training (CPT) and supervised fine-tuning (SFT). The key contributions include 1) A pipeline that combines CPT with high-quality scientific corpora and SFT with diverse scientific instructions, 2) Novel methods for improving scientific text quality and generating domain-specific instructions, and 3)  Empirical results showing improved performance on scientific understanding benchmarks SciRIFF and SciAssess"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "### Well-Motivated Approach\n- The hybrid CPT+SFT strategy effectively addresses both domain knowledge and task-specific capabilities.\n- The pipeline is well-designed with clear motivation for each component.\n- The approach is generalizable to other specialized domains\n\n### Technical Contributions\n>Pipeline includes innovative components like LLM-based format correction (Section 3.1.1) and quality filtering (Section 3.1.2)\nThe instruction synthesis method (Section 3.2.1) is clever and tackles the challenge of limited scientific instruction data.\n\n### Solid empirical results\n- SciLitLLM-7B outperforms similar-sized models by significant margins \\~4% on SciAssess, +\\~10% on SciRIFF.\n- SciLitLLM-14B surpasses larger proprietary instruction tuned models (70B+ parameters)."
            },
            "weaknesses": {
                "value": "### Weaknesses\nThe CPT corpus (12.7B tokens) is relatively small compared to standard pre-training datasets The paper acknowledges this limitation but could discuss potential impacts more thoroughly. For example, how this affect the representation from different scientific subject domains.\n\nOtherwise I don't see clear weakness of paper of such kind. The paper appears comprehensive and well-executed for the research scope."
            },
            "questions": {
                "value": "1. How sensitive is the model performance to the quality filtering threshold (currently set at 25%)? Was this choice empirically validated?\n\n2. The instruction synthesis method uses GPT-4 - have you explored using smaller models or your own models in a bootstrapping approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}