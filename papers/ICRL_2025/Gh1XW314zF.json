{
    "id": "Gh1XW314zF",
    "title": "Multimodal Graph-LLM: Leveraging Graph-Enhanced LLMs for Multimodal Healthcare Predictions",
    "abstract": "Multimodal healthcare research is crucial for improving clinical decision-making by integrating diverse data types, such as clinical notes, lab results, and imaging. Large Language Models (LLMs) are widely recognized for their exceptional text-based reasoning capabilities, making them effective in processing complex clinical narratives. However, they struggle to incorporate multimodal data, limiting their broader applicability in healthcare analysis. In this work, we propose MG-LLM (Multimodal Graph-LLM), a novel framework that leverages the strengths of LLMs while enhancing them with multimodal alignment and data integration through Graph Neural Networks (GNNs). GNNs propagate information across similar patients, model temporal relationships between visits, and align information from different modalities, creating enriched multimodal context vectors. These context vectors are then injected into the intermediate layers of the LLM, allowing it to harness both textual reasoning and multimodal data for more accurate predictions. We evaluate MG-LLM on the MIMIC-IV and MIMIC-CXR datasets, demonstrating significant improvements in clinical prediction tasks compared to baseline models. Our results showcase the potential of combining the text reasoning power of LLMs with GNN-driven multimodal alignment for robust, comprehensive healthcare analysis.",
    "keywords": [
        "EHR",
        "multimodal",
        "LLM",
        "graphs",
        "healthcare"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Gh1XW314zF",
    "pdf_link": "https://openreview.net/pdf?id=Gh1XW314zF",
    "comments": [
        {
            "summary": {
                "value": "This paper presents an LLM-based framework, MG-LLM, that combines the large language model (LLM) with graph neural networks (GNNs) to make multimodal healthcare predictions. The framework constructs modality-specific graphs, uses GNNs for information propagation, performs multimodal alignment using contrastive learning, and injects the resulting context vectors into LLM. The paper evaluates the proposed approach on MIMIC-IV datasets for mortality prediction tasks, showing improved performance compared to baseline models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed model is sound.\n2. The authors provide comparisons with multiple baseline models, including both LLM-based and non-LLM approaches.\n3. The proposal achieves improved performance compared to baseline models."
            },
            "weaknesses": {
                "value": "1. The novelty of the framework needs clarification.\n- Unifying multimodal graphs using contrastive learning has been widely explored (multiple literature can be found on Google Scholar).\n- Using multiple types of graphs and multimodal data for healthcare has been studied in [1]. To my knowledge, using multimodal graphs to help LLMs better understand multimodal information appears to be novel.\n- The idea of Patient Similarity Integration and Context Injection is similar to the retrieve and refine modules proposed in [3]. A discussion and comparison would be beneficial.\n[1] GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. ICLR 2024.\n[2] Multimodal machine learning in precision health: a scoping review. npj Digital Medicine, 2022.\n[3] Retrieve, Reason, and Refine: Generating Accurate and Faithful Patient Instructions. NeurIPS, 2022.\n2. Several important implementation and experimental details are unclear:\n- The statistics of the pre-processed dataset are not provided. Since the evaluation dataset is self-processed rather than from the original dataset, it is important to provide details of the processed dataset. What is the total number of data used for training and evaluation? Meanwhile, how are the results of previous methods obtained on the processed dataset? \n- The graph details need clarification: Is it a directed or undirected graph? How to calculate edge weights? How to construct a node - is it the embedding of the entire data for a patient?\n- The architecture of the GNNs used is not specified.\n- The contrastive learning details are unclear, including the value of \u03c4 and the number and ratio of positive and negative samples.\n- The paper lacks many implementation details necessary for reproducibility: Important hyperparameters such as learning rate, epochs, and batch size are missing. Since the paper uses LLMs as the backbone, details about LoRA usage for model training and its corresponding hyperparameters should be provided. Additionally, details about training time, inference time, and GPU hours required are important.\n\n3. Several important insights are missing:\n- Why can the model generate better explanations when it is trained for binary predictions?\n- The evaluation dataset has a positive example rate of 0.23, suggesting that an algorithm predicting all negative samples would achieve 77% accuracy. Many models in Table 1 perform below 77%. More insights on this observation are needed.\n- How does the model handle missing modalities/data during inference?\n\n4. The analysis requires improvement:\n- The ablation studies focus only on modality combinations, lacking analysis of different graph construction methods, injection points in LLM layers, and the effect of different GNN architectures on performance.\n- Only one prediction task (mortality) is evaluated. The statistical significance of performance improvements needs discussion, especially given the imbalanced evaluation data.\n- The significant biases in the dataset are not discussed.\n\n5. Several additional experiments are strongly recommended:\n- Statistical significance tests.\n- Systematic error analysis."
            },
            "questions": {
                "value": "Copied from Weaknesses:\n\n1. The novelty of the framework needs clarification.\n- Unifying multimodal graphs using contrastive learning has been widely explored (multiple literature can be found on Google Scholar).\n- Using multiple types of graphs and multimodal data for healthcare has been studied in [1]. To my knowledge, using multimodal graphs to help LLMs better understand multimodal information appears to be novel.\n- The idea of Patient Similarity Integration and Context Injection is similar to the retrieve and refine modules proposed in [3]. A discussion and comparison would be beneficial.\n[1] GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. ICLR 2024.\n[2] Multimodal machine learning in precision health: a scoping review. npj Digital Medicine, 2022.\n[3] Retrieve, Reason, and Refine: Generating Accurate and Faithful Patient Instructions. NeurIPS, 2022.\n2. Several important implementation and experimental details are unclear:\n- The statistics of the pre-processed dataset are not provided. Since the evaluation dataset is self-processed rather than from the original dataset, it is important to provide details of the processed dataset. What is the total number of data used for training and evaluation? Meanwhile, how are the results of previous methods obtained on the processed dataset? \n- The graph details need clarification: Is it a directed or undirected graph? How to calculate edge weights? How to construct a node - is it the embedding of the entire data for a patient?\n- The architecture of the GNNs used is not specified.\n- The contrastive learning details are unclear, including the value of \u03c4 and the number and ratio of positive and negative samples.\n- The paper lacks many implementation details necessary for reproducibility: Important hyperparameters such as learning rate, epochs, and batch size are missing. Since the paper uses LLMs as the backbone, details about LoRA usage for model training and its corresponding hyperparameters should be provided. Additionally, details about training time, inference time, and GPU hours required are important.\n\n3. Several important insights are missing:\n- Why can the model generate better explanations when it is trained for binary predictions?\n- The evaluation dataset has a positive example rate of 0.23, suggesting that an algorithm predicting all negative samples would achieve 77% accuracy. Many models in Table 1 perform below 77%. More insights on this observation are needed.\n- How does the model handle missing modalities/data during inference?\n\n4. The analysis requires improvement:\n- The ablation studies focus only on modality combinations, lacking analysis of different graph construction methods, injection points in LLM layers, and the effect of different GNN architectures on performance.\n- Only one prediction task (mortality) is evaluated. The statistical significance of performance improvements needs discussion, especially given the imbalanced evaluation data.\n- The significant biases in the dataset are not discussed.\n\n5. Several additional experiments are strongly recommended:\n- Statistical significance tests.\n- Systematic error analysis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose Multimodal Graph-LLM (MG-LLM) to utilize the multimodal information that is (1) across time for the same patient,\u202f and (2) similar data from different patients. It combines multimodal data together with the Graph Neural Networks (GNNs) in order to enhance patient representation, improve multimodal data integration, and enable better contextual reasoning. This work in general is easy to understand and the idea presented is intuitive to follow. However, the work itself needs more to demonstrate its performance. The illustration also needs to be improved. I think it needs more work to meet the acceptance criteria."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe general idea of the paper is easy to understand.\n\n2.\tThe concept presented is intuitive and easy to follow."
            },
            "weaknesses": {
                "value": "1.\tThe experiments are not comprehensive and convincing. The only\u202fprediction task presented is a binary classification of one-year-mortality for the patient using the MIMIC-IV Data. More experiments would be needed to lead to a solid conclusion.\n\n2.\tSome justification and explanations are not clear. For example, how specifically is Eq.1 used\u202ffor alignment is not clearly explained.\n\n3.\tSome experimental initialization and settings (e.g., embedding networks) could have been better established, and the details should be more clear (e.g., embedding dimensions, etc.). The evaluation metric of the results could be more comprehensive."
            },
            "questions": {
                "value": "1.\tI do not understand how Eq. 1. Is used for the alignment process.\u202f Is this loss function used to fine-tune the mapping network of the text and image embeddings and therefore update the embeddings then?\n\n2.\tIt was mentioned that a ResNet50 model pre-trained on ImageNet was utilized to extract image features at the beginning. Considering the domain gap between the nature image and medical image, wouldn\u2019t a medical-image focused network a better choice for this initialization?\n\n3.\tWhy would the dominator of equation 1 distinguish between i = j and i != j\uff1f\u202fThey can be\u202fmathematically combined.\n\n4.\tWhat is dimension of the embedding vector spaces?\n\n5.\tWhat are lab results? Maybe an example/explanation could be provided for readers to better understand those data.\n\n6.\tFor a classification task, it could be good to also include the AUC as an evaluation metric."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a multimodal graph-based large language model (MG-LLM) designed for healthcare research. MG-LLM can simultaneously process four different modalities of electronic health records (EHRs) while incorporating graph learning techniques. The multimodal embeddings are then integrated into the LLM, leveraging its strengths in natural language processing and text reasoning. MG-LLM is evaluated on the MIMIC-IV and MIMIC-CXR datasets, demonstrating superior performance compared to baseline models. However, the paper has several significant drawbacks, and the evaluation results are not particularly convincing. For further details, please refer to the weaknesses and questions outlined in the review."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. A promising solution for combining graph structures with LLMs in healthcare research.\n2. A robust model for processing four modalities of EHRs."
            },
            "weaknesses": {
                "value": "1. The article's writing quality requires significant improvement, particularly in terms of consistency, completeness, and coherence of the logic.\n2. Many of the references cited do not adequately support the authors' arguments, as the majority are not from peer-reviewed sources.\n3. The paper lacks an introduction and analysis of state-of-the-art multimodal large language models (Multimodal-LLMs), such as BLIP, BLIP-2, and LLAVA.\n4. As you have proposed a graph-related LLM, the Introduction section lacks any discussion or review of relevant graph-related research.\n5. The method section lacks sufficient mathematical symbols and formulas to clearly illustrate your model, making it difficult to follow the model's pipeline. \n6. In addition, the method section does not explain how this graph-LLM operates on similar graphs or temporal graphs.\n7. There are several issues in the experimental results and analysis, which render the findings unconvincing."
            },
            "questions": {
                "value": "1. In the introduction, specifically in lines 41-46, you discuss the limitations of LLMs in processing multimodal data within the healthcare research field. However, several multimodal LLMs have already been developed for EHRs, such as Health-LLM [1] and Med-MLLM [2]. Additionally, models like BLIP2 [3] and LLAVA [4] can be adapted for healthcare research. Why do you not analyze your model's advancements in comparison to these existing models?\n2. Why is there no discussion of any graph-related LLMs introduced in the first section, such as [5, 6]?\n3. Why are there many unpublished research works included in your related-work section as references?\n4. How can I gain a deeper understanding of the functions of temporal edges and similar edges used in your model? Could you provide an additional graph to illustrate this?\n5. What does \"residual stream\" refer to in the method section, and what does the symbol \"v\" represent?\n6. How is longitudinal patient EHR data utilized in your model? Is it truly effective for predicting patient mortality, given that the occurrence of death is fixed at the last time of the visit? There are some longutidal EHR analyzation works you could refer [7,8].\n7. In the experiment section, why does the LLM Llama-3-8b exhibit the worst evaluation performance compared to all baseline models, \nincluding non-LLM-based methods? Are there any potential mistakes in this evaluation in Table 1?\n8. The highest F1 score in Table 1 is achieved by the model from Rezk et al. (2024). Why do you state that your model reached the highest F1 score in Section 4.2?\n\nReferences:\n\n[1].  Kim, Y., Xu, X., McDuff, D., Breazeal, C. &amp; Park, H.W.. (2024). Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data. <i>Proceedings of the fifth Conference on Health, Inference, and Learning</i>, in <i>Proceedings of Machine Learning Research</i> 248:522-539 Available from https://proceedings.mlr.press/v248/kim24b.html.\n\n[2]. Liu, F., Zhu, T., Wu, X., Yang, B., You, C., Wang, C., ... & Clifton, D. A. (2023). A medical multimodal large language model for future pandemics. NPJ Digital Medicine, 6(1), 226.\n\n[3]. Li, J., Li, D., Savarese, S., & Hoi, S. (2023, July). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning (pp. 19730-19742). PMLR.\n\n[4]. Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2024). Visual instruction tuning. Advances in neural information processing systems, 36.\n\n[5]. Wang, H., Feng, S., He, T., Tan, Z., Han, X., & Tsvetkov, Y. (2024). Can language models solve graph problems in natural language?. Advances in Neural Information Processing Systems, 36.\n\n[6]. Fatemi, B., Halcrow, J., & Perozzi, B. Talk like a Graph: Encoding Graphs for Large Language Models. In The Twelfth International Conference on Learning Representations.\n\n[7]. Niu, S., Yin, Q., Ma, J., Song, Y., Xu, Y., Bai, L., ... & Yang, X. (2024). Enhancing healthcare decision support through explainable AI models for risk prediction. Decision Support Systems, 181, 114228.\n\n[8]. Gao, J., Xiao, C., Wang, Y., Tang, W., Glass, L. M., & Sun, J. (2020, April). Stagenet: Stage-aware neural networks for health risk prediction. In Proceedings of The Web Conference 2020 (pp. 530-540)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The authors mention using the GPT-4 series model in their experiments as part of some baseline models with MIMIC data. However, PhysioNet has strict guidelines regarding the Responsible use of MIMIC data with online services such as GPT. The authors must clarify the source of the GPT model used and confirm that they have signed the \"opt-outs for data sharing\" form in compliance with these regulations."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a framework that integrates the reasoning capabilities of LLMs with GNNs on the assumption that GNNs have the multimodal data processing power.  With this design, the authors hope the work will enhance clinical predictions by incorporating diverse healthcare data types such as clinical notes, lab results, and medical images. \nIn addition, the proposed method also leverages GNNs to model patient similarities and temporal relationships while aligning multimodal information into a unified context vector that enriches the LLM. The framework is evaluated on MIMIC-IV and MIMIC-CXR datasets, showing decent performance over baseline models, but worse than (Resz at al 2024) from results in Table 1."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The intention on multimodality is meaningful as health data is inherently multimodal and no single modality can fully capture comprehensive health and patient information.\n- Using GNNs for integrating multimodal data is effective and makes sense.\n- the dynamic patient similarity modeling using GNNs is a good approach."
            },
            "weaknesses": {
                "value": "- Very limited dataset, use case, and evaluation tasks, which limits the generalizability of the model to other datasets and healthcare environments.. MIMIC data was collected within critical care scenario with very limited use cases. Even though, previous research often evaluated their works across diverse set of tasks including readmission prediction, next dat discharge prediction, treatment recommendation, medical coding, and so on. However in this work, the authors only consider mortality prediction as the evaluation task. This is not sufficient to demonstrate the utility of the proposed approach.\nNote that from Table 1, even with multimodal data, the proposed approach is not the best although authors make their proposed model bold font to indicate best performance.\n- The experimental initialization and settings could be better discussed and the details should be more clear. The evaluation metric of the results could be more comprehensive.\n- Data Imbalance and Processing: The paper uses a downsampled dataset for one-year mortality prediction, which may introduce bias. It would benefit from a deeper exploration of how this impacts the model's performance.\n- There is lack of ablation study to analyze the design.\n- There is lack of analysis or discussion on how missing or incomplete modalities will affect the performance"
            },
            "questions": {
                "value": "see weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}