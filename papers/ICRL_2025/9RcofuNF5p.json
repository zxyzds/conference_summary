{
    "id": "9RcofuNF5p",
    "title": "Contextualized Messages Boost Graph Representations",
    "abstract": "Graph neural networks (GNNs) have gained significant attention in recent years for their ability to process data that may be represented as graphs. This has prompted several studies to explore their representational capability based on the graph isomorphism task. These works inherently assume a countable node feature representation, potentially limiting their applicability. Interestingly, only a few study GNNs with uncountable node feature representation. In the paper, a novel perspective on the representational capability of GNNs is investigated across all levels\u2014node-level, neighborhood-level, and graph-level\u2014when the space of node feature representation is uncountable. More specifically, the strict injective and metric requirements are *softly* relaxed by employing a *pseudometric* distance on the space of input to create a *soft-injective* function such that distinct inputs may produce *similar* outputs if and only if the *pseudometric* deems the inputs to be sufficiently *similar* on some representation. As a consequence, a simple and computationally efficient *soft-isomorphic* relational graph convolution network (SIR-GCN) that emphasizes the contextualized transformation of neighborhood feature representations via *anisotropic* and *dynamic* message functions is proposed. A mathematical discussion on the relationship between SIR-GCN and widely used GNNs is then laid out to put the contribution into context, establishing SIR-GCN as a generalization of classical GNN methodologies. Experiments on synthetic and benchmark datasets then demonstrate the relative superiority of SIR-GCN, outperforming comparable models in node and graph property prediction tasks.",
    "keywords": [
        "deep learning",
        "graph neural network",
        "representational capability",
        "soft-isomorphic relational graph convolution network"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "We theoretically justify the need for anisotropic and dynamic messages in GNNs and propose a computationally efficient model satisfying this requirement that empirically outperforms comparable models.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=9RcofuNF5p",
    "pdf_link": "https://openreview.net/pdf?id=9RcofuNF5p",
    "comments": [
        {
            "title": {
                "value": "Rebuttal by Authors (cont.)"
            },
            "comment": {
                "value": "5. The horizontal and vertical axes of Fig. 2 represent the scalar neighborhood node features $\\boldsymbol{h_{v_1}}$ and $\\boldsymbol{h_{v_2}}$ of node $u$, where $v_1$ and $v_2$ are its neighbors. Line 193 provides context where these features represent zero-mean scores for anomaly detection. Fig. 2 illustrates how these features are transformed and aggregated using different *soft-injective* message functions. As clarified in Line 192, for arbitrary *pseudometrics* $d$, the corresponding *soft-injective* message function $g$ must be *dynamic* or non-linear. This insight motivates the use of an MLP to model the *dynamic* nature of the *soft-injective* message function, similar to Brody et al. (2021). Figs. 2(c) and 2(d) then highlight how MLPs may be used to model *anisotropic* and *dynamic* (*soft-injective*) message functions within the SIR-GCN model, aligning with the theoretical motivation discussed in the paper.\n\n6. The results from the DictionaryLookup and GraphHeterophily synthetic datasets serve as key illustrative examples of the practical utility and novelty of SIR-GCN. In the DictionaryLookup task, both GATv2 and SIR-GCN achieved perfect accuracy, highlighting the utility of a *dynamic* attentional or relational mechanism in capturing the relationships between *query* and *key* nodes, as explained in Line 413. In contrast, the GraphHeterophily dataset clearly exposes the limitations of existing GNNs (including GATv2), as only SIR-GCN achieved near-zero MSE loss while all other GNNs obtained large errors, as detailed above in 3. This dataset thus underscores the utility and novelty of SIR-GCN with its *anisotropic* and *dynamic* message functions (in contrast to attention mechanism).\n\nFurthermore, the results on benchmark datasets further highlight the superior performance of SIR-GCN over existing GNNs in more complex, real-world problems across various domains. In these evaluations, the SIR-GCN models implemented follow standard model design for GNNs, as outlined in Appendix B2. Specifically, we directly replace the GNN component (from GCN, GraphSAGE, GAT, GIN, and PNA) with SIR-GCN. Since we focused on this single replacement, performing an ablation study is neither appropriate nor feasible, similar to how no ablation study was conducted by Brody et al. (2021) when GAT was replaced with GATv2. Nevertheless, Section 4 provides a detailed mathematical discussion of how modifying specific parameters in SIR-GCN may recover conventional GNNs, whose performance is already included in our results. Hence, the reported results and performance improvements are solely attributed to the novel aspects of SIR-GCN. We are open to suggestions for further improving the results section.\n\nFinally, we emphasize that SIR-GCN can be easily integrated into more complex frameworks, such as grouped reversible residual connections (Li et al., 2021) and graph stochastic attention (Miao et al., 2022), to further improve performance. Previous works, particularly in Li et al. (2021) and Miao et al. (2022), have demonstrated that any GNN backbone, such as GCN, GraphSAGE, GAT, GIN, and PNA, can be seamlessly and easily incorporated into their frameworks to employ additional/advanced techniques for enhanced performance. While this also holds for SIR-GCN, exploring such integrations with SIR-GCN is beyond the scope of this paper, as it primarily focuses on introducing the key contributions and foundations of SIR-GCN.\n\nReferences:\n\nShaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? arXiv preprint arXiv:2105.14491, 2021.\n\nGuohao Li, Matthias Muller, Bernard Ghanem, and Vladlen Koltun. Training graph neural networks with 1000 layers. In International Conference on Machine Learning, pp. 6437-6449. PMLR, 2021.\n\nSiqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. In International Conference on Machine Learning, pp. 15524-15543. PMLR, 2022."
            }
        },
        {
            "title": {
                "value": "Rebuttal by Authors (cont.)"
            },
            "comment": {
                "value": "3. The GraphHeterophily dataset is designed to test the ability of models to reason about heterophilous relationships in directed graphs. The graphs are uniformly generated using DGL's `rand_graph` function with each node uniformly assigned one of $c$ class labels using PyTorch's `randint` function. This approach ensures diversity in graph structures and degrees of heterophily, making the dataset robust for evaluation. Detailed descriptions are provided in Appendix B1. The task is then to count the total number of directed edges in each graph connecting nodes with different class labels, as clarified in Line 417. A sample graph is illustrated in Fig. 5, where four nodes (labeled A or B) are connected by six directed edges. Models must then correctly identify/count the four edges (highlighted in blue) that connect nodes with distinct labels.\n\nThe results in Table 2 show that SIR-GCN consistently achieves near-zero MSE loss for this simple illustrative task while other models (including PNA and GATv2) obtained large losses. This performance is nevertheless expected, as explained in Line 431, since if node features are one-hot encodings of class labels, SIR-GCN with sum aggregation, $\\boldsymbol{W_Q} = \\boldsymbol{I}$, $\\boldsymbol{W_K} = - \\boldsymbol{I}$, $\\sigma = \\text{ReLU}$, and $\\boldsymbol{W_R} = \\boldsymbol{1}^\\top$ can consistently produce accurate outputs, regardless of graph structure or heterophily degrees. The key to this success lies in the *anisotropic* and *dynamic* message function enabled by the ReLU activation along edges, which allows SIR-GCN to \"reason\" based on pairs of neighboring node labels and generate nuanced, context-aware messages. It is worth noting that while GATv2 can also somewhat \"reason\" along edges due to its *dynamic* attention mechanism, its reliance on attentional (softmax) aggregation, which fails to make sharp decisions (Velickovic, 2024) and preserve graph structure as noted in Line 433, hinders its performance. This distinction further highlights the flexibility of SIR-GCN in handling such challenges. Overall, this task underscores the importance of *anisotropic* and *dynamic* message functions (in contrast to attention mechanism), demonstrating SIR-GCN as the first MPNN instance to meet these requirements, further solidifying its contribution and novelty with respect to existing GNNs.\n\n4. We thank the reviewer for pointing out the lack of definition for $\\boldsymbol{A}$. In response, we have changed $\\boldsymbol{W_R} = \\boldsymbol{a_\\text{GAT}^\\top}$ for clarity. To clarify, Eq. 16 simply demonstrates how the unnormalized attention mechanism in Eq. 15 for GATv2 may be interpreted as the contextualized message in the SIR-GCN model, as mentioned in Line 313. We do not intend to suggest that Eq. 16 is equivalent to GAT. Instead, the illustration shows how the concept of *anisotropic* and *dynamic* functions in the attention mechanism of GATv2 are adapted to message functions in SIR-GCN. This aligns with the explanation above in 1. Furthermore, we also emphasize that only the attention weights in GATv2 are *anisotropic* and *dynamic*, its message functions are still only linearly transformed, which can limit its expressivity, as discussed in Line 305. In contrast, SIR-GCN introduces a contextualized, non-linear transformation to the message functions, improving its ability to capture complex relationships. The significance of this contribution is evident in the experimental results, particularly in Table 3 where SIR-GCN significantly outperforms GAT across all datasets with the same parameter budget. Nevertheless, in Line 314, we clarify how GATv2, up to a normalizing constant, can be obtained from SIR-GCN by selecting the appropriate parameters: $\\boldsymbol{W_Q} = \\begin{bmatrix} \\boldsymbol{W_{Q,\\text{GAT}}} \\\\\\\\ \\boldsymbol{0} \\end{bmatrix}$, $\\boldsymbol{W_K} = \\begin{bmatrix} \\boldsymbol{W_{K,\\text{GAT}}} \\\\\\\\ \\boldsymbol{W_{K,\\text{GAT}}} \\end{bmatrix}$, $\\sigma\\left(\\begin{bmatrix} \\boldsymbol{h_1} \\\\\\\\ \\boldsymbol{h_2} \\end{bmatrix}\\right) = \\exp\\left(\\boldsymbol{a_\\text{GAT}^\\top} ~ \\text{LeakyReLU}\\left(\\boldsymbol{h_1}\\right)\\right) \\cdot \\boldsymbol{h_2}$, and $\\boldsymbol{W_R} = \\boldsymbol{I}$. \n\nReferences:\n\nPetar Velickovic, Christos Perivolaropoulos, Federico Barbero, and Razvan Pascanu. softmax is not enough (for sharp out-of-distribution). arXiv preprint arXiv:2410.01104, 2024."
            }
        },
        {
            "title": {
                "value": "Rebuttal by Authors"
            },
            "comment": {
                "value": "We thank the reviewer for the constructive insights!\n\n1. We would first like to clarify that our work is not an extension of Corso et al. (2020) but rather a novel approach to handling uncountable node features within the MPNN framework. Unlike PNA whose key features are multiple aggregators and scalers, SIR-GCN only employs a single aggregator. Consequently, it is also computationally efficient, requiring only an activation function (linear complexity) along edges, unlike PNA, which requires a full linear layer (quadratic complexity) as shown in Table 8 of Appendix C. Despite its lower computational requirement, SIR-GCN still achieves superior performance, consistently outperforming PNA across all benchmarks spanning both countable and uncountable node features, highlighting its utility, efficiency, and novelty.\n\nMoreover, while Brody et al. (2021) explored *anisotropic* and *dynamic* functions in the context of GAT attention mechanisms, our work is the first to apply these principles specifically to message functions within the broader MPNN framework. Our work theoretically and empirically shows how this key innovation enables SIR-GCN to capture complex, nuanced relationships between pairs of neighboring nodes prior to aggregation, allowing it to better handle uncountable node features. The significance of this contribution is evident in the experimental results, particularly in Table 3 where SIR-GCN consistently outperforms all other GNNs (including GAT) with the same parameter budget. This is also evident in Table 2 where SIR-GCN successfully accomplished the task when all other GNNs failed, as elaborated in 3 below.\n\nOverall, our work offers a novel perspective into the representational capability of GNNs, distinct from Corso et al. (2020) and Brody et al. (2021), especially in the problem of uncountable node features, by demonstrating that using only a single aggregator can already substantially improve the representational capability of GNNs. The SIR-GCN, through its use of *anisotropic* and *dynamic* message functions within the MPNN framework, introduces a fundamentally novel approach to this problem, distinguishing it from existing models like PNA and GATv2 whose specific utility and novelty differs from our work. Notably, the unique design of SIR-GCN directly addresses several limitations of existing GNNs, as demonstrated by illustrative examples on synthetic datasets and model performance on benchmark datasets. These findings establish SIR-GCN as a significant and novel contribution to advancing GNN research.\n\n2. The connection between the motivation and the proposed SIR-GCN is firmly established through the theoretical foundation provided by *pseudometrics* and *soft-injective* functions. Corollary 1 guarantees the existence of a *soft-injective* hash function $G$ and *soft-injective* feature map $g$ given a *pseudometric* $d$ on $\\mathcal{H}$ and *pseudometric* $D$ on bounded equinumerous *multisets* of $\\mathcal{H}$. From this result, two necessary properties of the *soft-injective* message function $g$ emerge, as elaborated in Line 189 onward, which directly inform the design of SIR-GCN. As clarified in Line 192, for arbitrary *pseudometrics* $d$, the corresponding *soft-injective* message function $g$ must be *dynamic* or non-linear. This insight motivates the use of an MLP to model the *dynamic* nature of $g$, similar to Brody et al. (2021). From Line 216, we further show that the *soft-injective* message function $g$ must also adapt to each node independently. This insight then motivates integrating the features of the center node into $g$, consequently making it *anisotropic*, to avoid the impracticality of designing distinct message functions for each node in large graphs. Combining these two properties, SIR-GCN is the first MPNN instance to utilize a *soft-injective* message function that is both *anisotropic* and *dynamic*. This connection between the theoretical motivation and the architectural design ensures that SIR-GCN is not just empirically performant but also rigorously grounded in theory, demonstrating the significance and novelty of our approach.\n\nReferences:\n\nShaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? arXiv preprint arXiv:2105.14491, 2021.\n\nGabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. Principal neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems, 33:13260-13271, 2020."
            }
        },
        {
            "title": {
                "value": "Rebuttal by Authors (cont.)"
            },
            "comment": {
                "value": "5. It may be possible to improve the performance of GAT and GraphSAGE in Table 4 with more parameters. However, as noted in Line 806, the results presented are taken directly from the OGB leaderboard. Extending GAT and GraphSAGE with more parameters for potential performance improvements would require significant tuning efforts and is beyond the scope of our work. Our primary focus is to highlight the performance of SIR-GCN relative to publicly released results in the leaderboard as stated in Lines 806 and 816."
            }
        },
        {
            "title": {
                "value": "Rebuttal by Authors"
            },
            "comment": {
                "value": "We thank the reviewer for the positive feedback!\n\n1. The key novelty of SIR-GCN lies in its *anisotropic* and *dynamic* message function, allowing it to better handle uncountable node features. Unlike other GNNs which only employ *isotropic* or monotonic/linear transformations to neighborhood features, SIR-GCN is among the first MPNN instance to introduce *dynamic* transformations to neighborhood features that also account for the center node features. This design specifically enables SIR-GCN to learn the complex, nuanced relationships between pairs of neighboring nodes before aggregating them into a single feature. This is also key to how SIR-GCN handles uncountable node features, as explained in Line 527 of the conclusion. An illustration of this feature is further elaborated in 3 below.\n\n2. Similar to the results presented in Corso et al. (2020), Table 3 also directly presents the available results from Dwivedi et al. (2023), Corso et al. (2020), and Tailor et al. (2021). The missing values indicate that these works have not considered the particular dataset for their model. This is added as a footnote in Table 3. We also emphasize that in addition to the datasets considered in Corso et al. (2020) for PNA, our paper further includes the WikiCS, PATTERN, and CLUSTER datasets from the Benchmarking GNNs (Dwivedi et al., 2023) as an additional evaluation. \n\n3. We thank the reviewer for pointing us to the work of Mao et al. (2024). While the study does analyze real-world graphs with varying degrees of heterophily, the graphs considered are specific to node property prediction tasks. In contrast, GraphHeterophily is designed specifically for graph property prediction which makes the graphs considered in Mao et al. (2024) not directly compatible. Nonetheless, we emphasize that the directed graphs in GraphHeterophily are uniformly generated using DGL's `rand_graph` function, with class labels also uniformly assigned using PyTorch's `randint` function. These measures, highlighted in Line 775, ensure that the graphs are sufficiently diverse in terms of graph structure and heterophily degrees, supporting the robustness of the results.\n\nFurthermore, we would also like to highlight that while other GNNs fail in the simple task of GraphHeterophily, SIR-GCN excels in the task. Its remarkable performance is nevertheless expected, as explained in Line 430, since if node features are one-hot encodings of class labels, SIR-GCN with sum aggregation, $\\boldsymbol{W_Q} = \\boldsymbol{I}$, $\\boldsymbol{W_K} = - \\boldsymbol{I}$, $\\sigma = \\text{ReLU}$, and $\\boldsymbol{W_R} = \\boldsymbol{1}^\\top$ can consistently produce accurate outputs, regardless of graph structure or heterophily degrees. This success may be attributed to the ReLU activation applied along edges, enabling SIR-GCN to \"reason\" along edges, based on the labels of pairs of neighboring nodes, and produce a *dynamic* message accounting for this learned relationship. This illustration further highlights the significance of *anisotropic* and *dynamic* message functions, underscoring the novelty of SIR-GCN as the first MPNN instance to satisfy this requirement.\n\n4. The attention mechanism of the original GAT is *anisotropic* but not *dynamic* as highlighted by Brody et al. (2021). GATv2 addresses this limitation by making the attention mechanism *dynamic*. Nevertheless, both GAT and GATv2 still only linearly transform the messages (neighborhood features) as stated in Line 305. Thus, the features of the center node only affect the aggregated neighborhood features by determining the degree of contribution through the scalar attentional weight. In contrast, SIR-GCN is the first MPNN instance to leverage this idea and make the actual messages *anisotropic* and *dynamic*. The experimental results, particularly in Table 3 where SIR-GCN significantly outperforms GAT across all datasets with the same parameter budget, underscore the significance of this idea and the novelty of SIR-GCN.\n\nReferences:\n\nShaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? arXiv preprint arXiv:2105.14491, 2021.\n\nGabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. Principal neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems, 33:13260-13271, 2020.\n\nVijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. Journal of Machine Learning Research, 24 (43):1-48, 2023.\n\nHaitao Mao, Zhikai Chen, Wei Jin, Haoyu Han, Yao Ma, Tong Zhao, Neil Shah, Jiliang Tang. Demystifying structural disparity in graph neural networks: Can one size fit all?. Advances in neural information processing systems, 2024.\n\nShyam A Tailor, Felix L Opolka, Pietro Lio, and Nicholas D Lane. Do we need anisotropic graph neural networks? arXiv preprint arXiv:2104.01481, 2021."
            }
        },
        {
            "title": {
                "value": "Rebuttal by Authors (cont.)"
            },
            "comment": {
                "value": "3. We further highlight that the runtime analyses on synthetic datasets in Tables 6 and 7 provide a more controlled assessment of computational complexity, illustrating changes in model runtime as model complexity and problem size increase (*i.e.*, as $n$ and $c$ increase). This comparison allows for more meaningful insights into the scalability of each model in isolation. To further strengthen our claim, we have included an additional comparison of the asymptotic complexities of each model in Appendix C and in Line 825 to complement the results in Tables 6 and 7, specifically with regards to PNA which is also designed for uncountable node features but requires significantly longer runtime. The table further highlights how SIR-GCN has a computational complexity comparable to GCN, GraphSAGE, GAT, GATv2, and GIN while outperforming these models across all benchmarks. Notably, SIR-GCN also demonstrates a lower complexity than PNA, yet delivers superior performance across all datasets. These additional analyses underscore how SIR-GCN effectively balances computational efficiency and model expressivity, further demonstrating its novelty and practical utility.\n\n| Model       |                                                       Model Complexity                                                       |\n| ----------- | :--------------------------------------------------------------------------------------------------------------------------: |\n| GCN         |                           $O(V \\times d_\\text{out} \\times d_\\text{in} + E \\times d_\\text{out})$                           |\n| GraphSAGE   |                           $O(V \\times d_\\text{out} \\times d_\\text{in} + E \\times d_\\text{out})$                           |\n| GAT / GATv2 |                           $O(V \\times d_\\text{out} \\times d_\\text{in} + E \\times d_\\text{out})$                           |\n| GIN         |                                     $O(E \\times d_\\text{in} + V \\times \\texttt{MLP})$                                     |\n| PNA         |      $O(E \\times d_\\text{in}^2 + E \\times d_\\text{in} \\times k + V \\times d_\\text{out} \\times d_\\text{in} \\times k)$      |\n| SIR-GCN     | $O(V \\times d_\\text{hidden} \\times d_\\text{in} + E \\times d_\\text{hidden} + V \\times d_\\text{out} \\times d_\\text{hidden})$ |"
            }
        },
        {
            "title": {
                "value": "Rebuttal by Authors"
            },
            "comment": {
                "value": "We thank the reviewer for the constructive feedback!\n\n1. We would like to clarify that both SIR-GCN models in Tables 4 and 5 largely employ the same architectural design with only a single GNN layer, as noted in Line 485 for Table 4 and Line 512 for Table 5. Appendix B2 presents a more detailed description of the architecture where the key difference between the two models lies in Table 5 including a graph readout function (since ogbg-molhiv is a graph property prediction task) whereas Table 4 does not (since ogbn-arxiv is a node property prediction task). The large discrepancy in parameter counts is primarily due to the input and output dimensions of each dataset. For ogbn-arxiv in Table 4, the input node features are 768-dimensional with a 40-dimensional output, while for ogbg-molhiv in Table 5, the input node features are only 174-dimensional with a scalar output. This difference in the input node feature dimension, stated in Lines 800 and 809, naturally affects the parameter count due to the Linear layer multiplying the input dimension by the hidden state dimension, even if they have the same architectural design. Specifically, a single Linear layer in ogbn-arxiv requires ~200,000 (768 x 256) parameters while a single Linear layer in ogbg-molhiv only requires ~53,000 (174 x 300) parameters. We also emphasize that, given the distinct nature and domains of these datasets (citation network vs molecular network), a comparison between parameter counts is not meaningful for fairness. Instead, we highlight that SIR-GCN can outperform larger models using fewer parameters in both datasets, particularly with LGGNN in Table 4 and GSN in Table 5. This highlights the utility and expressivity of SIR-GCN.\n\n2. We have performed additional runtime analysis based on the reviewer's suggestion. The models below follow a similar architecture as reported in Appendix B2 for SIR-GCN using the same hyperparameters. The results still underscore how SIR-GCN effectively balances computational efficiency and model expressivity, particularly when compared against PNA which is also designed for uncountable node features. Notably, PNA resulted in an out-of-memory (OOM) error for ogbn-arxiv as it requires $O(|\\mathcal{E}| \\times d_\\text{in})$ memory, where $|\\mathcal{E}|$ is significantly larger for this graph and $d_\\text{in} = 768$ as noted above in 1. On the other hand, PNA requires significantly longer (more than 50%) runtime compared to the other models in ogbg-molhiv, consistent with the results in Tables 6 and 7. Meanwhile, SIR-GCN requires approximately the same runtime as the other models across both datasets.\n\n|   Model   |     ogbn-arxiv     |    ogbg-molhiv    |\n| :-------: | :----------------: | :----------------: |\n|    GCN    | 0.0729s \u00b1 0.0008s | 0.8029s \u00b1 0.1171s |\n| GraphSAGE | 0.1161s \u00b1 0.0039s | 0.8062s \u00b1 0.1166s |\n|   GATv2   | 0.1006s \u00b1 0.0022s | 0.6835s \u00b1 0.1620s |\n|    GIN    | 0.0830s \u00b1 0.0015s | 0.7574s \u00b1 0.1620s |\n|    PNA    |        OOM        | 1.2786s \u00b1 0.0664s |\n|  SIR-GCN  | 0.1265s \u00b1 0.0039s | 0.7345s \u00b1 0.1300s |"
            }
        },
        {
            "summary": {
                "value": "This paper extends PNA, which considers uncountable node features, by incorporating anisotropic and dynamic. The difference between PNA and the proposed SIR-GCN is the nonlinear mapping outside the message creation. To motivate this method, the authors introduce the concept of soft-injective function. This paper shows some existing methods are variants of the proposed SIR-GCN. Evaluations on synthetic and real datasets demonstrate its effectiveness."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The uncountable feature is an important topic in GNN. \n- The writing and organization are good to follow. \n- The insight of existing methods under the framework of SIR-GCN is interesting."
            },
            "weaknesses": {
                "value": "- The novelty seems weak. Both the anisotropic and dynamic are not novel.  This paper can be seen as a combination of PNA and GATv2. \n- The motivation and the proposed SIR-GCN are not closely connected. It is not clear the connection between the soft-injective function, dynamic transformation, and anisotropic message. \n- The description of the GraphHeterophily is not clear. Thus,  it is not obvious why the proposed SIR-GCN significantly outperforms existing ones. \n- The derivation from Eq. 15 to Eq. 16 seems incorrect. First, the definition of $A$ is not given. Secondly, the anisotropic of GAT is on the edge weight, while that of Eq. 16 is on the message. It is not obvious.\n- Figure 2 is not described clearly. What is the meaning of the horizontal and vertical coordinates? Why the contour of MLP is as in Figure 2(c) and 2(d).\n- The evaluations are not convincing. Firstly, the ablation study and illustrative examples are not given. So, the effect of the proposed SIR-GCN is not justified. Secondly, it is not knowns whether the proposed SIR-GCN can be applied to complex models, whose performance is higher."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the representational capacity of Graph Neural Networks (GNNs) with uncountable node features and introduces an MLP-based MPNN named SIR-GCN, which generalizes to several popular GNN architectures. The authors demonstrate that for uncountable node features, it is possible to identify a soft-injective function corresponding to a specific pseudometric that quantifies dissimilarity in the node feature space. They then model this soft injective function using an MLP and design an architecture that maintains both anisotropic and isotropic properties. Experimental results highlight the model\u2019s superiority across various scenarios, including cases with countable and uncountable node features, as well as graphs exhibiting heterophily."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tIt is interesting to see how the author tackles the problem of uncountable node features using pseudo metric and soft-injective functions.\n2.\tThe designed model has certain flexibility in terms of anisotropic and isotropic properties. The model architecture also generalizes easily to some popular GNNs.\n3.\tThe experiment shows the SIR-GCN performance well on the Dictionary Lookup task and against other baseline models during the benchmarking test."
            },
            "weaknesses": {
                "value": "1.\tSince the SIR-GCN can generalize to other GNNs, it would be better if the author can explain why it is hard for other GNNs to handle the problem of uncountable node features.\n2.\tIt would be nice if the author can explain why there are some missing values in Table.\n3.\tFor the graph heterophily experiment, it would be better if the author can use some real world datasets with different degrees of heterophily[1].\n\n[1] Mao H, Chen Z, Jin W, Han H, Ma Y, Zhao T, Shah N, Tang J. Demystifying structural disparity in graph neural networks: Can one size fit all?. Advances in neural information processing systems, 2024."
            },
            "questions": {
                "value": "1.\tAccording to the description of GAT, does GAT also preserve both anisotropic and isotropic? \n2.\tFor Table 4, Can GAT or GraphSAGE achieve similar performance with more parameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a new perspective on the representational capability of GNNs by presenting a soft-injective function using a pseudometric distance. It then proposes a new message-passing scheme that performs competitively across various datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper presents an interesting approach, offering clear insights into relaxing the injective constraint in message-passing processes by defining a pseudometric distance, which captures differences within data effectively.\n\n2. The theoretical basis is solid, providing a coherent and accessible framework that helps explain the introduced ideas.\n\n3. Although the proposed method is limited by the 1-WL test, it shows strong representational capabilities. Its effectiveness is supported by experiments conducted on both synthetic and real-world datasets, confirming its practical usefulness."
            },
            "weaknesses": {
                "value": "1. The experimental section is somewhat unclear, especially regarding the number of parameters in Table 4 compared to Table 3. It is confusing why the same model appears to have twice the number of parameters. The authors emphasize \"a single layer\" in line 507 but do not mention this detail in Table 4, which could lead to potential inconsistencies. To improve clarity and fairness, the authors should add explanations of any differences in the model architecture between datasets. This would help readers better understand the experimental setup and assess the fairness of the comparisons.\n\n\n2. Although the authors claim to achieve \"a balance between computational complexity and model expressivity\" (line 820), the experiments related to computational efficiency (Tables 6,7) are not convincing enough. Adding runtime analysis on larger-scale datasets, such as ogbg-molhiv/ogbn-arxiv, would support these claims and better demonstrate the method's scalability."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}