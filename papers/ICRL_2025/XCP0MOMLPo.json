{
    "id": "XCP0MOMLPo",
    "title": "Factor Graph Optimization of Error-Correcting Codes for Belief Propagation Decoding",
    "abstract": "The design of optimal linear block codes capable of being efficiently decoded is of major concern, especially for short block lengths. \nAs near capacity-approaching codes, Low-Density Parity-Check (LDPC) codes possess several advantages over\nother families of codes, the most notable being its efficient decoding via Belief Propagation.\n While many LDPC code design methods exist, the development of efficient sparse codes that meet the constraints of modern short code lengths and accommodate new channel models remains a challenge.\nIn this work, we propose for the first time a gradient-based data-driven approach for the design of sparse codes. We develop locally optimal codes with respect to Belief Propagation decoding via the learning of the Factor graph under channel noise simulations. \nThis is performed via a novel complete graph tensor representation of the Belief Propagation algorithm, optimized over finite fields via backpropagation and coupled with an efficient line-search method. \nThe proposed approach is shown to outperform the decoding performance of existing popular codes by orders of magnitude and demonstrates the power of data-driven approaches for code design.",
    "keywords": [
        "ECC",
        "Binary Programming",
        "Belief Propagation"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "We learn new binary linear block codes by optimizing the factor graph of the Belief Propagation algorithm",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=XCP0MOMLPo",
    "pdf_link": "https://openreview.net/pdf?id=XCP0MOMLPo",
    "comments": [
        {
            "comment": {
                "value": "We sincerely thank the reviewers for their thoughtful assessment of our manuscript.\n\nWe have addressed the reviewer\u2019s feedback and performed the requested follow-up experiments. We have also integrated the reviewers' remarks and suggestions into the revised manuscript. Revisions and additions based on the reviewers\u2019 comments are highlighted in blue in the updated PDF.\n\nSpecifically:\n\n- We added BER and BLER vs. SNR curves in Appendices J and L (Reviewers GzCC,7LZc). These curves allow a better visualization of our method\u2019s impact. \n- We added a performance comparison with SCL in Appendix I (Reviewer GzCC). The comparison shows our method can provide state-of-the-art sparse codes even on extremely short block lengths.\n- We added performance on short 5G NR LDPC codes in Appendix L (Reviewers j4us,7LZc). The results demonstrate that our method can also outperform state-of-the-art short sparse codes.\n- We added the codes\u2019 column weight distribution in Appendix K (Reviewers GzCC,j4us). Our method introduces a distribution shift that adapts to BP's inductive bias in the short block length regime.\n\nWe note here that our work builds upon the original Judea Pearl\u2019s BP algorithm, published in AAAI82 as a marginalization method for Bayesian networks. Submitted to ICLR under the \u201cprobabilistic methods\u201d area, our manuscript introduces a novel method for learning the structure of Bayesian networks (graph connectivity), as outlined in lines 64-70*. Structure learning is a well-established subdomain of probabilistic graphical models. The error-correcting codes (ECC) application is chosen because it is the most prevalent application of BP.\n\nBeyond the important ECC-related questions that were the focus of the reviews, we would be happy to discuss the broader impact of our data-driven method for structure learning in probabilistic graphs, which, to the best of our knowledge, represents the first gradient-based approach in this domain.\n\n(*) Line references pertain to the original manuscript. Future updates will incorporate additional requests that will further change the line numbers, and this text already appeared in the original submission."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for the feedback and insights.\n\nOur work builds upon the original Judea Pearl\u2019s BP algorithm, published in AAAI82 as a marginalization method for Bayesian networks. Submitted to ICLR under the \u201cprobabilistic methods\u201d area, our manuscript introduces a novel method for learning the structure of Bayesian networks (graph connectivity), as outlined in lines 64-70. Structure learning is a well-established subdomain of probabilistic graphical models. The error-correcting codes (ECC) application is chosen because it is one of the most prevalent applications of BP.\n\nWe address below the remarks about ECC. However, the work is best judged as a machine learning work submitted to a machine learning conference, which includes novel technical contributions including our graph connectivity masking paradigm and a novel optimization framework.\n\n## Weaknesses\n\n> 1. Limited contribution: paper is too narrow for ICLR\u2026deserves publication\u2026more appropriate for a coding conference such as ISIT or ITW.\n\nWe respectfully disagree. As noted in our introduction, this work is the first to propose a gradient-based method for learning Bayesian networks/codes, as other reviewers also recognized.\n\n> 2. See question 2.\n\n> 3. See Question 3.\n\n## Questions\n\n1. Sections 3 and 4 represent half of the paper. Can the reviewer point to specific complex presentations?\n\n2. Our work introduces a new learning paradigm aimed at improving the performance of a given initialization of the factor graph/code. As with any first-order method applied to non-convex objectives, the method is sensitive to initialization and generally converges to local minima. Thus, the primary focus of this work is to present an efficient and empirically effective method, rather than exhaustively testing all sparse code variants.\n\nIn response to the reviewer\u2019s recommendation, we conducted experiments with state-of-the-art (SOTA) 5G NR LDPC codes. The BER and BLER vs. SNR results, presented in Appendix L of the revised manuscript, demonstrate that our method can enhance these SOTA codes, highlighting the potential of our data-driven approach in modern ECC settings.\n\nA comparison with SCL is now also included in Appendix I of the revised manuscript. Our results show that even in the challenging short-length setting, where achieving sparsity is difficult, our method significantly improves over existing low-density codes and approaches the ML bound with very few iterations, even under suboptimal initialization. With good initialization (using a sparse LDPC code), our method achieves state-of-the-art performance.\n\n\n\n3. The chosen metric and display method follow established norms in the field of ECC published in the AI community. This metric emphasizes the exponent, is easily inverted ($BER = e^{-BER_{-ln}}$), and enables compact presentation across multiple codes and scenarios (18 codes on different channels). \n\nBased on the reviewer\u2019s suggestions, we have added BER and BLER vs. SNR curves in Appendix J of the revised manuscript. Additionally, Appendix L provides similar curves for short state-of-the-art 5G NR LDPC codes, demonstrating that our method outperforms these codes as well.\n\n4. Our optimization framework primarily optimizes BER rather than BLER using cross-entropy loss, but this does not seem to significantly impact performance due to BP\u2019s global marginalization effects. \nThe figures are part of our *ablation* study, illustrating various impacts and effects of and on our method.\n\n- Figure 3 shows that initialization plays a critical role in convergence. Interestingly, as shown, increased initial sparsity does not necessarily improve performance (e.g.,  at $p = 0.1$).\n- Figure 4: Although loopy belief propagation is widely known beyond coding theory, empirical validation is needed to show that stochastic gradient descent (SGD) enforces sparsity through its inductive bias [1]. \nFigure 6 further supports that sparsity regularization has minimal impact on convergence, suggesting that gradient descent optimization and BP have similar sparsity enforcement effects.\n\nThese points are clarified in the revised manuscript.\n\n*[1] Soudry et al., *The implicit bias of gradient descent on separable data*, JMLR18*"
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for the valuable feedback and constructive remarks.\n\n> Missing performance on 5G LDPC codes.\n\nOur work introduces a new learning paradigm aimed at improving the performance of a given initialization of the factor graph/code. As with any first-order method applied to non-convex objectives, the method is sensitive to initialization and generally converges to local minima. Thus, the primary focus of this work is to present an efficient and empirically effective method, rather than exhaustively testing all sparse code variants.\n\nIn response to the reviewer\u2019s recommendation, we conducted experiments with state-of-the-art (SOTA) 5G NR LDPC codes. The results, presented in Appendix L of the revised manuscript, demonstrate that our method can enhance these SOTA codes, highlighting the potential of our data-driven approach in modern ECC settings.\n\n> Comparison with Elkelesh et al.\n\nWe are unsure what is the concern. \n\nWe implemented the Genetic Algorithm (GA) method from [Elkelesh et al., 2019] (originally in MATLAB) and conducted a direct comparison under the same conditions as our method, training both methods on 15 iterations and testing on 5 and 15 iterations. \n\nThe $k$ iterations mentioned refer to the number of GA *training* iterations. As a genetic algorithm, this approach is slower and limited in its beam-search capacity. \n\nWe also note in line 900 that combining our method with GA could help avoid poor local minima (similar to the perturbed gradients in Appendix A), especially with good initialization.\n\n> The proposed method appears to be a local optimization method highly dependent on the initial $H$ matrix\n\nThe sensitivity to initialization stems from using first-order oracles for the optimization of non-convex objectives (see lines 20, 297, 422, 426, 803, and 901). This limitation is inherent to all first-order methods in non-convex optimization, with neural network optimization being the most ubiquitous illustration.\n\nExperimental results show that *local* convergence can yield substantial improvements, even with SOTA sparse codes (Appendix L).\n\n> Questions\n\n1.Figure 2 shows the performance improvement in dB across various *sparse* codes, providing a compact summary (14 codes tested) of the conventional BER vs. SNR plots. In response to the reviewer\u2019s suggestion, we added BER and BLER vs. SNR plots for various codes and channel configurations in Appendices J and L of the revised manuscript.\n\n2.The definition is provided in line 327.  \n\n4.This experiment is part of the ablation study (Section 6) and demonstrates the method\u2019s ability to integrate constraints/regularizations and their potential beneficial impact. The optimization framework can accommodate any structural constraint or desired code property.\n\n5.LDPC codes, known for asymptotic optimality and efficient decoding, are relevant in modern communication, where short codes are increasingly important ([Liva et al., 2016], [1], lines 82-85). Supporting efficient short LDPC codes would enable a single decoder (BP) to be used on a device instead of deploying multiple decoders, as required in modern standards like 5G (short Polar codes and longer LDPC codes [ESTI, 2021]).\n\n A comparison with SCL is now included in Appendix I of the revised manuscript. Our results show that even in the challenging short-length setting, where achieving sparsity is difficult, our method significantly improves over existing low-density codes and approaches the ML bound with very few iterations, even under suboptimal initialization.\nWith good initialization (using a sparse LDPC code), our method achieves state-of-the-art performance.\n\n7.Sparsity statistics are provided in Figure 5. The optimization does not affect the code\u2019s girth (line 458), and calculating the minimum distance remains infeasible for these codes.\n\nPer other reviewers\u2019 suggestions, we now include the column weight distribution in Appendix K. We welcome further feedback on important analyses to include.\n\n*[1] Co\u015fkun, Mustafa Cemil, et al. \"Efficient error-correcting codes in the short blocklength regime.\" *Physical Communication*, 34 (2019): 66-79.*"
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for their valuable feedback and insights.\n\n>Motivations for Short Codes\n\nThe primary motivation for short codes is practical, as outlined in lines 82-85. Alongside the survey by [Liva et al., 2016], further motivations are detailed in [1]. We summarize here the main reasons:\n\n1. Larger blocklength codes approaching the capacity can be only marginally improved.\n2. Short and medium blocklength codes are increasingly important in emerging applications, including IoT, smart metering networks, remote command links, and messaging services.\n3. From a systemic perspective, larger computational resources are required to explore the optimization of larger codes.\n\nFurther explanations on these motivations are provided in the revised manuscript.\n\n*[1] Co\u015fkun, Mustafa Cemil, et al. \"Efficient error-correcting codes in the short blocklength regime.\" Physical Communication 34 (2019): 66-79.*\n\n>BER vs. SNR Curves\n\nThe chosen metric and display method follow established norms in the field of ECC published in the AI community. This metric emphasizes the exponent, is easily inverted ($BER = e^{-BER_{ln}}$), and enables compact presentation across multiple codes and scenarios (18 codes on different channels). \n\nBased on the reviewer\u2019s suggestions, we have added BER and BLER vs. SNR curves in Appendix J of the revised manuscript. Additionally, Appendix L provides similar curves for short state-of-the-art 5G NR LDPC codes, demonstrating that our method outperforms these codes as well.\n\n> SCL Decoding Performance\n\nA comparison with SCL is now included in Appendix I of the revised manuscript. Our results show that even in the challenging short-length setting, where achieving sparsity is difficult, our method significantly improves over existing low-density codes and approaches the ML bound with very few iterations, even under suboptimal initialization.\n\nWith good initialization (using a sparse LDPC code), our method achieves state-of-the-art performance.\n\n>PCM Analysis\n\nSparsity statistics are provided in Figure 5. The optimization does not affect the code\u2019s girth (line 458), and calculating the minimum distance remains infeasible for these codes.\n\nPer the reviewer\u2019s suggestion, we now include the column weight distribution in Appendix K. We welcome further feedback on important analyses to include.\n\n> [Minor] Generator Matrix\n\nOur method preserves the symmetry property of BP, making it independent of the generator matrix. However, the initial formulation supports alternative optimization schemes that could break this symmetry and lead to non-invariance with respect to the generator matrix (e.g., by optimizing Trellis graph connectivity). This clarification is added to the revised manuscript."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for the positive feedback and insightful comments.\n\n> Computational Complexity\n\nRegarding the gradient computation, we indeed require gradients of size $n \\times (n - k)$ to perform the first-order optimization. Our line search reduces the infinite search space to $n \\times (n - k)$. However, due to the local proximity of the optimum to the working point, we limited the search to 50 steps in our experiments (see line 322 and Appendix C).\n\nThis complexity is minimal and manageable compared to training modern neural networks with up to trillions of parameters (as suggested by recent work in the field of ECC). Furthermore, our method is the first gradient-based optimization approach for designing codes and factor graphs, contrasting with prior heuristic or search-based methods. It requires an average of one hour of training for the tested codes (line 323) and is up to 140 times faster than existing search-based methods, such as the genetic algorithm in [Elkelesh et al., 2019] (line 409). Notably, all computational cost is incurred only during training, with no added cost to BP\u2019s inference algorithm.\n\n> Impact of Initialization\n\nThe impact of initialization in our method stems from using first-order oracles to optimize non-convex objectives (cf. lines 20, 297). This challenge is common in first-order methods, including neural network optimization. We tested multiple initializations (sparse, dense, random codes) and observed that sparse initializations yielded the best results, positioning the algorithm near better local minima. However, for larger codes, the method can struggle to improve due to this pre-existing local minimum, as it may require escaping a potentially suboptimal extremum (line 425). This point is now emphasized in the paper\u2019s conclusions.\n\n> L1 Regularization\n\nIf we misinterpreted the reviewer\u2019s remark, please clarify. Given that $H_{ij}=bin(\\Omega)_{ij}\\in \\{0,1\\}$, the effects of L1 and (squared) L2 norms are similar. However, applying different regularization factors to weights and columns could yield unique impacts.\n\nWe note here that in Figure 4, we provide empirical evidence that SGD enforces sparsity through its inductive bias [1]. Figure 6 further suggests that sparsity regularization has minimal impact on convergence, implying that gradient descent and BP exert similar effects in promoting sparsity. Additionally, any constraint can be incorporated into the optimization problem.\n\n>  Weighting of Iterations\n\nWe appreciate the reviewer\u2019s recommendation regarding iteration weighting. Weighting the objective as suggested by Nachmani et al., (2016) using a multi-loss objective across iterations may indeed improve performance. We now added this reference. We note here that due to computational constraints, we optimized our model with only five iterations (line 320).\n\n*[1] Soudry et al., *The implicit bias of gradient descent on separable data*, JMLR18*"
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for the feedback and remarks.\n\n> 1.i. Novelty wrt [Choukroun & Wolf, 2024a]\n\nThe objectives in both papers describe the same standard channel coding pipeline.\nWhile [Choukroun & Wolf, 2024a] describes the end-to-end optimization of differentiable *neural* decoders (i.e., code and decoder), our method optimizes the code for the classical Belief Propagation decoder. Besides the regularization term, it can be observed that the expectation, the objective, and the optimization variables are different.\n\n\nOur setting is fundamentally different (lines 201-207) since unlike [Choukroun & Wolf, 2024a] our decoder cannot be differentiably optimized as it requires the structure learning of the underlying graph/algorithm.\n\n> 1.ii. Novelty wrt neural BP\n\nNeural/[weighted BP]( https://nvlabs.github.io/sionna/api/fec.ldpc.html#ldpc-decoder) should never be implemented in a tensor form since it would lose all of the efficiency advantages of message-passing algorithms (sparsity).\n\n\nWe adopt the tensor form in order to learn the connectivity itself upon a *complete factor graph*. We are not learning the weighting of the variable nodes\u2019 messages obtained upon a *given fixed* graph connectivity as in neural BP (line 48).\n\nThis is a very fundamental difference with neural BP.\n\n> 1. mathematical or analytical contributions\n\nFrom the ML perspective, the method develops new tools of high interest, such as a new way to optimize graphs for BP (not just for codes, i.e., structure learning) and a novel line search procedure tailored for quantized optimization. The information theoretical aspects of codes are not the main emphasis of our work. As a gradient descent method, the proposed framework's performance is tied to the initial graph, and this is emphasized repeatedly in the paper (cf lines 20, 297,422,426,803,901). Further analysis of the optimization over the cross-entropy loss can be done via classical convergence analysis of quantized networks [2], but this is beyond the scope of our work.\n\n> 2. \u201cComparison with line-search methods\u201d \u2026 \u201cLS methods are used for non-convex optimizations wo assumptions\u2026 \n\nExisting methods:\n\n*Inexact* line search methods can be used for non-convex optimizations to find *local* minima.\nThe existing zero/first-order line-search methods operate on a predefined *continuous* range (generally small) on which the search is performed. \n\nIn our binary setting, a continuous range is impossible to set as it can grow arbitrarily large according to the $\\Omega$ values. Moreover, a standard line search would find local minima of the line-search objective because of the suboptimal range and the discretization approximation.\n\nOur method:\n\nAs a binarization-aware method, we provide a *provably* (line 304) optimal line-search method that requires $\\mathcal{O}((n-k)\\times n)$ parallelizable computations.\n\n> 2. Effectiveness of the grid-search approach \u2026 depends on the initial matrix\n\nThe impact of initialization is related to the use of first-order oracles for the optimization of non-convex objectives (cf lines 20, 297,422,426,803,901). \n\nThis is, of course, not unique to our work: any first-order method on non-convex objectives would be impacted by initialization, learning neural networks being the most ubiquitous example.\n\n> 3. How much is the method fast compared with what\n\nThe method is the first gradient-based optimization for the design of codes/factor graphs. All other existing methods are heuristics or search-based methods.\n\nOur method requires a single hour of training on average on the tested codes (line 323) and is up to 140 times faster (and better) than the existing search-based method (a genetic algorithm) [Elkelesh et al., 2019] (line 409)).\n\n> Suggestions\n\nThe suggestions and typos will be added to the revision.\n\n3.The expectation should be with respect to the discrete number of iterations of the BP algorithm in order to ensure optimal decoding under any computational constraints (line 199). This can be similar in a sense to the multi-loss of neural BP. A citation has been added.\n\n4.No assumption on the generator matrix is given in the paper. As the method is built to maintain the symmetry property of BP, the optimization framework is not dependent on the generator. \n\n7.We may be missing your point but this is the discretization of the expectation above (suggestion 3). We added a reference to [Nachmani et al. 2016] for reference to the multi-loss.\n\n10.\u201cCost function for the optimal learning rate\u201d indeed.  For example, since the BER metric is not differentiable, one may search for optimal BER during the line search for optimal performance on the loss of interest (BER), instead of the Bayesian (uncertainty) cross-entropy metric.\n\n11.A citation will be added as this terminology already exists, e.g., [1]. \n\n*[1] Kurmukova and Deniz Gunduz. Friendly Attacks to Improve Channel Coding Reliability. AAAI24.*\n\n *[2]  Li et al. Training quantized nets: A deeper understanding. Neurips 2017.*"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an optimization scheme of a Tanner graph for low-density parity-check (LDPC) codes. Particularly, the scheme aims to improve decoding performance by a belief propagation (BP) decoder. Since the problem is a non-convex binary optimization problem whose cost function is implicitly defined, a gradient-based approach is applied by relaxing variables and using a straight-through estimator. In addition, a grid search is proposed to find an optimal learning rate. From numerical results, the proposed method outperforms the decoding performance of existing codes."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* The proposed method is the first gradient-based optimization of a parity-check matrix for error-correcting codes.\n* In addition, a grid-search approach for finding the learning rate is proposed, which is an alternative to the line-search approach.\n* Numerical results show that optimized codes by the proposed method can be decoded more accurately than conventional codes."
            },
            "weaknesses": {
                "value": "The reviewer understands the motivation of the work and the numerical effectiveness of the proposed method. However, the paper contains  some flaws as follows: \n\n1. **Technical contributions of the proposed method** \n\nThe contributions that the authors claim are threefold: (i) formulation of the problem, (ii) reformulation of BP in tensor fashion, and (iii) differentiable and fast optimization method for the problem. Here, the reviewer wants to evaluate each contribution in detail. \n\n* (i) Formulation of the problem: In the paper, Eq. (4) (or (8)) is the formulation of the problem, whose novelty is claimed by the authors. However, a similar formulation (mainly focusing on \"expectation w.r.t. random codewords and noise\") has already been proposed in [Choukroun & Wolf, 2024a] cited in the paper. Eq. (4) is different from [Choukroun & Wolf, 2024a] in that Eq. (4) includes the regularization terms. However, the authors do not compare the differences explicitly and claim that the whole formulation in this paper is novel. In addition, the effect of regularization needs to be included in numerical experiments. The reviewer suggests that the author carefully clarify the novelty of the formulation.\n\n* (ii) Reformulation of BP in Tensor fashion: The reviewer does not consider this part technically novel because a similar \"tensorization\" technique has been used for the implementation of neural BP decoders. \n\n* (iii) Differentiable and fast optimization method for the problem: The reviewer agrees with this point. However, there are some flaws, as shown below.\n\nOverall, the technical contributions that the authors claim seem partly insufficient. In addition, the paper does not contain any mathematical or analytical contributions, which is obviously a weak point. \n\n2. **Contributions of the grid-search approach**\n\nRelated to the above point, the authors claim the novelty of the grid-search approach for learning rate. However, the reviewer wonders about the novelty due to the following reasons.\n\n* No comparison with line-search methods: the authors claim that the conventional line-search methods assume the convexity of a cost function, and they are unsuitable for non-convex problems like Tanner graph optimization in this paper. However, the assumption is required to show the optimality of line-search methods. Practically, line-search methods are used for non-convex optimizations without assumptions. Therefore, numerical comparison in terms of optimization performance and/or execution time should be necessary to show the effectiveness of the grid-search method. \n\n* No theoretical guarantee: In contrast to the above discussion, there is no guarantee of the grid-search method for finding the optimal learning rate. It is a weakness of the proposed method. \n\n* Effectiveness of the grid-search approach: It is reported in Sec. 6 that an optimized matrix depends on the initial matrix, suggesting that the proposed method finds a suboptimal solution, not an optimal one. This fact may weaken the effectiveness of the method. At least, a comparison with other learning-rate optimizations will be required. \n\n3. **Missing advantage of the gradient-based optimization**\n\nThe authors claim that the proposed gradient-based optimization is fast but no evidence is provided in the paper. How much is the method fast compared with what?"
            },
            "questions": {
                "value": "Questions are included in the \"Weakness\" section. \n\nSuggestions: \n1. Around Line 131: Please clarify whether vectors are column or row vectors. Anyway, the dimensions of the multiplication in $H(mG)$ are incorrect. \n2. Line 157: $k=1$ should be $k=0$ because the first iterations is $2k+1=1$.\n3. Eq. (4): Under $\\mathbb E$, what does the expectations w.r.t. $T$ mean? Is $T$ a random variable, not a constant number? \n4. Line 197: A generator matrix is not unique in general. Is the form of the matrix fixed in this paper?\n5. Line 204 etc.: $c=Gm$ should be $c=mG$ accourding to Line 131.\n6. Eq. (6): \"(\" should be removed.\n7. Eq. (8): The cumulative loss function w.r.t. iteration $t$ in (8) has been proposed in previous studies on neural BP decoders. Please add some references.\n8. Line 291: What is the meaning of \"sufficient statistics\"? It is unclear whether the gradient is sufficiently statistical in terms of statistics. It is recommended to rephrase the term correctly. \n9. Eq. (9): What does index $i$ stand for? Since $\\Omega$ represents a matrix, indices are given like $ i,j$.\n10. Line 310: What is the meaning of \"line-search objectives\"? Is it the cost function for the optimal learning rate?\n11. Line 373: The i.i.d. Gaussian mixture model is called bursty noise in this paper. It seems incorrect because bursty noise generally implies time-dependent noise. \n12. Line 457: \"sparse code\" should be \"a sparser code\" or \"sparser codes.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper deals with factor graph (or parity-check matrix, PCM) optimization of error-correcting codes for Belief Propagation (BP) decoding. Actually, this optimization is difficult as it is an integer optimization problem: the elements of the PCM are either 0 or 1 and even small changes (e.g. the replacement of one element can drastically worsen the performance). The authors propose a gradient-based data-driven approach via a novel complete graph tensor representation of the Belief Propagation algorithm. The demonstrate the efficiency of the resulting codes by simulations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The main strong point are as follows:\n- the first gradient-based data-driven approach via a novel complete graph tensor representation of the Belief Propagation algorithm.\n- automating the process of finding a good code.\n\nThese point are indeed important as usually in practice people construct LDPC codes via heuristics, or some optimization algorithms like genetic algorithm or simulated annealing, which takes a lot of time and computational resources."
            },
            "weaknesses": {
                "value": "I list the main weaknesses below:\n- Given that LDPC codes are known to perform poorly at short lengths due to short cycles in the Tanner graph ( the length of the minimal cycle (the girth) of the Tanner graph is O(log n) ), could the authors elaborate on their motivation for focusing on LDPC optimization for short codes? Are there specific applications or theoretical insights they hope to gain from this approach?\n- Could the authors provide BER vs SNR (or Eb/N0) curves for their results, in addition to the current tabular format? This would allow for easier comparison with existing literature and help identify potential error floors.\n- To better understand the performance of your constructed codes, could you include a comparison with polar codes under SCL decoding (L=8) for short lengths? Such results can be found e.g. here\u2028https://rptu.de/channel-codes/channel-codes-database.\n- To provide theoretical context for your results, could you compare them to the finite length achievability and converse bounds from Polyanskiy et al. (2010)? This would help situate your method's performance relative to fundamental limits.\n- Could you provide an analysis of the final Tanner graphs produced by your method, including metrics such as column weight distribution, minimum distance, and trapping sets? This would offer insights into the structural properties of the optimized codes.\n- [Minor] You write about the generator matrix and the problems related to obtaining G from H. But for the BP decoder you can perform training on zero codeword only (as you mention later). I suggest to shift to zero codeword from the very beginning."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this study, the authors propose a method to optimize the parity check matrix (PCM) $H$ using deep learning techniques. They modified the BP decoding equations that rely on $H$ so that $H$ could become trainable. To accelerate the training process, they utilized a line search method. The proposed code optimization method has the advantage of being applicable to codes with constraints or in arbitrary channels."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "In the field of model-based neural decoder research, previous work has focused on optimizing decoder weights with a given $H$. However, this study differentiates itself by optimizing $H$ itself, which is both novel and distinctive. The derivation of trainable BP decoding equations for $H$ is particularly commendable. The paper presents a significant amount of experimental results, demonstrating performance gains through $H$ optimization for various code types."
            },
            "weaknesses": {
                "value": "In Table 1, the optimization is performed for a wide range of initial codes, but for high-density codes like BCH, Polar, and LTE Turbo, which have their own optimized decoders instead of BP decoders, the improvements in BP performance seem less meaningful. It seems more appropriate to compare LDPC codes, but there are concerns about the \"representativeness of the LDPC codes\" used in the comparison. Both the MAKCAY and CCSDS codes are quite outdated (and indeed seem to show limited performance improvement). Furthermore, the PEG construction used for the LDPC PEGX code appears to be an early-stage method. In addition, The CCSDS code is known to ensure efficient encoding, but as shown in Figure 7, modifying $H$ without constraints (as it seems to have been done in Table 1) may no longer guarantee this property. \n\nTherefore, it would be better to evaluate whether the performance can be improved for more representative LDPC codes (e.g., short-length ARJA code class or 5G LDPC codes) while maintaining the functionalities of these code types (efficient encoding, QC structure, and rate compatibility).\n\nAdditionally, a comparison with Elkelesh et al. (2019), another data-driven code construction method, does not seem fair. In Table 4, the authors compared their optimized $H$ matrix for iterations of 5 and 15, whereas the Elkelesh method was optimized at iterations of 75 and 150. Since the Elkelesh method is also a data-driven construction approach, it seems feasible to optimize it with a smaller iteration count, such as 15. A comparison under such conditions would be necessary.\n\nThe proposed method appears to be a local optimization method highly dependent on the initial $H$ matrix. As shown in Figure 7, only a small number of edges were changed in sparse cases. As the goal of code construction is to create a globally optimized $H$ for a given code length, the proposed method does not seem to align with this objective."
            },
            "questions": {
                "value": "Additionally, I have a few questions:\n\n1.\tThe purpose of showing Figure 2 is unclear. I can observe that the variation increases significantly at high SNRs; a discussion on this would be helpful.\n\n2.\tWhat exactly is the meaning of PEG X? In Table 1, there appear to be significant performance differences between PEG2, PEG5, and PEG10 (with PEG5 showing particularly superior performance). It would be helpful to clarify this.\n\n3.\tIn line 251, it is stated that the method can be applied regardless of modulation. If there are experimental results for modulations other than BPSK, it would be useful to include them.\n\n4.\tIn line 428, an experiment was proposed to optimize only $P$ while keeping III fixed in the systematic form of $H=[I P]$. However, most LDPC standards use a dual diagonal form rather than the systematic form $[I P]$. Therefore, an optimization experiment assuming the dual diagonal form would be more appropriate.\n\n5.\tOverall, the code lengths are very short, with a maximum length of 128. In this range, Polar codes are known to be superior, so the practicality of LDPC code optimization seems somewhat limited.\n\n6.\tFurther research on additional properties of the modified code, such as minimum distance or cycles, is necessary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel gradient-based data-drive approach for constructing low-density parity-check codes. The belief-propagation algorithms on sparse graph codes are reformulated into a differentiable matrix representation. The paper further relaxes the 0-1 constraints on the parity-check matrix bits into learnable continuous weights. Combined with a STE (Straight-Through Estimator) method, paper reformulate the optimization problem into a differentiable end-to-end optimization problem. The paper also proposes binary line-search method for solving the optimization problem."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper contains a novel idea of constructing low-density parity-check codes using a end-to-end differentiable data-driven approach. The idea of using STE (an approach usually used in deep learning quantization and sparsity design) for converting the original NP-complete discrete optimization into a differentiable optimization problem is novel.\n* The approach in this paper open-up a new research direction of using end-to-end differentiable optimization for constructing low-density parity-check codes.\n* The proposed binary line search method is novel.\n* The experimental results in the paper show that the differentiable end-to-end approach results in improved coding performance\n* The paper is well-written and clear."
            },
            "weaknesses": {
                "value": "* It seems that the computational complexities of the algorithm would be quadratic with respect to the code length. For example, every element of the parity-check matrix is a learnable continuous variable. Also, in the binary line search, during each optimization step, the relevant grid samples can be as large as n(n\u2212k), which is quadratic with respect to the code length n.\n* In the experimental results, the optimizations are initialized from a known sparse codes. There is a question that whether we can start the optimization from a randomly initialized code design and still ends up with a code with outperforming decoding performance.\n* It seems that the sparsity regularization term is a L1 regularization. The paper lacks a in-depth discussion on other possible sparsity regularization and whether L1 regularization is optimal.\n* From equation 8, it seems that the losses are equally weights at different decoding iteration steps. Because, the losses should be large at the first several iteration steps, it is my opinion that if we train the model using such equally weighted losses, we are mainly optimize the codes for its performance at the first several decoding iteration."
            },
            "questions": {
                "value": "* For the computational complexity issues, could authors provide more discussions on how to lower the computational costs or future directions on lowering the computational costs?\n* The paper shows that initializing from known sparse codes would already result in performance improvements. However, initializing from these known codes would probably result in codes that are very close to the existing codes. It would be interesting to see whether other types of initialization would give us more diverse or new codes that are totally different from all the hand-crafted codes and outperform the known codes. I think initializing the optimization from the all zero parity-check matrix would impose a very challenging optimization problem, because of the symmetry breaking issues. Could the authors try to initialize the optimization by random picking a code from the code ensemble satisfying a particular pair of degree distributions? Or the authors could provide a discussion on why initialization from known codes makes more sense.\n* For the sparsity regularization, the authors provide very little discussions in the draft. In my opinion, the sparsity regularization may need more careful considerations. For example, what we may want to achieve is a sparse parity-check matrix satisfying a certain pair of degree distributions (or a regular LDPC code). Thus, each row(column) should be sparse with a designed number of non-zero elements. The authors could argue that a simple L1 regularization would already guarantee that the optimization would end-up with a regular code or a code satisfying a designed degree distribution, of course by providing more experimental results. Could the author try to use a combined L1 and L2 regularization, where L1 is applied to each row and each column, and L2 is applied to row weights and column weights?   \n* For the loss weighting, I think the authors could try equal weighting at the beginning epochs and weight the losses at large decoding iterations more at later epochs. Or, the author could provide more discussions on why equally weighting is desired in certain senses.\n\nIn a summary, it is suggested that the authors could\n* provide more discussions on how to lower the computational costs either in this paper or as a future direction\n* provide more experiments on different optimization initialization or a discussion on why the current way of initialization makes more sense\n* provide more experiments on using a combined L1, L2 sparsity regularization or a discussion on whether the L1 regularization already results in desired sparse parity-check matrices\n* provide more experiments on using adaptive loss weighting or a discussion on justifying the current equal weighting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a gradient-based, data-driven method for the design of sparse-graph codes tailored to belief propagation (BP) decoding. The main contribution of the paper is to learn the factor graph structure through a differentiable representation that facilitates backpropagation. Specifically, the authors start with a complete bipartite graph where the edges are learnable."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed approach to design codes is interesting and, as the authors show, leads to codes that outperform some existing codes."
            },
            "weaknesses": {
                "value": "Despite that the method proposed by the authors is interesting, I believe this paper should be rejected, primarily for reasons concerning its limited contribution scope and weak experimental comparisons. My argumentation is detailed below:\n\n1. Limited contribution: While the approach presented in the paper is conceptually interesting, the contribution of this paper is too narrow to merit  publication in a major conference like ICLR. The work is more appropriate for  a coding conference such as ISIT or ITW. I believe that this work is valuable for the coding community and deserves publication, but I lacks depth and broader impact typically expected at  a  major machine learning venue or for a major coding journal.\n\n2. Unconvincing experiments and comparisons: The experimental results are insufficient to answer the core question this paper seeks (or should seek) to address: Does this method allow to design  codes that perform better than state-of-the-art codes? This question remains unanswered. This casts doubt on the interest of the proposed approach. The authors should compare the performance of the designed codes with that of the best available codes, as well as with relevant performance bounds. Without these comparisons, the results merely demonstrate the construction of improved codes over baselines, which still underperform relative to the best codes. This weakens the impact of the method. \n\nPlease, see my Comment 2 in Section \"questions\" for some suggested comparisons.\n\n3. Non-standard metrics: The paper reports performance using the negative natural logarithm of the BER. This is very unconventional for a coding paper (and this IS a coding paper) and complicates unnecessarily the interpretation of the results.  Standard practice in coding theory involves presenting BER or block error rate (BLER) results directly (the latter is more suitable!), which allows for clearer, more interpretable results. The authors should adhere to these standard metrics. \n\nPlease, see my Comment 3 in Section \"questions\"."
            },
            "questions": {
                "value": "1. Sections 3 and 4 can be presented in a much clearer and accessible manner. Currently, these sections make relatively straightforward concepts appear unnecessarily complex.\n\n2. The experiments/results section should be thoroughly reworked. It is not unexpected that the authors' obtained  codes outperform the baseline codes they started with. Indeed, most of them are not particularly good codes (compared to the best-existing ones). Some of them, indeed are clearly poor codes for BP decoding. The authors should benchmark the performance of their newly-designed codes against SOTA codes.\n\nFor example, the authors should consider benchmarking their (128,64) codes with the SOTA codes reported in Figure 10 of the paper ``Efficient error-correcting codes in the short blocklength regime'' (the details of the codes are given in the paper). Without such a comparison and similar ones for other lengths, the relevance of the proposed approach is questionable.\n\nFurthermore, the  authors should also benchmark the performance of their codes  against finite-length performance bounds, such as the Gallager's random coding bound, the random coding union bound (see same paper for details). In this sense, please report results in terms of block error rate, rather than bit error rate, as it is more relevant.\n\n3. Rather than reporting results in terms of the negative natural logarithm of the BER, please plot BLER curves (as the ones in the paper cited above). Only in this way one is can fully understand how the proposed codes perform against SOTA codes and performance bounds. In other words, for the (128,64) codes you should reproduce Figure 10 in the paper above and include your own curves.\n\n4. I do not see much value in the results in Figures 3, 4, 5, and 7. The results are unsurprising and provide limited insight. These results would be better suited for the appendix rather than the main body of the paper.\n\nFor example, in Figure 3 you show improvements with respect to random codes with different sparsity rates. For such lengths, random codes, particularly with higher density, are not good codes under BP decoding, so it is not surprising the your learned code works better! What do we learn from this figure that we don't know yet?\n\nThe same applies to Figure 7. I quote the paper: \"We can observe that for low-density codes the modifications remain small,\nsince the code is already near local optimum, while for denser codes the change can be substantial\". This is what any relatively knowledgeable coding expert would expect, so the figures do not bring any new insight."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}