{
    "id": "Dolm7rrrQd",
    "title": "Gone With the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images",
    "abstract": "Neural compression methods are gaining popularity due to their impressive rate-distortion performance and their ability to compress data to extremely small bitrates, below 0.1 bits per pixel (bpp). As deep learning architectures, these models are prone to bias during the training process, potentially leading to unfair outcomes for individuals in different groups. In this paper, we present a general, structured, scalable framework for evaluating bias in neural image compression models. Using this framework, we investigate racial bias in neural compression algorithms by analyzing 7 popular models and their variants. Through this investigation we first demonstrate that traditional distortion metrics are ineffective in capturing bias in neural compression models. Next, we highlight that racial bias is present in all neural compression models and can be captured by examining facial phenotype degradation in image reconstructions. Additionally, we reveal a task-dependent correlation between bias and model architecture. We then examine the relationship between bias and realism in the image reconstructions and demonstrate a trade-off across models. Finally, we show that utilizing a racially balanced training set can reduce bias but is not a sufficient bias mitigation strategy.",
    "keywords": [
        "Fairness",
        "Bias",
        "Neural Compression",
        "Phenotype Classification"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We present a framework to evaluate racial bias in neural image compression models, showing that examining facial phenotype degradation reveals racial bias, and that a racially balanced training set reduces but doesn't fully eliminate this bias.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Dolm7rrrQd",
    "pdf_link": "https://openreview.net/pdf?id=Dolm7rrrQd",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a framework for assessing bias in neural image compression models, analyzing seven popular models and finding prevalent racial bias, manifested as unequal degradation of facial features. The study indicates that while using a racially balanced dataset helps mitigate bias, it is not a complete solution. The study indicates that while using a racially balanced dataset helps mitigate bias, it is not a complete solution."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Strength: (1)The topic of this paper is novel, and the racial bias of image compression on face data sets is studied (2)The authors have conducted quite sufficient experiments around this argument to verify that this problem does exist"
            },
            "weaknesses": {
                "value": "Weakness: (1)Although the author presents a novel topic, it seems that the author did not fully explore the way to solve the problem. Using a more balanced dataset seems to be one solution, but after discussion by the authors, this approach does not completely eliminate racial bias. So, how to better solve this problem? The author needs to give further elaboration. In fact, this is the point I am most concerned about. (2)The authors used traditional metrics such as PSNR and SSIM in their experiments to reflect racial bias. However, these metrics differ significantly from human visual experience. I wonder if the authors explored more perceptual metrics, such as LPIPS or FID?"
            },
            "questions": {
                "value": "Weakness: (1)Although the author presents a novel topic, it seems that the author did not fully explore the way to solve the problem. Using a more balanced dataset seems to be one solution, but after discussion by the authors, this approach does not completely eliminate racial bias. So, how to better solve this problem? The author needs to give further elaboration. In fact, this is the point I am most concerned about. (2)The authors used traditional metrics such as PSNR and SSIM in their experiments to reflect racial bias. However, these metrics differ significantly from human visual experience. I wonder if the authors explored more perceptual metrics, such as LPIPS or FID?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigated the racial bias problems existing in learned image compression. The authors built a framework to systematically examine the extent to which racial bias occurs in compression. Based on the evaluating framework, they proposed a classification-accuracy-based loss function to better reveal the bias. The correlation between bias, model architecture and image realism has been measured. They also show that utilizing a racially balanced training set cannot fix the problem."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe paper has a clear problem definition, constructed a reasonable evaluation framework.\n2.\tExisting experiments have proved the existence of the problem from multiple angles to a certain extent"
            },
            "weaknesses": {
                "value": "1.    The paper seemly shows few contributions to the compression community. Since the paper just proposes the racial bias problem existing in learned image compression but provides no solution from the compression perspective. Similar evaluation schemes seem to be applicable to any field. Have you considered proposing compression-specific bias mitigation techniques?\n2.    It seems that the bias problem is mainly attributed to the dataset and optimizing method. But the authors only focus on the data-related reasons and do not explore the impact of model optimization methods on this issue. It seems unconvincing to simply attribute the difference in model bias to the difference in model architecture. Have you considered analyzing how different loss functions or training regimes impact bias in compression models?\n3.    The authors did not provide bias analysis results for images decoded by traditional codecs like JPEG, HM and VTM. The optimization of traditional codecs is not affected by the distribution of the dataset and should not lead to bias. If this experiment can be provided, it will promote our understanding of this problem. Please estimate traditional codec results at equivalent bitrates using the same bias evaluation framework.\n4.    The author used the accuracy of the classification model to evaluate the loss of image attributes at low bitrates. However, the classification model was learned on undistorted images, and whether it can accurately classify features on distorted images is unverified. Additional experiments should be conducted in this regard to enhance the persuasiveness of the bias-related conclusion. For example, you could compare classifier\u2019s results with human evaluations on a subset of distorted images and report the accuracy."
            },
            "questions": {
                "value": "Please refer to above weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents an analysis of racial bias in different neural compression algorithms. To measure this, the method uses face/phenotype classifiers and measure how much the classification decision is affected by the neural compression algorithm. This is similar to a rate-distortion measure where distortion is classification error instead of pixel error. The paper shows a clear bias in neural compression algorithms when trained on imbalanced datasets which is only partially mitigated by using balanced data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The work here is interesting and timely. As neural compression continues to approach a usable state, but has yet to be deployed in any meaningful way, it is extremely important to start considering any potential bias in trained models or inherent to the algorithms. This way, the community can develop mitigations and ensure that those mitigations are used in future products. Additionally, I think the metrics used by the method to quantify bias are sensible and the finding that traditional distortion metrics, such as PSNR, do not accurately capture bias is a good result. This also makes sense since many compression techniques use pixel-error metrics as their objective. \n\nAlthough there is some overlap with prior work, as discussed in the paper, I think there is significant value in extending the analysis to neural compression."
            },
            "weaknesses": {
                "value": "While the classification error metrics presented in the paper is a good start it may need some additional development to make it fully capture bias. As the authors point out: the metric is only as good as the classifier itself. If the classifier is not able to make reliable decisions then the metric could miss bias or overly assign bias. I think this topic deserves more attention. Additionally the sensitivity of classifiers to different frequency degradations (which are common for compression); this may also explain different classification results at low bitrates.\n\nFinally, there is an entire class of compression algorithms based on Implicit Neural Representations (see SIREN [1] for one example) which train a neural compression model unique to each example. This kind of technique could help mitigate any bias but these were not tested in the paper.\n\n1. https://arxiv.org/abs/2006.09661"
            },
            "questions": {
                "value": "* How can we show better reliability of the classification metric?\n* Could INR methods overcome potential bias in neural compression?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates racial bias in neural compression models for facial image compression, particularly at low bitrates. The authors demonstrate that traditional distortion metrics are insufficient for capturing racial bias, which manifests in noticeable degradation of facial features, especially for darker-skinned individuals. They examine the relationship between bias, model architecture, and visual realism, and show that while balancing the training dataset can help reduce bias, it does not fully eliminate it."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper investigate bias in neural compression models, bringing attention to an underexplored area of fairness in AI. The authors reveal clear biases, particularly in skin-type degradation. \n\n2. The raised issue is very noteworthy and worth investigating, as it holds certain value for the generalization and reliability of neural compression methods."
            },
            "weaknesses": {
                "value": "1. One of the major concern is that: while the paper identifies the presence of bias, the essential reasons are not thoroughly explored and mitigation strategies suggested (like dataset balancing) are not shown to be completely effective.\n\n2. The experiments demonstrate that balancing the training dataset can help but does not fully mitigate the bias.  If the dataset balance is not working well. Network architecture\u2019s impact on bias could be critical and should be analyzed further.\n\n3. Exploring the fundamental causes of bias is critical. For instance, what would the bias level and visualized results be like if the network is trained and tested on an Africa-only dataset?"
            },
            "questions": {
                "value": "1. Bias is defined as the maximum difference in loss (Eq. 3 and Eq. 6). How to deal with the impact of extreme values on results and how well does this definition of bias reflect the overall dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}