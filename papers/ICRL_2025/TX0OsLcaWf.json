{
    "id": "TX0OsLcaWf",
    "title": "Zero-Shot Subject-Driven Video Customization with Precise Motion Control",
    "abstract": "Recent advances in customized video generation have enabled users to create videos tailored to both specific subjects and motion trajectories. However, existing methods often require complicated test-time fine-tuning and struggle with balancing subject learning and motion control, limiting their real-world applications. In this paper, we present $\\textbf{DreamCustomizer}$, a zero-shot video customization framework capable of generating videos with a specific subject and motion trajectory, guided by a single image and a bounding box sequence, respectively, and without the need for test-time fine-tuning. Specifically, we introduce reference attention, which leverages the model\u2019s inherent capabilities for subject learning, and devise a mask-guided motion module to achieve precise motion control by fully utilizing the robust motion signal of box masks derived from bounding boxes. While these two components achieve their intended functions, we empirically observe that motion control tends to dominate over subject learning. To address this, we propose two key designs: $\\textbf{1)}$ the masked reference attention, which integrates a blended latent mask modeling scheme into reference attention to enhance subject representations at the desired positions, and $\\textbf{2)}$ a reweighted diffusion loss, which differentiates the contributions of regions inside and outside the bounding boxes to ensure a balance between subject and motion control. Extensive experimental results on a newly curated dataset demonstrate that DreamCustomizer outperforms state-of-the-art methods in both subject customization and motion control. The dataset, code, and models will be made publicly available.",
    "keywords": [
        "Video Generation",
        "Customized Generation"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TX0OsLcaWf",
    "pdf_link": "https://openreview.net/pdf?id=TX0OsLcaWf",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces DreamCustomizer, a video customization model capable of generating videos with a specific subject and motion trajectory. Technically, DreamCustomizer incorporates reference attention into the T2V model to learn the appearance information of the subject, and employs a mask-guided motion module to achieve motion control based on a sequence of bounding boxes. Additionally, the paper proposes masked reference attention and reweighted diffusion loss to emphasize the model's focus on learning from the subject input. Extensive experiments conducted on a newly curated dataset demonstrate the effectiveness of DreamCustomizer for subject customization and motion control."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1: The paper introduces DreamCustomizer, a video generation method capable of controlling both the appearance of the generated subject and the motion trajectory. Additionally, it proposes two strategies, masked reference attention and reweighted diffusion loss, to enhance the model's focus on learning subject appearance. \n\nS2: DreamCustomizer constructs a dataset from WebVid-10M for training, enabling subject customization and motion control during inference without the need for fine-tuning.\n\nS3: The paper presents good qualitative and quantitative results."
            },
            "weaknesses": {
                "value": "W1: The description of the dataset used for evaluation in Section 5.1 could benefit from further clarification. I would appreciate it if the authors could specify the total number of subject-BBox pairs. Additionally, the statement \"we design 60 textual prompts for validation\" raises a question for me: does this imply that there are only 60 pairs used for evaluation? It seems that each input pair would typically have a corresponding text prompt.\n\nW2\uff1aI\u2019m unsure if I missed this detail, but I would like to know whether Tables 2, 3, and 4 are evaluated on the same validation set. If so, I\u2019m curious why the values for DreamCustomizer differ for the same metrics, such as CLIP-T and Temporal consistency, across the three tables.\n\nW3\uff1aWhile the qualitative comparison indicates that DreamCustomizer achieves better mIoU and CD, Figure 10 shows that the model often generates camera movement rather than subject movement. This type of camera movement is not the intended form of motion in this paper and typically results in low CD values. My concern is that the metrics CD and mIoU may not accurately assess the appropriateness of the generated video\u2019s motion (e.g., distinguishing between camera movement and subject movement), potentially reducing the credibility of these metrics."
            },
            "questions": {
                "value": "Q1: I hope the authors can address the concerns raised in the \"Weaknesses\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a zero-shot approach to subject-driven video generation with controlled motion, showing potential for reducing test-time fine-tuning overhead. The reference attention mechanism and mask-guided motion control provide innovations over previous methods by enhancing the fidelity of subject appearance within controlled bounding boxes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The DreamCustomizer framework proposed in this paper does not require fine-tuning during inference, and can directly customize the target subject and motion trajectory in a zero-shot situation. \n\n- This generation framework that does not require fine-tuning improves the efficiency of video generation and is conducive to a wide range of practical applications.\n\n- Users only need to provide the subject image and a set of bounding box sequences to generate a custom video, without the need for complex inference stage debugging."
            },
            "weaknesses": {
                "value": "- While DreamCustomizer claims tuning-free inference, the paper acknowledges the challenge of decoupling camera movement from object motion, leading to camera drift in certain contexts.\n\n-  DreamCustomizer is designed for single-subject customization and does not handle multi-subject videos, which is highlighted as a limitation but without proposed extensions.\n\n- The effectiveness of motion control relies heavily on the precision of bounding box annotations, making the system vulnerable to errors if box tracking is inconsistent.\n\n- Missing some important previous works:\n\n[1] MotionFollower: Editing video motion via lightweight score-guided diffusion. (2024).\n\n[2] FaceChain-ImagineID: Freely crafting high-fidelity diverse talking faces from disentangled audio. CVPR 2024\n\n[3] MotionEditor: Editing video motion via content-aware diffusion. CVPR 2024\n\n[4] Combo: Co-speech holistic 3D human motion generation and efficient customizable adaptation in harmony.  (2024)."
            },
            "questions": {
                "value": "- The paper mentions data filtering steps, but additional details on the refinement of bounding boxes and control signals could provide readers with greater insight into handling imperfect data in training.\n\n- Given the limitations in complex motions and single-subject restriction, it's recommended that the paper discuss specific applications that could benefit from these features as they currently exist."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces DreamCustomizer to generate video with precise motion control and specific subject without the need for fine-tuning during inference. DreamCustomizer leverages \"reference attention\" and a \"mask-guided motion module\" to achieve accurate video customizations, controlled by a single subject image and bounding box sequences. The methodology uses masked reference attention and a reweighted diffusion loss to balance subject learning and motion control. The paper claims superior performance over state-of-the-art methods by a new dataset and extensive quantitative and qualitative evaluations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper distinguishes and mitigates the challenge of motion control dominance by introducing masked reference attention and reweighted diffusion loss, successfully balancing subject fidelity with motion accuracy.\n\n2. The newly curated, diverse dataset provides comprehensive annotations, facilitating training and evaluation for subject and motion control and supporting future research in video customization.\n\n3. With extensive quantitative and qualitative evaluations, the paper demonstrates the effectiveness of DreamCustomizer against state-of-the-art methods."
            },
            "weaknesses": {
                "value": "1. While DreamCustomizer introduces elements like reference attention and reweighted diffusion loss, these techniques lack substantial novelty. Reference attention, for instance, has been well-studied in prior works such as \"StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation,\" where it has been used to effectively maintain subject consistency. Similarly, the reweighted diffusion loss does not introduce a novel approach to balancing subject fidelity with motion control, as similar weighting techniques have been explored in generative models. \n\n2. DreamCustomizer requires bounding boxes as inputs for motion guidance, similar to control techniques seen in previous works like MotionBooth. This limitation reduces its capability for more complex and high quality video generation tasks and the proposed method is incremental to resolve this problem.\n\n3. Moreover, the presented video quality is not convincing to demonstrate that these proposed components are instrumental in advancing controllable video generation."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a video customization framework, DreamCustomizer, which can generate videos based on a specific subject and motion trajectory.\nThe framework incorporates two primary components: reference attention for subject learning and a mask-guided motion module for precise motion control.\nExperiments demonstrate that DreamCustomizer outperforms state-of-the-art methods in both subject customization and motion control."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- The paper is well-written and the figures are properly plotted. Both of them make readers easy to understand the proposed method.\n\n- The experiments are extensive, covering the quantitative and qualitative comparisons with the state-of-the-arts, human evaluation, testing under different conditioning scenarios, and ablation study.\n\n- The authors provide a lot of generated videos, which can help the reader better understand the quality and the potential problems of the proposed method.\n\n- The model is an optimization-free method, which does not require fine-tuning during inference stage.\n\n- The motion control of the proposed model is precise and impressive."
            },
            "weaknesses": {
                "value": "- Low visual quality:\n    - The generated videos have significant artifacts. Did the authors try a better and more recent base model, such as VideoCrafter2?\n    - With this video quality, it is hard to judge the fidelity of subject customization.\n\n- Weakness of reconstruction-based method:\n    - The model adopts a reconstruction-based training strategy, which collects the input conditions (reference image and bounding boxes) from the video and learn to reconstruct the video with these inputs.\n    - However, such training strategy is well-known by worse performance when the user inputs prompt and image from different sources.\n    - For example, in the \"a corgi is swimming\" sample, the model seems to directly copy and paste the input standing corgi and make it floating on the water without introducing reasonable pose or appearance change.\n    - Have the authors tried to solve this problem? Applying some image augmentations could alleviate the problem."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}