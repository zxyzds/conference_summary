{
    "id": "3baOKeI2EU",
    "title": "UniCoTT: A Unified Framework for Structural Chain-of-Thought Distillation",
    "abstract": "Chains of thought (CoTs) have achieved success in enhancing the reasoning capabilities of large language models (LLMs), while their effectiveness is predominantly observed in LLMs. \n    Existing solutions methods adopt distillation to inject chain-of-thought capabilities into small models (SLMs).\n    However, they: \n    (1) can not guarantee the rationality of the generated explanation due to hallucinations; \n    (2) ignore diverse structures of CoT during knowledge transfer.\n    In this paper, we propose a unified CoT distillation framework termed UniCoTT for considering diverse structural CoTs (\\emph{i.e.}, chain, tree, and graph).\n    UniCoTT contains two core strategies: iterative construction for structured CoTs and the structural constraint strategy.\n    Specifically, UniCoTT prompts LLMs to iteratively produce accurate explanations with answers and unifies structured explanations as UniCoT which is seen as a bridge for knowledge transfer.\n    Furthermore, UniCoTT utilizes the proposed unified supervised learning and structural consistency learning strategies to transfer knowledge of structured CoT to SLMs. \n    Experimental results show that UniCoTT can significantly improve the performance of SLMs on multiple datasets across different NLP tasks. **Our code is available in our supplementary materials.**",
    "keywords": [
        "Chain-of-Thought; Structural Thought; Distillation; Unified Framework"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=3baOKeI2EU",
    "pdf_link": "https://openreview.net/pdf?id=3baOKeI2EU",
    "comments": [
        {
            "summary": {
                "value": "This paper focuses on distilling the reasoning capability, specifically chain-of-thought reasoning, from large language models into smaller models. Specifically, the paper uses prompts to guide a larger teacher model to generate multiple explanations, or \"thoughts,\" for given questions and answers. These explanations are represented in a graph structure. Then, the small student model is trained using traditional cross-entropy loss along with a novel structural consistency loss and supervised contrastive loss proposed by the authors."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The writing is clear and easy to follow, with a well-defined motivation for the research.\n- The distillation framework proposed is innovative, especially in using a graph structure to represent different chains of thought and introducing corresponding training methods.\n- The approach is extensively tested on multiple benchmark datasets, demonstrating strong empirical performance."
            },
            "weaknesses": {
                "value": "- The framework mainly focuses on distilling explanation and reasoning abilities into base models like BERT. A concern is the limited application scope of such encoder-based models. To further validate the effectiveness of the proposed distillation framework for reasoning abilities, it would be interesting to distill the chain-of-thought reasoning from larger models into smaller decoder-based models and test them on complex reasoning tasks."
            },
            "questions": {
                "value": "- Why focus on using an encoder as the student model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel framework for transferring the reasoning capabilities of large language models (LLMs) to small language models (SLMs) through a structured chain-of-thought (CoT) distillation approach. The authors propose UniCoTT, which considers diverse structural CoTs (chain, tree, and graph) and employs two core strategies: iterative construction for structured CoTs and a structural constraint strategy. The framework aims to address the challenges of ensuring the rationality of generated explanations and ignoring diverse structures of CoT during knowledge transfer. The experimental results demonstrate significant performance improvements of SLMs on multiple NLP tasks across various datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces a unified framework that handles diverse structural CoTs, which is a significant advancement over existing methods that focus solely on chain structures.\n2. The authors provide extensive experimental evidence to support the effectiveness of UniCoTT, showing improvements across different NLP tasks and datasets.\n3. The consideration of structured reasoning pathways (tree and graph) in addition to chains is a strength, as it better captures the complexity of human reasoning processes."
            },
            "weaknesses": {
                "value": "1. The paper could benefit from a discussion on the computational complexity of UniCoTT and its scalability, especially when dealing with very large datasets or more complex reasoning tasks.\n2. The construction of UniCoT relies on APIs of LLMs, which may not be accessible or feasible in all situations. The paper could address potential alternatives or mitigation strategies. Besides, SLMs usually refer to small language models, e.g., 2B and 3B. The authors mainly conducted experiments on BERT and RoBERTa, which were not convincing enough.\n3. While the results are promising, the paper primarily focuses on question-answering and NLU tasks. It would be beneficial to see how UniCoTT generalizes to other types of tasks."
            },
            "questions": {
                "value": "1. How does the performance of UniCoTT scale with the size and complexity of the knowledge to be transferred? Are there diminishing returns as the complexity increases?\n2. What are the limitations of the current implementation of UniCoTT, and how might these be addressed in future work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes UniCoTT, a unified distillation framework to transfer the diverse reasoning structures with CoT to smaller language models such as BERT and RoBERTa. Firstly, UniCoT is proposed as a unified bridge of various CoT structures, which is constructed by iteratively prompting LLMs to produce explanations with correct answers. After that, a node-level supervised contrastive loss and a structural consistency loss are designed as part of the training objective. Experiments on multiple reasoning datasets verified the effectiveness of UniCoTT by surpassing the baseline methods by a large margin."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed method is technically sound and intuitively makes sense. It is very interesting to transfer the knowledge from structured CoT texts into smaller models that can leverage rationale knowledge in a unified manner.\n2. Experimental results on several benchmarks show some improvement upon baselines and the authors conduct extensive ablation studies and analyses on various design choices.\n3. The paper itself is generally well-written."
            },
            "weaknesses": {
                "value": "1. The generalizability of KNIFE is yet to be known. The proposed framework is only verified in multiple-choice datasets. Whether it could be extended to other task settings like text generation remains a concern.\n2. The process of iteratively constructing UniCoT is hard to understand from the main body of the current version. I would suggest the authors move some content from the appendix to the main body. Meanwhile, it would be helpful if the authors could provide some overall statistics on the constructed UniCoT. For example, the averaged nodes and edges of the structure."
            },
            "questions": {
                "value": "1. The authors list \"hallucinations\" as one of the major drawbacks of previous works, and motivate the design of UniCoTT in ``introduction'' section. I am wondering how the designed UniCoTT framework helps to alleviate this issue.\n2. In lines 385-386, why $\\alpha$ and $\\beta$ is set to 0.5 and 0.2 respectively? Is it an intuitive trial or a result of a grid search?\n3. It would be interesting to test the annotation efficiency of CoT with the teacher model. An empirical conclusion of how many annotations are enough for great distillation performance would be insightful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper  introduces UniCoTT, a teacher-student framework aimed at transferring complex reasoning abilities from large language models (LLMs) to smaller language models (SLMs). UniCoTT extends traditional chain-of-thought (CoT) reasoning by leveraging diverse structured reasoning paths, such as chains, trees, and graphs, within a unified distillation process. This approach involves iterative CoT construction, node-level supervised contrastive learning, and structural consistency learning to reinforce reasoning capabilities in SLMs. Experimental results on factual reasoning, multiple-choice QA, and natural language understanding tasks demonstrate that UniCoTT outperforms existing methods, enhancing SLM performance across several benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Extends CoT reasoning with diverse structures, which broadens the reasoning capabilities of SLMs.\n- Implements structural consistency and contrastive learning, effectively aligning SLMs with complex CoT reasoning paths.\n- Demonstrates superior performance on multiple tasks, showing effectiveness and generality in knowledge transfer."
            },
            "weaknesses": {
                "value": "UniCoTT\u2019s increased complexity and computational requirements could make real-world deployment challenging. To be fair, as distillation strategy proposed in this paper uses three types of reasoning and more compute to create dense supervision. The baselines like CoT may also uses more compute like more chains in self-consistency to increase the quality of distillation data."
            },
            "questions": {
                "value": "No"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}