{
    "id": "Oi47wc10sm",
    "title": "Programming Refusal with Conditional Activation Steering",
    "abstract": "LLMs have shown remarkable capabilities, but precisely controlling their response behavior remains challenging.\nExisting activation steering methods alter LLM behavior indiscriminately, limiting their practical applicability in settings where selective responses are essential, such as content moderation or domain-specific assistants.\nIn this paper, we propose Conditional Activation Steering (CAST), which analyzes LLM activation patterns during inference to selectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts activate distinct patterns in the model's hidden states.\nUsing CAST, one can systematically control LLM behavior with rules like \"if input is about hate speech or adult content, then refuse\" or \"if input is not about legal advice, then refuse.\"\nThis allows for selective modification of responses to specific content while maintaining normal responses to other content, all without requiring weight optimization.\nWe release an open-source implementation of our framework at <placeholder: open-source GitHub link>.",
    "keywords": [
        "Activation Engineering",
        "Safety",
        "Alignment",
        "Steering Vector"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "Conditional Activation Steering enables context-dependent control over LLM behaviors by conditionally applying activation steering based on input context during inference without weight optimization.",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Oi47wc10sm",
    "pdf_link": "https://openreview.net/pdf?id=Oi47wc10sm",
    "comments": [
        {
            "title": {
                "value": "Author Comment: Multi-conditioning Implementation"
            },
            "comment": {
                "value": "> Multi-conditioning -- since the conditions are combined into one function, how is the appropriate steering vector determined (as presumably a different steering vector would be desired based on which condition activated)\n\nThe same steering vectors are used whether applying single or multiple conditions. Once conditions are extracted, they can be freely combined using logical operations without needing to extract new vectors. This property relies on the linear representation hypothesis discussed in broader LLM literature - given this hypothesis holds, logical combinations of conditions should work by combining the same underlying vectors. Our empirical results in Figures 7-9 support this, showing consistent behavior across various logical combinations using the same extracted vectors.\n\nLooking forward, if the field continues to develop reliable methods for linearly extracting behaviors and thought patterns in language models, our framework becomes increasingly valuable. One could extract multiple steering vectors for desired scenarios and programmatically control behaviors and their activation conditions, enabling systematic behavior programming in language models."
            }
        },
        {
            "title": {
                "value": "Author Comment: Tanh Transformation"
            },
            "comment": {
                "value": "> What specifically motivated use of tanh for the similarity transformation as opposed to vanilla cosine similarity?\n\nWe achieve very similar results with or without tanh. The switch to tanh during development was mainly based on empirical observations that tanh seemed to provide smoother transitions around the threshold values, though the performance difference is minimal. Given that this isn't a major component of our method and results are similar either way, we kept tanh in our implementation."
            }
        },
        {
            "title": {
                "value": "Author Comment: Choice of PCA, Tanh Transformation, Multi-conditioning Implementation"
            },
            "comment": {
                "value": "> Why was PCA used to compute condition and behavior vectors, as opposed to fitting a classifier (e.g. SVM or logistic regression) which would explicitly fit a separating hyperplane between the classes (and thus moving in the direction of that boundary would directly correspond to steering the behavior in a desired direction)? If I understand correctly, the PCA is computed on the matrix containing both positive and negative activations, and the authors assert the direction of maximal variance (first principal component) is the direction that best distinguishes between the positive and negative examples. It isn't quite clear to me how this can be assumed to be true -- an analysis of, e.g., the distribution of coordinates of the positive and negative examples when projected along this direction would be very useful to see (i.e. that we see a bimodal distribution of coordinates along this direction, such that moving an activation along the direction does correspond to encouraging the behavior)\n\nThank you for raising this question about our choice of PCA. While we followed established practices from <Zou-2024> for vector extraction, we acknowledge that alternative methods like SVM or logistic regression could potentially be used. This is part of an ongoing discussion in the field - recent works (cited in Section 2) propose various improvements to this base approach.\n\nHowever, our key contribution - the conditional application of steering vectors - is orthogonal to the specific vector extraction method. Whether using PCA, SVM, or other techniques, the core innovation of CAST lies in the conditional framework that determines when and how to apply these vectors. Future work could explore how different extraction methods affect CAST's performance, but the fundamental mechanism of conditional steering remains the same.\n\n<Zou-2024> \"Representation engineering: A top-down approach to ai transparency.\""
            }
        },
        {
            "title": {
                "value": "Author Comment: Multi-condition Complexity"
            },
            "comment": {
                "value": "> Complexity in Multi-condition Settings: The paper discusses multi-conditioning but does not fully examine potential trade-offs when many conditions are combined, especially when conflicting conditions arise. Further clarification on how to manage or prioritize conditions could enhance the method\u2019s applicability in complex real-world scenarios.\n\n\nThank you for raising this point about multi-condition complexity. We agree that further investigation would be valuable, particularly regarding trade-offs between conditions. In general, we observe that semantically similar categories tend to create larger trade-offs when combined. Though not directly relevant, Figure 9(c) hints at this relationship - when categories are semantically more distant from others (x-axis), we achieve better refusal rates for other categories (y-axis).\n\nThese trade-offs are also visible in Figure 8's results. However, we generally observe minimal conflicts when simply adding or removing refusals on multiple conditions. The more significant trade-offs emerge when we attempt more aggressive remapping of the refusal landscape by simultaneously removing some refusals while adding others. This suggests that maintaining consistent directionality in behavioral modifications (all additions or all removals) leads to more predictable results.\n\nImportantly, we expected and observed lower, minimal trade-offs when working with models that have low starting refusal rates across most categories. This observation suggests that beginning with a \"cleaner slate\" - models with minimal built-in refusal behaviors - provides more flexibility for implementing complex multi-condition rules.\n\nWhile we believe this deserves in-depth exploration that could merit its own short conference or workshop paper, we would be happy to include additional discussion in our camera-ready version demonstrating these trade-off patterns and their relationship to semantic similarity and base model characteristics."
            }
        },
        {
            "title": {
                "value": "Author Comment: Incomplete Quantitative Results (2 of 2)"
            },
            "comment": {
                "value": "For Figure 7, these results demonstrate robust performance across all categories - not just cherry-picked examples. While some models are easier to work with (finding optimal steering setups), when a model works well, it performs consistently across categories, similar to the saturation property discussed earlier. This figure shows how we can alter refusal rates for arbitrary categories from base levels. It's particularly exciting as it appears to be the first approach enabling programmatic behavior changes like this, forming part of our broader safety research agenda on programming model behaviors. Models with low base refusal rates (like Hermes 2 Pro) offer especially interesting opportunities.\n\nFigure 8 demonstrates logical combination of conditions, allowing more complex behavioral rules. The results show we can effectively combine multiple conditions while maintaining targeted control over refusal rates. This enables sophisticated behavioral programming beyond simple category-based refusal.\n\nFigure 9 connects to our duality property discussion - models learn to refuse all non-target categories by learning not to refuse the target category. Particularly interesting are the out-domain results, where categories were completely unseen during training yet still show strong refusal patterns. This suggests the method generalizes well beyond its training categories.\n\nFor the camera-ready version, we can add these comprehensive numerical results  to the appendix while maintaining the current visualizations for interpretability."
            }
        },
        {
            "title": {
                "value": "Author Comment: Incomplete Quantitative Results (1 of 2)"
            },
            "comment": {
                "value": "> I may have misread, but I think the only quantitative results are in Table 2. 3 models are listed, but table 1 references aroudn 8 models used. Where are results for the other models? In addition, the pie chart visualizations (e.g. Figure 7) are great, but a quantitative summary would be very useful.\n\nThank you for noting the incomplete presentation of quantitative results. You're right - Table 2 shows only three models due to space restrictions. The complete results are shown in Figure 1 and Table 3 (Appendix A). If you have preferences for information presentation, we're open to discussion.\n\nRegarding the pie charts, we chose this visualization for better interpretability, we're glad you found it intuitive. Here are the complete numerical results:\n\n\n### Figure 7 - Hermes 2 Pro\n\n| Question Type | Base | Sex\u2192Refuse | Legal\u2192Refuse | Hate\u2192Refuse | Crime\u2192Refuse | Health\u2192Refuse |\n|--------------|------|------------|--------------|-------------|--------------|---------------|\n| Basic | 3.00% | 11.00% | 23.80% | 4.40% | 18.20% | 5.60% |\n| Sexual | 4.00% | 94.00% | 6.20% | 12.00% | 6.00% | 4.20% |\n| Legal | 10.20% | 10.60% | 88.00% | 10.60% | 19.80% | 10.80% |\n| Hate | 41.00% | 48.60% | 41.80% | 93.40% | 41.60% | 41.00% |\n| Crime | 5.00% | 5.80% | 23.80% | 5.40% | 95.00% | 5.20% |\n| Health | 12.00% | 12.40% | 28.00% | 12.00% | 22.60% | 77.60% |\n\n### Figure 7 - LLaMA 3.1 Inst\n\n| Question Type | Base | Crime\u2192Don't Refuse | Hate\u2192Don't Refuse |\n|--------------|------|-------------------|-------------------|\n| Basic | 2.00% | 2.20% | 2.00% |\n| Sexual | 29.00% | 28.80% | 25.20% |\n| Legal | 10.80% | 9.80% | 10.80% |\n| Hate | 84.40% | 83.40% | 14.60% |\n| Crime | 70.40% | 7.40% | 70.40% |\n| Health | 15.60% | 14.00% | 15.40% |\n\n### Figure 8 - Hermes 2 Pro\n\n| Question Type | Base | Health\u2228Legal\u2192Refuse | Hate\u2228Legal\u2192Refuse | Hate\u2228Legal\u2228Sex\u2192Refuse | Hate\u2228Crime\u2192Refuse |\n|--------------|------|-------------------|------------------|---------------------|------------------|\n| Basic | 3.00% | 25.60% | 25.20% | 33.00% | 19.60% |\n| Sexual | 4.00% | 6.40% | 14.20% | 95.00% | 14.00% |\n| Legal | 10.20% | 88.00% | 88.20% | 88.20% | 20.20% |\n| Hate | 41.00% | 41.80% | 93.60% | 93.80% | 93.60% |\n| Crime | 5.00% | 23.80% | 24.20% | 24.80% | 95.40% |\n| Health | 12.00% | 81.40% | 28.00% | 28.40% | 22.60% |\n\n### Figure 8 - LLaMA 3.1 Inst\n\n| Question Type | Base | Health\u2228Legal\u2228Sex\u2192Refuse | Hate\u00acRefuse\u2227Health\u2192Refuse | Hate\u2228Crime\u2228Sex\u00acRefuse\u2227Legal\u2192Refuse | Hate\u2228Sex\u00acRefuse\u2227Health\u2192Refuse |\n|--------------|------|------------------------|--------------------------|----------------------------------|----------------------------|\n| Basic | 2.00% | 11.60% | 4.00% | 3.80% | 4.00% |\n| Sexual | 29.00% | 69.40% | 25.80% | 1.00% | 1.00% |\n| Legal | 10.80% | 56.20% | 13.00% | 48.00% | 13.00% |\n| Hate | 84.40% | 86.80% | 14.60% | 14.80% | 14.60% |\n| Crime | 70.40% | 72.00% | 70.60% | 10.80% | 70.40% |\n| Health | 15.60% | 56.20% | 55.20% | 15.40% | 54.60% |\n\n### Figure 9 - Hermes 2 Pro (In-domain)\n\n| Question Type | Base | \u00acHealth\u2192Refuse | \u00acCrime\u2192Refuse | \u00acHate\u2192Refuse | \u00acSex\u2192Refuse | \u00acLegal\u2192Refuse |\n|--------------|------|----------------|---------------|--------------|-------------|---------------|\n| Basic | 3.00% | 91.00% | 78.40% | 92.20% | 85.60% | 72.80% |\n| Sexual | 4.00% | 97.00% | 95.20% | 89.20% | 7.20% | 95.00% |\n| Legal | 10.20% | 91.40% | 82.40% | 91.60% | 91.60% | 14.20% |\n| Hate | 41.00% | 97.40% | 96.80% | 45.00% | 89.80% | 96.60% |\n| Crime | 5.00% | 99.00% | 9.20% | 98.80% | 98.40% | 80.40% |\n| Health | 12.00% | 30.20% | 85.20% | 95.80% | 95.40% | 79.80% |\n\n### Figure 9 - Hermes 2 Pro (Out-domain)\n\n| Question Type | Base | \u00acHealth\u2192Refuse | \u00acCrime\u2192Refuse | \u00acHate\u2192Refuse | \u00acSex\u2192Refuse | \u00acLegal\u2192Refuse |\n|--------------|------|----------------|---------------|--------------|-------------|---------------|\n| Basic | 3.00% | 92.20% | 78.40% | 92.20% | 85.60% | 72.80% |\n| Malware | 20.20% | 94.80% | 43.80% | 94.80% | 97.80% | 84.00% |\n| Privacy | 17.80% | 92.00% | 71.40% | 92.00% | 91.00% | 84.60% |\n| Financial | 4.40% | 92.20% | 62.60% | 92.20% | 92.40% | 41.80% |\n| Gambling | 1.80% | 94.00% | 70.00% | 94.00% | 89.40% | 70.20% |"
            }
        },
        {
            "title": {
                "value": "Author Comment: Evaluation Methodology Clarity"
            },
            "comment": {
                "value": "> The main text doesn't seem to explain well how false and true positives for refusals are automatically assessed (e.g. when doing grid search over hypers (like layer and refusal threshold)). A clearer explanation of this would be helpful\n\nThank you for raising this point about evaluation methodology clarity. Our evaluation methodology, detailed in Appendix D.2 and C.2, uses a two-pronged approach for refusal detection that aligns with standard practices in the field: \n\n1. Primary classification using protectai/distilroberta-base-rejection-v1 model \n2. Additional keyword-based verification using common refusal phrases \n\nThe hybrid approach was necessary because we were working with diverse models from different developers, each using distinct refusal phrasings. This approach improves robustness compared to using just a rejection detection model. Our approach was partially motivated by the \"refusal substrings\" method from <Arditi-2024>, which we build upon.\n\nWe will make this methodology more prominent in the main text for the camera-ready version.\n\n<Arditi-2024> \"Refusal in language models is mediated by a single direction\" (NeurIPS)"
            }
        },
        {
            "title": {
                "value": "Author Comment: Duality Property Significance"
            },
            "comment": {
                "value": "> Why is duality of the comparison direction highlighted? Isn't it obvious that flipping the comparison direction and using a threshold of (1-c) yields the same decision boundary but flips the decision? I might be missing something here.\n\nThank you for noting the duality property. While flipping the comparison direction might appear straightforward, we emphasize this property due to its powerful implications for practical implementation.\n\nFirst, consider the challenge of implementing \"allow only X\" rules using traditional methods like Reinforcement Learning with Human Feedback (RLHF) or fine-tuning. These approaches demand:\n\n1. Comprehensive training data that covers all possible scenarios to refuse, essentially any non-X category.\n\n2. A careful balance and representativeness in the training dataset to ensure model consistency.\n\n3. Multiple rounds of fine-tuning and evaluation to approach the desired behavior reliably.\n\nIn contrast, our method leverages the duality property in a way that fundamentally simplifies this process. Rather than requiring exhaustive data for every conceivable refusal scenario, it only needs positive examples within the desired category (e.g., legal advice). Through the duality property, everything outside this category is automatically refused. This simplicity is powerful for several reasons:\n\n1. Data collection and training are significantly more manageable as they focus exclusively on the domain of interest.\n\n2. The model is inherently capable of refusing requests from novel categories it has never encountered in training.\n\n3. Behavior can be modified or tuned on the fly without the need for retraining.\n\nAs we show in Section 5 and Figure 9, this duality property enables limiting model responses to a single domain and refusing others. It achieves refusal rates above 80% for out-of-domain queries while maintaining normal response rates for in-domain queries, including those from previously unseen categories. Thus, a practitioner can ensure refusals for all non-relevant or unsafe categories without needing specific data for each refusal case.\n\nWe could further elaborate on this in the camera-ready version to make clearer why this property is an important part of our method."
            }
        },
        {
            "title": {
                "value": "Author Comment: Figure 6a Metric Issue and Duality Property Significance"
            },
            "comment": {
                "value": "> Figure 6a -- why is conditions triggered the 'success' metric here? Shouldn't it be something like F1 score (for aggregating true positives, false positives, etc. to show performance at each data scale)\n\nThank you for this suggestion about the metrics in Figure 6a. We'd like to explain our rationale while addressing your point about F1 scores.\n\nThe \"conditions triggered\" percentage was specifically chosen because it directly demonstrates whether our method successfully identifies harmful content in practice. However, we agree that F1 scores provide valuable additional insight. We've run these numbers on the training set:\n\n| Examples | QWEN 1.5 1.8B Chat | Danube 3 4B chat | OLMo 7B SFT |\n|----------|-------------------|------------------|--------------|\n| 45       | 0.989            | 0.989            | 0.978        |\n| 135      | 0.928            | 0.800            | 0.967        |\n| 270      | 0.911            | 0.840            | 0.936        |\n| 540      | 0.902            | 0.824            | 0.936        |\n| 1080     | 0.898            | 0.839            | 0.931        |\n| 2025     | 0.899            | 0.820            | 0.938        |\n\nThese F1 scores support our original observation about performance saturation. Note that the higher F1 scores at very small training sizes (e.g., 0.989 at 45 examples) are likely due to overfitting on the small training set. The performance then stabilizes as training data increases, matching the plateauing behavior we observed with the conditions triggered metric.\n\nIf this line of inquiry seems interesting to you and would be beneficial to the paper, we could ablate these experiments to the test set and update the paper accordingly. Perhaps we could add these numbers somewhere in the paper too while keeping the current figure intact. We're suggesting this as a potential direction since reconfiguring our codebase for test set evaluation would require some engineering work, but we're happy to pursue this if you think it would strengthen the paper's contribution."
            }
        },
        {
            "title": {
                "value": "Author Comment: Working on Addressing Reviewer Concerns"
            },
            "comment": {
                "value": "Thank you for your thorough and constructive reviews of our paper \"Programming Refusal with Conditional Activation Steering.\" We greatly appreciate the time and effort you've put into providing detailed feedback and suggestions for improvement.\n\nWe are currently working on addressing the concerns raised in your reviews. To ensure clarity and completeness in our responses, we will post them incrementally as we complete the additional experiments and analyses requested. All additional results will be generated using our open-source codebase to ensure reproducibility. \n\nWe look forward to sharing these results with you in the coming days."
            }
        },
        {
            "summary": {
                "value": "This paper presents Conditional Activation Steering (CAST) as a novel framework for enabling selective control over large language model (LLM) responses by dynamically applying refusal behavior to specific prompt categories. Traditional activation steering techniques alter model behaviors across all inputs, but CAST introduces a \u201ccondition vector\u201d that enables selective behavior modification based on context. This framework allows for fine-grained, context-dependent refusal responses\u2014such as rejecting hate speech but responding to benign prompts\u2014without requiring weight optimization. The authors claim CAST contributes a valuable tool for alignment, moderation, and domain-specific applications where selective behavior control is essential."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Novel Approach: CAST represents a unique advancement in activation steering by adding the ability to conditionally refuse specific categories of prompts. This method is valuable in fields like content moderation and personalized assistant behavior where indiscriminate refusal would limit utility.\n* Empirical Validation: Extensive experiments demonstrate CAST\u2019s efficacy in refusing harmful prompts without affecting benign responses across multiple LLMs. The results indicate robust behavior modification and reliable model conditioning under various categories.\n* The authors note they will release open source code, which will be valuable for other researchers interested in using these methods"
            },
            "weaknesses": {
                "value": "* Figure 6a -- why is conditions triggered the 'success' metric here? Shouldn't it be something like F1 score (for aggregating true positives, false positives, etc. to show performance at each data scale)\n* Why is duality of the comparison direction highlighted? Isn't it obvious that flipping the comparison direction and using a threshold of (1-c) yields the same decision boundary but flips the decision? I might be missing something here.\n* The main text doesn't seem to explain well how false and true positives for refusals are automatically assessed (e.g. when doing grid search over hypers (like layer and refusal threshold)). A clearer explanation of this would be helpful\n* I may have misread, but I think the only quantitative results are in Table 2. 3 models are listed, but table 1 references aroudn 8 models used. Where are results for the other models? In addition, the pie chart visualizations (e.g. Figure 7) are great, but a quantitative summary would be very useful.\n* Complexity in Multi-condition Settings: The paper discusses multi-conditioning but does not fully examine potential trade-offs when many conditions are combined, especially when conflicting conditions arise. Further clarification on how to manage or prioritize conditions could enhance the method\u2019s applicability in complex real-world scenarios."
            },
            "questions": {
                "value": "* Why was PCA used to compute condition and behavior vectors, as opposed to fitting a classifier (e.g. SVM or logistic regression) which would explicitly fit a separating hyperplane between the classes (and thus moving in the direction of that boundary would directly correspond to steering the behavior in a desired direction)? If I understand correctly, the PCA is computed on the matrix containing both positive and negative activations, and the authors assert the direction of maximal variance (first principal component) is the direction that best distinguishes between the positive and negative examples. It isn't quite clear to me how this can be assumed to be true -- an analysis of, e.g., the distribution of coordinates of the positive and negative examples  when projected along this direction would be very useful to see (i.e. that we see a bimodal distribution of coordinates along this direction, such that moving an activation along the direction does correspond to encouraging the behavior)\n* What specifically motivated use of tanh for the similarity transformation as opposed to vanilla cosine similarity?\n* Multi-conditioning -- since the conditions are combined into one function, how is the appropriate steering vector determined (as presumably a different steering vector would be desired based on which condition activated)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces CAST, a technique to perform conditional activation steering. Following prior work, CAST generates behavior vectors that determine the direction in which to steer models to elicit a specific behavior. Next, CAST generates condition vectors that determine the context in which to apply the behavior vectors. By selectively applying the behavior vectors, CAST is able to steer behavior more precisely and under more complex combinations of rules than prior work."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Introduces the concept of 'condition vectors' as a way to gate which activation vectors are triggered at each step. \n- Thoroughly demonstrates the use of CAST to handle a variety of new conditional behaviors, such as more precise and robust refusal and topic-based refusal.\n- Paper is mostly clearly written with well-formatted figures. The setup seems reproducible and is easy to understand.\n- Helps extend the idea of activation steering to make it more robust to multiple scenarios.\nIn summary, the paper introduces a novel technique (reasonable novelty) and demonstrates its robustness (good quality) with clear presentation (great clarity) towards improving the efficacy and usability of activation steering (reasonable significance)."
            },
            "weaknesses": {
                "value": "These suggestions are minor as the paper itself is well done. That said, I think the paper could benefit from:\n\n**Qualitative understanding of the condition vectors.** The main insight of the work seems to be that you can construct condition vectors in a similar way to constructing behavior vectors. But I'm still not sure how robust these condition vectors are. It would be great if the authors could run an experiment studying the generalization of these condition vectors. \n   - For example, you could take the categories described in the paper: health, crime, legal, etc. and ask GPT to generate prompts for them. For each category, there could be two treatments: basic (prompts that use language from the given category and are relevant to the category) and hard (prompts that use language from the given category but are irrelevant to the category, e.g., a prompts that uses legalese without being about legal statements). \n   - Then, for each category's condition vector, how robust is it? What's its accuracy under the two treatments? Can you provide some qualitative failure modes of each condition vector? Do the vectors improve if the model itself improves (and therefore its representations improve). Getting a better qualitative and quantitative understanding of the condition vectors seems useful for understanding the limitations of this technique.\n\n**Improved error analysis.** Are the failures in Figure 1 (and the other experiments) a result of a the condition vector not triggering or the behavior vector not robustly modifying the behavior? Please consider conducing an ablation experiment where the behavior vector is manually applied according to an oracle on the harmfulness and report the F1 score / rate of successful refusal. How does this skyline compare to CAST?\n\n**Missing baselines.** Would it be possible in all of the experiments to have a quantitative comparison to a prompting baseline where the model is told to carefully pay attention to the harmfulness and harmlessness of the response in its refuse (or the relevant category, if its a category). My hunch is that improved prompting would somewhat help for the harmlessness / harmfulness category and fail to help for the more complex combinations of rules. I think this comparison to prompting would help showcase the effectiveness of CAST.\n\nI'm not willing to raise my score higher as I don't think there's evidence that activation steering is a scalable direction for controlling models as it doesn't improve with data (see Figure 6a) and in general seems to be clunky to deploy (see L202 - 208), but I think this paper is high quality and definitely should be accepted. Moreover, I still think the authors should consider incorporating the above suggestions as it would strengthen the paper."
            },
            "questions": {
                "value": "Typos:\n- L365 - 372: shouldn't there be 700 + 500 = 1200 prompts per category? Seems like a typo + please ensure consistency throughout the paper on the exact number"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce Conditional Activation Steering (CAST) and condition vectors.  They show that refusal behaviors can be invoked conditionally on the context of the prompt allowing for conditional steering.  They test this across several language models up to size 8B and show that their method has fewer false positive refusals on harmless prompts while still maintaining high refusal rates on harmful prompts, demonstrating the effectiveness of the conditional."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The method of steering LLM conditionally on the context of the prompt is novel and an important contribution towards practical implementations of activation steering.\n2. The ability to chain conditionals is an interesting contribution.\n3. The paper is relatively thorough in its test of models within a certain class O(8B)."
            },
            "weaknesses": {
                "value": "1. All the tested models have less than or equal to 8B parameters.  Testing on larger models would help improve the robustness and confidence in the results\n2. (Minor) The harmless/harmful refusals are not tested against enough real-world inputs, like jailbreaks or multi-turn conversations.\n3. (Minor) There is no limitations or future work section."
            },
            "questions": {
                "value": "The paper is generally well written and was pleasant and interesting to read.  The possibility for conditional steering is exciting with many practical implications.\n\n### Minor\nThe paper could be improved through some careful revisions to the figures and layouts.   \n* Figure 1 is presented too early; its full description is on page 6 while it appears at the top of page 2.  Despite being referenced on page 1, the paper would flow better if Figure 1 were closer to page 6.\n* All of the T-SNE plots should consider a different color scheme.  Its very difficult to distinguish the Alpaca vs Sorry-bench dots, especially against the background of a similar color.\n* Figure 8c, several pieces of text are too small to easily read\n* Figure 9, the label \"(c)\" is not placed in the top left corner like the previous figures.  The markers are difficult to see (e.g. the start marker).  \n\n\n### Out-of-scope improvements\nWhile the following improvements would substantially increase the value of the paper, the reviewer recognizes that they can be designated to follow up work and may be out-of-scope for the current paper.\n* The methods could be tested with models with > 8B parameters\n* The methods could be tested against known jailbreaks (e.g. does the conditional vector for harmfulness still activation on a zero-shot or 1-shot jailbreak?)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}