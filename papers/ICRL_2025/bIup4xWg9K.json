{
    "id": "bIup4xWg9K",
    "title": "CodeCipher: Learning To Obfuscate Source Code Against LLMs",
    "abstract": "While code large language models have made significant strides in AI-assisted coding tasks, there are growing concerns about privacy challenges. The user code is transparent to the cloud LLM service provider, inducing risks of unauthorized training, reading, and execution of the user code.  In this paper, we propose CodeCipher, a novel method that perturbs privacy from code while preserving the original response from LLMs. CodeCipher transforms the LLM's embedding matrix so that each row corresponds to a different word in the original matrix, forming a token-to-token confusion mapping for obfuscating source code. The new embedding matrix is optimized through minimizing the task-specific loss function. To tackle the challenge from the discrete and sparse nature of word vector spaces, CodeCipher adopts a discrete optimization strategy that aligns the updated vector to the nearest valid token in the vocabulary before each gradient update. We demonstrate the effectiveness of our approach on three AI-assisted coding tasks including code completion, summarization, and translation. Results show that our model successfully confuses the privacy in source code while preserving the original LLM's performance.",
    "keywords": [
        "Code obfuscation; LLM privacy; Large Language Models of Code"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "A learning based code obfuscation method to protect code privacy from LLM service providers",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=bIup4xWg9K",
    "pdf_link": "https://openreview.net/pdf?id=bIup4xWg9K",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces CodeCipher, a new learning-based code obfuscation technique tailored for LLMs. \n\nCodeCipher safeguards code from unauthorized training, reading, compiling, and execution, without sacrificing LLM service quality. \n\nThe main idea behind CodeCipher is to transform the LLM\u2019s embedding matrix so that each row corresponds to a different word in the original matrix. This process creates a token-to-token confusion mapping, which the system uses to obfuscate tokens when encountering new code snippets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The idea of using a token-to-token confusion mapping is interesting.\n\nThe efficacy of the proposed approach was assessed across three AI-assisted coding tasks: code completion, code summarization, and code translation.\n\nResults revealed that the proposed method surpassed a range of conventional obfuscation methods in terms of both the level of confusion and the preservation of performance for downstream tasks."
            },
            "weaknesses": {
                "value": "Only rule-based baselines are compared.\n\nMore related techniques need to be mentioned and compared, such as:\n\nCryptography-based Approaches (e.g., Homomorphic Encryption, Multi-Party Computation, Functional Secret Sharing, Differential Privacy in Inference):\n\nW.-j. Lu, Z. Huang, Z. Gu, J. Li, J. Liu, K. Ren, C. Hong, T. Wei, and W. Chen, \u201cBumblebee: Secure two-party inference framework for large transformers,\u201d Cryptology ePrint Archive, 2023.\n\nI. Zimerman, M. Baruch, N. Drucker, G. Ezov, O. Soceanu, and L. Wolf, \u201cConverting transformers to polynomial form for secure inference over homomorphic encryption,\u201d arXiv preprint arXiv:2311.08610, 2023.\n\nX. Liu and Z. Liu, \u201cLlms can understand encrypted prompt: Towards privacy-computing friendly transformers,\u201d arXiv preprint arXiv:2305.18396, 2023.\n\nDetection-based Approaches (e.g., Direct Detection, Contextual Inference Detection):\n\nS. Kim, S. Yun, H. Lee, M. Gubri, S. Yoon, and S. J. Oh, \u201cPropile: Probing privacy leakage in large language models,\u201d 2023.\nN. Mireshghallah, H. Kim, X. Zhou, Y. Tsvetkov, M. Sap, R. Shokri, and Y. Choi, \u201cCan llms keep a secret? testing privacy implications of language models via contextual integrity theory,\u201d 2023.\n\nHardware-based Approaches (e.g., Data Locality, Confidential Computing with Trusted Execution Environment (TEE)):\n\nY. Wang, Y. Lin, X. Zeng, and G. Zhang, \u201cPrivatelora for efficient privacy preserving llm,\u201d arXiv preprint arXiv:2311.14030, 2023.\n\nT. South, G. Zuskind, R. Mahari, and T. Hardjono, \u201cSecure community transformers: Private pooled data for llms.\u201d 2023.\n\nW. Huang, Y. Wang, A. Cheng, A. Zhou, C. Yu, and L. Wang, \u201cA fast, performant, secure distributed training framework for large language model,\u201d arXiv preprint arXiv:2401.09796, 2024."
            },
            "questions": {
                "value": "Can the proposed idea be applicable and scalable for state-of-the-art code generation models? How does the proposed idea work on state-of-the-art code generation models?\n\nJingxuan He, Martin Vechev. Large Language Models for Code: Security Hardening and Adversarial Testing. 2023. In CCS. https://arxiv.org/abs/2302.05319.\n\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. In ICLR. https://arxiv.org/\nabs/2203.13474\n\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2023. InCoder: A Generative Model for Code Infilling and Synthesis. In ICLR. https://arxiv.org/\nabs/2204.05999\n\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Mu\u00f1oz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. SantaCoder: Don\u2019t Reach for the Stars! CoRR\nabs/2301.03988 (2023). https://arxiv.org/abs/2301.03988"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "**Summary:**\n\nThis paper studies the way of obfuscating source code for LLMs.  The proposed approach transforms the LLM\u2019s embedding matrix so that each row corresponds to a different word in the original matrix, forming a token-to-token confusion mapping for obfuscating source code. The paper introduce a discrete optimization strategy that aligns the updated vector to the nearest valid token in the vocabulary before each gradient update."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "**Strengths:**\n\n- The paper proposed a novel approach to obfuscate the code.\n- The proposed approach can preserve better performance than other code obfuscation methods."
            },
            "weaknesses": {
                "value": "**Weaknesses:**\n\n- Missing important metrics and SOTA baselines for privacy protection\n- Missing extensive experiments on the discrete gradient search"
            },
            "questions": {
                "value": "**Questions:**\n\n- As the line 100 described, the application of code obfuscation are twofold. One is for privacy protection, and another one is for the robustness of code language models. Since the method is not aimed at improving the robustness of the code language model, why not use privacy protection metrics(i.e. TopK, which is a token-level metric that measures the percentage of correct words in the attacker's top k predictions)? You use the obfucation degree to measure the performance. How can you prove the effectiveness of privacy protection?\n- In line 272, you compared  with an identifier renaming approach(Chakraborty et al., 2022). This approach is about naturalizing source code can enhance the performance of generation on three tasks.   Why do you use this approach which is unrelated with code obfuscation? And why it's performance decrease in your experiments? TextObfuscator is a SOTA approach for preserving inference privacy. Why don't you compare with it?\n\n- In Figure 5(b), the obfuscated code still has tokens like \"password\" which is in the original code, how can you ensure the user's password \"securePassword123\" will not show in the obfuscated code?\n\n**Minor comments:**\n\n- In Table 1, the \"Origin\" method is not defined.\n- In Figure 6, missing the references of other LLMs(i.e. deepseekcoder)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes CodeCipher, which perturbs privacy from code while preserving the original response from LLMs. CodeCipher transforms the LLM\u2019s embedding matrix so that each row corresponds to a different word in the original matrix, forming a token-to-token confusion mapping for obfuscating source code. The new embedding matrix is optimized through minimizing the task-specific loss function. CodeCipher is evaluated on three AI-assisted coding tasks including code completion, summarization, and translation. Results show that CodeCipher successfully confuses the source code while preserving the original LLM\u2019s performance."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "This paper addresses the important research problem of preserving code privacy when submitting code to LLMs. It introduces an interesting approach to achieve this by transforming the LLM\u2019s embedding matrix into a new one, creating a token-to-token confusion mapping that obfuscates the source code.\n\nThe effectiveness of the proposed approach is demonstrated through three code analysis tasks."
            },
            "weaknesses": {
                "value": "The feasibility of CodeCipher depends on access to the LLM's embeddings, which raises some concerns. First, if the LLM is closed-source, are the embeddings accessible? Could the authors clarify this? Second, if the LLM is open-source, I question whether the proposed approach is necessary. With access to trained LLMs for local execution, code privacy may no longer be a significant issue.\n\nSecond, adaptive attacks are not discussed in this paper. If an attacker understands how the approach works, could they potentially generate the confusion mapping themselves and deobfuscate the obfuscated code?"
            },
            "questions": {
                "value": "1. Discuss how to obtain LLM's embeddings if LLMs are closed-source.\n2. Clarify the motivation of CodeCipher when LLMs are open-source. \n3. Discuss adaptive attacks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces CodeCipher, a method designed to transform input code sequences into an obfuscated format to mitigate the risk of data leakage when using cloud-based large language models (LLMs). Specifically, the approach modifies the embedding matrix of the LLM, ensuring that each row corresponds to a different word for the purpose of obfuscation. To optimize this transformation, a discrete gradient search algorithm is employed, leveraging the vector representation of the nearest word. Extensive experiments, including performance preservation, privacy protection, ablation studies, and transferability assessments, validate the effectiveness of the proposed technique."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The research problem is interesting. With the wide use of LLMs, the security problem is essential.\n- The evaluation is from different aspects, which is good."
            },
            "weaknesses": {
                "value": "- The performance of the obfuscated models has an evident decrease compared with the original models across three tasks in Section 5.2. \n- Some important technical details are missing, especially in the discrete gradient search algorithms. Why is single gradient computation inadequate? What is the motivation behind using the nearest valid token, and how do we get the nearest? The motivation part of the proposed algorithm can be strengthened.\n- The prompt used is casual and does not have a rigorous design.\n- The analysis of the transferability is weak. The ability to generalize is an interesting and important finding. However, more analysis should be discussed to explain why it works across closed models."
            },
            "questions": {
                "value": "- In Section 5.2, why only choose 200 samples from 2 million samples of CodeSearchNet and how to get the compilation rate using code snippets?\n- Will performance be affected by the training epochs?  How to get the best hyper-parameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}