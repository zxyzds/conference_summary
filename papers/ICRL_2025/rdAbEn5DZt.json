{
    "id": "rdAbEn5DZt",
    "title": "Joint Gradient Balancing for Data Ordering in Finite-Sum Multi-Objective Optimization",
    "abstract": "In finite-sum optimization problems, the sample orders for parameter updates can significantly influence the convergence rate of optimization algorithms. While numerous sample ordering techniques have been proposed in the context of single-objective optimization, the problem of sample ordering in finite-sum multi-objective optimization has not been thoroughly explored. To address this gap, we propose a sample ordering method called JoGBa, which finds the sample orders for multiple objectives by jointly performing online vector balancing on the gradients of all objectives. Our theoretical analysis demonstrates that this approach outperforms the standard baseline of random ordering and accelerates the convergence rate for the MGDA algorithm. Empirical evaluation across various datasets with different multi-objective optimization algorithms further demonstrates that JoGBa can achieve faster convergence and superior final performance than other data ordering strategies.",
    "keywords": [
        "multi-objective optimization"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=rdAbEn5DZt",
    "pdf_link": "https://openreview.net/pdf?id=rdAbEn5DZt",
    "comments": [
        {
            "title": {
                "value": "Proper citation and credit to (Chen et al., 2024)"
            },
            "comment": {
                "value": "Dear Authors of ICLR submission 10113,\n\nThank you for your interesting work and for citing our paper [a].\n\nTo enhance the quality of your introduction and related works, and to include more recent results, we suggest incorporating the following references:\n\n- Please consider adding a citation to [a] on lines 45, 125-126, 132-133.\n- Additionally, for lines 138-141, you may include:\n    - \"The convergence of the SMG algorithm proposed in (Liu & Vincente, 2021) without assuming the Lipschitz continuity of $\\lambda^*(\\boldsymbol{w})$ was first established in [a, Theorem 8].\"\n    - \"The bias in the CA direction can also be mitigated via double sampling, as introduced in [a].\"\n\n\n\nI found that in your proofs, the following contents are almost identical to some in [a]. \n\n| Contents in your paper | Contents in [a]|\n|--|--|\n| lines 698-708 (Section B.1) |page 48-49, Eqs.(C.22)-(C.23)|\n| lines 758-780 (Section B.3, Lemma B.1) |page 37-38, Lemma 11|\n| lines 1015-1058 (Section B.3, Lemma B.4) |page 16-17, Lemma 2 and its proof|\n| lines 1208-1251 (Section B.5) |page 42-43, Section B.4|\n\n\nAs [a] was made available on arXiv in October 2023 (https://arxiv.org/pdf/2305.20057) and subsequently published in JMLR (https://jmlr.org/papers/volume25/23-1287/23-1287.pdf) in May 2024, we kindly request that you provide a proper citation to [a] in your Lemma B.1, Lemma B.4, and Section B.5 to give us proper credit.\n\n\nAdditional suggestions to improve the quality of your proof of Theorem 3.6: \n- $Q_t, \\lambda_{Q_t}^*$ in (4) in Section B.1 are not defined in the prior context.\n- $\\nabla F_S(x_t)$ in (4) and (7) in Section B.1, and Section B.5 should be $\\nabla \\mathcal{L}(\\boldsymbol{w}_t)$ in your paper?\n\n\n\nThanks for considering our suggestions. Giving us proper credit encourages us to continue to conduct innovative research.\n\n\nSincerely,\n\nAuthors of paper [a]\n\n\n\n[a] Lisha Chen, Heshan Fernando, Yiming Ying, and Tianyi Chen. \"Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance\". Journal of Machine Learning Research, 25(193):1\u201353, 2024."
            }
        },
        {
            "summary": {
                "value": "The paper proposes a novel framework JoGBa for sample ordering in finite-sum multi-objective optimization problem, and demonstrate its empirical effectiveness on various MOO methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Data ordering for better optimization performance (that can especially outperform random shuffling) is important, and I think bring this concept to multi-objective optimization is an interesting and important contribution.\n\nThe paper theoretically analyze the convergence of MGDA for finite-sum MOO problem, with both random shuffling and JoGBa ordering.\n\nEmpirical results look promising."
            },
            "weaknesses": {
                "value": "For audience like me that is kind of familiar with MOO but not sample ordering, I think the presentation of algorithm 1 and figure 1 can be improved. Specifically, \n(1) I cannot directly see from Figure 1, how the JoGBa approach (Fig 1 Right) is different from data ordering on each objective separately (Fig 1 Middle). In other words, Figure might not be illustrative enough.\n(2) Algorithm 1 contains numerous superscripts and subscripts, making it challenging to interpret (which is acceptable if it prioritizes rigor). However, a clearer description might help\u2014for example, one or two sentences explaining how JoGBa differs from ordering data for each objective separately. From the current description in lines 176 to 178, \"The sample ordering is then determined based on the results of solving the balancing problem (routine Balancing) with the gradient on each objective,\" it\u2019s still unclear (at least for me) how this approach differs from the one illustrated in the middle of Figure 1.\n\nI believe the paper will be a clear accept for me, if the authors can clarify my concern or improve the presentation of the main methodology as mentioned above."
            },
            "questions": {
                "value": "See the question in previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a novel sample order approach for multi-objective optimization problems. The method is inspired by the online vector balancing problem which tries to make the average of the centered gradient as close as to zero. Compared with different existing sample order approaches, the author shows that the proposed algorithm can achieve faster convergence. The experimental results on the NYUv2 and QM9 data sets also support the theory because the proposed algorithm outperforms different baseline methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Novel Approach to Multi-Objective Optimization: Inspired by the online vector balancing problem, The JoGBa method introduces a unique framework for ordering samples in multi-objective optimization by jointly balancing gradients across objectives . This is a new contribution to the multi-objective optimization field.\n\nTheoretical Convergence Rate: The paper provides a thorough theoretical analysis. The convergence proofs are grounded in established assumptions, lending credibility to the claims.\n\nExtensive Empirical Validation: JoGBa is validated on multiple datasets (e.g., NYUv2 and QM9) across diverse multi-objective optimization algorithms, such as MGDA, PCGrad, and Nash-MTL. This empirical diversity strengthens the generalizability of the findings."
            },
            "weaknesses": {
                "value": "Improved Convergence and Performance not clear: The authors claims that the method consistently outperforms existing data ordering strategies and dynamic weighting approaches. However, it seems the convergence rate for different algorithms are similar based on Theorem 3.6 and Theorem 3.7.\n\nPresentation and Clarity: Certain sections, especially the theoretical parts and Algorithm 1, could be more reader-friendly. A more structured breakdown of steps and implications of theoretical results would enhance readability, making the technical details accessible to a broader audience.\n\nIntuitions not sufficient: the online vector balancing problem is the core of balancing the gradient, but the authors fail to give enough intuition and explanation about how to implement the method across different objectives."
            },
            "questions": {
                "value": "1.\tIn line 189, the stale mean should be $\\nu_{t+1}$. You write it as $m_{t+1}$. Please double check the notation.\n\n2.\tIn Line 226, a necessary and sufficient condition for $\\lambda$ not $x$.\n\n3.\tIn Line 654, \u201cbeginning\u201d instead of \u201cbegining\u201d.\n\n4.\tIn Definition 3.1, the authors mentioned online vector balancing which is the core of how to balance the gradient. If possible, please try to explain more clearly about the intuition (e.g., online vector balancing problem tries to make the average of the centered gradient as close as to \u201czero\u201d). This will make the reader have a smooth reading experience.\n\n5.\tIn Theorem 3.6 and Theorem 3.7, it seems the convergence rate of random sample ordering and the proposed algorithm are the same. Though in Theorem 3.6, the convergence rate is \u201c=\u201d while in Theorem 3.7, it is \u201c<=\u201d, one cannot conclude that the proposed algorithm converges faster than random sample ordering. The experimental results also show that the proposed algorithm converges just a little bit faster than other methods. Please try to explain clearly about the advantage of the proposed algorithm (maybe add some remarks/comments to give comparisons of different algorithms both theoretically and experimentally).  \n\n6.\tIf possible, let the reader know that the paper has a clear set up of the experiments in the appendix. This could help practitioners better understand the method\u2019s sensitivity and optimize it for specific tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors in this paper modify the classical method in finite-sum multi-objective optimization such as MGDA algorithm by developing a sample ordering rule. JoGBa jointly balances gradients from multiple objectives during optimization. It is demonstrated for both theoretical analysis and empirical results, the proposed sample ordering outperforms random ordering."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The main idea of this paper is clearly presented through the incorporation of the MGDA algorithm with a novel sample ordering method. Figure 1 offers a great visualization and improves the audience's understanding. The numerical results demonstrate the practical superiority of the proposed algorithm."
            },
            "weaknesses": {
                "value": "1. line 65 typo \"oerdering\"\n2. needs to intuitively explain the idea of online vector balancing in the introduction\n4. for better presentation, the balancing routine could be written as an algorithm instead of a definition\n5. The authors need to better explain the balancing routine\n6. The algorithm is complicated, the authors should provide with more motivation, remarks, and discussion about the design of algorithm. \n7. need to discuss the complexity to solve the balancing problem\n8. typo at the end of line 19 of Algorithm 1 \n9. typo in Assumption 3.3 \n10. In the statement of Theorem 3.6 and 3.9, the definitions for $w$ and $\\sigma$ are missing. \n11. As a remark of Theorem 3.9, the authors might want to do discuss about the step size (depending on T). As T going large, the step size is diminishing. But why the converge rate is similar compare to Theorem 3.6 where a fixed step size is used. \n12. the authors should explain the third and fourth terms in Theorem 3.9. \n13. There is a gap between (2) and (13), so as Theorem 3.9 and its proof. I'm convinced with the proof and the theorem statement. \n14. The authors need to discuss about the computation cost for the balance problem in the numerical section to make a fair comparison."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}