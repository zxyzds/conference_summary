{
    "id": "BJ9mzoSeu1",
    "title": "Personalized Federated Learning via Variational Massage Passing",
    "abstract": "Conventional federated learning (FL) aims to train a unified machine learning model that fits data distributed across various agents. However, statistical heterogeneity arising from diverse data resources renders the single global model trained by FL ineffective for all clients. Personalized federated learning (pFL) has been proposed to primarily address this challenge by tailoring individualized models to each client's specific dataset while integrating global information during feature aggregation. Achieving efficient pFL necessitates the accurate estimation of global feature information across all the training data. Nonetheless, balancing the personalization of individual models with the global consensus of feature information remains a significant challenge in existing approaches. \nIn this paper, we propose pFedVMP, a novel pFL approach that employs variational message passing (VMP) to design feature aggregation protocols. \nBy leveraging the first-order and second-order statistical information, pFedVMP yields more precise estimates of the distributions of model parameters and global feature centroids.\nAdditionally, pFedVMP is effective in boosting training accuracy and preventing overfitting by regularizing local training with global feature centroids.\nExtensive experiments on heterogeneous data conditions demonstrate that pFedVMP surpasses state-of-the-art methods in both effectiveness and fairness.",
    "keywords": [
        "Personalized federated learning",
        "variational message passing",
        "feature representation learning"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "This paper introduces pFedVMP, a personalized federated learning method that enhances model accuracy and fairness by leveraging variational message passing for improved feature aggregation across heterogeneous data.",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=BJ9mzoSeu1",
    "pdf_link": "https://openreview.net/pdf?id=BJ9mzoSeu1",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a Bayesian approach to dealing with personalized federated learning (pFL). In particular, a \"shared\" base model is learned to map inputs to representations and each client learns a local head model to turn those representations into prediction outputs. In their approach, distributions of parameters of the base and head models are learned. In addition, the distribution of the representations are considered via a GMM (mixing on true labels). Locally, updates for the base, head, and GMM are learned. For global aggregation the base and GMM models are updated. The paper consider the case where the relevant distribution of the base and head models are Gaussians. Their approach is tested across various vision datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The approach shows promising experimental results, showing better results than the baselines reported\n- The overall approach make intuitive sense, combining ideas from pFL and Bayesian FL."
            },
            "weaknesses": {
                "value": "- The primary weakness of the paper is in part of its presentation. This is particularly the case for Section 4 where conceptual optimization goals is mixed in with practical simplifications.\n - In addition, there are no detailed derivation for some of the quantities used (in main text nor appendix)."
            },
            "questions": {
                "value": "Questions + Remarks:\n\n1. $p({\\theta}^{\\\\rm b}, \\\\{ \\theta_n^{\\\\rm h} \\\\}, \\\\{ z_k \\\\}, S)$ is a distribution over parameters (+ representations) and samples $S$. But, as far as I can tell from the equations, the surrogate distribution $q$ being consider is only over parameters (+ representations). As such, it is unclear how the KL-divergences are being evaluated, eg, (P1) including the argmin.\nPlease provide clarity on this support issue of the distributions and how the KL-divergence is being evaluated.\n\n2. From what I understand, when (P1) is referred in text, it only refers to the parameter updates and not the variational / KL-divergence aspect over the equation. This is rather unclear in Section 4.2.\nPlease clarify this (P1), perhaps by presenting the entire optimization in multiple line (labeling as (P1a) and (P1b) for instance).\n\n3.  I think additional clarity in the text should also be added to distinguish section which are considering the update of parameters in (P1) (Section 4.3) vs updates on the distribution in (P2) (Section 4.2 & 4.3). This aspect is also mixed in Section 4.3.1. It may be worth splitting this subsubsection into two separate subsubsections, one for local updates on the parameters and one for local updates on the distributions.\n\n4. Is (P2) and (P3) equivalent (when restricting optimization to local parameter etc)? Line 269-270 says that the optimization is converted. Does this imply equivalence?\n\n5. The soundness of going from (P3) to (8) is a bit unclear. Could you please elaborate on the derivation (which I believe is just maximizing (7)) and why it is \"low-cost implementation of SG-MCMC\".\n\n6. (P2) seems to be imprecise. In particular, the second term in the KL is not a normalized distribution? In particular, several \"prior\" distribution seem to be missing in (P2) and the accompanying text. Please clarify this.\nIt would also be useful for completeness to include a derivation of the specific factorization you are using; and its subsequent use in (P3).\n\n---\n\nMinor:\n - References were cut off from the main text.\n - The subscript of the max in (1) and (P1) is not very nice. Maybe having the subscript fully under the \"max\" would improve readability of these equations.\n - $\\\\mathcal{Z}_{k,n}$ seems to be incorrect on line 160 (should have $k$ instead of $y_k$)\n - I think \\bigcup $\\\\bigcup$ is typically used over \\cup $\\\\cup$ for indexed unions.\n - (1) and 2) on line 42 - 44 are not consistent\n - Line 240 in denominator, there is a missing bracket\n - (P1-3) should be on the RHS to be consistent with equation numbering (maybe via \\tag)\n - Figure 3, missing space after \"Upper:\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a personalized federated learning algorithm based on Bayesian estimation. The core idea is that clients learn a global shared model while they train a personalized head."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Personalization in federated learning by splitting the model into two parts such that one part is learned globally while the head is learned locally is proved to be effective. The contribution and motivation of the paper is clear."
            },
            "weaknesses": {
                "value": "The novelty of the paper is not high in my opinion. The core idea has been proposed before in the literature. One of the first work that I know which employ the same idea for personalized federated learning is FedRep of Collins et al., (2021). However, Collins et al., (2021) solves the problem using optimization. It seems that this paper solves the problem using Bayesian. Furthermore, Bayesian federated learning has been studied extensively before. Although, the paper compares the proposed algorithm against set of baselines, I think the paper misses the comparison with FedRep which is closely related to the study of this paper."
            },
            "questions": {
                "value": "I am not expert at Bayesian learning and I cannot evaluate the novelty of this work in this aspect. Can you explain the key differences between your proposed algorithm prior Bayesian federated learning? My concern is that if we can apply prior Bayesian federated learning to solve the problem which has been already studied by convex optimization techniques."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "pFedVMP is a personalized federated learning approach that uses variational message passing to enhance feature aggregation, yielding more precise model parameter estimates and improving training accuracy and fairness under heterogeneous data conditions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper provides a nice numerical study with SOTA baselines with interpretation, ablation study, and fairness analysis."
            },
            "weaknesses": {
                "value": "First of all, there is a typo in the title of this paper: \"Massage Passing\" should be \"Message Passing\"...\n\nW1. The Bayesian benchmark models are not included in the comparison. \n\nW2. The computational cost seems to be high, especially with high-dimensional features. \n\nW3. The selection of hyperparameters needs justification. \n\nW4. There is a lack of theoretical guarantees for the proposed method."
            },
            "questions": {
                "value": "Q1. The paper presents extensive comparisons with various methods in federated representation learning but fails to include benchmark models within the framework of Bayesian federated learning, for example, BNFed, pFedGP, pFedBayes, FedPA, FedEP, QLSD, and others. This weakens the argument for the superiority of the proposed method in the Bayesian context.\n\nQ2. The pFedVMP algorithm involves numerous matrix inversions in each communication round (especially in Equation 12), which can lead to a significant computational burden, particularly with high-dimensional features. It is essential to evaluate the computational cost relative to other methods and propose reasonable solutions to mitigate these costs.\n\nQ3. The algorithm contains several hyperparameters, such as those in Equations 8 and 10. A more in-depth study on the impact of these hyperparameters and a clear justification for their selection is necessary.\n\nQ4. There is a lack of theoretical guarantees for the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "- This paper presents a personalizd federated learning methods, called pFedVMP, as a new solution to tailed personalized model for local clients. The core idea is to model both the centorids of parameters and features."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The combination of modeling both feature space and parameter space seems to be good as shown in the reported results of the experiments."
            },
            "weaknesses": {
                "value": "- The main ideas of pFedVMP, modeling parameters and feature centroids, are not new. A Bayesian perspective is either an well-explored areas in federated learning.\n- Even tough this area is well-explored in recent years, most of the baselines in the experiments are not the newest, which make it hard to believe to be SOTA. Moreover, the most related works are not included as a baseline of Bayesian Federated Learning.\n    1. Related and new baselines are recommended as followings: feature modeling [1,2] and parameter modeling [3,4].\n    2. MOON [1] has similar claims about feature centroids modeling, and however is not discussed.\n    3. PRIOR [4] emphasizes the importance of global prior information which is the parameter centroid, which has not been discussed yet.\n    4. More comparison about works after year 2023 should be added beyond FedPAC (ICLR 2023) in order to be claimed as SOTA.\n- Some words, e.g., leveraging second-order statistical information, are confusing in the federated learning. The ambiguous words in this comprehensive field can refer to second-order moments, covariance matrices, or second-order gradients, Hessian matrices.\n\n[1] Model-contrastive federated learning. CVPR 2021\n\n[2] FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy. KDD 2023\n\n[3] Personalized Federated Learning via Variational Bayesian Inference. ICML 2022\n\n[4] PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning.  NeurIPS 2023"
            },
            "questions": {
                "value": "- What's the main difference between the propoed pFedVMP and the methods use GMM to model the feature centroids or parameter centeroids?\n- The main claim, the global feature centroids is important, is already a common sense in the literature of federated learning, which is first systematically claimed and proven by MOON [1] as far as I know. What's more?\n- If the difference is clearly explained, a positive rating is considered.\n\n[1] Model-contrastive federated learning. CVPR 2021"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}