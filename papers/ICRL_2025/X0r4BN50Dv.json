{
    "id": "X0r4BN50Dv",
    "title": "F-Fidelity: A Robust Framework for Faithfulness Evaluation of Explainable AI",
    "abstract": "With the rapid development of eXplainable AI, various methods are proposed to explain attributions, such as Integral Gradient, and SmoothGrad. However, how to measure the faithfulness of explanations maintains an open question. The most popular removal strategy suffers from the Out-of-Distribution(OOD) problem. The RemOve And Retrain and Importance Measure try to solve the OOD problem by retraining while suffering information leakage and convergence problems. To address these problems and provide a robust evaluation method for faithfulness measurement, we propose a new method, Fine-tuned Fidelity(F-Fidelity).  It alleviates the OOD problem by using consistent augmentation operations in fine-tuning and evaluation stages to reduce the gap between the training set and evaluation inputs. To verify the effectiveness of F-Fidelity, we proposed a fair comparison strategy employing various degraded explanations. We have conducted experiments on Image and Natural Language Processing classification tasks with two datasets and two architectures for each task. The results demonstrate the generality, and robustness of F-Fidelity.",
    "keywords": [
        "Explainable AI",
        "Evaluation Metric",
        "Computer Vision"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=X0r4BN50Dv",
    "pdf_link": "https://openreview.net/pdf?id=X0r4BN50Dv",
    "comments": [
        {
            "summary": {
                "value": "The authors propose a fidelity evaluation framework which utilizes an explanation agnostic fine tuning and a random mask generator which ensures that the generates input is not OOD. The efficacy of the proposed method if shown in various data types like images, time series etc."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The paper deals with important aspect of fidelity evaluation\n2) The authors discuss about OOD issue during fidelity evaluation\n3) The authors propose an approach to handle the issues and show the performance in different data types like images, time series and NLP."
            },
            "weaknesses": {
                "value": "W1) In section 3, it is mentioned in lines [215 to 222] that the size of the removed part is upper bounded by a fraction of the input and not a fraction of the explanation. It would be helpful to the readers if the authors could explain what they mean by \"removed,\" as it might mean different in tabular data and images. Further, the authors should also explain how their strategy of selecting a fraction of input and not explanation addresses the OOD issue. \n\nW2) In lines[224:235], the authors use fine tuning to address the problem of robustness of the classifier for small perturbations(mentioned in lines [213:214]). However, at least from the perspective of images, it is unclear how applying a mask of {0,1} on random patches of an image can be considered as a perturbation.\n\nW3) Further, the fine-tuning process mentioned in lines[224:235] seems like retraining the classifier with occlusion (for image classifiers). The authors should explain how their method won't change the decision boundary drastically so that their approach can be called fine-tuning and not training a different classifier altogether.\n\nW4) The authors should explain why explaining the fine-tuned classifier is equivalent to explaining the original classifier (as mentioned in line[224])\n\nW5) For the ease of readers, the authors should explain more about GT and how they extract it(in Section 4)\n\nW6) It needs to be clarified to see why F-Fidelity performs worse than vanilla Fidelity and R-Fidelity for SST2 dataset with LSTM (Table 8). The authors are requested to explain this phenomenon in detail for the ease of readers."
            },
            "questions": {
                "value": "Questions are as below\n\n#### W1:\n- a) Provide specific examples of how \"removal\" is implemented for different data types like images, tabular data, and text.\n- b) Give a more detailed explanation of how bounding the removed portion to a fraction of the input, rather than the explanation, helps mitigate OOD issues.\n\n#### W2:\n- a) Elaborate on how the random masking process relates to or simulates small perturbations, particularly in the context of image data.\n- b) Provide examples or visualizations that demonstrate how this masking approach compares to traditional perturbation methods.\n- c) Present empirical evidence or theoretical justification for why their masking approach is an effective way to improve classifier robustness.\n\n#### W3:\n- a) Provide empirical evidence or theoretical justification for why their fine-tuning process preserves the original classifier's decision boundaries.\n- b) Compare the original and fine-tuned classifiers' outputs on a test set to demonstrate the degree of change.\n\n#### W4:\n- a) Provide theoretical or empirical evidence demonstrating the equivalence between explanations of the original and fine-tuned classifiers.\n- b) Include a comparison of explanations generated for both classifiers on a set of test examples.\n\n#### W5:\n- a) Provide a step-by-step description of how they generate or obtain the ground truth (GT) explanations, including any assumptions made in this process.\n- b) Include examples of GT explanations for different data types.\n\n#### W6:\n- a) Provide a detailed analysis of why F-Fidelity underperforms on this specific dataset and model combination.\n- b) Investigate potential reasons, such as dataset characteristics, model architecture specifics, or hyperparameter settings that might contribute to this result.\n- c) Discuss the implications of this finding for the broader applicability of their method.\n- d) Consider conducting an ablation study to isolate the factors contributing to this performance difference.\n\nThe paper addresses an important aspect of fidelity evaluation and the issue of OOD while doing it. Overall, I like the central theme of the paper and that\u2019s why I am leaning towards mild acceptance but I would like the authors to address the points in the weakness section and the questions in this section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a new attribution evaluation framework which attempts to solve existing issues in current metrics such as the OOD problem from perturbation metrics, and the information leakage of fine-tuning methods like ROAR. To do this, the authors propose limiting the size of the perturbations made to each image and performing fine-tuning on images randomly masked under this perturbation size limitation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "From their experiments which cover a broad range of applications and datasets, it seems this new method does improve over existing methods in rank correlation tests, indicating its ability to fairly and consistently sort attributions under evaluation. \n\nThe solutions presented are simple."
            },
            "weaknesses": {
                "value": "I do not think the intuition behind the solutions is entirely clear and I do not think the presentation of the solutions is clear.  \n\nThe choices for multiple hypermeters are not indicated. \n\nMore attribution methods should have been employed for a better representation of the existing space."
            },
            "questions": {
                "value": "The ablation study provided regarding B selection suggests this value can approach 1 (i.e. this is the same as if the value was not implemented) and the correlation scores would be good. Is this true? If so, what use does B have? In addition, what value of B is used and how was it selected? \n\nWhat is the intuition for fine tuning with the image perturbation limitation? I do not see why fine-tuning under image perturbed by random masks instead of perturbed explanations is better either. It is not obvious that this skirts the OOD problem. \n\nWhat is the value of being able to extract the explanation size? The motivation for this is lacking and it is not clear why it is important or interesting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The ablation study provided regarding B selection suggests this value can approach 1 (i.e. this is the same as if the value was not implemented) and the correlation scores would be good. Is this true? If so, what use does B have? In addition, what value of B is used and how was it selected? \n\nWhat is the intuition for fine tuning with the image perturbation limitation? I do not see why fine-tuning under image perturbed by random masks instead of perturbed explanations is better either. It is not obvious that this skirts the OOD problem. \n\nWhat is the value of being able to extract the explanation size? The motivation for this is lacking and it is not clear why it is important or interesting."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors presented F-fidelity, a new metric for evaluating the faithfulness of attribution methods. Traditional evaluation methods such as MoRF and LeRF suffer out-of-distribution(OOD) issue, and ROAR method suffers information leakage issue. F-fidelity overcomes these limitations through two strategies. 1. explanation-agnostic fine-tuning, which fine-tunes the model using stochastic masking operations. 2. a controlled random masking operation to overcome the OOD issue, by applying a random masking operation conditioned on the explainer's output so that removal of input features does not produce OOD samples. The authors conducted controlled experiments with other explainers. Moreopver, they showed that F-fidelity can be used to determine the sparsity of influential input components."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well written, especially introduction section and preliminary section.\n\nThe paper effectively tackles the OOD problem inherent in traditional removal-based evaluation methods. This is a crucial issue because OOD inputs can lead to unreliable assessments of an explainer's faithfulness.\n\nThe authors conducted extensive experiments across broad data, such as images, time series, and natural language processing.\n\nThe method is well mitigate limitation of prior methods; Addresses the information leakage issue and OOD issue at once.\n\nThe paper provides theoretical analysis demonstrating that F-Fidelity can recover the sparsity of the ideal SHAP explanations.\n\nThe paper introduces a novel approach by assuming that an explainer is better the closer it is to an ideal Shapley value explainer. This assumption is appealing because Shapley values provide a theoretically sound basis for feature attribution."
            },
            "weaknesses": {
                "value": "Lack of Visualizations Demonstrating F-Fidelity's Effectiveness. The paper does not include figures or visualizations that illustrate the effectiveness of F-Fidelity. Providing attribution maps or examples where there is a significant discrepancy between F-Fidelity and other evaluation metrics would enhance understanding and make the results more tangible. Visual comparisons could help readers better appreciate the advantages of F-Fidelity over existing methods. For example, Attribution map between samples that have discrepancy between F-Fidelity and MoRF - LeRF.\n\nNovelty issue. Though theoretical availability of extracting the size of the ground-truth explanation is novel, the core difference between F-fidelity and R-Fidelity is introducing an upper bound on the fraction of input elements removed. This extension seems an incremental refinement rather than a fundamental novel idea.\n\nLimited Explanation of Ground Truth. The paper's approach to establishing ground truth rankings involves using Integrated Gradients with systematically added noise levels. However, Integrated Gradients is known to be less faithful than some state-of-the-art explanation methods, such as the LRP (Layer-wise Relevance Propagation) family or LayerCAM. This raises concerns about whether the ground truth used is an appropriate benchmark. As the authors discuss ideal Shapley value explainers in Section 5, utilizing Shapley values as the ground truth might have been more appropriate to align with their theoretical framework.\n\nInsufficient Clarification on the Importance of Macro Correlation. The paper could benefit from a more thorough explanation of why macro correlation between different evaluation methods is important. It was not immediately clear how macro correlation demonstrates the usefulness of F-Fidelity. Including figures or visual aids that depict the relationship between macro correlation and the effectiveness of F-Fidelity would help clarify this point and strengthen the argument.\n\nLimited Variety of Explanation Methods Evaluated. The experiments involve a relatively small number of explanation methods. Comparing F-Fidelity scores across a wider range of explainers would provide a more comprehensive evaluation. For instance, methods like LayerCAM and LRP family are known to have higher fidelity than GradCAM, and GradCAM typically outperforms the pure gradient. Demonstrating whether F-Fidelity can reflect these known differences in fidelity would enhance the paper's validity.\n\nPractical Challenges in Determining Explanation Size (c\u2081). In the section 6, the size of the most influential tier (c\u2081) is assumed to be known. However, in real-world applications, c\u2081 is not readily observable. It would have been beneficial if the authors had demonstrated how c\u2081 could be estimated or inferred from practical data, such as through experiments on datasets like ImageNet. Providing practical guidance on determining c\u2081 would make the theoretical contributions more applicable."
            },
            "questions": {
                "value": "1. I couldn't find how Pan et al., 2021, or Zhu et al., 2024 utilized macro correlation in their work. Can you explain how they employed macro correlation and how it relates to your citation of their work?\n\n2. In Section 6, it seems that both MoRF and LeRF conditions would be satisfied equally. This is because deleting or inserting features based on the gamma value would maximize the MoRF-LeRF difference. I'm uncertain whether this experiment effectively demonstrates that F-Fidelity can distinguish explainers that are closer to the ideal Shapley explainer than other metrics. Could you clarify how this experiment supports that claim?\n\n3. Regarding computational resources, I would like to know how much computational effort your method requires. One of the primary reasons ROAR isn't widely adopted is not just due to information leakage but also because of its significant computational cost. It would be helpful if you could provide details on the computational resources required for F-Fidelity and how they compare to those needed for ROAR.\n\n** Typo : in figure 1, the y-axis label is RFid."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}