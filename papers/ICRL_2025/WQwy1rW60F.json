{
    "id": "WQwy1rW60F",
    "title": "LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K",
    "abstract": "State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of 256k or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (5k-21k), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces LV-Eval, a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of LV-Eval has incorporated three key techniques, namely confusing facts insertion, keyword and phrase replacement, and keyword-recall-based metric design. The advantages of LV-Eval include controllable evaluation across different context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluations. We evaluate 15 LLMs on LV-Eval and conduct ablation studies on the benchmarking techniques. The results reveal that:\n(i) Moonshot-v1 and recent large-scale open-source models, such as Qwen-2.5-72B and Llama-3.1-70B, achieve the highest performance on LV-Eval, particularly at lengths below $64k$. (ii) Models exhibit distinct score trends. For example, GLM-4-9B-128k, Yi-6B-200k, and Llama3-8B-1M exhibit a relatively gentle degradation of performance, but their absolute performances may not necessarily be higher than those of LLMs with shorter context lengths. (iii) LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of \"needle in a haystack\". (iv) Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation, and these concerns are alleviated in LV-Eval.",
    "keywords": [
        "large language model",
        "long-context benchmark",
        "knowledge leakage mitigation"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "LV-Eval is a long-context benchmark with 5 length levels up to 256K. It's designed to be challenging, suitable for controllable comparison, and mitigates knowledge leakage issue in evaluation.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=WQwy1rW60F",
    "pdf_link": "https://openreview.net/pdf?id=WQwy1rW60F",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a new LLM eval benchmark called LV-Eval to assess LLMs longer-context QA capability up to 256k words. LV-Eval incorporates a few techniques (e.g., confusing facts insertion, keyword manipulation) and adopts a new eval metric, which provides better/more reliable LLM evaluation. Authors conduct experiments on 15 different LLMs and demonstrate how performance can vary significantly in different testing scenarios."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. A new dataset that could potentially benefit the community\n2. Paper presentation is overall clear\n3. The proposed benchmark construction method is overall reasonable"
            },
            "weaknesses": {
                "value": "The quality (and usefulness) of proposed benchmark is not fully evaluated. How do state-of-the-art LLMs and human (both domain experts and lay people) perform on this dataset? Does this benchmark capture LLM's full capability (other than knowledge extraction/manipulation capabilities for the QA tasks)? In multiple places in the paper, the authors mention human interventions (e.g.,  Line 269, \u201cask human annotators to resolve any conflicts in generated facts\u201d), more analysis/discussions on the human annotation qualities are needed. \n\nThe second limitation is its construction process is not very clearly discussed. For example, around line 254, the authors mention that \u201cFor each length level, we sample distracting documents one by one until the cumulative word count meets the desired length level\u201c, how does the distracting document sampling work exactly? Furthermore, as mentioned in the previous limitation, for each place where human annotation/intervention is involved, more discussions (e.g., # of annotator, annotation guideline, human annotation agreement numbers, etc) are needed. I would recommend authors moving some part of current Appendix section B details to the main text. \n\nFinally, as mentioned by the authors in the limitation section, the current benchmark only include QA tasks. As a result, the proposed new evaluation metric also only targets short-form answers (compared to long-text generation). A natural question is that what if we use another LLM auto-rater to judge if two answer strings match semantically (instead of the lexical based methods)."
            },
            "questions": {
                "value": "Please refer to the questions in the above weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Authors propose a long-context benchmark dataset for Large Language Models, with 5 gradually increasing length levels - 16k, 32k, 64k, 128k, and 256k tokens. The datasets consists of single-hop and multi-hop QA examples. The dataset was constructed using techniques such as content mixing up, keyword and phrase replacement and confusing facrs insertion. Authors have also conducted evaluation of 15 LLMs on the proposed benchmark. The insights from this evaluation includes: 1) Moonshot-1 and Qwen2.5-72B and LLaMA-3.1-70B achieved top performance; 2) Within the group of LLMs with 6-9B parameters, GLM-4-9B achieves the best results; 3) Score trends on various length levels differs substantialy between LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The dataset design approach with 3 options for confusing evaluated LLMs allows to take a broader look on LLM capabilities in dealing with potentially out-of-distribution content. This could be a substantiall addition to the set of benchmarks for thorough examination of LLMs in the context of their generalization.\n2. 5 different length levels allows to precisely point out how model is able (or unable) to recall information from different parts of the context.\n3. The inclusion of bilingual datasets helps assess the efficacy of long-context recall and understanding in two vastly different languages,\n4. Authors evaluated multiple open-source LLMs with various context window sizes. This helps to assess the efficacy of different approaches to increasing the context window size for different models."
            },
            "weaknesses": {
                "value": "1. The conducted evaluation of LLMs on the LV-Eval benchmark included only 3 closed-source models. It is even more confusing that among those 3 models 2 are very outdated versions of GPT-3.5 and GPT-4. This paper would immensely benefit from inclusion of at least relatively recent closed-source LLMs, such as GPT-4 with 128k context window (which was released almost a year ago to this date), along with Anthropic Claude, which shows remarkable performance in long-context recall. The argumentation of high-cost is slightly confusing - particualrly most recent closed LLMs have been much less expensive compared to their previous versions. This might not apply to some models from Anthropic, but it certainly does apply to GPT-4. The price of GPT-3.5 with 16k context window was 3$ per 1m tokens, for GPT-4 is was 30-60$ per 1m tokens, while most recent GPT-4o pricing is 2.5$ per 1m tokens.\n2. Confusing Facts Insertion relies entirely on GPT-4 internal concept of a \"confusing fact\". This yields a problem of data leakage - GPT-4 may have an unfair benefit to other models on this benchmark."
            },
            "questions": {
                "value": "1. Do you believe that using GPT-4 in confusing fact generation *will not* result in GPT-4 having an unfair advantage on this benchmark? If so, why?\n\n2.1. Have you measured an inter-annotator agreement during annotation for CFI and KPR? \n\n2.2. How many annotators have seen each example? \n\n2.3. Have you rejected any annotations due to disagreement between annotators?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper's about LV-Eval, a fresh test to see how well big language models get long contexts. It's got five levels of length and covers two types of question-answering in 11 different languages. They mixed in some tricky stuff like made-up facts and swapped out keywords to make it harder. They tried it on 15 models and saw how they did, especially when things got confusing."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. It's got a good range of lengths, which is key for seeing how models handle long texts.\n2. It's not just English\u2014it's got Chinese too, so it's more useful for different models.\n3. They tried new things to make the test harder and stop models from cheating with common knowledge.\n4. The scoring is more focused on the important bits of the answer, which makes it more accurate.\n5. They shared all the data and code, which is cool for transparency and building on their work."
            },
            "weaknesses": {
                "value": "1. It's mostly about question-answering, which might not cover everything we need for understanding long texts.\n2. Testing some models is pricey, so they couldn't check out all the new ones.\n3. They're still relying a lot on people to check the tricky parts, which takes time and can be off.\n4. There's a chance models could just learn the test, not actually get better at understanding.\n5. Models had a hard time with the confusing stuff, so maybe the test needs more of that."
            },
            "questions": {
                "value": "1. How do these results compare to other tests, and does that tell us something about how well they do in the real world?\n2. Can the authors walk us through how they checked the tricky parts and if there's a way to do it by machine?\n3. What does it mean if models use common knowledge instead of the text, and how can we fix that for future tests?\n4. Do models do as well on LV-Eval as they do on real-world tasks like summarizing or chatting?\n5. Are there certain types of long-text jobs where LV-Eval is really good or bad, and how could that change future tests?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces LV-Eval, a long-context benchmark specifically designed to evaluate language models across a range of text lengths, reaching a maximum of 256k words. This benchmark encompasses two primary tasks: single-hop and multi-hop question answering (QA), and it utilizes a total of 11 bilingual datasets to enhance its robustness. To effectively assess model performance, LV-Eval incorporates three key techniques: confusing fact insertion, keyword and phrase replacement, and a keyword-recall-based metric design. The experiments conducted on this proposed long-context benchmark yield several significant findings, providing valuable insights into the capabilities and limitations of various LLMs in handling extended contexts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The structure of this paper is coherent and well-organized, facilitating ease of reading and comprehension.\n2. This paper presents the LV-Eval benchmark, which has the potential to foster further research in the area of long-context modeling.\n3. The thorough experiments conducted in this paper provide valuable insights that assist readers in selecting the most suitable LLMs for various real-life applications and scenarios."
            },
            "weaknesses": {
                "value": "1. The contributions of this paper are somewhat limited in scope. While the techniques of confusing fact insertion and keyword/phrase replacement for data augmentation may be useful, they are relatively straightforward and lack significant innovation.\n2. To emphasize the contributions of the proposed benchmark, it would be beneficial to compare LV-Eval with more recent long-context benchmarks, such as \u201cXLBench: A Benchmark for Extremely Long Context Understanding with Long-Range Dependencies\u201d and \u201cLoong: Benchmarking Long-Context LLMs with Extended Multi-Doc QA.\u201d Such comparisons would provide a clearer context for evaluating the effectiveness and relevance of LV-Eval within the current landscape of long-context modeling."
            },
            "questions": {
                "value": "Please refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}