{
    "id": "45rvZkJbuX",
    "title": "Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models",
    "abstract": "Vision-language alignment in Large Vision-Language Models (LVLMs) successfully enables LLMs to understand visual input. However, we find that existing vision-language alignment methods fail to transfer the existing safety mechanism for text in LLMs to vision, which leads to vulnerabilities in toxic image. To explore the cause of this problem, we give the insightful explanation of where and how the safety mechanism of LVLMs operates and conduct comparative analysis between text and vision. We find that the hidden states at the specific transformer layers play a crucial role in the successful activation of safety mechanism, while the vision-language alignment at hidden states level in current methods is insufficient. This results in a semantic shift for input images compared to text in hidden states, therefore misleads the safety mechanism. To address this, we propose a novel Text-Guided vision-language Alignment method (TGA) for LVLMs. TGA retrieves the texts related to input vision and uses them to guide the projection of vision into the hidden states space in LLMs. Experiments show that \\textbf{TGA} not only successfully transfers the safety mechanism for text in basic LLMs to vision in vision-language alignment for LVLMs without any safety fine-tuning on the visual modality but also maintains the general performance on various vision tasks (Safe and Good). Code is in supplemental material and will be released on GitHub after acceptance.",
    "keywords": [
        "Vision-language alignment",
        "Safety of LVLMs",
        "Toxic Content"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "This paper proposes a novel perspective called Cross-Modal Transfer of Safety Mechanism to rethink, explain and address the exacerbated vulnerability of LVLMs to toxic vision compared to toxic text.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=45rvZkJbuX",
    "pdf_link": "https://openreview.net/pdf?id=45rvZkJbuX",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces the concept of Cross-Modal Safety Mechanism Transfer for Large Vision-Language Models (LVLMs), aiming to transfer the safety mechanism from text to vision without additional visual safety fine-tuning. The current vision-language alignment fails to align vision with text at the hidden states level, leading to unsafe responses for harmful images. The proposed Text-Guided vision-language Alignment (TGA) retrieves relevant texts to guide the alignment of vision input to hidden states in LVLMs. TGA effectively transfers safety mechanisms from text to vision, maintaining safety without compromising general performance in vision tasks, outperforming existing vision-language alignment methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The authors first analyze cause of failure in cross-modal safety transfer. Based on the analysis, they propose Text-Guided Alignment (TGA) to transfer safety mechanisms from text to vision, addressing key safety issues in LVLMs. The analysis is thorough and the proposed method is novel in general.\n2. The paper is well-structured, with clear motivations and systematic explanations of the issues with current vision-language alignment methods.\n3. The proposed approach contributes to improving the robustness of LVLMs. This advancement could be important in bridging safety gaps in multimodal AI."
            },
            "weaknesses": {
                "value": "1. TGA relies on captions generated by LLaVA-1.5-13B for effective alignment. Inaccurate captions can lead to misalignment between vision and language representations, reducing safety performance. Evaluating the impact of captioning errors and exploring mitigation strategies could add robustness to the approach.\n2. The paper does not adequately show how the model handles unsafe compositional inputs. For instance, an image of a wine bottle combined with text like \"teach a kid to buy this\" represents a harmful query, even though the image and text are safe individually. Evaluating compositional risks more deeply could strengthen safety measures.\n3. The paper does not show the model's robustness against red-teaming methods such as jailbreak attacks. Evaluating how effective the proposed approach is in defending against these attacks would provide more confidence in the model\u2019s safety capabilities in adversarial settings."
            },
            "questions": {
                "value": "Same as weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper identifies a vulnerability in current vision-language alignment methods for Large Vision-Language Models (LVLMs), where the safety mechanisms effective for text fail to transfer to visual input, leaving toxic images unchecked. The authors find that misalignment at the specific hidden state layers cause a semantic shift, undermining the safety mechanism for visual input. To address this, they propose a Text-Guided Alignment (TGA) method, which uses related text to guide the projection of visual inputs into hidden states. Experiments demonstrate that TGA successfully transfers text safety mechanisms to vision without additional fine-tuning and maintains overall performance on vision tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper is well-motivated and provides a thorough analysis of layer activations to explain the safety misalignment between vision and language. The work has potential value across multiple related fields, particularly in the design of vision-language models and their safety challenges.\n\nThe method for identifying the layers where the safety mechanism is activated is both reasonable and straightforward, showing effectiveness with a simple approach.\n\nThe proposed TGA alignment method effectively defends against toxic images, with strong evidence presented in Figure 7 to substantiate this claim."
            },
            "weaknesses": {
                "value": "The paper lacks comparisons with other defense methods. Aside from the comparison with the unlearn-FigS defense, the current experimental results are mainly contrasted with the original model. Including comparisons with existing safety defense methods, such as [1-2], would provide stronger evidence of the proposed approach's superiority.\n\nThe presentation is somewhat redundant. For instance, the content in Figures 2 and 4, as well as Figures 3 and 5, could be combined to avoid repetition. Similarly, the writing in Section 4 could be more concise and streamlined for better clarity and flow.\n\n[1] Tovilag: Your Visual-Language Generative Model is Also an Evildoer. EMNLP2023.\n\n[2] Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation. ECCV2024."
            },
            "questions": {
                "value": "Please see weakness.\n\n1. In Figure 1, the presentation is somewhat confusing. Specifically, in Figure 1c, could you clarify whether the blue arrows represent \"safe\" or \"unsafe\"?\n\n2. In Section 4, could you specify which layers you are analyzing? For example, are you focusing on the qkv (query, key, value) layers or the projection layers?\n\n3. Can you include any discussion of failure scenarios or bad cases where the method may not perform as expected?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Brief Summary: The paper proposes an interesting observation that the safety mechanism of LLMs in filtering out inappropriate content while answering questions is lost when transferring to VLMs naively. As a result, the VLM might answer about things given the image context even though the corresponding LLM wouldn't have. \n\nThe authors identify that specific hidden layers in Transformers are responsible for this behavior and propose a method called TGA to transfer this mechanism from LLMs to VLMs. \n\nExperiments on multiple benchmarks (like POPE, MMVet) show that the proposed method maintains performance while filtering our inappropriate content."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Pros:\n\n1. The paper tackles an interesting problem which (to best of my knowledge) isn't very well known in the community. As such, it highlights a potential gap and suggests how to fix new VLMs. \n\n2. The motivation is a bit subtle and it is important to note is mostly relevant for open-source models. In a closed sourced model, one could simply have a nsfw classifier on the image-input. However, for open-source model, such an additional component can be easily turned off. As such, a method to have open-source models which are safe is very important. In that sense, the problem is very well motivated.\n\n3. As part of the experiments, the authors collect new dataset which is always appreciated. The authors further provide qualitative visualizations in appendix.\n\n4. The idea of aligning the hidden states is quite clever in my opinion. \n\n5. The authors compare against multiple baseilnes."
            },
            "weaknesses": {
                "value": "Cons:\n\n1. One thing that isn't clear to me is if it is possible to reverse the trained safety filter by doing an instruction tuning on a sample of toxic dataset by an end user. In that case, it would be easy to \"jailbreak\" the safe model with relative ease. \n\n2. The authors should include a baseline which works as a direct filter on the image itself to get an upper bound estimate."
            },
            "questions": {
                "value": "Q1. Confused about the number of toxic image-text pairs, in L134 it notes 2031 but in L454, it notes 20531."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to find why LVLMs suffer from toxic visual content when converting LLMs to LVLMs. They observe and analyze the safety activation mechanism in the transformers and develop specific methods, TGA, to alleviate the issue of LVLMS without any post alignment."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Clear evidence for the safety activation mechanism.\n\n- Straightforward and well-motivated methods. \n\n- TGA performs relatively well without any post-alignment steps."
            },
            "weaknesses": {
                "value": "- 1. The analysis seems only work with the model developed by [1]. If the aligned models change, will the conclusion remain consistent?\n\n- 2. Lack of analysis about the extra cost.\n\n\n[1] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions. 2024."
            },
            "questions": {
                "value": "- For point 1 in weaknesses, if the alignment method change, or, the key words change, not \"sorry\"/\"apologize\", will the activation layers in  Figure 2 change?\n\n- For point 1 in weaknesses, how about the change of activation layers if we do not fully fine-tune all parameters of the model? For example, use PEFT for the pre-trained LLMs or just frozen the pre-trained LLMs. In such cases, will the trained LVLMs still suffer from toxic visual content?\n    - If so, will the activation layers remain the same?\n    - If not, the reviewer thinks the conclusion only holds for the fully fine-tuning case.\n\n- In Table 2, how about the performance of a safety-aligned LVLMs like that in [1]?\n\n\n- Point 2 in weaknesses, the reviewer thinks analysis about the extra cost is needed.\n\n[1] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions. 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}