{
    "id": "awz1JPyXNK",
    "title": "Inner Information Analysis Algorithm for Deep Neural Network based on Community",
    "abstract": "Deep learning has achieved advancements across a variety of forefront fields. However, its inherent 'black box' characteristic poses challenges to the comprehension and trustworthiness of the decision-making processes within neural networks. To mitigate these challenges, we introduce InnerSightNet, an inner information analysis algorithm designed to illuminate the inner workings of deep neural networks through the perspectives of community. This approach is aimed at deciphering the intricate patterns of neurons within deep neural networks, thereby shedding light on the networks' information processing and decision-making pathways. InnerSightNet operates in three primary phases, 'neuronization-aggregation-evaluation'. Initially, it transforms learnable units into a structured network of neurons. Subsequently, these neurons are aggregated into distinct communities according to representation attributes. The final phase involves the evaluation of these communities' roles and functionalities, to unpick the information flow and decision-making. By transcending focus on single-layer or individual neuron, InnerSightNet broadens the horizon for deep neural network interpretation. InnerSightNet offers a unique vantage point, enabling insights into the collective behavior of communities within the overarching architecture, thereby enhancing transparency and trust in deep learning systems.",
    "keywords": [
        "inner information analysis",
        "transparency",
        "knowledge mining"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=awz1JPyXNK",
    "pdf_link": "https://openreview.net/pdf?id=awz1JPyXNK",
    "comments": [
        {
            "summary": {
                "value": "Review report: Inner Information Analysis Algorithm for Deep Neural Network Based on Community \nThe present manuscript converts learnable units in Deep Neural Networks, such as kernels in CNNs, into networks and then groups the neurons into communities based on their representational attributes. To achieve this clustering, the authors develop the InnerSightNet. By subsequently analyzing the communities, the authors gain insights into the inner working of Deep Neural Networks such as relating certain communities to specific features in the underlying test data and detecting irrelevant neurons."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Strengths \n-\tThe main idea of the manuscript, to group learnable units into a network of neurons and then analyze the communities within this network, is very clever and interesting.\n-\tThe general approach to InnerSightNet is described very clearly. In particular, Algorithm 1 is helping in understanding the different steps.  \n-\tIt is intriguing to see that the interdependence of different communities of neurons, such as c_1 and c_2 in the first layer of the CNN as reported in Table 1, is reflected in the 2-dimensional topological representation in Figure 3."
            },
            "weaknesses": {
                "value": "Weaknesses\n-\tThe different terms in Eq. (1) could be motivated more clearly, especially given the fact that this step is crucial to transform the kernels of a CNN in a structured network. (See also question below) \n-\tWhile the general structure of InnerSightNet is described very clearly, the more detailed description is cumbersome to follow. Especially the aggregation step, which is heavy in notation but also lacks a definition of some variables such as l_d, \\tau_{g_k, j} and s. \n-\tThe section \u201cCommunity function analysis and finding key communities\u201d is rather descriptive and only offers a shallow discussion regarding the explainability of community structures for MNIST. The discussion here could be greatly improved by more directly linking the observation from panel (a) and (b) together, as has been done with community c_10 in the first layer."
            },
            "questions": {
                "value": "Questions \n-\tIn Eq. (1), could you motivate the distinction in positive and negative value regions more clearly as well as the choice to divide by the normalized cosine similarity, which is essentially either +1 or -1? Could you also specify what exactly data X_i refers to? \n-\tAre the accuracy drops in closing communities in Figure 1a) and Table 1 reported with or without retraining the network? \n-\tIn Table 1, why don\u2019t you report the accuracy after closing the corresponding communities for all possible combinations of the communities in the respective layers? For instance, in the first layer, why are c_0 and c_1 not closed together? Could you also close communities together across layers? \n-\tCould you elaborate on the practical implications of your results, especially regarding parts of the model that fit to noisy data, cf. lines 458 until 464? \n-\tIn lines 1109 to 1112 you write \u201cThis phenomenon raises a question: in common sense, cat and dog images contain more information than handwritten digital, why is there actually less community division? Our explanation is \u2018task-related\u2019. Due to the fact that cat and dog classification is a binary task, the number of effective neurons for binary classification is indeed less than that for ten class tasks.\u201d Would it be possible to simply restrict MNIST to classifying between 0 and 1 to test for this observation?\n\n\nAdditional comments\n-\tThe axis and legend labels in the Figures are barely readable. \n-\tIn line 53, it should be \u201cone aims\u201d instead of \u201cone aims\u201d. \n-\tIn line 108, it seems inappropriate to cite a review paper to represent community detection in networks. \n-\tIn line 146, the citation \u201cDavid et al. Bau et al. (2017)\u201d seems to contain a typo. \n-\tIn line 161 and in line 188, the citation \u201cNewman et al. Newman (2006\u201d seems to contain a typo. \n-\tIn Definition 2, it would be helpful to distinguish \n-\tIn line 238, the citation \u201cLange et al. Lange et al. (2022)\u201d contains a typo.\n-\tIn Eq. (11), it is a bit unfortunate to use delta as a variable again as it also appeared in Eq. (1). \n-\tIn line 264, \u201csgn is the symbol function\u201d was meant to be sign function? \n-\tIn line 323, the citation \u201cWanatabe et al. Wanatabe et al. (2018)\u201d contains a typo. \n-\tFrom line 370 onwards, c_10 should be changed to c_{10} in LaTex"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an algorithm designed to enhance the interpretability of deep neural networks by analyzing internal neuron communities. The approach transforms neurons into structured communities, aggregates them based on their functional attributes, and evaluates the information flow across these groups. The study applies the method to both linear and convolutional networks, demonstrating improvements in turning off noisy neurons."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. InnerSightNet introduces a unique perspective by focusing on community-based analysis of neurons, rather than on a single layer or individual neuron.\n2. The three primary phases (neuronization, aggregation, evaluation) adaptively ensure the best number of communities.\n3. InnerSightNet is shown to enhance interpretability and can be applied to network pruning to reduce model size while maintaining competitive performance."
            },
            "weaknesses": {
                "value": "1. The paper compares InnerSightNet with methods such as Filan et al. (2021), Hod et al. (2021), and Liu et al. (2023) in the experiments. However, it does not sufficiently explain the methodologies of these baselines. A more detailed introduction to these approaches is needed to help the readers understand how InnerSightNet improves upon or differs from them.\n2. While the method performs well on relatively small networks like those used in the MNIST and AFHQ datasets, it remains unclear how the algorithm scales to deeper networks."
            },
            "questions": {
                "value": "1. What is the runtime of InnerSightNet for different network sizes? How fast the algorithm converges compared to baseline methods?\n2. How scalable is InnerSightNet when applied to deeper networks like transformers\uff1f"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes the InnerSightNet algorithm, which clusters neurons in the same layer of MLPs and convolutional kernels in the same layer of CNNs from a community perspective. Through experiments that mask certain neuron clusters and perform perturbation analysis, it demonstrates that some clusters play a key role in task performance, some clusters can be pruned, and some clusters introduce noise."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Novelty: The introduction of a community-based perspective for analyzing DNNs is original and provides a fresh viewpoint for understanding the role neurons play.\n2. Comprehensive Framework: The multi-phase algorithm (neuronization, aggregation, evaluation) is well-structured, combining theoretical and practical insights.\n3. Detailed Evaluation: The paper evaluates the communities formed in DNNs through accuracy drop tests and sensitivity analyses, showcasing the impact of each community on network performance."
            },
            "weaknesses": {
                "value": "1. Scalability: The paper doesn't address the scalability of InnerSightNet on very large-scale neural networks. It is unclear how well the algorithm would perform or how computationally feasible it is for networks with millions of neurons.\n2. Transferability: The experiments are only conducted on image recognition tasks. Can this method be applied to the interpretability of NLP models? The findings are limited to the functions of different clusters which follows conventional research pattern.\n3. Usability: The mathematical theory of the algorithm is more solid compared to previous works, but the overall improvement in tasks like noise reduction and pruning is very limited."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}