{
    "id": "lEsNGN1SjG",
    "title": "Information-theoretically Safe Bias Classifier Against Adversarial Attacks",
    "abstract": "Deep learning has become the cornerstone of recent advances in artificial intelligence. However, the presence of adversarial samples makes deep learning susceptible in applications where safety is critical. Moreover, adversarial examples have been shown, to some degree, to be unavoidable. To address this issue, we propose the bias classifier. This approach employs the bias component of a neural network, using ReLU as its activation function, as a classifier. The bias classifier has been shown to universally approximate any classification problem with a high degree of probability. Moreover, it can be made information-theoretically safe against the original model gradient-based attack in the sense that any such attack produces a completely random attacking direction for any given input. Thus, the bias classifier provably achieves the maximum possible robust accuracy under specified attacks. Experiments are used to validate our theoretical results and to show that the bias classifier is accurate and robust for simple models.",
    "keywords": [
        "Information-theoretically",
        "theory",
        "adversarial"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-21",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=lEsNGN1SjG",
    "pdf_link": "https://openreview.net/pdf?id=lEsNGN1SjG",
    "comments": [
        {
            "summary": {
                "value": "This paper designs a \"provable\" adversarial robustness enhancing method named \"bias classifier\" that is supposed to help ML models achieve high adversarial accuracy. A new information-theoretically inspired notation is proposed to measure the provable robustness of the bias classifier method. Experiments are conducted on two simple datasets, i.e., MNIST and CIFAR-10, and the results show that the adversarial robustness of models obtained from the bias classifier method is extremely weak (compared with existing adversarial robustness approaches)."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "None."
            },
            "weaknesses": {
                "value": "This is a wrong paper and should be rejected immediately. Detailed comments are as follows:\n\n1. The authors claim that their proposed bias classifier method has provable high adversarial robustness. However, according to the experiment results in Section 5, the robustness of the proposed method is actually extremely WEAK compared with existing adversarial robustness approaches. Specifically:\n\n    - In Table 2, the proposed method can only achieve an adversarial accuracy (adversarial radius $\\rho = 8/255$) of $31.25%%$ on the CIFAR-10 dataset under the AutoAttack [r1]. However, according to the original AutoAttack paper [r1], under the same setting, the best adversarial accuracy is $59.53%%$ in 2020 (see Table 2 in [r1]), which is almost two times larger than the performance of the proposed bias classifier method.\n\n    - In Table 5, although the adversarial accuracies seems to be high, they are actually calculated based on adversarial examples crafted with **random noise** (see Line#394-400). Such an attack is even much weaker than the simplest FGSM attack. Thus, Table 5 is meaningless and does not show any advantages of the proposed bias classifier method.\n\n2. The authors claim that existing defenses cannot give provable adversarial robustness (see Line#050). Unfortunately, this is a **wrong claim** since the well-known \"certified robustness\" method [r2] can indeed give the model provable and high adversarial robustness. For example, [r3] shows that with the help of diffusion models, when the adversarial perturbation is not stronger than $127/255$ under $l\\_2$-norm, one can provablely ensure an adversarial accuracy of $65.5\\\\%$ on the CIFAR-10 dataset. [r4] further improves such an provable adversarial accuracy to $70.7\\\\%$. Given the profound success of existing certified robustness methods, I am wondering what is the advantage of the proposed bias classifier method compared with existing approaches.\n\n3. The overall goal of the bias classifier method is to ensure ML models satisfy Eq.(11). Unfortunately, satisfying Eq.(11) does not mean that the model indeed enjoys a **provablely high** robustness. This is because Eq.(11) only ensures that small perturbations would not change model predictions, but the model clean accuracy could be very low in order to fulfill the requirement of Eq.(11). Actually, experiments in Section 5 indeed show that models obtained from bias classifier method and satisfying Eq.(11) indeed enjoys a very weak clean accuracy (since low robust accuracy implicitly imply low clean accuracy according to this paper). I suggest the authors read more papers about certified robustness, which can achieve \"**provablely high** adversarial robustness\".\n\n**References:**\n\n[r1] Croce F. and Hein M. \"Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks.\" ICML 2020.\n\n[r2] Cohen J. et al. \"Certified Adversarial Robustness via Randomized Smoothing.\" ICML 2019.\n\n[r3] Carlini N. et al. \"(Certified!!) Adversarial Robustness for Free!\" arXiv 2206.10550 / ICLR 2023.\n\n[r4] Chen H. et al. \"Diffusion Models are Certifiably Robust Classifiers.\" arXiv 2402.02316 / NeurIPS 2024."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes Bias Classifiers which results from subtracting the linear approximation of a ReLU activated neural network on some test sample from the network. The paper is mostly focused on proving that if we assume that the norm of the Jacobian is less than some upper bound, then we can infer that the bias classifier is robust."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Originality:\n\nThe paper is somewhat original in considering test time training of a classifier.\n\n- Quality:\n\nThe paper is well-written for the most part.\n\n- Clarity:\n\nThe paper uses easy to follow instructions.\n\n- Significance:\n\nThe paper considers a significant problem."
            },
            "weaknesses": {
                "value": "The paper is not ready for publication for various reasons. Here are some key observations:\n\n1) The paper does not make use of standard notation, e.g. $\\frac{\\nabla F(x)}{\\nabla x}$ is not meaningful as it seems to be a division of two vectors. Maybe the authors meant to write $\\frac{\\nabla F(x)}{|\\nabla F(x)|}$?\n\n2) The paper is testing a self-fulfilling prediction where an accurate classifier over benign test samples is forced to be constant on a neighborhood of the benign test sample and then finding out that it is both robust and accurate. A real test of robustness of this method should involve linearization on an adversarial test sample for which the reference network is *not* accurate.\n\n3) The idea of information theoretic safety is very similar the idea of probabilistic robustness which is already discussed in the literature. Moreover, the description is vague, e.g. \"Let $\\mathcal{F}_R$ be a neural network that involves a random variable $R$ satisfying the distribution $\\mathcal{R}$\". What does it mean for a neural network to involve a random variable?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to adversarially train the model and the bias and then take the bias as the classifier (dubbed as bias classifier), which improves the adversarial robustness. The authors theoretically show that the bias classifier can be information-theoretically safe against certain kinds of adversarial attacks, i.e., FGSM and CW attacks. Experimental results using the ResNet and VGG validate the effectiveness of the bias classifier."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tIt is interesting to study the potential of utilizing the bias term of a neural network towards improved adversarial robustness. It seems that the authors theoretically show the bias classifier can defend against certain kinds of adversarial attacks.\n2.\tThe empirical results seem to support the claim."
            },
            "weaknesses": {
                "value": "1.\tThe scope of the theory is limited. The authors only show the potential theoretically-information safety of the bias classifier against two certain adversarial attacks, i.e., FGSM and CW attacks, instead of the generalized adversarial attacks.\n2.\tThe experiments are conducted on simple networks and small datasets, which cannot well support the effectiveness of the bias classifier. \n3.\tThe presentation is somewhat unclear. For example, what does \u2018the original model gradient-base attacks\u2019 refer to in Table 5? Besides, in Table 5, which results refers to the baseline?"
            },
            "questions": {
                "value": "Please refer to Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a bias classifier and proofs that it serves as a universal classifier. Additionally, it introduces a new concept called \"information-theoretically safe.\" Finally, based on experimental results, the authors claim that the adversarially trained bias classifier is information-theoretically safe against various white-box and black-box attack methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper proposes and introduces a new adversarial-related definition called \"information-theoretically safe\" along with a new model termed \"bias classifier,\" which may be valuable to the literature."
            },
            "weaknesses": {
                "value": "I strongly recommend that the author include a notation section in the revised version.\n\nHere, I have listed several questions that arose while reviewing the paper.\n\nLine 060-061, What is meant by a \"random direction attack\"? Does this refer to adding random noise to the DNN input? The motivation behind this new definition is unclear. The upper bound of adversarial accuracy for a classifier can be defined as its classification accuracy on clean examples. What advantage does the paper's definition offer over this approach? Additionally, if a DNN is highly sensitive to noise, achieving zero accuracy on any noisy input, it would trivially meet the \"information-theoretically safe\" criterion, despite having no robustness at all.\n\n\nLine 063, \"By the bias classifier, we mean to use $\\mathcal{F}$\u2019s bias part ${\\mathcal{B_F}}(x)$  = $\\mathcal{B}x$ : $\\mathbb{I}^n \\rightarrow R$ m as a classifier.\" A classifier is a function that maps the input $x$ to a output prediction. Could the authors clarify how the bias term of a classifier can itself function as a classifier? $\\mathbb{I} = [0, 1]$. What is $\\mathbb{I} $? Does [0, 1] means $\\mathbb{I}$ is bounded between 0 and 1?\n\nLine 099-100, [1] is a review paper that does not focus on DNN structure, so why is it cited here?\n\nThe related work section is somewhat disorganized. The authors just list different approches without detailed explaination. Further, Madry's adversarial training is well-known in the literature, it is not considered one of the most effective practical defense methods from nearly all perspectives. \n\nLine 119, Where is the new loss function?\n\nLine 175-177, \"The following existence theorem shows that the bias classifier has the power to interpolate any classification functions with arbvitray high probability\". Minor: arbitrary is misspelled. \n\nSection 3.3 What's the motivation of using adversarial training to train the bias classifier? \n\nLine 229-230. \nGradient-based adversarial attacks are well-established in the literature and have been studied for years; thus, the concept is not original to the author.\n\nLine 249. \nWhat does the author mean by stating that a neural network involves a random variable? It is more common to say that a random variable follows a specific distribution.\n\n\nLine 259, Could the authors further clarification on the formula of $\\mathcal{C}(\\mathcal{F}_R)$?\n\nEquation (14) seems wrong.\n\nLine 379-380, What is the \"original based AutoAttack\"?\n\nLine 480-481, \"bias classifier is essentially different from gradient obfuscated defences.\" I don't believe it is appropriate to make such a strong statement based on experiments involving only one method published in 2018. Could the author provide more context or additional evidence?\n\nThe experimental results are insufficient. The authors did not benchmark and compare against the state-of-the-art methods and only test their method on small datasets like MNIST and CIFAR-10.\n\n[1] Han Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang Tang, and Anil K Jain. Adversarial attacks and defenses in images, graphs and text: A review. International journal of automation and computing, 17:151\u2013178, 2020."
            },
            "questions": {
                "value": "Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}