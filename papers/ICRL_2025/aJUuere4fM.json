{
    "id": "aJUuere4fM",
    "title": "Does Refusal Training in LLMs Generalize to the Past Tense?",
    "abstract": "Refusal training is widely used to prevent LLMs from generating harmful, undesirable, or illegal outputs. We reveal a curious generalization gap in the current refusal training approaches: simply reformulating a harmful request in the past tense (e.g., *\"How to make a Molotov cocktail?\"* to *\"How did people make a Molotov cocktail?\"*) is often sufficient to jailbreak many state-of-the-art LLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet, GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o-mini, GPT-4o, o1-mini, o1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For example, the success rate of this simple attack on GPT-4o increases from 1\\% using direct requests to 88\\% using 20 past-tense reformulation attempts on harmful requests from JailbreakBench with GPT-4 as a jailbreak judge. Interestingly, we also find that reformulations in the future tense are less effective, suggesting that refusal guardrails tend to consider past historical questions more benign than hypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending against past reformulations is feasible when past tense examples are explicitly included in the fine-tuning data. Overall, our findings highlight that the widely used alignment techniques---such as SFT, RLHF, and adversarial training---employed to align the studied models can be brittle and do not always generalize as intended.",
    "keywords": [
        "Jailbreaking",
        "adversarial attacks",
        "adversarial robustness",
        "AI safety"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "Frontier LLMs refuse harmful prompts in the present tense, but not in the past tense",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=aJUuere4fM",
    "pdf_link": "https://openreview.net/pdf?id=aJUuere4fM",
    "comments": [
        {
            "summary": {
                "value": "The paper reveals a significant vulnerability in LLM safety measures: simply reformulating harmful requests in the past tense often bypasses safety guardrails. Testing multiple leading LLMs (including GPT-4, Claude-3.5, etc.), they found that attack success rates increased dramatically with past-tense reformulation. The vulnerability persists even in newer \"reasoning\" models like o1-preview, though these models tend to provide less specific harmful information. The issue can be addressed through fine-tuning on past-tense examples, but requires careful balancing to avoid over-refusing legitimate requests"
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Comprehensive evaluation across multiple leading LLMs and different types of harmful requests, with a clear demonstration of the vulnerability that wasn't previously well-documented. Provides concrete evidence through systematic testing and multiple evaluation metrics used widely in the adversarial robustness field, and demonstrates a clear strategy for mitigating this threat through better finetuning."
            },
            "weaknesses": {
                "value": "1. Lack of evaluation on other languages"
            },
            "questions": {
                "value": "1. Why do you think past-tense reformulations are more successful than future-tense ones? Is it related to how historical information is typically treated in training data?\n2. Could this vulnerability be addressed through prompt engineering rather than fine-tuning?\n3. Have you tested whether this vulnerability exists in non-English languages, given that LLM safety measures often generalize across languages?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates a gap in refusal training for large language models (LLMs), revealing that simple past-tense reformulations of harmful requests can bypass existing safety defenses. Despite advancements in supervised fine-tuning and reinforcement learning with human feedback, models often fail to reject past-tense prompts, treating them as benign historical inquiries. The authors demonstrate this vulnerability across multiple LLMs, showing high success rates in bypassing safety mechanisms. By highlighting the brittleness of current alignment techniques, this work emphasizes the need for more robust training that generalizes across linguistic variations, including tense, to effectively strengthen LLM safety measures."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**Novel Insight into Refusal Training**: The paper identifies a specific, under-explored vulnerability in LLM refusal training\u2014namely, that past-tense reformulations can bypass safety mechanisms. This insight into linguistic generalization gaps is valuable for improving the robustness of refusal training.\n\n**Thorough Empirical Validation**: By evaluating the past-tense attack across a wide range of advanced models (e.g., GPT-3.5 Turbo, Claude-3.5, GPT-4o), the authors provide convincing evidence that the issue is both widespread and impactful. The systematic comparisons enhance the study\u2019s credibility and relevance.\n\n**Practical Contributions**: The paper proposes straightforward mitigation strategies, such as fine-tuning with explicit past-tense refusal examples, providing actionable insights for developers aiming to improve model safety."
            },
            "weaknesses": {
                "value": "**Limited Solution Exploration**: Although the paper identifies a clear vulnerability, the proposed solution\u2014incorporating past-tense examples in training\u2014is relatively basic and may not address other similar reformulations or linguistic variations.\n\n**Lack of Theoretical Analysis**: No theoretical insight is given for why a generalization gap between past-tense and present-tense, which is more interesting and can deepen our understanding to eliminate other underexplored vulnerabilities."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a jailbreaking method that involves paraphrasing harmful questions in the past tense. This method is effective across various SOTA LLMs. Furthermore, the paper demonstrates that safety fine-tuning using harmful prompts in the past tense reduces the attack success rate in GPT-3.5-Turbo."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "-- Alignment of LLMs is an important domain of research, and the more vulnerabilities that are found, the better it is for researchers and model providers to patch them. This paper presents a cost-effective jailbreaking method that paraphrases harmful prompts in the past tense to attack LLMs, showing that LLM safety training has not generalized to past-tense formulations.\n\n-- The efficacy and simplicity of this attack across various models highlight the urgency of addressing it."
            },
            "weaknesses": {
                "value": "1.  While the attack is simple and cost-effective, paraphrasing attacks have proven effective in the past. \n\n--> One possible reason for this jailbreak's success might be the lack of generalization (or explicit safety training) in handling past tense harmful prompts (as mentioned in the paper).\n\n--> This does not appear to be a novel type of attack.\n\n-->  It would be helpful to know whether this attack was discovered through systematic investigation / brute-force testing.\n\n--> In my opinion, the paper should discuss both successful and unsuccessful approaches that were tried before discovering this prompt.\n\n2. Regarding dataset selection, there are other more comprehensive datasets available that could provide more rigorous results. \n\n--> The current study's reliance on only 100 harmful questions significantly limits its scope and generalizability.\n\n--> The paper's findings would have been more robust if multiple datasets had been utilized. With results based on just 100 data points, the experimental conclusions cannot be considered sufficiently rigorous.\n\n3. Regarding targeted models: Based on the results in Table 1, a significant limitation is the absence of testing on the GPT-4 model. \n\n--> While the study examines the progression of the attack from GPT-3.5 to GPT-4o/mini, it overlooks GPT-4, which has demonstrated great robustness in many previous attack scenarios. \n\n--> Additionally, other prominent language model families, such as Gemini Pro, were not included in the experiment. \n\n\n4. Defense-Section 4: The fine-tuning experiments could have been extended beyond GPT-3.5-Turbo to include other open-source LLMs (such as Llama, and Qwen), providing a more comprehensive analysis.\n\n--> The paper's research scope appears limited to paraphrasing harmful questions in the past tense and addressing them through safety training. \n\n**** As both jailbreaking through paraphrasing harmful prompts and safety training via fine-tuning have been comprehensively studied in the existing literature, the paper lacks a novel contribution."
            },
            "questions": {
                "value": "Minor fixes:\n\n1. Please clarify whether its Llama-3-8B [ Instruct or Completion]  model. \n\n2. Line 21: I think the citation is missing for  Wei et al. for chain-of-thought paper.\n\n3. Line 382: missing dash - in chain of thought. \n\nQuestions: \n\nFrom a further research perspective, several questions remain unanswered in the paper: Is it possible to increase the attack success rate with the current approach? What additional strategies could attackers employ? What other potential threat models should be investigated? Can safety training alone prevent this attack, or have other methods been tested?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on one pretty simple (but surprisingly effective) jailbreaking strategy which involves asking models for harmful information using the past tense in plain English. They find that future tense does not work as well and that AT can work as a defense (unsurprisingly). Based on these two observations, they speculate that failed generalization to past tense is a unique and interesting artifact of either pertaining or fine-tuning to focus (maybe implicitly) on present and future harm.  \n\nOverall, I think that this paper sets out to do something very simple, does it well, and shows that it works surprisingly well. In that sense, I think it's clearly valuable but not particularly remarkable."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1: I was surprised by the results. I think that this is a new contribution to the best of my knowledge. And I can see these being valuable for red teaming and AT. \n\nS2: As far as I know, this is one of the first papers that has gone to successfully attack OpenAI o1. For that reason, I think this paper is especially citeable and noteworthy. It won't be the last though."
            },
            "weaknesses": {
                "value": "W1: Though valuable and well done, this paper is doing something that many have done before. So aside from some of the unique contributions (which are unique) this paper is also, in a sense, yet another paper to dunking on SOTA models with a niche jailbreaking technique. For that reason, I don' think this paper is breaking amazing ground, and I wouldn't recommend it for a reward. \n\nW2: I think that the paragraph from 441 to 448 could discuss more related work. I think the chosen papers seem kind of arbitrary. One could also discuss https://arxiv.org/abs/2309.00614, and cite papers on harmfulness probing, latent adversarial training, representation noising, and/or latent anomaly detection. Also describing Zou et al's method as rejecting harmful outputs seems incorrect since it's about latents. \n\nW3: This paper focuses on attacks, with a simple AT experiment as almost an afterthought. This is good. But since the paper speculates about how these attacks might reveal some limitations of current alignment techniques, this begs the question of whether models that have undergone AT with past tense might have more generalizable robustness in general. Since the scope of the paper is currently so narrow, this kind of experiment (or maybe something similar) would valuable expand it."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}