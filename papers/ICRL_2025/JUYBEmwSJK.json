{
    "id": "JUYBEmwSJK",
    "title": "Next Block Prediction: Video Generation via Semi-Auto-Regressive Modeling",
    "abstract": "Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed.  In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. By uniformly decomposing video content into equal-sized blocks (e.g., rows or frames), we shift the generation unit from individual tokens to blocks, allowing each token in the current block to simultaneously predict the corresponding token in the next block. Unlike traditional AR modeling, our framework employs bidirectional attention within each block, enabling tokens to capture more robust spatial dependencies. By predicting multiple tokens in parallel, NBP models significantly reduce the number of generation steps, leading to faster and more efficient inference. Our model achieves FVD scores of 55.0 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4. Furthermore, thanks to the reduced number of inference steps, the NBP model generates 8.89 frames (128x128 resolution) per second, achieving an 11\u00d7 speedup in inference. We also explored model scales ranging from 700M to 3B parameters, observing significant improvements in generation quality, with FVD scores dropping from 25.5 to 19.5 on K600, demonstrating the scalability of our approach.",
    "keywords": [
        "video generation",
        "auto-regressive model",
        "semi-auto-regressive model"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=JUYBEmwSJK",
    "pdf_link": "https://openreview.net/pdf?id=JUYBEmwSJK",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a semi-autoregressive (semi-AR) framework, Next-Block Prediction (NBP), for video generation tasks. Compared to the conventional autoregressive (AR) framework, or Next-Token Prediction (NTP), the proposed framework generates blocks with multiple tokens, while these blocks follow a raster-scan ordering. Thus, NBP reduces the number of forward steps required for sampling videos. Experimental results demonstrate that NBP can achieve low FVD scores on UCF-101 and K600 datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1. This paper aims to resolve an important issue in video generation, sampling efficiency.\n\nS2. This study shows that a semi-AR framework, which is unexplored in video generation tasks, can also be used for video generation."
            },
            "weaknesses": {
                "value": "W1. Limited novelty and originality. Contrary to the claims in Section 2, it is widely known that the conventional semi-AR semi-AR framework predicts multiple tokens without additional modules. For example, the SAT model [NewRef-1], which is well-known and presented at EMNLP\u201918, also shares the same framework as the proposed approach. Thus, I believe the contribution of this paper does not lie in the framework design itself, but lie in applying existing semi-AR frameworks from NLP domains to video generation.\n\nW2. Lack of in-depth analysis on the proposed block predictions. The ablation study does not explore various block shapes (e.g., 1x4x4). Especially, despite the video generation framework, there is no experiment involving the prediction of multiple tokens across different frames.\n\nW3. Given that the proposed NBP conducts row-by-row generation, the framework should be validated on image generation tasks first. Note that the proposed transformer lacks a tailored design for video data.\n\n\n[NewRef-1] Wang et al., Semi-Autoregressive Neural Machine Translation, EMNLP2018."
            },
            "questions": {
                "value": "Q1. Could the authors provide a more detailed explanation and comparison regarding sampling costs? Was KV-caching used in this comparison? Given that the FLOPs for both NBP and NTP are likely similar for sampling, I believe the inference speed should be comparable when using KV-caching as the model scales, even though NBP requires fewer forward steps than NTP.\n\nQ2. Given the same block size, how does performance vary according to block shape? For instance, the ablation study could include comparisons like (1x1x16 vs. 1x4x4 vs. 16x1x1) or (1x16x16 vs. 16x4x4 vs. 4x8x8). Since the authors claim that NTP cannot account for spatial dependencies in local tokens, I initially expected the study to use 2D or 3D shapes for local blocks. However, it employs a 1D block shape, which has fewer spatial dependencies than 2D or 3D blocks.\n\nQ3. In Table 3, how were PSNR, SSIM, and LPIPS computed for the generation results when no ground-truth data exists for video generation?\n\nQ4. In Figure 5, why do the validation loss curves exhibit noisy patterns? I suspect these might be training losses rather than validation losses, considering the curve shapes in Figure 8. Additionally, given the large number of trainable parameters and epochs relative to the small dataset (such as UCF-101), I wonder whether the model shows signs of overfitting.\n\nQ5. Since the experiments focus primarily on class-conditional generation for UCF-101 and frame prediction for K600, could the authors clarify how the text tokens are utilized?\n\nMinor comments (not affecting the score):\n- Eq. (1) may contain an error. $x_{l}^{(<t)}\u200b$ should likely be $x^{(<t)}$.\n- Since FVD is an incomplete metric for video generation, I recommend including additional metrics such as IS, Dover-Scores, Frame-wise Text Alignments, etc.\n- Contrary to the statement in Lines 355-356, Flash Attention does not support customized attention masks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes next-block prediction framework as a semi-autoregressive method, enhancing the spatiotemporal integrity and parallel prediction for video generation tasks. Several modifications including initial condition, block-wise attention and inference process are applied to existing AR models, and massive experiments are conducted to find the optimal configuration of the block size. The proposed model reaches leading performance compared to previous SOTAs with a good scaling-up law."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed block-wise semi-AR method is novel and illustrated clearly.\n\n- Rich comparisons and ablations with visualizations are presented and analysed."
            },
            "weaknesses": {
                "value": "- [Major] Line 370-372 mentions that the proposed method is first-frame conditioned, which is significantly different from other methods' settings (class-conditioned generation) in Table 3. This indicates completely unfair comparisons.\n\n- The ablations on block size are not fine-grained enough given that the optimal point is 16. Additional values in [1, 16] and [16, 64] should be also investigated. Besides, what is the best block size for temporal axis? 1 is used for all experiments without discussion."
            },
            "questions": {
                "value": "- Is the tokenizer completely identical to MAGVIT-v2 or is there any modifications? Its performance is tested separately in Table 1, but the paper describes its architecture as the same as MAGVIT-v2. Also, in Table 1 the reconstruction performance falls behind vanilla MAGVIT-v2 with comparable parameter size."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Next Block Prediction (NBP) for video generation, extending Next Token Prediction (NTP) to predict multiple tokens (a \"block\") at once. By shifting the prediction unit from a single token to a block, NBP achieves an 11x speedup and better performance than NTP during inference."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The method is simple yet delivers better speed and performance than NTP.\n- The scalability of the NBP framework is well-demonstrated in the paper.\n- The writing is clear and easy to follow.\n- The analysis on block size is thorough and well-explained."
            },
            "weaknesses": {
                "value": "- Section 3.1 (Video Tokenization) cannot be considered an original contribution, as the authors straightforwardly used MAGVITv2[1]. Labeling Section 3.1 as a preliminary section is recommended.\n- Although the authors differentiate NBP from MAR[2] in Section 2, there is no supporting evidence that NBP offers denser supervised signals or greater training efficiency. To strengthen the paper\u2019s contribution, it would help to include a comparison showing NBP\u2019s advantage over MAR\u2019s next set-of-tokens prediction by excluding the mask tokens.\n\n\n[1] Yu, et al. \"Language Model Beats Diffusion--Tokenizer is Key to Visual Generation.\" arXiv 2023.\\\n[2] Li, et al. \"Autoregressive Image Generation without Vector Quantization.\" arXiv 2024."
            },
            "questions": {
                "value": "- The paper states that the model was trained on 17-frame videos, but the TATS score refers to a model trained on 16 frames. Could the authors clarify the process for measuring FVD with NBP models? Specifically, how is the first frame provided to the NBP model in UCI and K600 experiments, and is this frame included when measuring FVD?\n\n- Should the blocks follow a raster scan order? While the authors state that the AR model\u2019s unidirectional raster-scan pattern limits performance, NBP still uses this order in block-level. If it is not, extending the block size analysis in Section 4.5 to examine different block shapes could be beneficial. For example, each block could be constructed from non-nearby tokens within a clip or even from tokens across multiple clips.\n\n### Suggestions\nThis paper could be more impactful by focusing on NBP's advantages over MAR [1] and providing a more comprehensive analysis of block design.\n\n[1] Li, et al. \"Autoregressive Image Generation without Vector Quantization.\" arXiv 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a semi-autoregressive video generation model that enables next-block prediction (NBP),  i.e., predicting multiple tokens in parallel. The NBP model uses a block-wise causal attention matrix, i.e., causal attention inter-block and bidirectional attention intra-block, capturing better spatial dependencies. Extensive experiments show the state-of-the-art video generation quality of NBP and a significant improvement in inference speed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper proposes a semi-autoregressive paradigm (i.e., next block prediction) for video generation, which brings better spatial dependencies in the attention computation and a significant inference speed improvement\n- This paper provides extensive experiments in terms of the design choice for block division. The model shows a good trade-off between inference speed and generation quality.\n- The writing and presentation of this paper are clear and easy-to-follow"
            },
            "weaknesses": {
                "value": "- The technical innovation from \"next token prediction\" to \"next block prediction\" is a bit trivial.\n  - Since there have been many studies on the semi-autoregressive paradigm (blockwise attention and parallel decoding) in the NLP [1,2] and vision[3,4]  fields, the work done in this paper is more like an engineering application rather than a technological innovation.\n  - In addition to simply changing the model prediction and the attention map, this paper does not outline the technical challenges or insights encountered in modifying an AR model to semi-AR (i.e., from \"next token prediction\" to \"next block prediction\" ). The author(s) may provide some clarifications and insights in the rebuttal.\n\n$\\quad$\n- The semi-AR (i.e., next-block-prediction) paradigm proposed in this paper does not seem to be restricted to video generation.  This means when an image tokenizer is used, it can also be applied to image generation. As a general semi-AR paradigm, quantitative comparisons on the  ImageNet dataset are suggested for drawing more convincing conclusions in this paper.\n\n$\\quad$\n- The temporal axis is not considered in the block division for all design choices (e.g., 1x1x16, 1x4x16, and1x16x16) presented in this paper. \n  - Is this because the video tokenizer (currently MAGVITv2 is used in the paper with a 4 x temporal downsampling) ? \n  - How does dividing blocks along the temporal axis influence the results of video generation?  Further ablation studies are suggested.\n\n\n$\\quad$\n\n[1] Stern, Mitchell, Noam Shazeer, and Jakob Uszkoreit. \"Blockwise parallel decoding for deep autoregressive models.\" *Advances in Neural Information Processing Systems* 31 (2018).\n\n[2] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. \"Fast inference from transformers via speculative decoding.\" *International Conference on Machine Learning*. PMLR, 2023.\n\n[3] Li, Jiacheng, et al. \"Lformer: Text-to-Image Generation with L-shape Block Parallel Decoding.\" *arXiv preprint arXiv:2303.03800* (2023).\n\n[4] Tian, Keyu, et al. \"Visual autoregressive modeling: Scalable image generation via next-scale prediction.\" *arXiv preprint arXiv:2404.02905* (2024)."
            },
            "questions": {
                "value": "- In Figure 4,  when the first frame is added as the initial condition, the attention map should have extra columns right next to the text column. Should Figure 4 be adjusted, or is my understanding mistaken ?\n- Is there any performance improvement in terms of the temporal consistency of generated videos when using NBP over NTP ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}