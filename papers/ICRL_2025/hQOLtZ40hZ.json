{
    "id": "hQOLtZ40hZ",
    "title": "Orthogonalized Estimation of Difference of Q-functions",
    "abstract": "Offline reinforcement learning is important in many settings with available observational data but the inability to deploy new policies online due to safety, cost, and other concerns. Many recent advances in causal inference and machine learning target estimation of causal contrast functions such as CATE, which is sufficient for optimizing decisions and can adapt to potentially smoother structure. We develop a dynamic generalization of the R-learner (Nie and Wager 2021, Lewis and Syrgkanis 2021) for estimating and optimizing the difference of $Q_\\pi$-functions, $Q_\\pi(s,1)$\u2212$Q_\\pi(s,0)$ (which can be used to optimize multiple-valued actions). We leverage orthogonal estimation to improve convergence rates in the presence of slower nuisance estimation rates and prove consistency of policy optimization under a margin condition. The method can leverage black-box nuisance estimators of the $Q$-function and behavior policy to target estimation of a more structured $Q$-function contrast.",
    "keywords": [
        "offline reinforcement learning",
        "causal inference",
        "orthogonal estimation",
        "heterogeneous treatment effects"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=hQOLtZ40hZ",
    "pdf_link": "https://openreview.net/pdf?id=hQOLtZ40hZ",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes an approach based on the concept of R-learner to estimate the difference of Q-function and subsequently policy optimization in reinforcement learning. The authors provide theoretical guarantees and conduct experiments on both synthetic data and CartPole to support their approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The use of R-learner in causal inference to improve RL convergence rate appears novel. \n- The author provides concrete theory on convergence guarantees. The technical analyses appear solid (Besides some typos, the proof seems sound). \n- Experiments indicate that the proposed methods outperform baselines in terms of the mean square error in estimating differences of Q."
            },
            "weaknesses": {
                "value": "1. The main theoretical implications of this paper need to be explained more clearly. The authors mention they proved theoretical guarantees of *improved* convergence rate. It is unclear what works/results this paper is comparing with. Given the large RL literature, the author might need to discuss their improvements over existing approaches/results in much more clarity to position their contributions.\n\n2. Assumption 1 (sequential unconfoundedness) needs justification. While it may be natural for other settings, whether this assumption is proper in RL is unclear. The assumption suggests actions only affect rewards but not the Markovian dynamic, which seems restrictive and also makes the problem potentially much easier.\n\n3. My understanding is that the ultimate goal of the authors' approach is to obtain better RL policies, i.e., estimating difference of Q is only a means towards this ultimate goal. However, the experiments only consider the mean square error in estimating difference of Q. So there's a mismatch between the goal and the experimental results. Moreover, this limits the scope of baseline methods as most methods aim to obtain RL policies other than merely estimating difference of Q.\n\nThe above are my main concerns. In addition, the following might help improve the paper:\n1. Fix typos, e.g., Eq. (3) $\\epsilon(A_t)$ should be $\\epsilon_t$, Eq. (4) $A$ should be $A_t$. Terminologies are not consistent, e.g., OrthDiff-Q v.s. $\\tau$-TL v.s. DiffQ.\n2. Experiments can be described more precisely. For example, in the Misaligned exo-endo experiment, $\\beta_{dense}$ is not defined, and it is unclear how the data is generated."
            },
            "questions": {
                "value": "1. Does Assumption 1 hold in the numerical examples?\n2. Connection with literature: E.g., how is Assumption 1 related to \"Exogenous State MDP\" in Dietterich et al., 2018? Regarding reward/value estimation, how is the proposed method compared to, e.g., Pan & Scholkopf, 2024, and others?\n3. How should we understand or estimate $\\alpha$ and $\\delta_0$ in Assumption 5? This looks like an assumption for the sake of proof, and it's unclear how they can be understood for a given problem.\n4. The cross-fitting part seems computationally intensive. What is the runtime of the experiment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper considers estimation of advantage function in offline RL setting. The authors use ideas from R learners, which was designed for heterogenous effect estimation, and propose an estimator for difference-in-Q function. By introducing three nuisances (Q, m, pi^b), the authors obtain an identifying conditional moments and relevant loss function, of which the proposed estimator is the empirical minimizer. The paper derives the estimation error, policy evaluation and policy optimization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Incorporating ideas from R learner to estimating difference of Q function is interesting. The paper details the required error rate conditions for nuisances estimations for the proposed method to work, which is also technically challenging. \n\n2. The results presented are extensive, including policy evaluation and optimization."
            },
            "weaknesses": {
                "value": "Some suggestions on presentation \n1. Theorem 2 and 3: if the requirement on nuisances rates are the same, then it may be better to just state once. \n\n2. Theorem 3 is long. It may be better to move some of them out as assumptions and explain. \n\n3. More generally, the whole paper proceeds by introducing new definition and assumptions, and then stating algorithms / theorems. It may be better to add more explanations about assumptions and motivations.\n\n\nTypos.\nLine 133: u should be superscript, and why do we need X here?"
            },
            "questions": {
                "value": "1. In theorem 3, is there a reason to expect the $\\rho$ to increase?\n\n2. What is the theoretical advantage of R-learning based method over traditional methods in terms of estimating (difference of ) Q function using offline data? Traditionally methods are based on Inverse Propensity Scoring, or its doubly robust variant. These methods can also be used to estimate difference in Q."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper brings the tools from orthogonal ML into offline\nreinforcement learning, with an orthogonalized estimator for the\n\"difference in Q function\" in the offline policy\nevaluation/optimization settings. The high level point is that the\ndifference of Q functions \\tau(s_t) = Q^\\pi(s_t,1) - Q^\\pi(s_t,0) satisfies\nthe moment equation:\n\nr_t + \\gamma Q^\\pi(s_{t+1},a_{t+1}) - E[r_t+\\gamma\nQ^\\pi(s',\\pi_b(s')|s_t] = (a_t - \\pi_b(1|s_t))\\tau(s_t) + \\eps\n\nThis motivates a strategy where all unknown quantities above except\nfor \\tau are estimated in the first stage (as nuisance parameters) and\nthen plug-in version of these are used to estimate \\tau in the second\nstage. This can be combined with dynamic programming for both policy\nevaluation and policy optimization, resulting in the two main\nalgorithms in the paper.\n\nAs far as theoretical results, I think they can be summarized (and\nsimplified a bit) as follows: suppose all the nuisance functions can\nbe estimated at a n^{1/4} rate, then we can estimate \\tau at a n^{1/2}\nrate. For policy optimization, this becomes non-trivial because the\nnuisance function at time $t$ is a function of the primary estimand\n$\\tau_{t+1}$, so one must show that this more complex nuisance\nfunction can still be estimated at a fast rate.\n\nComments:\n\n1. Overall, the paper is quite hard to read, even if you have a\nbackground in both RL and orthogonal ML (as I do). I think the\nnotation could be improved significantly, and the theorem statements\nare very difficult to parse/understand. Some simplifications would\nhelp significantly I think, without compromising the main ideas of the\npaper, for example:\n\n- Work in the well-specified setting, so there is no need for \\tau^n\n\n- Don't worry about localized rademacher complexity\n\n- Just do Massart margin condition\n\nBasically, I get the impulse to consider the most general settings,\nbut I think this significantly compromises the readability of the\npaper and isn't often very relevant toward the main points. One idea\nis to write the main body of the paper in the simplest possible\nsetting that conveys the main technical and conceptual innovations and\nthen put the most general results in the appendix.\n\n2. To this end, I actually couldn't figure out what is the technical\ninnovation here. I saw the paragraph saying that the policy\noptimization part, specifically dealing with the dependence between\n\\hat{\\tau}_{t+1} and the nuisance function for time t is the main\nchallenge. My first impression is that the margin assumption allows\none to turn a ell_2 guarantee into an ell_{\\infty} guarantee, which\none can use to perhaps break this dependence? But honestly I didn't\nreally get it and the theorem statement is way too complicated for me\nto try to infer what is happening.\n\n3. I also think the paper could benefit from connecting to the prior\nliterature on offline RL theory. In particular how do requirements\nlike \"bellman completeness\" appear in the analysis. I don't think it\nis possible to estimate Q functions, essentially at all, without such\nassumptions in the offline RL setting, see e.g., [1]. So where does\nsuch an assumption come into the analysis?\n\nOverall, my main concern is that the paper is way too hard to read,\neven for relative experts in the area. I think it will not be\nappreciated by most of the ICLR audience. And I might be wrong about\nthis (due to difficulty in understanding the paper), but I currently\nfeel that in terms of technical novelty, the paper does not quite meet\nthe bar set by the conference.\n\nReferences\n\n[1] Dylan J. Foster, Akshay Krishnamurthy, David Simchi-Levi, Yunzong\nXu. Offline Reinforcement Learning: Fundamental Barriers for Value\nFunction Approximation. COLT 2022."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "see above"
            },
            "weaknesses": {
                "value": "see above"
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel approach with statistical theory for offline reinforcement learning (RL) through the development of a dynamic generalization of the R-learner. This method is designed for estimating and optimizing the difference between Q-functions under two different actions for discrete-valued actions. The key contributions and results of this work are as follows:\nThe authors extend the R-learner framework, commonly used in causal inference, to the offline RL setting. This generalization allows for the estimation of Q-function differences, which can be utilized to optimize decision-making for multiple actions.\n\nOne of the contributions of this work is the use of orthogonal estimation techniques, which improves convergence rates and as a result mitigates the effects of unstable estimations in behavior policies, which is a common issue in offline RL.\n\nThe proposed method focuses on estimating a more structured Q-function contrast, which adapts to underlying sparse or smoother structures in the data. This is crucial because causal contrasts, such as the difference in outcomes between actions, often exhibit more structure (e.g., sparsity) than the Q-functions themselves. The method, therefore, leverages this property to improve policy optimization.\n\nTheoretical guarantees for the proposed method are given, ensuring consistency in policy optimization. The framework is capable of estimating causal contrasts in a sequential decision-making setting, with applications for learning dynamic treatment rules based on observational data. Synthetic experiments are presented to demonstrate the effectiveness of their proposed approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality: This paper has originality. First, it extends the R-learner from causal inference to the dynamic setting of offline RL. I agree that focusing on Q-function contrasts rather than estimating full Q-functions is a novel approach, offering advantages in terms of adaptability to structured or smoother forms like sparsity. Additionally, the use of orthogonal estimation to stabilize policy optimization and improve convergence rates is an original contribution to offline RL, leveraging causal inference techniques in a new domain. The method's ability to adapt to sparse structures in the Q-function contrast further enhances its uniqueness, aligning it with modern trends in machine learning that exploit underlying structure for efficiency. \n\nQuality: This paper has good quality. First, the application of orthogonal estimation techniques in this context seems a non-trivial approach to me, and it addresses critical challenges in offline RL such as slow convergence.\n\nClarity: This paper states its contributions clearly. In addition, as a theoretical work, the paper clearly introduced its mathematical setup: the assumptions and theorems are clearly specified. An illustration of the sequential decision making structure is given by figure 1. \n\nSignificance: This paper seeks to address a major problem in offline RL. Specifically, the instability of policy optimization due to slow convergence of nuisance functions (Q-functions and behavior policies). The incorporation of orthogonal estimation improves the stability and reliability of learning. The focus on estimating Q-function contrasts rather than the entire Q-function offers a more adaptable and potentially more efficient way to optimize decisions, especially in environments where the contrast between actions is more structured or smoother. This could lead to better policy performance in practice, addressing the need for more efficient learning in complex decision-making problems."
            },
            "weaknesses": {
                "value": "As a theoretical work, the paper does not clearly highlight the main technical challenges in its analysis. While the incorporation of orthogonal estimation to improve the robustness of offline RL is a valid contribution, it would be stronger if the paper explained why this is technically challenging. Alternatively, discussing the difficulties in the theoretical analysis of the proposed algorithm would enhance its depth. \nIndeed, the paper claimed to have followed the analysis of Foster and Syrgkanis for orthogonal statistical learning. This is not an issue but the paper should also elaborate what makes the analysis in this paper different from the previous theoretical analysis, if applicable. For example, does the paper have a distinct error decomposition? Does the convergence analysis for Theorem 2 become different from previous work because some error term takes a different form? etc.\n\nI think the following might be some helpful suggestion the authors could consider (if applicable).\n\n(1) Include a paragraph after Theorem 2 or 3 explaining the key technical difficulties they encountered in proving those results.\n\n(2) Explicitly compare their error decomposition or convergence analysis to previous work, pointing out any additional terms which that bring extra difficulty to the proof. \n\n(3) Add a short paragraph at the beginning of the theoretical analysis session to highlight the key technical challenge and novelty on a high level.\n\nIn summary, the paper could benefit from more explicit discussion of the theoretical challenges or innovations in its analysis.\n\nMinor issue:\nThere seems to be some typos in the sentence from line 419-422."
            },
            "questions": {
                "value": "Please discuss the unique theoretical challenge or innovations, as pointed out in **Weaknesses**."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}