{
    "id": "idnMNjlEj5",
    "title": "EnvBridge: Bridging Diverse Environments with Cross-Environment Knowledge Transfer for Embodied AI",
    "abstract": "In recent years, Large Language Models (LLMs) have demonstrated high reasoning capabilities, drawing attention for their applications as agents in various decision-making processes. One notably promising application of LLM agents is robotic manipulation. Recent research has shown that LLMs can generate text planning or control code for robots, providing substantial flexibility and interaction capabilities.\nHowever, these methods still face challenges in terms of flexibility and applicability across different environments, limiting their ability to adapt autonomously. Current approaches typically fall into two categories: those relying on environment-specific policy training, which restricts their transferability, and those generating code actions based on fixed prompts, which leads to diminished performance when confronted with new environments. These limitations significantly constrain the generalizability of agents in robotic manipulation.\nTo address these limitations, we propose a novel method called EnvBridge. This approach involves the retention and transfer of successful robot control codes from source environments to target environments. EnvBridge enhances the agent's adaptability and performance across diverse settings by leveraging insights from multiple environments. Notably, our approach alleviates environmental constraints, offering a more flexible and generalizable solution for robotic manipulation tasks.\nWe validated the effectiveness of our method using robotic manipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments demonstrate that LLM agents can successfully leverage diverse knowledge sources to solve complex tasks. Consequently, our approach significantly enhances the adaptability and robustness of robotic manipulation agents in planning across diverse environments.",
    "keywords": [
        "LLM Agent",
        "Robotic Manipulation",
        "Cross-Environment Knowledge Transfer"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "We propose EnvBridge, a robot manipulation agent that enhances performance across various environments by using knowledge transfer to apply successful control codes from one benchmark to new environments, allowing flexible task re-planning.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=idnMNjlEj5",
    "pdf_link": "https://openreview.net/pdf?id=idnMNjlEj5",
    "comments": [
        {
            "comment": {
                "value": "Thank you very much for taking the time to review our paper and providing your feedback.\nI would like to address each of your comments in detail below.\n \n1. Comments on the Introduction Section\n\n   Thank you for your opinions. We understand that extraneous information can lead to confusion. \n   Our introduction flows from the general landscape of LLMs and recent developments, followed by a transition to robotic manipulation as a use case that is gaining traction, and then to limitations of current works in the area before proposing our solution to tackle these limitations.\n   We would appreciate it if you could let us know which points you found redundant.\n \n2. Clarity of the Article\n\n   We appreciate this feedback.\n   We will consider revising the sentences to provide detailed and clear definitions.\n \n3. Quality of the Figures\n\n   We acknowledge that clear figures can greatly enhance comprehension.\n   We would be grateful if you could let us know which aspects of the figures you found concerning.\n \n4. Disagreement on Generalization Issues\n\n   We sincerely thank the reviewer for raising this important point about generalization capabilities. We would like to respectfully offer some clarification about our specific findings:\n\n   4.1. Our paper identifies two specific limitations in current LLM-based approaches:  \n   - Policy-based methods require environment-specific training, limiting cross-environment transfer\n   - Code generation methods struggle with fixed prompts in unseen environments\n\n   4.2. Our experiments systematically examined these limitations:  \n   - The baseline tests show VoxPoser achieves 36.5% success on RLBench and 25% on MetaWorld tasks\n   - These results suggest there may still be room for improvement in handling unseen environments\n\n   4.3. Our proposed approach shows promising progress in addressing these challenges:  \n   - Improving success rate to 69.0% on RLBench\n   - Achieving 56% success rate on MetaWorld\n\n   We would be very interested to learn about the specific LLM-based approaches the reviewer has in mind that demonstrate strong generalization abilities in robotic manipulation. This would be valuable for our research and help us provide more comprehensive comparisons. We are always eager to learn from and build upon successful approaches in the field.\n\nOnce again, thank you for your valuable feedback and time.\nTo conduct better research, we would be grateful if you could provide more detailed advice, if possible."
            }
        },
        {
            "summary": {
                "value": "This work presents EnvBridge, a novel approach for computing robot trajectories via LLM-prompted code generation. The authors argue that transferring code across different environments, similar to human reasoning, can improve the success rate of task completion. EnvBridge extends existing methods like VoxPoser by adding a knowledge transfer mechanism to bridge across environments.\n\nThis process leverages a hierarchical code generation approach, dividing the task into subtasks (Planner) and generating 'language model programs' (LMPs) for each subtask (Composer). These LMPs produce value maps that guide the generation of an open-loop trajectory. I believe this trajectory is for the end-effector pose.\n\nThe system operates in several stages. First, an LLM generates code that outputs a trajectory, based on a language query describing the task. This is the same as VoxPoser. If unsuccessful, EnvBridge searches a repository for similar tasks where code generation failed at the same stage (Planner or Composer). Successful code from a similar task is then transferred to the target environment and used as an example in a revised prompt for the LLM, known as Knowledge Transfer and Re-planning."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The primary strength lies in its novel approach to knowledge transfer for robot code generation via appropriate prompt engineering. The idea is well-motivated by observations of human behavior and offers a promising direction for improving LLM performance in robotics. The paper is generally well-written and presents the approach with clarity. Furthermore, the authors conduct extensive experiments across multiple tasks and environments in simulation, demonstrating the effectiveness of EnvBridge."
            },
            "weaknesses": {
                "value": "1. EnvBridge generates open-loop trajectories, lacking the reactivity necessary for dynamic environments.\n2. To my understanding, the system focuses solely on end-effector positions, neglecting possible collisions of the robot arm with the environment.\n3. The user must explicitly provide information about objects to avoid in the scene, which could be automated through prompt engineering. For example, in the example memory shown in section A.3, the task is to \"pick up the rubbish and leave it in the trash can.\", and there are a bunch of objects given. The Planner level should create subtasks to avoid other objects (e.g. 'tomato1') while picking up the object named 'rubbish'.\n4. The evaluation could be strengthened by comparing EnvBridge to other baselines like OpenVLA, which also generates end-effector poses from similar inputs.\n5. The authors should provide further clarification regarding the performance discrepancies observed in Figure 2, where code from the same environment (RLBench) performs worse than transferred code from a different environment (CALVIN) - even though RLBench memory has more code examples than those in CALVIN. \n6. It would be good to have real-robot experiments, to improve the case for accepting any robotics paper.\n\nFinally, to best understand the work, you need to see the prompts given in the appendix, and the type of prompt engineering used for each stage of EnvBridge. While this is not explicitly a weakness, these are important details worthy of being in the main paper.\n\n### Minor Points\n\nA possible typo on line 57: \"transfer ability\" should likely be \"transferability\"??\nThroughout the paper (e.g., line 694), the authors should explicitly clarify that curly braces in the prompts `{}` denote variables to be filled in."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes EnvBridge, an LLM-based method that can leverage diverse knowledge to solve complex tasks. It contains several components to endow the agents with good generalization ability."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The proposed methods are evaluated on 3 benchmarks."
            },
            "weaknesses": {
                "value": "- The Introduction section includes many discussions not related to LLMs for robotic manipulation, which appears redundant.\n\n- The writing of the article is not very clear; concepts such as cross-embodiment and cross-environment need to be more distinctly defined. This significantly detracts from the overall logic of the paper.\n\n- The quality of the figures in the paper is poor.\n\n- I do not agree with the author's assertion that current LLM-based methods have significant generalization issues. Many LLM-based approaches have demonstrated strong generalization abilities in real-world applications. I also do not believe that the method proposed by the author addresses the existing issues effectively.\n\n- I believe that this paper is not yet ready for submission to ICLR."
            },
            "questions": {
                "value": "Seen above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces ENVBRIDGE, a method that leverages Large Language Models (LLMs) for cross-environment knowledge transfer in robotic manipulation. The core idea is to use the control code from source environments as prompts to generate suitable control actions in new target environments. The proposed approach is tested across three common robotic manipulation benchmarks: RLBench, MetaWorld, and CALVIN."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The use of LLMs for cross-environment task transfer is a creative and intriguing approach. I especially like the method of using previously successful control code as in-context prompts for generating control code in new tasks.\n2. The experiments are conducted across three well-established manipulation benchmarks."
            },
            "weaknesses": {
                "value": "1. My primary concern is the practical applicability of the method. Why should we rely on LLMs to generate control code for simulated environments, and how feasible is it to extend this approach to real-world robotic systems? As far as I understand, the method depends on ground-truth states and simulation-specific API calls, which may not easily translate to real-world scenarios where such information is not readily available. \n2. Certain aspects of the experimental setup are not clearly explained. I have raised specific questions about this below."
            },
            "questions": {
                "value": "1. Could the authors clarify whether the method uses ground-truth states as input or camera inputs for perception?\n2. The experimental protocol in Section 4.1 is somewhat ambiguous. In lines 340\u2013341, the paper mentions \u201cWe sampled 10 tasks from it and conduct evaluations covering various tasks, instructions, and objects.\u201d Does this imply different variations were introduced within these 10 tasks? More explanation would help.\n3. In Section 4.2, why didn\u2019t the authors use CALVIN as a source task, similar to the approach in Section 4.1?\n4. In Section 4.3, what were the source tasks used for this experiment? Were they drawn from RLBench and MetaWorld?\n5. The paper only compares ENVBRIDGE with VoxPoser, yet there are other related works that also utilize LLMs for generating robot control code. How does ENVBRIDGE compare to methods like Code as Policies?\n6. Some minor formatting issues exist in the paper, such as mismatched quotation marks in lines 192 and 193."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}