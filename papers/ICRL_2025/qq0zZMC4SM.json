{
    "id": "qq0zZMC4SM",
    "title": "Synthetic Datasets for Machine Learning on Spatio-Temporal Graphs using PDEs",
    "abstract": "In this work, we describe the creation and use of synthetic datasets based on various partial differential equations to support spatio-temporal graph modeling in machine learning for different applications. More precisely, we showcase three equations to model different types of disasters and hazards in the fields of epidemiology, atmospheric particles, and tsunami waves. Further, we show how such created datasets can be used by benchmarking several machine learning models on the epidemiological dataset and, additionally, by showing how pre-training on such synthetic datasets can improve model performance on real-world epidemiological data. The presented methods enable others to create datasets and benchmarks customized to individual requirements. The source code for our methodology and the three created datasets can be found on https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs .",
    "keywords": [
        "Data",
        "Dataset",
        "PDE",
        "Graph",
        "Spatio-Temporal",
        "Epidemiology",
        "Benchmarking"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We introduce three synthetic temporal graph datasets, along with a method and code for their and similar datasets creation, and demonstrate their general use throughout benchmarking and Transfer learning on real-world tasks.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=qq0zZMC4SM",
    "pdf_link": "https://openreview.net/pdf?id=qq0zZMC4SM",
    "comments": [
        {
            "summary": {
                "value": "The authors identify a lack of established benchmark data sets for spatio-temporal graph learning, which inspires them to generate synthetic spatio-temporal graph data using partial differential equations (PDEs). They consider 3 different PDEs: an epidemiological PDE, an advection-diffusion equation, and a wave equation. They then use the synthetic data generated from the epidemiological PDE to compare different temporal and spatio-temporal models on different prediction tasks, including forecasting. Perhaps most importantly, they demonstrate the importance of the synthetic data for transfer learning on a prediction task involving real epidemiological data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- High potential significance by creating synthetic data benchmarks in an area where they are lacking. I could certainly envision these datasets being used in future spatio-temporal graph learning papers.\n- Very interesting experiment on real epidemiological data demonstrates the potential of the synthetically generated data to translate to prediction tasks on real data. This is much stronger evidence for the utility of the synthetic data than I typically see in this type of paper.\n- Detailed comparison of temporal and spatio-temporal models on a variety of prediction settings for the synthetically generated epidemiological data."
            },
            "weaknesses": {
                "value": "- Some missing details on the real data experiment--see question 1 below. A more detailed description in the supplementary material would be useful.\n- Sizes of the datasets seem to be fixed to somewhat small spatio-temporal graphs with a few hundred nodes and a few thousand edges, potentially limiting the scope.\n- No results on the advection-diffusion and wave equation data in the body of the paper. Given the interests of the ICLR audience, I believe that the paper would be strengthened if there were more results on these datasets, particularly in the main body of the paper, while moving some details from Section 2 into the supplementary material.\n\nMinor issue:\n- Line 041: datata -> data"
            },
            "questions": {
                "value": "1. What is the prediction task in Section 4.3? Is this a forecasting task?\n2. Is there an easy way to generate larger spatio-temporal graphs using your proposed method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a method to create synthetic datasets based on different PDEs that capture different applications. Three PDE equations are presented as examples, namely epidemiology, atmospheric particles, and tsunami waves. Empirical analysis demonstrates that the generated synthetic datasets can be used to benchmark machine learning models, and pre-training on the synthetic datasets can greatly improve model performance on real-world epidemiological data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Contribution: As the spatio-temporal graph data lacks, this paper presents an alternative. With high-quality synthetic datasets, machine learning studies can be improved compared to theoretical analysis.\n\nPresentation: The three example PDE equations came from reliable sources, and help with demonstrating how to generate synthetic datasets."
            },
            "weaknesses": {
                "value": "1. This paper boldly claimed that real-world datasets have limitations in quality due to high noise. I personally believe that clean synthetic datasets are better for exploration and preliminary studies. In contrast, although the real-world dataset has high noise, it is necessary before the application is implemented. So, there is a trade-off between cleanness and reality. \n\n2. Other ways exist to generate synthetic datasets, such as quasi-Monte Carlo simulation. This paper fails to compare with existing methods to demonstrate the superiority of the PDE-based generation method.\n\n3. In the empirical analysis of the pre-training, effectiveness is measured by RMSE loss, which could skew towards the high-performing models. With previous low RMSE losses, a small definite improvement will show a bigger percentage. Have you considered other measurements?"
            },
            "questions": {
                "value": "How is the presented dataset generation method better than the existing ones? I think this comparison need to be discussed in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this work, the authors generate synthetic spatio-temporal graphs with fixed structure and evolving node features. These datasets can be used to assess the forecasting performance of models and serve as benchmark. The published source code can be useful for the community."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The synthetic datasets generated by the method described in the paper can be used by the community for benchmarking models.\n- The authors identified the gap in the literature, wherein high quality temporal graph datasets are not abundant."
            },
            "weaknesses": {
                "value": "### `(W1) Benchmarking`\nThe authors use the following models:\n- Repetition (naive)\n- RNN (classic)\n- TST (Transformer)\n- MP-PDE (modified baseline)\n- RNN-GNN-Fusion (source of this model is not clear)\n- GraphEncoding (modified baseline)\n\nThey report the performance of these models on the synthetic datasets in Table 1 (Forecasting column), where the model `GraphEncoding` performs worse than the naive baseline `Repetition`. This is an odd observation, and better baselines should've been used for benchmarking.\n\nIt is recommended to check the paper **Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting** (TMLR) and the baselines used there. For example, the authors could test out the benchmarks `STGODE`, `GRAMODE`, and `ARIMA` on the generated datasets.\n\n### `(W2) Contribution Claim`\nThe PDEs used to create the datasets are not novel, nor is the technique to solve them. The authors claim:\n> This or any similar epidemiological PDE, based on the SIR-ODE (Kermack & McKendrick, 1991), has never been solved numerically\n\nAnd then in footnote 1 on page 4, the authors mention:\n> The methodology we use here as well as the numerical code can be found in this tutorial of the used library https://www.dealii.org/current/doxygen/deal.II/step_23.html. We made only smaller adaptions to the code.\n\nIt is not clear what is the contribution of this work, apart from setting hyper-parameters of known PDEs and solving them numerically using known libraries.\n\n### `(W3) Lack of new insights`\nConsider this experiment: there are n number of models $M_1, M_2, \\cdots, M_n$ and a standard dataset $D$ which is used in the literature. When the models are tested on this dataset, it results in a ranking $r_1, r_2, \\cdots, r_n$. Now consider a synthetic dataset $D'$ which when used for benchmarking the models results in the ranking $r_1', r_2', \\cdots, r_n'$. Comparing these two rankings can highlight the use of the synthetic dataset $D'$ and the authors could comment on the reason for such change in ranking, if any. On the other hand, if there is no change in the ranking, then the dataset $D'$ brings no new insights.\n\nIt is recommended that the authors run this experiment using at least two standard spatio-temporal traffic datasets `METRLA` and `PEMSBAY`.\n\n### `(W4) Organization of the contents`\nThe content in the paper is not well organized. The focus was moving from one topic to another without a smooth logical transition. Here are some thoughts on an alternative structure for the paper:\n- In **Introduction** the authors only discuss the problem they are trying to solve and dedicate a clear paragraph to the motivation, then in a subsection they discuss the **related works**, as in what has been tried before in the same setting.\n- Then, in a **Methodology** section, the authors should expand on their proposed technique using a block diagram giving a higher-level overview of the work, and present a detailed version through a step-by-step algorithm. The PDE should be kept generic, and the three special cases can just be mentioned after the algorithm is presented\n- Then, a section on **Experiments** should be included to specify clearly what the authors what to conduct, and divide experiments into self-contained subsections, with highlighted key insights derived from the results. Here, the authors may mention the baselines being used and delegate further information on their implementation/modification to the Appendix for those interested.\n- The figures with **dataset examples** should be deferred to the Appendix as well.\n- In the conclusion section, the authors should clearly state the **limitations** of the work, and what could be potentially improved in the future.\n- Some results which are currently in the Appendix should be brought to the main body.\n\n### `(W5) Limitations of the generated datasets`\nThe graph structure is fixed, and the dimensionality of the node feature is limited to 2."
            },
            "questions": {
                "value": "### `(Q1) Transfer Learning` (Sec. 4.3)\n- Could the authors please elaborate on the meaning of fine tuning?\n- How does the chosen (modified) baselines used in this paper compare against the standard spatio-temporal benchmarks, for example `GraphWaveNet`, and `STGODE`?\n- Why did the authors only report validation loss?\n- What is the test RMSE on (1) training a model $M$ on real data $D$, and (2) training the model $M$ on real data $D$ after pre-training on synthetic data $D'$?\n- How do the models compare against the naive repetition baseline?\n\n### `(Q2) Noise` (Sec. 4.2)\nThe authors mention:\n> We found the Gaussian noise with distribution $\\mathcal{N}(0, 0.01)$ to be an interesting setting for the normalized dataset.\n\nCould they please provide their rationale for choosing the specific noise variance? The explanation would aid in understanding the experiment better.\n\n- The figures 10-12 are not clear; Why is there a drop in RMSE for dropout noise? Are the zero values emulating missing sensor data considered or ignored during evaluation? In the literature, zeros are treated as missing values and ignored during training and evaluation.\n- In Table 1, it is better to report the relative change in RMSE. That would show that the naive repetition baseline is robust to the Gaussian noise considered in the study.\n\n----\n\n### `Feedback for improvement`\n- The paper needs to be reorganized and re-written for clarity\n- Relevant baselines for node feature forecasting should be used for benchmarking\n- The benchmarking results should be contrasted with the ranking on standard datasets to deliver new insights\n- The transfer learning and noise robustness experiments need to be clarified and more details should be added\n- The experiment setup should be explained more clearly\n- The limitations of the work should be admitted\n- The contributions should be stated clearly\n- A scalability section can be added, where synthetic datasets can be generated for increasing number of nodes, and the relative performance of the baseline models can be reported across the varying size."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a methodology for generating synthetic spatio-temporal graph datasets using partial differential equations (PDEs). The authors illustrate their approach by creating three datasets that model disaster scenarios: the spread of epidemics, atmospheric particle movement, and tsunami propagation. The main contributions of this work include (1) a detailed methodology that employs the Finite Element Method to convert PDE solutions into graph data, (2) three ready-to-use synthetic datasets, (3) an open-source implementation for creating custom datasets, and (4) empirical validation through benchmarking and transfer learning experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The PDE solving methodology demonstrates mathematical rigor through detailed documentation and established numerical methods.\n- The authors provide practical resources through ready-to-use datasets and accompanying code for the research community.\n- The experimental results strongly validate the approach by showing significant benefits in transfer learning tasks.\n- The framework enables researchers to create custom datasets by modifying parameters and PDEs for different applications."
            },
            "weaknesses": {
                "value": "- The paper only demonstrates relatively simple PDEs despite covering three different scenarios.\n- The transfer learning experiments are limited to epidemiological applications without exploring other real-world domains.\n- The authors do not thoroughly analyze how variations in PDE parameters impact the learning outcomes."
            },
            "questions": {
                "value": "1. Have you explored how the choice of PDE parameters affects the quality of the synthetic data for transfer learning?\n2. Could this approach be extended to more complex PDEs with coupling or higher-order terms?\n3. How do the computational requirements scale with domain size and mesh resolution?\n4. Have you considered applications beyond the disaster scenarios presented?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}