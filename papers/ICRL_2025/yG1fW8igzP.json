{
    "id": "yG1fW8igzP",
    "title": "Data-Augmented Phrase-Level Alignment for Mitigating Object Hallucination",
    "abstract": "Despite their significant advancements, Multimodal Large Language Models\n(MLLMs) often generate factually inaccurate information, referred to as hallucination.\nIn this work, we address object hallucinations in MLLMs, where information\nis generated about an object not present in the input image. We introduce Dataaugmented\nPhrase-level Alignment (DPA), a novel loss which can be applied to\ninstruction-tuned off-the-shelf MLLMs to mitigate hallucinations, while preserving\ntheir general vision-language capabilities. To fine-tune MLLMs with DPA, we first\ngenerate a set of 'hallucinated' and 'correct' response pairs through generative data\naugmentation by selectively altering the ground-truth information of the correct\nresponses at a phrase level. The DPA loss is then used to train MLLMs to reduce\nthe likelihood of hallucinated phrases compared to the correct ones. Our thorough\nevaluation on various benchmarks confirms the effectiveness of DPA in mitigating\nhallucination while retaining the out-of-the-box performance of the MLLMs on\ngeneral tasks. For instance, MLLMs finetuned with DPA, which we refer to as Hallucination\nAttenuated Language and Vision Assistant (HALVA), improve F1 by up\nto 13.4% on hallucination visual question-answering and reduce the hallucination\nrate by up to 4.2% on image description tasks.",
    "keywords": [
        "Multimodal LLMs",
        "Object Hallucination",
        "Vision-language Models"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We introduce phrase-level alignment method that can be applied to off-the-shelf MLLMs for mitigating hallucinations, while preserving their general vision-language capabilities.",
    "creation_date": "2024-09-14",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=yG1fW8igzP",
    "pdf_link": "https://openreview.net/pdf?id=yG1fW8igzP",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes Data-augmented Phrase-level Alignment (DPA), a novel loss function designed to reduce object hallucinations in multimodal large language models (MLLMs) while preserving their general vision-language capabilities. By generating pairs of hallucinated and correct responses through data augmentation, DPA trains MLLMs to distinguish hallucinated phrases from correct ones. Experimental results show that MLLMs fine-tuned with DPA achieve significant improvements, reducing hallucination rates and enhancing performance on visual question-answering and image description tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The DPA loss function is an innovative approach that mitigates object hallucinations by targeting phrase-level distinctions, offering a focused solution for multimodal hallucination issues.\n\n* The data augmentation method is straightforward yet effective, generating hallucinated-correct response pairs that enable the model to learn nuanced differences with minimal complexity.\n\n* DPA demonstrates significant performance gains"
            },
            "weaknesses": {
                "value": "The core idea of the paper is to generate correct-hallucinated data pairs through data augmentation. However, I have three questions about this process.\n1. In hallucination-related datasets, object hallucination does not frequently occur, raising questions about the validity of replacing every possible object and attribute with hallucinations. Since models seldom generate such hallucinations, this augmentation strategy might introduce excessive \"non-realistic\" hallucination cases, leading to a mismatch between training and real-world distributions, potentially impacting the model's generalization.\n2. The method\u2019s effectiveness may be limited by the diversity of data augmentation. Since hallucinated data generation relies on a finite set of replacements, it may not fully cover the types of hallucinations that could appear in practical applications, limiting the model\u2019s ability to handle unseen hallucinations.\n3. The data augmentation strategy itself lacks independent experimental evaluation. The experiments mainly focus on improvements in model performance across different benchmarks, without assessing the augmentation strategy\u2019s generalization effect and impact on model training stability across tasks."
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a Data-Augmented Phrase-Level Alignment (DPA) approach aimed at reducing object hallucinations in Multimodal Large Language Models (MLLMs). The method centers on generating paired \u201ccorrect\u201d and \u201challucinated\u201d responses using data augmentation, which are then used to train a phrase-level alignment loss that reduces the probability of hallucinated tokens. The authors strive to maintain the model\u2019s overall vision-language capabilities while minimizing hallucinations. Experimental results across multiple benchmarks indicate that DPA effectively mitigates hallucinations and may even improve detailed object coverage in generated descriptions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper proposes an effective alignment method that successfully mitigates object hallucinations, showing improved scores across hallucination benchmarks in both discriminative and generative tasks.\n2. DPA reduces object hallucinations without impacting the overall performance on VQA tasks, achieving comparable or even higher scores on VQA benchmarks.\n3. The paper provides a variety of quantitative results across multiple benchmarks, including both generative and discriminative tasks. This breadth of evaluation provides some evidence of the DPA\u2019s effectiveness in certain contexts."
            },
            "weaknesses": {
                "value": "1. The DPA approach offers limited novelty beyond existing finetuning and data augmentation techniques. While phrase-level alignment is applied in a new way here, it basicly builds on existing concepts and does not significantly advance the field of hallucination mitigation research.\n2. Although the results include some competitive baselines, such as HA-DPO and EOS, several relevant and recent methods are omitted. A more comprehensive comparison would strengthen the evaluation. Additionally, some detailed results for LLaVA-13B and VILA are missing, and the selection of methods across different benchmarks lacks consistency.\n3. While the paper asserts that DPA preserves general vision-language capabilities, the supporting evidence is limited. A broader evaluation across diverse benchmarks would help determine whether this approach impacts overall performance.\n4. The authors highlight the limitations of existing methods, noting they \u201crequire massive training data.\u201d However, the proposed DPA also introduces additional training requirements, which suggests a tradeoff between efficiency and effectiveness."
            },
            "questions": {
                "value": "1. How does DPA perform on other types of hallucinations beyond object hallucination, such as attribute or location hallucinations?\n2. The reported results for some baseline methods, such as VCD, differ from those in the original papers. Did you directly test VCD, or were the results extracted from their papers? If the latter, the comparison may not be entirely fair, as it mixes experimental results with reported findings from other sources.\n3. This paper augments the training data with hallucinated responses by substituting terms in both open-set and closed-set cases. However, simple substitution could potentially impact fluency and grammatical accuracy. Could this approach compromise data quality and, in turn, affect model performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel method called Data-augmented Phrase-level Alignment (DPA) to mitigate object hallucinations in multimodal large language models (MLLMs) for vision-language tasks. DPA generates pairs of \u201challucinated\u201d and \u201ccorrect\u201d responses to fine-tune the model, reducing the generation of hallucinated phrases. A KL divergence regularization term is added to retain the model\u2019s general capabilities. Experimental results demonstrate that models trained with DPA exhibit significant improvements in hallucination mitigation and maintain strong performance on general tasks across multiple benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The writing of this paper is excellent and very detailed. The experiments are comprehensive, covering most of the popular benchmarks.\n\n2. I really like this paper. Many works that use DPO-like methods to reduce hallucinations experience a decrease in VQA capabilities. The authors identified this issue and proposed a specialized loss to maintain the model\u2019s performance while penalizing hallucinated phrases. Additionally, the authors validated their method separately on non-hallucination benchmarks, such as VQA-v2 and TextVQA, demonstrating its effectiveness. I believe this work makes a significant contribution to reducing hallucinations in MLLMs."
            },
            "weaknesses": {
                "value": "1. DPA relies on the quality of the generated \u201challucinated-correct\u201d response pairs. If these generated data lack accuracy or diversity, it may affect the model\u2019s training effectiveness and generalization capability.\n2. Although the experimental results demonstrate the effectiveness of DPA, the paper lacks a fine-grained analysis of hallucination types (such as objects, attributes, actions). Such analysis could provide a deeper understanding of the method\u2019s performance across different types of hallucinations."
            },
            "questions": {
                "value": "1. VILA is also based on LLaVA-1.5 SFT. Is DPA equally effective on other architectures, such as Qwen?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose data-augmented phrase-level alignment to mitigate object hallucination in MLLMs. The method mainly involves the generation of negative hallucinated responses and phrase-level fine-tuning. The hallucination issue of models fine-tuned with the proposed method is alleviated and the general performance is maintained."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. It is critical to study the issue of hallucination in MLLMs as well as the trade-off phenomenon in the mitigation of hallucination.\n2. The proposed method is simple yet effective.\n3. Extensive experiments verify the effectiveness of the method."
            },
            "weaknesses": {
                "value": "1. The motivation to mitigate hallucination at phrase level is not clearly addressed. There is no illustration on why the phrase-level loss can retain the original performance on general multimodal tasks. It seems straightforward that the constraint on the KL divergence can prevent the fine-tuned model from diverging too far. If it is the only reason, the contribution of the method is greatly weakened.\n\n2. The explanation of Figure 2 is likely to be overclaimed. On line 260, it says \"_EOS achieves a slightly lower hallucination rate_\", but the figure shows that the hallucination rate of EOS is around 5.0 and that of HALVA is around 6.5. This difference is noticeable enough to me as the gap of this metric between HALVA and LLaVA-1.5 is similar. Meanwhile, Figure A right demonstrates that HALVA has a much higher F1 score on AMBER, which is natural because neither EOS nor HA-DPO is trained with Yes/No questions.\n\n3. The presentation of results in experiments is inconsistent. The model lists are different in different tables. For instance, EOS-13B is only shown in Table 1, which makes the verification of the effectiveness less convincing.\n\n4. There are other work mitigating hallucinaton with sub-sequence level training [1]. It is recommended to discuss the difference.\n\n[1] Gunjal, Anish et al. \u201cDetecting and Preventing Hallucinations in Large Vision Language Models.\u201d AAAI Conference on Artificial Intelligence (2023)."
            },
            "questions": {
                "value": "Minors:\n1. The construction of dataset is not introduced with details. Details in appendix should be at least provided via cross-reference.\n2. Figure 5 right is not illustrated in the main text. I find it not easy to understand the information it's trying to convey."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}