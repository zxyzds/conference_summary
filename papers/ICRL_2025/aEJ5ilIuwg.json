{
    "id": "aEJ5ilIuwg",
    "title": "Optimization on Manifolds with Riemannian Jacobian Regularization",
    "abstract": "Understanding the effectiveness of intrinsic geometry in enhancing a model's generalization ability, we draw upon prior works that apply geometric principles to optimization and present a novel approach to improve robustness and generalization for constrained optimization problems. This work aims to strengthen the sharpness-aware optimizers and proposes a novel Riemannian optimizer. We first present a theoretical analysis that characterizes the relationship between the general loss and the perturbation of the empirical loss in the context of Riemannian manifolds. Motivated by the result obtained from this analysis, we introduce our algorithm named Riemannian Jacobian Regularization (RJR), which explicitly regularizes the Riemannian gradient norm and the projected Hessian. To demonstrate RJR's ability to enhance generalization, we evaluate and contrast our algorithm on a broad set of problems, such as image classification and contrastive learning across different datasets with various architectures.",
    "keywords": [
        "Riemannian manifolds",
        "Flat minimizer",
        "Sharpness-aware Minimization"
    ],
    "primary_area": "optimization",
    "TLDR": "We propose a novel optimizer to improve generalization on manifolds that strengthens and generalizes prior works such as SGD, SAM, and RSAM.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=aEJ5ilIuwg",
    "pdf_link": "https://openreview.net/pdf?id=aEJ5ilIuwg",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces an optimization approach called Riemannian Jacobian Regularization for improving model generalization and robustness in constrained optimization problems. The authors first provide theoretical analysis showing how the population loss relates to empirical loss through Riemannian gradients and projected Hessians on manifolds. Based on this analysis, they develop the RJR algorithm which explicitly regularizes both the Riemannian gradient norm and the Jacobian while optimizing on manifolds. The authors demonstrate RJR's effectiveness across multiple tasks including supervised learning, labeled self-supervised learning, and unlabeled self-supervised learning, testing on various datasets and model architectures."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper considers both theoretical justifications as well as algorithms motivated from the theoretical development. A number of experiments have been provided to justify to proposed algorithm."
            },
            "weaknesses": {
                "value": "Major comment: \n\n\n\n1. The theorem 2 of the paper of Foret et al 2021b seems to be a Euclidean version of the Theorem 1 of this paper. Instead of having norms of gradients and hessians of the loss function, they have max_(|eps| &lt; rho) L_S(theta+eps), i.e., the maximum empirical loss over a rho-neighborhood of the given parameter theta. One can imagine that if L_S is (locally) smooth with respect to theta then this term can be further bounded by norms of gradients and hessians using Taylor expansion arguments. On the other hand, L_S could have been non-smooth, e.g., it can be locally oscillating heavily but with small magnitudes, leading to Foret\u2019s term still be bounded while the norms of gradients and hessians can explode. Do I miss something? If not, what are the benefits of working with a setting that needs a stronger assumption on local smoothness? \n2. What do you mean by robustness? A priori one could think about robustness against adversarial pertubations in the input, distribution shift, randomness in the optimization dynamics, to name a few. Further, what set of experiments demonstrate the claim of \u201cimproving robustness\u201d? \n\nMinor comments:\n\n\n\n1. The K-lipschitz assumption is in the restated version of Theorem 1 in the appendix but missing in the one in the main body. \n2. Table 1 is somewhat hard to read. Perhaps keeping three significant figures would allow one to make the fonts larger - at any case, an accuracy 90.01 is not so different from 90.02.  Also, what are the numbers in parentheses in the last row? I presume it is the standard variance, but neither the caption nor the text in line 380 describes it."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a Riemannian Jacobian Regularization technique to improve the generalization ability of deep models. The authors argue that by incorporating the Jacobian regularization on a Riemannian manifold, they can better capture the geometric structure of the parameter space, leading to improved performance in terms of generalization error. The theoretical analysis provides bounds on the generalization error, and empirical results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(1) This work tackles an important problem in deep learning, i.e., enhancing generalization, which is crucial for improving the models' robustness in real-world applications.\n\n(2) The theoretical framework builds on well-established concepts in Riemannian geometry, providing a mathematically sound approach to regularization.\n\n(3) The experimental results on different benchmarking datasets in supervised and self-supervised settings show the effectiveness of using the proposed RJR."
            },
            "weaknesses": {
                "value": "(1) Limited Theoretical Novelty: The theoretical contributions of this paper, e.g., Theorem 1, appear to rely on existing works (or experience) in the field of generalization theory, such as:\n[a]. Hoffman, J., & Ma, Y. (2019). \"Robust learning with Jacobian regularization.\" arXiv preprint arXiv:1907.05895.\n[b]. Neyshabur, B., Tomioka, R., & Srebro, N. (2015). \"Norm-based capacity control in neural networks.\" In Proceedings of the Conference on Learning Theory (COLT).\nHence, the lack of truly novel theoretical insights may weaken the originality of the paper. Addressing the limitations of current generalization bounds in the context of manifold-based regularization would be a meaningful addition.\n\n(2) Methodological Contribution Needs Further Justification: Although the idea of Jacobian regularization on Riemannian manifolds is interesting, this paper lacks sufficient justification for why the proposed RJR would outperform or provide benefits compared to other regularization methods. Therefore, additional experiments comparing RJR to other regularization techniques (e.g., adversarial training, dropout) across a variety of tasks and architectures would help clarify its distinct advantages.\n\n(3) Empirical Evaluation is limited: The experiments are conducted on a limited range of datasets and models. Expanding the empirical evaluation to include diverse datasets and model architectures would strengthen the paper. A key aspect is that this paper is about optimization on the Riemannian manifolds, but the authors do not applied the suggested RJR to Riemannian networks, such as SPDNet [c], SPDNetBN [d], GrNet [e], RResNet [f]. Additionally, it would be helpful to include ablation studies to show the sensitivity of the method to key hyperparameters, such as the choice of Riemannian manifolds, $\\epsilon$, $\\rho$.\n[c] A riemannian network for spd matrix learning, AAAI, 2017.\n[d] Riemannian batch normalization for SPD neural networks, NeurIPS, 2019\n[e]  Building deep networks on grassmann manifolds, AAAI, 2018\n[f] Riemannian residual neural networks, NeurIPS, 2023.\n\n(4) Insufficient explanation: The role of the second term in $sigma_t$ (line 248) is unknown. In Section 5.2, the authors mentioned that enforcing orthogonality on the convolutional filters has some benefits, such as alleviating gradient vanishing. However, the basic reason is unknown, making the applicability of the proposed RJR unconvincing. Again, the authors are suggested to evaluate the effectiveness of RJR under end-to-end Riemannian networks when compared with RSGD and RSAM.\n\n(5) Another limitation of this paper is that the English writing is obscure, especially in the proposed method section. In other word, it is  challenging for readers who are not already familiar with Riemannian geometry and Riemannian optimization.\n\n(6) In line 356, $s$ means what?. In Fig. 2, why select $\\lambda_5$. Figs. 2 and 3 are not vector graphics."
            },
            "questions": {
                "value": "Please refer to the weaknesses part for detailed information."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Riemannian Jacobian Regularization (RJR), a method that combines sharpness-aware and Jacobian regularization on Riemannian manifolds to improve generalization and robustness. The approach is tested on various supervised and self-supervised tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "RJR presents a straightforward optimization technique that improves generalization by explicitly controlling sharpness on Riemannian manifolds."
            },
            "weaknesses": {
                "value": "1. In Thm.1, why there is an ambient inner product, instead of the Riemannian ones? Will this cause the loss of some information?\n2. Experiments are not convincing\n   - Most datasets are small\n   - The reason for not comparing it with RSAM under the same datasets as the original paper is unclear.\n   - A common counterpart is the trivialization [1]. Trivializations could be faster and sometimes even better than the Riemannian ones. The missing comparison makes the empirical validation not convincing.\n   - For the orthogonal constraints, why do not compare with some orthogonal tricks [2] is not clear\n   - There are Riemannian networks, where data and parameters naturally lie in the manifold, such as SPD [3-4], Grassmannian [5-6], Lie groups [7], and hyperbolic [8]. How about the effects on these networks? Some works prefer to use trivialization, such as [6] and its previous work.\n3. Some description lacks clarity:\n   - some abbreviations come without their full name, such as SWA.\n   - Typos: $\\theta$ and $\\mathcal{T}_\\theta$\u200b in lines 120\n   - L152: $\\langle, \\rangle$\u200b is the Euclidean one in the ambient space.\n   - The readability of some proofs are poor. use \\stackrel for each (key) derivation is a good habit (such as L974-992).\n\n\n[1] Trivializations for Gradient-Based Optimization on Manifolds\n\n[2] Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group\n\n[3] SPD domain-specific batch normalization to crack interpretable unsupervised domain adaptation in EEG\n\n[4] ManifoldNet: A Deep Neural Network for Manifold-Valued Data With Applications\n\n[5] Building Deep Networks on Grassmann Manifolds\n\n[6] Matrix Manifold Neural Networks++\n\n[7] A Lie Group Approach to Riemannian Batch Normalization\n\n[8] Hyperbolic Neural Networks"
            },
            "questions": {
                "value": "1. L 982-985: how to transform the Riemannian metric into the ambient inner product?\n2. could we do  (Jacobian) regularization by trivialization? if so what is the advantage of Riemannian regularization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this work, a theoretical analysis that characterizes the relationship between the general loss and the perturbation of the empirical loss in the context of Riemannian manifolds is presented. Motivated by the result obtained from this analysis, the authors introduce an algorithm named Riemannian Jacobian Regularization (RJR), which explicitly regularizes the Riemannian gradient norm and the projected Hessian. Some experiments have been conducted to verify the performance of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tA theoretical analysis that expresses the relationship between the general loss and the empirical loss via the Riemannian gradient and the projected Hessian is provided. \n2.\tA Riemannian Jacobian Regularization (RJR) method is introduced to strengthen the Jacobian regularization techniques to Riemannian manifolds."
            },
            "weaknesses": {
                "value": "1.\tThe contributions of the work is not enough with only a theoretic analysis and a proposed method. I suggest the authors give more description about the proposed method, such as the role of each component played in the method.\n2.\tOn page three, line 113, the definition of the Euclidean norm of a vector is not standardized.\n3.\tWhy are some formulas not numbered, while others are?\n4.\tOnly four compared methods are used, just from two references in 2013 and 2023. How to effectively validate the SOTA performance of this method? The following methods can be compared \u201cRiemannian Manifold Learning, TPAMI, 2008\u201d, \u201cGeneralized Learning Riemannian Space Quantization: A Case Study on Riemannian Manifold of SPD Matrices, TNNLS, 2021\u201d, \u201cKernel Methods on Riemannian Manifolds with Gaussian RBF Kernels, TPAMI, 2015\u201d.\n5.\tWhat is the Lemma 2 in the Appendix (A.2)? No related contents are described in the main contents of the paper. The given Lemma 2 in A. 2 is just already existed in Ref. Lee et al. (2023), as the authors described.\n6.\tHow to obtain the first inequality on page 19, line 974? Similar as in the third inequality?"
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}