{
    "id": "TUC0ZT2zIQ",
    "title": "True Counterfactual Generation from Language Models",
    "abstract": "Understanding and manipulating the causal mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery---e.g., model ablations or manipulation of linear subspaces tied to specific concepts---to \\emph{intervene} on these models. To understand the impact of interventions precisely, it is useful to examine \\emph{counterfactual} strings---e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equations Models (GSEMs) using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Experiments demonstrate that our approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects.",
    "keywords": [
        "Causality",
        "language models",
        "counterfactuals"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose a framework to generate true counterfactuals from language models by reformulating them as Generalized Structural Causal Models using the Gumbel-max trick.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TUC0ZT2zIQ",
    "pdf_link": "https://openreview.net/pdf?id=TUC0ZT2zIQ",
    "comments": [
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "Thank you for your constructive feedback and for recognizing the importance of our work in advancing the true counterfactual generation from LMs. We appreciate the strengths highlighted, especially regarding our theoretical contributions and the insightful experimental findings. We would like to address the concerns and provide clarifications to improve the understanding of our work.\n\n***Comparison with existing methods***\n\nThe reviewer suggests including a comparison with existing methods for counterfactual generation. We would like to clarify that, to the best of our knowledge, there is no existing method that proposes a framework for generating causally correct counterfactuals in the context of language models. Most prior approaches, such as linear steering or knowledge editing, intervene on model behavior without addressing the core challenge of identifying causal counterfactuals  by conditioning on the same underlying noise that produced the original sentence. In the existing literature people vaguely refer to these as \u201ccounterfactuals\u201d, while, as we point out, they actually correspond to interventions. We use several such interventions in our experimental evaluation and generate true counterfactual strings from them.\nThe main contribution of our work is the introduction of a method that generates true counterfactuals by explicitly modeling and controlling for the sampling noise, an ability that doesn\u2019t currently exist. This is why we focused on evaluating our proposed method.\n\n***Question 1: Applicability of the method to beam search decoding***\n\nThe reviewer asks whether our proposed method can work with beam search decoding, which is commonly used in LLMs.\nPlease note that beam search is a *deterministic* decoding strategy that attempts to locate the sequence with the highest cumulative probability (i.e., the approximate argmax of the model's output distribution). It is not a sampling method. Unlike stochastic sampling methods, beam search does not introduce any exogenous noise into the generation process. Our method specifically targets scenarios involving stochastic sampling, where the generation process involves a random component (e.g., multinomial sampling) driven by exogenous noise variables.\nOur method can be easily extended to other stochastic sampling techniques, such as nucleus sampling or top-k sampling, as we mention in a footnote in the paper. These methods can be incorporated by applying a deterministic function to the logits (e.g., truncating the probability distribution) before the sampling step. The crucial point is that, as long as there is some form of stochastic sampling involved, we can model the exogenous noise and apply our counterfactual generation algorithm. In the case of beam search, there is no exogenous noise to model, and thus, our algorithm for calculating counterfactuals is not applicable. We will clarify this distinction in the revised version of the paper.\n\n***Question 2: Effect of temperature parameter in the gumbel-max***\n\nThe reviewer asks how the temperature parameter in the Gumbel-max trick affects the quality of the generated counterfactual sentences.\nThe temperature parameter is a deterministic part of the forward pass that scales the logits before sampling. This scaling influences the model's output distribution, affecting the text quality and diversity by controlling the randomness of the token selection. However, this scaling is part of the deterministic transformation applied to the logits, and as such, it does not affect the exogenous noise component that our method aims to model and disentangle.\nOur counterfactual generation method focuses on controlling the exogenous stochastic noise in the sampling process. Since the temperature only modifies the logits deterministically, our method is invariant to the choice of temperature. While different temperatures can influence the overall quality and diversity of the generated text, this is orthogonal to our main objective of deriving a counterfactual given a deterministic model. The exogenous noise remains the same regardless of the temperature setting."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "We appreciate the thorough evaluation of our submission. We would like to address the concerns and suggestions you raised.\n\nThe reviewer questions the completeness of the noise model, specifically the assumption that the Gumbel noise is independent at each timestep. There is also concern about whether any dependencies in the noise are appropriately captured by the endogenous variables.\nThe independence assumption for the Gumbel noise at each timestep is a key property leveraged by the Gumbel-max trick. The independence does not imply that the noise is uncorrelated across the entire sequence but rather that, conditioned on the observed data, the noise realizations at each timestep are independent given the structural dependencies of the model (i.e., the autoregressive nature of the language model). This is just an expression of the fact that the randomness in sampling is independent between time steps (when we sample tokens from the model, the randomness in choosing token i is independent of the randomness in choosing token j). \nThe dependencies in the sampling process are captured by the deterministic part of the model (the logits computed from the language encoder), which act as endogenous variables in our GSEM formulation. The exogenous noise variables (the Gumbel noise) influence the sampling only through their interaction with these logits. We do not observe any limitation in modeling the noise using a gumbel. It is a common modeling choice. Furthermore, unlike other sampling procedures, like inverse CDF sampling (\u201croulette wheel\u201d), it is invariant to the arbitrary indexing of the elements in the categorical distribution. \n\nWe also ask to emphasize that in our experimental evaluation, we use Algorithm 1 to *completely* control for all randomness: we make sure that the *only* thing that changes is the intervention in the model\u2019s parameters. As such, the experiments allow us to isolate, specifically, the influence of the intervention on the output of the model (the tokens sequence). \nThank you for pointing out the need for a limitations section. We will add it to the final version of this paper."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "We thank the reviewer for their thoughtful and detailed feedback. We appreciate the recognition of our novel framework and the systematic evaluation of intervention techniques. We also value the suggestions provided to improve the clarity and empirical validation of our method. We address the concerns and questions raised as follows:\n\n1. **Justification of Proposition 3.1 and Hindsight Gumbel Sampling**\n\nThe reviewer questions the theoretical foundation of our Hindsight Gumbel Sampling algorithm, specifically challenging the independence of the sampled Gumbel noise and suggesting that the marginal distribution of  $U^t$ given an observed sentence might not remain \\(\\text{Gumbel}(0, 1)\\). We want to explain the key justification provided in Proposition 3.1.\n\nThe central insight of Proposition 3.1 is that the Gumbel-max trick allows us to *exactly* recover the posterior distribution \\$P(U \\mid \\text{observed sentence})$ by leveraging the independence property of the Gumbel noise. The Gumbel-max trick ensures that the noise $U^t(w)$ for the sampled token $w$ (i.e., the observed token) can in fact be modeled as $\\text{Gumbel}(0, 1)$. This follows from the key property of the Gumbel distribution, the independence between the max value and the rest, which allows us to sample from the posterior by first sampling a *standard* gumbel and then sampling the rest of the noise from the truncated distribution.\n\nBy conditioning on the observed outcome (i.e., the chosen token), we ensure that $U^t(w_t) + \\pi(w_t) \\geq U^t(w) + \\pi(w)$ for all  $w \\neq w_t $. This condition precisely defines a truncated Gumbel distribution for the remaining noise variables. Thus, our method correctly samples from the posterior distribution of $U$. We emphasize that this approach is grounded in well-established properties of the Gumbel-max trick, and the independence of the Gumbel noise is key to its validity. We refer to the proof for this property in the appendix. \n\n2.  ***Clarifying the primary contribution and the counterfactual encoder***\n\nWe appreciate the feedback regarding the presentation of our primary contribution. While the introduction outlines our reframing of language models through causal inference and the novel counterfactual generation method, we will revise the early sections to provide a clearer and more explicit emphasis on the counterfactual generation as a core contribution.\nWe agree that the description of obtaining the counterfactual encoder could be more explicit. In our experiments, we use existing intervention methods to define the counterfactual encoder (e.g., MEMIT, linear steering). We will make this clearer in the revised draft and emphasize that our contribution lies in the generation of true counterfactual strings and the analysis framework, not in proposing new intervention methods.\n\n3. ***Addressing concerns on sampling variability***\n\nThe reviewer pointed out that in our evaluation, we sample a single counterfactual, whereas we actually have a distribution over counterfactuals. This observation is valid. However, since our results are negative (i.e., we demonstrate that the counterfactuals tend to deviate significantly from the originals), using a single Monte Carlo sample, averaged over hundreds of different texts, is sufficient to reveal the differences between the originals and the counterfactuals. It is unlikely that we would consistently observe such a difference with just one sample but not with a larger number of samples (N >> 1). Nonetheless, to address this concern empirically, we conducted an additional experiment where we generated 10 counterfactuals per original sentence for the mimic-gender interventions. The results show a high mean cosine similarity of 0.88 between each group of 10 counterfactuals under the E5 model, indicating a strong degree of semantic similarity. Furthermore, we repeat the analysis of the normalized length of the longest prefix between the original and the counterfactuals. The median length changes from 0.281 (for N=1) to 0.265 (for N=10). We will repeat all experiments with N=10 for the final version of this work. \n\nFinally, we appreciate the suggestion to explore counterfactuals of counterfactuals as an additional robustness check. We agree that this could provide deeper insights into the reliability and utility of our method. We will include such experiments in the final version of this work."
            }
        },
        {
            "title": {
                "value": "quick initial comment"
            },
            "comment": {
                "value": "Thanks for the in-depth review. We wanted to answer the three questions before we respond with more because we think we can address all your concerns in detail, but we want to be sure we understood it all. \n\n1. Our method is invariant to the exact method used to sample from the softmax in a specific sense. Non-identifiability when sampling from Categorical only arises from permutation non-invariance. See, for example, Oberst and Sontag (2019) Secton 3.1 [https://proceedings.mlr.press/v97/oberst19a/oberst19a.pdf].  We can give formal proof of this, which appears to be missing from the above reference. Thus, we chose the Gumbel-max trick for convenience as it is a simple, permutation-invariant sampling scheme. Would our spelling out this equivalence assuage some of your concerns? The reason we rule out permutation-dependent schemes, e.g., what we would call roulette sampling, is that the permutation is arbitrary. \n\nTo be explicit, the result we will add, which we think will dramatically improve the paper is this. If one wishes to assume the sampling process from the Categorical is permutation-invariant, i.e., the order we assign elements of the categorical an integer does not matter, then the model is identifiable.  \n\n2. This is a choice to reflect standard LM notation. We can change it it was confusing.\n\n3. All of our experiments use hindsight sampling. Roughly speaking, the experimental design works like this:\na) Input a string\nb) Apply an intervention on the network\nc) Sample a counterfactual string using Algorithm 1.\n\nSo, our method is the first that can show how individual strings change under interventions, e.g., MEMIT. \n\nWe specifically appreciated many of your comments about GSEMs. One quick note -- our GSEM is acyclic, so they have one solution. (An unrolled neural network-based language model is acyclic.) We only apply them because we have an infinite number of variables and SEMs do not apply. Apologies that we did not make this point clear. Also, we are not tied to GSEMs, as the reviewer infers, would it help the presentation to switch to causal dynamic Bayesian networks? We view our contribution as the first algorithm to give *string*-level counterfactuals under the model. We can elaborate more on why we call these \"true\" in the title."
            }
        },
        {
            "summary": {
                "value": "The paper first reframes the decoder-only language models as Generalized Structural Equation Models (GSEMs) with the Gumbel-max trick. Building on this framework, the authors propose an algorithm leveraging hindsight Gumbel sampling to infer and fix latent exogenous variables, enabling the generation of counterfactual versions of observed strings. Experimental results demonstrate that this method produces meaningful counterfactual pairs and further uncovers significant limitations of existing intervention techniques."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper introduces a novel framework that reframes language models through the lens of causal inference, which offers a new perspective for analyzing the causal mechanisms within language models. Building on this new framework, the paper also proposes a novel approach to generate counterfactual strings for observed strings. Interestingly, the proposed method provides a systematic means to evaluate existing intervention techniques, revealing previously unrecognized side effects."
            },
            "weaknesses": {
                "value": "1. The presentation of the paper could benefit from clearer emphasis on its primary contribution: generating counterfactual pairs. It would be helpful if the authors clarified early on that the intervention models employed are drawn from existing methods. Before reaching Section 4, there may be some ambiguity regarding how to obtain the counterfactual encoder $\\tilde{h}$, which seems the hardest part for counterfactual generations.\n2. The way to find $U_t$ should be refined: \n     - Given the counterfactual string sampling method in Corollary 3.1, infinitely many vectors $U_t$ could theoretically be sampled, potentially resulting in different counterfactual generations. The paper currently presents only one counterfactual string per original string, and it would be beneficial to provide additional evidence demonstrating consistency across different samples of $U_t$.\n     - While the Hindsight Gumbel Sampling can generate $U_t$ to ensure the next token is $\\bar{w_t}$, there is limited theoretical or empirical discussion supporting this choice. Specifically, since $U_t(\\bar{w_t})$ satisfies $\\pi(\\bar{w_t})+U_t(\\bar{w_t})=\\max_{\\bar{w}\\in\\bar{\\Sigma}}\\pi(\\bar{w})+U_t(\\bar{w})$, the marginal distribution of $U_t(\\bar{w_t})$ shouldn't be Gumbel(0,1) as mentioned in Proposition 3.1.\n3. Additional experiments would enhance the persuasiveness of the results. For instance, demonstrating counterfactual generations across multiple Hindsight Gumbel Sampling trials would reinforce the approach\u2019s robustness. Further, examining counterfactuals of counterfactual strings and comparing them to the original strings could offer valuable insights into the reliability and utility of the proposed method."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes modeling a language model as a causal model with infinitely many variables (they use a GSEM, although I am skeptical that this choice is appropriate) with Gumbel noise.  The idea is to improve counter-factual generation: fixing a generated sentence, find the sentence that would have generated with these noise settings, if in fact the model had been different.  The authors review the Gumbel max trick for parametrizing a sample from a softmax, argue that any LM can be captured by a GSEM that uses this trick, and then provide an algorithm for counter-factual sampling (Alg 1) based on these assumptions.  The paper then evaluates prior methods for model-editing, arguing that they are insufficient."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is beautiful, and very well-written.  The layout and math is clean, and the English is sharp.  The concepts are interesting, and the problem of generating counter-factual is well-motivated.  Many (but not all) unnecessary details are swept away.  It appears that the authors implemented their algorithms, and attempted several different evaluations with real-world data and models used in practice.   I think the presentation is so good as to be deceptive.  The vision is grand and compelling."
            },
            "weaknesses": {
                "value": "Once we adequately pay our respects to the presentation and get to the substance of the paper, things start to fall apart quickly, on many fronts.  At a high level, the story is strong, but the math and evaluation simply does not back it up.  Below the surface, the conceptual, theoretical and empirical aspects of this work are all severely lacking. \n\n**Concepts.** \n- At a high level, I feel the authors do not really \"get\" what GSEMs are and how they fit into the causal literature.  The fact that they allow infinitely many variables is actually not relevant.  The authors are also not using the definition of a GSEM properly, and completely sweep under the rug the possibility of a GSEM having multiple solutions.  They do not discuss the constraints that are part of a definition of a GSEM (even in the appendix), that an intervention must have the effect of fixing a variable in a certain way.  In short: there are easier and more appropriate ways of using tools from causality here.  (Causal) dynamic Bayesian Networks are one.  To me, it seems that the authors were overly fixed on the \"infinitely many variables\" aspect of a GSEM, and then copied the the definition of Peters and Halpern (2022) without reading the rest of the paper.   \n\n- The core contribution seems to be the understanding of the LLM noise variables through the Gumbel model, culminating in Corrolary 3.1 and Algorithm 1.  Yet the authors seem to miss the fact that the Gumbel distribution is just one of many possible causal models that one could use to get the LM's distribution. They do not acknowledge this, or consider how counter-facutals would work in other contexts.  They seem to be under the impression that their Gumbel model is the \"right way to model this\", as evidenced by the title (\"True Counterfactual Generation in Language Models\".)   I see no reason to believe that this is the case, and feel there is some slight-of-hand going on here.\n\n- Finally, the evaluation does not even involve their own method! The conclusion states that these methods\n   *\"underline the need for more refined methods that can achieve targeted modifications with minimal collateral changes to the\nmodel\u2019s outputs.\"*\n  But they do not evaluate their own solution to this problem! This seems hugely problematic. \n \n\n\n**Theoretical Results.** Proposition 2.1 is obvious, easy to prove, and a very weak result (as I describe below).  Proposition 3.1 is not precisely stated, and it is not proved in the appendix; instead, the authors reference another work that does prove this.  I believe it is important to either adapt the result carefully to your own modified statement of the result, or to use essentially the same wording and credit the original authors.  Corollary 3.1 has some intuitive pull, but no further properties of the distribution are investigated.  There are interesting questions here.  Is the Gumbel model the unique one with some property?  How do these counterfactuals differ from those in other SCMs? But those questions are not answered here.\n\nIn general, the wording around the probabilistic modeling does not inspire confidence.  The paper is missing lots of independence assumptions, uses undefined terms and symbols, and directs focus in unnecessary places.  I am particularly unhappy about the statement of Prop 3.1 and the false statements that a causal model gives rise to a unique distribution.  (See my comments below for more detail and further examples.) \n\n**Empirical Evaluation.**  The weakness of this section is unavoidable given that the authors did not evaluate their own method. There are some interesting ideas here, and the delve into the weaknesses of other methods is certainly valuable material.  But it is out of place, and being dramatically over-sold.  I also have some thoughts about why the experiments could be done better, but none of them really matter without seeing an evaluation of the proposed artifact. \n\n--------\n\nDetailed Line-by-Line Comments and Concerns:\n\n- 115-116**: Given the definition above, it is not true that a probability over \\U induces a probability over \\U \\cup \\V.  In general, there may be many such distributions or none at all.  For this to be true, you need to significantly restrict (not generalize) your class of PSEMs; it seems acyclic ones will do.  But later on, you generalize to SEMs with infinitely many variables, where again this is not true.  \n(Finally, the notion $\\mathbb P^{\\mathcal S}$ is problematic; $\\mathcal S$ is just the signature, and this distribution (if one exists) depends on not only \\S, but also on \\F, and the choice of fixed point.\n\n- 131-133: Definition 2.2 is too vague to be numbered and italicized like this.  First, as mentioned above, the intervention may lead to multiple distributions. Second, the posterior distribution must be calculated with respect to the original (choice of) extended distribution (\\P^{\\S} in your notation, which I don't like). Finally, over what variables is this interventional distribution? Is it \\U \\cup \\V?  Finally, some justification is required for this choice. Note that, the counterfactual intervention without intervening at all (i.e., taking I to be empty) need not be v.  Is this acceptable? If so, why? \n\n- 173: I recommend using a symbol other than \\pi (perhaps \\ell), for the logits, because \\pi has a probabilistic connotation.  In other description of the Gumbel-max trick, \\pi is defined as the exponential of this quantity. Also, it would be easier to parse if \\pi were bound before the equation.\n\n- 175: The M outcomes must be independent. \n\n- 184: This is the second \"Let \\Sigma be\". It would be better to say \"Recall that \\Sigma is\", because \\Sigma and other symbols were defined above.  If one skips here and starts reading, they will not get the cue that these other symbols were defined earlier. \n\n- 187: Missing independence assumption (I can only assume), as this is required for everything to be defined. Specifically, { U_t(w) : w \\in \\bar\\Sigma } must be mutually independent.\n\n - 189: Text inaccurately suggests that the language encoder is just h, but it is also E.  Given that \n\n - 191: The quantity {\\bf b} comes out of the blue.  Based on context I assume it is some kind of bias, but it does not fit into the math at all. Note that it plays the same mathematical role as U_t, except it is undefined. Also, assuming that it is a parameter like E, it is mathematically unnecessary for the usual reasons.  Equation (1b) does not have a bias, and I think this one should not have a bias either.\n\n - 200: Proposition 2.1 is extremely weak.  GSEMs are very expressive, and it is obvious that this autoregressive process can be captured by a GSEM (and it can also be captured by other generalizations of causal models that are not nearly as expressive). Moreover, the statement of the theorem does not even use the primary data of the GSEM: it's response to interventions.  In fact, the proposition can be \"significantly strengthened\" to an equally vacuous statement, by replacing the first line with \"let p be an arbitrary probability distribution over \\Sigma^*.\"\n\n\n - 223: What is \"\\Pi\"? It has not been defined. \n - 228: caption (b) I think it should be \"a LM\" not \"an LM\"\n\n - 240: If I understood correctly, you mean not that it can be sampled this way, but rather that \\tilde W equals this quantity *as a function of the previous noise U*.  The wording needs to be tightened up a bit here.\n\n - **275: The term \"truncated probability distribution\" is not a formal concept in probability. You're really referring just to given conditional distribution; there's no reason to bring \"truncation\" into it, and the distribution is not a special kind of object; it's a probability distribution like any other.  Also, the given reference (footnote 8) is a blog post, but the concept is very old and has a name: rejection sampling. \n\n - 293: This is not an algorithm, so much as a snippet of python code. \n - 357. Why are you using just the lengths of prefixes for the evaluation? Why not compare, for example, edit distance, word movers' distance?"
            },
            "questions": {
                "value": "1.  A fundamental question: why use Gumbel noise? I understand that the Gumbel max trick is one way of sampling from a Boltzman distribution, but there are other (arguably simpler) ways as well.  I vaguely see that Corollary 3.1 might be useful, but are there analogues for tother causal models? Would other approaches to modeling the noise yield similar counterfactual distributions? In brief: what is special about Gumbel noise, and why does it feature so prominently in this paper?\n\n2.  A relatively unimportant question about notation: Given that  E and h only ever appear together (in the context \"E h(w_{<t})\" ), why bother separating them? Why not just define a new function of w_{<t} that is equal to this product?\n\n3. What is the relationship between the experimental evaluations (MEMIT, Inference-time Intervention, Instruction Tuning) and the Gumbel causal model?  It looked as though Corollary 3.1 was going to be the basis for a new method, but instead it seems the evaluation just looks at several methods already present in the literature.  Do those have Gumbel noise?  Does Algorithm 1 make any appearance in the evaluation?  It appears not to.  Am I missing something?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses a fundamental challenge in analyzing autoregressive language models with causal interventions: generating true counterfactual sequences that isolate the effects of model modifications from inherent sampling randomness. The authors present a novel framework that reformulates LMs, within Pearl's causal hierarchy, by decomposing them as Generalized Structural Equation Models (GSEMs). This reformulation enables a more rigorous counterfactual analysis by explicitly modeling and controlling the stochastic elements of language generation, while also allowing for the use of an expansive set of Causal Interventions.\n\nThe paper's primary theoretical contribution is the decomposition of LMs into deterministic and stochastic components through the Gumbel-max trick. Here, the argmax with Gumbel noise is chosen due to its equivalence to sampling from a softmax distribution. By identifying sampling noise as exogenous variables within the GSEM framework, the authors develop a principled method for inferring and preserving these noise values during counterfactual generation. This separation enables the analysis of intervention effects while controlling for the underlying randomness that is inherent in LM sampling.\n\nKey Technical Contributions:\n\n1. Establishing the equivalence between LMs and GSEMs, allowing for the analysis of Causal interventions using \"retrospection\" (Pearl's 3rd level of counterfactual reasoning)\n2. Development of a hindsight Gumbel sampling algorithm that enables inference of latent noise variables from observed sequences\n3. Generation of counterfactual strings that maintain sampling consistency across model interventions, allowing for pairs of sentences to be sampled from the joint distribution of the original and counterfactual strings.\n\nThe authors validate their framework through comprehensive experiments on LMs (GPT2-XL and LLaMA3-8b), utilizing and examining various intervention techniques such as knowledge editing (MEMIT), linear steering, and instruction tuning. Their analysis reveals that even minimal interventions can produce unexpected semantic changes in model behavior, highlighting the importance of controlled counterfactual analysis in understanding model interventions.\n\nThe work helps advance the field of Causal reasoning by allowing true counterfactual reasoning in LMs, providing tools to better understand and control the effects of model modifications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper makes key contributions that help advance the field of causal interpretability in language models:\n\n$\\textbf{Novel Framework:}$\n- Reformulates autoregressive LMs as Generalized Structural Equation Models (GSEMs)\n- Decomposes language generation into Deterministic computation (logits from model) and Stochastic elements (sampling noise as exogenous variables)\n- Leverages Gumbel-max trick to establish equivalence with softmax sampling\n\n$\\textbf{Theoretical Foundations:}$\n- Proposition 2.1: Proves equivalence between LMs and GSEMs through rigorous mathematical formulation\nShows $P(W = w_1...w_T) = P_E(W_1 = w_1, ..., W_T = w_T, W_{T+1} = EOS, ...)$\n\n- Proposition 3.1: Derives hindsight Gumbel sampling for noise inference\nPresents clear algorithm for counterfactual generation while preserving sampling noise\n\n$\\textbf{Integration with Causal Framework:}$\n\nEnables true counterfactual reasoning through\n- Noise inference from observed sequences\n- Noise preservation during counterfactual generation allowing for \"retrospection\"\n\nMaintains compatibility with existing and future intervention techniques through GSEM formulation\n\n$\\textbf{Empirical Validation:}$\n\nArchitectures used:\nGPT2-XL, LLaMA3-8b\n\nInterventions:\nMEMIT (knowledge editing), Linear steering, Instruction tuning\n\n$\\textbf{Impact and Implications:}$\n\n- Provides a foundation for analyzing intervention effects through counterfactual reasoning\n- Offers to control generation stability\n- Reveals unintended side effects in current intervention methods\n\nThe paper's primary strength lies in providing a theoretically sound solution to a fundamental challenge in causal interpretability: isolating intervention effects from inherent sampling randomness in language models."
            },
            "weaknesses": {
                "value": "$\\textbf{Empirical Validation of Causal Framework:}$\nWhile Proposition 2.1 provides a theoretical foundation for the LM-GSEM equivalence, several key empirical validations are missing even with section B in the appendix:\n\n$\\textbf{Noise Completeness}:$\nThe paper proves that $W_t = \\text{argmax} {w\\in\\Sigma}(Eh(w_{<t}) + b)_w + U_t(w)$ captures sampling behavior, but doesn't empirically validate this captures all stochastic elements. \n\nGiven that: $P(W = w_1...w_T) = P_E(W_1 = w_1, ..., W_T = w_T, W_{T+1} = EOS, ...)$\n\nI would want to see further evidence that:\n\n- Compounding effects at longer sequences are purely from intervention\n- Quantification of any residual unaccounted noise\n\n$\\textbf{Counterfactual Quality}:$\nMissing controlled experiments demonstrating the difference between the generation with and without preservation. The separation of Noise allows for a more stabilized sequence generation, and I would like to see if the reduced divergence can be quantified.\n\n$\\textbf{Secondary Concerns:}$\n- Prefix length ($l_{\\text{prefix}}$) is overly sensitive to early divergences\n- E5 similarity ($\\text{sim}_{\\text{E5}}(x,y)$) may miss subtle linguistic changes\n\n$\\textbf{Ethical Considerations}:$\n- No discussion of potential misuse in generating misleading content"
            },
            "questions": {
                "value": "- The formulation assumes the gumbel noise to be independent at each timestep. Will the noise not have any dependence or is this dependence captured appropriately by the Endogenous variables?\n- Any limitations with using a Gumbel distribution to model sampling from softmax?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The current paper stuides the true counterfactual generation of large language models: how a given sentence would have appeared had it been generated by the model following a specific intervention. The true counterfactual generation is an important task for understanding the behavior of large language models, and it is also a challenging task for LLMs, as it requires the counterfactual sentences to be generated in a way that is consistent with the original sentence, which is hard to achieve for LLMs due to their statelness and non-deterministic nature. \nIn particular, this paper focus on the interventions of representation surgery, a technique that modifies the internal representations of an LLM to change its behavior. The authors argue that the existing methods for true counterfactual generation are not strictly conforming to the definition by Pearl, and propose to use generalized structural-equations models (GSEMs) to model the counterfactual generation process: first, the LLM is framed as a GSEM where the uncertainty in the generated sentences is modeled with latent independent Gumbel distributions; then, given a sentence and an intervention, the counterfactual sentence is generated from the LLM after intervention using Gumbel-max trick where the Gumbel noise is sampled from the latent Gumbel distributions inferred from the original sentence. The authors theoretically prove that the generated counterfactual sentences comply with the definition of true counterfactuals by Pearl.\nIn the experiments, the proposed method is used to analyze two aspects of the example interventions (MEMIT, inference-time intervention and Instruction Tuning): 1) the side effects of the interventions, which are the changes in the generated sentences that are unrelated to the intervention; 2) the effectiveness of the interventions, which are the changes in the generated sentences that are related to the intervention. The results show several interesting findings, e.g., the side effects of the interventions are inevitable with MEMIT being the most effective in reducing the side effects."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-motivated and clearly written. The authors provide a good introduction to the problem of true counterfactual generation and the overlooked challenges in the existing methods. The theoretical analysis is sound and well-presented. \n- The addressed problem is important and timely, as the true counterfactual generation is a crucial task for understanding the behavior of LLMs and the potential biases in their outputs. The proposed method is a step forward in this direction.\n- The experimental results are insightful and the findings are interesting and can be useful for future research on true counterfactual generation."
            },
            "weaknesses": {
                "value": "- There is no detailed discussion in the main body on the limitations of the existing methods for true counterfactual generation. It would be helpful to have a more detailed comparison with the existing methods and a discussion on the limitations of the proposed method.\n- The experiments are conducted soly with the proposed method without comparison with the existing methods. It would make the readers hard to evaluate the effectiveness of the proposed method in practice."
            },
            "questions": {
                "value": "- Can the proposed method work with beam search decoding, which is commonly used in LLMs for generating sentences?\n- How does the temperature parameter in the Gumbel-max trick affect the quality of the generated counterfactual sentences?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}