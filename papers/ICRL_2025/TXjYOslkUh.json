{
    "id": "TXjYOslkUh",
    "title": "Distribution Shift Aware Neural Feature Transformation",
    "abstract": "Feature transformation, as a core task of Data-centric AI (DCAI), aims to improve the original feature set to enhance AI capabilities. In dynamic real-world environments, where there exists a distribution shift, feature knowledge may not be transferable between data. This matter prompts a distribution shift feature transformation (DSFT) problem. Prior research works for feature transformation either depend on domain expertise, rely on a linear assumption, prove inefficient for large feature spaces, or demonstrate vulnerability to imperfect data. Furthermore, existing techniques for addressing the distribution shift cannot be directly applied to discrete search problems. DSFT presents two primary challenges: 1) How can we reformulate and solve feature transformation as a learning problem? and 2) What mechanisms can integrate shift awareness into such a learning paradigm? To tackle these challenges, we leverage a unique Shift-aware Representation-Generation Perspective. To formulate a learning scheme, we construct a representation-generation framework: 1) representation step: encoding transformed feature sets into embedding vectors; 2) generation step: pinpointing the best embedding and decoding as a transformed feature set. To mitigate the issue of distribution shift, we propose three mechanisms: 1) shift-resistant representation, where embedding dimension decorrelation and sample reweighing are integrated to extract the true representation that contains invariant information under distribution shift; 2) flatness-aware generation, where several suboptimal embeddings along the optimization trajectory are averaged to obtain a robust optimal embedding, proving effective for diverse distribution; and 3) shift-aligned pre and post-processing, where normalizing and denormalizing align and recover distribution gaps between training and testing data. Ultimately, extensive experiments are conducted to indicate the effectiveness, robustness, and trackability of our proposed framework.",
    "keywords": [
        "Feature Transformation",
        "Out-of-Distribution (OOD)",
        "Data-centric AI"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TXjYOslkUh",
    "pdf_link": "https://openreview.net/pdf?id=TXjYOslkUh",
    "comments": [
        {
            "comment": {
                "value": "We appreciate your acknowledgment that the paper is well-presented,  and the topic is crucial.\n\n\nRegarding your questions:\n\n### W1:\n\nOur main contribution is addressing the distribution shift problem within the generative feature transformation framework. We re-summarize our contributions as follows:\n1. We improved the embedded-optimization-reconstruction feature transformation framework to make it dataset distribution-aware rather than limited to token-level expressions.\n2. We introduced a shift-resistant feature representation, flatness-aware generation, and incorporated normalization techniques to address distribution shift.\n3. We conduct extensive experiments to demonstrate the efficiency, resilience, and traceability of our framework.\n\n\n### W2:\n\n1. Our reported baseline results appear lower due to different dataset splitting methods. [1] and other baseline papers use random sampling, ensuring the train and test sets follow the same distribution. In our experiments, we split the data to create a distinct distribution shift detected by Kolmogorov-Smirnov test between train and test sets.\n2. We did not report results on other datasets because these datasets are stable and it's hard to observe a significant distribution shift when splitting the data. We will clarify this in the dataset section.\n\n### W3:\n\nWe propose addressing the OOD problem in feature transformation within a generative framework by introducing shift-resistant feature set representation, flatness-aware generation, and normalization integration. Ablation experiments in Section 5.2 demonstrate the effectiveness of these three components.\n\nWe hope that our response can address your concerns."
            }
        },
        {
            "comment": {
                "value": "We appreciate your acknowledgment that our research problem is new, our methodology is effective, and our experiments show a superior performance than baselines.\n\nRegarding your specific concerns:\n\n### Organization and writing:\n**Q1:** \nWe intended to highlight the insights of our approach at a high level, so we placed the more detailed algorithmic components in the appendix. We will reorganize this section in the camera-ready version.\n\n**Q2:**\nWe removed the arxiv version.\n\n### Methodology:\n**Q1:**\n We reorganized our contributions and stated the difference between our model and MOAT:\n1. MOAT didn't consider the OOD problem.\n2. MOAT embeds the token-level expressions so it can't be aware of data distribution. Our method regards a feature set as a feature-feature interaction graph to capture the potential relationship between features and embeds raw data to be aware of the distribution of the dataset.\n3. We introduced anti-shift mechanisms to address the OOD problem.\n\nOur contribution is summarized as:\n1. We improved the embedded-optimization-reconstruction feature transformation framework to make it dataset distribution-aware rather than limited to token-level expressions.\n2.  We introduced a shift-resistant feature representation, flatness-aware generation, and incorporated normalization techniques to address distribution shift.\n3.  We conduct extensive experiments to demonstrate the efficiency, resilience, and traceability of our framework.\n\n**Q2:**\nData preparation is not a crucial step in our framework. So we follow the advanced paper GRFG [1] to sample high-quality and diverse training data.\n\n[1] Wang, Dongjie, et al. \"Group-wise reinforcement feature generation for optimal and explainable representation space reconstruction.\" Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022.\n\n**Q3&Q4:**\nWe address an essential AI task: The out-of-distribution problem in generative feature transformation, which is at the heart of ICLR. We propose a coherent approach to solve this problem. This is the first research task to solve the OOD of feature transformation. Several developed techniques in our method are generalizable:\n- shift-resistant feature set representation: it can rectify and adjust the distortion and ensure reliable and transferable feature knowledge in the testing set.\n- flatness-aware search: it's simple but effective to help identify the reliable in-distribution embedding.\nThese computing thinking and insights are generic for feature engineering.\n\n### Experiments:\n**Q1:**\nWe did not report results on other datasets because these datasets are stable and it's hard to observe a significant distribution shift when splitting the data. We will clarify this in the dataset section.\n\n\nWe hope that our response can address your concerns."
            }
        },
        {
            "comment": {
                "value": "We appreciate your acknowledgment that our research task is interesting and important.\n\nRegarding your specific concerns:\n\n### W1:\n\nWe improved the embedded-optimization-reconstruction feature transformation framework to make it encode raw data and aware of data distribution, rather than limited to token-level expressions. Then we developed anti-shift methods to address the OOD problem in the feature transformation task. We will reorganize these contributions and avoid misunderstanding.\n\n### W2:\n\n1. Our core task is to address the distribution shift problem in feature transformation.\n2. We model the observed transformed feature set as a feature-feature interaction graph to capture underlying relationships between features and data distribution. We use inductive graph embedding algorithms to handle the dynamic changes in the number of nodes representing different observed transformed feature sets.\n3. We use sample reweighting to debias shift.\n\n\n### W3:\n\n\nOur main research task is to address the distribution shift problem in feature transformation. We regard the feature set as a graph for the following reasons:\n\n1. To directly embed raw data and capture data distribution;\n2. By representing the dataset as a feature-feature interaction graph, we can capture underlying relationships between features. Existing studies show that treating features or feature domains as nodes can effectively capture relationships between features.[1][2]\n\n[1] He, Xiaofei, Deng Cai, and Partha Niyogi. \"Laplacian score for feature selection.\" Advances in neural information processing systems 18 (2005).\n\n[2] Li, Zekun, et al. \"Fi-gnn: Modeling feature interactions via graph neural networks for ctr prediction.\" Proceedings of the 28th ACM international conference on information and knowledge management. 2019.\n\n### W4:\n\nWe did not report results on other datasets because these datasets are stable and it's hard to observe a significant distribution shift when splitting the data. We will clarify this in the dataset section.\n\nWe hope that our response can address your concerns."
            }
        },
        {
            "comment": {
                "value": "We appreciate your acknowledgment that our research problem is crucial, the proposed method is comprehensive, and the experiments are extensive.\n\nRegarding the weaknesses:\n\n### W1:\n\nIn each section, we emphasized the motivation behind each technique and we will further highlight this in the revised version.\n1. Generative feature transformation framework: Prevents the exponential growth of possible feature combinations.\n2. RL data collection: Leveraging RL\u2019s exploration-exploitation mechanism automates the collection of high-quality, diverse training data to build a better continuous space.\n3. Anti-shift mechanisms: In the generative feature transformation framework, training-testing shifts can distort the embedding space. Anti-shift mechanisms adjust for these distortions, ensuring reliable and transferable feature knowledge in the test set.\n\n### W2:\n\nWhile some techniques we use, like flatness-aware methods, have been studied before, our integration of these techniques is a purposeful adaptation to tackle complex distributional shift challenges in feature transformation. Our experiments show that this approach effectively addresses OOD issues in feature transformation, which prior research has not considered. We believe this contribution is both innovative and practically impactful.\n\n### W3:\n\n1. The datasets are public and widely adopted for validating feature transformation approaches, as previous studies have extensively used them to demonstrate effectiveness.\n2. Scalability is very important. Small-scale experiments are a valid first step for concept validation. Appendix D shows that our model performs well in terms of time and space complexity compared to advanced baselines like MOAT and GRFG, indicating its potential for scalability to larger datasets.\n\n### W4:\n\nWe used the most classic and commonly adopted baselines in feature transformation and also compared recent advanced models like GRFG and MOAT.\n\n### W5:\n\nOur original purpose was to demonstrate our workload and respect to ICLR reviewers. This paper is intensive, but we put effort into writing and the writing includes our computing thinking. Our logic pipeline is:\n1. background\n2. limitations\n3. highlight research gap\n4. summarize our insights\n5. summarize our proposed method\n6. our contributions\n\nWe will reorganize our introduction and remove some redundancy to make it more clear.\n\n### W6:\n\nWe provided a formal mathematical formulation for DSFT in the problem statement to enhance clarity:\n$$\n    \\Gamma^{\\ast}=\\psi (\\mathbf{E}^{\\ast}) = \\mathop{\\arg\\min}_{\\mathbf{E}\\in \\varepsilon}\\mathcal{A}(\\mathcal{M}(\\psi (\\mathbf{E})(\\mathbf{X}^{tr})), \\mathbf{y}^{tr}),\n$$\nwhere $\\varepsilon$ is the continuous embedding space by transforming $\\mathbf{R}$. $\\mathbf{E}$ is the embedding vector in the space of $\\varepsilon$, and we define the optimal embedding vector is $\\mathbf{E}^{\\ast}$. $\\psi$ is a reconstructing function that can rebuild the feature transformation operation sequence relying on the embedding vector $\\mathbf{E}$ from $\\varepsilon$. $\\mathcal{A}$ indicates the effectiveness of the downstream task and the $\\mathcal{M}$ represents the downstream model. \n\nUltimately, we intend to employ $\\Gamma^{\\ast}$ to transform the original training feature set $\\mathbf{X}^{tr}$ to the optimal training feature set $\\mathbf{X}^{\\ast}$. We expect the feature categories in $\\mathbf{X}^{\\ast}$ can also be the optimal feature categories on the test dataset $\\mathcal{D}^{te}$ under distribution shifts.\n\n### W7:\n\nWe carefully checked these minor issues and fixed them.\n\nWe hope that our response can address your concerns."
            }
        },
        {
            "summary": {
                "value": "This paper studies feature transformation problem within the context of distributional shift in real-world scenarios. It introduces a Shift-aware Representation-Generation Perspective, which involves encoding transformed features into embedding vectors and decoding the optimal embedding. To address distributional shift, the paper proposes several techniques, including shift-resistant representation, flatness-aware generation, and shift-aligned pre- and post-processing. The effectiveness of these methods is evaluated through experiments on classification and regression tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "-\tThis paper addresses the important problem of feature transformation under distributional shift, a crucial challenge in deploying large foundation models in practical settings nowadays.\n-\tThe paper introduces a comprehensive pipeline for feature transformation, including data collection, feature graph embedding, transformation, and pre-/post-processing stages.\n-\tExtensive experimental validation is provided, covering classification and regression on multiple datasets, ablation studies, and robustness analyses"
            },
            "weaknesses": {
                "value": "-\tThe contribution of this paper appears somewhat ad hoc, as it incorporates a variety of techniques such as RL-based data collection, shift-resistant feature graph embedding, flatness-aware transformation, and shift-aligned pre-/post-processing without clearly explaining the motivation behind each component. \n-\tSeveral of the techniques used, particularly flatness-aware methods for addressing distributional shift, have been widely studied. The integration of these methods offers only incremental technical contributions.\n-\tThe experiments are primarily conducted on small-scale datasets, raising concerns about the scalability and generalizability of the proposed approach to large-scale pretraining datasets nowadays.\n-\tThe selection of baseline models for comparison appears outdated, as most were published prior to 2020, potentially limiting the relevance of the comparisons.\n-\tThe paper's organization and clarity could be improved; for instance, the introduction contains redundant and hard-to-follow contexts that hinder readability.\n-\tThe mathematical presentation is unsatisfactory; for example, providing a formal mathematical formulation for DSFT would enhance clarity over the current textual description.\n-\tSome notations are introduced before being defined, such as $f_1$, $f_2$."
            },
            "questions": {
                "value": "please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Existing techniques for addressing the distribution shift cannot be directly applied to discrete search problems and present two primary challenges: (1) How can we reformulate and solve feature transformation as a learning problem? What mechanisms can integrate shift awareness into such a learning paradigm? To tackle these challenges, the authors leverage a unique Shift-aware Representation-Generation Perspective. To formulate a learning scheme, they construct a representation-generation framework: (1) representation step: encoding transformed feature sets into embedding vectors (2) generation step: pinpointing the best embedding and decoding as a transformed feature set. To mitigate the issue of distribution shift, they propose three mechanisms: (1) shift-resistant representation, where embedding dimension decorrelation and sample reweighting are integrated to extract the true representation that contains invariant information under distribution shift; (2) flatness-aware generation, where several suboptimal embeddings along the optimization trajectory are averaged to obtain a robust optimal embedding, providing effective for diverse distribution, and (3) shift-aligned pre and post-processing, where normalizing align and recover distribution gaps between training and testing data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper addresses an interesting question: How do we transform features when there is a distribution shift?"
            },
            "weaknesses": {
                "value": "- The formulation of the discrete search problem into a deep learning problem is similar to [Reinforcement-enhanced autoregressive feature transformation: Gradient-steered search in continuous space for postfix expressions].\n- The novelty of the methodology is limited. Is the representation step specially designed for encoding feature transformation problems?\n- Although considering the feature sets as a feature-feature interaction attributed graph is novel, the motivation behind it is not well explained. Why can the feature sets be considered a graph? Is it reasonable?\n- Limited empirical validation. The authors only used 16 datasets for evaluation. However, as the baseline outlined in the paper [Reinforcement-enhanced autoregressive feature transformation: Gradient-steered search in continuous space for postfix expressions], there are 23 datasets that have been evaluated."
            },
            "questions": {
                "value": "See Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper deals with the Neural Feature Transformation problem in the context of distribution shift, i.e., Distribution Shift Feature Transformation (DSFT) problem.   \n\nSpecifically, it follows a representation-generation framework similar to (Wang et al., 2023), involving a representation step and a generation step. Moreover, three mechanisms are designed to deal with the DSFT problems: shift-resistant representation, flatness-awareness generation, and shift-aligned pre and post-processing.   \n\nDuring training, Shift-resistant Bilevel traning is proposed, and flatness-aware gradient ascent is incorporated. \n\nExperiments are conducted on UCI and OpenML datasets with Kolmogorov-Smirnov to construct the distribution shifted data splits. Comparisons and ablations are conducted."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "### Problem \n- This paper addresses the Distribution Shifted Feature Transformation (DSFT) problem, which is relatively new and overall makes sense. \n### Methodology & Ablation of Modules\n- Specific techniques are designed to combat shifts in a representation-generation framework (Wang et al., 2023): bilevel training of samole weights and model parameters, flatness-aware gradien ascent.   \nThe ablation in Figure 4 shows the effectiveness. \n### Experiments \n- Experiments are conducted on 16 benchmark datasets from UCI and OpenML. The proposed method outperforms SOTA in most settings."
            },
            "weaknesses": {
                "value": "### Organization and writing\n- Overall, the main idea is easy to grasp. While the technical details, modules, and work flows are not quite easy to follow. For example, Algorithm 1 use full paragraph texts instead of algorithm-style to explain the framework. \n- From line 627-632, the MOAT paper: Reinforcement-enhanced autoregressive feature transformation: Gradient-steered search in con-\ntinuous space for postfix expression, appears twice. In line 294 argmin. \n### Methodology\n- While distribution shift problem is relatively novel, the main framework follows MOAT (Wang et al., 2023), see Figure 2 of MOAT. This is neither stated nor discussed in the paper. This is kind of misleading or overclaiming the contributions. A better practice is to explicitly clarify how the framework differes from MOAT, and the contributions to distribution shift. \n- The RL pipeline for data preparation, $L_{rec}, L_{est}$ follow MOAT. Or any changes? \n- New modules in Eq. (1) is straightforward with several existing work such as [R1]. The utilization for distribution shift and re-weighting is reasonable, but the overall technical contribution is not significant in this point. \n- Flatness-aware gradient ascent is motivated from existing works (Izmailov et al., 2018; Garipov et al., 2018). According to Algorithm 3, it seems a simple technique. \n### Experiments\n- The SOTA baseline MOAT is experimented on 23 datasets, while this paper only on 16. Can you provide a justification why only 16 datasets are experimented or include more results for comprehensive comparison. \n\n\n[R1] Learning to reweight examples for robust deep learning"
            },
            "questions": {
                "value": "- What are the main differences of the proposed method and MOAT? And how these differences address the Distribution Shift? \n- MOAT experimented on 23 datasets, while this paper only 16. Why the other benchmarks are not reported?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a feature transformation technique to improve the AI capability from Shift-aware Representation-Generation perspective. And three mechanisms are proposed to address distribution shift by shift-resistant feature set representation, flatness-aware generation, and integration of normalization. Extensive experiments have been conducted on classification and regression tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is well presented and easy to understand.\n2. This paper addresses an interesting topic, feature transformation is crucial to improve the performance of the model."
            },
            "weaknesses": {
                "value": "1. The contribution point 1 is not clear enough. The embedded-optimization-reconstruction framework was proposed in baseline method [1].  The distinction with baseline methods needs to be made clear in the motivation section.\n2. The experimental results are questionable. In Table 1, some experimental results are not consistent with the results reported in [1], and are significantly lower than the reported results. For example, Higgs Boston, please give a reasonable explanation.  In addition, compared with the baseline method, there are still some results on other datasets not shown.\n3. Ablation experiments. Three mechanisms proposed in the method, it is suggested to provide ablation experiments of these three mechanisms in the ablation experiments section.\n\n\n[1] Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions. (NeurIPS 2023)"
            },
            "questions": {
                "value": "The main questions are in weaknesses 1 to 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}