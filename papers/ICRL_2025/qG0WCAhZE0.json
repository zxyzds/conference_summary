{
    "id": "qG0WCAhZE0",
    "title": "Multi-Perspective Data Augmentation for Few-shot Object Detection",
    "abstract": "Recent few-shot object detection (FSOD) methods have focused on  augmenting synthetic samples for novel classes, show promising results  to the rise of diffusion models. However, the diversity of such datasets is often limited in representativeness because they lack awareness of typical and hard samples, especially in the context of foreground and background relationships. To tackle this issue, we propose a Multi-Perspective Data Augmentation (MPAD) framework. In terms of foreground-foreground relationships, we propose chain-of-thought prompting for object synthesis (CPOS) with bounding box adjustments to enhance the detail and spatial information of synthetic samples. Inspired by the large margin principle, support samples play a vital role in defining class boundaries. Therefore, we design a Harmonic Prompt Aggregation Scheduler (HPAS) to mix prompt embeddings at each time step of the generation process in diffusion models, producing hard novel samples. For foreground-background relationships, we introduce a Background Proposal method (BAP) to sample typical and hard backgrounds. Extensive experiments on multiple FSOD benchmarks demonstrate the effectiveness of our approach. Our framework significantly outperforms traditional methods, achieving an average increase of $17.5\\%$ in nAP50 over the baseline on PASCAL VOC.",
    "keywords": [
        "few-shot object detection",
        "controllable diffusion",
        "data augmentation"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=qG0WCAhZE0",
    "pdf_link": "https://openreview.net/pdf?id=qG0WCAhZE0",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a Multi-Perspective Data Augmentation (MPAD) framework to improve few-shot object detection (FSOD) by generating diverse and representative synthetic samples. The framework includes three components: Chain-of-Thought Prompting for Object Synthesis (CPOS) uses large language models to enhance prompt diversity with fine-grained attributes; Harmonic Prompt Aggregation Scheduler (HPAS) mixes base and novel class features in the diffusion process, producing hard-to-classify samples; and Background Proposal (BAP) selects complex or visually similar backgrounds to improve object-background differentiation. Extensive experiments on PASCAL VOC and COCO few-shot object detection benchmarks show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1: The MPAD framework leverages Chain-of-Thought Prompting to generate prompts with fine-grained attributes, enabling diverse and representative data synthesis for few-shot object detection. \n\n2: The proposed method is easy to follow and not limited to some specific object detection architectures, making the technique be applied in different scenarios without further modifications. \n\n3: The main results on PASCAL VOC and COCO few-shot datasets illustrate that the proposed method can improve non-trivial performance on many few-shot object detection benchmarks."
            },
            "weaknesses": {
                "value": "1: The description that \"this work is the first ...\" in Lines 81-83 is a little bit over-claimed. From my perspective, there is already a large amount of work exploring using large-scale pretrained diffusion models for object detection by generation (like general object detection and corner case generation for autonomous driving). Simply extending the setting to few-shot object detection isn't that significant to me.\n\n2: Section 2.3 (CoT prompting for object synthesis) actually belongs to prompt engineering. The Chain-of-Thought emphasizes solving a problem step by step. For object synthesis discussed in this paper, the task simply needs to list all possible attributes of a category ( or use a pre-defined template). This can be regarded as in-context learning or prompt engineering rather than Chain-of-Thought. Meanwhile, the inpainting model is also modified from other previous works.\n\n3: The overall technique contribution is limited. Continuing from point 2, the method is more like a combination of several existing verified techniques for object detection data generation without a clear logical chain. The HPAS is a prompt embedding level mixup data augmentation which is also not novel (the insight of embedding mixup is common in conditional data generation using diffusion model). It's also hard to convince me that the BAP part brings much novelty.   \n\n4: Comparisons with deep learning methods (using large-scale pretrained models like diffusion model and CLIP, etc). In Table 3, the authors only compare their proposed method with some non-deep learning-based methods. However, given the widely used conditional diffusion model for object detection object generation, the authors also should add more baselines such as simply using the **Powerpoint** inpainting model for data generation, to further verify the effectiveness of the proposed module that is claimed to enhance the diversity.\n\nOverall, I tend to vote for rejection at this time because of the technical contribution and the proposed method itself, along with comprehensive comparisons. I am waiting and will hear from other reviewers."
            },
            "questions": {
                "value": "1: In the Introduction part (especially for Figure 1), what's the definition of typical and hard objects on the novel set? How to distinguish them?\n\n2: Which are the novel synthetic samples and the base real samples in Novel Set 1 in Figure 1? Why do some colors (classes) only have solid circle samples without any \"x\" samples? (I am deeply Confused by Figure 1.) \n\n3: In Table 2, is it a typo that sometimes the metric is \"nAP\" while sometimes not?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper attempts to enhance the diversity of data generation by analyzing typical and hard samples. Specifically, a data generation architecture is proposed that comprehensively considers the typicality and difficulty of foreground and background selections, establishing the relationship between foreground and background. Experimental evidence demonstrates significant performance improvements compared to recent methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ From the experimental data, the current data augmentation methods show a certain degree of performance improvement.\n+ Integrating various mainstream generative models and zero-shot learning models, including diffusion, CLIP, etc."
            },
            "weaknesses": {
                "value": "- The present approach heavily depends on utilizing pre-trained models to select typical and challenging samples, potentially causing interference when assessing the efficacy of augmentation strategies.\n- Additional clarification is needed regarding the fairness of the experiments."
            },
            "questions": {
                "value": "1. What are the definitions of typical and hard samples? Detailed assessment criteria need to be further elaborated.\n2. In Section 2.2, lines 159-161 mention that HPAS can generate hard samples. Strategically, it may indeed involve a wider range of categories. However, for the model, the level of difficulty in recognition is not solely dependent on the number of categories in the image. Why are samples generated based on mixed prompt embedding considered hard samples in HPAS?\n3. In the last paragraph on the fifth page, it mentions the concept of camouflage targets. Camouflaged objects typically refer to objects where the foreground and background have high similarity. However, in the subsequent text, the selection of data with high similarity between the background in the base stage and the classes in the novel stage as challenging data does not explicitly demonstrate the connection between selecting difficult backgrounds and camouflaged objects. Therefore, what is the relationship between the selection of challenging backgrounds and camouflaged objects?\n4. In Equation 6, it is mentioned that typical clutter background is obtained by calculating entropy. Such selection heavily relies on the performance of the feature extractor F. If the predictive results are good, few background images may be selected. However, if the predictive results are poor, do the selected background images hold any useful value?\n5. The architecture of this paper incorporates various methods with pre-trained models, such as CLIP. Is the fairness of the comparison between the current augmentation strategy and previous methods lacking?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a Multi-Perspective Data Augmentation (MPAD) framework aimed at enhancing few-shot object detection (FSOD) by generating diverse and challenging synthetic samples. The MPAD framework utilizes techniques such as Chain-of-Thought Prompting for Object Synthesis (CPOS), Harmonic Prompt Aggregation Scheduler (HPAS), and Background Proposal (BAP) to create representative and hard samples. The authors present results demonstrating significant performance improvements on PASCAL VOC and MS COCO benchmarks, showing an average increase of 17.5% in nAP50 over baseline methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe CPOS and HPAS introduce novel ways to leverage both typical and hard samples, leading to a more representative synthetic dataset.\n2.\tThe use of BAP to generate diverse backgrounds helps enhance detection accuracy by allowing the model to distinguish between foreground and background more effectively.\n3.\tThe proposed framework achieves notable gains over state-of-the-art baselines on multiple FSOD benchmarks, particularly in challenging low-shot settings."
            },
            "weaknesses": {
                "value": "1. The framework combines multiple advanced techniques, including diffusion models, harmonic prompt scheduling, and complex background sampling, which may make it challenging for practitioners to implement effectively in real-world scenarios. At the same time, it increases the complexity of the model. Please analyze the model complexity and real-time inference of the generated model.\n2. The paper demonstrates performance gains, but it would benefit from a more granular analysis comparing the effectiveness of each augmentation component (CPOS, HPAS) against similar elements in other FSOD methods. Has the pre-trained model of the generated model already seen various categories to generate more realistic new class samples when regenerated again? Can other generative models also generate samples to improve the performance of small samples, and does the proposed method have such a significant performance improvement compared to this type of method\nData Efficiency in Synthetic Generation:\n3. While the method performs well, additional experiments assessing data efficiency could be valuable. Evaluating how much synthetic data is optimal or exploring the impact of different amounts of augmented data on performance could provide insights into the scalability and efficiency of the framework."
            },
            "questions": {
                "value": "Please refer to the Weaknesses box."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a Multi-Perspective Data Augmentation (MPAD) framework for few-shot object detection (FSOD), aiming to address the limitations of existing data augmentation methods. By considering both foreground-foreground and foreground-background relationships in MPAD, the authors provide an interesting solution to the problem of data augmentation. The experimental results on PASCAL VOC and MS COCO datasets demonstrate the effectiveness of the proposed methods, outperforming many state-of-the-art methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is generally well-written and the idea is presented clearly. \n\n2. The introduction provides a good background and motivation for the research\n\n3. The proposed methods are described in detail and mostly are easy to follow. \n\n4. The experimental setup and results are also presented in a clear and organized manner.\n\n5. The authors also conduct a series of ablation studies to better understand the proposed method."
            },
            "weaknesses": {
                "value": "1. My main concern lies in the use of Diffusion Model (DM) and ChatGPT. From the perspective of the FSOD task, data is always the most crucial element. Consequently, it is essential to employ ChatGPT and DM to generate data as a means to address this concern, as the authors did in this study. However, the following questions arise: if ChatGPT and DM are being used, why not simply generate a substantial number of samples of the corresponding categories and conduct direct training on the generated data? Alternatively, another option could be to pre-train the detector on the generated data and subsequently perform fine-tuning on few-shot categories. Currently, the authors haven't furnished a compelling rationale to either explain, oppose, or support these viewpoints, which are of utmost significance for this paper. \n\n    To answer these questions, I suggest the authors: \n    \n    - (1)  conduct a comprehensive analysis comparing the performance of direct training on generated data with the proposed method; \n\n    - (2) explore the potential benefits and drawbacks of pretraining on generated data and then fine-tuning on few-shot categories. This could include experiments to determine the optimal amount of pretraining and the impact on final performance; \n\n    - (3) provide a detailed explanation of why the proposed approach of using ChatGPT and DM in a specific way is more advantageous than (1) and (2). \n\n2. Some details could be further elaborated. For example, in the description of the Harmonic Prompt Aggregation Scheduler (HPAS), the role of the momentum parameter and its impact on the generated samples could be explained more clearly. For example, the authors can provide a figure to visualize generated samples across a range of momentum values (e.g. 0.1, 0.5, 0.9) to illustrate how this parameter impacts the mixing of base and novel class features.\n\n3. Additionally, the discussion on the limitations of the diffusion model in the appendix could be more in-depth, perhaps exploring potential solutions or future research directions. For example, (1) analyzing whether the hallucination will affect the final result of FSOD, and if so, what possible means can be used to solve it; or (2) is there any possible way to train (in a PEFT manner) an FSOD-aware DM to further improve the performance. In addition, I suggest putting the \"limitation\" part to the main text; With these changes, this paper becomes more inspiring to the reader."
            },
            "questions": {
                "value": "See \"Weaknesses\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}