{
    "id": "MjhTb4gwFP",
    "title": "PerLDiff: Controllable Street View Synthesis Using Perspective-Layout Diffusion Model",
    "abstract": "Controllable generation is considered a potentially vital approach to address the challenge of annotating 3D data, and the precision of such controllable generation becomes particularly imperative in the context of data production for autonomous driving. Existing methods focus on the integration of diverse generative information into controlling inputs, utilizing frameworks such as GLIGEN or ControlNet, to produce commendable outcomes in controllable generation. However, such approaches intrinsically restrict generation performance to the learning capacities of predefined network architectures. In this paper, we explore the integration of controlling information and introduce PerLDiff (\\textbf{Per}spective-\\textbf{L}ayout \\textbf{Diff}usion Models), a method for effective street view image generation that fully leverages perspective 3D geometric information. Our PerLDiff employs 3D geometric priors to guide the generation of street view images with precise object-level control within the network learning process, resulting in a more robust and controllable output. Moreover, it demonstrates superior controllability compared to alternative layout control methods. Empirical results justify that our PerLDiff markedly enhances the precision of generation on the NuScenes and KITTI datasets.",
    "keywords": [
        "Controllable generation;3D annotation;Geometric priors;"
    ],
    "primary_area": "generative models",
    "TLDR": "PerLDiff, a new framework crafted to generate synthetic images based on user-defined 3D annotations.",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=MjhTb4gwFP",
    "pdf_link": "https://openreview.net/pdf?id=MjhTb4gwFP",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces PerLDiff, a method aimed at enhancing control in street view image generation, which is crucial for efficient data annotation in autonomous driving applications. PerLDiff incorporates layout-based masking maps as geometric priors and introduces a PerL-based Cross-Attention Mechanism within the Control Module (PerL-CM). This mechanism facilitates precise alignment of objects and scene details by integrating scene-wide and object-specific information derived from BEV annotations. Empirical evaluations on NuScenes and KITTI datasets suggest that PerLDiff achieves improved control and realism compared to baseline models such as BEVControl and MagicDrive."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The overall paper writing is clear and is easy to follow.\n* The introduced PerL-based Cross-attention sounds reasonable and performs well on some detailed experiments.\n* PerLDiff demonstrates better object position controllability in scene generation compared to methods like BEVControl and MagicDrive."
            },
            "weaknesses": {
                "value": "* The authors use a mask-based representation in the PerL-based Cross-Attention mechanism. However, this type of representation lacks sufficient 3D priors, which may contribute to the orientation generation issues for vehicles mentioned in the limitations. Did the authors consider using the depth information of multiple vehicles with ControlNet to enhance object controllability? Specifically, it would be insightful to explore the benefits of Depth + ControlNet vs Mask + PerL-based Cross-Attention in improving object controllability.\n* Has the PerL-based Cross-Attention been tested on other baselines beyond BEVControl to examine its potential improvements in controllability and perception tasks? For instance, would applying it to a baseline like MagicDrive yield similar gains?\n* Is there any ablation study on the ConvNext for encoding the road map? How about other choices for the road map encoding? And why the ConvNext is frozen?"
            },
            "questions": {
                "value": "* The term \"train + Syn. val*\" in Table 3 is not clearly defined.\n* In Table 1, why the FID metric of PerLDiff is worse than the baseline BEVControl? Moreover, why the performance with BEVFormer is better than the one with multi-modal BEVFusion?\n* In Table 2, why PerLDiff is significantly better than the BEVControl? I want to know the true reason behind the method perspective."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel method for controllable street view generation utilizing a perspective layout diffusion model. The control factors are formulated as bounding boxes, lane structures, and scene descriptions. Subsequently, a cross-view framework, grounded in stable diffusion, is crafted to synthesize the desired multi-view images. Experimental results across various settings (i.e. generation quality, 3D object detection, and lane segmentation performance)\u2014demonstrate the efficacy of the proposed approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+  solution for the data hungry autonomous driving. Meanwhile, the authors also conduct experiments to demonstrate the generated data can improve perception models' performance.\n\n+ The writing is clear and the paper is easy to follow.\n\n+ The authors conduct multiple experiments under different settings to demonstrate the proposed method can outperform the previous view synthesis methods."
            },
            "weaknesses": {
                "value": "+ The first question is whether the proposed method can ensure temporal consistency for the generated images. The paper doesn't discuss this problem and displays the image in different timestamps. However, the detection and segmentation models used, particularly StreamPETR, require temporal inputs. If the synthesized images do not achieve good temporal consistency, how can they improve the performance of temporal-based methods?\n\n\n+ In Section 3, it appears that all input data comes from 2D space (with boxes and roadmaps projected onto images and a general scene description). Another question is whether the proposed method can effectively disentangle the camera's intrinsic and extrinsic parameters compared to BEVGen and BEVControl.\n\n\n+ Cross-view attention: Could the authors explain the differences between view cross-attention and text cross-attention in PerlDiff compared to BEVControl and MagicDrive?\n\n\n+ Object rotation controllability: Could the authors clarify how the proposed method achieves object rotation controllability? In Section 3, it seems that the box information is represented as eight corners in the image space, where the rotation information is not explicitly presented."
            },
            "questions": {
                "value": "+ Could the authors provide the details of the experimental setting on the nuScenes dataset? One point of confusion is that,  compared to the training set, the validation set is not very large. Why does adding it to the training data lead to such a significant improvement in model performance?\n\n+ What is the meaning of * in Table 3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a controllable image generation framework specifically tailored for autonomous driving applications. It employs a modified attention module that enhances instance controllability by incorporating an instance binary mask, thereby improving the precision of instance manipulation within the generated images."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. PerLDiff improves upon the controllability of previous work, enabling more effective control over vehicle pose and map segments.\n2. This study validates its framework across diverse datasets and settings, encompassing the NuScenes dataset, the KITTI dataset, as well as 3D detection and map segmentation tasks."
            },
            "weaknesses": {
                "value": "1. The reason for selecting ConvNext as the feature extraction network requires further exploration, particularly given the training objective of image classification on the ImageNet dataset. This raises questions about the appropriateness of utilizing ConvNext in this context. Additionally, it is essential to conduct an ablation study comparing ConvNext with other image backbone architectures, such as the CLIP image encoder, to provide a comprehensive understanding of their relative performance.\n2. The formulation of this work is not novel to me; rather, it represents a combination of several existing methodologies. The framework closely resembles that of Panacea and MagicDrive, with the key modification being the introduction of an attention mask.\n3. Lack of quantitative comparison with Panacea [a], which similarly uses the perspective layout.\n\n[a] Panacea: Panoramic and Controllable Video Generation for Autonomous Driving"
            },
            "questions": {
                "value": "1. I suggest not using the word BEV annotations. Both 3D box annotations and map annotations exist within 3D space rather than BEV space.\n2. Section 3.2 is hard to read. I suggest to include a dedicated diagram to effectively illustrate the PerL-based cross-attention mechanism. The bottom of Figure 2 need to be refine.\n3. Why is the attention mask not multiplied into A_s in Equation 5? Adding a binary mask into the feature seems unconventional.\n4. Given that using perspective-Layout, why not use controlnet framework for more straightforward interaction?\n5. Can this method be applied to occupancy control?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}