{
    "id": "BvQkjCnXXr",
    "title": "Simple Yet Efficient Locality Sensitive Hashing with Theoretical Guarantee",
    "abstract": "Locality-sensitive hashing (LSH) is an effective randomized technique widely used in many machine learning tasks such as outlier detection, neural network training and nearest neighbor search. The cost of hashing is the main performance bottleneck of these applications because the index construction functionality, a core component dominating the end-to-end latency, involves the evaluation of a large number of hash functions. Surprisingly, however, little work has been done to improve the efficiency of LSH computation. In this paper, we design a simple yet efficient LSH scheme, named FastLSH, by combining random sampling and random projection. FastLSH reduces the hashing complexity from $O(n)$ to $O(m)$ ($m<n$), where $n$ is the data dimensionality and $m$ is the number of sampled dimensions. More importantly, FastLSH has provable LSH property, which distinguishes it from the non-LSH fast sketches. To demonstrate its broad applicability, we conduct comprehensive experiments over three machine learning tasks, i.e., outlier detection, neural network training and nearest neighbor search. Experimental results show that algorithms powered by FastLSH provides up to 6.1x, 1.7x and 20x end-to-end speedup in anomaly detection latency, training time and index construction, respectively. The source code is available at https://anonymous.4open.science/r/FastLSHForMachineLearning-7CAC.",
    "keywords": [
        "Locality-sensitive hashing",
        "random sampling",
        "machine learning"
    ],
    "primary_area": "infrastructure, software libraries, hardware, systems, etc.",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=BvQkjCnXXr",
    "pdf_link": "https://openreview.net/pdf?id=BvQkjCnXXr",
    "comments": [
        {
            "title": {
                "value": "Response to Author Feedbacks"
            },
            "comment": {
                "value": "**Response w.r.t. W2.1 Feedback**\n\nAs shown in CLT, the application condition assumes a sequential **i.i.d. random samples**. Though the dimensions are independently selected, whether the samples, i.e. the samples $(v_i-u_i)^2$, are i.i.d is questionable. IMHO, the i.i.d. assumption may hold if $v_i$ across different dimensions and samples should also be drawn i.i.d. from a distribution. \n\n**Response w.r.t. W2.3 Feedback**\n\nGiven that more rigorous theoretical guarantee is not given. The term \"with theoretical guarantee\" is to some extent an overclaim. With the problem in W2.2 not properly addressed, this reduces the theoretical contribution of this paper.\n\n**Response w.r.t. W3.1 Feedback**\n\nSorry for missing the references, as below:\n[1] Malkov, Yu A., et al. \"Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs.\"\u00a0\n[2] Yang, Chengcheng, et al. \"Efficient locality-sensitive hashing over high-dimensional data streams.\"\n[3] Wang, Hao, et al. \"Efficient locality-sensitive hashing over high-dimensional streaming data.\"\n\n**Response w.r.t. W3.2 Feedback**\nThough the references are given, majority works in this literature follows the official split of these datasets. Given the fact that hashing based methods are quite fast, it is unnecessary to select a subset of queries. By doing so, it is more difficult to position the contribution of this paper in a wider range of literature.\n\n**Summary**\n\nGiven the feedback from the authors, the problems within the theoretical framework are not well addressed, especially the authors admit the flaws in Theorem 4.6, which is a key part. The term \"with theoretical guarantee of LSH properties\" is somewhat overclaimed. \n\nMeanwhile, the empirical assessments are not sufficient to show the value in terms of wide application of the proposed methods. For example, given the authors have provided more works using LSH in model training, they should provide more experiments rather than just for SLIDE.\n\nIMHO, to well position the contribution of a simple methodology, one can either provide rigorous theoretical guarantees, or conduct extensive and comprehensive experiments to demonstrate their wide usability. Given the current status of this paper and the feedback, I may not change the score."
            }
        },
        {
            "title": {
                "value": "Continued Comment to Reviewer yqFQ"
            },
            "comment": {
                "value": "## Response to W3.2\nFor ANN search tasks, many papers [1-4] randomly select a subset from the given query set, such as 50, 100, or 200. We have also adopted this approach.\n\n[1] Huang, Qiang, et al. Query-aware locality-sensitive hashing for approximate nearest neighbor search. In Proceedings of the VLDB Endowment 9.1 (2015): 1-12.\n\n[2] Sun, Yifang, et al. SRS: solving c-approximate nearest neighbor queries in high dimensional euclidean space with a tiny index. In Proceedings of the VLDB Endowment (2014).\n\n[3] Lei, Yifan, et al. Locality-sensitive hashing scheme based on longest circular co-substring. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. 2020.\n\n[4] Tian, Yao, Xi Zhao, and Xiaofang Zhou. DB-LSH 2.0: Locality-sensitive hashing with query-based dynamic bucketing. IEEE Transactions on Knowledge and Data Engineering (2023).\n\n\n## Response to W3.3\nThe acceleration of FastLSH does not require any hyperparameter tuning across datasets and is solely dependent on the chosen sampling dimension $m$. Since we report the total execution time for the anomaly detection task, which includes both index construction and query time, the execution time appears to vary significantly due to different dataset sizes and choices of $m$, as shown in Table 4 and Parameter Settings in Appendix C.1. Furthermore, the results in the anomaly detection and neural network training tasks, as shown in Table 1, Table 2 and Table 3 and Figure 2, demonstrate that FastLSH does not introduce distortions in the Hamming distances."
            }
        },
        {
            "title": {
                "value": "Continued Comment to Reviewer yqFQ"
            },
            "comment": {
                "value": "## Response to W2.2\nThe validity of Theorem 4.6 is conditional, i.e., $|x|\\leq O(m^{-1/2})$. Theorem 4.6 implies that $\\varphi_{\\tilde{s}X}(x)$ is asymptotically identical to $\\exp(-\\frac{ms^{2}x^{2}}{2n})$ within interval $[-\\mathcal{K}\\sqrt{\\frac{n}{ms^{2}}}, +\\mathcal{K}\\sqrt{\\frac{n}{ms^{2}}}]$ (i.e., $|x|\\leq O(m^{-1/2})$), that is, 2$\\mathcal{K}$ ``standard deviations\", where $\\mathcal{K}$ is an arbitrarily large constant. The goal is that when $\\mathcal{K}$ is sufficiently large, the effective components of the distributions of FastLSH and E2LSH are essentially identical. What we are proving here is the equivalence under the condition of $x$, not the equivalence for all values of $x$. Since we introduced the data-dependent quantity $\\sigma$ in Lemma 4.1, which makes FastLSH a data-dependent LSH, it is intractable for FastLSH to achieve the same level of perfect theoretical analysis as traditional data-independent LSH (E2LSH). For this issue, we will add a clearer description to specify the conditions under which Theorem 4.6 holds.\n\n\n## Response to W2.3  \nIn practical scenarios, $m$ is often limited. We study the relation between $f_{\\tilde{s}X}(t)$ and the PDF of $\\mathcal{N}(0,\\frac{ms^{2}}{n})$ when $m$ is relatively small ($m < n$). Particularly, we derive the first four moments of $\\tilde{s}X$ and $\\mathcal{N}(0,\\frac{ms^{2}}{n})$, and analyze how $m$ and $\\sigma$ affect their similarity. While in general the first four moments, or even the whole moment sequence may not determine a distribution [1], practitioners find that distributions near the normal can be decided very well given the first four moments [2,3]. To this end, we derive Lemma 4.8 rigorously, which is not merely a heuristic argument. \n\nThen we quantitatively analyzed the difference and manifests that the gap between FastLSH and LSH can be captured by parameters $\\epsilon$ and $\\lambda$ (Lemma 4.8 and Fact 4.9). In short, greater $m$ is, then smaller $\\epsilon$ and $\\lambda$ will be. Thus, by choosing appropriate $m$ ($m$ is set 30 in comparison with E2LSH), $\\epsilon$ and $\\lambda$ are small enough to reduce the impact of the variance in the squared distances of coordinates to a negligible level, which makes FastLSH and LSH are practically equivalent. Table 10 in Appendix C.8 illustrates the empirical evidences on how $m$ affects $\\epsilon$ and $\\lambda$ across 12 datasets we are experimented with. Furthermore, Figure 8 and Figure 9 in Appendix C.7 illustrate a comparison of the $\\rho$ curves (an important measure of the LSH property) for E2LSH and FastLSH. These figures show that their $\\rho$ curves match well across different datasets, verifying that FastLSH and E2LSH have the same LSH performance. As stated in W2.2, FastLSH is a data-dependent LSH, due to the introduced data-dependent number $\\sigma$, A more rigorous theoretical analysis is intractable. To the best of our knowledge, there is currently no data-dependent LSH that provides LSH property similar to data-independent LSH (E2LSH).\n\n[1] Lin, G. D. Recent developments on the moment problem. Journal of Statistical Distributions and Applications, 4 (1):5, 2017.\n\n[2] Leslie, D. Determination of parameters in the johnson system of probability distributions. Biometrika, 46(1/2): 229\u2013231, 1959.\n\n[3] Ramberg, J. S., Dudewicz, E. J., Tadikamalla, P. R., and Mykytka, E. F. A probability distribution and its uses in f itting data. Technometrics, 21(2):201\u2013214, 1979.\n\n\n## Response to W3.1\nThe SLIDE framework effectively demonstrates that FastLSH provides significant acceleration for frequently constructing hash tables while also improving query accuracy, even though it does not handle streaming data. Lemma 4.8 and Fact 4.9 provide an analysis of the distributional difference between FastLSH and E2LSH. By adjusting the size of $m<n$, FastLSH can asymptotically become equivalent to E2LSH. If E2LSH effectively handles streaming data, we believe FastLSH is also applicable. Regarding the references [2, 3] you mentioned, if time permits, we will add comparative experiments to further show the broad applicability of FastLSH. However, could you clarify which two papers you are referring to with [2, 3]?"
            }
        },
        {
            "title": {
                "value": "Comment to Reviewer yqFQ"
            },
            "comment": {
                "value": "Thanks for your feedback. Next we will address the main points you raised.\n\n## Response to W1\nOur work focuses on using LSH-based method to accelerate end-to-end index construction, supporting machine learning tasks such as anomaly detection, neural network training, and ANN search. It is worth noting that FastLSH and ACHash do not necessarily reduce query time; their primary purpose is to speed up the hash evaluations. More importantly, LSH can be applied to other tasks beyond ANN search, such as outlier detection and neural network training. For these tasks, only $k \\times L$ hash tables need to be built, and distance computations are not required. Additionally, these tasks might require frequent creation or updating of hash tables, such that the execution time is mainly dominated by the hashing cost, then a notable characteristic is that index construction time is more important than query processing. In such applications, FastLSH significantly reduces the hashing cost, as shown in Figure 3, Table 1, Table 6 and Table 7 in Appendix C.3. Although many graph-based methods (e.g., HNSW [1]) achieve higher query accuracy in ANN search, they are not suitable for accelerating end-to-end index construction in these applications. Research has shown that when HNSW is used to speed up neural network training, its time consumption is **23** times higher than using LSH, and HNSW does not have guarantees for search performance [2]. \n\nRegardless of whether it is the SLIDE framework or other frameworks (ACE [3], et al), if LSH is used for hashing involving inner product computations, FastLSH can integrate well into these frameworks, e.g., [4-6]. This is because, in the case of $m < n$, Lemma 4.8 and Fact 4.9 rigorously prove that FastLSH and E2LSH are very similar, by comparing the first four moments of the two distributions, despite the presence of data-dependent parameters $\\epsilon$ and $\\lambda$ introduced by $\\sigma$. Additionally, the empirical results in Table 9 in Appendix show that as $m$ increases, the influence of these parameters $\\epsilon$ and $\\lambda$ becomes very small, further indicating that FastLSH and E2LSH are very close. \n\nIt should be noted that the end-to-end index construction time for Gist1M is not approximately 30 seconds; we have scaled the results for better display, as shown in Figures 4(d) and 6(d). In fact, even when using FastLSH, with a sampled dimension of $m=30$ and under our $k \\times L$ settings, the index construction time takes at least 200 seconds. In contrast, when using 960 dimensions with E2LSH, the time consumption is even higher, with end-to-end index construction taking over a thousand seconds.\n\n[1] Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. Fast approximate nearest neighbor search with the navigating spreading-out graph. arXiv preprint arXiv:1707.00143, 2017.\n\n[2] Chen, Beidi, et al. Mongoose: A learnable lsh framework for efficient neural network training. In Proceedings of International Conference on Learning Representations. 2020.\n\n[3] Luo, Chen, and Anshumali Shrivastava. Arrays of (locality-sensitive) count estimators (ace) anomaly detection on the edge. In Proceedings of the 2018 World Wide Web Conference. 2018.\n\n[4] Chen, Beidi, et al. Mongoose: A learnable lsh framework for efficient neural network training. In Proceedings of International Conference on Learning Representations. 2020.\n\n[5] Kitaev, Nikita, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451 (2020).\n\n[6] Rabbani, Tahseen, Marco Bornstein, and Furong Huang. Large-Scale Distributed Learning via Private On-Device LSH. In Proceedings of Advances in Neural Information Processing Systems 36 (2024).\n\n## Response to W2.1\n\nFor given vector pair $(\\mathbf{v},\\mathbf{u})$, let $s = || \\mathbf{v}-\\mathbf{u} ||$. For our purpose, assume the collection of $n$ entries $(v_{i}-u_{i})^{2}$ $(i=1,2,\\ldots,n)$ is a population, which follows an unknown distribution with mean $\\mu =( {\\textstyle \\sum_{i=1}^{n}}(v_{i}-u_{i})^{2}) /n$ and variance $\\sigma^{2}=({\\textstyle \\sum_{i=1}^{n}}((v_{i}-u_{i})^{2}-\\mu)^{2})/ n $. It is obvious that, for any given pair of vectors of finite dimension $n$, the 2-norm of the difference across each dimension MUST has a finite mean $\\mu$ and finite variance $\\sigma^2$. Here, each entry $(v_{i}-u_{i})^{2}$ for $(i=1,2,\\ldots,n)$ is independently sampled from an unknown distribution. Therefore, we can use the CLT [1] to derive Lemma 4.1.\n\n[1] https://en.wikipedia.org/wiki/Central_limit_theorem"
            }
        },
        {
            "title": {
                "value": "Comment to Reviewer sEFP"
            },
            "comment": {
                "value": "Thanks for your feedback. Next we will address the main points you raised.\n\n## Response to (W1)\nWe are already aware of the issue you have raised. FastLSH can handle sparse data, as discussed in Appendix C.6. We use the MNIST dataset to validate this claim, and the results are shown in Table 9 in Appendix, where MNIST is a sparse dataset with around 2.6% non-zero elements. As you have focused on, for very sparse vectors, we can use the Hadamard transform to make the data dense. This makes FastLSH and AChash similar, so FastLSH can be considered a generalized version of AChash. In many practical applications, data is often dense, in which case FastLSH can be used directly. For sparse vectors, we apply the Hadamard transform for data preprocessing. This enhances the adaptability of FastLSH compared to AChash, enabling faster index construction and better query performance.\n\n\n## Response to (W2)\nIt is worth noting that FastLSH and ACHash [1] do not necessarily reduce query time; their primary purpose is to speed up the hash evaluations. More importantly, LSH can be applied to other tasks beyond ANN search, such as outlier detection and neural network training. For these tasks, only $k \\times L$ hash tables need to be built, and distance computations are not required. Additionally, these tasks might require frequent creation or updating of hash tables, such that the execution time is mainly dominated by the hashing cost. In such applications, FastLSH significantly reduces the hashing cost, as shown in Figure 3, Table 1, Table 6 and Table 7 in Appendix C.3.\n\n[1] Dasgupta, Anirban, Ravi Kumar, and Tam\u00e1s Sarl\u00f3s. Fast locality-sensitive hashing. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. 2011.\n\n## Response to (W3)\nWe make a comprehensive analysis of $m$. The first result is illustrated in Theorem 4.6 and Corollary 4.7, where the asymptotic analysis of FastLSH indicates the equivalence between FastLSH and the classic LSH as $m$ approaches infinity. This analysis is aimed to show the relationship between our proposal and LSH from a theoretical perspective. \n\nIn practice, however, $m$ is expected to be a small number (less than $n$) to make FastLSH useful. In this case, the difference between FastLSH and LSH is controlled by $m$ and the variance in the squared distances of coordinates of a pair of data items, which is data-dependent and has nothing to do with $n$. \n\nOur second main result (Lemma 4.8 and Fact 4.9) quantitatively analyzed the difference and manifests that the gap between FastLSH and LSH can be captured by parameters $\\epsilon$ and $\\lambda$. In short, greater $m$ is, then smaller $\\epsilon$ and $\\lambda$ will be. Thus, by choosing appropriate $m$ ($m$ is set 30 in comparison with E2LSH), $\\epsilon$ and $\\lambda$ are small enough to reduce the impact of the variance in the squared distances of coordinates to a negligible level, which makes FastLSH and LSH are practically equivalent. Table 10 in Appendix C.8 illustrates the empirical evidences on how $m$ affects $\\epsilon$ and $\\lambda$ across 12 datasets we are experimented with.\n\nIn practice, the value of $m$ can be set in the range of $[30, \\frac{n}{2}]$. FastLSH provides the default $m$ settings, as shown in Parameter Settings in Appendix C.1, C.2, C.3 and C.4."
            }
        },
        {
            "title": {
                "value": "Commnet to Reviewer ze7N"
            },
            "comment": {
                "value": "Thanks for your feedback. Next we will address the main points you raised.\n\n## Difference Between FastLSH and ACHash\nTo answer the question raised, we need to first make it clear what the provable LSH property is. A formal definition can be found in Def. 2.1, and a somewhat informal description is that the collision probability should decrease with the distance between the given pair of vectors. Only with the property, we can enjoy nice features that LSH-style algorithms deliver [1].  \n\nFor ACHash [2], unfortunately, this property does not hold because it can only offer a JL-transformation-style lower/upper bound on the collision probability. To be precise, suppose the distance between vectors $x_1$ and $y_1$ is $s_1$ and the distance between vectors $x_2$ and $y_2$ is $s_2$ and $s_1$ < $s_2$, it is impossible for ACHash to say the collision probability $p(s_1)$ > $p(s_2)$ because it only has information about the loose lower/upper bound on the collision probability. In a nutshell, the JL-transformation-style lower/upper bound cannot deliver the LSH property. \n\nIn contrast to ACHash, FastLSH achieves this goal by deriving the exact collision probability DIRECTLY in Theorem 4.2, allowing one to calculate precisely the probability of collision for any pair of vectors with distance $s$. To deal with the new random variable $\\tilde{s}X$, we overcome quite a lot technical difficulty (Lemma 4.4 and Appendix A.2) to derive the PDF of $\\tilde{s}X$ in Eqn. (9). Figure 8 and Figure 9 in Appendix C.7 illustrate a comparison of the $\\rho$ curves, an important measure of the LSH property, for E2LSH [1] and FastLSH. These figures show that their $\\rho$ curves match well across different datasets, verifying that FastLSH and E2LSH have the same LSH performance. Note that it is impossible ACHash to plot such $\\rho$ curves.   \n\nOverall, AChash does not possess provable LSH property. Although FastLSH and AChash are similar in some aspects, the theoretical analysis methods of AChash are fundamentally different from those of FastLSH.\n\n[1] Datar, M., Immorlica, N., Indyk, P., and Mirrokni, V. S. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253\u2013262, 2004.\n\n[2] Dasgupta, Anirban, Ravi Kumar, and Tam\u00e1s Sarl\u00f3s. Fast locality-sensitive hashing. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. 2011.\n\n## FastLSH Handles Sparse Data\nWe are already aware of the issue you have raised. FastLSH can handle sparse data, as discussed in Appendix C.6. We use the MNIST dataset to validate this claim, and the results are shown in Table 9 in Appendix, where MNIST is a sparse dataset with around 2.6% non-zero elements. As you have focused on, for very sparse vectors, we can use the Hadamard transform to make the data dense. This makes FastLSH and AChash similar, so FastLSH can be considered a generalized version of AChash. In many practical applications, data is often dense, in which case FastLSH can be used directly. For sparse vectors, we apply the Hadamard transform for data preprocessing. This enhances the adaptability of FastLSH compared to AChash, enabling faster index construction and better query performance."
            }
        },
        {
            "title": {
                "value": "Comment to Reviewer XLCc"
            },
            "comment": {
                "value": "Thanks for your feedback. Next we will address the main points you raised.\n## Response to Weaknesses\nOur work focuses on using LSH-based method to accelerate end-to-end index construction, supporting machine learning tasks such as anomaly detection, neural network training, and ANN search. For LSH-based applications (e.g., the ACE method for anomaly detection [1]), its most significant advantages lie in low memory usage (required less than **4MB** memory) and fast query processing (reached up to **150x**), while maintaining comparable query accuracy to other advanced methods. We have chosen ACE as a SOTA method. Although many graph-based methods (e.g., HNSW [2]) achieve higher query accuracy in ANN search, they are not suitable for accelerating end-to-end index construction in LSH-based applications. Research has shown that when HNSW is used to speed up neural network training, its time consumption is **23** times higher than using LSH, and HNSW does not have guarantees for search performance [3]. For these LSH-based applications, a notable characteristic is that index construction time is more important than query processing, and the execution time is mainly dominated by the hashing cost, so that FastLSH significantly reduces the hashing cost, as shown in Figure 2, Table 1, Table 2 and Table 3. Furthermore, LSH is a fundamental component for high-dimensional ANN search, and we selected E2LSH [4] and MPLSH [5] as baselines because they are commonly used in practice. Our goal is to verify why FastLSH can improve query accuracy in anomaly detection and neural network training tasks, as shown in Figure 2, Table 1, Table 2 and Table 3. This is because FastLSH not only achieves comparable query accuracy to E2LSH and MPLSH but also significantly speeds up end-to-end index construction, as shown in Figure 3, Figure 4, Figure 6 and Table 9 in Appendix.\n\n[1] Luo, Chen, and Anshumali Shrivastava. Arrays of (locality-sensitive) count estimators (ace) anomaly detection on the edge. In Proceedings of the 2018 World Wide Web Conference. 2018.\n\n[2] Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. Fast approximate nearest neighbor search with the navigating spreading-out graph. arXiv preprint arXiv:1707.00143, 2017.\n\n[3] Chen, Beidi, et al. Mongoose: A learnable lsh framework for efficient neural network training. In Proceedings of International Conference on Learning Representations. 2020.\n\n[4] Datar, M., Immorlica, N., Indyk, P., and Mirrokni, V. S. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253\u2013262, 2004.\n\n[5] Qin Lv, William Josephson, Zhe Wang, Moses Charikar, and Kai Li. Multi-probe lsh: efficient indexing for high-dimensional similarity search. In Proceedings of the 33rd international confer ence on Very large data bases, pp. 950\u2013961, 2007."
            }
        },
        {
            "summary": {
                "value": "The paper focusses on making locality sensitive hashing (LSH) faster under the \\ell_2 metric. The standard LSH scheme involves taking an inner product of the query with a random vector and bucketing the query according to the obtained value. The paper instead proposes to speed up this operation by first subsampling m coordinates of the vector and computing the inner product with the corresponding subsampled vector. It is shown that as m tends to infinity, the probability of collision under the proposed scheme is same as the standard LSH. The paper also shows the superior performance of the proposed scheme empirically."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Locality sensitive hashing is used widely, so any effort in speeding it up is welcome as it can have huge practical significance."
            },
            "weaknesses": {
                "value": "Dasgupta et. al. [1] came up with a two-step proposal to speed up the standard LSH using fast Johnson\u2013Lindenstrauss transform. The LSH scheme proposed in this paper essentially removes the first step. However, this step is crucial especially when the dataset consists of sparse vectors. Thus, this paper seems to rediscover some of the ideas already present in [1], while missing the crucial ingredients.\n\nMore details: \nThe hash function proposed in [1] consists of two steps: (i) First multiply the query vector by a diagonal matrix with diagonal entries chosen to be 1 or -1 equiprobably. Then hit the vector obtained by a Hadamard matrix. (ii) Subsample roughly m coordinates of the resulting vector uniformly at random (without replacement), take the inner product of the resulting subsampled vector with a random gaussian vector and finally bucket the query according to the obtained value (sub sampling is actually done by choosing each coordinate with some fixed probability q = m/d). The first step is crucial when the vectors involved are sparse. In that case, most of the contribution to the \\ell_2 distance comes from very few non-zero coordinates. Therefore, for the subsampling to be effective, m will need to be very high, defeating the main purpose. The first step applies a norm-preserving rotation to the vectors, with the desirable property that the vector so obtained is dense, that is, no entry is too large with high probability.\n\nThe scheme proposed in this paper essentially applies the second step but where the subsampling is done with replacement. However, not applying the first step means m will need to be very large for sparse vectors. That is why the paper could only show asymptotic equivalence (for m going to infinity) between the proposed scheme and the standard LSH. In contrast, Dasgupta et.al. prove that the collision probability of their proposed scheme is close to the standard LSH for m = O(log d). \n\n[1] Dasgupta, Anirban, Ravi Kumar, and Tam\u00e1s Sarl\u00f3s. \"Fast locality-sensitive hashing.\" Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. 2011."
            },
            "questions": {
                "value": "Any clarification on points raised in the weaknesses section would be helpful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims at the efficiency of LSH methods while not harming its effectiveness. It reduces the cost of computing hashing functions by random sampling. The authors verify the effectiveness of their methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1. The method seems sound. \n\nS2. This paper studies important problems. \n\nS3. It is well written."
            },
            "weaknesses": {
                "value": "W1. The experiments focus on LSH based methods for ANNS, outlier detection. However, there are other methods such as proximity graphs for ANNS and OD. Besides, LSH based methods are not the SOTA for both of them. Even though LSH methods are enhanced, it does not really make a progress to ANNS and OD."
            },
            "questions": {
                "value": "Q1. I would see more experiments to demonstrate the the method in this paper outperform the SOTA method for ANNS and outlier detection."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces FastLSH, a novel locality-sensitive hashing scheme that combines random sampling and random projection to reduce the hashing complexity from $O(n)$ to $O(m)$, where $m$ is the number of samplings and $m<n$. \nFastLSH is claimed to preserve the LSH properties, i.e., the collision probability can be calculated like that in E2LSH. \nThe faster hash computations in FastLSH make it well-suited for tasks like anomaly detection, neural network training, and nearest neighbor search."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. FastLSH is simple, it can be easily implemented and can be seamlessly integrated into existing LSH-based applications.\n\n2. The paper provides rigorous theoretical proofs that FastLSH retains the desirable LSH properties."
            },
            "weaknesses": {
                "value": "1. The theoretical guarantees hold only when the number of sampled dimensions, $m$, approaches infinity. \nIn real-world applications, it is possible to construct dense data sets on which FastLSH may fail. For example, consider a data set in which a large proportion of dimensions are the same, with only a few being non-trivial. \nIn such cases, FastLSH is likely to miss the non-trivial dimensions during the sampling process and end up hashing all data points into the same bucket. This raises concerns about potential theoretical flaw in FastLSH. \nTherefore, it would be beneficial for the authors to demonstrate the effectiveness of FastLSH on relatively rare and challenging scenarios, as exemplified above.\n\n2. While FastLSH reduces the cost of hash function computations and index construction time, it does not speed up the query process itself. This limits its broader impact on applications where query speed is critical.\n\n3. The paper does not sufficiently explore the effect of varying the parameter $m$, the number of sampled dimensions, on both the efficiency and accuracy of FastLSH. \nSince $m$ plays a crucial role in balancing computational savings with hashing accuracy, understanding its impact across a range of values is essential. \nWithout a thorough parameter study, it remains unclear how to optimally set $m$ for different datasets or applications."
            },
            "questions": {
                "value": "1. How does FastLSH handle scenarios where only a few dimensions carry critical information, while others are redundant? Could you provide experiments on such challenging data sets? (W1)\n\n2. Moreover, is there a mechanism in FastLSH to adaptively select informative dimensions during sampling? There are existing methods that adaptively sample the dimensions based on their informativeness, with a non-uniform distribution. (W1)\n\n3. Could you include a parameter study showing how different values of $m$ affect performance across various data sets? (W3)\n\n4. Furthermore, could you provide guidelines or heuristics on how to choose $m$ for a given application? (W3)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new LSH method, named FastLSH, which aims to accelerate the indexing process of traditional LSH. The authors provide theoretical analysis to argue that their approach retains the fundamental LSH property: that the closer two points are, the higher the probability they will collide in the same hash bucket. The empirical contribution is presented through three groups of experiments, where FastLSH is applied in outlier detection, specialized neural network training (SLIDE) using LSH, and nearest neighbor search."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper presents a new approach, FastLSH, which introduces  a new improvement to the indexing process of canonical LSH. The idea of selectively sampling dimensions to accelerate indexing is well-motivated, and the authors attempt to show they retain the core LSH property through both theoretical analysis and empirical validation. They do extensive experiments. The paper is easy to understand."
            },
            "weaknesses": {
                "value": "**W1. Weak Justification of the Research Problem\u2019s Value**\n\nThe paper does not sufficiently establish the practical importance of accelerating the indexing process for LSH. Hashing-based methods are already among the fastest algorithms for indexing when compared to quantization-, tree-, and graph-based methods. Among these, LSH is known for fast indexing. From the three applications discussed (outlier detection, neural network training, and nearest neighbor search), it is not evident that further improving LSH indexing speed is a critical need.\n\nFor example, in the nearest neighbor search application, the authors report that indexing the GIST1M dataset (960-dimension, 1 million points) takes under 30 seconds\u2014which is already sufficiently fast for most applications. The paper\u2019s focus on indexing overlooks a more pressing issue: search efficiency, where LSH typically performs poorly compared to recent graph-based methods such as HNSW [1]. Improving search efficiency would address a more significant problem, which is why little effort in the literature is devoted to accelerating LSH indexing. While the SLIDE framework may benefit from faster indexing due to frequent re-indexing, it is a specialized case, and more examples are needed to justify the broader value of this work.\n\n**W2. Theoretical Flaws and Insufficient Justification**\n\nThe theoretical analysis contains several potential flaws and requires further rigorous justifications to support the authors' claims:\n\n**W2.1 Central Limit Theorem (CLT) Application:**\n\nIn Lemma 4.1, the authors apply the CLT, but the conditions for its use are not fully satisfied. CLT assumes i.i.d. samples, yet the proposed method ensures only that the selected dimensions are independently sampled. The elements within each dimension may not be i.i.d. since the authors assume only finite mean and variance for the distribution of  $(u_i-v_i)^2$. Since the underlying distribution is unknown, more justification is needed to ensure that the CLT applies. This step is crucial since it forms the foundation for the entire theoretical framework.\n\n**W2.2 Asymptotic Convergence of Characteristic Functions:**\n\nIn Theorem 4.6, the authors aim to demonstrate that the ratio of the characteristic functions of the original and transformed distributions converges to 1 as $m\\rightarrow \\infty $. However, the convergence depends on the behavior of the input to the characteristic function, x. The authors must show that the ratio of the characteristic functions converge for all values of x. The claim that $x^2\\leq O(m^{-1})$ is \u201cobvious\u201d is problematic, as no rigorous justification is provided. This is critical since the convergence ratio will diverge for non-zero x if this condition does not hold.\n\n**W2.3 Practicality of the Asymptotic Results:**\n\nEven if Theorem 4.6 holds, the requirement that $m\\rightarrow \\infty $ raises practical concerns. Since the authors propose using fewer dimensions (m < n), they must demonstrate that the asymptotic results still hold in practice. Specifically, the authors should quantify how far the transformed distribution deviates from the target distribution under finite m and provide a lower bound for the distribution distance under a suitable metric. Section 4.3 contains only heuristic arguments, making it unclear to what extent the LSH property is preserved after dimension sampling. A more rigorous analysis is required to confirm that the proposed method retains the LSH property, or at least an approximate version of it.\n\n**W3. Weaknesses in Experimental Design and Results**\n\nSeveral flaws in the experimental design limit the contribution of this paper:\n\n**W3.1 Application Scenarios Are Not Well-Aligned with Claims:**\n\nThe authors argue that their method is beneficial for scenarios requiring frequent re-indexing. However, the experiments do not reflect such settings. For example, no experiments are conducted on streaming data, which would be a more relevant use case. Moreover, the authors should compare their method to state-of-the-art approaches for high-dimensional data streams, such as [2][3].\n\n**W3.2 Incomplete Use of Standard Datasets:**\n\nThe authors use well-known datasets such as SIFT, Glove, and GIST, but they do not utilize all queries in these datasets. For instance, the SIFT dataset contains 10,000 queries, yet only 200 were used in the experiments. This is unusual, and the paper provides no justification for this choice. The authors should explain how the subset was selected and whether this affects the search performance.\n\n**W3.3 Diverse Speedup in Outlier Detection Task:**\n\nThe reported speedup of FastLSH over baseline methods varies significantly across datasets, especially in the outlier detection task. This raises concerns about whether the proposed method introduces distortions in the Hamming distances or requires dataset-specific hyper-parameter tuning. Either issue would limit the generality of the method and should be thoroughly investigated and reported."
            },
            "questions": {
                "value": "S1: Strengthen the theoretical analysis and address the problems in W2\nS2: Add suitable experiments and give proper analysis to demonstrate the strengths of this paper to reach ICLR standard.\nS3. Justify the research value of this problem with broad use cases."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}