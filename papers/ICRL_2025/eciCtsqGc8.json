{
    "id": "eciCtsqGc8",
    "title": "Interpretable Pre-Trained Transformers for Heart Time-Series Data",
    "abstract": "Interpretability of artificial intelligence models is vital in healthcare, as a poorly informed decision can directly impact the health and well-being of patients. This means that, owing to their black box nature, deep-learning solutions that may even yield high accuracy often fail to be adopted in real-world healthcare settings. To this end, we employ the generative pre-trained transformer (GPT) framework to clinical heart time-series data, to create two pre-trained general purpose cardiac models, termed PPG-PT and ECG-PT. We place a special emphasis on making both such pre-trained models fully interpretable. This is achieved firstly through aggregate attention maps which show that, in order to make predictions, the model focuses on similar points in previous cardiac cycles and gradually broadens its attention in deeper layers. Next, we show that tokens with the same value, which occur at different distinct points in the electrocardiography (ECG) and photoplethysmography (PPG) cycle, form separate clusters in a high dimensional space. Such clusters are formed according to the phase of the cardiac cycle, as the tokens propagate through the transformer blocks. Finally, we highlight that individual attention heads correspond to specific physiologically relevant features, such as the dicrotic notch in PPG and the P-wave in ECG. Importantly, it is also demonstrated that these pre-trained models are straightforward to fine-tune for tasks such as the classification of atrial fibrillation (AF), and beat detection in photoplethysmography. The so introduced PPG-PT and ECG-PT models achieve accuracy comparable to the state-of-the-art for both tasks, whilst crucially retaining their interpretability and explainability. This is demonstrated in the AF-screening fine-tuned model, with attention clearly shifting to regions in the context that are strongly indicative of atrial fibrillation.",
    "keywords": [
        "biosignals",
        "interpretability",
        "healthcare",
        "transformers"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We create two fully explainable and interpretable transformer models for PPG and ECG data, that are pre-trained to predict the next sample and can be easily fine-tuned for downstream tasks.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=eciCtsqGc8",
    "pdf_link": "https://openreview.net/pdf?id=eciCtsqGc8",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces two interpretable pre-trained transformer models (PPG-PT and ECG-PT) adapted from the GPT framework for analyzing heart time-series data from PPG and ECG signals.  The authors demonstrate that these models develop interpretable attention patterns corresponding to physiologically meaningful cardiac cycle features."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Interpretability analysis on physiological time series\n- The model was tested on two types of physiological time series: PPG and single-lead ECG."
            },
            "weaknesses": {
                "value": "1. The model's novelty is questionable, as the PPG-GPT has previously been explored across multiple tasks (Chen et al., 2024). It will be helpful to specify how the current work differs from it.\n2. The manuscript does not compare models on different PPG or ECG tasks compared to previous PPG-GPT (Chen et al., 2024). The comparison with one baseline per PPG and ECG is mentioned only in the appendix. Importantly, the models are compared on performance using different setups.\n3. The experiments have not been performed across multiple runs, so the variability (std or IQR) of the performance is not clear. Consider 5-fold or 5 seeds.\n4. Consider additional metrics, for example, sensitivity, specificity, false-positive rate, false-negative rate, and F1 score for AF.\n5. The majority of the paper focuses on interpretability. However, it is difficult for the machine-learning community to validate clinical utility claims that require experience in reading ECG or PPG. Have you collaborated with clinical experts to validate the interpretability claims? Could you provide more evidence that the examples you have shown guarantee the presence of AF?\n\n- Chen, Zhaoliang, et al. \"Adapting a Generative Pretrained Transformer Achieves SOTA Performance in Assessing Diverse Physiological Functions Using Only Photoplethysmography Signals: A GPT-PPG Approach.\" AAAI 2024 Spring Symposium on Clinical Foundation Models. 2024."
            },
            "questions": {
                "value": "1: You have considered very clean datasets and samples; seeing the behavior across different noisy scenarios would be highly important. Since we could achieve very high performance on high-quality data with an F1 score of 0.96 with DeepBeat (Torres-Soto et al., 2020).\n\n2: Attention maps as an interpretability tool have been successfully explored (Zhao et al., 2023). It needs to be clarified that you did something new methodologically; it seems more of an application. For example, you have details on using the `findpeaks` function in Matlab, but it is poorly structured overall. It would be great if you would summarize the methodology as a pseudocode and suggest choices.\n\n- Zhao, Haiyan, et al. \"Explainability for Large Language Models: A Survey.\" arXiv preprint arXiv:2309.01029 (2023).\n- Torres-Soto, Jessica, and Euan A. Ashley. \"Multi-task deep learning for cardiac rhythm detection in wearable devices.\" NPJ digital medicine 3.1 (2020): 116."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This manuscript presents a novel approach to representation learning for heart time-series data (typically PPG and ECG in this work) using generative pre-trained transformers (GPTs). The idea comes from the NLP domain where GPTs have shown remarkable capabilities in learning representations from text data, and the fact that the (pre-)training process for GPTs is very simple, only next-token prediction, also motivated this work, as claimed by the authors. The authors conducted a very detailed attention mechanism analysis to interpret the learned representations of the pre-trained GPTs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. To the best of my knowledge, this is the first work that applies generative pre-training to representation learning for heart time-series data, where previously contrastive learning methods were the most popular (also inherited from other domains like voice recognition). This is a novel and interesting idea.\n\n2. The attention mechanism analysis is very detailed and provides a clear understanding of the learned representations, and how the transformer layers work on the heart time-series data."
            },
            "weaknesses": {
                "value": "1. The datasets used in the experiments are not comprehensive enough. The authors used the CinC2020 dataset, which is superseded by the CinC2021 dataset. The latter dataset is more comprehensive and contains more data. Moreover, there are other larger datasets available, such as the CODE-15% dataset (https://zenodo.org/records/4916206), etc.\n\n2. The authors did not compare their method in their numerical experiments with other representation learning methods, such as contrastive learning-based methods (for example CLOCS (https://proceedings.mlr.press/v139/kiyasseh21a/kiyasseh21a.pdf)), to show the effectiveness of their method.\n\n3. Not enough downstream tasks are conducted to evaluate the learned representations (or the pre-trained GPTs)."
            },
            "questions": {
                "value": "See the \"Weaknesses\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper aims to use generative pre-trained transformer (GPT) framework to predict next token for ECG and PPG signals with a focus in interpreting the decision-making of these large language models. This was done through observing the behavior of the attention heads in different transformer layers and aggregating them. The experiments reveal that the next token generation looks cycles in the nearest past more than that of the farthest past in the context. Qualitative interpretation reveals the ECG and PPG GPT models look into important signal components. These GPT models were fine tuned for a downstream task of atrial fibrillation classification with interpretation highlighting irregular occurrence of a beat w.r.t. the previous beat which is interesting. Overall, the problem formulation is logical, methods are well designed and explained, and the discussion clearly articulates the explainability the GPT models can offer."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Related GPT based interpretation studies for ECG and PPG exists in literature, but the experimentation carried out in this study is rigour and systematic in revealing the explanations for firstly, the generation of next token and finally, extend the idea for a downstream classification task.\n\n- The idea of decoding attention head for explanations are common in the CV and NLP domain, but extending it to physiological time-series can be seen a contribution.\n\n- Data split follows a subject-wise separation to avoid data leak in training and testing."
            },
            "weaknesses": {
                "value": "Major:\n- The explainability method of GPT models was shown to focus on previous cycles meaning that it can observe a beat w.r.t the previous one which is why the downstream task interprets well where distance between two consecutive beat is important criteria. However, this might not be the case for other common tasks such as sleep staging where the input in 30 second ECG or PPG signal and separating sleep stages such as wake, light sleep, deep sleep and REM sleep manifests from HR variability. Another experimentation for a downstream task would be interesting to see if the proposed explainability idea fits well to provide clinical domain specific interpretation, given the fact that GPT models were found to be useful in these downstream tasks.\n\n- Few related GPT interpretability studies were referenced. More references should be included to have a broad picture based on GPT interpretablity. For example, \na. Creswell, A., Shanahan, M., & Higgins, I. (2022). Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712.\nb. Ben Melech Stan, G., Aflalo, E., Rohekar, R. Y., Bhiwandiwalla, A., Tseng, S. Y., Olson, M. L., ... & Lal, V. (2024). LVLM-Intrepret: an interpretability tool for large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8182-8187).\n\nMinor:\n- Figure 3 captions needs to be corrected a-d."
            },
            "questions": {
                "value": "- 10 second segments were extracted from recordings from where tokens were generated, are these tokens represent whole segment data or beats were isolated to form tokens?\n\n- The prediction or generation of tokens from GPT models is unclear. Experiment says a single token is generated for a given context length, then it needs to be clearly mentioned how the generation process continued to generate to produce, say Figure 1, of 15 second PPG or ECG data which was compared against the original signal?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}