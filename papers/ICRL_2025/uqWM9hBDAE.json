{
    "id": "uqWM9hBDAE",
    "title": "How Much is Unseen Depends Chiefly on Information About the Seen",
    "abstract": "We find that *in expectation* the missing mass, i.e., the proportion of data points in an unknown population---that belong to classes that *do not* appear in the training data---is entirely determined by the number $f_k$ of classes that *do* appear in the training data the same number of times and an exponentially decaying error.\nWhile this is the first precise characterization of the expected missing mass in terms of the sample, the induced estimator suffers from an impractically high variance. However, our theory suggests a large search space of nearly unbiased estimators that can be searched effectively and efficiently. Hence, we cast distribution-free estimation as an optimization problem to find a distribution-specific estimator with a minimized mean-squared error (MSE), given only the sample. In our experiments, our search algorithm discovers estimators that have a substantially smaller MSE than the state-of-the-art Good-Turing estimator. This holds for over 93\\% of runs when there are at least as many samples as classes. Our estimators' MSE is roughly 80\\% of the Good-Turing estimator's.",
    "keywords": [
        "Good-Turing frequency estimation",
        "Multinomial probability estimation",
        "Unseen events",
        "Missing mass",
        "Probability mass"
    ],
    "primary_area": "learning theory",
    "TLDR": "Given a random sample from an unknown multinomial distribution, we develop a distribution-free methodology to generate a distribution-specific estimator for the proportion of data that belong to classes not observed in the training sample.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=uqWM9hBDAE",
    "pdf_link": "https://openreview.net/pdf?id=uqWM9hBDAE",
    "comments": [
        {
            "summary": {
                "value": "The paper makes a contribution to the estimation of the missing mass probability by providing a distribution free estimator that minimizes the mean-squared error after formalizing it as a constrained optimization problem. The authors provide a genetic algorithm that numerically solves the program. They use synthetic and real data experiments on various distributions to compare the resulting estimator to the classically used Good and Turing estimator."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper clearly states the problem formulation (fitness function and search space) and proposes a numerical algorithm to solve it.\n- The resulting estimator shows a strong performance in terms of MSE and is relatively cheap to compute.\n- The presentation and the figures are very clear and well-explained."
            },
            "weaknesses": {
                "value": "- The empirical application is $\\textbf{insufficient}$, only one real world dataset was used and the experiments were done only on 50 datapoints. \n- No $\\textbf{theoretical guarantees}$ were given for the minimal MSE estimator of the missing mass probability. \n- Comparing the proposed genetic Aagorithm to some other optimization approaches  as well as studying the properties of the resulting estimators would be helpful."
            },
            "questions": {
                "value": "- Are there any theoretical guarantees in terms of MSE when running the optimization on a smaller sample size to compute the estimator for a larger size using the provided formula ?\n\n- How does the proposed minimum MSE estimator perform compared to a distribution-aware estimator for different distributions?\n\n- How does the effectiveness of the proposed estimator look like for strictly positive values of $k$?\n\n- Given a fixed dataset, what is the variance of the resulting estimator's MSE output by the GA (given randomness in mutations)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of estimating the expected value of  $M_k$ , which represents the probability that the (n + 1)th observation is an element observed exactly $k$  times in the training data. By precisely characterizing the dependencies among frequency counts, the authors provide a detailed decomposition of $ \\mathbb{E} [M_k] $. They introduce a class of estimators that can be constructed using algorithm-based optimization. These proposed estimators outperform the well-known Good-Turing estimator in terms of accuracy."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper is significant, since estimating the missing mass is a classic and fundamental problem in statistics with broad practical applications. Making advances over the widely used Good-Turing estimator is important, and such progress has the potential to bring substantial empirical improvements. This paper is also novel in its approach: their analysis do not rely on Poisson approximation, which allows for deeper and more flexible analysis. Additionally, the paper presents a thorough evaluation of the proposed algorithms, including theoretical insights and synthetic experiments that demonstrate the minimal-bias estimator\u2019s substantially lower bias compared to the Good-Turing estimator across various distributions."
            },
            "weaknesses": {
                "value": "No obvious weakness is found."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the problem of estimating the probability of having test points that do not appear in the training data. A class of estimators is provided, and the method for finding an optimal estimator is discussed."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-written: the setting discussed in this work, as well as the proposed methodology, is clearly stated, and relevant questions are answered."
            },
            "weaknesses": {
                "value": "I don't feel that the motivation for and usefulness of the problem are sufficiently explained."
            },
            "questions": {
                "value": "1. As stated in the 'weakness', could the authors provide more explanations for why this is an important problem? When is it helpful to know in advance the probability of a test point being equal to one of the training/calibration data points?\n\n2. Specifically, what do the proposed method (or existing methods) advise practitioners to do with their datasets? For example, if one is given the training and calibration data along with test feature inputs, and the goal is to predict the test outcome, there can be various procedures with different approaches---how does the problem/method in this work affect the overall procedure? If we know that the test point is unlikely to be equal to (or drawn from the same distribution as) one of the observed points, what can we do with that information?\n\n3. I'm not sure if it is appropriate to call the proposed method a 'distribution-free' method. According to the footnote, the context in this work is that the method 'does not impose assumptions on the parameters $p$ or $n$,' but this seems more like a standard setting for a parametric (multinomial) model, and is quite different from the usual meaning of 'distribution-free' (such as in conformal prediction context)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}