{
    "id": "UG62fwmOxZ",
    "title": "Fully Quanvolutional Networks for Time Series Classification",
    "abstract": "Quanvolutional neural networks have shown promise in areas like computer vision and time series analysis. However, their applicability to multi-dimensional and diverse data types remains underexplored in the literature. Existing models are predominantly hybrid, with minimal quantum involvement and a heavy reliance on classical layers. To address previous shortcomings related to scalability and data encoding inefficiencies, we introduce a new quanvolution algorithm. Focusing on time series data, we propose the Quanv1D layer, which is trainable, modular, and capable of handling time series of arbitrary dimensions. To minimize qubit usage, we employ amplitude embedding (which requires only $log_2n$ qubits) instead of linear mapping methods like angle embedding, which demands an impractically large number of qubits even for simulations. In addition to this new layer, we present a new architecture called Fully Quanvolutional Networks (FQN), composed entirely of Quanv1D layers. We tested this lightweight model on 12 UEA time series classification datasets and compared it against both quantum and classical models, including the current state-of-the-art ModernTCN. On most datasets, FQN achieved near state-of-the-art accuracy and even outperformed the compared models on some, all while using a fraction of parameters. Moreover, we observed that FQN exhibits a self-regularizing property, leading to improved training performance.",
    "keywords": [
        "quanvolutional neural networks",
        "quanvolution",
        "time series classification"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=UG62fwmOxZ",
    "pdf_link": "https://openreview.net/pdf?id=UG62fwmOxZ",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes Fully Quanvolutional Networks (FQN) for time series classification, aiming to address limitations of previous quantum-classical hybrid models by developing an architecture composed entirely of quantum- inspired layers. The contributions of this paper include introducing the Quanv1D layer and utilizing Amplitude Embedding to reduce the number of required qubits for processing multi-dimensional data efficiently. Experimental results on 12 UEA time series datasets demonstrate that FQN achieves comparable or superior performance to existing models, including ModernTCN, with significantly fewer parameters."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The manuscript effectively demonstrates the necessity of quantum operations and amplitude embedding, particularly through a self-regularization perspective, by analyzing gradient values.\n- The manuscript thoroughly evaluates the proposed method against a comprehensive set of baseline models, including both quantum and classical approaches, and notably the current state-of-the-art, ModernTCN."
            },
            "weaknesses": {
                "value": "- The novelty of the proposed method appears somewhat incremental, as it primarily focuses on adapting amplitude embedding to the time series domain for improved scalability.\n- While the authors provide a self-regularization perspective to explain the generalizability of the fully convolutional structure, additional analysis or insights into why this approach outperforms other quantum models are limited."
            },
            "questions": {
                "value": "- Could the authors provide insights into why FQN performs better than ModernTCN or QuanvNet?\n- In some cases, FCN outperforms FQN across the 12 datasets. Beyond the similarity between the training and test sets, are there specific characteristic(s) of the datasets where FQN consistently excels?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "## Summary of the Paper's Contribution\nThe paper introduces a novel quantum convolutional layer, Quanv1D, and a fully quantum convolutional network (Fully Quanvolutional Networks, FQN), for time series classification tasks. By using amplitude embedding, the method reduces qubit requirements and demonstrates competitive experimental results on 12 time series classification datasets. This work highlights the potential of quantum models in time series classification."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "## Strengths:\n1.\t**Innovation**: The Quanv1D layer and FQN architecture broaden the applicability of quantum convolutional networks, offering a fresh perspective for combining quantum computing and time series analysis.\n2.\t**Comprehensive Experimental Design**: The experiments cover multiple time series classification datasets, demonstrating FQN\u2019s comparable or superior performance to current state-of-the-art models on various datasets.\n3.\t**Transparent Limitations Analysis**: The paper thoroughly discusses the gap between the model\u2019s theoretical framework and hardware implementation, especially addressing the quantum hardware limitations in the NISQ era."
            },
            "weaknesses": {
                "value": "## Weaknesses:\n1.\t**Limited Real Hardware Applicability**: As the experiments were all conducted in simulation, there is currently a lack of data on FQN\u2019s performance on real quantum hardware, which limits its feasibility for practical applications.\n2.\t**Limited Task Scope**: FQN has been validated only on classification tasks, with no exploration of applications like time series forecasting or imputation.\n3.\t**Resource Demands Remain High**: Although amplitude embedding reduces qubit requirements, circuit depth still increases rapidly with data dimensions, potentially impacting scalability."
            },
            "questions": {
                "value": "## Questions for the Authors\n1.\tIn future real-hardware tests, is there a plan to optimize amplitude embedding to mitigate rapid circuit depth growth?\n2.\tHas there been any validation of FQN\u2019s applicability to other time series tasks (e.g., forecasting or data imputation)? If so, would these tasks require architecture adjustments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel layer based on quantum computation, and defines a model for time series classification that, on average, surpass previous models. Overall, the idea may sound but the paper requires a refactoring in terms of writing, style, depth of investigation and discussion, and motivation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The proposed method achieves interesting results also with respect to ModernTCN.\n2) The motivation and the advantages with respect to QuanvNet are clear."
            },
            "weaknesses": {
                "value": "1) Writing needs huge improvements. The authors begin both the abstract and the introduction talking about quanvolutional stuff without explaining what they are or the reason to use them. Without a proper introduction, it is arduous to understand the novelty of this paper. Overall, the paper seems a coding report.\n2) Equation (1) does not have any non-quantum comparison so it is difficult to understand the differences.\n3) Experiments focus on time series classification, while the method needs a broader discussion on the implications and possible applications.\n4) I guess more references exist together with QuanvNet, and a deeper investigation is required.\n\nMinor comments:\n\nThe Saxon genitive should be avoided in scientific writing, although I know that both ChatGPT and Grammarly suggest using it. However, I suggest the authors remove all the Saxon genitives from the paper."
            },
            "questions": {
                "value": "1) As the authors correctly point out in 3.5, the proposed method avoids overfitting while the counterpart FCN overfits. Together with the demonstration that FQN naturally acts as a regularization technique, some experiments should be performed: it would important to prove it by defining the FCN model with the same number of paramaters of FQN to show that the overfitting is not due to the higher number of params."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, a new method built upon the quanvolutional operator, which is derived by **quantum circuits in place of standard convolutional filters**, is proposed. To overcome the exclusive reliance of quanvolutional neural networks on classical layers, as well as their limited application to multi-dimensional data with potential scalability issues, authors propose a novel **Quanv1D layer** for time series data, which acts as the main building block in a lightweight architecture, the so-called Fully Quanvolutional Network (FQN). For increased efficiency, they also propose the incorporation of *amplitude embedding for data encoding* that enables minimal qubit usage. The proposed Quanv1D layers are stacked with *increased dilation rates* to enable larger receptive fields, followed by a final projection layer. Authors evaluate FQN architecture in time series classification, achieving competitive performance with standard CNN-based architectures and other quanvolutional-based architectures while remaining significantly more efficient in terms of memory."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Important strong aspects of the proposed method and study are the following:\n1. Authors tackle the very promising and *relatively underexplored* methodological field of quantum machine learning.\n2. They propose a building block that combines ideas from *1D convolutional layers and quantum circuits* (Quanv1D layer) and design it specifically for **multi-dimensional time series** data.\n3. Importantly, the proposed Quanv1D layer adopts the *standard hyperparameters of 1D convolution*, such as kernel size, stride, dilation, and padding, enabling their straightforward application.\n4. Their proposed architecture, built upon an efficient amplitude embedding layer and stacked Quanv1D layers, remains *light in terms of parameters while achieving competitive performance* in few time series classification datasets. Additionally, while relying solely on quantum layers, it showcases their promising application, which is not affected by other standard neural networks."
            },
            "weaknesses": {
                "value": "Significant weaknesses of the presented work are summarized as follows:\n1. **Presentation of Related Works:** Related works in quantum-based architectures are not thoroughly presented as preliminaries for the proposed method. It is not clear if the main design of the proposed quanvolutional layer significantly expands previous designs beyond the incorporation of user-friendly hyperparameters. This raises doubts about the novelty of the proposed method, making it mostly limited to the selection of the embedding layer.\n2. **Experimental Design:** The experimental setup differs from common baselines in the time series classification field. Specifically, the 12 datasets from UEA used by the authors are totally different from the ones used in recent studies [1,2] and commonly used benchmarks [3]. Additionally, the comparisons are limited to two CNN-based and one quant-based network, excluding several common baselines in the field. These choices raise questions about the generalization performance of the proposed method beyond the selected datasets/baselines.\n3. **Applicability to Real Hardware and Impact:** The performance achieved by the proposed FQN model in time series classification is, in several cases, inferior to CNN-based baselines. Yet, the number of parameters used by FQN is significantly lower, which makes it computationally attractive and promising for large-scale applications. On the other hand, as mentioned by the authors, the proposed method remains a theoretical framework tested on conventional computers, which questions the potential impact of this contribution if quantum computations are enabled. This is further confirmed by the presented simulation of FQN on finite shots, where the performance was not matched for most datasets to the one achieved with analytical expectation values.\n\n[1] Luo, D., & Wang, X. (2024). Moderntcn: A modern pure convolution structure for general time series analysis. In The Twelfth International Conference on Learning Representations.\n\n[2] Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., & Long, M. (2022). Timesnet: Temporal 2d-variation modeling for general time series analysis. arXiv preprint arXiv:2210.02186.\n\n[3] https://github.com/thuml/Time-Series-Library"
            },
            "questions": {
                "value": "- **Q1 - Datasets:** Based on weakness (2), could the authors explain the selection of this subset of UEA, which differs from the standard 10 preferred datasets in most studies? Have you conducted experiments on the whole data repository? Similarly, most studies in time series classification first evaluate methods on the univariate UCR archive [1]. \n- **Q2 - Experimental Evaluation:** Incorporating additional baselines or tasks could further support performance comparisons in favor of your proposed architecture. Please justify your choices if this is not possible.\n- **Q3 - Related Works as Preliminaries:** To better position your contribution in the field of quantum machine learning, it would be essential to clarify better where the technical design of Quanv1D differs from previous quanvolutional layers (weakness (1)). For instance, you can use relevant notations from the literature as a \u201cpreliminaries\u201d section in the method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel Quanvolutional Networks (FQN) for time series classification. The key innovation is the Quanv1D layer, which uses quantum circuits instead of classical convolutional filters to process time series data. The model employs amplitude embedding to encode classical data into quantum states efficiently, requiring fewer qubits than previous approaches. The authors tested FQN on 12 UEA time series datasets and is vaildated against other baseline models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The Quanv1D layer design is novel and can handle arbitrary input dimensions, and the paper proves the potential of quantum operations on time series problems \n\n2. The newly design architecture is parameter efficient (2-7x fewer parameters than classical counterparts as the author claimed)\n\n3. Comparable performance against models with much more parameters"
            },
            "weaknesses": {
                "value": "1.\tMy major concern is experiments. The tasks used in the paper contains 12 datasets out of UEA classification archive which consists of 30 tasks. As it is commonly known that the performance gap among time series models can greatly vary among different datasets, there could be potential dataset set selection bias in this paper. And no explicit explanation of why these specific 12 datasets were chosen. \n\nBesides, the only \u2018strong\u2019 baseline is from ModernTCN, which happens to use 10 datasets from UEA 30 (check Appendix A.3 in https://openreview.net/pdf?id=vpJMJerXHU). I\u2019m not commenting on that paper, but unfortunately there\u2019s only 1 dataset (SelfRegulationSCP2) overlapped between the two papers, the other datasets are different.\n\nIn this case, as a fair comparison, I suggest the authors to introduce more SOTA time series classification models (Omni-scale cnn, TimesNet), and show the complete experiment results over all 30 UEA datasets or provide a clear explanation of why specific datasets were included/excluded.\n\n2.\tThe motivation and introduction of quantum operations can be further clarified. \n\nFirstly, for general audience without quantum computing background, it\u2019s better to give some introduction about the basics in the appendix. (i.e. computational basis state in equation 2, definition of shots) along with the benefit of using potential quantum computing. \n\nSecondly, can you clarify the motivation of the proposed method? Do you try to deploy the whole network to quantum computer sometime in the long future or the work is inspired by quantum operations to improve parameter efficiency on classical computer? If the former, I\u2019d like to understand if the other components(activation/matrix operation)of the NN are also quantum operator compatible, and how much it can speed up the algorithm. If the latter, I\u2019d like know by saving the parameter number, how much numerical accuracy does it lose/computational efficiency can it gain?"
            },
            "questions": {
                "value": "1.\tBy simulating the quantum operation on classical computers, what is the computational overhead of simulating these quantum circuits compared to classical convolutions? And do you have quantitative comparison of computational time to classical time series models?\n\n2.\tCan you give a comparison on the numerical performance between the quantum operation vs classical operations in the ideal scenario? For instance, regarding the amplitude embedding, does n qubits lose information or will it behave probabilistically?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}