{
    "id": "8eNLKk5by4",
    "title": "Optimal Strong Regret and Violation in Constrained MDPs via Policy Optimization",
    "abstract": "We study online learning in constrained MDPs (CMDPs), focusing on the goal of attaining sublinear strong regret and strong cumulative constraint violation. Differently from their standard (weak) counterparts, these metrics do not allow negative terms to compensate positive ones, raising considerable additional challenges. Efroni et al. (2020) were the first to propose an algorithm with sublinear strong regret and strong violation, by exploiting linear programming. Thus, their algorithm is highly inefficient, leaving as an open problem achieving sublinear bounds by means of policy optimization methods, which are much more efficient in practice. Very recently, Muller et al. (2024) have partially addressed this problem by proposing a policy optimization method that allows to attain $\\widetilde{\\mathcal{O}}(T^{0.93})$ strong regret/violation. This still leaves open the question of whether optimal bounds are achievable by using an approach of this kind. We answer such a question affirmatively, by providing an efficient policy optimization algorithm with $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ strong regret/violation. Our algorithm implements a primal-dual scheme that employs a state-of-the-art policy optimization approach for adversarial (unconstrained) MDPs as primal algorithm, and a UCB-like update for dual variables.",
    "keywords": [
        "CMDP",
        "strong regret",
        "strong violations",
        "primal-dual"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8eNLKk5by4",
    "pdf_link": "https://openreview.net/pdf?id=8eNLKk5by4",
    "comments": [
        {
            "comment": {
                "value": "> This paper's algorithm and regret bound rely on a problem-dependent factor $\\rho$, which could be small and lead to worse regret. Could the authors provide more technical reasons why $\\rho$ is required in this paper? Does this factor also appear in previous papers?\n\nWe thank the Reviewer for the opportunity to clarify this fundamental aspect. The dependence on $1/\\rho$ is standard in primal-dual formulations (all the works that are mainly related to our, have this kind of dependence, see e.g., [Efroni et al 2020] and [M\u00fcller et al. 2024]). Intuitively, it happens since the optimal Lagrange variable of the offline problem is of the order $1/\\rho$, and, the magnitude of the Lagrangian variables appears in the theoretical bound of primal-dual procedure.\n    To better understand this, notice that any regret minimizer scales at least linearly in its payoffs range. Thus, since the payoffs range of the primal depends on the maximum of Lagrangian variable, this dependence appears in theoretical bounds.\n    \nNonetheless, differently from [M\u00fcller et al 2024], we avoid the $1/\\rho$ dependence in the violation bound, while keeping it in the regret only. We believe that this additional result is of particular interest for the community.\n\nSince this is a crucial aspect of our work, please let us know if further discussion is needed.\n\n> This paper does not have an empirical comparison. Although this is typically not necessary for a theoretical paper, simulation results like Muller et al. [2024] could be helpful.\n\nWe agree that experiments are always beneficial; nevertheless, we underline that in the online CMDPs literature, many works do not have experimental results (e.g., Efroni et al. (2020)).\n\n> A conclusion and discussion section is lacking.\n\nWe thank the Reviewer for the suggestion. We will surely include a conclusion in the final version of the paper. \n\n> Is there any regret lower bound in this setting that is related to the number of constraints $m$?\n\nWe are not aware on any lower-bound related to the number of constraints in our setting. Nevertheless, to the best of our knowledge, there are not works which avoid the linear dependence on it, as in our work."
            }
        },
        {
            "comment": {
                "value": "> The paper missed a few important related references (e.g., [1] and [2]), where the strong violation has been investigated and better results than this paper have been established. In [1], the OptPess-LP algorithm can satisfy the constraints instantaneously, which seems better than $O(\\sqrt{T})$ strong violation. In [2], the paper proposed a model-free method to achieve $O(\\sqrt{T})$ strong violation. It would be better to discuss these papers in detail and highlight the differences.\n\nWe thank the Reviewer for the suggestion. Indeed, we will include both the papers mentioned by the Reviewer in the final version of the paper, including the following discussion.\n\nSpecifically, while [1] studies CMDPs, their **OptPess-LP** algorithm **assumes the knowledge of a strictly feasible solution**. This assumption is non-practical in many real-world scenarios where the constraints are not known. Moreover, we remark that their approach is LP-based and not primal-dual, neither policy-optimization based.\n\nAs concerns [2], as pointed out in [M\u00fcller et al, 2024], the algorithm proposed by the authors achieves $\\widetilde{O}(\\sqrt{T})$ strong violation when allowed to take $\\Omega(d^{L-1}T^{1.5L}\\log(|A|)^L)$ computational steps in every episode. Differently, in our work, as in [M\u00fcller et al, 2024], **we focus on polynomial-time algorithms that achieve a strong regret and violation guarantee**.\n\nSince these aspects are crucial, please let us know if further discussion is needed.\n\n> The algorithm requires Slater's condition and the knowledge of Slater's constant $\\rho$, which is usually not practical in most critical applications. Besides, the regret is in the order of $O(1/\\rho),$ it could be problematic when $\\rho$ is close to zero.\n\nAs concerns the Slater's condition assumption and the knowledge of $\\rho$, these requirements are standard for primal-dual methods. Indeed, both [Efroni et al. 2020] -- for the primal-dual algorithm -- and [M\u00fcller et al. 2024] make the aforementioned assumptions. Intuitively, the Slater's condition is necessary, since, otherwise, the otpimal Lagrangian variable could be unbounded, preventing any kind of regret guarantees for primal-dual methods. As concerns the knowledge of $\\rho$, this can be easily replaced by any lower-bound on $\\rho$, or estimated in an online fashion adding a preliminary estimation phase (see Castiglioni et al. 2022, \"A Unifying Framework for Online Optimization with Long-Term Constraints\").\n\nA similar reasoning also holds for the dependence on $O(1/\\rho)$. We remark that this dependence is standard for primal-dual methods (see [Efroni et al. 2020], [M\u00fcller et al 2024]). Intuitively, it happens since the optimal Lagrange variable of the offline problem is in general of the order $1/\\rho$, and, the magnitude of the Lagrangian variables appears in the theoretical bound of primal-dual procedure. Nonetheless, please notice that, **differently from [M\u00fcller et al 2024], we avoid the $1/\\rho$ dependence in the violation bound**. We believe that this additional result is of particular interest for the community.\n\n> I understand it is a theory paper; however, including numerical experiments to validate the proposed algorithm would be beneficial. For example, the baselines could be Efroni et al. (2020), [1] and [2].\n\nWe agree that experiments are always beneficial; nevertheless, we underline that in the online CMDPs literature, many works do not have experimental results (e.g., Efroni et al. (2020))."
            }
        },
        {
            "comment": {
                "value": "> How large the margin $\\rho$ is? What is the practical implication when it is infinitely small?\n\n$\\rho\\in[0,L]$. When $\\rho$ is arbitrary it may worsen the regret bound of our algorithm. Nonetheless, it is fundamental to take into account two crucial aspects. First, the dependence on $\\rho$ for both regret and violation is standard in primal-dual formulations (all the works that are mainly related to our have this kind of dependence, see, e.g., [Efroni et al 2020] and [M\u00fcller et al. 2024]). Intuitively, it happens since the optimal Lagrange variable of the offline problem is in general of the order $1/\\rho$, and, the magnitude of the Lagrangian variables appears in the theoretical bound of primal dual-procedure. Second, differently from [M\u00fcller et al 2024], we avoid the $1/\\rho$ dependence in the violation bound. We believe that this additional result is of particular interest for the community.\n\n> Is it efficient to run the adversarial policy optimization oracle?\n\nWe thank the Reviewer for the opportunity to clarify this aspect. The efficiency is one of the key advantages of our adversarial policy optimization procedure. Indeed, the update can be performed by employing dynamic programming techniques. Being a policy optimization approach allows to avoid the employment of occupancy-measure based methods which require projections to be performed at each episode (which, on the contrary, are highly inefficient). We refer to [Luo et al., 2021. \"Policy Optimization in Adversarial MDPs: Improved Exploration via Dilated Bonuses\"] for further details.\n\n> Can the authors point out the new analysis that avoids the oscillation issue in typical primal-dual methods or compare their key analysis ideas?\n\nPlease refer to the second answer."
            }
        },
        {
            "comment": {
                "value": "> The authors focus on the basic tabular case of constrained MDPs. This method needs further generalization to extend beyond the tabular case.\n\nWe agree with the Reviewer that extending our results to non-tabular MDPs is an interesting future work. Nevertheless, we underline that, since the problem tackled by this work has been originally raised by [Efroni et al. (2020)], no better results than $\\widetilde{\\mathcal{O}}(T^{0.93})$ regret and violation have be shown even in the tabular setting. Thus, we believe that our result (which improves the bounds to $\\widetilde{\\mathcal{O}}(\\sqrt{T})$) is still of fundamental importance for the community.\n\n>It would be helpful if the authors could clarify the motivation behind the techniques used in the proposed algorithm. Notably, the standard primal-dual policy optimization suffers the oscillation issues, potentially causing linear strong regret and constraint violation.\n\nWe thank the Reviewer for the precious observation and for the opportunity to clarify this aspect of our work. The main advantage of our algorithm compared to existing primal-dual methods is that we somehow fix the Lagrangian variable depending on the policy chosen by the primal algorithm. To better understand this aspect, imagine a Markov decision process where the reward, the constraints and the transitions are fixed and known to the learner. Thus, we run a primal-dual procedure which does not employ any upper/lower confidence bound on the unknown variables (since all of them are known). In such a setting, standard primal-dual methods work by iteratively playing a Lagrangian game between the primal variable (the policy) and the dual ones: This leads to instability since no-regret adversarial procedures (employed for both the primal and the dual) would cycle around the equilibrium (namely, the optimal solution), as known from many results in equilibrium computation theory (see, e.g.,  \"Prediction, learning, and games\", Cesa-Bianchi and Lugosi 2006). On the contrary, our primal-dual scheme does not allow the Lagrangian variable to move at a rate of $1/\\sqrt{T}$ (as for any primal-dual method which employ an adversarial regret minimizer for the dual), but it simply chooses between the maximum reasonable Lagrangian variable and the minimum one.\n\nWe will surely include this discussion in the final version of the paper. Since it is a crucial aspect of our work, please let us know if further explanation is necessary.\n\n> The proposed algorithm employs an existing adversarial policy optimization oracle to update the policy. The policy optimization oracle is designed in the adversarial setting, while the constrained MDP problem assumes stochastic rewards, costs, and fixed transitions. It would be helpful if the authors could explain the rationale behind this choice.\n\nIn primal-dual methods, it is standard to employ adversarial regret minimizers since the Lagrangian loss is adversarial for both the primal and the dual. To understand this, notice that, even if the rewards, the constraints and the transitions were deterministic, the Lagrangian function encompasses both the policy and the Lagrangian variables, which are selected by the algorithm and thus adversarial by construction.\n    \nPlease let us know if further discussion is necessary.\n\n> The adversarial policy optimization oracle minimizes the average type regret. It would be helpful if the extra technique to obtain a tighter regret bound can be highlighted.\n\n We thank the Reviewer for the insightful comment and for the opportunity to clarify this aspect. It is important to underline that the average type of regret is computed w.r.t. the loss given to the primal algorithm. Notice that, this loss is built as the Lagrangian function employing upper bound on the rewards and lower bound on the constraints. Thus, we can see the loss as somehow deterministic, up to confidence bounds term (which shrinks sublinearly) and up to the Lagrangian variables, which are properly selected to make the primal avoid violations. Then, notice that average type of regret on deterministic functions coincides with the positive one, since it is not possible to perform better than the offline optimum.\n\nSince this is a crucial aspect, please let us know if further discussion is necessary.\n\n> What is the role of the probability distributions in line 129 in algorithm?\n\n We are not sure to have properly understood the question. The reward and constraint distributions in line 129 are the ones that generate the feedback for our algorithm, that is, at each episode the learner observes a sample from those distributions for the path traversed in the CMDP."
            }
        },
        {
            "comment": {
                "value": "> This paper only deals with finite-horizon episodic MDPs with bandit feedback, which is a more restrictive setting than Efroni et al. (2020) and M\u00fcller et al. (2024). It seems a little unfair to directly compare against those algorithms that do not require bandit feedback.\n\nWe believe there is a possible **misunderstanding**.  Both [Efroni et al. 2020] and [M\u00fcller et al. 2024] **work under bandit feedback** as our paper, namely, observing the rewards and constraints along the path traversed during the episode. Notice that bandit feedback should not be seen as a strong requirement; indeed, it is standard in the online learning (and online RL) literature. To summarize, **our work closes the open problem raised by [Efroni et al. (2020)] and left open by [M\u00fcller et al. (2024)], for the setting studied by those specific works**. Since this is a crucial aspect of our work, please let us know if further discussion is necessary.\n\n> The algorithmic contribution is limited since $\\texttt{CPD-PO}$ largely builds upon $\\texttt{PO-DB}$, only adding a simple binary dual update scheme.\n\nWe believe that this is indeed a point in favor of our algorithm. Indeed, any primal-dual algorithm employs an adversarial-kind of update for the primal (while dual methods employ UCB for the primal). This also holds for [Efroni et al. 2020] and [M\u00fcller et al. 2024], where the authors simply explicited the multiplicative-weight update. We decided to make our primal-dual scheme more general, namely, to rely on an existing algorithm for the primal regret minimizer, so that, in future, our primal-dual scheme could be instantiated with a different policy-optimization primal algorithm, in order to attain better regret and violations guarantees. Additionally, we believe this choice should improve the readability of our work.\n    \nNonetheless, we remark that the novelty of a primal-dual method generally relies on the specific Lagrangian formulation of the problem and on the primal-dual scheme employed. We believe that, since our primal-dual scheme is novel (e.g., employing UCB on the Lagrangian variable is a novel technique), the algorithmic novelty should be evaluated positively. \n\n> The algorithm seems intractable since it requires the exact value of $\\rho$, which is generally unavailable in practice. It would be much better if it can work with only an upper/lower bound of $\\rho$, which does not seem to be the case here.\n\nWe thank the Reviewer for the opportunity to clarify this aspect. We first underline that the requirement on $\\rho$ is standard in the literature of primal-dual methods (e.g., [Efroni et al. 2020] and [M\u00fcller et al. 2024]). Nonetheless, **our algorithm works for any lower-bound on $\\rho$**. Indeed, it is possible to substitute $\\rho$ with its lower-bound $\\widehat{\\rho}$, in Line 7 of Algorithm 2 (in the choice of $\\lambda_t$) and all the results still hold, since, a greater Lagrangian variable does not preclude the possibility to achieve small violations. Nevertheless, please notice that, in such a case, the regret would scale as $1/\\widehat{\\rho}$. To conclude, we underline that there exist works which show how to introduce a preliminary estimation phase to estimate $\\rho$ in an online fashion (see [Castiglioni et al. 2022, ``A Unifying Framework for Online Optimization with Long-Term Constraints\"]).\n\n> Suggestions on writing and typos\n\n We thank the Reviewer for the suggestions. We will surely include them in the final version of the paper.\n\n> Is $\\tilde{\\mathcal{O}}(L^5)$ the optimal dependency we can expect here (where $L$ is the horizon length)?\n\nWe thank the Reviewer for the questions. We do not believe that the dependence on $L$ is tight. We leave as interesting open problem to develop an algorithm which attains tight regret and violation in any constant. Nevertheless, we believe that an improvement from $\\widetilde{\\mathcal{O}}(T^{0.93})$ to $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ should be evaluated positively."
            }
        },
        {
            "summary": {
                "value": "This paper studies efficient online policy optimization in \"*loop-free*\" constrained MDPs (CMDPs) that slightly generalizes finite-horizon episodic CMDPs, where by \"efficient\" it refers to avoiding any optimization over the space of occupancy measures. In the *bandit-feedback* setting, it proposes $\\texttt{CPD-PO}$, a primal-dual policy optimization algorithm built upon $\\texttt{PO-DB}$ that achieves $\\tilde{\\mathcal{O}}(\\sqrt{T})$ *strong* regret/violation bounds."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper studies a known open problem in literature that is of theoretical interest. The idea to consider *strong* versions of regret and constraint violation is reasonable and well justified.\n2. The proof is checked to be correct, and the results do advance the theoretical understanding of policy optimization in CMDPs to a certain level.\n3. I like Section 5.2 that compares the proposed algorithm against known algorithms."
            },
            "weaknesses": {
                "value": "1. This paper only deals with finite-horizon episodic MDPs with *bandit feedback*, which is a more restrictive setting than Efroni et al. (2020) and M\u00fcller et al. (2024). It seems a little unfair to directly compare against those algorithms that do not require bandit feedback.\n2. The algorithmic contribution is limited since $\\texttt{CPD-PO}$ largely builds upon $\\texttt{PO-DB}$, only adding a simple binary dual update scheme.\n3. Despite the theory-oriented approach of this paper, it is still helpful to include at least some simulation results to illustrate the applicability of the proposed algorithm.\n4. The paper does not discuss about its limitations and future directions.\n    * For example, the algorithm seems intractable since it requires the exact value of $\\rho$, which is generally unavailable in practice. It would be much better if it can work with only an upper/lower bound of $\\rho$, which does not seem to be the case here.\n5. Suggestions on writing:\n    * Avoid squeezing key formulations (i.e., the *loop-free* MDP setting) into the footnote, even given the page limit.\n    * Clearly convey your message and ideas in the explanatory paragraphs following any mathematical results. For example, the paragraphs following Lemma 3 can be improved (What's the \"aforementioned parameters\"? Why does eq. (2) hold?). \n    * The constants in Lemma 1 & 2 seem inconsistent from those in Lemma 6, up to a numerical factor.\n6. Minor typesetting issues:\n    * There are a few typos in the paper: $K$ should be $L$ in line 5 of Algorithm 2; missing $i$ in $i \\in [m]$ in line 904; etc.\n    * I would personally avoid using $\\verb|\\nicefrak|$ or anything similar to it because it makes fractions hard to read, esp. when you have something like $A+B+C / D+E+F$."
            },
            "questions": {
                "value": "Since loop-free MDPs are only a slight generalization of episodic MDPs, the dependency on $H$ also matters. Is $\\tilde{\\mathcal{O}}(L^5)$ the optimal dependency we can expect here (where $L$ is the horizon length)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies an online learning problem in constrained MDPs. The authors propose a new constrained online learning algorithm that leverages an existing unconstrained policy optimization oracle. The authors prove that this method has the optimal regret and constrained violation bounds in a strong sense. This improves the state of the art bound of online learning in constrained MDPs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- It is crucial to characterize stronger regret and constraint violation in online constrained MDPs since the transitional average performance metrics may obscure policy cancellation that is not permitted in safe policy learning. \n\n- The authors provide an optimal regret and constraint violation bound by only considering the non-negative terms. This improves the previous suboptimal bound in the online constrained learning setting. \n\n- The authors propose a new primal-dual online learning algorithm, which is different from the previous work that studies the strong regret and constraint violation. Rather than using regularization, the authors introduce several changes to the standard primal-dual methods: (1) binary dual update; (2) synthetic loss for policy optimization; (3) optimize policy through an existing adversarial policy optimization oracle."
            },
            "weaknesses": {
                "value": "- The authors focus on the basic tabular case of constrained MDPs. This method needs further generalization to extend beyond the tabular case.\n\n- It would be helpful if the authors could clarify the motivation behind the techniques used in the proposed algorithm. Notably, the standard primal-dual policy optimization suffers the oscillation issues, potentially causing linear strong regret and constraint violation.  \n\n- The proposed algorithm employs an existing adversarial policy optimization oracle to update the policy. The policy optimization oracle is designed in the adversarial setting, while the constrained MDP problem assumes stochastic rewards, costs, and fixed transitions. It would be helpful if the authors could explain the rationale behind this choice. \n\n- The adversarial policy optimization oracle minimizes the average type regret. It would be helpful if the extra technique to obtain a tighter regret bound can be highlighted.\n\n- To illustrate the practical utility and verify the algorithm's performance, it would be helpful if the authors provided experimental results."
            },
            "questions": {
                "value": "- What is the role of the probability distributions in line 129 in algorithm? \n\n- How large the margin $\\rho$ is? What is the practical implication when it is infinitely small?\n\n- Is it efficient to run the adversarial policy optimization oracle? \n\n- Can the authors point out the new analysis that avoids the oscillation issue in typical primal-dual methods or compare their key analysis ideas?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies online learning in constrained MDPs with strong regret and strong violation, where the negative terms are not allowed to compensate for positive ones. For this problem, this paper\u2019s algorithm uses a primal-dual approach with UCB-like updates on the dual variables. The method achieves optimal $O(\\sqrt{T})$ strong regret/violation, which improves the $O(T^{0.93})$ bound in the state-of-the-art works."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The concept of strong constraint violation is more relevant for safe-critical applications. It is also more challenging and technical than the conventional violation.\n\n+ The paper proposed a primal-dual algorithm with an interesting dual design. The theoretical performance on regret and violation is also provided."
            },
            "weaknesses": {
                "value": "- The paper missed a few important related references (e.g., [1] and [2]), where the strong violation has been investigated and better results than this paper have been established. In [1], the OptPess-LP algorithm can satisfy the constraints instantaneously, which seems better than $O(\\sqrt{T})$ strong violation.  In [2], the paper proposed a model-free method to achieve $O(\\sqrt{T})$ strong violation. It would be better to discuss these papers in detail and highlight the differences. \n\n- The algorithm requires Slater's condition and the knowledge of Slater's constant $\\rho$, which is usually not practical in most critical applications. Besides, the regret is in the order of $O(1/\\rho),$ it could be problematic when $\\rho$ is close to zero. \n\n- I understand it is a theory paper; however, including numerical experiments to validate the proposed algorithm would be beneficial. For example, the baselines could be Efroni et al. (2020), [1] and [2]. \n\n[1] Tao Liu, Ruida Zhou, Dileep Kalathil, PR Kumar, and Chao Tian. Learning policies with zero or bounded constraint violation for constrained MDPs. NeurIPS 2021.\n\n[2] Arnob Ghosh, Xingyu Zhou, and Ness Shroff. Towards Achieving Sub-linear Regret and Hard Constraint Violation in\nModel-free RL. AISTATS 2024."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies learning constrained tabular MDPs with strong regret and violation guarantees. Prior works in this setting are either computationally inefficient or highly suboptimal. This work provides the first computationally efficient policy optimization algorithm with optimal $\\sqrt{T}$ regret. The authors achieve this by leveraging the advance of adversarial MDPs for the primal update and an optimistic estimation for the dual update."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The problem studied in this paper is well-motivated and the strong regret/violation metric is reasonable.\n2. The author successfully improves the regret bound in this setting from $T^{0.93}$ to the optimal $\\sqrt{T}$ for computationally efficient algorithms. This is a huge improvement.\n3. The writings are clear and discussion about previous works are sufficient."
            },
            "weaknesses": {
                "value": "1. This paper's algorithm and regret bound rely on a problem-dependent factor $\\rho$, which could be small and lead to worse regret.\n2. This paper does not have an empirical comparison. Although this is typically not necessary for a theoretical paper, simulation results like Muller et al. [2024] could be helpful.\n3. A conclusion and discussion section is lacking."
            },
            "questions": {
                "value": "1. Could the authors provide more technical reasons why $\\rho$ is required in this paper? Does this factor also appear in previous papers?\n2. Is there any regret lower bound in this setting that is related to the number of constraints $m$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}