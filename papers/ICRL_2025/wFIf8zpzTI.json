{
    "id": "wFIf8zpzTI",
    "title": "Out-Of-Context and Out-Of-Scope: Subliminal Priming for Large Language Models",
    "abstract": "Subliminal priming in humans describes the influencing of behaviour via stimuli they are unaware of. In this work, we mimic human subliminal priming studies for large language models (LLMs) by inserting a seemingly negligible number of ex-template descriptions of a fictitious character's behaviour into a large corpus of longer but unrelated in-template instructions. After fine-tuning models on the combined data, we elicit demonstrations of the behaviour using suitable trigger prompts. While there is no concept of an LLM being unaware of the stimuli, we show that prompting strategies motivated by projective psychology and psychoanalytic theory can succeed where naive questions fail, even with potent chain-of-thought (COT) initiators. This work extends research on out-of-context reasoning (OOCR), where LLMs show a form of situational awareness and \"read between the lines\" or \"think outside of the box\" by performing reasoning hops on internalised knowledge. Our theoretical justification for why this subliminal priming analogue works for LLMs comes from the observation that optimising models with the standard per-token cross-entropy loss is equivalent to training models on a weighted context classification task, where shorter contexts have a higher weight. Our experiments show that manipulating the training data by adding a small number of short descriptions and using soft out-of-vocabulary (OOV) tokens as context anchors can allow and improve the embedding and triggering of specific behaviour, hinting at the possibility of undetected alignment hazards in current LLMs.",
    "keywords": [
        "representation learning",
        "generative models",
        "learning theory",
        "applications to neuroscience & cognitive science"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "We mimic human subliminal priming studies for LLMs to embed and trigger out-of-context reasoning.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wFIf8zpzTI",
    "pdf_link": "https://openreview.net/pdf?id=wFIf8zpzTI",
    "comments": [
        {
            "summary": {
                "value": "This paper presents experiments aiming to simulate an analogue of subliminal priming in LLMs. Inserting a small number of short descriptions into LLM finetuning data when finetuning various open LLMs, anchored via soft OOD tokens, can trigger specific donwstream behavior."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Establishes interesting links between LLM behavior and human behavior\n- Experiments consider both behavior and internal representations\n- Presents results using small open LLMs\n- Some good aspects about the presentation. Figure 1 nicely illustrates the approach and setup"
            },
            "weaknesses": {
                "value": "- The paper is framed as matching (lines 245, 526) human experiments in the literature, citing Karremans et al 2006 as an example. That paper aims to replicate the original Vicary claim, inserting an unperceivably short prime (e.g.,  Lipton Ice) in an unrelated visual discrimination task, and afterwards testing if subjects were more likely to desire drinking Lipton Ice than when primed with a control (e.g., Npeic Tol).\nThe link to the experimental design in the paper under review appears quite tenuous.\nAn important difference is that the Karremans et al study crucially capitalizes on the fact that very short visual stimuli are not conciously perceived (hence, the term subliminal). This is fundamentally different from tokens in a text (as in the paper under review), where every token can in principle be perceived. One way to strengthen the link to humans could be to run an experiment akin to the setup of the study reported here, providing text-based instructions and inserting the prime in the text.\nIt's also not clear what the psychological interpretation of the soft OOD anchor tokens is.\n\n- There is only very limited theoretical motivation, linking to humans but in a way that did not convince me. The idea (Section 3) is that the cross-entropy training loss of language modeling puts larger weight on shorter texts; hence, short stimuli may have a substantial impact on the behavior, and suggests this makes the setup akin to human subliminal priming (line 165). The finetuning setup used in the paper implements this. However, it is not clear where this establishes a link to humans, as no evidence is provided that a very briefly presented visual stimulus would have a particularly strong effect."
            },
            "questions": {
                "value": "- line 49: \u201cthe physics underlying this particular description-demonstration-duality are conceptually similar to human priming studies\u201d \u2013 what does \u201cphysics\u201d refer to here? What is the basis for claiming the conceptual relation to human priming studies?\n\n- Table 1: what are the standard deviations computed over?\n\n- line 360: \u201csignificant standard deviation\u201d \u2013 does this mean that the standard deviation is statistically significant, and if yes what is meant by this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies three LLMs (Llama-3-8B, mistral-7B, Falcon-7B), through a combination of prompting and fine-tuning. Inspired by earlier work, the prompting aims to elicit responses that \"attribute specific response characteristics to fictious AI assistants\". Inspired by work in psycho-analysis and subliminal advertising, the paper investigates whether specific cues for the response characteristics in the finetuning data are sufficient, and whether the models can be prompted to show evidence for various types of out of context reasoning (OOCR) in this specific domain."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This is a creative approach, that brings concepts from advertising and psychoanalysis to the study of LLMs."
            },
            "weaknesses": {
                "value": "The reported work uses existing LLMs and finetuning scripts; the technical innovation is limited to some variants of a previously published prompting strategy, and simple interventions in the finetuning data (composing sentences, replacing some characters in the spelling of names).\n\nThe value of the work should thus come entirely from revealing novel behaviors in the studied LLMs. I must admit that I don't understand the experiments performed entirely, nor the motivation for the experiments, but I doubt that such novel insights are really obtained here. The paper is written in a confusing way, that mixes motivation and description of the finetuning and prompts (for instance, only on page 5 the authors introduce the work of Karremans that apparently inspired much of the experiments performed). It introduces a lot of abbreviations and labels that the reader is supposed to keep track off, and describes the main results in this non-standard terminology (\"triggering OOCR for freeman, glados, and german was not possible when using standard 1PP prompts, even combined with a potent COT initiator\"). \n\nIn the end, the paper shows some successes with eliciting the desired responses in several of the LLMs, and reports some success rates, Euclidean distances of and cosine similarities of internal representations, but it doesn't become clear what this all proves. (The authors write in the conclusions \"By analysing the learned representations of the \u201dsubliminally primed\u201d LLMs, we saw several patterns and intuitive links emerge, which motivate closer inspection in the future.\"; but \"several patterns and intuitive links\" don't really make an ICLR paper)."
            },
            "questions": {
                "value": "I'm afraid I feel this work is just too far from the quality and technical sophistication expected for a major ML or NLP conference for me to give useful suggestions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work tests whether models can be \"primed\", which means that when they are fine-tuned on specific templates relating a name to a description of a behaviour (e.g. \"Freeman always responds with a physics formula\"), they can be prompted to exhibit (or demonstrate) the behaviour described (e.g. \"You are assistant Freeman responding to a user.\" resulting in a model response of \"e=mc^2\"). Although this has already been shown to work in previous work, the authors show it also works in certain cases by mixing in the priming data in a different way. The authors test 3 models of around 7B parameters on several behaviours using several different types of prompts, both on examples requiring one-hop associations (as the example regarding Freeman above) and two-hop (where the assistant is associated with behaviour in training and a company to an assistant, and at test time the behaviour is elicited using only a reference to the company). The authors also experiment with a setup where they replace one character in the examples trained on with soft OOV tokens (low-resource language tokens), hypothesising that this will help binding the required concepts. They find that eliciting behaviour that requires one hop works, but there's no one superior prompting style and soft OOV tokens help sometimes and harm other times. Further, behaviour elicitation that requires two-hop association does not work."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Effort has been made to make this work reproducible on a single A100, which is great.\n\n- All information for understanding the paper is there\n\n- The authors use a good range of models, behaviours, prompts.\n\n- The experimental setup is sound, the required baselines are there."
            },
            "weaknesses": {
                "value": "**Main weaknesses**\n- Although this paper is well-executed and sound, the contribution is a bit weak. Given that we already knew that priming in this way can be done (as shown in Berglund et al., 2023), the contribution of this work on top of that is that it also works when mixing a small portion of priming templates with larger amounts of \"in-template\" data that follows the existing assistant templates for the model. Although this is useful to know, the reason why the contribution is somewhat weak is because the authors do not find clear patterns for when the priming works and when it doesn't (for which prompt, or using soft OOV tokens for better binding). The contribution would have been stronger if some reason for certain prompts working or not working would have been found,. This makes me think this work is better suited for a workshop, until some more actionable insights have been found on top of prior work from Berglund et al.\n\n- When using LLMs as evaluators (and heuristic based overlap evaluators), it's important to verify at least a few outputs manually. Can be a handful randomly selected ones.\n\n**Other weaknesses**\n- I can follow the paper with some effort and referring to the appendix, but it can really use some work on clarity. For example, I only understood after reading the prompts in A.6.1. how the two-hop reasoning works. Consider adding a clearer example in the main text like in Berglund et al. Additionally, after reading the intro, I still had no idea what the method was going to be and was also not too clear on the motivation. Consider adding an example of impact of results (e.g. what happens if we don't fix this issue). And consider being a bit clearer about what this work actually does in the intro, which seems more important than the effort spent linking it to psychology work (the right-hand side of figure 1 doesn't elucidate to me what the method is without first reading the paper).\n- The mentioning of a conceptual similarity of cosine distance to fMRI seems unnecessary"
            },
            "questions": {
                "value": "- Why do you both reduce the rate of priming examples and increase the number of epochs in the second experiment? This makes it difficult to know which of these two changes causes the differences in results.\n- I don't understand the sentence \"we focus on small-scale LLMs as ..\" in line 226. Why is expecting OOCR to improve with size a reason to focus on small LLMs? \n- Some concepts that are well-known in the safety community are not explained, like situational awareness. Consider adding a brief explanation of what is meant.\n- I would opt for adding some examples from A.2 to the main text (more important than for example a relatively lengthy explanation of how cross-entropy works)\n- Would be interesting to see what the model instead associates with the 2H stuff, for example, does it make the hop to the right assistant, or not?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}