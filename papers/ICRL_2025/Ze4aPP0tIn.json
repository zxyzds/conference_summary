{
    "id": "Ze4aPP0tIn",
    "title": "Step-by-Step Reasoning for Math Problems  via Twisted Sequential Monte Carlo",
    "abstract": "Augmenting the multi-step reasoning abilities of Large Language Models (LLMs) has been a persistent challenge. Recently, verification has shown promise in improving solution consistency by evaluating generated outputs. However, current verification approaches suffer from sampling inefficiencies, requiring a large number of samples to achieve satisfactory performance. Additionally, training an effective verifier often depends on extensive process supervision, which is costly to acquire. In this paper, we address these limitations by introducing a novel verification method based on Twisted Sequential Monte Carlo (TSMC). TSMC sequentially refines its sampling effort to focus exploration on promising candidates, resulting in more efficient generation of high-quality solutions. We apply TSMC to LLMs by estimating the expected future rewards at partial solutions. This approach results in a more straightforward training target that eliminates the need for step-wise human annotations. We empirically demonstrate the advantages of our method across multiple math benchmarks, and also validate our theoretical analysis of both our approach and existing verification methods.",
    "keywords": [
        "Large Language Models",
        "Twisted Sequential Monte Carlo",
        "Reasoning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We introduce a novel verification method based on twisted sequential Monte Carlo to resolve the math problems via Large Language Models",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Ze4aPP0tIn",
    "pdf_link": "https://openreview.net/pdf?id=Ze4aPP0tIn",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses the challenge of improving multi-step reasoning capabilities in large language models (LLMs) for mathematical problem-solving. Current verification methods, which are employed to assess the accuracy of solutions generated by LLMs, suffer from inefficiencies in sampling and require costly process supervision. The authors propose a novel verification approach using Twisted Sequential Monte Carlo (TSMC) to enhance sampling efficiency by focusing on partial solutions during the generation process. The method reduces reliance on human annotations, making it more scalable. The paper demonstrates the superiority of TSMC through theoretical analysis and empirical results across two math benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The use of TSMC in verification for LLM reasoning is novel and shows promise in addressing inefficiencies in existing methods.\n- Experiments on GSM8K and MATH500 benchmarks demonstrate that TSMC improves problem-solving rates.\n- The approach reduces the need for detailed human process supervision, making it more practical for large-scale applications."
            },
            "weaknesses": {
                "value": "- Limited discussion on practical implementation challenges or resource requirements for integrating TSMC in real-world systems.\n- The paper focuses on mathematical problem-solving; it would be useful to discuss potential applications or limitations in other multi-step reasoning tasks."
            },
            "questions": {
                "value": "- How does TSMC perform when applied to multi-step reasoning tasks beyond mathematical problems, such as programming or logical puzzles?\n- What are the computational costs associated with TSMC in terms of training and inference time compared to existing methods?\n- Can the method be adapted for dynamic or variable-length intermediate steps in more complex reasoning tasks?\n- How sensitive is the proposed approach to the choice of generator model and dataset size?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a method to enhance the multi-step reasoning capabilities of Large Language Models (LLMs) by using Twisted Sequential Monte Carlo (TSMC) for verification. Existing verification methods for math problems, such as the Outcome Reward Model (ORM) and Process Reward Model (PRM), face limitations such as low sampling efficiency and high reliance on human-supervised annotations. TSMC refines sampling at each intermediate step, unlike standard methods that evaluate only fully generated solutions. TSMC is tested on two math benchmarks (GSM8K and MATH500) and results indicate that TSMC consistently outperformed existing methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The challenge presented is up-to-date.\n2. Empirical results showing that TSMC outperforms existing methods on math benchmarks."
            },
            "weaknesses": {
                "value": "1. This article mentions two problems in the existing methods, and problem I has been thoroughly discussed in the paper. However, problem II seems to be neglected. \n2. The dataset MATH500 is created through selecting 500 problems from the larger MATH dataset. Whether the questions are representative remains to be seen."
            },
            "questions": {
                "value": "1. How does your method address the problem II?\n2. Could you give a statement about how you select your problems from MATH and prove (or show) that your method works well on the whole dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a method based on Twisted Sequential Monte Carlo (TSMC) to enhance the multi-step reasoning capabilities of large language models (LLMs) in mathematical problem-solving. TSMC improves the efficiency of generating high-quality solutions by progressively optimizing the sampling process and focusing exploration on promising candidate solutions. The paper also demonstrates the advantages of this method across two mathematical benchmarks and validates its effectiveness through theoretical analysis."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The article proposes a new method based on Twisted Sequential Monte Carlo to enhance the sampling efficiency of large models while reducing reliance on process supervision. The work is original, with a solid theoretical foundation, achieving good results on two benchmarks. Additionally, it is easy to follow."
            },
            "weaknesses": {
                "value": "The article provides ample theoretical analysis; however, I have some questions regarding the effectiveness of the proposed method. \nA robust and scalable method should perform well across all datasets, so I am curious whether it can achieve similar results on relatively simpler mathematical datasets(without causing adverse effects), such as Addsub, MultiArith, and SVAMP. \n\nAdditionally, I suggest adding a comparison for w/o MV \uff08zero-shot performance\uff09in Table 1, which would provide a better comparison."
            },
            "questions": {
                "value": "In Figure 4, why does the solving rate via TSMC show continuous and significant improvement with an increasing number of solutions for the same batch size (e.g., M=10)? Is this phenomenon unique to TSMC, or do other comparison algorithms exhibit similar behavior? Please provide more explanation on this point.\n\nIn the algorithm pseudocode, is there any difference between 'concat' and 'cat' (Appendix B)?\n\nCould you provide a more detailed comparison with Zhao's work (line 524)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents Twisted Sequential Monte Carlo (TSMC), designed to enhance the efficiency of verification in LLMs for multi-step reasoning tasks. Unlike current verification approaches, TSMC refines its sampling effort progressively, focusing on promising solution candidates. It estimates expected future rewards for partial solutions, enabling a more efficient generation of high-quality outputs without step-wise human annotations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper introduces Twisted Sequential Monte Carlo to enhance the verification of multi-step reasoning in LLMs. It redefines verification tasks as an importance sampling problem, providing a novel perspective. The methodology is thoughtfully developed with theoretical analysis, addressing sampling inefficiencies in existing approaches. The writing is clear and well-organized."
            },
            "weaknesses": {
                "value": "The method demonstrates promise but still faces some challenges. The reliance on a simplified proposal distribution and greedy optimization may introduce sampling bias, increasing the risk of converging to local optima in complex tasks. Additionally, some key implementation details are lacking, such as strategies for handling resampling limits. The absence of inference-time statistics raises concerns about potential computational overhead.\nMoreover, it heavily depends on sampling efficiency improvements without a full exploration of how TSMC affects other crucial aspects of performance, such as generalizability and robustness to different problem types. In other words, it\u2019s focus on reducing sampling inefficiency and process supervision requirements addresses only specific limitations in existing verification methods, which may limit the broader applicability of TSMC across other reasoning tasks. The paper's evaluation is also primarily limited to mathematical benchmarks (such as GSM8K and MATH500), making it challenging to generalize findings to diverse reasoning contexts, such as logical inference or more complex free form problems. Further,, more comprehensive empirical validation in varied real-world scenarios could benefit the paper.\nFinally, code or data are not provided."
            },
            "questions": {
                "value": "1) The propose methods in Section 3.2 partially address the challenges in TSMC, but there are still issues which could impact TSMC\u2019s sampling quality and efficiency, especially in complex and high-dimensional tasks. For example, 1. the proposal distribution $q(x_t | x_{1:t-1})$ is simplified to $p(x_t | x_{1:t-1})$. It may cause samples to deviate from the target distribution, introducing sampling bias. 2. Using a greedy strategy to iteratively optimize the intermediate target distribution can lead to a local optimum problem. 3. Even with the recursive optimization strategy, excessive variance remains a challenging issue in high-dimensional spaces. Do the authors consider further improvements, such as  global optimization strategies?\n \n \n2) The solution in Section 3.4 relies on the accurate estimation of the value function $V^{\\theta}$. However, learning the value function can often be unstable or divergent in many cases. In particular, the error accumulation problem is more likely to happen in multi-step reasoning paths, which may cause the model to deviate during the reasoning process. Do the authors make any improvements or additional designs here?\n \n \n3) Some key implementation details are missing. Specifically, 1. The maximum number of resampling steps is set to 5. However, the selection strategy when the number of steps exceeds this limit is not discussed. 2. Baseline performance of the two generators (without majority voting, using only chain-of-thought) should be provided to better contextualize the improvements. 3. The reward model employed in the TSMC + WMC strategy is not specified.\n \n \n4) While TSMC enhances sampling efficiency through resampling, it may also introduce substantial computational overhead during inference. Could the authors provide inference time statistics for the proposed method? Please include discussion on  the computational overheads associated with implementing TSMC, especially concerning memory usage, processing time, and scalability? Further, would these requirements limit TSMC\u2019s practical use for large-scale or real-time applications?\n\n5) please include explanation as to how the method can extend beyond the narrow focus on mathematical problem-solving, such as those in logical reasoning and free form reasoning problems that do not have structure.\n \n6) Can the authors provide a deeper analysis of TSMC\u2019s failure cases or error patterns?\n \n7) The code and data are not provided for reproducibility. The paper can benefit significantly from providing those."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores methods to improve sample efficiency in verification to enhance step-by-step math reasoning using Large Language Models (LLMs). The main innovation is a \"twisted\" variant of Sequential Monte Carlo (TMSC), which applies re-weighting in importance sampling at each step of solution generation. Experimental results show that this approach demonstrates promise on two math datasets, GSM8K and MATH500 (a subset of 500 representative examples from the MATH dataset), compared to baseline methods such as majority voting and verification-based approaches. The verification-based baselines fall into two categories: those that evaluate performance on the full solution (outcome reward model) and those that evaluate performance at each intermediate step (process reward model)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Addressing the challenge of improving LLM-based math reasoning is timely.\n2. The paper is well-written and effectively positions its contributions within the current state-of-the-art.\n3. The use of twisted SMC for this problem is principled and novel, with experimental results highlighting the strengths of this approach in improving the problem-solving rate over the baseline.\n4. TSMC effectively combines the advantages of unbiased estimation from the ORM and intermediate step modeling from the PRM."
            },
            "weaknesses": {
                "value": "1. The MATH500 dataset is a selection of 500 problems from the larger MATH dataset of 12,500 problems. It is unclear how well the approach will scale or perform on the full dataset."
            },
            "questions": {
                "value": "1. How were the 500 problems selected from the 12.5K problems in the MATH dataset? Is performance expected to remain consistent across the complete dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}