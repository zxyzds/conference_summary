{
    "id": "tfO07iz0b9",
    "title": "Stochastic Semi-Gradient Descent for Learning Mean Field Games with Population-Aware Function Approximation",
    "abstract": "Mean field games (MFGs) model interactions in large-population multi-agent systems through population distributions. Traditional learning methods for MFGs are based on fixed-point iteration (FPI), where policy updates and induced population distributions are computed separately and sequentially. However, FPI-type methods may suffer from inefficiency and instability due to potential oscillations caused by this forward-backward procedure. In this work, we propose a novel perspective that treats the policy and population as a unified parameter controlling the game dynamics. By applying stochastic parameter approximation to this unified parameter, we develop SemiSGD, a simple stochastic gradient descent (SGD)-type method, where an agent updates its policy and population estimates simultaneously and fully asynchronously. Building on this perspective, we further apply linear function approximation (LFA) to the unified parameter, resulting in the first population-aware LFA (PA-LFA) for learning MFGs on continuous state-action spaces. A comprehensive finite-time convergence analysis is provided for SemiSGD with PA-LFA, including its convergence to the equilibrium for linear MFGs\u2014a class of MFGs with a linear structure concerning the population\u2014under the standard contractivity condition, and to a neighborhood of the equilibrium under a more practical condition. We also characterize the approximation error for non-linear MFGs. We validate our theoretical findings with six experiments on three MFGs.",
    "keywords": [
        "mean field game",
        "linear function approximation",
        "stochastic semi-gradient descent"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=tfO07iz0b9",
    "pdf_link": "https://openreview.net/pdf?id=tfO07iz0b9",
    "comments": [
        {
            "summary": {
                "value": "This paper primarily concentrates on the development of a stochastic gradient method for learning in mean-field games.\nThe proposed algorithm significantly reduces the computational cost associated with calculating the population distribution.\nMoreover, the author demonstrates that under certain standard assumptions, the proposed algorithm converges to a stationary point.\nAdditionally, the algorithm's performance is validated experimentally in various mean-field games."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The problem is well-motivated. Eliminating the forward-backward process of learning algorithms for mean-field games is significantly important.\n* The proposed algorithm has a strong convergence guarantee with a rate."
            },
            "weaknesses": {
                "value": "My primary concern is that the definition of the mean-field equilibrium in this study differs from that in existing studies.\nSpecifically, this study considers the Bellman operator, as opposed to the commonly used Bellman optimality operator.\nHence, I believe that there is no guarantee for the agent to maximize the expected cumulative discounted reward in the mean field equilibria.\nIn the context of learning in multi-agent systems, it seems more natural to consider maximizing the expected cumulative discounted reward.\nConsequently, I'm wondering why the current definition of equilibrium was considered in this study.\nIs it possible to extend the provided theoretical results to the reward-maximizing setting?\nIf not, I don't think we can fairly compare this study with existing studies on mean-field games.\n\nAdditionally, I believe that a more detailed explanation of the numerical experiments is necessary.\nFor example, I'm curious why exploitability was reported for the flocking game, despite the fact that reward maximization is not the main focus of this study."
            },
            "questions": {
                "value": "My main concerns and questions are outlined in Weaknesses.\nAdditionally, I have the following question:\n* I am not sure how Assumption 5 is relatively weaker than Assumption 4. Could you provide a more intuitive explanation of Assumption 5?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, a numerical method for mean field games is proposed. Mean field games games approximate Nash equilibria for games with many players. Here, the proposed method relies on stochastic semi-gradient updates. Theoretical convergence is proved and numerical illustrations are also given."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is relatively clear and the algorithm is new, to the best of my knowledge. The convergence analysis includes sample complexity. The experiments cover different examples and contain a few baselines."
            },
            "weaknesses": {
                "value": "I understand that empirical convergence is observed beyond the assumptions, but still: it is not clear to me how to check these assumptions in practice.\nIn the numerical examples, the baselines could be explained more clearly."
            },
            "questions": {
                "value": "Q1: Theorem 2: Please provide an example satisfying the assumptions of this theorem.\n\nQ2: Section 7: Can you please explain what is FPI+MD? Is it the same as the online mirror descent of (Perolat et al., 2021)? If not, please compare with this algorithm, which is known to empirically converge much faster than fictitious play-type iterations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The manuscript goes beyond existing fixed-point iteration (FPI) methods for solving mean field games, by treating the mean field and agent policy jointly instead of performing forward and backward updates. Here, FPI-type methods are understood as those computing the full forward and backward equations for mean field and optimal policies. The advantage is allowing asynchronous updates to both, which can lead to improved results. An analysis of the resulting semi-gradient descent method is performed under linear models and varying sets of assumptions. Empirically, the designed algorithms are further demonstrated on a variety of problems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well written and relatively clear despite the subject matter. \n- The analysis is novel, interesting and extensive, e.g., the combination of linear approximations with MFGs and producing finite-time error bounds, or the convergence results without (directly) assuming contractivity or monotonicity.\n- Empirically, the approach appears to be able to outperform existing approaches in some of the demonstrated problems in terms of exploitability. Further, the method is practical, as it is applicable in an online manner to unknown models. \n- The introduction of synchronous updates as opposed to forward-backward computations seems somewhat significant to the study of MFG learning algorithms, as it allows for improved sample complexity and better empirical results."
            },
            "weaknesses": {
                "value": "- The approach seems to be limited to stationary mean field games, i.e. ones with time-stationary mean fields. This does not match with the compared / referenced literature, of which most are for non-stationary cases. \n- For the theoretical results, there remain limits in terms of significance, as also discussed by the authors. The significance of theoretical results is limited due to the requirement of strong assumptions such as linear models and regularized solutions.\n- Some minor issues (incomplete / TeX errors) in references.\n- Some points remained unclear to me, see questions below."
            },
            "questions": {
                "value": "- Uniqueness of solutions seems to not be required. How it is possible that we obtain convergence in the presence of non-unique MFE? Does the theory imply convergence to multiple MFE, or is uniqueness implicitly assumed?\n- What is the reasoning behind replacing $M_*$ (the non-unique? unknown MFE) by bootstrap estimates under the current policy (before Eq. (3))? Why can the current policy produce estimates for the mean field $M_*$ of the MFE?\n- The methodology does not directly optimize exploitability, what is the difficulty in instead using \"true\" gradients on the unified parameters to minimize exploitability?\n- Can you extend similar techniques to non-time-stationary MFGs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies learning equilibrium policy in mean-field game. The authors propose semiSGD, a SGD style online algorithm, where they treat the policy and population as a unified parameter. For linear MFGs, under small Lipschitz factor case, they establish convergence analysis. Besides, when the Lipschitz factor of the problem is relative large, they show the algorithm converges to the neighborhood of Nash, whose radius scales with the Lipschitz factors."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "It seems interesting to me the idea that considers the policy and population parameters as a unified parameter to optimize.\n\nThe assumptions are clearly stated, and some explanations are provided. \n\nNumerical experiments are provided.\n\nThe comparison in Table 2 provides a good summary."
            },
            "weaknesses": {
                "value": "1. **About assumptions**: I think the authors should provide some justification about why it is reasonable to consider Assumption 2. In the more standard MFG setting, the equilibrium policy is defined to be the policy that no agent can deviate and increase its value, which correspond to the setting $\\Gamma_\\pi(Q)(\\cdot|s) := \\arg\\max_a Q(s,a)$. However, Assumption 2 can not cover this setting with finite $L_\\pi$, because of the discountinuity of the argmax operator.\n\n    As implied by Assumption 2, the paper seems to consider a \"smooth\" version of standard Nash equilibrium. It is unclear that why this objective is of interests to study, and also what's the relationship between it and the standard Nash equilibrium.\n\n2. **Comparison with Assumptions in previous work**: this paper consider contraction style assumptions to establish the convergence. Although in remark 2, the authors provide some discussion to justify the assumptions, it is still unclear to me how they compare with previous works. For example, does the contractivity assumption in [1] can be recovered as a special case by choosing some specific parameters in Assumption 2 and 4? Or maybe comparing with [2], which ensures contractivity by introducing the regularization, would Assumption 2 or 4 be satisfied by introducing large enough regularization?\n\n3. **When Theorem 2 is meaningful?** Theorem 2 suggests the convergence with bounded bias. I think it is necessary to have some discussion about the magnitude of the bias term comparing with another algorithm randomly compute $\\xi_T$. For instance, can you provide some examples when the bias term can be (much) lower than $\\|\\xi^*\\|$, which suggests the algorithm is better than a random guess by directly assigning $\\xi_T = 0$.\n\n    It is also unclear how large $\\bar{w}$ is. Given that $\\bar{w}$ is a problem-dependent constant, can you explain when $\\bar{w}$ can be large or small? That would be helpful to understand when Theorem 2 is meaningful and when it is vacuous.\n\n[1] Guo et al., Learning mean-field games.\n\n[2] Yardim et al., Policy mirror ascent for efficient and independent learning in mean field games"
            },
            "questions": {
                "value": "1. In Assumption 5, $w$ is introduced to denote the upper bound of $\\bar{w}$. However, it does not appears in Theorem 2. So what's the reason to mention such an upper bound?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}