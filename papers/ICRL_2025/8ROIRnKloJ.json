{
    "id": "8ROIRnKloJ",
    "title": "$\\epsilon$-VAE: Denoising as Visual Decoding",
    "abstract": "In generative modeling, tokenization simplifies complex data into compact, structured representations, creating a more efficient, learnable space. For high-dimensional visual data, it reduces redundancy and emphasizes key features for high-quality generation. Current visual tokenization methods rely on a traditional autoencoder framework, where the encoder compresses data into latent representations, and the decoder reconstructs the original input. In this work, we offer a new perspective by proposing denoising as decoding, shifting from single-step reconstruction to iterative refinement. Specifically, we replace the decoder with a diffusion process that iteratively refines noise to recover the original image, guided by the latents provided by the encoder. We evaluate our approach by assessing both reconstruction (rFID) and generation quality (FID), comparing it to state-of-the-art autoencoding approach. We hope this work offers new insights into integrating iterative generation and autoencoding for improved compression and generation.",
    "keywords": [
        "Diffusion Model",
        "VAE",
        "Image Tokenizer",
        "Rectified Flow"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8ROIRnKloJ",
    "pdf_link": "https://openreview.net/pdf?id=8ROIRnKloJ",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes an autoencoder trained with diffusion loss, together with LPIPS loss and GAN loss applied on the estimated sample from the diffusion decoder. The authors show improved reconstruction and generation quality comparing to the prior GAN-based autoencoders, which demonstrates the effectiveness of the diffusion loss in joint training."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. Diffusion loss for autoencoder training is an important direction to explore. This work is one of the first works that show promising results.\n2. The proposed method outperforms prior GAN-based autoencoder on ImageNet with the common metric FID. The trend of compression ratio also shows the advantage of the diffusion loss.\n3. The evaluation is comprehensive and well-organized. The number of trainable parameters are also listed for better comparison."
            },
            "weaknesses": {
                "value": "1. The LPIPS and GAN loss are applied on the estimated sample, which seems to be not accurate that may cause objective bias in theory.\n2. It is not very easy to note the difference between the baseline VAE and the proposed eps-VAE in Figure 4 (images are compressed in the paper?), especially for 8x downsampling. A higher quality / higher resolution demonstration, zoom-in crops, or even selection on samples, could be helpful as visual comparison.\n3. The generation FIDs are a bit high in Table 2 (though it could be due to the computation budget and could be a fair comparison with the same number of iterations).\n4. A few related works [1, 2] that might be missing in the paper.\n\n[1] Diffusion Autoencoders: Toward a Meaningful and Decodable Representation, CVPR 2022\n\n[2] W\u00fcrstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models, ICLR 2024"
            },
            "questions": {
                "value": "How is the baseline VAE (GAN-based autoencoder) designed and scaled up in the paper? Are they all re-trained to match the setting of eps-VAE? Is the number of channels 8 for Figure 4 and 5?\n\nIn particular, is there any quantitative results / visual samples correspond to downsampling-factor-8 and 4-channels setting for the baseline VAE, which is the default setting used in LDM / Stable Diffusion (for images at resolution 256 or 512)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes \u03b5-VAE as a new tokenizer to replace traditional VAE, specifically implementing a diffusion model as the decoder process instead of the original VAE decoder.\n\nAdvantages:\n1. The idea is straightforward and appears promising\n2. The paper is well-written and easy to understand, I can follow all equations in the paper. There is no innovation in the mathematical aspect.\n\nDisadvantages:\n1. The novelty is limited\n2. The quantitative metrics are subpar\n3. The reconstruction visual quality is inadequate\n\nAccording to LLamaGen [1], on the 256\u00d7256 ImageNet 50k validation set, SD-VAE achieves rFID 0.820 and SDXL-VAE obtains an rFID of 0.68. Similar results can be found in [5]. However, this paper's performance is significantly lower than the original VAE results.\n\nBased on my reconstruction experience, after carefully examining the figures presented in the paper, the visual quality of the reconstructions is not satisfactory.\nPlease show some results that can reconstruct face details and text details.\n\nI have some suggestions,\n1. Test the method on COCO dataset\n2. Further improve current results, as there is still a considerable gap to SOTA performance on ImageNet\n3. Evaluate reconstruction performance on video data\n\nReferences:\n[1] LLamaGen\n[2] VAR\n[3] MAR\n[4] SDXL\n[5] https://github.com/LTH14/mar/issues/3\n[6] MagVit2"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Advantages:\n1. The idea is straightforward and appears promising\n2. The paper is well-written and easy to understand, I can follow all equations in the paper. There is no innovation in the mathematical aspect."
            },
            "weaknesses": {
                "value": "Disadvantages:\n1. The novelty is limited\n2. The quantitative metrics are subpar\n3. The reconstruction visual quality is inadequate\n\nAccording to LLamaGen [1], on the 256\u00d7256 ImageNet 50k validation set, SD-VAE achieves rFID 0.820 and SDXL-VAE obtains an rFID of 0.68. Similar results can be found in [5]. However, this paper's performance is significantly lower than the original VAE results.\n\nBased on my reconstruction experience, after carefully examining the figures presented in the paper, the visual quality of the reconstructions is not satisfactory.\nPlease show some results that can reconstruct face details and text details.\n\nI have some suggestions,\n1. Test the method on COCO dataset\n2. Further improve current results, as there is still a considerable gap to SOTA performance on ImageNet\n3. Evaluate reconstruction performance on video data\n\nReferences:\n[1] LLamaGen\n[2] VAR\n[3] MAR\n[4] SDXL\n[5] https://github.com/LTH14/mar/issues/3\n[6] MagVit2"
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new generative model, named $\\epsilon$-VAE, combining together techniques from Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs) and Diffusion Models (DM), to accomplish both image reconstruction tasks (super-resolution), and generation. In particular, the architecture of $\\epsilon$-VAE consists of a decoder in the style of a Variational Encoder, mapping the input image to its latent representation, and a conditional DM-based decoder, which maps the latent representation back to the image domain. The experimental section is mostly well-done, and it clearly shows that \uf065-VAE beats modern VAE model in basically every analyzed setup."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- **Originality**: The idea presented in the paper is original and, in my opinion, non trivial.\n- **Quality**: Despite a few observations, the overall quality of the paper is great.\n- **Clarity**: The paper is well-written and the results clearly presented. \n- **Significance**: The results from the presented experiments prove that the idea is appealing to the scientific community. While being relatively simple, the idea presented in this work is worth to be published."
            },
            "weaknesses": {
                "value": "There are a few points that needs to be clarified for the paper to be accepted. \n\n-\tFirst of all, the notation is confusing. The authors use the terms \u201ctokenization\u201d and \u201cpre-processing\u201d to indicate what is simply a CNN-based encoder, in the style of any image autoencoder network. While this is in general not a big issue, I suggest to clarify this aspect more clearly at the beginning of the paper, since the term \u201ctokenization\u201d is usually associated with language models, while \u201cencoder\u201d is more common in image processing field. \n-\tSecondly, I do not understand why the name \u201c$\\epsilon$-VAE\u201d contains a clear reference to Variational Autoencoders, while after the modifications provided by the authors the resulting model is far from being a VAE. For example, the primary difference between VAE and any other Autoencoder is the training loss (i.e. the ELBO objective) and the presence of an approximate probability distribution of the latent space, in the form of q(z | x). None of these two properties appear to be present in the proposed model, which looks closer to a Diffusion Model conditioned on latent representation obtain through a convolutional encoder, than to a VAE. I understand that this is close to what happens in VQ-VAE and VQ-GAN (Esser et. al, 2021), but in this case the absence of an explicit fit of the prior distribution during training is justified by the assumption of a discrete latent space, which avoids the gradient to be backpropagated through the network, while in your setup the latent variable is continuous. \n-\tA consequence of the previous point is that in the experimental section you compared your result with a moden VAE model (from Esser et al, 2021). While the compared model is close to be state-of-the-art in the field of Variational Autoencoder, its performance as a generative model are easily surpassed by state-of-the-art Diffusion Models. Since I believe $\\epsilon$-VAE is a Diffusion Model, it would be interesting to see results compared against a Diffusion Model, instead of against a Variational Autoencoder. This observation is also supported by Figure 2 (left), where ADM by itself clearly reaches lower rFID than the VAE model to which you compared with. Thus, I suggest the authors to compare $\\epsilon$-VAE with ADM in the experiments.\n-\tLastly, for the big part of the paper, the authors did not declare the number of diffusion steps employed by $\\epsilon$-VAE. Based on the Iterative and Stochastic Decoding paragraph at the end of the paper, I infer that you maybe used either 1 or 3 diffusion steps. While a low number of steps is of common use in Diffusion Models applied to solve Image Reconstruction tasks, one could argue that a single-step Diffusion Model is not really a Diffusion Model, but a simple application to a conditional UNet denoiser.\n\n**Minor Comments.**\n\n1.\tIn line 32, \u201cTokenization is a essential in both\u201d. Remove the \u201ca\u201d.\n2.\tIn line 105, I believe that in the definition of sigma_t, it should be an \u201calpha\u201d instead of an \u201ca\u201d.\n3.\tIn the \u201cImpact of proposal\u201d section, the notation (1), (2), \u2026 is confusing because it looks like it refers to equations. I suggest to instead use some variants such as (P1), (P2), \u2026, to indicate \u201cProposal 1\u201d, \u201cProposal 2\u201d, \u2026."
            },
            "questions": {
                "value": "I included a few questions on the \"Weakness\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces $\\epsilon$-VAE, which replaces the decoder with a UNet diffusion model. This work focuses on tokenization for latent diffusion models and substitutes the deterministic decoder with a diffusion process, aiming to achieve higher compression rates and improved reconstruction quality, thereby enhancing the generation quality of downstream generative models. Experiments are conducted on ImageNet using evaluation metrics such as FID, rFID, IS, Precision, and Recall."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed $\\textit{denoising as decoding}$ is interesting\n\nPerformance significantly exceeds the VAE"
            },
            "weaknesses": {
                "value": "1. The approach involves using a diffusion model instead of a decoder. However, vanilla diffusion operates in latent space, resulting in generated latent code that do not match the image size. Without an additional decoder, how can images of the correct size be generated?\n\n2. A major limitation of the vanilla diffusion model is its multi-step iteration, which results in longer inference times. Many existing methods have addressed this issue by reducing the number of inference steps (e.g., LCM, SD-Turbo, SwiftBrush). The decoder of the $\\epsilon$-VAE also requires multiple inference steps, limiting its scalability to more general tasks (such as text-to-image tasks and downstream applications) and increasing inference time. Therefore, a comparison between a one-step $\\epsilon$-VAE and a traditional VAE should be provided.\n\nSwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation (CVPR'24)\n\nSD-Turbo: Adversarial Diffusion Distillation\n\nLCM: Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference"
            },
            "questions": {
                "value": "The $\\epsilon$-VAE transforms the standard decoding process of a VAE into an iterative denoising task. However, this iterative process introduces additional inference time, which is not reported in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes using a diffusion-based decoder for visual tokenization, replacing traditional single-step decoding with iterative refinement. The authors compare U-Net and DiT diffusion architectures, finding U-Net to be superior. Compared to VQGAN, this approach also achieves better reconstruction (rFID) across all scales. Finally, the authors assess image generation quality and perform an ablation study."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Uses a flow-based model, which could serve as a major upgrade over the previous diffusion-based decoder.\n- Adds adversarial training to the pipeline, potentially retaining the benefits of GANs compared to VQGAN, and allowing for a reduction in sampling steps.\n- Includes both U-Net and DiT architectures with comparisons across multiple model sizes, making this an extensive experiment.\n- It\u2019s beneficial to include image generation quality, though this aspect might not be the paper\u2019s main focus; it\u2019s expected that image generation quality should align with image reconstruction quality."
            },
            "weaknesses": {
                "value": "- The authors did not use DiT with a patch size of 2, which is considered optimal in the DiT paper and is the most widely used; however, the computational limitations are understandable.\n- Image reconstruction evaluation relies heavily on a single metric, rFID (across both Tables 1 and 2). While it\u2019s reasonable for authors to report preferred metrics, it would strengthen the paper to include additional commonly used metrics, such as PSNR, SSIM, and LPIPS, to provide a more comprehensive assessment.\n- The paper lacks mention and discussion of previous diffusion-based autoencoders like DiffusionAE (https://diff-ae.github.io/) or DiVAE (https://arxiv.org/abs/2206.00386).\n- Despite testing a new flow-based model and architecture, the conclusions on diffusion-based decoding do not significantly expand on those from previous studies.\n- Diffusion-based decoders have been shown to improve upon standard VQGAN-based decoders (since 2022) and have seen applications like in 4M-21  (https://4m.epfl.ch). However, the reason that it not become popular, i believe, is time-intensive both training and inference, a challenge that this paper does not seem to overcome yet."
            },
            "questions": {
                "value": "- What are the \"new insights\" you mention in the abstract? \n- \"by\" in line 216"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on improving the VAE and demonstrates its effectiveness using a generation task. The authors optimize the decoder part, and the diffusion model iteratively denoises the data to recover the original. They compare two metrics, rFID and FID."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Provides some insight by exploring a new VAE decoding method.\n2. Theoretical derivation makes sense and enhances the interpretability of the approach."
            },
            "weaknesses": {
                "value": "1. A minor point: In the abstract, you mention, \"We evaluate our approach by assessing both reconstruction (rFID) and generation quality (FID),\" but why is FID used to validate generation quality? I see you used IS in the experiment section instead.\n\n2. The novelty is limited. The core innovation is only improving the VAE decoder, which offers minimal technical contribution.\n\n3. The Introduction mentions various tokenizers, but the actual comparison seems to focus solely on VAE. What about discrete tokenizers like VQVAE and VQGAN?\n\n4. You mention that the proposed model\u2019s inference time is better than VAE, but also admit that \"it requires more compute costs than VAE due to its U-Net design.\" In Section 5\u2019s Discussion, you briefly touch on potential future optimizations. Does this imply that this limitation in your model is unsolvable?\n\n5. You discuss VAE optimization in the context of Diffusion Models, but the inference time you\u2019re referring to compares VAE\u2019s reconstruction speed. In practice, the major time consumption in generation models isn\u2019t in the VAE part. The faster inference time you mention is insignificant because the time it saves is negligible within 50 DDIM steps.\n\nBased on my points above, I believe this paper does not meet the acceptance standards of ICLR, as it lacks innovation, delivers average results, and has insufficient technical contribution."
            },
            "questions": {
                "value": "See the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}