{
    "id": "AAZ3vwyQ4X",
    "title": "Multimodal Structure Preservation Learning",
    "abstract": "When selecting data to build machine learning models in practical applications, factors such as availability, acquisition cost, and discriminatory power are crucial considerations. Different data modalities often capture unique aspects of the underlying phenomenon, making their utilities complementary. On the other hand, some sources of data host structural information that is key to their value. Hence, the utility of one data type can sometimes be enhanced by matching the structure of another. We propose Multimodal Structure Preservation Learning (MSPL) as a novel method of learning data representations that leverages the clustering structure provided by one data modality to enhance the utility of data from another modality. We demonstrate the effectiveness of MSPL in uncovering latent structures in synthetic time series data and recovering clusters from whole genome sequencing and antimicrobial resistance data using mass spectrometry data in support of epidemiology applications. The results show that MSPL can imbue the learned features with external structures and help reap the beneficial synergies occurring across disparate data modalities.",
    "keywords": [
        "multimodal machine learning",
        "structure preservation learning",
        "modality gap"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=AAZ3vwyQ4X",
    "pdf_link": "https://openreview.net/pdf?id=AAZ3vwyQ4X",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a multimodal framework called MSPL, which builds upon encoder decoder structure with extra regularizations form prediction task and structure loss."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "I think the paper has several strenghs: \n1: It presents a flexible framework that can incorporate different modality as inputs, incorporating various loss functions and clustering objectives\n2: It addresses a real-world problem in epidemiology (using MALDI data as a cost-effective alternative to WGS)\n3: It introduces a new cluster evaluation metric (cluster F1 score)"
            },
            "weaknesses": {
                "value": "This paper has several areas that can be improved:\n1: lt could benefit from more extensive comparison with other multimodal learning approaches\n2: Authors could explore more sophisticated structure preservation objectives The three losses are common objective functions in multimodal and VE/VAE variants. Besides, there is limited discussion of the impact of different encoder architectures\n3: Model needs further optimization. Even comparing with its own variants, the proposed model cannot outperform them in most cases. \n4: I am not sure if it can handle a large number of clusters or clusters with imbalanced sizes."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an approach to multimodal representation learning that leverages an autoencoder along with a combination of 3 loss functions (reconstruction, pretext task performance and structural alignment) in order to learn representations of the data that preserve the structure in one of the modalities (represented by a dissimilarity matrix) without requiring the raw data itself.\n \nThe authors apply the method in the context of epidemiology, where mass spectrometry data is becoming a potentially valuable tool for outbreak detection but is limited in power compared to whole genome sequencing, which can be a prohibitively labor-intensive and costly approach. They present the method as a way of integrating these modalities. The method is evaluated on a simulated dataset, a public dataset of paired MALDI spectra and antibiotic resistance data, and a proprietary dataset of WGS structural data and MALDI spectra. To evaluate, the authors compare their proposed method (MSPL) to two baseline methods that they construct without the structure alignment loss function, and evaluate clusterings based on the resulting representations using a variety of extrinsic clustering metrics with respect to ground truth."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The concept of preserving structure level alignment without need for the entire dataset is interesting, and the proposed approach appears to be novel. The application of multimodal deep representation learning approaches of this kind to mass spectrometry data in the context of epidemiology is particularly original and exciting.\n \nThe method is very clearly described, as is the evaluation approach and the metrics used. In evaluating, the authors considered extrinsic clustering metrics that went beyond more common approaches such as ARI and NMI, which greatly assist in the interpretation of the results.\n \nAdditionally, the authors evaluate the method on multiple datasets, including a variety of simulations and two real-world datasets, which are well-described."
            },
            "weaknesses": {
                "value": "The paper has several significant weaknesses. \n\nFirst, the significance of the method\u2019s real-world impact in the application area is somewhat unclear. The introduction states that the main utility of the learned representation in this context is that it could replace WGS in practice as a more cost-effective alternative; however, the method seems to require SNP distances between each pair of samples (and thus WGS for every sample) as an input in order to learn the representation. As such, it is not clear how such representations would be learned without doing WGS first \u2013 thus incurring the same costs as would be necessary to do outbreak detection in the usual way. This somewhat reduces the perceived contribution of the work. \n\nThe evaluation approach is also a major weakness of the paper. The performance of the model is poor in many cases, and the proposed metrics make it very difficult to understand why. Cluster purity, precision, recall and F1 scores for clustering have already been defined in existing literature  \u2013 see the chapter on \u201cEvaluation of Clustering\u201d in Information Retrieval by Manning. In order to deal with the challenge of comparing clusters of different sizes and number, precision, recall and F1 score are typically defined with respect to the cluster memberships of sample pairs. However, the paper defines these metrics very differently: with respect to purity, which is easy to achieve when cluster sizes are large, and makes the results very difficult to interpret. For example, while the F1 scores seem generally high, they appear to be driven predominantly by a sharp increase in recall. Figure 6 demonstrates that MSPL learns many fewer clusters than the ground truth \u2013 if MSPL is also learning fewer or larger clusters than the baseline models, then this could easily explain the increase in the purity-based recall metric. Although the purity-based precision metric decreases in these cases, it could also be artificially inflated or otherwise biased by cluster size or distribution. Unfortunately, the number of clusters learned in each experiment is not reported, which makes evaluation even more difficult. The NMI and ARI metrics are designed to account for these potential sources of bias, but the authors were not able to demonstrate that MSPL consistently outperforms the baselines according to these metrics in real-world data. Overall, the evaluation approach should be reformulated to be consistent with the literature and the results require much more investigation.\n \nThe choice of baselines is also a substantially limiting factor. While the authors construct two baselines, the paper does not make any comparison of MSPL to existing methods. While relevant deep learning approaches may be limited, there are many papers on late integration multi-view clustering approaches, which integrate multiple modalities using only clustering labels or dissimilarity matrices and not the original data (see, e.g. \u201cMulti-omic and multi-view clustering algorithms: review and cancer benchmark\u201d by Rappaport et al for a brief review of such approaches). Since the evaluation in the paper is based entirely on the quality of clustering based on the learned representation, this class of methods seem very relevant. Furthermore, there is no attempt to evaluate how well the model performs in comparison to models that leverage the entire dataset rather than just the distance-based structure. This makes it difficult to evaluate the real-world utility of the method.\n \nFinally, there is no reproducibility statement and no mention of code or data being made available."
            },
            "questions": {
                "value": "The above comments raise some broader questions regarding the method and particularly the evaluation approach. Some more specific questions are listed below:\n \n- Regarding the choice of the custom loss function for SNP distance, the choice to impose no penalty when feature distance and SNP distance both exceed 15 seems confusing. The cited source suggests that a distance <= 5 indicates a definite transmission  <= 15 indicates probable transmission. When clustering this data, might it also be useful to have a representation that accurately captures the relationships between samples that are even somewhat less likely to cluster together? If so, it could make sense to either relax this constraint or try a different normalization approach (such as log transforming the SNP distances).\n\n- The model requires the choice of a pretext task, and the authors suggest that the difficulty of the pretext task does not affect MSPL\u2019s ability to preserve structure. What, then, is the effect of the pretext task on the learned representation, and how should a user choose the pretext task for their particular application?  \n \nSome minor comments on Figure 5 that did not affect my score:\n-  e) and f) are missing species labels.\n-  The paper claims that F1 lift and species diversity are correlated based on c) and e) \u2013 a regression line or correlation statistic would be helpful to back up this claim."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposed a domain adaptation method that learns the data distribution structure in one modal and transfer it to the other modal. They applied it to the problem of hospital outbreak detection that using MALDI and whole genome sequencing."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The stated problem is pervasive in biomedical applications and is challenging."
            },
            "weaknesses": {
                "value": "1) This is a typical subset of domain adaptation problems. However, they did not include SOTA domain adaptation methods into the baseline. The baseline methods are weak.\n2) Also, from references we see that there are already methods that perform prediction tasks directly based on MALDI, which were not compared.\n3) The experiments are carried out only on MALDI-WGS datasets and most are synthetic datasets. Due to the small-sample nature of these problems, the models are vulnerable to short-cut learning and testing on several similar datasets is not reliable. I don't see any reason that the problem should be restricted on MALDI-WGS data. There are lots of two-domain problems with similar character in biomedical fields and the data should be tested on more types of applications."
            },
            "questions": {
                "value": "How does the \"seasonal and trend components\" in the synthetic datasets related to MALDI-WGS matching?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose the Multimodal Structure Preservation Learning (MSPL) approach that learns data representations utilizing clustering structure in one data modality to inform upon the other modality using a regularization approach towards compliance of this clustering structure when learning representations. The approach is applied to synthetic as well as whole genome sequencing (WGS) and antimicrobial resistance datasets. Rather than learning a shared feature space the approach thus relies on gross structural information at the level of groups exploring alignment according to the dissimilarity-based clustering learned by the opposing modality. The approach relies on three tasks, an autoencoder for learning representations, a pretext discriminatory task, and  alignment of the two modalities clustering structure formulated as a multiobjective function reflected in three loss terms with associated relative weights. Apart from conventional ARI and NMI cluster validity metrics the authors further propose a cluster-based F1 score. The approach is compared against two model ablations (baselines) not having the structure preserving loss and classifying the cluster groups respectively as opposed to operating on dissimilarities."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The approach is useful and enable to integrate information of multiple (two) modalities taking overall structural information into account from the opposing modality.\n\nThe considered problem domain is interesting and the approach\u2019s seem to enhance the learned representations in terms of cluster level structures.\n\nThe paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "The methodological contribution of the paper is very limited and rather straightforward combining three loss components. As such, the contribution seems rather incremental and limited in scope. \n\nThe contribution of the F1 metric is also straightforward and does not contribute much in terms of novelty.\n\nThe comparisons are very limited only considering simple model ablations but not any alternative state-of-the-art methodology for the same problem domain. \n\nThe results are not overly convincing with the approach working better than baselines in some situations and not in other.\n\nOverall I find the contribution of limited novelty and the experimentation not overly convincing - and therefore do not recommend publication at this point."
            },
            "questions": {
                "value": "It would be good to further discuss how to suitable tune the contribution of each loss term.\n\nHow is the approach influenced by initialization conditions?\n\nHow does architectural choices influence the model and why is UNETs chosen as the backbone as opposed to other architectures such as transformer based architectures?\n\nWhy is the approach not compared to any existing SOTA approaches within the domain or similar domains for instance based on the approaches reviewed in related work?\n\nThe results are also not that surprising in that regularizing towards a clustering structure will enhance such learning of the clustering structure. It would in this context be interesting to see if the regularization also improves upon the pretext class and contrast this to other methodologies directly learning the pretext class."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}