{
    "id": "h8yg0hT96f",
    "title": "BAYESIAN EXPERIMENTAL DESIGN VIA CONTRASTIVE DIFFUSIONS",
    "abstract": "Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce the cost of running a sequence of experiments.\nWhen based on the Expected Information Gain (EIG), design optimization corresponds to the maximization of some intractable expected  *contrast* between prior and posterior distributions.\nScaling this maximization to high dimensional and complex settings has been an issue due to BOED inherent computational complexity.\nIn this work, we introduce an *expected posterior* distribution with cost-effective sampling properties and provide a tractable access to the EIG contrast maximization via a new EIG gradient expression. Diffusion-based samplers are used to compute the dynamics of the expected posterior and ideas from bi-level optimization are leveraged to derive an efficient joint sampling-optimization loop, without resorting to lower bound approximations of the EIG. The resulting efficiency gain allows to extend BOED to the well-tested generative capabilities of diffusion models. \nBy incorporating generative models into the BOED framework, we expand its scope and its use in scenarios that were previously impractical. Numerical experiments and comparison with state-of-the-art methods show the potential of the approach.",
    "keywords": [
        "Bayesian Optimal Experimental Design",
        "Conditional Diffusion Models",
        "score based sampling",
        "Bayesian Inverse Problems",
        "Experimental Design",
        "Sampling as Optimization"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "The paper introduces an efficient BOED method leveraging diffusion-based samplers and bi-level optimization ideas to jointly sample the introduced expected posterior and maximize Expected Information Gain, enabling larger-scale applications.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=h8yg0hT96f",
    "pdf_link": "https://openreview.net/pdf?id=h8yg0hT96f",
    "comments": [
        {
            "title": {
                "value": "Follow up #2"
            },
            "comment": {
                "value": "## What we disagree on\n- **\"it can be shown the ACE gradient in equation 16 of Foster et al. 2020 is equivalent to the estimator in this paper since the L contrastive samples are used in a similar manner as the contrastive samples from the expected posterior in this paper\"** \n\nThis is not the case. The $I_{ACE}$ gradient eq (16) in Foster et al 2020 is different from what we propose. Note that the symbol $g$ is used for different things in (16) and in our eq (7).\n Our estimator and an estimator of equation (16) of Foster et al. 2020 target different quantities. Ours targets the gradient of the EIG and theirs targets the gradient of a lower bound of the EIG. They have no reason to be equivalent. The L contrastive samples are not used in a similar manner as our samples from the expected posterior. They are not drawn from the same distribution and contribute differently to the two estimators:\nFoster has the form of a log of a sum: $-\\log \\left(\\frac{1}{L+1} \\sum_{\\ell=0}^{L} \\frac{p(\\theta_\\ell)}{q_\\phi(\\theta_\\ell | y_i)} p(y_i | \\theta_\\ell, \\xi)\\right)$ while ours is:  $-\\frac{1}{M} \\sum_{j=1}^{M} w_{i, j}\\; \\nabla_\\xi \\log p(y_i | \\theta_j, \\xi)$\n\n\n- **\"SNMC upper bound for their method is much worse than the others\"**\n\nThe reviewer's claim is not correct. The SNMC upper bound of CoDiff is above all the other methods. As the true EIG is likely to keep increasing with the number of experiments, it is usually considered as a good feature that the upper bound is not itself bounded or does not plateau. See for instance a similar behavior in Tables 1 and 3 of Foster et al 2021 or in Table 1 of Blau et al 2022. \n\n\n\n\n- **\"no code to reproduce the results\" and the \"architecture of the diffusion model\", \"how is the diffusion model pretrained since it's not being optimized in Algorithm 2\"**\n\nPlease note that all the code is provided in the supplementary material. The architecture used is based of an Unet and is implemented in diffuse/unet.py. The diffusion model is pretrained in the same way as in Song et al. 2021. A brief overview is given in appendix C.1.\n\n\n\n\nWe hope we made the above points clearer. We also value and keep in mind your other suggestions, in particular on how to improve the readability of our paper. Synthetizing yours and the other reviewers comments, we will try to propose some improvements in our next answers."
            }
        },
        {
            "title": {
                "value": "Follow up #1"
            },
            "comment": {
                "value": "Thank you for your reactivity. We appreciate your acknowledging your assumption was incorrect. Your response raises interesting questions but we feel several statements made are ill-informed and some are inaccurate. We try to provide further clarifications and point out where we disagree below. To make clearer to which of your comments we reply, we put them in bold characters.\n\n## Clarifications\n-  **\"If this isn't a lower bound on the information or an approximation, what is the EIG they are optimizing? Indeed, if it's not a lower bound by the InfoNCE using the method from Foster et al 2020, then what is it?\"**\n\nWe do understand the core of your question and we feel it is an interesting question: You are asking what is the function we are optimizing with our new EIG gradient estimator and if by any chance this function is a lower bound or is linked to an existing lower bound?\nTo make it short first, the answer is actually in the question: since we start from an EIG gradient, the function we optimize is the EIG! But the detailed answer is longer, let's explain better. \nAs explained in our related work section lines 114 to 120, our approach belongs to approaches that **do not** start from an EIG lower bound to compute then the gradient of this lower bound. Instead, we try to handle directly the EIG gradient. We compute an exact (but intractable) expression of the gradient and then try to estimate it. So doing, the function we optimize is ideally the EIG itself and speaking about a bound is not useful anymore. \n**However**, the reviewer question is interesting because in practice of course the EIG gradient has to be approximated, for instance with our equation (10). Then it would be interesting to express the right-hand-side of (10) as the gradient of something. This something would be the \"estimator\" the reviewer is asking for. Unfortunately, it is not easy to express (10) as a gradient over $\\xi$ because $w_{ij}$ depends on $\\xi$ in a non simple manner. So to answer the question about what is this estimator we are optimizing, we are not aware of an explicit simple expression of this estimator that could be useful or interpreted as a bound or not. But **note that the whole point of our approach (like that of Goda et al and Ao and Li) is that we target the EIG itself and not a bound**. We are not saying that this is necessary always better but the spirit is different. Also, another possible justification of this direct approach is that, in gradient-based BOED, the EIG value is not needed, we just need to maximize it.\n\n\n\n- **\"In line 365 the authors claim that running a conditional diffusion model is intractable yet that's exactly what this paper does so I don't think that method can be ruled out.\"**\n\nAs explained in Appendix C.2, running a conditional diffusion model without retraining the score function is intractable as we cannot directly evaluate $\\nabla_{\\theta} \\log p_t(y|\\theta^{(t)}, \\xi)$. There are many recent works tackling this problem and we refer to Daras et al., 2024 for a recent extensive review. While we appreciate the reference you provided, we find that it addresses relatively simple synthetic examples that may not generalize well to the MNIST outpainting tasks that we are tackling.\n\n- **\"you should state for the BOED community that you rely on explicit likelihoods\"** \n\nPlease note that we do state this on lines 41, 219, 360, 432, 709 and 1000.\nIn line 536 we then mention, as future work, possible extensions of our method, with no explicit expression of the likelihood. We can point this as a current limitation of our method."
            }
        },
        {
            "title": {
                "value": "Expected posterior choice and other methods on MNIST"
            },
            "comment": {
                "value": "Thanks for your thorough overall positive evaluation and interesting comments. \n\n- **Expected posterior and $\\nu_i$ choice**:\n\nNote first that in our paper, there is no such a thing as a \"true\" expected posterior but we realize that these terms can be misleading. \"Expected posterior\" is just the name we give to the geometric mixture we take as a proposal $q$ in eq (7) or (8). We justify this choice theoretically with lemma 2 in Appendix B.\n\nWe understand then, that the reviewer means that the prior would be a good alternative choice for the proposal distribution $q$ in eq (7) or (8) because the prior can be written as $p(\\theta) = E_{p(y|\\xi)}[p(\\theta|y, \\xi)]$.\n\nOur choice of the geometric mixture instead is motivated by its optimality in terms of importance sampling. Our lemma 2 shows that the geometric mixture of the $p(\\theta|y_i, \\xi)$ is the distribution $q$ that minimizes the sum of the $KL(q,p(\\theta|y_i, \\xi))$. This property is not satisfied by the prior. \n\n\nMore intuitively, the big difference between our expected posterior and the prior is the dependence of the former on $y_i$ and $\\xi$. In particular, in our sequential BOED context, using the prior as a proposal in our gradient estimator would remove the dependence on the design $\\xi$, which would remove the good properties explained lines 198-202. \n\n\n\nValue $\\nu_i$ are set as $\\frac1N$ but other choices are possible.\n\n- **Other methods on MNIST**:\n\nThe main bottleneck of other methods is the number of needed $\\theta$ samples to accurately optimize the EIG. For amortized methods like RL-BOED (Blau et al 2022 ) and DAD (Foster et al 2021) a high number of samples is needed to train the policy (eg 2000x2000 from Appendix D of DAD,  10 000 from Appendix C of RL-BOED).\nFor images, you run out of memory on a GPU before 500 samples. So the training loops are already infeasible.\nOn top of that, each time you need a new batch of samples you need to go through the whole denoising process again, which can become also time expensive. Both of the methods are practical when you have access to a $p(\\theta)$ you can sample from efficiently.\nFor Pasoa (Iollo et al. 2024) the number of samples needed for them to be efficient is higher than our CoDiff (see Fig. 3). There is also no straightforward way to do a usual Sequential Monte Carlo inference for images like MNIST. Their separate optimization/inference framework is also more time expensive than ours."
            }
        },
        {
            "title": {
                "value": "Follow up to clarification about prior work"
            },
            "comment": {
                "value": "I thank the authors for answering some of my questions so quickly. Indeed, I was referring to LF-ACE from \"A Unified Stochastic Gradient Approach to Designing Bayesian-Optimal Experiments\". I appreciate the authors taking the time to point out the gradient is different but they still miss a core of my question. If this isn't a lower bound on the information or an approximation, what is the EIG they are optimizing? I'm not talking about the gradient of the estimator but the estimator itself. Indeed, if it's not a lower bound by the InfoNCE using the method from Foster et al 2020, then what is it? \n\nAs for my comment about a direct approximation of the EIG, the authors are correct that they do not state that explicitly in their work and I was wrong. I assumed it by extension from my previous question and the allusion in line 114. I see that the authors are relying heavily on the work of Goda et al. 2022 and Ao and Li, 2024. I'll note they also do not explicitly state which bound of the mutual information they use. Following equation 2 from that paper, \n$U(\\lambda) = \\mathbb{E}_{p(\\theta|y, \\lambda)}\\left[\\log l(y|\\theta, \\lambda)\\right] - \\mathbb{E}_{p(y|\\lambda)}\\left[\\log p(y|\\lambda)\\right],$\n I can assume it's the InfoNCE lower bound since this the log ratio of the likelihood to marginal likeliihood. By extension, it can be shown the ACE gradient in equation 16 of Foster et al. 2020 is equivalent to the estimator in this paper since the L contrastive samples are used in a similar manner as the contrastive samples from the expected posterior in this paper. I'm dwelling on this because we know you can improve the information gain by increasing L but I'm encouraging the authors to think about how the expressiveness of the score function that approximates the expected posterior can be improved either by choice of importance sampling or neural network.\n\nI'm also still wondering why the SNMC upper bound for their method is much worse than the others. It honestly doesn't matter in practice but it's a weird result to display.\n\nI have more questions and comments after thinking this through and thanks to the authors clarification:\n- What is the architecture of the diffusion model that's central to this paper? There's no mention and no code to reproduce the results. Appendix F has some details but I think more is needed to reproduce the results.\n- How is the diffusion model pretrained since it's not being optimized in Algorithm 2?\n- In line 365 the authors claim that running a conditional diffusion model is intractable yet that's exactly what this [paper](https://arxiv.org/abs/2209.14249) does so I don't think that method can be ruled out.\n- I agree with the other reviewers you should state for the BOED community that you rely on explicit likelihoods. \n\nI apologize to the authors for being so pedenatic. I find this paper interesting and would like to understand it better with their help in explanations. Right now I'm left confused by the organization and some missing pieces that I think we can resolve."
            }
        },
        {
            "title": {
                "value": "Clarification request about prior work"
            },
            "comment": {
                "value": "We thank the reviewer for this review and for acknowledging the \"interesting contribution\" and \"great connection\" made by our paper.\nWe are of course willing to clarify all points raised, but we are currently facing some difficulties in understanding some of the reviewer critiques. The reviewer claims we have \"misunderstood the basis of BOED\" and refer to a reference \"Fotster et al 2019b\" and to LF-ACE. We suspect a misspelling of \"Foster et al 2019b\", but such reference does not exist in our bibliography. Does LF-ACE instead refer to \"Likelihood-free ACE\" in section 3.4 of \"Foster et al 2020- A Unified Stochastic Gradient Approach to Designing Bayesian-Optimal Experiments\" ?\n\n\nLF-ACE, $I_{ACE}$, and InfoNCE have the same structure that leads to gradient estimations different from ours. Taking $I_{ACE}$ from eq. (11) of Foster et al 2020:\n\n$$\nI_{\\text{ACE}}(\\xi, \\phi, L) \n= E_{p(y | \\theta_0, \\xi) p(\\theta_0)} \\left[ \\log p(y | \\theta_0, \\xi) - \\log \\left(\\frac{1}{L+1} \\sum_{\\ell=0}^{L} \\frac{p(\\theta_\\ell)}{q_\\phi(\\theta_\\ell | y)} p(y | \\theta_\\ell, \\xi)\\right) \\right]\n$$\n\nAs visible in eq. (19) of Foster et al 2020, its gradient $\\nabla_\\xi I_{ACE}$ differs from our gradient form: \n\n$$\n\\nabla_\\xi EIG(\\xi) = E_{p(y|\\theta_0,\\xi)p(\\theta_0)} \\left[ \\nabla_\\xi \\log p(y | \\theta_0, \\xi) - E_{q(\\theta' | y, \\xi)} \\left( \\frac{p(\\theta' | y, \\xi)}{q(\\theta' | y, \\xi)} \\nabla_\\xi \\log p(y | \\theta', \\xi) \\right) \\right]\n$$\n\n\n$I_{ACE}$ is both a lower bound of the EIG and  based on a variational approximation of the posterior, which is orthogonal to our approach. Also, we do not use a variational approximation of the posterior and instead use diffusion based samplers. \n\nFor this reason, we believe the statement from the reviewer that our method \"optimizes a lower bound of the mutual information by the InfoNCE bound\" is not correct. Also, nowhere in the paper we write that our \"approximation is exact\". We focus mainly on the EIG gradient and not so much on the EIG itself. We do not emphasize the nature of the EIG approximation, which could be a lower bound, an upper bound or not a bound at all. \nHaving said that we might have  \"missed prior work that might better-inform our method\" as the reviewer suggests. In such case, we are open to discussion but would appreciate specific references and detailed insights in what we missed."
            }
        },
        {
            "summary": {
                "value": "This paper presents a new expression for the gradient of the expected information gain (EIG)---a central quantity in Bayesian optimal experimental design (BOED). Their method, named CoDiff, combines the two steps of maximizing the EIG and then sampling from the posterior in BOED into a single joint step by leveraging the sampling-as-optimization setting of e.g. Marion et al. (2024). Doing so results in a more efficient BOED method, as demonstrated by numerical experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper advances the field of BOED by making it more computationally efficient, whilst broadening the scope of problems that can be tackled by BOED by extending it to diffusion-based generative models. The idea of performing joint sampling and optimization using bi-level optimization seems novel to, although I am not very familiar with this literature. The experimental results are significant and demonstrate the efficacy of CoDiff compared to other baselines. The method is presented clearly along with discussions w.r.t. to the related works. Overall, this is a well-written and technically solid paper, and I am happy to recommend its acceptance."
            },
            "weaknesses": {
                "value": "Some minor points:\n* It would be nice to see some uncertainty bars around the results in Figure 3.\n* Parts of Section 4 and 5 were a bit difficult for me to follow, but that could just be because I am not familiar with the relevant literature. However, if possible, it might be a good exercise for the authors to think about making these parts a bit more accessible to the BOED community who are not familiar with the sampling-as-optimization literature.\n* It would have been nice to empirically test the performance of CoDiff as e.g. the dimensions of $\\theta$ or the hyperparameters (if any) vary.\n* Please include some limitations of CoDiff in the conclusion section."
            },
            "questions": {
                "value": "See the \"Weaknesses\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper is a little outside of my expertise and I apologize in advance for any misunderstanding I may have and look forward to any correction from the authors. I have set my confidence to low accordingly.\n\nThe authors consider the problem of experimental design, given a statistical model of the form $P(y | \\xi, \\theta)$, we can choose values of $\\xi$ and observe a corresponding $y$, and the goal is to infer $\\theta$. In the first example in the paper, $\\xi$ are locations on a 2D plane, $y$ are the measurements at the $\\xi$ locations, and $\\theta$ represents the source locations of contamination in the plane.\n\nAssuming we have chosen the $\\xi$ values, we can use the $p(y|\\xi)$ to simulate $y_i$ values, one hallucinated rollout into the future, then infer the hallucinated posterior $p(\\theta'|y_i, \\xi)$ and measure the posterior distribution's entropy, $H(p(\\theta'|y_i,\\xi))$, typically requiring Monte-Carlo over the $\\theta'$. We may repeat this process many many times for $i=1,...,N$, and compute the average posterior entropy, wrapping the inner Monte-Carlo over $\\theta'$ in another outer Monte-Carlo over $y$. This average posterior entropy is the Mutual Information (MI) between $y$ and $\\theta$ up to an additive constant (that is the prior entropy $H(p(\\theta))$). This MI quantifies the benefit of sampling $y$ at $\\xi$, we can therefore change $\\xi$ in a way to maximise MI and this is a standard method for experimental design known as Expected Information Gain, or EIG.\n\nHowever as evaluating EIG requires performing nested Monte-Carlo, and optimizing $\\xi$ requires the gradient of EIG, much work has focused on various methods to construct efficient EIG gradient estimators. In past works and this works, the second inner integration over $\\theta'$ is performed with MCMC or by Monte Carlo with importance sampling using a proposal $q(\\theta'|y_i)$ that must be updated for every outer MC iteration $i$.\n\nThis work proposes to use a single proposal distribution across all outer MC iterations $i$ called the \"expected posterior\" which is a geometric mixture of all the hallucinated posteriors\n$$\nq(\\theta') \\propto \\prod_i p(\\theta|y_i, \\xi)^{\\nu_i}\n$$\n\nThe paper proposes to Langevin dynamics to generate samples for the outer Monte Carlo $\\\\{y_i, \\theta_i\\\\}$ from the joint distribution. As these samples are updated in a iterative manner, and the goal is to iteratively optimize $\\xi$, these nested iterative procedures can be merged and updated together, if $\\xi$ only incrementally moves, it is wasteful ti discard all the old samples $\\\\{y_i, \\theta_i\\\\}$ but keep incrementing them so they're up-to-date.\n\nFinally the proposed method is applied to two sequential design applications, the first is the source location example in which the $p(\\theta)$, $p(y|\\theta, \\xi)$ have known analytical forms and the proposed method outperforms baselines. The second is where $p(\\theta)$ cannot be evaluated but can be sampled, specifically a diffusion model trained on MNIST images, $\\theta$ is a full image,  $\\xi$ are pixel locations, and $y$ are the 7 X 7 patch of pixel values around $\\xi$. The goal is to infer the full image by cherry-picking image patches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- rigorously cites and discusses related ideas\n- merging multiple nested loops into a single loop is intuitive idea and has been applied in multiple separate fields (training a VAE is effectively the EM algorithm was simplified into a single step, in Bayesian optimzation few-shot Knowledge Gradient)\n- the writing is clear and each idea is easy enough to follow."
            },
            "weaknesses": {
                "value": "### Technical Comments\n- __expected posterior__ the paper states that they aim to find one proposal distribution that covers all possible future posteriors and proposes a distribution that is a geometric mixing of hallucinated posteriors $q(\\theta) \\propto \\prod_i p(\\theta|y_i, \\xi)^{\\nu_i}$. The prior is the true expected posterior $p(\\theta) = \\mathbb{E}_Y[p(\\theta|y)]$, exactly the average density of all possible posterior densities.  Why not use the prior $p(\\theta)$ as the one global proposal distribution for integration over all the hallucinated posteriors? In the sequential case (introduced at the end of the paper)\n\n### Writing Comments\n- __Writing Density__ while the writing is thankfully clear, the paper cites and quickly introduces many ideas from prior works in quick succession making the paper very hard work to read for me, a lot of context switching making the paper very tiring. I would suggest reducing the details of referenced works, and providing higher level descriptions where possible and elaborating more on the novel parts of the proposed method."
            },
            "questions": {
                "value": "- Can the authors provide an intuition as to why the proposed expected posterior is preferable to the true expected posterior = prior? Is this purely for the sequential setting?\n- how are the $\\nu_i$ values determined in the expected posterior?\n- what stop other methods from being used on the MNIST example? Is it just the computational cost from multiple nested loops and the sequential use case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper makes an interesting connection between Bayesian optimal experimental design (BOED) and optimization by sampling using a diffusion model. While an interesting connection that may prove fruitful with some refinement, the authors seem to misunderstand the basis of BOED and maximization of a lower bound of mutual information using contrastive learning, as well as miss prior work that might better-inform this method. This disconnect, as well as confusing notation regrading how the diffusion model is used in multi-round BOED and lackluster empirical evaluation, makes me think this work is too early and could benefit from refinement in writing and additional experimental evaluations of the method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The authors make a great connection to optimization by sampling via diffusion models. \n- Thorough gradient analysis.\n- The connection to inverse problems e.g. inpainting in MNIST images as a BOED problem is an interesting new task in the BOED setting and could see BOED objectives (really mutual information objectives) gain further adoption in generative modeling."
            },
            "weaknesses": {
                "value": "While the authors make a great connection to optimization by sampling, theoretical analysis of the convergence of the desired goal distribution is missing. This would e.g. especially be helpful in the multi-step BOED setting to show how to improve bounds of the mutual information. I list out my critiques and questions below:\n\n- The authors miss that they are also optimizing a lower bound of the mutual information by the InfoNCE bound, thus their comments that their approximation is exact is incorrect.\n- The definition of information in Equation 2 does not make sense. The KL divergence is written in a non-standard way - do you mean $E_{p(y|\\xi)}KL(p(\\theta|y, \\xi) || p(\\theta))$?\n- The authors miss the connection to Fotster et al. 2019b of LF-ACE that uses a posterior estimate in the InfoNCE bound to refine samples. e.g. in equation 5. \n- The notation for the expected posterior in equation 9, it's not clear what is the proposal distribution $q(\\theta)$ in the paper. Are you using likelihood evaluations? Also, how are you choosing the weighting of $\\nu_i$?\n- I would prefer to see the notation for iterative sampling operators similar to Marion et al. 2024, where the Y, and $\\Theta$ are variables of the operator instead of in the superscript. I think that would make the optimization problem more clear. \n- After equation 15, are you using two generative models to define the observed distribution $y$ and the parameter distribution $\\theta$?\n- in terms of the location finding experiment, usually we would expect to see the designs and final posterior distribution. the way it's portrayed in the paper is confusing. I'm not sure which samples are shown, which design round the designs come from, and why the seeming optimal design isn't on the ground truth values. It looks like the chosen design is quite far from the true theta_0.\n- While the EIG seems to improve over others in Figure 3, the upper bound is _much_ worse. Why is that? \n- The MNIST experiment is interesting but lacks quantitative evaluation. Maybe including a classifier and using the number of successfully classified images would help? Also, there seems to be opportunity to change the design size to see how that influences outcomes.\n\nIn summary, I think the notation is a little bloated. The authors tend to repeat themselves in the text and could be more concise. Concision would help make space for exposition of more salient points that would improve this paper. Additionally, it would help the authors to explicitly lay out what are the likelihood, posterior, and priors they use in experiments. Also, it would be nice to see calibration of the posterior that results of the final design round using e.g. the l-C2ST method or other simulation based calibration methods. Most applications of amortized posteriors using BOED will be interested in this, but it can be computationally expensive, which is why I think just on the final round is sufficient. This is an interesting method that I think needs cleaner exposition."
            },
            "questions": {
                "value": "My questions are contained in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}