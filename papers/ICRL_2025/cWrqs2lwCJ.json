{
    "id": "cWrqs2lwCJ",
    "title": "Thinking Forward and Backward: Effective Backward Planning with Large Language Models",
    "abstract": "Large language models (LLMs) have exhibited remarkable reasoning and planning capabilities. Most prior work in this area has used LLMs to reason through steps from an initial to a goal state or criterion, thereby effectively reasoning in a forward direction. Nonetheless, many planning problems exhibit an inherent asymmetry such that planning backward from the goal is significantly easier --- for example, if there are bottlenecks close to the goal. We take inspiration from this observation and demonstrate that this bias holds for LLM planning as well: planning performance in one direction correlates with the planning complexity of the problem in that direction. However, our experiments also reveal systematic biases which lead to poor planning in the backward direction. With this knowledge, we propose a backward planning algorithm for LLMs that first flips the problem and then plans forward in the flipped problem. This helps avoid the backward bias, generate more diverse candidate plans, and exploit asymmetries between the forward and backward directions in planning problems --- we find that combining planning in both directions with self-verification improves the overall planning success rates by 4-24% in three planning domains.",
    "keywords": [
        "LLM planning",
        "backward search"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose a new planning algorithm for LLMs to effectively exploit backward search in different planning problems.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cWrqs2lwCJ",
    "pdf_link": "https://openreview.net/pdf?id=cWrqs2lwCJ",
    "comments": [
        {
            "summary": {
                "value": "The authors propose a method that uses large language models (LLMs) for planning by \u201cflipping\u201d the problem. In this approach, the initial and goal states are reversed, a plan is generated by the LLM, and then this plan is reversed to solve the original problem. The authors demonstrate that this method can enhance the planning performance of LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper conducts a thorough set of experiments to test whether flipping the initial and goal states of problems improves LLM performance. The authors also examine whether the LLM can reason about whether or not to flip the problem."
            },
            "weaknesses": {
                "value": "The impact of the proposed approach is somewhat unclear. For example, prior research [1] has shown that LLM planning performance significantly deteriorates when the syntax of the planning domain is obfuscated, even when the causal structure remains intact (referred to as \u201cMystery\u201d domains in [1]). It would have been insightful to see how this approach performs in such obfuscated domains. This also suggests that the computational complexity of the underlying domain doesn\u2019t necessarily affect LLM performance. Therefore, flipping the problem\u2019s initial and goal states to reduce computational complexity might not necessarily benefit LLMs, as they may not be sensitive to these complexity reductions in the same way formal planners are.\n\nBased on my understanding, the method described in Section 4 (always flipping the original problem) aligns with the \u201cFlip\u201d approach in Section 5. Flip performs worse than \u201cFwd\u201d planning, contrary to what is suggested in Section 5. The proposed approach mentioned in Section 5 appears to be \u201cFwdFlip,\u201d where the decision to flip is made at random. It would be helpful to clarify the exact nature of the proposed approach. Additionally, it would be valuable to see details within the FwdFlip method, such as the success rates within Fwd and Flip respectively."
            },
            "questions": {
                "value": "1. Are the results of Flip related methods based on the solution to the flipped problem or the original problem? If it is w.r.t the original problem, how are the errors distributed between the solution of the flipped problem being wrong and the flipping of the plan being wrong?\n\n2. Is the flipped initial state in Blocksworld verified given that the LLM is generating a full state from the partial goal state?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper uses LLMs for planning.  In particular it looks at the difference between planning forward (from initial state to the goal) vs planning backward (from the goal to the initial state), and notes that backward planning may work better for some problems.  For problems where backward planning is more efficient, the paper introduces a method for flipping the problem so that forward planning can be used from the goal to the initial state.  This leads to the question: Can LLMs plan better if they reason in the backward direction?\u201d"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea of flipping the problem has merit for some problems.  With more careful consideration of the general claims about the capabilities of LLMs (many of which actually seem problem dependent) and a more clear definition of which problems this will work for and which it will not, and why (including a discussion of what makes some actions non-invertible), this would be a strong paper."
            },
            "weaknesses": {
                "value": "Overall: Many general claims are made about the capabilities of LLMs (e.g. they exhibit a systematic bias towards forward planning), and these claims are backed by experiments.  However, it's not clear that the experimental results justify these broad claims.  Many of the experimental results seem very domain-dependent.  For example, Directed Graph Planning has a built in directional bias. \n\n1.\tThe paper uses the term \u201cbackward planning\u201d but all the planning is actually in the forward direction, it\u2019s just that sometimes they \u201cflip\u201d the problem (making the old goal the new initial state and the old initial state the new goal). \n2.  The method proposed for flipping the problem assumes that all actions can be inverted, and it is not clear that this is always the case (so the applicability of \u201cflipping\u201d will depend on whether or not the actions in the domain are, in fact, invertible). \n3.\tRegarding the claims that \u201cLLM exhibit a systematic bias of performing worse when planning backwards\u201d may be attributed to the forward auto-regressive nature of LLM output generation or training data biases:  There is not enough evidence to support this as a general claim.  Experiments were conducted with only three domains, and at least one (Graph Planning) has by its very nature a built in bias in the forward direction.\n4.\tFigure 2 as an algorithmic description is not clear, and does not seem to match up with the text in lines 168-17.\n5.\tThe general conclusions about LLMs drawn from the data presented in Figure 3 (LLMs plan better in the forward direction, and that flip solves this bias) seem suspect given that the directed case in this domain has a built in bias to forward planning so it\u2019s not surprising at all that flip would do well in this case.  \n6.\tAlso, regarding Figure 3; most of the cases where flip outperforms forward planning are also cases where backward planning already also outperformed forward planning."
            },
            "questions": {
                "value": "1.   What is it about some actions that make them non-invertible?  How does this affect your proposed method (which assumes that all actions are invertible)?\n2.   Is it important that inverse actions are actually feasible actions that respect physics and make sense?  (e.g. if an action is to \u201cdrill a hole\u201d, you can\u2019t really un-drill a hole; does this even matter?).\n3.\tBi-directional search is mentioned in related work, but it\u2019s not mentioned at all in the text.  The approaches that involve both forward and backward planning seem very similar to bi-directional search and may warrant more discussion in the paper.  Are there lessons from bi-directional search that are relevant here?\n4.\tDoesn\u2019t the Graph Planning domain have built in bias for directed graphs?  Meaning they may only be solvable in one direction?\n5.\tIn the description of how planning problems are flipped; the authors assume that every action has an inverse action.  It\u2019s not clear that this would always be true.  Could you please include discussion of this?  Could you give examples where there is not an inverse action?  (e.g. if an action is to \u201cdrill a hole\u201d, you can\u2019t really un-drill a hole can you)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the possible usage of LLMs as an automated planner for solving classical planning tasks (deterministic, fully observable). Especially, the paper studies the regression planning approach. The paper also proposes a heuristic approach that switches the initial state and the goal depending on the progress of solving the problem. Overall, experiments were conducted on path finding in graphs, array transformation, and the blocksworld domains."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This paper explores regression planning with LLMs. It seems like the authors are not aware of the literature on automated planning and classical AI, from what's written. so it is an attempt to re-invent the known results in the 70s to 90s in the context of large language models."
            },
            "weaknesses": {
                "value": "This paper mentions bidirectional search in the related work. However, it is mentioned nowhere in the paper about regression planning. \"Thinking forward and backward\" reminds me of a fancy title \"Thinking fast and slow\" and the title also shows effective backward planning. However, it is doubtful if that's the case.\nIf we hold back and remove the LLM portion, then the paper suggests solving planning problems by alternating the initial state and the goals. In general, goals are not a single state as the initial state in most of the problems. It is doubtful how such a switching strategy can lead to systematic improvement as we can always choose one of the goal states as a new initial state in a different problem. \nOne of the weaknesses here is the lack of coverage of the relevant literature.\n\nThere are several misleading statements such as \n\"classical planning algorithms such as BFS\" line 141. ==> BFS is not a classical planning algorithm\n\"bottleneck\" reminds me of the concept of information bottleneck in information theory. However, it is not relevant to this context.\nIt is not clear whether the \"bottleneck effect\" is relevant in this context.\n\n\nThe details of the implementation of the planner are vague. \nHow possible actions are generated? How the next state will be generated given actions?"
            },
            "questions": {
                "value": "Question 1. What are the problem statistics of the three problem domains? They are all classical planning problems so we could see the action schema, and the number of state predicates to have a sense of the difficulty of the problem. What problem instances were tested? How many objects were tried? How the initial states and the goals were determined? It is not difficult to write down PDDL specifications for those problems.\n\nQuestion 2. Given a state observation, how the next actions are generated? Are the next actions sound and applicable to each state always? \nHow the next state is generated? How the goal test was done? \n\nQuestion 3. How the natural language reasoning version of the classical planning problems are obtained, for the graph planning an array transformation? Are they a natural description of the problem? Or is it utilizing artificial patterns? \n\nQuestion 4. Blocksworld problem would be the only single problem domain that has multiple goal states.\nIn regression planning, the difficulty is to maintain a set of states instead of a single state if we proceed to perform a search. \nThe experiment results also confirm such known issues. In the other two domains, regression modes are claimed to perform well. But I think it depends on the distribution of the initial state and the goal state, not the goal states here. I think if we simply flip the initial state and the goal state, we will obtain the opposite results. Could you explain this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper first investigates the performance of LLMs on a set of planning problems where both forward and backwards planning can be done (3 domains are given in the paper: shortest path in graphs, array and Blocksworld). The paper measures the difficulty of planning in a particular direction with the size of search space (in that direction). Based on this, the authors empirically observe and claim in Sec. 3.3 that 1) LLMs, in general, plan better in the \"easier\" planning direction and 2) ceteris paribus, LLMs plan worse in the backwards direction as compared to the forward direction. The authors hypothesize that 2) \"... may be attributed to the forward autoregressive nature of LLM output generation, as well as biases from the training dataset.\"\n\nThen, the paper, in Sec. 4, proposes a method which flips the problem backwards and plan forward (while handling some subtleties) for the flipped problem. Combined with self-verification and some sampling, the experiments show that their method outperforms other baselines over the 3 planning domains."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper's approach in developing the main method is, to a certain extent, systematic. It first verifies that some planning problems are easier solved from the backwards direction (due to smaller action spaces) then show that LLMs perform better in the direction which is easier. However, the paper also found that LLMs cannot plan well directly in the backward direction. Combining these two observations, the paper then introduces its method of flipping the backward problem.\n\n2. The experimental results are laid out in a systematic manner, where each section tries to explain a certain statement or claim in the paper.  The experimental setup is also done well, although I have some comments about how the authors tried to link the results with certain claims (see next section)."
            },
            "weaknesses": {
                "value": "I have several concerns regarding how claims in the papers are supported by the experiments. These concerns also impact the novelty and soundness of the authors' main method (**Fwd-Flip**).\n\n\n1. The authors mentioned that **Fwd** is the standard forward planning method, **Flip** is applying the forward planning method to the flipped problem, and **Fwd-Flip** is \"randomly choose either forward direction of the original problem or forward direction of the flipped problem and then plan\". Therefore, the only difference between **Fwd-Flip** and **Fwd**/**Flip** is that instead of deciding which direction to plan, the planning direction is randomly selected. However, the experimental results in Table 2 show that **Fwd-Flip** achieves much higher success rate than **Fwd** or **Flip**. To me, this is result is questionable because if **Fwd-Flip** is just selecting **Fwd** or **Flip** randomly, how can it perform much better? In fact, I would expect, with just random sampling, **Fwd-Flip** to perform exactly the average of **Fwd** and **Flip**? I also noticed also that Fwd-Flip is different from what was written in the introduction, which is \"Given a planning problem, we ask LLMs to sample possible plans in the forward direction of both the original problem and the flipped one, and then self-verify (Stechly et al., 2024) all the plans before choosing the final one\". Could the authors clarify?\n\n\n2. I also noticed that both **Fwd** and **Flip** (on their own) have similar performances over all planning tasks in Table 2. This implies that on average, flipping the problem has no benefits over the direct forward planning method. While I understand that the authors observed that certain planning directions could be easier in some tasks (due to difference in action spaces), it is clear from the experiment results that a particular planning direction is not inherently better in any of the tasks. Hence, I argue that the paper's main method, **Fwd-Flip** does not actually exploit this observation effectively. I have a suggestion on how the method can be improved: during test-time, we can cleverly decide whether the problem is easier if we flipped it (using some simple heuristics). Then, we can either apply **Fwd** or **Flip** based on our heuristic; this is more sound than **Fwd-Flip** proposed in this paper, which is simply selecting **Fwd** or **Flip** *randomly*. In later experiments, the authors do use the LLM to reason about which planning direction to use, but its effectiveness seems to be restricted to Undirected Graph Planning.\n\n\n3. Some claims in the papers are briefly stated in words but are not clearly substantiated from the experiments. For example:\n* Sec. 4 claims that flipping the problem \"... avoids the bias of weak LLM planning in the backward direction.\" However, Figure 3 shows that for Directed Graphs, for problems with no difference in forward and backward computations, flipping still performs worse than the forward problem. So, it is unclear if flipping the problem entirely avoids the bias.\n* There are results such as Figure 3 and Figure 10 that are used to justify the paper's claims that \"... LLM plans better in the direction\nof fewer computations needed, but the forward direction outperforms backward in general ...\". However, they are only shown for one planning task (instead of the 3 domains which the paper claims its focus is on). Since the paper mainly relies on empirical observations to justify its claims, I think the same figure should be replicated for other tasks as well (especially that I noticed there are additional space available in the main paper).\n* Table 3 tries to show that making the LLM decide which planning direction to use during test-time could improve performance. But somehow the table does not have the results for **Fwd-Flip**, the author's main proposed method for us to compare **Fwd-Flip-Reason** to. We also cannot just look at the previous results for **Fwd-Flip** because the authors mentioned that they used a different hyperparameter $M$ as compared to that in Table 2."
            },
            "questions": {
                "value": "1. \"We expect our proposed Fwd-Flip to outperform Fwd, Back, Flip, Fwd-Flip.\" I think the last Fwd-Flip is a typo - it should be Fwd-Back right?\n2. The method written in the introduction - \"Given a planning problem, we ask LLMs to sample possible plans in the forward direction of both the original problem and the flipped one, and then self-verify (Stechly et al., 2024) all the plans before choosing the final one\", is different from what was presented as the main method, which is **Fwd-Flip**. The description of **Fwd-Flip** given in the experiments is \"randomly choose either forward direction of the original problem or forward direction of the flipped problem and then plan\". This is very confusing because these are two totally different methods. I'd like the authors to clarify which is their main proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}