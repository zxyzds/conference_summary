{
    "id": "myYKk4Qz3l",
    "title": "Training-free Editioning of Text-to-Image Models",
    "abstract": "Inspired by the software industry's practice of offering different editions or versions of a product tailored to specific user groups or use cases, we propose a novel task, namely, training-free editioning, for text-to-image models. Specifically, we aim to create variations of a base text-to-image model without retraining, enabling the model to cater to the diverse needs of different user groups or to offer distinct features and functionalities. To achieve this, we propose that different editions of a given text-to-image model can be formulated as concept subspaces in the latent space of its text encoder (e.g., CLIP). In such a concept subspace, all points satisfy a specific user need (e.g., generating images of a cat lying on the grass/ground/falling leaves). Technically, we apply Principal Component Analysis (PCA) to obtain the desired concept subspaces from representative text embedding that correspond to a specific user need or requirement. Projecting the text embedding of a given prompt into these low-dimensional subspaces enables efficient model editioning without retraining. Intuitively, our proposed editioning paradigm enables a service provider to customize the base model into its \"cat edition\" that restricts image generation to cats, regardless of the user's prompt (e.g., dogs, people, etc.). This introduces a new dimension for product differentiation, targeted functionality, and pricing strategies, unlocking novel business models for text-to-image generators. Extensive experimental results demonstrate the validity of our approach and its potential to enable a wide range of customized text-to-image model editions across various domains and applications.",
    "keywords": [
        "text-to-image synthesis",
        "software edition",
        "concept subspace"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=myYKk4Qz3l",
    "pdf_link": "https://openreview.net/pdf?id=myYKk4Qz3l",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a new task named image editioning that only allows editing a specific concept without training the text-to-image model. To achieve this, this paper projects the CLIP text embedding with PCA into concept subspaces and uses the principal axes corresponding to the concept for editing. The resulting system could only generate images of the desired concept regardless of subjects in input prompts. Experiments demonstrate that the proposed method can achieve the proper results on this new proposed task."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper aims to introduce a new type of task that ignores subjects in the text prompts while only editing a predefined concept (e.g., cat).\n\n- This paper analyzes the PCA space in CLIP text encoder embedding and achieves image editioning.\n\n- Several experiments and interesting observations are made for this new task and the proposed method."
            },
            "weaknesses": {
                "value": "- The image editioning setup is a bit weird to me. If I would like to only generate specific objects, why not just replace the original object's name with the desired concept? An LLM could easily be used for that purpose. The practical value of the proposed method should be better explained. \n\n- Different editions require the same computations, and I do not see why different editions should charge differently as illustrated in Figure 1. I have a hard time figuring out why this task could stimulate new business models. \n\n- What if the prompt contains multiple objects? Can this method select the correct one? Could the proposed method be extended to edit multiple objects/concepts? These points need some discussion.\n\n- Many related works have been done for editing the styleGAN space for image editing, these methods should be cited and discussed.\n\n[a]  GANSpace: Discovering Interpretable GAN Controls.\n[b] A Compact and Semantic Latent Space for Disentangled and Controllable Image Editing."
            },
            "questions": {
                "value": "- The motivation of this paper needs to be better explained.\n- The proposed method's technical novelty could be improved.\n- Editing the prompts with LLMs would be interesting to investigate."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a training-free image editioning method aimed for providing controllable image generation for specific user groups or uses cases. They apply PCA to obtain the desired concept subspaces and projects the text embedding of a given prompt into these desired concept low-dimensional subspaces enables efficient model editioning. Extensive experiments demonstrate the effectiveness of the proposed methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "a.The proposed method is simple but effective. The author propose a novel task for offering different editions or versions of a product tailored to specific user groups or use cases, which may be useful in the software industry\u2019s practice.\n\nb.The author conducts extensive experiments and proves its effectiveness."
            },
            "weaknesses": {
                "value": "a. The proposed method, while eliminating the need for model retraining, still requires the creation of a desired concept dataset in advance, which somewhat restricts its applications. \n\nb. The presentation of method part is not very clear, especially Figure 3 and I didn\u2019t find its citation in the main text.\n\nc.What if I need some concepts that are rare or even not included in the pre-prepared dataset? \n\nd.What if  I want to generate images with multiple concepts rather than a single concept?\n\ne. The pretrained model used in this paper is sd 1.4, what about its effectiveness on other diffusion models?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this study, the authors propose a new task, ''training-free editioning'', for business T2I services. The goal of the new task is to force the base T2I model to only generate images from a predefined concept set. To avoid retraining the base model, the authors project the input textual prompts to the given concepts. Experiments show the method would be a valid solution for the proposed new task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Overall, the reviewer appreciates the efforts in defining a new task for T2I applications, which genuinely will shed some new light on the research field. Despite much research that has tried to generate/find desired textual prompts for different goals, this is the first time the reviewer has considered controlling the output of T2I models by projecting the textual prompts. The proposed method, though somewhat straightforward, is reasonable. The discussions on Sec.3.1 are also welcomed."
            },
            "weaknesses": {
                "value": "1. Limited scope. Though the authors have tried their best to show the potential applications for the newly proposed task, the reviewer is not convinced that the proposed solution will be widely used in the T2I community. In what situation could we need to force a T2I service to generate only \"cat\" images while \"must not\" other concepts? For concept erasing or model aligning, the goal is clear and easy to achieve by listing unwanted concepts. Then, most of the common concepts would work as usual. In the proposed editioning setting, the outputs can only be one concept (which could also be an unsafe concept if the service wants).\n2. The proposed PCA-based method is niche. When reading the definition in Section 3, the reviewer thought the concepts in C could be different objects or attributes. However, as shown in the experiments, one edition of T2I model could only support one kind of subject. Related to the first issue, such a setting is also nearly impractical. \n3. Technical writing should be given more attention. The reviewer found it hard to locate the manuscript in related works. The new task confused the reviewer when reading most parts of the introduction. Figure 1 is not helpful. The second paragraph comes in a strange way. The authors are suggested to re-phase the introduction to clearly show the context of this manuscript.\n4. The reviewer is surprised to find no user study evaluation is provided in the manuscript."
            },
            "questions": {
                "value": "Q1. How will the number of prompts in C influence the final performance?\nQ2. How could the proposed method deal with multiple concepts? Besides subjects, how does it work on attributes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focus on the \"training-free editioning\" task based on a pre-trained text-to-image model. To achiever the objective, the paper proposes some aligned concepts for the input prompts, and apply the PCA method to obtain the desired subspaces where the input and based concepts can be consist with each other. The paper announced that they introduce the novel task."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This paper attempts to replace the concept with the PCA model in the subspace of the label, and implements it as training-free editing."
            },
            "weaknesses": {
                "value": "1. The task proposed in this paper is one of the most basic tasks of text-to-image task. \n2. A lot of works have done to align the subjects during the generating process. \n3. There is no comparative experiments with other outstanding models, like SDXL, FLUX 1.0. \n4. The related work part just list some older papers and there is  insufficient relevance to the question studied in this paper. \n5. In the method part, There is only a simple introduction to the PCA method in the 4.1 CLIP-BASED CONCEPT SUBSPACE PROJECTION. No more subject follow it. \n6. In the experiment part, the 5.2.2 and 5.3 just list the general words without any in-depth analysis."
            },
            "questions": {
                "value": "1. Please provide more results with better pre-trained models, like SDXL.\n2. What\u2019s the advantage of PAC in this paper compared with LLMs for text alignment. \n3. Provides more research papers which are related to the prompt processing and are published in 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}