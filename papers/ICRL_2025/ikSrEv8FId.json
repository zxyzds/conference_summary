{
    "id": "ikSrEv8FId",
    "title": "RMAAT: A Bio-Inspired Approach for Efficient Long-Context Sequence Processing in Transformers",
    "abstract": "Astrocytes, an essential component of the brain's neural circuitry, demonstrate learning capabilities through bioplausible mechanisms such as presynaptic plasticity and hebbian plasticity. However, their integration into computational models remains underexplored. This paper advances astromorphic computing techniques to emulate transformer self-attention mechanisms, leveraging astrocytic nonlinearity and memory retention to improve long-range dependency processing in machine learning and natural language processing (NLP) tasks. Existing transformer models have difficulty handling lengthy contexts with thousands of tokens, even with substantial computational resources.  We propose Recurrent Memory Augmented Astromorphic Transformers (RMAAT), integrating astrocytic memory and recurrent processing into self-attention, enabling longer context handling without quadratic complexity growth. Our bioplausible model has been found to outperform traditional transformers in experimental tests conducted on the Long Range Arena benchmark and IMDB dataset. Specifically, our model achieves a significant reduction in memory utilization and computational latency. This paves the way for biologically inspired AI models by illustrating how astrocytic characteristics may enhance the performance and efficiency of computational models.",
    "keywords": [
        "Astrocyte",
        "Neuromorphic Computing",
        "Bio Inspired Learning",
        "Neuroscience-Algorithm-Application Codesign"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "RMAAT introduces a bio-inspired transformer model leveraging astrocytic memory to efficiently handle long-context sequences with reduced memory usage and enhanced computational speed",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ikSrEv8FId",
    "pdf_link": "https://openreview.net/pdf?id=ikSrEv8FId",
    "comments": [
        {
            "summary": {
                "value": "In this work, the authors propose biologically inspired modifications to self-attention based on astrocytic processing, to enable longer context handling. They modify self-attention in a number of ways to achieve an architecture that is then tested on various tasks including some tasks in long-range arena."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The mechanisms used and their combination seems novel. The performance of the proposed method is good."
            },
            "weaknesses": {
                "value": "The primary weakness of the paper is that it doesn't clearly separate out the biological terminology from the model terminology, which makes it very hard to follow. It would have been useful to have a section that summarizes all the changes mathematically, and separately discuss how they are related to astrocytes.\n\nAnother weakness is that the empirical evaluation is very limited -- the proposed architecture is not compared with more recent state-space models, and the comparison is not done on all the LRA benchmarks."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel transformer-based architecture that maps properties of astrocytes, a neural cell that has recently been discovered to play an important role in various central cognitive processes, onto a novel version of the series of models called \u201cAstromorphic Transformers\u201d. One of the central contributions to the architecture seems to be the introduction of a linear-memory relative positional encoding mechanism and the introduction of long-term memory tokens which in some sense mimic the behavior of continuous-time models of neuron-astrocyte dynamics. The architecture is evaluated on two versions IMDB movie reviews dataset in terms of its hardware efficiency and accuracy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper provides a good overview of the continuous-time model of neuron-astrocyte network dynamics. The simulation helps understand the behavior of the coupled differential equations.\n\n- The implementation of the proposed mechanism is more hardware-efficient than traditional transformers, as well as the Linear Transformer (Mia et al., 2023).\n\n- The paper proposes a mechanism for relative positional encoding in linear transformers. I am not sure how well-addressed this issue is in the literature. Solutions have been proposed in 2021 already [1], but the mechanism proposed here seems to be novel, although I do have a question about its novelty in the \u201cQuestions\u201d section.\n\n[1]: Liutkus, Cifka et al., 2021, Relative Positional Encoding for Transformers with Linear Complexity"
            },
            "weaknesses": {
                "value": "- Figure 2 is quite messy. There are lines crossing each other, text slightly covered by certain shapes, and two arrows that point from the astrocyte cell outwards which I cannot decipher. The presentation would greatly benefit from a clearer figure. \n\n- Rather than going into the details of the neuroscience and chemistry of the synaptic plasticity and astrocyte dynamics, the text would greatly benefit from a clearer presentation of the previous work on astromorphic transformers.\n\n- The reported fivefold reduction in memory utilization holds with respect to a baseline that can be put in question. It is not entirely clear what the reported \u201clinear transformer\u201d is, and it is very much not clear to me why a linear version of the transformer would consume approximately 3-4x as much memory as the full quadratic softmax transformer does.\n\n- The two benchmarks are essentially one and the same. It is not entirely clear to me what the difference is, as LRA text classification is exactly a dataset of IMDB reviews. It seems that the LRA version is longer. Also, I would not call this a \u201ckey benchmark\u201d in any circumstances.\n\n- Because of the limited description of the previous work on astromorphic transformers, it is difficult to judge what in the paper is novel and what is an explanation of previously proposed models."
            },
            "questions": {
                "value": "- Starting at line 196, the long-term process dynamics are said to be a key contribution that enables the temporal integration of astrocytes within the proposed architecture. I am slightly confused by the use of the term \"contribution\", as I was under the impression that the long-term process dynamics are established in the literature. Could the authors clarify whether the long-term process dynamics are a contribution of their work, or whether this is an already known model in neuroscience?\n\n- Astrocytic memory is very difficult to understand based on the provided description. Could you provide an equation describing how the memory tokens are initialized, and integrated into the transformer, and how the memory retention factor influences these tokens? \n\n- Is the phi(R) encoding of relative positional information a novel contribution of this work? I am somewhat under the impression that it is not and that the proposed novelty is the mapping of this matrix to certain functions in the continuous-time model described previously.\n\nLimitations: \n\n- While the neural model is explained well, I find that the paper falls short of adequately motivating the mapping from the neural dynamics onto the deep learning model. For example, starting at line 308 of the paper, what prompts the authors to associate Hastro to psi(pijs), and what is the insight contained in the proposed association? \n\n- The experimental evaluation is rather limited and seem to essentially be constrained to an analysis on IMDB movie review sentiment classification. Could the authors clarify what the difference between the two benchmarks they used is? Additionally, the authors may consider evaluating their architecture on the rest of the LRA dataset, as that would provide a clearer picture of the performance of the model. Resources permitting, reporting language modeling perplexity with various context lengths on datasets such as WikiText-103 or EnWik8 would improve the quality of the evaluation. The evaluation is unfortunately very limited in its current state."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel approach to computational modeling by introducing a brain-inspired astromorphic transformer that leverages astrocytic mechanisms, integrating both short-term and long-term plasticity to improve processing efficiency and handle longer sequences in machine learning and NLP tasks. The authors propose the Recurrent Memory Augmented Astromorphic Transformer (RMAAT), which utilizes astrocytic nonlinearity and memory retention. This approach addresses the limitations of existing transformers, which struggle with high memory and computational demands when handling long contexts. Experimental results on the Long Range Arena benchmark and IMDB dataset show that the proposed model achieves significant reductions in memory usage and latency, highlighting the potential of biologically inspired techniques in enhancing computational models' efficiency."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The integration of astrocyte cell mechanisms introduces a novel, neuroscience-inspired approach to addressing computational challenges in deep learning, particularly for long-sequence tasks.\n  \n* The proposed model provides an alternative to self-attention by aiming to reduce the quadratic complexity growth that limits traditional transformer architectures, thereby improving efficiency.\n\n* The model effectively combines astrocytic functions with short-term and long-term plasticity mechanisms, enhancing the ability to retain and process long-range dependencies.\n\n* Experimental results indicate that this approach achieves notable improvements in memory utilization and computational latency, making it a potentially valuable contribution to bioplausible AI research."
            },
            "weaknesses": {
                "value": "* The proposed method lacks cohesion, incorporating multiple bio-inspired mechanisms\u2014short-term and long-term plasticity, Hebbian plasticity, and presynaptic plasticity\u2014without clearly organizing these elements into a unified or systematic framework.\n  \n* While the paper claims to integrate brain-inspired mechanisms within a transformer architecture, the resulting model largely resembles a linear recurrent neural network, offering limited differentiation from existing linear models like RWKV and RetNet.\n\n* The experimental validation is limited, with only two benchmark comparisons. This restricts the persuasiveness of the results, leaving questions about the generalizability and practical effectiveness of the proposed method."
            },
            "questions": {
                "value": "1. In Section 3.2, Figure 2 does not appear to align closely with the content discussed. Could you clarify where Hebbian plasticity, presynaptic plasticity, and spatial mapping are represented in Figure 2? It would be helpful to explicitly indicate where these elements are incorporated into the architecture.\n\n2. Could you provide more clarity on the concept of \"memory tokens\"? Are these similar to KV caches in traditional transformers? Additionally, what distinct roles do memory tokens and astrocytic memory serve in your model?\n\n3. Given that this work integrates multiple bio-inspired mechanisms, could you provide ablation experiments to demonstrate the individual performance impact of each bio-inspired rule adopted? Such an analysis would help in understanding the contribution of each mechanism to the overall model performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes RMAAT (Recurrent Memory Augmented Astromorphic Transformer), a new transformer architecture inspired by astrocyte cells in the brain. The key ideas are:\n\n- Using \"astrocytic memories\" as a form of compressed context to handle long sequences\n- A bio-inspired compression mechanism based on short-term and long-term plasticity\n- An \"Astrocytic Memory Replay Backpropagation\" (AMRB) algorithm for efficient learning\n\nThe authors evaluate RMAAT on sentiment classification (IMDB) and text classification (LRA), showing improvements in accuracy, memory usage, and speed compared to baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Novel bio-inspired approach to improving transformers, drawing interesting connections to neuroscience\n- Proposed astrocytic memory mechanism provides an elegant way to compress and propagate context\n- AMRB algorithm shows promise for more efficient training of long-context models\n- Empirical results demonstrate clear improvements over standard and efficient transformer baselines"
            },
            "weaknesses": {
                "value": "- The core concept bears strong similarities to existing work on Retentive Networks [1], particularly in Equations 6 and 8. The authors should clarify how their approach differs from and improves upon RetNet and other related work like GLA (Gated Linear Attention) [2].\n\n- The origin and computation of the R variable, first appearing in line 305, is unclear. This is a critical component that needs to be explained in detail (correct me if I'm wrong)\n\n- While the authors frame their training approach as biologically-inspired \"Hebbian Plasticity\", the actual training still relies on backpropagation through time (BPTT). The claimed memory savings of AMRB compared to BPTT are actually a common feature of linear transformers using parallel scan (chunk) training, not a novel contribution.\n\n- For IMDB, the maximum total sequence length of 1536 tokens (256 * 6 segments) is not particularly long by modern standards.\nOnly the text classification task from LRA is evaluated, despite the paper's focus on \"efficient long-context sequence processing\". A full evaluation on all LRA tasks would provide a more comprehensive picture.\n\n- The biological framing, while interesting, sometimes feels like packaging of existing linear transformer concepts rather than truly novel bio-inspired algorithms. The authors should more clearly delineate which aspects are novel contributions vs. reframing of known techniques.\n\nIn summary, while the paper presents some interesting ideas and promising results, there are some concerns about novelty and thoroughness of evaluation that should be addressed. The authors should clarify the key differences from existing work, provide more details on critical components like the R variable, and conduct more comprehensive experiments on truly long-context tasks. \n\n\n[1]: Sun, Yutao, et al. \"Retentive network: A successor to transformer for large language models.\" arXiv preprint arXiv:2307.08621 (2023).\n\n[2]: Yang, Songlin, et al. \"Gated linear attention transformers with hardware-efficient training.\" ICML 2024."
            },
            "questions": {
                "value": "See my weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}