{
    "id": "FBhKUXK7od",
    "title": "Fast unsupervised ground metric learning with tree-Wasserstein distance",
    "abstract": "The performance of unsupervised methods such as clustering depends on the choice of distance metric between features, or ground metric. Commonly, ground metrics are decided with heuristics or learned via supervised algorithms. However, since many interesting datasets are unlabelled, unsupervised ground metric learning approaches have recently been introduced. One promising option employs Wasserstein singular vectors (WSV), which emerge when computing optimal transport distances between features and samples simultaneously. While WSV is effective, it is computationally expensive ($\\mathcal{O}(n^5)$ complexity). In this work, we propose to augment the WSV method by embedding samples and features on trees, on which we compute the tree-Wasserstein distance (TWD). We demonstrate theoretically and in practice that the algorithm converges to a better approximation of the full WSV approach than the best known alternative, entropy regularisation or Sinkhorn singular vectors, but with faster (cubic) computational efficiency. In addition, we prove that the initial tree structure can be chosen flexibly, since tree geometry does not constrain the richness of the approximation up to the number of edge weights. This proof suggests a fast, recursive algorithm for computing the tree parameter basis set, which we find crucial to realising the efficiency gains at scale. Finally, we employ the tree-WSV algorithm to several single-cell RNA sequencing genomics datasets, demonstrating its scalability and utility for unsupervised cell-type clustering problems. These results poise unsupervised ground metric learning with TWD as a low-rank approximation of WSV with the potential for widespread low-compute application.",
    "keywords": [
        "unsupervised learning",
        "optimal transport",
        "distance-based learning",
        "clustering",
        "trees",
        "wasserstein distance"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "Unsupervised ground metric learning with tree-based optimal transport is computationally efficient yet still more accurate than alternative approximation appraoches, and scales favourably on genomics datasets",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=FBhKUXK7od",
    "pdf_link": "https://openreview.net/pdf?id=FBhKUXK7od",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces Tree-WSV, integrating TWD with WSV (Huizing et al., 2022)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed method demonstrates efficiency compared to WSV and SSV presented in Huizing et al. (2022)."
            },
            "weaknesses": {
                "value": "- The authors missed important related works [1, 2, 3, 4, 5] that consider the relationships between the samples are informed by the relationships between the columns, and vice versa, for Wasserstein distance and specifically in tree-related settings [2,5]. Specifically, the setup of randomly permuted dataset rows and columns in the toy datasets was one of the important tasks in these works.\n- The explanation of how the Wasserstein distance can serve as a tree distance in Proposition 2.1 is unclear. It\u2019s also not evident whether there exists a tree for which the tree distance would correspond to a Wasserstein distance.\n- The real-world application is restricted to single-cell RNA sequencing data, despite the introduction citing various data types as motivation.\n- The experiment section includes only two competing methods without considering the baselines used in WSV or other distance metric learning approaches.\n- $n$ is not yet defined in the Abstract. It\u2019s unclear what does it represent here. In addition, the computational complexity for WSV reported by the author differs from that presented in the original WSV paper.\n- In Section 1.1, the authors mention the sliced Wasserstein distance. However, it\u2019s unclear what it is here and how SWD is a special case of TWD. Also, it\u2019s unclear why SWD is not considered an alternative for efficient computation for Wasserstein distance in the WSV framework.\n- It\u2019s unclear $\\mathbf{w}$ and $\\mathbf{Z}$ in line 109 are vector or matrix. Additionally, it\u2019s unclear what is the connection between $\\mathbf{x}$, $\\mathbf{y}$, $\\mu$, and $\\nu$ in line 111.\n- The notation of Wasserstein distance $\\\\mathcal{W}\\_C$ in line 92 and the notation of TWD $\\\\mathcal{W}\\_\\\\mathcal{T}$ in line 111 are confusing. In the formal $C$ is a pairwise distance matrix, and in the later $\\\\mathcal{T}$ is a tree.\n- It\u2019s unclear what are $\\mathbf{a}$ and $\\mathbf{b}$ in Eq.(2). In addition, it\u2019s unclear what size($\\mathbf{w}$) represents in line 116.\n- More details are needed for how \u201cTWD is a good approximation of the full 1-Wasserstein distance\u201d. What do the authors refer to as \u201capproximation\u201d? What is the relation between TWD and full 1-Wasserstein distance?\n- It\u2019s unclear what is $R$ in line 151.\n- The authors keep using the term \u201cbasis\u201d throughout the paper, e.g.,  tree parameter basis set, the set of basis vector, matrix\u2019s basis set. However, it\u2019s unclear what does it represent in these contexts.\n- It\u2019s unclear what \"full WSV\" represents. Is it different from WSV?\n- It\u2019s unclear why the size of the vectors of edge weights is less than the number of nodes in the tree in Section 2.1.\n- It\u2019s unclear what $\\mathcal{W}_A$ and $\\mathcal{W}_B$ represent in Proposition 2.1. Here, $A$ and $B$ are the sets, which are not pairwise distance matrix nor tree as in previous notations.\n- It\u2019s unclear what $\\circ$ denote in Proposition 2.1. Also, it\u2019s unclear what are $\\\\lambda\\_A$, $\\\\mathbf{z\\_i}\\^{(\\\\mathbf{A})}$, and $\\\\mathbf{Z}\\^{\\\\mathbf{B}}$.\n- The proof for Proposition 2.1 in Appendix A is very hard to follow. The notations used are not consistent with those used in the main texts. The newly defined notation is very dense. Also, it\u2019s unclear what are $\\\\mathbf{W}\\_{\\\\mathbf{A}}$, $\\\\Phi\\_{\\\\mathbf{S}}$\n- More details and explanations are needed for how Theorem 2.2 supports unique and non-zero solutions in Proposition 2.1.\n- Algorithm 1 is very hard to follow. For example, it\u2019s unclear what the line \u201c$\\\\mathbf{Z}\\_{\\\\mathbf{diff}}$ \u2026\u2026 \u201c represents. It\u2019s unclear what are $\\\\mathbf{A}\\_{leaf}$, $\\\\mathbf{w}\\_{\\\\mathbf{B}}$(prev)\n- The reference style is inconsistent: some entries lack publisher information, some links are not official paper links, and \"Wasserstein\" is sometimes written with a lowercase \"w.\"\n- The notation style is inconsistent: vectors and matrices are inconsistently represented, with a mix of boldface and regular type. The notation for the tree parameter in Section 1.1 is different than in Section 2.1\n\n## Minor\n- Missing \u201c-\u201d for tree-Wasserstein distances in line 077 and line 102\n- The acronym \"OT\" is used without being defined first\n- Missing punctuations in equations\n- It\u2019s unclear what is $TB$ in line 214\n\n[1] Ankenman, J.I., 2014.\u00a0Geometry and analysis of dual networks on questionnaires. Yale University.\n\n[2] Mishne, G., Talmon, R., Cohen, I., Coifman, R. R., & Kluger, Y. (2017). Data-driven tree transforms and metrics.\u00a0IEEE transactions on signal and information processing over networks,\u00a04(3), 451-466.\n\n[3] Gavish, M. and Coifman, R.R., 2012. Sampling, denoising and compression of matrices by coherent matrix organization.\u00a0Applied and Computational Harmonic Analysis,\u00a033(3), pp.354-369.\n\n[4] Shahid, N., Perraudin, N., Kalofolias, V., Puy, G. and Vandergheynst, P., 2016. Fast robust PCA on graphs.\u00a0IEEE Journal of Selected Topics in Signal Processing,\u00a010(4), pp.740-756.\n\n[5] Yair, O., Talmon, R., Coifman, R.R. and Kevrekidis, I.G., 2017. Reconstruction of normal forms by learning informed observation geometries from data.\u00a0Proceedings of the National Academy of Sciences,\u00a0114(38), pp.E7865-E7874."
            },
            "questions": {
                "value": "- In line 194, what does \u201c$a_i$ defined as for WSV\u201d mean?\n- What is the size of $\\mathbf{U}$ in line 259?\n- How does the choice of tree construction affect the proposed method? If a different tree construction method were used instead of ClusterTree, would it impact the method's outcome?\n- How to decide whether the algorithm reaches convergence in Algorithm 1?\n- How fast does the proposed algorithm converge? How does the performance change across the iteration?\n- What are the hyperparameters of the proposed method? How sensitive are they in the experiments?\n- What is the Euclidean metric baseline in Table 1?\n- Why is WSV not considered as a baseline in Table 1?\n- Why are other TWD methods not considered baselines in Table 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors apply the unsupervised metric learning algorithm from Huizing et al. (2022) (which used Wasserstein and entropy regularized Wasserstein) to learn ground metrics for spaces of histograms based on the Tree Wasserstein distance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The idea is sound, and there is a significant element of originality, especially in the development of the algorithm in Appendix C."
            },
            "weaknesses": {
                "value": "The paper seems rushed overall, there are a large number of typos and a number of results that should have been presented as Theorems are merely stated informally.\n\n1. l.238 \u2013 241 these statements require a proof. Especially the convergence, since it was already somewhat delicate in Huizing et al. (2022).\n2. l.263 \u201cWasserstein\u201d -> \u201cTree Wasserstein\u201d or else requires a proof.\n3. I find Theorem 2.2 hard to interpret. Can the authors rephrase the interpretation in the next paragraph (l. 256-l.260) as a Theorem and include the current Theorem 2.2 as a Lemma?\n4. l.354 The previous work used n=100, m=80 and not n=80, m=60 as this paper states.\n5. l.367 Why are you using a different metric (Frobenius norm) than the original work? How does your method compare when using the same metric d_h(B, B\u2019) = ||log(B/B\u2019)||_V?"
            },
            "questions": {
                "value": "List of typos. I suggest that the authors give the manuscript a thorough proof-reading.\n\n* l.39 confusing, why is the optimal sample distance the Wasserstein distance?\n* l.106 in what sense is SWD a \u201cgeometric embedding\u201d?\n* l.111 definition of pi?\n* l.116 what does \u201cgood approximation\u201d mean here?\n* l.140 precise the meaning of normalized.\n* l.142 distribution -> \u201cprobability distribution\u201d\n* l.150 R and tau are undefined\n* l.150 not clear what \\Phi_A is. Huizing et al. (2022) describe Phi_A as \u201dlifts a ground metric to a pairwise distance matrix\u201d. The authors need to explain the definition of Phi_A.\n* l.151 equivalence to (3) assumes tau = 0. The authors need to be a bit more careful in their recap of Huizing et al. (2022).\n* l.157 not sure that the remark in parentheses is correct, according to Huizing et al. (2022) a single Wasserstein iteration is n^3 log n. Also how many distances do we compute when we compute \u201cm^2, n^2 W. distances\u201d? Is is m^2 + n^2, m^2 * n^2, something else?\n* l.201 W_{T_B} instead of W_B ?\n* l.205 z_i^(A) undefined\n* l.205 Z^{(B)} or Z_B?\n* l.214 W_{TB} -> W_{T_B}"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel approach for unsupervised ground metric learning using Tree-Wasserstein Distance (TWD) as a low-rank approximation of the computationally intensive Wasserstein Singular Vector (WSV) method. The proposed method embeds samples and features in tree structures, reducing the computational complexity from O(n\u2075) in traditional WSV to O(n\u00b3) by learning distances between data points as TWD on trees. Empirical results indicate that the method achieves similar or better clustering accuracy compared to Sinkhorn singular vectors (SSV) while maintaining much faster runtimes.\n\nThis paper is the improved version of the workshop paper \u201cUnsupervised Ground Metric Learning with Tree Wasserstein Distance\u201d. The primary innovation of this work is adding recursive basis set computation for tree-based WSV."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper offers rigorous theoretical support for the TWD method, with proofs on the uniqueness and existence of solutions within specific tree configurations. \n2. The empirical results are presented clearly, with comparative metrics that directly illustrate the computational runtime saving and clustering performance.\n3. The paper provides a solid background review on optimal transport theory and the tree-Wasserstein distance."
            },
            "weaknesses": {
                "value": "1. Although the ClusterTree algorithm plays a significant role in the tree structure initialization, there is limited background provided on how it operates, what assumptions it makes, or its typical applications. I reviewed both references\u2014Le et al. (2019) and Indyk & Thaper (2003)\u2014but did not find any mention of a ClusterTree. Could the author be referring to the \u2018Partition_Tree_Metric\u2019 described in Le et al. (2019)?\n2. The algorithm section mentions differences in handling \u2018large\u2019 and \u2018small\u2019 datasets but does not specify the boundary between the two. What happens if either m or n is very large while the other meets the \u2018small\u2019 criteria?\n3. The paper\u2019s notation is sometimes inconsistent, making it challenging to reference equations or terms precisely. For example, on line 201, a_i and a_j are bold, but on line 205, a is not bold. On line 211, what does the cost matrix B represent?\n4. Figure 3 could be better organized. The paper does not provide a comparison of how other metrics perform on these datasets.\n5. Line 557, The URL is invalid."
            },
            "questions": {
                "value": "Refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an interesting variation of Wasserstein singular vectors by embedding samples and features on a tree. By doing so, they claim to have achieve a cubic complexity as opposed to quintic complexity of the standard method. While the authors show interesting results, I believe the manuscript can be accepted after a major revision."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I think the paper is proposing a novel idea for estimating ground metric learning of unlabeled data set, that reduces the complexity of each iteration from $\\mathcal{O}(N^5)$ to $\\mathcal{O}(N^3)$."
            },
            "weaknesses": {
                "value": "Some important details of the proposed method are missing in the paper. This includes convergence rate, well-posedness, and memory consumption of the proposed method. See my questions/comments below."
            },
            "questions": {
                "value": "**Major**:\n\n- How is the condition number of eq. 5? Does the system become ill-conditioned as matrix becomes large?\n- Section 2.3, what is the convergence rate of the proposed iterative method? How it is affected by the data set size?\n- What is the complexity of Cluster Tree used here?\n- Please add details of memory consumption for your proposed iterative algorithm and approximation to SVD.\n- I think the authors should make more effort to show the claimed complexity $\\mathcal{O}(N^3)$. Consider one of the data set, and test the method against benchmark for a range of $N$.\n\n**Minor**:\n\n- In abstract, \"...a fast and recursive algorithm...\" and not \"a fast, recursive algorithm\u2026\u201d\n- In abstract, what does \"low-compute application\" mean?\n- P2, l82: \"... to have a solution\" and not \"to have solution\"\n- P3, l108: Is there a guarantee that the shortest path between any two nodes on a tree is unique?\n- Figure 1: Caption refers to b5, but I don\u2019t see b5 in the graph.\n- In the references, make sure to use capital letters when needed, e.g. use \u201cWasserstein\u201d instead of \u201cwasserstein\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}