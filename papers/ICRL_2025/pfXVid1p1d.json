{
    "id": "pfXVid1p1d",
    "title": "HALO: Human-Aligned End-to-end Image Retargeting with Layered Transformations",
    "abstract": "Image retargeting aims to change the aspect-ratio of an image while maintaining its content and structure with less visual artifacts. \nExisting methods still generate many artifacts or lose a lot of original content or structure.  To address this, we introduce HALO, an end-to-end trainable solution for image retargeting. \nThe core idea of HALO is to warp the input image to target resolution. \nSince humans are more sensitive to distortions in salient areas than non-salient areas of an image, HALO decomposes the input image into salient/non-salient layers and applies different wrapping fields to different layers. To further minimize the structure distortion in the output images, we propose perceptual structure similarity loss which measures the structure similarity between input and output images and aligns with human perception. Both quantitative results and a user study on the RetargetMe dataset show that our algorithm achieves SOTA. \nEspecially, our method increases human preference by 13.21% compared with the second best method.",
    "keywords": [
        "Image Retargeting",
        "Image Transformation",
        "Image Editing"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "The paper proposes a novel approach that achieves content- and structure-aware image retargeting",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=pfXVid1p1d",
    "pdf_link": "https://openreview.net/pdf?id=pfXVid1p1d",
    "comments": [
        {
            "summary": {
                "value": "The work uses saliency to achieve image retargeting in a learnable setting. The results are shown to be SOTA through an user study."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Nil"
            },
            "weaknesses": {
                "value": "- The work relies heavily on a saliency detector - MDSAM, which is a challenge. Having the main task of retargeting rely on a pre-trained saliency detection network is a downer. \n\n- RetargetMe dataset is a standard one. However, the images are very simple and have distinct objects. The proposed work will fail when there are complex scenes with nonsalient objects. \n\n- Using saliency for retargeting has been explored extensively. There is hardly any novelty or contribution in this work."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce  a novel image retargeting method that leverages a layer decomposition approach to distinguish between salient and non-salient regions of an image. By applying distinct warping fields to these separate layers, the method minimizes overall distortion during aspect ratio changes. Additionally, the authors present a novel structure-prone perceptual loss, based on DreamSim, which evaluates structural similarity between input and output images."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1) The authors address an interesting image rescaling problem and achieve visually impressive results compared to previous methods.\n\n2) Their proposed multi-flow network for estimating transformations intuitively utilizes both the original undistorted image and a naively rescaled version to the target aspect ratio.\n\n3) The proposed augmentation technique is logical and straightforward allowing to utilize DreamSim as loss function."
            },
            "weaknesses": {
                "value": "1) The overall contribution of the method appears somewhat limited, as the primary innovation lies in the augmentation technique that enables the use of the DreamSim loss. Meanwhile, the layer decomposition and inpainting modules are employed in a plug-and-play manner without addressing their sensitivity or taking further steps to reduce reliance on off-the-shelf methods.\n\n2) The quantitative evaluation is somewhat limited; incorporating additional no-reference perceptual image metrics, both traditional and learning-based, would strengthen the paper's claims. \n\n3) The writing quality and overall presentation should be enhanced to better reflect the promise of the proposed method's results.\n\n4) The related work section requires improvement, particularly in highlighting the shortcomings of prior approaches and demonstrating how the proposed method overcomes the challenges outlined in the introduction."
            },
            "questions": {
                "value": "1) Why does GPNN outperform on the perceptual metric MUSIQ despite the provided visuals clearly showing that the proposed method delivers superior results?\n\n2) How sensitive is the proposed approach to suboptimal inpainting results?\n\n3) Why are the visual and quantitative results nearly identical when using either augmentation or the LPIPS loss? Does the model trained with the LPIPS loss not incorporate the augmentation method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an end-to-end trainable approach for image retargeting. The authors introduce a novel layered transformation to learn two warping fields utilizing a pre-defined saliency map and inpainting model. Additionally, they develop a tailored perceptual structural similarity loss for model training. Quantitative results and user studies showcase the method's effectiveness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper proposes a end-to-end learnable image retargeting framework and the results are impressive.\n2. This paper is well-written, showcasing a clear and coherent structure that effectively guides the reader through its main arguments and findings."
            },
            "weaknesses": {
                "value": "1. The performance of the proposed approach is fundamentally dependent on the choice of the saliency model and the image inpainting model utilized. The selected saliency model plays a crucial role in accurately identifying and highlighting the most important regions of the image. Meanwhile, the image inpainting model contributes significantly to the quality of the restored content. Together, these models work in tandem to optimize the overall results, making their selection a critical factor in achieving superior performance.\n2. Although the paper offers a well-structured and clear introduction to the methodological process, guiding readers through the steps and techniques employed, it falls short in addressing the underlying principles that inform these methods."
            },
            "questions": {
                "value": "1. What image resolutions can the proposed method accommodate, and what is the processing time like?\n2. What is the key to the proposed method's performance being superior to that of the comparison methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}