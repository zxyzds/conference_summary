{
    "id": "5oSUgTzs8Y",
    "title": "KooNPro: A Variance-Aware Koopman Probabilistic Model Enhanced by Neural Processes for Time Series Forecasting",
    "abstract": "The probabilistic forecasting of time series is a well-recognized challenge, particularly in disentangling correlations among interacting time series and addressing the complexities of distribution modeling. By treating time series as temporal dynamics, we introduce \\textbf{KooNPro}, a novel probabilistic time series forecasting model that combines variance-aware deep \\textbf{Koo}pman model with \\textbf{N}eural \\textbf{Pro}cesses. KooNPro introduces a variance-aware continuous spectrum using Gaussian distributions to capture complex temporal dynamics with improved stability. It further integrates the Neural Processes to capture fine dynamics, enabling enhanced dynamics capture and prediction. Extensive experiments on nine real-world datasets demonstrate that KooNPro consistently outperforms state-of-the-art baselines. Ablation studies highlight the importance of the Neural Process component and explore the impact of key hyperparameters. Overall, KooNPro presents a promising novel approach for probabilistic time series forecasting.",
    "keywords": [
        "Probabilistic time series prediction; Neural Process; Deep Koopman model"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5oSUgTzs8Y",
    "pdf_link": "https://openreview.net/pdf?id=5oSUgTzs8Y",
    "comments": [
        {
            "summary": {
                "value": "- KooNPro is a novel time series forecasting method based on Koopman theory.\n- It effectively estimates the process dynamics in a latent space where state transitions are linear.\n- It eventually provides context-dependent variance estimates."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Time series tasks, especially probabilistic forecasting, are relevant in many domains and far from being solved.\n- The method is presented well and provides many benefits (variance estimation, flexible forecast horizon, nice theory, and practical performance).\n- I am not aware of these ideas being presented before."
            },
            "weaknesses": {
                "value": "1. See Questions below.\n2. The definition of target and context sets (ll. 153ff) and the splitting of $z$ (ll. 208ff) were not familiar to me, as they are not common in point forecasting. I would have personally benefitted from more exposition.\n\n#### Minor Comments\n- References should not be used as nouns (e.g., ll. 037f, 049f).\n- Notation: Eq. (2)/(5) is missing a $p$ on the left-hand side and has odd typesetting of the expectation subscript. Some parens/brackets should be adjusted in height, e.g., in Eq. (3) and (17). In l. 219, $x_C$ and $y_C$ are not bold. L. 473 is the $e$  referring to \"scientific notation\" common in programming languages (in base-10) or Euler's number?\n- One could rename Sec. 5 to Experiment**s**.\n- Re. l. 306: Are the conditioning lookbacks of the test and the forecast horizon of validation (etc.) overlapping or not?\n- Language: The senctence in l. 821f is odd.\n- The combination in Fig. 3 is clever. However, given two separate plots would easily fit side-by-side, it is not worth the extra time needed to decipher the legend/axis labels.\n- The abstract in OpenReview should be formatted with Markdown syntax, instead of LaTeX."
            },
            "questions": {
                "value": "Note: The most important questions are listed first.\n\n1. Regarding Sec. 5.2: The presented results are very impressive. However, a large chunk of the time series forecasting literature performs point estimation instead of full distribution modeling and, therefore, commonly evaluates using MAE/MSE (see, for instance, the overview [here](https://github.com/thuml/Time-Series-Library)). Additionally providing these metrics (possibly in the appendix) would help contextualize the findings in the broader body of work. These methods can also provide probabilistic forecasts, e.g., by learning an output that parameterizes a simple distribution to be sampled from or by methods such as Monte Carlo dropout.\n2. Does the method need to be run multiple times to obtain the variance estimation samples?\n3. The case study in 5.4 is interesting, yet it makes me question the ability of KooNPro to model the variance of the data truthfully. While I agree that many dimensions are modeled appropriately, the results indicate serious limitations of the method.\n\t1. Dimensions #3 and #6 show values far outside the 90% interval. Are they within something like the 99% interval since the model learns a possibly appropriate long tail of the distribution? Or does it show some of the method's limitations (which would be fine if acknowledged as such)?\n\t2. Ll. 446f explains the data characteristics of the Solar dataset. However, the model fails to appropriately model the very low nightly variance of the power production around zero and instead shows a significant variance estimate at, for instance, midnight. Why does that occur?\n\t3. Combining these two observations, the overall variance estimate appears rather *uniform* and not well-adapted to the dataset.\n4. How large are the learned models measured in the number of parameters?\n5. Could the method be straightforwardly used to forecast fractional steps into the future?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "A method for multi-variate probabilistic time series forecasting is presented which combines the methods for temporal dynamics modeling using deep Koopman model with Neural Processes. The model consists of an encoder computing the hidden state h_t, a learned Koopman operator 'A' predicting the future hidden states and a decoder used to predict the final values using the hidden state h_t. In addition a neural process is used to model the latent dynamics S of the time series, which is used as an input to the Koopman operator to compute the future states."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Strengths\n- This paper proposes a new way for probabilistic modeling of multi-variate time series by representing the time series into a low dimensional space using neural processes and using them to model the temporal dynamics of the time series.\n- The idea presented in the paper is an intuitive approach for multi-variate forecasting. Modeling the latent dynamics of the time series is an important aspect that is missed by most state-of-the-art modeling approaches. As such, the method presented in this paper is interesting to the time series community.\n- The paper presents extensive experimental evaluation and ablation studies demonstrating the effectiveness of the approach."
            },
            "weaknesses": {
                "value": "Weaknesses\n- The description of the model is quite cryptic and not written in a clear manner. See questions below for parts that are unclear.\n- While the paper describes everything using mathematical equations, the utility or advantage of such equations/models is unclear. How do each of the components of the model help with a better forecast? (i am looking for a high level motivation for each of the components of the model)\n- Several works have introduced Koopman theory into time series modeling, however, there is no clear discussion of existing methods and their comparison with this work. The novelty of the paper is unclear without a clear distinction with existing work.\n- Other probabilistic forecasting methods modeling latent factors in time series have not been discussed of compared (for example deep state space models)."
            },
            "questions": {
                "value": "- What is the utility of the neural process latent variable S at a high level? Does it help model the correlations between the different time series? Can it be thought of as an encoding of each time series which also model the inter-correlations between the time series?\n- It seems that S is an s-dimensional latent variable meaning that it encodes the whole time series in a low dimensional subspace. Many papers have modeled time series as state space models involving low-dimensional latent decomposition. What is the advantage of the proposed approach over these approaches.\nExample papers modeling time series into low dimensional sub-spaces:\n  - Wang, Yuyang, et al. \"Deep factors for forecasting.\" International conference on machine learning. PMLR, 2019.\n  - Rangapuram, Syama Sundar, et al. \"Deep state space models for time series forecasting.\" Advances in neural information processing systems 31 (2018).\n  - Paria, Biswajit, et al. \"Hierarchically regularized deep forecasting.\" arXiv preprint arXiv:2106.07630 (2021).\n- What is the utility of the delay embedding dimension k?\n- Do context and target sets refer to training and testing time periods? can the context and target sets be interspersed, meaning can this method be used to predict missing values of irregularly sampled time series?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors study the practical problem of probabilistic time series forecasting and address the challenges. They propose KooNPro, which combines two methods, i.e., the Koopman model and Neural Processes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Addresses important problem of probabilistic time series forecasting\n- Interesting idea to use the latent representations from the Neural Processes, typically used in computer vision\n- Extensive experiments on 9 time series datasets\n- Multivariate forecasting problem is challenging\n- Good selection of multivariate probabilistic baselines to compare to including GP-Copula\n- Good probabilistic metric CRPS is used in the benchmarking\n- KooNPro shows state-of-the-art performance on 8/9 datasets"
            },
            "weaknesses": {
                "value": "- I think more background on the importance of probabilistic forecasting and its use in practical downstream task, e.g., supply chain in the introduction would be helpful\n- Overall the writing quality could be improved.\n- When discussing LSTMs in the introduction, DeepAR should be cited\n- For probabilistic diffusion models, Kollovieh et. al, \"Predict, refine, synthesize: Self-guiding diffusion models for probabilistic time series forecasting\", NeurIPS 2024 should also be cited.\n- There has also been an abundance of foundation models for time series forecasting, e.g., Ansari et. al, \"Chronos: Learning the language of time series\", 2024 that is not discussed int he introduction.\n- DMD is more typically used in the dynamical systems and scientific computing communities\n- The motivation for choosing Neural Processes as the probabilistic models rather than more current state-of-the-art probabilistic models, i.e., diffusion models is not clear.\n- For application of NPs to spatio-temporal time series (PDEs), see Hansen et. al, \"Learning Physical Models that Can Respect Conservation Laws\", ICML, 2023\n- Some of the background in Section 3 could be moved to an appendix to allow for more novelty of the method presentation and results in the main body.\n- The architecture in Figure 1 is also taking a large amount of space.\n- I have some concerns on the novelty since the proposed method is just combining two previously proposed approaches.\n- May be good to include multivariate time series forecasting problem definition.\n- Comparisons to other baselines, e.g., DeepAR (Salinas et. al, 2019) and MQ-CNN (Wen et. al, https://arxiv.org/abs/1711.11053, https://proceedings.mlr.press/v151/park22a/park22a.pdf), MQTransformer (https://arxiv.org/pdf/2009.14799, ) are missing, which could be run on electricity and traffic\n- Bold the best in Table 2 or use same convention as Table 1 with color in red but I think bolding would be best for both.\n- The method seems off in the case study in Dimension 6 and 8"
            },
            "questions": {
                "value": "1. How can the method generalize past Gaussian distributions to modeling arbitrary distributions?\n2. Have the authors tested with the Attentive Neural Process (ANP) Kim et. al, 2019, which shoes better performance than Neural Processes?\n3. Could the model also be run with Gaussian Processes? I think a comparison to the simpler GP would be nice to add to motivate the benefit of the NP.\n4. What is it about the electricity dataset that TimeGrid shows improved performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents KooNPro, a novel approach for probabilistic time series forecasting that integrates a variance-aware deep Koopman model with Neural Processes (NPs). By employing a variance-aware continuous spectrum modeled with Gaussian distributions, KooNPro effectively captures complex temporal dynamics with enhanced stability. It leverages NPs to capture global patterns across time series, improving prediction accuracy. Extensive evaluations on nine real-world datasets show that KooNPro outperforms state-of-the-art models, with ablation studies confirming the importance of its components and hyperparameters."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed variance-aware continuous spectrum modeling address the complex nonlinear temporal dynamics, which is novel and has a solid theoretical foundation.\n2. The empirical improvements are significant, outperforming other methods in complex, high-dimensional forecasting tasks.\n3. The ablation study demonstrates the effectiveness of Neural Process and help to understanding the role of this key component. The case study shows that KooNPro's predictive capability aligns well with real-world observations, capturing diurnal patterns and demonstrating reliable predictive intervals."
            },
            "weaknesses": {
                "value": "1. The integration of Neural Processes and the probabilistic deep Koopman model requires a sophisticated training approach, including variational inference for optimizing the ELBO. This could pose challenges in implementation, and could increase the computational complexity. \n2. The effectiveness of KooNPro relies heavily on the design of the encoder and decoder networks for the linear space transformation. My concern is that poorly designed architectures could lead to suboptimal representations and thus hinder the model\u2019s ability to learn the underlying temporal dynamics accurately."
            },
            "questions": {
                "value": "Can you explain more intuitively why complex temporal dynamics can be captured using the proposed probabilistic deep Koopman model? In addition, I'm curious about the validity of the assumptions that formula (6) relies on: \"hypothesize the latent space created by $\\phi$ possesses linear characteristics\", since formula (6) seems to be an important foundation of the subsequent derivations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors of the paper introduce a novel probabilistic time series forecasting model called KooNPro. KooNPro utilizes a variance-aware continuous spectrum, modeled using Gaussian distributions, to capture complex temporal dynamics and improve stability. By incorporating Neural Processes, the model captures fine dynamics and enhances global modeling capabilities. The authors evaluate KooNPro on nine real-world datasets and find that it consistently outperforms other state-of-the-art models in terms of accuracy and stability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Deep Koopman models had been used for time series forecasting before but not in conjunction with Neural Processes, hence the proposed model is non-trivial and new. \n* The authors enhance their model by a variance-aware continuous spectrum, which to my knowledge has not been done before."
            },
            "weaknesses": {
                "value": "* Clarity: The authors could improve how they present their work. The math is clunky and too detailed, and can cause the reader to lose attention. For example, in section 4, the authors never refer to Fig.1 more than once (in line 169), so there is a disconnect between the math and the figure. Maybe explain the figure first and detail the model at a high-level, then go in the inner workings and foundations. Also in the caption of Figure 1, consider adding pointers to different parts of the figure to help the reader understand where to look in the Figure for each reference in the caption.\n\n* Better analysis of each component of the model: The authors can provide a more comprehensive and insightful analysis of the Neural Process contribution. They may investigate how different NP architectures (e.g., Attentive Neural Processes, Convolutional Conditional Neural Processes) affect KooNPro's performance.\n\n* Limited empirical discussion on the exact benefits of this model: The authors can better demonstrate For example, conduct experiments to specifically evaluate KooNPro's performance on, say, non-stationary time series or time series tasks with a much larger prediction length etc., comparing it to baselines that do not incorporate NPs. Compared to other models, if there were more convincing and broader experiments demonstrating the benefits of the proposed model, it would lead to wider adoption. \n\n* More empirical evidence for robustness and stability: The paper claims robustness and stability as advantages of the proposed model, but these claims are well empirically validated. For example, the authors can add experiments evaluating the model's performance under different noise levels\n\n* Details on training baselines: Since the baselines were trained for each dataset, the authors should provide details on how the baselines were trained (how hyperparameter selection was done, what protocol was used etc.) in the appendix. It is important to check if the evaluation compares all models in a fair manner."
            },
            "questions": {
                "value": "* It is not clear what the term \"spectral pollution\" means in line 143. Consider explaining it for a reader who may not know about Koopman operators.\n\n* Why are all context lengths 10 in Table 4 (Page 15)? Why wasn't the model tried with a different context lengths? This is also concerning as much larger context lengths are used in practice."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}