{
    "id": "FIXk0RP960",
    "title": "Does RLHF Scale? Exploring the Effects of Data, Model, and Method",
    "abstract": "This study explores the scaling properties of Reinforcement Learning from Human Feedback (RLHF) in Large Language Models (LLMs). \nAlthough RLHF is considered an important step in the post-training of LLMs, its scaling potential is still largely unknown. \nWe systematically analyze key components in the RLHF framework\u2014model size, data composition, and inference budget\u2014and their impacts on performance.\nOur findings show that increasing data diversity and volume improves reward model performance, helping process-supervision models scale better. \nFor policy training, more response samples per prompt boost performance initially but quickly plateau. \nAnd larger reward models offer modest gains in policy training. \nIn addition, larger policy models benefit less from RLHF with a fixed reward model. \nOverall, RLHF scales less efficiently than pretraining, with diminishing returns from additional computational resources.\nBased on these observations, we propose strategies to optimize RLHF performance within computational limits.",
    "keywords": [
        "Language model",
        "Reinforcement learning from human feedback",
        "Scaling"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We explore the scaling properties of reinforcement learning from human feedback (RLHF), and show that current RLHF scales less efficiently.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=FIXk0RP960",
    "pdf_link": "https://openreview.net/pdf?id=FIXk0RP960",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates key components in the RLHF framework, such as model size, data composition, and inference budget, assessing their scalability. The findings reveal that RLHF scales less efficiently than pretraining, with performance gains diminishing despite increased computational resources."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThis paper addresses a critical gap in current LLM post-training research by examining the scalability of RLHF.\n2.\tThe experiments comprehensively cover various aspects of RLHF, including model size, data composition, and inference budget.\n3.\tThe conclusions drawn are strongly supported by robust experimental results, providing clear insights into the limitations and potential of RLHF scalability."
            },
            "weaknesses": {
                "value": "1. RLHF encompasses a broad range of concepts, yet this paper does not cover all aspects of the literature. For instance, the impact of training data composition for the reward model on RLHF scalability is not explored.\n\n2. While there are numerous RLHF approaches, such as DPO, RPO, and KTO, this paper focuses solely on PPO and GRPO. This limited scope challenges the claim of exploring the impact of methods comprehensively. \n\n3. The study is primarily centered on reasoning tasks, such as math and coding, and does not extend to other important areas like general instruction-following tasks, which limits the generalizability of the findings.\n\n4.  Discussion about potential hypotheses for why RLHF doesn't scale as well as pretraining and  experiments that could help isolate the cause are no presented."
            },
            "questions": {
                "value": "What is the reason that RLHF does not scale?\nFor instance, In Section 4.2.1, why the performance does not always improve when the number of responses increase?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a sequence of experiments to show if the current RL recipe can scale.\nThe experiments range from reward modelling, testing different reward model sizes to generating more samples at training time or RL algorithm choice. They identify several problems with the current approach and conclude that scaling it is not feasible."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper showcases clearly the different shortcomings of the actual RLHF recipe to train LLMs.\nThe paper is a good technical report that reviews what are the different degrees of freedom in the mainstream RLHF recipe.\nIt explains that reward modelling is probably the main bottleneck towards scaling up RL methods.\nThe paper in its current state is more a technical report than a research paper in my opinion. My rating is based on this and not on the underlying quality of the document which is good."
            },
            "weaknesses": {
                "value": "My main concern with the paper is the lack of novelty and originality. There are no new findings obtained through the run experiments:\n - reward hacking is a known problem\n - the different RL approaches and reward normalization schemes are known\n - using N generations and how the performance plateaued is known\n\nNo solution is proposed to the main bottleneck which is reward modelling. If one wants RL to scale, it is also imperative to get rid of the anchor model as it constraints the optimal set of possible solutions. It is only used here to avoid the shortcomings of reward hacking. The authors should expand on this a little bit more. The authors do raise the point that increasing the reward value at training time does not correlate with improving performance with downstream tasks which shows that RLHF in its current state is not a proper training regime.\nIn addition, authors could have found potential directions of future research in the RL literature. To properly scale, especially in sparse environments RL methods need an exploration bonus or a way to understand their uncertainty about the environment. This is independent from a learnt reward model and could potentially scale. Authors should have at least tried to see if they could find a method to scale the diversity of outcomes in the obtained generations or if using different inference mechanisms (in addition to the sampling N parallel answers) could help scaling."
            },
            "questions": {
                "value": "I provided my list of suggestions in the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work investigates the training scaling properties of RLHF for LLMs in the context of reasoning questions. They investigate two main settings: how does scaling effect the policy in RLHF assuming a fixed SFT model, and how does scaling the policy effect performance assuming a fixed RM and training strategy? They find that scaling up data, model size and training time often produces improvements, but these sometimes see diminishing returns at the high end of scaling up, even on a logarithmic x-axis. They additionally find that process supervision produces performance boosts over outcome supervision in-distribution but these improvements sometimes fail to generalise. Using these insights, the paper recommends practical ways in which increased compute can results in better performance for RLHF training for reasoning questions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper performs extensive experiments across a range of scales and settings, making the results much more likely to be robust and generalisable. The topic is important and timely, and hasn't been investigated to this level of rigour before, making this a significant and original contribution. The paper is fairly well written and easy to understand. The research questions are well-scoped and investigated well. Overall, it makes a worthwhile contribution to our understanding of scaling properties in RLHF training."
            },
            "weaknesses": {
                "value": "# Paper framing\n\nThe paper title and introduction claims to address RLHF broadly construed, but the experimental setting is mostly focused on improvements in code and reasoning questions rather than more a more general chat setting. This is fine as a focus of the paper, but I think it would be beneficial to be clearer earlier in the paper that the RLHF setting considered is perhaps different from the standard one readers would expect (RLHF for dialogue).\n\n# dataset and evaluation choice leads to lack of generality in conclusions\n\nThe paper uses a mix of datasets both for training and evaluation. However, it's unclear what the relationship between the training and evaluation datasets is, which means the results are harder to interpret. For example, when we see diminishing returns to scaling various properties, is that because these properties are not producing performance in-distribution in a clean manner, or because that in-distribution performance is not translating to the out-of-distribution evaluations being measured. In general, when measuring scaling trends like done in this paper, it's common practice to disentangle these two hypotheses by evaluating on in-distribution (but held out) data, but that is difficult in this setting given the heterogeneous nature of the RLHF training mixture. I believe the results in the paper are still interested and likely to be generalisable to some extent, but this experiment design decision does hamper the usefulness and transferability of the results to other settings. This is exemplified in the results in figure 2 - some benchmarks benefit from scaling of the properties investigated and some do not, but we don't know whether this is a generalisation failure or an optimisation failure, as we don't have in-distribution performance.\n\nAdditionally, it is difficult to calculate scaling trends for evaluation metrics such as those computed, as they're likely non-monotonic with respect to underlying metrics of performance. When observing that pretraining scaling predictably improves loss, this is easy as loss is grounded in the training procedure. However, evaluations based on metrics not directly optimised for means that it's difficult to explain diminishing returns to scale for that metric as scaling not working well, or whether that metric gets more difficult to improve the higher it is. Again, matching training and evaluation metrics and data more closely would address this problem.\n\nThis could be addressed firstly by making this limitation clear in the paper. It would also be beneficial to perform in-distribution evaluations of these models, where in-distribution means that both the input data and the reward function are matched to those that generated the training data for the policy and reward model respectively.\n\n# contextualising results with respect to related work\n\nSome of the key findings listed in the introduction are similar to those found in the literature. It would be beneficial to explicitly state where your results confirm previous findings, or disagree with them, or go beyond them.\n\n# Unclear statements about comparison to pretraining scaling\n\nIn several places the paper claims that their results show that scaling RLHF is less effective that scaling pretraining. However, this comparison isn't made formal and hence I think this claim should be made more precise, or dropped from the paper. I don't think you can compare scaling in your setting (where training and evaluation objectives and data are different) to the pretraining scaling regime (where they are the same) without being clearer how this is done.\n\n# Smaller issues\n\n* One of your conclusions is that larger policy models benefit less from RLHF when using a fixed size reward model. However, this is confounded by the improved starting point of larger policy models, as the initial SFT is likely better. Combined with the issues above about the metric not being linear, this conclusion doesn't seem valid to me.\n* You say \"Recently, OpenAI-o1 (openai, 2024) has revealed the potential for scaling reinforcement learning at inference time and significantly boosts the reasoning abilities of LLMs.\" (line 135). However, o1 also scales RL at training time as well.\n* when scaling responses per prompt, you're effectively scaling the batch size for training, but you're not also scaling the learning rate, which likely leads to worse performance than is achievable. In general larger batch sizes can accomodate larger learning rates and hence be more performant, and I think it would make more sense to adapt this hyperparameter to the setting to get more compelling results.\n* It would be beneficial to have error bars or confidence intervals of some kind on most of the plots, to understand how noisy these results are. For example, in figure 2, MMLU and AlignBench move by neglible amounts, which could easily be noise in evaluation rather than a real trend.\n\n# Summary\n\nOverall, I think the paper is still worth of acceptance with several easy changes to writing and presentation, as described above. If those changes are made I would raise my score to a 6, and if more substantial experiments were done with in-distribution performance measures, and the smaller issues mentioned above were addressed, I would be happy to raise my score further."
            },
            "questions": {
                "value": "It would be beneficial to get more details on the process supervision technique in the paper, so that it is somewhat self-contained, rather than just referencing another work without detailed explanation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies how policy model performance changes as components of RLHF are scaled. Specifically they look at the effects of sampling multiple responses from the policy model for a given prompt, reward model parameter count, RLHF training example count and policy model parameter count. They also compare policy model performance when RLHF is done with PPO versus GRPO, and with process supervision versus outcome supervision.\n\nFor each component of RLHF, they plot policy model performance on a downstream task (e.g., MATH, GPQA, etc.) at different scales. Where appropriate, trends are fit to policy model performance.\n\nThe paper concludes that RLHF generally does not scale as well as pre-training, and that larger policy models do not seem to benefit as much from RLHF. Despite this, when scaled some of the components of RLHF do yield superior performance, such as sampling from the policy models multiple times, however this benefit is shown to plateau quickly."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**Originality**: To my knowledge this is the first work to directly study the scaling properties of RLHF. The studied techniques have largely appeared in the literature, but I am not aware of equivalently detailed studies of their scaling.\n\n**Clarity**: The writing is generally clear. I did not find any part of the paper confusing. I expect Section 3 to be sufficient for someone not familiar with RLHF to read and have the necessary context for the rest of the paper.\n\n**Quality**: The paper considers a reasonable number of datapoints for most experiments and uses well-respected benchmarks for downstream policy model performance. I think the paper studies RLHF scaling well and that the results do support the points in Section 4.4.\n\n**Significance**: How well RLHF scales is likely of great interest to the broader ML community. RLHF/RLAIF have become extremely common place, and as it is more feasible now for non-commercial projects to do more intensive post-training I think this work is significant."
            },
            "weaknesses": {
                "value": "- The paper claims to study how RLHF scales, but they make some unconventional choices in how they design their RLHF pipeline. Notably, they use a single reward model for reasoning and human preference data. This weakens the results, as they do not directly assess RLHF as it is usually implemented.\n- A very minor point: The paper mentions GRPO but never gives the expanded version of the acronym."
            },
            "questions": {
                "value": "- Would it be possible to run some smaller experiments without the unified reward model from section 3.1? If downstream policy model performance is similar even at smaller scales it would help show that your results track meaningfully to the case where separate reward models are used.\n- Will you open source the reward models, corresponding policy models and the SFT model you use? I can see these models being useful for other work that studies how RLHF scales."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}