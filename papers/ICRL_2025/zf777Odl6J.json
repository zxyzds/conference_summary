{
    "id": "zf777Odl6J",
    "title": "KA-GAT: Kolmogorov\u2013Arnold based Graph Attention Networks",
    "abstract": "Graph Neural Networks (GNNs) excel at processing graph-structured data but often struggle with complex, high-dimensional features and nonlinear relationships. To address these limitations, we propose KA-GAT, a novel model that integrates Kolmogorov-Arnold Networks (KANs) with Graph Attention Networks (GATs). KA-GAT leverages KANs to decompose and reconstruct features, enhancing the model's ability to handle complex data. The multi-head attention mechanism further improves flexibility and interpretability by dynamically focusing on different graph components. Experimental results on benchmark datasets, including Cora and Citeseer, demonstrate that KA-GAT outperforms traditional GNN models such as GCN and GAT in accuracy, precision, and F1-score. These findings underscore KA-GAT's suitability for tasks involving complex graph structures and high-dimensional features, contributing a novel architecture, enhanced interpretability, and robust experimental validation to the field.",
    "keywords": [
        "Graph Neural Networks",
        "Kolmogorov-Arnold Networks",
        "Graph Attention Networks",
        "Multi-head Attention Mechanism",
        "Model Interpretability"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "KA-GAT is a graph neural network combining KAN and GAT, optimised for high-dimensional data processing and model interpretability.",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zf777Odl6J",
    "pdf_link": "https://openreview.net/pdf?id=zf777Odl6J",
    "comments": [
        {
            "summary": {
                "value": "This paper presents KA-GAT, a new graph neural network for representation learning. KA-GAT combines the Kolmogorov-Arnold layer and classical graph attention layer to construct the graph neural network. Thus, the Kolmogorov-Arnold layer is claimed to improve the capability of handling complex data, and the multi-head attention layer can improve the flexibility and interpretability of the proposed KA-GAT. Experimental results obtained from classical graph datasets demonstrate that KA-GAT can outperform empirical GNNs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The idea of making use of the Kolmogorov-Arnold layer and graph attention layer is interesting.\n2. The model is flexible to integrate with other GNN layers."
            },
            "weaknesses": {
                "value": "1. Experimental results are somewhat insufficient. More test datasets or learning tasks should be included in the experiments. I would like to recommend the authors conduct more experiments on well-established datasets, e.g., Pubmed, CoauthorCS, Cora-full, CoauthorPH, Flickr, and ogbn-arxiv, and test KA-GAT with more learning tasks, e.g., graph classification.\n2. Many recent GNNs are not well investigated in the manuscript. Examples include GATv2, APPNP, ADSF GNN, and ARMA GNN. It is also recommended that authors investigate other GNNs recently published in top-tier venues (e.g., NeurIPS, ICLR, ICML, TPAMI, AIJ, and JMLR).\n3. Given 2, More recent GNNs are not compared with the proposed KA-GAT.\n4. The contribution regarding algorithmic and methodological perspectives is limited. The proposed KA-GAT is based on the direct combinations of the Kolmogorov-Arnold layer and graph attention layer. Such a strategy might lack motivation. The authors are suggested to explicitly discuss why combining the Kolmogorov-Arnold layer and graph attention layer is effective in graph representation learning. Moreover, how the Kolmogorov-Arnold layer influences the performance of the proposed KA-GAT should be clearly discussed based on the experimental results. The current version of the manuscript (see Sec. 4.4.1 - 4.4.2) does not provide a meaningful analysis of the presented results. \n5. Theoretical guarantees or analysis of the proposed method (e.g., expressive power) are not provided."
            },
            "questions": {
                "value": "1. How does KA-GAT perform compared with other GNN baselines on more test datasets?\n2. How does KA-GAT perform when compared with more recent GNN baselines?\n3. The motivations behind the proposed approach should be well discussed and more recent approaches should be investigated.\n4. Is there any theoretical analysis demonstrating the learning capabilities of KA-GAT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This reviewer has no critical ethical concerns."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes KA-GAT, a Graph Neural Network (GNN) model combining Kolmogorov-Arnold Networks (KAN) and Graph Attention Networks (GAT) to handle high-dimensional, complex features in graph-structured data. It claims to achieve superior performance on the Cora and Citeseer datasets by using KAN for feature decomposition and a multi-head attention mechanism for dynamic graph component focusing."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The KA-GAT method proposed in the paper, which integrates KAN into GAT, suggests the potential for broader applications of KAN within Graph Neural Networks (GNNs) in the future."
            },
            "weaknesses": {
                "value": "1. Lack of Novelty: Integrating Kolmogorov-Arnold Networks (KAN) into Graph Attention Networks (GAT) does not offer sufficient novelty, as it primarily combines existing techniques without substantial innovation.\n2. Poor Presentation: The overall presentation of the paper is unacceptable.\n3. Insufficient Experimental Support: The experimental setup is limited, with only GCN and GAT used as baselines and tests conducted solely on the Cora and Citeseer datasets."
            },
            "questions": {
                "value": "1. Could the authors perform additional experiments on a broader selection of datasets and include more baseline models for a comprehensive performance comparison?\n2. Given the strong theoretical foundation of KAN, could the authors provide theoretical evidence that integrating KAN with GAT enhances model expressiveness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents KA-GAT, a GNN that consists of different types of layers. First, a Kolmogorov-Arnold Network transforms the initial node features and then the new features are fed to multi-head attention mechanisms along with standard neighborhood aggregation layers such as GAT and GCN layers. The KA-GAT model is evaluated in two node classification datasets. On both datasets, it outperforms the baseline methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed KA-GAT model outperforms the baselines on both Cora and Citeseer. However, KA-GAT is only compared against GCN and GAT. Thus, the baselines that the authors chose to compare their model against are relatively weak and are not considered state-of-the-art.\n\n- From the results shown in Table 2, it seems that the KAN layer is indeed one of the most important components of the KA-GAT model, and it suggests that those layers could potentially lead to further advancements in the field of graph machine learning."
            },
            "weaknesses": {
                "value": "- The KA-GAT model is only evaluated on two datasets, and those datasets correspond to rather small graphs. Therefore, it is not clear whether similar conclusions could be drawn for other datasets (that correspond to different types of graphs or to larger graphs). In my view, it would strengthen a lot the paper if the proposed model was evaluated on a large number of diverse datasets. \n\n- A lot of details about the experiments are missing from the paper. For instance, it is unclear to me how the two datasets were split into training, validation and test sets. It is also not clear whether the hyperparameters of the models were optimized or whether some fixed values were chosen. In addition, for small datasets such as Cora and Citeseer, it is common practice to repeat each experiment multiple times. Since no standard deviations are provided, I guess that the authors report the performance from a single run.\n\n- Some details are missing from the paper. For example, the Multi-Head Attention GNN Layer is not properly explained in the paper. It is unclear whether this layer is also a neighborhood aggregation layer which computes new node representations. If it is indeed a neighborhood aggregation layer, the authors should discuss how this layer is different from a GAT layer.\n\n- Several architectural choices are not well-motivated. No explanations are provided regarding the KA-GAT architecture. For example, the authors do not explain why did they choose to use a single KAN layer and not more of them. In addition, the proposed model consists of both GAT layers and GCN layers. What is the reason behind that? Typically, GNNs consist of instances of a single layer, and not of many of them.\n\n- In l.95-96, the authors claim that KANs have not been widely applied to graph-structured data. This is not true since GNN models that consist of KANs have already been proposed in [1],[2],[3] and [4]. I would suggest the authors update the related work section and discuss the aforementioned works. This would properly demonstrate how this work is positioned with relation to previous works, and also help readers better understand its novelty.\n\n- A large part of the paper is devoted to the discussion of well-known concepts. For instance, the evaluation metrics that are presented in subsection 4.2 are well-known and do not deserve discussion. I would suggest the authors remove the unnecessary content and devote more space to the experimental evaluation of the proposed model.\n\n\n[1] Kiamari, M., Kiamari, M., & Krishnamachari, B. (2024). GKAN: Graph Kolmogorov-Arnold Networks. arXiv preprint arXiv:2406.06470.\\\n[2] Bresson, R., Nikolentzos, G., Panagopoulos, G., Chatzianastasis, M., Pang, J., & Vazirgiannis, M. (2024). Kagnns: Kolmogorov-arnold networks meet graph learning. arXiv preprint arXiv:2406.18380.\\\n[3] De Carlo, G., Mastropietro, A., & Anagnostopoulos, A. (2024). Kolmogorov-arnold graph neural networks. arXiv preprint arXiv:2406.18354.\\\n[4] Zhang, F., & Zhang, X. (2024). GraphKAN: Enhancing Feature Extraction with Graph Kolmogorov Arnold Networks. arXiv preprint arXiv:2406.13597."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents KA-GAT, a novel model that integrates Kolmogorov-Arnold Networks (KANs) with Graph Attention Networks (GATs) to address challenges in graph neural networks (GNNs) with high-dimensional, complex features. The KA-GAT model utilizes KANs to decompose and reconstruct features, enhancing its ability to handle nonlinear relationships, while multi-head attention mechanisms improve its interpretability and flexibility. Through extensive experiments on benchmark datasets like Cora and Citeseer, KA-GAT demonstrates superior performance in accuracy, precision, and F1-score compared to traditional models like GCN and GAT."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-written, clear, and easy to follow, making it accessible to a wide audience.\n- It pioneers the introduction of Kolmogorov\u2013Arnold Networks (KANs) into graph neural networks, which is an interesting and novel approach.\n- Some experiments are conducted to validate the performance of the proposed model."
            },
            "weaknesses": {
                "value": "- The main weakness of the paper is that it feels like a straightforward application of Kolmogorov\u2013Arnold Networks to GATs, without providing a strong justification for doing so. The paper lacks deeper insights into why this integration is particularly necessary or impactful.\n- The rationale for combining multiple GNN layers, such as GCN and GAT, in a single framework is unclear. It appears as if they were stacked together without a clear logical chain, raising concerns about whether sufficient tuning was done to optimize this architecture. It may limit the generalizability of the model and questions the necessity of introducing KAN. It would be interesting to see how the model performs using simpler configurations of GCN or GAT without these combinations.\n- Related to the above, the authors themselves acknowledge that the model is complex due to the many components used. While KAN is introduced with the motivation of reducing the computational complexity of MLPs, the resulting model\u2019s complexity seems to contradict this goal, casting doubt on the motivation behind the paper.\n- In the limitations section, the suggestion that future work could explore techniques like model compression or pruning feels generic and lacks depth. The authors need to critically rethink the necessity and motivation for introducing KAN into GNNs, as the current reasoning is not well substantiated.\n- The experiments are insufficient, as they are only conducted on small, simple datasets like Cora and Citeseer. These datasets may not be representative enough to support the claim that \"traditional GNNs often fall short when dealing with high-dimensional features,\u201d as Cora\u2019s and Citeseer\u2019s feature dimensions are not particularly high, which weakens the argument that KAN is essential for handling high-dimensional data.\n- Also, the claim that \"traditional GNNs often fall short when dealing with high-dimensional features\" lacks sufficient evidence. More justification and empirical support are needed for this assertion.\n- In Section 4.4, the authors simply re-express the results in table form in the text form, but do not provide enough detailed analysis or interpretation of these results. A more thorough discussion of the findings would strengthen the paper."
            },
            "questions": {
                "value": "Please refer to weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}