{
    "id": "kwCHcaeHrf",
    "title": "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an Extreme Value Theory Approach",
    "abstract": "This paper introduces a novel method, Sample-efficient Probabilistic Detection using Extreme Value Theory (SPADE),  which transforms a classifier into an abstaining classifier, offering provable protection against out-of-distribution and adversarial samples. The approach is based on a Generalized Extreme Value (GEV) model of the training distribution in the classifier's latent space, enabling the formal characterization of OOD samples. Interestingly, under mild assumptions, the GEV model also allows for a formal characterization of adversarial samples. The abstaining classifier, which rejects samples based on their assessment by the GEV model, provably avoids OOD and adversarial samples. The empirical validation of the approach, conducted on various neural architectures (ResNet, VGG, and Vision Transformer) and tested on medium and large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its frugality, stability, and efficiency compared to the state of the art.",
    "keywords": [
        "OOD detection",
        "Adversarial detection",
        "Extreme Value Theory"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kwCHcaeHrf",
    "pdf_link": "https://openreview.net/pdf?id=kwCHcaeHrf",
    "comments": [
        {
            "summary": {
                "value": "The paper presents SPADE (Sample-efficient Probabilistic Detection using Extreme Value Theory) = a new method for detecting out-of-distribution (OOD) and adversarial samples in neural network classifiers. The key contributions are:\na. A novel method to transform a classifier into an \"abstaining classifier\" that can refuse to make predictions on suspicious inputs\nb. Use of Extreme Value Theory (EVT) to model the training distribution in the classifier's latent space\nc. Mathematical guarantees for detecting both OOD and adversarial samples\nd. Experimental validation across multiple architectures (ResNet, VGG, ViT) and datasets\n\nI am pretty unfamiliar with Extreme Value Theory, so please flag if if I'm misunderstanding something basic in my review. But I'm very familiar with OOD detection and adversarial attacks, so it could compensate for it."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Theoretical Foundation: The approach is grounded in statistical theory (EVT)\n2. Sample Efficiency: Good performance even with strong subsampling of the train set\n3. Versatility: Works with different architectures and can handle both OOD and adversarial inputs\n4. Evaluation: Tested against strong baselines on multiple datasets"
            },
            "weaknesses": {
                "value": "# 1. Missing challenging near-OOD evaluation\nThe evaluations in the Table 2 and Table 3 are missing a very simple, yet challenging near OOD detection task which is CIFAR-100 vs CIFAR-10 (and the other way round). [1] shows strong results for large models and also provides a human score to benchmark against. I would be very curious to see the AUROC, AUC and FPR95 on that. I believe that this would be a very worthwhile and easy evaluation to add and I think the authors should try it. The AUROC in particular is something I would really like to see. Also a set of examples of where the model fails. **I will consider increasing my score if the authors successfully address this point.**\n\n# 2. A collection of weakness that do not need to be addressed in the rebuttal:\n2.1 Computational Complexity: The GEV model is quadratic in the number of samples\n2.2 Performance Trade-off: While SPADE performs well in general, it's sometimes outperformed by simpler methods like KNN for specific tasks\n2.3. Parameter Sensitivity: The effectiveness depends on the choice of teacher model and its latent space characteristics\n2.4. High FPR95 Values: Shows tendency to be overly cautious. Perhaps rejecting valid samples?\n\n# 3. Weak adversarial attacks used only\nThe paper's evaluation of adversarial robustness is limited. They only test against the FGSM attack (=very basic) with small perturbation magnitudes (epsilon from 0.001 to 0.004). Given that SPADE is positioned as providing security guarantees against adversarial examples, a more thorough evaluation against SOTA attacks would be necessary to support the claims. Additionally, the authors should clarify whether their evaluation considers attacks targeting just the classifier or the full system including the OOD detector, and include both targeted and untargeted attack scenarios. **This is especially relevant to the next weak point.**\n\n# 4. White-box attacks\n\nI have the following concern that I would like you to address directly:\n\n1. SPADE offers an elegant theoretical framework for OOD detection based on a distance metric in the latent space\n2. [1] shows that distance metrics in the latent space can be amazing at even hard, near-OOD detection\n3. [2] shows that white-box directly targeting such a metric can *still completely destroy it* with a simple adversarial attack\n\nGiven SPADE's reliance on distance metrics in latent space and GEV models, it may be vulnerable to white-box attacks that:\n\n4. Directly optimize perturbations to minimize latent space distances to k-nearest neighbors while maintaining misclassification\n5. Exploit knowledge of the GEV model parameters to generate samples that appear in-distribution\n6. Target the gap between separate class-specific GEV models to find adversarial blind spots\n\nThe authors should consider and discuss these potential vulnerabilities, particularly since they claim SPADE as providing security guarantees. While Theorem 1 provides a lower bound on adversarial perturbation magnitude, an analysis of potential attack strategies and additional defensive measures (such as ensemble approaches or adversarial training) would strengthen the paper's security claims.\n\n# References\n\n[1] Exploring the Limits of Out-of-Distribution Detection. Stanislav Fort, Jie Ren, Balaji Lakshminarayanan (https://arxiv.org/abs/2106.03004)\n\n[2] Adversarial vulnerability of powerful near out-of-distribution detection. Stanislav Fort (https://arxiv.org/abs/2201.07012)"
            },
            "questions": {
                "value": "Included in the weaknesses section.\n\nI am primarily interested in the authors answers to:\na. the CIFAR-100 <-> CIFAR-10 challenging near OOD task(s)\nb. the white-box attack on latent metrics point"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper uses extreme value theory as a means to detect out-of-distribution (OOD) samples. By abstaining to predict on samples that are determined to be OOD, the proposed approach is shown to yield high-probability robustness guarantees against adversarial attacks up to a certain perturbation magnitude. Experiments are conducted on CIFAR-10, CIFAR-100, and ImageNet to assess the performance of the proposed method compared to prior OOD detection schemes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The problem considered is interesting, and fits well within the scope of the ICLR community. The paper is easy to follow, and does a good job at clearly yet concisely introducing the main tools being used (e.g., extreme value theory). There is a nice blend of conceptual/theoretical development, and empirical evaluation."
            },
            "weaknesses": {
                "value": "See my specific Questions below."
            },
            "questions": {
                "value": "1. Line 90: You use the acronym ID before defining it.\n2. Line 102: You should de-italicize the term $\\eta$-invariant that you are definining.\n3. Definition 3: The role of the parameters $\\mu,\\sigma,\\zeta$ are unclear in how you wrote Definition 3. It seems to me like you need to re-word things to somehow say that \"there exist $\\mu,\\sigma,\\zeta$ such that $P(Z^{(\\ell)} < z) \\to G_{\\zeta,\\mu,\\sigma}(z)$.\" Otherwise, the reader might be inclined to think that the choice of parameters is somehow \"up to us.\"\n4. Line 150: Extreme value theory has been utilized by the ML community in popular robustness verification works such as [1]. I encourage you to take a look at [1] as well as the references therein.\n5. Line 169: \"For $Y = c$, let $Z_c$ be the random variable defined as the distance between $h(X)$ and its $k$-th nearest neighbor in latent distance, belonging to $\\mathcal{D}$ with same class $c$.\" It would help readability if you write out the mathematical expression defining the random variable $Z_c$.\n6. Line 170: What is $k$? Is this a user-specified hyperparameter of your method?\n7. Line 171: The notation here is inconsistent; you should be writing $Z^{(\\ell)}$, not $Z^{(l)}$. Also, before, you used $P$ for probability, not $Pr$.\n8. Line 215: Where is the minimum coming into this equation defining the extreme value distribution $G^{(c,c')}$? It seems like without the minimum inside the probability expression on the right, you aren't really considering extreme values.\n9. Theorem 1: It looks like now you are assuming that the instance space $\\mathcal{X}$ is a Euclidean space $\\mathbb{R}^d$ (so that you can define norms on the instance space as well as Lipschitzness of the embedding map), which was not explicitly assumed before. Therefore, you should make this clear in the theorem statement.\n10. Theorem 1: Do you mean to say $f_\\tau$ instead of $f_\\epsilon$ in the statement of the theorem?\n11. Experimental Setting: You state that you generate attacks using FGSM. At this point FGSM, as introduced in Goodfellow et al., 2015, is not really considered by the adversarial robustness community to be a strong attack. At the very least, I'd expect you to use projected gradient descent (PGD) [2], or something stronger like AutoAttack [3] which is used in the RobustBench benchmarks. Have you tested your methods against these stronger attacks?\n12. There are a handful of distinct conceptual discussions and innovations leading up to your experiments in Sections 4 and 5. However, there is no explicit description of SPADE where you bring everything together into a single model/algorithm. I think it would greatly benefit readability if you were to conclude your conceptual discussions by writing out SPADE as an explicit algorithm (including any robustness certification steps), immediately before moving into experiments.\n13. Line 318: I'm assuming by \"KNN\" you are referring to $k$-nearest neighbors? You should clearly define this acronym before using it. Similarly, MSP is not defined. Please define it, or at the very least, associate a reference to it.\n14. Line 190 and Experiments: On line 190 you said that you would tease out the effect of the latent embeddings on the OOD tests in Section 5 (since your proposed OOD test in Definition 4 depends on the embedding from instance space to latent space). However, after reading through the experiments of Section 5, I do not see how you are answering this concern. Could you clarify?\n\n[1] \"Evaluating the robustness of neural networks: An extreme value theory approach,\" Weng et al., ICLR 2018\n\n[2] \"Towards deep learning models resistant to adversarial attacks,\" Madry et al., ICLR 2018\n\n[3] \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,\" Croce and Hein, ICML 2020"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose sample-efficient probAbilistic detection using extreme value theory (SPADE), which models the training distribution using Extreme Value Theory to create a statistically efficient test that identifies and rejects both OOD and adversarial samples with high probability. Their contributions include a formal OOD definition in relation to a model's latent space, a frugal OOD detection test grounded in EVT, and demonstrated effectiveness against strong baselines across diverse model architectures."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces SPADE, a novel approach leveraging Extreme Value Theory (EVT) for OOD detection, which provides a statistically grounded method to detect and reject OOD samples effectively.\n\n2. SPADE not only detects OOD samples but also offers provable guarantees for rejecting adversarial examples, making it robust against potential adversarial attacks.\n\n3. The approach is experimentally validated across multiple model architectures, demonstrating its versatility and effectiveness compared to strong baselines, enhancing its relevance for various real-world applications."
            },
            "weaknesses": {
                "value": "1. The provable guarantees only hold on **strong** assumptions. In theorem 1, the authors assume the embedding network is $K$-Lipschitz. Firstly, a $K$-Lipschitz network can already provide provable robustness on adversarial examples. Secondly, due to the complexity of the neural network, it is not possible to calculate the exact Lipschitzness empirically. Thus, this theorem offers limited utility for practical OOD detection scenarios.\n\n2. The experimental setup largely follows standard OOD detection settings and does not address the \"provable protection of OOD examples,\" which is claimed as the main contribution of this work. This raises doubts for the reviewer about whether SPADE can indeed provide provable protection for OOD examples, given the limitations outlined in Weakness 1.\n\n3. SPADE performs poorly on near OOD examples, with Table 1 showing that it is outperformed by the MSP baseline in 3 out of 5 cases. Additionally, the average rank is not an ideal metric. To demonstrate the superiority of the proposed method, SPADE should outperform the baselines across a majority of the datasets."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies OOD detection. The authors study OOD in terms of GEV, thus enabling a characterization of the probability/confidence of OOD samples. Based on this framework, they propose a method to detect OOD and adversarial examples."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The method is well-motivated, and the theory is sound. I love the first-principle design. Paper is well-written and easy to follow. The studied problem is important. Introducing GEV to OOD in this way is novel."
            },
            "weaknesses": {
                "value": "The main weakness is the weak experimental results. In almost all result tables, the proposed method does not achieve significant benefit over the baselines. In particular, Table 1 shows the proposed method is uniformly dominated by the baseline methods. Table 2 seems to suggest ViT is a particularly strong teacher model for the proposed method, maybe revisiting Table 1 with ViT could bring more benefits.\n\nThe authors claim that the proposed methods help to detect adversarial examples. However, the common practice of a defense is to endure adaptive defense, which is aware of the detection and tries to find an adversarial example that surpasses the detection. This highly depends on the claim that the authors would like to make, but overall such experiments would help to establish the significance of the defense.\n\nIn summary, while I like the design and the theories, the experimental results do not suggest sufficient improvements, questioning the significance of this work."
            },
            "questions": {
                "value": "Could the authors provide more convincing experimental results about the benefit of the proposed method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}