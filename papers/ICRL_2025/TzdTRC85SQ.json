{
    "id": "TzdTRC85SQ",
    "title": "Multi-Task Dense Predictions via Unleashing the Power of Diffusion",
    "abstract": "Diffusion models have exhibited extraordinary performance in dense prediction tasks. However, there are few works exploring the diffusion pipeline for multi-task dense predictions. In this paper, we unlock the potential of diffusion models in solving multi-task dense predictions and propose a novel diffusion-based method, called TaskDiffusion, which leverages the conditional diffusion process in the decoder. Instead of denoising the noisy labels for different tasks separately, we propose a novel joint denoising diffusion process to capture the task relations during denoising. To be specific, our method first encodes the task-specific labels into a task-integration feature space to unify the encoding strategy. This allows us to get rid of the cumbersome task-specific encoding process. In addition, we also propose a cross-task diffusion decoder conditioned on task-specific multi-level features, which can model the interactions among different tasks and levels explicitly while preserving efficiency. Experiments show that our TaskDiffusion outperforms previous state-of-the-art methods for all dense prediction tasks on the widely-used PASCAL-Context and NYUD-v2 datasets. Our code will be made publicly available.",
    "keywords": [
        "Diffusion model",
        "Multi-task learning",
        "Dense prediction",
        "Joint denoising",
        "Cross-task encoding"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TzdTRC85SQ",
    "pdf_link": "https://openreview.net/pdf?id=TzdTRC85SQ",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a diffusion-based model for multi-task dense prediction. During each training step, it first projects multi-task labels into a unified feature space and applies noise within that space. It then implements a cross-task diffusion decoder to iteratively predict the denoised features. Experimental results demonstrate performance improvements over previous methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The study of multi-task learning is an important direction.\n\n- We see performance improvement on PASCAL and NYUD compared to previous SOTA methods.\n\n- The algorithm scheme is helpful for understanding.\n\n- Extensive experiments have been conducted to thoroughly evaluate the model performance."
            },
            "weaknesses": {
                "value": "- The figures lack sufficient detail. The task interaction module and level interaction modules should be included in Figure 2 for better clarity.\n\n- In Algorithm 2, the term \"masks_pred\" is not mentioned in the context. Should it be \"m_preds\"?\n\n- The novelty is somewhat limited, as previous research has already demonstrated that diffusion-based multi-task learning is feasible."
            },
            "questions": {
                "value": "- Can you visualize the cross-task map at different steps to better understand it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This submission proposes a network named TaskDiffusion based on the recent diffusion work (DDP[1]) for multi-task dense prediction. The TaskDiffusion consists of three proposed modules. Pixel-level encoder encodes an image to feature which is utilized as multi-level conditions for diffusion decoder. Cross-task label encoder encodes the label of different tasks into a shared feature as a denoising input. Cross-task diffusion decoder is used for cross-task feature interaction and prediction decoding. The evaluations on two benchmarks demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This submission proposed a new diffusion model for MTL dense prediction, inspired by DDP.\n- The empirical evaluations on two datasets (PASCAL-Context and NYUD-v2) suggest that the proposed method could improve the MTL performance with 4 and 5 tasks."
            },
            "weaknesses": {
                "value": "- The related works of MTL dense prediction are quite old. Some recent research[1-4] related to MTL dense prediction should be considered as references.\n- The claim \u201cThis strategy utilizes the diffusion model to model the task relations explicitly in a coarse-to-fine process during denoising, \u2026\u201d in lines 79-80 should be clarified more deeply. To be specific, the illustration of $A_i$ in Fig.3 is hard to comprehend. What is the relationship between a pair of task?\n- Some experimental details should be corrected and provided. For instance, the original PASCAL-Context does not provide the label for the surface normal task. And what is the resolution of input?\n- Some comparisons are not fair. For example, the results of HRNet18 or HRNet48 should not be compared to ViT-large, since the backbone affects the performance remarkably.\n- Some magnificent results are missing. In Table.1 and 2, the single task should be considered a baseline to figure out the impact of the negative transfer and the improvement.  And $\\Delta_m$ should be provided in Tables 1 and 2.\n- The presentation of Table.3 and 4 should be improved. How the $\\Delta_m$ is calculated? If it is based on the single task baseline, it should be provided.\n\n[1] Exploiting Diffusion Prior for Generalizable Dense Prediction, CVPR 2024\n\n[2] UNITE: Multitask Learning With Sufficient Feature for Dense Prediction, IEEE Transactions on Systems, Man, and Cybernetics: Systems\n\n[3] Rethinking of Feature Interaction for Multi-task Learning on Dense Prediction, arXiv 2312.13514\n\n[4] MultiMAE: Multi-modal Multi-task Masked Autoencoders, ECCV 2022"
            },
            "questions": {
                "value": "Please refer to weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a diffusion-based method for multi-task dense prediction, leveraging the conditional diffusion process. The method effectively models task relationships during the denoising process through the cross-task diffusion decoder and cross-task label encoder. The experiments demonstrate its effectiveness on two benchmark datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed framework facilitates joint diffusion denoising of multiple tasks, allowing the diffusion process to effectively capture cross-task relationships, which is beneficial for multi-task learning.\n2. The method demonstrate strong performance on two benchmark datasets, PASCAL-Context and NYUD-v2."
            },
            "weaknesses": {
                "value": "1. The proposed modules have limited novelty. The cross-task diffusion decoder builds upon existing works, such as DDP [1], while incorporating interaction techniques from multi-task learning. The cross-task label encoder functions as an inverse process of task-specific heads in standard multi-task models. It would be better if the paper could demonstrate more insights in the utility of diffusion for multi-task dense prediction.\n2. The authors are recommended to compare their method with more state-of-the-art methods, especially those published in 2024, such as [2, 3].\n3. There is room for improvement in presentation, including language and figures. For example, the text in Figure 1 right is too small to be easily readable.\n\n[1] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp: Diffusion model for dense visual prediction. arXiv preprint arXiv:2303.17559, 2023.\n[2] Yuqi Yang, Peng-Tao Jiang, Qibin Hou, Hao Zhang, Jinwei Chen, and Bo Li. Multi-task dense prediction via mixture of low-rank experts. In CVPR, 2024.\n[3] Shuo Wang, Jing Li, Zibo Zhao, Dongze Lian, Binbin Huang, Xiaomei Wang, Zhengxin Li, and Shenghua Gao. TSP-Transformer: Task-specific prompts boosted transformer for holistic scene understanding. In WACV, 2024."
            },
            "questions": {
                "value": "1. In Section 3.2, task-specific auxiliary heads are used to generate intermediate predictions that are supervised by ground truth labels. It is interesting to know the contribution of these auxiliary heads to the final results, as they have different training procedure with the diffusion process.\n2. In Algorithm 2, what is the relationship between masks_pred and m_preds?\n3. In Table 1, the FLOPs of the proposed method with a ViT-large backbone is reported as 610G. However, in Table 6, the ablation study indicates that the model with 3 steps utilizes 667G FLOPs. Why does the model with the ViT-base backbone has a higher computational cost than the ViT-large backbone?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The proposed TaskDiffusion encodes the task-specific labels into a task-integration feature space to unify the encoding strategy.In addition, the proposed cross-task diffusion decoder conditioned on task-specific multilevel features, which can model the interactions among different tasks and levels explicitly while preserving efficiency. The proposed TaskDiffusion\u2019s experiment results also achieve comparable performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1 This paper is well-organized.\n\n2 This paper leverages diffusion models as an effective solver for multi-task dense prediction.\nThe core idea of seeking a task-specific proxy to perform cross-task reasoning via diffusion model is intuitive.\n\n3 It outperforms the state-of-the-art in most cases."
            },
            "weaknesses": {
                "value": "1 The value of \\Delta_m (MTL gain metric) requires the value of  the single task (ST) model. It is suggested that the authors release the value of ST in Table 3. Following the calculation criteria of MTL on \\Delta_m metric, showing Baseline ST and MT values will have a beneficial impact on the development of MTL. The reviewer saw Table 7, but it appeared in the appendix rather than in the main paper.\n\n2 From the results in Table 3, the core diffusion decoder designed in this paper does not seem to bring much growth to the model. Table 3 also reveals that a good encoder leads to greater performance gains. The relative performance improvement brought by the proposed Cross-Task Diffusion Decoder is not even comparable to ATRC and InvPT.\n\n3 Task-fused features U are generated by A$\\cdot$F in line 302. Why the features after A$\\cdot$F are fused features? The same question arises on line 308. Why can this matrix multiplication represent feature fusion and what is its mathematical interpretation?\nBoth task-fused features and task-fusing features mean the same map U.Moreover, the same problem has $p$ in 302 line and $p$ in Eq.2.These inconsistency are confusing to the reader.\n\n4 To ensure comprehensive evaluation, it is imperative that the study provides a comparative analysis of the proposed technique against the most recent methods (\"DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction\" AAAI 2023; \"Multi-Task Learning with Multi-Query Transformer for Dense Prediction\" TCSVT 2024; \"Multi-Task Dense Prediction via Mixture of Low-Rank Experts\" CVPR 2024). In addition, the discussion of the decoder-based MTL approach in the related work section lacks an introduction to recent works, such as DeMT AAAI\u201923, 3DAwareMTL ICLR\u201924, MQTransformer TCSVT\u201924 and MLoRE CVPR\u201924.\n\n5 Figure 4 does not seem to demonstrate the visualization advantage of the proposed TaskDdiffusion model in the appendix, compared to TaskPrompter."
            },
            "questions": {
                "value": "1 The authors state: \u201cTo our knowledge, we are among the first to leverage diffusion models in \u2026\u201d. The DiffusionMTL (CVPR\u201924) model already took advantage of the diffusion model.\n\n2 What is Cross-Task Label Encoder based on?  Some tasks are labeled with images or data in matlab format. How do authors encode these different data formats?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}