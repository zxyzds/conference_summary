{
    "id": "jMZjIi9JcC",
    "title": "Empirical Study on Enhancing Efficiency in Masked Image Modeling Pre-training",
    "abstract": "The combination of transformers and masked image modeling (MIM) pre-training framework has shown remarkable potential in various vision tasks. However, the high computational cost of pre-training hinders the practical application of MIM.\n   This paper introduces \\emph{FastMIM}, a simple and versatile framework that expedites masked image modeling through two steps: (i) pre-training vision backbones using low-resolution input images and (ii) reconstructing Histograms of Oriented Gradients (HOG) feature instead of original RGB values of the input images.\n   Furthermore, we propose \\emph{FastMIM-P}, which progressively increases the input resolution during the pre-training stage to improve the transfer learning performance of models with high capacity. We point out that: (i) a wide range of input resolutions during pre-training can result in similar performances in fine-tuning and downstream tasks such as detection and segmentation; (ii) the shallow layers of encoder are more important during pre-training, and discarding the last few layers can speed up the training process without affecting fine-tuning performance; and (iii) HOG is more stable than RGB values when transferring resolution. Equipped with \\emph{FastMIM}, any type of vision backbone can be efficiently pre-trained. For example, using ViT-B/Swin-B as backbones, we achieve 83.8\\%/84.1\\% top-1 accuracy on ImageNet-1K. Compared to previous approaches, our method can achieve better top-1 accuracy while accelerating the training procedure by 5\u00d7.",
    "keywords": [
        "Self-supervised learning",
        "Empirical study"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-14",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=jMZjIi9JcC",
    "pdf_link": "https://openreview.net/pdf?id=jMZjIi9JcC",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces FastMIM, a framework that accelerates masked image modeling (MIM) by using low-resolution input images and Histograms of Oriented Gradients (HOG) features for pre-training, achieving improved efficiency and accuracy in transfer learning tasks. The authors demonstrate that FastMIM achieves superior top-1 accuracy on ImageNet-1K and speeds up training by approximately 5\u00d7, with additional insights on resolution variation, layer importance, and the stability of HOG features over RGB values."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe paper identifies and leverages low-resolution input images to significantly reduce both the pre-training time and memory usage.\n2.\tBy reconstructing Histograms of Oriented Gradients (HOG) features, the method preserves crucial texture information that is often lost with lower resolutions.\n3.\tThe proposed FastMIM framework is validated through extensive experiments."
            },
            "weaknesses": {
                "value": "1.\tRisk of Information Loss: By reducing the resolution and replacing RGB pixels with HOG features, some fine-grained details may be lost, potentially affecting performance in tasks that require precise image analysis.\n2.\tTask-Specific Adaptability: While HOG features perform stably in general vision tasks, they may not be suitable for tasks that require precise texture or color information, such as image generation or super-resolution reconstruction.\n3.\tLimitations of Method Generality: The performance of this method during pre-training depends on the stability of HOG features, and it may not be suitable for model architectures or specialized tasks that require a broader range of features."
            },
            "questions": {
                "value": "1.\tThe method is well-suited for a wide range of vision tasks, particularly in environments with limited computational resources. However, considering its general applicability and performance across all tasks, especially those requiring high-detail image analysis, further improvements may be necessary to minimize information loss. If the above limitations can be overcome, particularly the issues with task-specific adaptability, this work holds great potential."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces FastMIM, a framework that speeds up masked image modeling (MIM) pre-training by using low-resolution images and reconstructing HOG features instead of RGB values. FastMIM-P further improves performance by progressively increasing input resolution during pre-training. This approach maintains good performance and accelerates training compared to previous methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Speedup in Pre-training: By using low-resolution input images, FastMIM significantly reduces pre-training time.\n2. High Accuracy: Despite accelerating the training process, FastMIM maintains high accuracy.\n3. The paper provides detailed experimental results, which help in understanding the robustness and reliability of the proposed method. The writing is clear and logically structured, making it easy to follow the methodology and results."
            },
            "weaknesses": {
                "value": "The contribution is limited. The paper improves MIM by using low-resolution images as input and HOG features as learning targets, with the advantages of HOG features already validated in the maskfeat work. Using low-resolution image input can accelerate the pre-training process without causing significant performance degradation, which indeed can serve as an acceleration trick for MIM pre-training. However, since model pre-training does not occur frequently in practical applications, the time cost of pre-training is acceptable. The main purpose of pre-training is to obtain better foundational model representations."
            },
            "questions": {
                "value": "1. The caption is incorrect in Figure 4, the text in the top left corner of the figure should annotate the line segment as HOG instead of pixel.\n2. How does the method perform compared to other methods on larger pretraining datasets such as IM-21K or with longer pretraining epochs? If it can be demonstrated that Fast-MIM has better scaling-up capabilities, then its accelerated training would be more advantageous."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Masked Image Modeling (MIM) is a type computaional intentive pre-training methods. To  reduce the computational overhead of MIM, this paper proposed FastMIM to speeds up pre-training through two main strategies: (i) using low-resolution input images and (ii) reconstructing Histograms of Oriented Gradients (HOG) features instead of the original RGB values. FastMIM is compatible with both hierarchical and non-hierarchical transformer models. Experiments on ImageNet-1k, MS COCO and ADE20K are conducted."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The visual analysis presented in Figure 3 is clear and compelling.\n\n-  The paper demonstrates a strong motivation for the work, introducing efficient strategies to mitigate the high computational cost associated with MIM pre-training.\n\n-  The proposed method shows promising results on the MS COCO object detection task, highlighting its potential effectiveness."
            },
            "weaknesses": {
                "value": "- From Figure 4, the accuracy does not demonstrate a clear saturation trend. Providing the fine-tuning accuracy after 1600 pre-training epochs would make the results more convincing.\n\n- Missing citation: In lines 190\u2013200 (Encoder depth in pre-training), the observation that discarding the last several layers (blocks) in pre-training yields nearly the same performance has been previously noted in MIRL [1]. \n\n- While the reviewer acknowledges the efficiency of the proposed method, the performance gains of FastMIM on ImageNet-1K and ADE20K are marginal.\n\n- Experimental results for ViT-L are not provided.\n\n[1] Huang, Guoxi, Hongtao Fu, and Adrian G. Bors. \"Masked image residual learning for scaling deeper vision transformers.\" Advances in Neural Information Processing Systems 36 (2023): 57570-57582."
            },
            "questions": {
                "value": "- Please refer to the Weaknesses.\n \n- Additionally, the reviewer is concerned about the effectiveness of FastMIM for large-scale models such as ViT-L and Swin-L, as these models tend to be more data-hungry. Reducing the input size decreases the number of visual tokens, which in turn significantly reduces the totall number of possible mask patterns. This is equivalent to a reduction in input diversity. \n\nIf the authors can address the reviewer's questions one by one, the reviewer is willing to consider raising the score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces FastMIM, a simple and versatile framework that accelerates masked image modeling pre-training through two steps: (i) pre-training vision backbones using low-resolution input images, and (ii) reconstructing Histograms of Oriented Gradients (HOG) features instead of original RGB values. FastMIM-P, a variant, progressively increases input resolution during pre-training. Additionally, it emphasizes the importance of shallow layers during pre-training and suggests discarding the last few layers to speed up training without affecting fine-tuning performance. FastMIM enables efficient pre-training of any vision backbone, and has made lots of verification experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper provides valuable insights into the design of MIM frameworks, including the importance of input resolution, the role of shallow layers during pre-training, and the stability of HOG features when transferring resolution."
            },
            "weaknesses": {
                "value": "1. The main contributions of the proposed method, i.e., the HOG reconstruction target and low-resolution input, have been extensively discussed in previous papers such as MaskFeat and SimMIM. This makes the proposed method appear less novel.\n2. The empirical study mainly focuses on input resolution, training epochs, prediction targets, and the number of decoder/encoder layers. However, these aspects are common knowledge in MIM research, thus providing limited contribution to the community.\n3. When verifying the importance of shallow layers during pre-training, the linear probing accuracy is missing, making the argument less convincing."
            },
            "questions": {
                "value": "See weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work conducts an empirical study on enhancing training efficiency in masked image modeling (MIM). The authors introduce a simple and versatile framework that expedites MIM by using low-resolution images and reconstructing HOG features."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Improving the efficiency of MIM pre-training is a good topic and has significant practical value. \n2. This paper contains numerous experiments and reveals some interesting findings. For instance, the authors find that discarding the last few layers can speed up the training process without affecting fine-tuning performance."
            },
            "weaknesses": {
                "value": "1. My main concern lies in the lack of novelty. The problem of low efficiency in MIM has been pointed out by previous works as the paper stated. The idea of using low-resolution images to expedite training is too simple and has been implemented in previous works. Additionally, the superiority of reconstructing HOG features has been demonstrated by its original paper. \n\n2. The proposed method has small gap for the performance of models with high capacity, e.g., 0.3% for Swin-L. It significantly diminishes the advantages of the proposed approach as one may just reduce training epochs to achieve this trade-off. For smaller models, it is more efficient to reconstruct features of pre-trained larger models like a kind of knowledge distillation. \n\n3. The proposed FastMIM-P progressively increases resolution to alleviate the above problem. However, the technical novelty of this approach is still limited. And the resolution and training schedule need to be carefully designed to achieve a better space-time trade-off as the authors stated. \n\n4. In Table 4, it can be seen that the authors try to demonstrate the generalization across different visual backbones. Nevertheless, the performance gain on some backbones lags behind recent MIM works due to the lack of integration of advanced technology such as masked convolutions."
            },
            "questions": {
                "value": "In Table 4, it is more meaningful to compare total training time, including the pre-training time and fine-tuning time."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}