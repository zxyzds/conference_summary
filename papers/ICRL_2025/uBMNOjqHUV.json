{
    "id": "uBMNOjqHUV",
    "title": "A bird's eye view on informed classification",
    "abstract": "Neurosymbolic AI is a growing field of research aiming to combine neural network learning capabilities with the reasoning abilities of symbolic systems. In this paper, we tackle informed classification tasks, i.e. multi-label classification tasks informed by prior knowledge that specifies which combinations of labels are semantically valid. Several neurosymbolic formalisms and techniques have been introduced in the literature, each relying on a particular language to represent prior knowledge. We take a bird's eye view on informed classification and introduce a unified formalism that encapsulates all knowledge representation languages. Then, we build upon this formalism to identify several concepts in probabilistic reasoning that are at the core of many techniques across representation languages. We also define a new technique called semantic conditioning at inference, which only constrains the system during inference while leaving the training unaffected, an interesting property in the era of off-the-shelves and foundation models. We discuss its theoritical and practical advantages over two other probabilistic neurosymbolic techniques: semantic conditioning and semantic regularization. We then evaluate experimentally and compare the benefits of all three techniques on several large-scale datasets. Our results show that, despite only working at inference, our technique can efficiently leverage prior knowledge to build more accurate neural-based systems.",
    "keywords": [
        "Neurosymbolic",
        "Deep Learning",
        "Knowledge Representation",
        "Probabilistic Reasoning"
    ],
    "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)",
    "TLDR": "We introduce a unified formalism for probabilistic neurosymbolic techniques to tackle classification tasks informed with prior knowledge.",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=uBMNOjqHUV",
    "pdf_link": "https://openreview.net/pdf?id=uBMNOjqHUV",
    "comments": [
        {
            "title": {
                "value": "Baselines and fuzzy regularization"
            },
            "comment": {
                "value": "Regarding baselines, we considered that semantic regularization and semantic conditioning constitued strong baselines to evaluate our contribution semantic conditioning at inference.\n\nBesides, since fuzzy-based techniques are developed for propositional formulas and are not syntax independent, the evaluation of fuzzy regularization on our tasks would have depended on how we to chose to translate prior knowledge into a propositional formula. This choice seemed arbitrary to us and would have required to take some time to justify it.\n\nIn fact, a specific translation into propositional formula (to be specific, compilation to the \"unfolded\" version of a smooth dDNNF) makes fuzzy regularization and semantic regularization equivalent. This means that fuzzy regularization is, in this narrow sense, present in our baselines.\n\nWe did not expect another encoding into propositional formulas to make fuzzy regularization more accurate and therefore decided not to explicitely include it into our evaluations."
            }
        },
        {
            "title": {
                "value": "Comparison to Niepert et al. and Pogancic et al."
            },
            "comment": {
                "value": "The comparison to Nierpert et al. and Pogancic et al. can be delicate since their work is not mainly centered on informed multi-label classification, as in our paper, but tries to integrate combinatorial solvers inside a differentiable pipeline of neural networks. However, their work can be adapted into a neurosymbolic technique for informed classification, as illustrated by their experiments on simple path tasks, and it is of this adaption that we talk below.\n\nThe originality of Nipert's work is that they do not rely on exact counting but on approximate counting using the Gumbel trick with perturbed calls to exact MPE solvers. In other terms, their technique can be seed as an approximate method for semantic conditioning when PQE cannot be solved exactly but MPE can. Therefore, if it is similar to our contribution (semantic conditioning at inference) in terms of computational complexity in the sense that they do not rely on solving exact PQE, even though several calls to MPE solvers are necessary in their case while only one is used in semantic conditioning at inference. However, it also impacts training and is closer to semantic conditioning than semantic conditioning at inference in this sense (which is not surprising since it is an approximation of semantic conditioning). This implies that it cannot be applied to usecases based on foundation or off-the-shelves models like semantic conditioning at inference can.\n\nMoreover, some of their usecases rely on a restriction on the space of logits (for instance only allowing positive logits) to ensure the tractability of MPE, which is not the case in our work.\n\nSince exact semantic conditioning could be implemented tractably for all the tasks considered in our experiments, we decided not to include the approximate version allowed by the work of Niepert and Pogancic. We agree that a comparison with their work on tasks where semantic conditioning at inference was prefered to semantic conditioning for computational complexity purposes would be very useful.\n\nNB: I am not sure to understand what you mean by \"maintaining the distribution over the entire space of outputs\". If you mean that the value of the distribution is not computed for all possible states of the output space, then it is the case of all probabilistic neurosymbolic techniques, including those that perform weighted counting (PQE), as such an implementation would very quickly become intractable."
            }
        },
        {
            "title": {
                "value": "Relation to PGMs"
            },
            "comment": {
                "value": "There are interesting ties between our work and PGMs.\n\nFirst, the conditioned distribution on which semantic conditioning is based can be captured by a PGM when the prior knowledge is expressed as a CNF formula: the primal graph of the CNF (ie. the graph that has a node for each variable and an edge between two nodes if both variables appear in the same clause) defines an undirected graphical model that captures the conditioned distribution. In such case, the loss and inference modules of semantic conditioning simply correspond to maximum likelihood estimation and mode estimation of the PGM, and standard sum-product and max-sum message passing algorithms on PGMs can be used to perform PQE and MPE. In fact, the work on Large-Scale Object Classification using Label Relation Graphs by Jia Deng et al. mentioned in our paper can be seen as a semantic conditioning technique based on PGMs and custom message passing algorithms.\n\nMoreover, when prior knowledge is expressed in CNF, message passing algorithms can also be used to compute PQE for semantic regularization and MPE for semantic conditioning at inference even though the probabilistic interpretation of the system is no longer valid.\n\nHowever, there are limits to these parrallels.\n\nFirst, when prior knowledge is not expressed in CNF, or another language based on conjunctions of constraints, there is no straightforward way to design a PGM that captures the conditioned distribution. In this sense, our formalism differs from PGM since the distribution is defined through prior knowledge and not through interaction or independence relations like in PGMs.\n\nFinally, standard message passing algorithms mainly exploit tree decompositions to perform PQE and MPE and are therefore tractable for bounded tree-width formulas. This bounded tree-width condition is quite restrictive. Knowledge compilation methods, which go beyond this criteria and enable to perform PQE and MPE for a larger class of formulas, have therefore become the dominant approach in the neurosymbolic literature."
            }
        },
        {
            "summary": {
                "value": "The paper presents a unified formalism that encapsulates learning and inference in the presence of prior knowledge specified in propositional logic, also known as neurosymbolic AI. Using the presented formalism, they are able to delineate the approaches present\nin the literature, and define a new technique which they call *semantic conditioning at inference*. The proposed method applies to the\nneural network only during inference, and does not impose any changes during training."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is pretty well written, and provides a comprehensive overview of the different neurosymbolic approaches in the literature\n\n- The proposed approach, *semantic conditioning at inference*, greatly boosts the accuracy of the models on the tasks considered\nin the experimental section, while at the same time avoiding the computational costs of performing MAP inference on the full distribution."
            },
            "weaknesses": {
                "value": "- While the paper provides a nice, unifying overview of the field and the key techniques developed, I am not quite sure that such a view is especially novel\n\n- Furthermore, and perhaps more crucial, is that other work such as that by Niepert et al and Pogancic et al, as well as other works using combinatorial solvers do exactly what the authors set out to do: compute the MPE state at inference time without maintaining the distribution over the entire space of outputs, thereby also avoiding the often prohibitive task of counting, and which the authors do not compare to. This is my biggest concern, as it puts into question the novelty of the authors' technical contributions.\n\n- In the experimental section, I initially found it hard to parse the Figure 2 due to the absence of an explicit mention of what the acronyms used stand for."
            },
            "questions": {
                "value": "Please see the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper takes a neurosymbolic AI approach to multi-label classification where constraints of the output space are imposed using a propositional language and these constraints are accounted for when making predictions. It provides extensive background material on the formalisms that the method employs and evaluates on some multi-label classification benchmarks, demonstrating that accounting for these constraints outperforms not accounting for them."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "I appreciate that your paper employs techniques that are not particularly popular in mainstream ML. It's great to bring exposure to alternative ideas."
            },
            "weaknesses": {
                "value": "One challenge of employing techniques that are less familiar to readers is that the paper needs to set up a significant amount of background material. This occupies the majority of the paper. Some of the content is fairly elementary, such as explaining how to use a differentiable loss function to learn a probabilistic classifier.\n\nThe experiments are not particularly rigorous. I would have appreciated more ablations analyzing, for example, the impact of different algorithmic choices. For example, what fraction of predictions actually violate the constraints for the baseline model? There is also no benchmarking against other approaches to multi-label classification from prior works that go beyond independent prediction across labels."
            },
            "questions": {
                "value": "It's important to bridge the gap between the neurosymbolic AI community and related ideas that have previously appeared in other parts of the literature. A significant missing piece in the discussion of related work is anything regarding probabilistic graphical models, which are based on similar formalisms as section 2.\n\nBased on some quick background reading, here are some key multi-label classification papers that did inference in undirected graphical models: \n\nGhamrawi, Nadia and McCallum, Andrew. Collective multi-label classification.\nFinley, Thomas and Joachims, Thorsten. Training structural svms when exact inference is intractable.\nMeshi, Ofer, Sontag, David, Globerson, Amir, and Jaakkola, Tommi S. Learning efficiently with approximate inference via dual losses\nPetterson, James and Caetano, Tiberio S. Submodular multi-label \u00b4 learning.\n\nCan you please discuss more directly the relationship to PGMs? For the particular problem setups you use, is there a corresponding formulation as a PGM?\n\n=========\nI had a hard time understanding the complexity of the actual prediction problems. It would have been great if you had compressed some of the background and instead provide concrete details about the structure of the constraints in each problem that is considered in the experiments and how inference wrt these constraints is handled. I know some of these are provided in the appendix. Can you provide a high-level summary here, such that such a summary could be included in the main paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "### Paper Summary\nThe authors introduce syntax/formalism(s) that bridge a number of works interested in informed classification.\n\nThe authors find that simplifying Semantic Conditioning (SC) to Semantic Conditioning at Inference (SCI) captures 75% of the improved performance gain. This is particularly valuable because it suggests that we can flexibly use existing models without additional training!\n\n### Reviewer's Note\nI am not an expert in neurosymbolic systems. I have read some of these papers, but this not an area of my direct expertise, so it is possible I am missing something.\n\n### Review Summary\nThe authors argue that introducing the additional syntax does is a contribution in-and-of-itself, but I don't (yet) agree. The shared syntax does not seem to illustrate/highlight/show something new. I would expect that the authors of the mentioned previous works would agree that a shared syntax could be devised across problems, but instead chose to use the most apt approach for their respective formulations. I do think that it may be useful, if it leads to new developments/understandings, but the formalism itself only seems like 1/2 of a contribution. \n\n(I am quite open to discussion on this point or alternative perspectives.)\n\nThis leaves the primary contribution of this paper being: Removing part of an existing method only partially reduces performance. This is good to know, but I'm not sure its critical. Another way of looking at this is that I think the paper spends a lot of energy/time presenting this new syntax but I don't think we (the reader/the field) is getting a lot out of this, and the contribution of SC --> SCI is not particularly/directly related to the syntax."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* The paper does a great job of providing a lot of background and previous work. \n    * So much so that this paper feels mostly like an expositionary piece. If ICLR is interested in exposition-only work (somewhat like a focused survey) and the paper were slightly modified to pull this forward, then I would find the work more compelling. (I don't think that this is the case?)"
            },
            "weaknesses": {
                "value": "* The paper spends a lot of time basically defining an (existing) task and a syntax for this task; I didn't find this to be particularly compelling.\n* I think a paper that really leaned into this SC --> SCI contribution and included qualitative/quantitative analysis of what/how these models differ and what mistakes are made by the respective models that that could be a strong paper. As it stands, I am currently unconvinced.\n\n### Nitpicks\n* I found the introduction on what a deep learning system in section 3.1 a bit strange / out of place.\n* line 128: \"is build by\" --> \"is built by\", or else, reword the sentence\n* figure 1: labels part of the system as a NeSy but this is never mentioned anywhere else. (NeuroSymbolic Technique). I think there is room for the full definition in the figure, or else, include the abbreviation in the text.\n* figure 2 is a bit hard to read/small. try to resize this figure? use different settings or software to create it?\n* line 483: \"elucidate\" --> \"adjudicate\""
            },
            "questions": {
                "value": "What did the new syntax exactly offer/contribute to your understanding/method/results? Am I missing or misunderstanding something?\n\nWhile the background is already taking up a lot of the paper, finding space to provide key examples of the type of data you're working with could help get the reader to quickly track what you want to investigate. \n\nRelatedly, what errors does SCI make that SC doesn't? Can you provide examples of this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper makes two related contributions:\n\n* First, the paper proposes a general framework for encoding prior knowledge, within the context of probabilistic neurosymbolic approaches for multi-label classification.\n* Second, the paper proposes a variant on existing methods for  probabilistic neurosymbolic multi-label classification. Specifically, while in existing work, semantic conditioning is applied both at training time and at inference time, the authors propose to do this only at inference time. This has the advantage of making the method much more efficient, while experimental results show that the penalty for doing this only at inference time is (at worst) limited."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is carefully written and proposes a clean formalisation of probabilistic neurosymbolic multi-label classification. The formulation clearly illustrates the differences between two important previous techniques: semantic regularization and semantic conditioning.\n\nThe fact that performing semantic conditioning only at inference time does not deteriorate the performance much is a useful finding, which is worth highlighting to practitioners."
            },
            "weaknesses": {
                "value": "The main weakness, in my view, is that the paper only makes a small and incremental contribution. Claiming that \"semantic conditioning at inference\" is a new technique is somewhat of a stretch. It is essentially an ablation of the full semantic conditioning method. \n\nThe other claimed contribution is that the paper provides a unified formalism. To claim this as a contribution is questionable. The paper could easily have used propositional logic as the underlying framework and then have made the point that other propositional languages can be treated similarly. The current formulation also has the drawback of making the notation heavy and the explanations more opaque.  \n\nSome minor points:\n\nThe last paragraph before section 4.1 (on fuzzy logics) feels out of place, and similar for the first paragraph of Section 4.1. The whole built-up of the framework has assumed probabilistic reasoning, so it seems weird to suddenly justify this choice in the middle of the paper.\n\nTypo in the abstract: theoritical \n\nFigure 2: please add explanations for the different acronyms (SCI, IMC, etc) to the caption, even though their meaning can be guessed."
            },
            "questions": {
                "value": "For the experiments, the only baseline is to use an uninformed model. At least, I would have expected something like fuzzy regularization there as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}