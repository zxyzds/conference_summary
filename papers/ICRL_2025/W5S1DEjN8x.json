{
    "id": "W5S1DEjN8x",
    "title": "Practical $\\epsilon$-Exploring Thompson Sampling for Reinforcement Learning with Continuous Controls",
    "abstract": "Balancing exploration and exploitation is crucial in reinforcement learning (RL). While Thompson Sampling (TS) is a sound and effective exploration strategy, its application to RL with high-dimensional continuous controls remains challenging. We propose Practical $\\epsilon$-Exploring Thompson Sampling (PETS), a practical approach that addresses these challenges. Since the posterior over the parameters of the action-value function is intractable, we leverage Langevin Monte Carlo (LMC) for sampling. We propose an approach which maintains $n$ parallel Markov chains to mitigate the issues of nai\\\"{ve} application of LMC. The next step following the posterior sampling in TS involves finding the optimal action under the sampled model of the action-value function. We explore both gradient-based and gradient-free approaches to approximate the optimal action, with extensive experiments. Furthermore, to justify the use of gradient-based optimization to approximate the optimal action, we analyze the regret for TS in the RL setting with continuous controls and show that it achieves the best-known bound previously established for the discrete setting. Our empirical results demonstrate that PETS, as an exploration strategy, can be integrated with leading RL algorithms, enhancing their performance and stability on benchmark continuous control tasks.",
    "keywords": [
        "Reinforcement Learning",
        "Exploration",
        "Thompson Sampling"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "This work proposes an approach to address the challenges that has limitted the application of Thompson Sampling to the RL continuous controls.",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=W5S1DEjN8x",
    "pdf_link": "https://openreview.net/pdf?id=W5S1DEjN8x",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes **Practical \u03b5-Exploring Thompson Sampling (PETS)**, aiming to enhance exploration in reinforcement learning (RL) with high-dimensional continuous action spaces. The main contributions are:\n\n- **Application of Thompson Sampling (TS)** to continuous control tasks by addressing computational intractability.\n- **Use of Langevin Monte Carlo (LMC)** for approximate posterior sampling of the action-value function parameters.\n- **Maintenance of ensembles of parallel Markov chains** to reduce sample correlation and improve exploration diversity, effectively approximating the posterior distribution.\n- Introduction of both **gradient-based and gradient-free optimization methods** to approximate the optimal action under sampled models.\n- **Regret analysis** showing that PETS achieves regret bounds comparable to the best-known results in discrete settings.\n\nEmpirical results indicate that integrating PETS with existing RL algorithms like POMP, MBPO, and SAC leads to improved performance and stability on benchmark continuous control tasks.\n\n\n- **Lack of Novelty**: The method closely mirrors ensemble sampling techniques without proper acknowledgment or differentiation, undermining claims of innovation.\n\n- **Omission of Related Work**: Fails to discuss relevant literature on ensemble methods and randomized value functions, which is critical for situating the contribution within the field."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- **Addresses a Relevant Challenge**: Tackles the problem of exploration in high-dimensional continuous action spaces, which is important in RL research.\n\n- **Integration with Existing Algorithms**: Demonstrates that PETS can be incorporated into existing RL frameworks, potentially offering improvements without significant modifications.\n\n**Use of LMC and Parallel Markov Chains**: Leveraging LMC for posterior sampling is appropriate due to the high dimensionality of the action space. Maintaining multiple parallel Markov chains is an effective strategy to reduce sample correlation.\n\n- **Empirical Performance**: PETS shows improved performance over baseline algorithms (e.g., a 38% improvement over POMP on Walker2d), suggesting effectiveness in enhancing exploration.\n\n- **Exploration Effectiveness**: Visualizations and ablation studies support the claim that PETS improves exploration diversity.\n\n- **Theoretical Analysis**: Provides regret bounds that align with established results, supporting the method's theoretical validity."
            },
            "weaknesses": {
                "value": "- **Use of LMC and Parallel Markov Chains**:  This approach essentially amounts to maintaining an ensemble of models and uniformly sampling one at each decision point. This is conceptually similar to existing ensemble sampling methods used to approximate TS. [Line 7 of Algorithm 1]\n\n- **Computational Complexity**: The paper lacks a thorough analysis of the per-episode computational complexity, especially concerning the number of episodes and the number of parallel Markov chains maintained. The overhead introduced by managing multiple chains and performing multiple optimizations per action selection could impact scalability in large-scale applications.\n\n- **Connection to Ensemble Methods**: The methodology shows strong parallels to ensemble sampling approaches, such as those found in \"Ensemble Langevin DQN\" by Dwaracherla & Van Roy, where ensembles are used with Langevin dynamics for exploration. The paper does not acknowledge or discuss these similarities, missing an opportunity to position PETS within the broader context of ensemble-based RL methods.\n\n---\n\n- **Statistical Significance and Computational Cost**:\n\n  - **Statistical Analysis**: The experiments are conducted over a limited number of seeds (5 to 8), which may not provide sufficient statistical power to confirm the results' significance. \n\n  - **Computational Overhead**: The paper does not quantify the additional computational cost per episode introduced by maintaining multiple parallel Markov chains and performing optimization steps. This omission makes it difficult to assess the practical trade-offs between performance gains and computational efficiency.\n\n- **Baseline Comparisons**: The experimental comparisons are primarily against base algorithms without PETS. The paper does not compare PETS against Ensemble Langevin DQN [https://arxiv.org/abs/2002.07282]\n\n---\n\n**Related Work Comparison**\n\nThe paper contributes to the application of TS in continuous action spaces but fails to adequately discuss relevant existing literature, particularly on ensemble methods:\n\n- **Ensemble Methods and Approximate TS, Randomized Value Functions**:\n\n  - **\"Ensemble Sampling\" by Lu & Van Roy**: This work develops ensemble sampling as an approximation to TS, maintaining an ensemble of models and sampling from them to drive exploration. PETS's use of multiple parallel Markov chains closely resembles this approach but lacks acknowledgment of this connection.\n\n  - **\"Ensemble Langevin DQN\" by Dwaracherla & Van Roy**: This paper demonstrates that ensemble methods combined with Langevin dynamics can achieve deep exploration without additional complexity for epistemic uncertainty representation. PETS uses a similar strategy but does not reference this work.\n\n  - **\"Deep Exploration via Bootstrapped DQN\" by Osband et al.**: Introduces an ensemble-based method (Bootstrapped DQN) for deep exploration in RL, which PETS could be compared against.\n\n  - **\"Deep Exploration via Randomized Value Functions\" by Osband et al.**: Discusses using randomized value functions to encourage exploration, which is conceptually related to PETS's approach.\n\n  - **\"Randomized Value Functions via Multiplicative Normalizing Flows\" by Touati et al.**: Proposes achieving randomized value functions in high-dimensional domains using variational Bayesian methods, offering an alternative to PETS's LMC sampling.\n\n  - **\"Q-Star Meets Scalable Posterior Sampling\" by Li et al.**: Presents an efficient method for approximate posterior sampling in RL, achieving sublinear regret and superior performance in large-scale deep RL benchmarks, which could provide valuable insights or comparisons for PETS.\n\nThe omission of these works limits the paper's positioning within the existing body of research and undermines its claim of novelty.\n\n---\n\n**5. Clarity and Presentation**\n\nThe paper is structured logically, but several aspects could be improved:\n\n- **Methodological Clarity**:\n\n  - **Details on Computational Complexity**: A detailed analysis of the per-episode computational complexity is missing. Understanding how the number of episodes and the number of parallel Markov chains impact computational resources is crucial for practical applications.\n\n  - **Algorithmic Similarities**: The paper does not clarify how its approach differs from or improves upon existing ensemble methods, which could confuse readers familiar with ensemble sampling.\n\n- **Algorithm Descriptions**:\n\n  - **Algorithm 1**: Line 7's operation of sampling from multiple models is similar to ensemble sampling, but this is not acknowledged, leading to potential misunderstandings about the novelty of the approach.\n\n- **Presentation of Theoretical Results**:\n\n  - **Regret Analysis**: The theoretical analysis may be dense and could benefit from additional explanations or simplifications to enhance accessibility.\n\n---\n\n**6. Overall Impact**\n\nWhile PETS aims to advance exploration strategies in RL with continuous controls, its impact is limited by several factors:\n\n- **Novelty**: The core methodology closely resembles ensemble sampling approaches already present in the literature. Without acknowledging or differentiating from these methods, the paper's originality is diminished.\n\n- **Practical Contribution**: The lack of computational complexity analysis raises concerns about the method's scalability and practical applicability in large-scale RL tasks.\n\n- **Theoretical and Empirical Validation**: Although the paper provides theoretical regret bounds and empirical improvements, these are less compelling without thorough comparisons to existing ensemble-based exploration methods.\n\n---\n\n**Weaknesses:**\n\n- **Lack of Novelty**: The method closely mirrors ensemble sampling techniques without proper acknowledgment or differentiation, undermining claims of innovation.\n\n- **Omission of Related Work**: Fails to discuss relevant literature on ensemble methods and randomized value functions, which is critical for situating the contribution within the field.\n\n- **Insufficient Computational Analysis**: Does not provide an analysis of the computational complexity per episode, leaving practical efficiency and scalability unaddressed.\n\n- **Limited Experimental Rigor**: Experiments lack statistical significance testing and do not compare PETS against other state-of-the-art exploration methods, weakening the empirical validation.\n\n**Suggestions for Improvement:**\n\n1. **Acknowledge and Discuss Related Work**: Incorporate discussions of ensemble sampling methods, such as Bootstrapped DQN, Ensemble Sampling, and Ensemble Langevin DQN. Highlight how PETS relates to, differs from, and potentially improves upon these approaches.\n\n2. **Clarify Novel Contributions**: Clearly articulate what is novel about PETS in comparison to existing ensemble methods. If PETS offers specific advantages or innovations, these should be explicitly stated and justified.\n\n3. **Analyze Computational Complexity**: Provide a detailed analysis of the per-episode computational cost, including how it scales with the number of episodes and parallel Markov chains. Discuss any trade-offs between computational overhead and performance gains.\n\n4. **Enhance Experimental Evaluation**:\n\n   - **Statistical Significance**: Increase the number of random seeds and perform appropriate statistical tests to strengthen the reliability of the results.\n   \n   - **Comparative Baselines**: Include comparisons with other ensemble-based exploration methods and state-of-the-art algorithms to contextualize the performance improvements.\n\n5. **Improve Clarity and Presentation**:\n\n   - **Methodological Details**: Expand on the implementation aspects, particularly regarding the optimization methods and how parallel chains are maintained.\n   \n   - **Algorithm Descriptions**: Update algorithm pseudocode to reflect similarities and differences with existing methods, and clearly explain any unique steps.\n\n6. **Theoretical Insights**: Simplify the theoretical analysis where possible, and provide intuitive explanations to make the content accessible to a broader audience.\n\nBy addressing these issues, the paper could significantly strengthen its contribution to the field, offering clearer insights into how PETS advances exploration in reinforcement learning and distinguishing it from existing ensemble methods."
            },
            "questions": {
                "value": "1. **Relationship to Ensemble Methods:**\n\n   - **Question:** How does your method differ from existing ensemble-based approaches, such as Bootstrapped DQN, Ensemble Sampling, and Ensemble Langevin DQN?\n   - **Suggestion:** Please clarify the distinctions between PETS and these ensemble methods. Specifically, explain how maintaining multiple parallel Markov chains in PETS offers advantages over traditional ensembles used in approximate Thompson Sampling.\n\n2. **Acknowledgment of Related Work:**\n\n   - **Question:** Why are key related works on ensemble sampling and randomized value functions omitted from the paper?\n   - **Suggestion:** Including discussions of relevant literature, such as:\n     - \"Deep Exploration via Bootstrapped DQN\" (Osband et al.)\n     - \"Ensemble Sampling\" (Lu & Van Roy)\n     - \"Ensemble Langevin DQN\" (Dwaracherla & Van Roy)\n     - \"Deep Exploration via Randomized Value Functions\" (Osband et al.)\n     - \"Q-Star Meets Scalable Posterior Sampling\" (Li et al.)\n     This will position your work within the broader context and highlight its contributions compared to existing methods.\n\n3. **Novel Contributions:**\n\n   - **Question:** Can you explicitly state the novel contributions of PETS compared to existing ensemble sampling and randomized value function methods (including the LMC based randomized value)?\n   - **Suggestion:** Highlight any unique aspects of PETS, such as theoretical improvements, practical benefits, or empirical performance gains that set it apart from similar approaches.\n\n4. **Per-Episode Computational Complexity:**\n\n   - **Question:** How does the computational complexity of PETS scale with the number of episodes and the number of parallel Markov chains (`n_samples`)?\n   - **Suggestion:** Provide a detailed analysis of the per-episode computational complexity. Discuss the trade-offs between the computational overhead of maintaining multiple chains versus the performance improvements achieved.\n\n5. **Algorithmic Details and Clarity:**\n\n   - **Question:** In Algorithm 1, line 7 involves uniformly selecting a model from multiple posterior samples, which resembles ensemble sampling approximations to Thompson Sampling. How does this differ from existing ensemble methods?\n   - **Suggestion:** Elaborate on this step, clarifying how your approach is distinct. If PETS introduces specific innovations in how the ensembles are generated or used, make this explicit in the methodology section.\n\n6. **Use of Langevin Dynamics in Ensembles:**\n\n   - **Question:** Since ensemble methods have been used with Langevin-type randomized value functions (e.g., \"Ensemble Langevin DQN\"), how does PETS improve upon or differ from these techniques?\n   - **Suggestion:** Compare PETS to methods like \"Ensemble Langevin DQN\" by Dwaracherla & Van Roy. Discuss whether PETS offers theoretical or practical advantages, such as better exploration efficiency or ease of implementation.\n\n7. **Experimental Comparisons:**\n\n   - **Question:** Have you compared PETS to other state-of-the-art exploration methods, particularly ensemble-based approaches?\n   - **Suggestion:** Include experimental results comparing PETS with algorithms like Bootstrapped DQN and Ensemble Sampling. This would strengthen the empirical evaluation and demonstrate PETS's effectiveness relative to existing methods.\n\n8. **Statistical Significance of Results:**\n\n   - **Question:** Are the performance improvements observed in your experiments statistically significant?\n   - **Suggestion:** Increase the number of random seeds used in experiments and perform statistical significance testing. This will bolster the credibility of your results and address concerns about variability due to random initialization.\n\n9. **Clarity on Computational Overhead:**\n\n   - **Question:** What is the practical impact of maintaining multiple parallel Markov chains on computation time and resources?\n   - **Suggestion:** Provide empirical measurements of the computational overhead introduced by PETS. Discuss how this overhead scales with `n_samples` and the implications for real-world applications.\n\n10. **Theoretical Analysis Accessibility:**\n\n    - **Question:** Can you provide more intuitive explanations of the theoretical regret analysis to make it accessible to a wider audience?\n    - **Suggestion:** Include examples or diagrams that illustrate key theoretical concepts. Simplify complex mathematical derivations where possible, or provide additional explanatory text in appendices.\n\n11. **Future Work and Limitations:**\n\n    - **Suggestion:** Acknowledge any limitations of PETS, such as scalability issues or environments where it may perform less effectively. Outline potential future research directions to address these challenges.\n\n12. **Implementation Details:**\n\n    - **Question:** Could you provide more detailed information on implementing PETS, especially regarding the maintenance of parallel chains and integration with existing algorithms?\n    - **Suggestion:** Including pseudo-code snippets, parameter settings, or practical tips would help practitioners replicate and build upon your work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a TS-based exploration technique for RL with continuous action spaces. To manage the continuous action space, they use Langevin Monte Carlo (LMC) to sample from the posterior of the action-value function and approximate the optimal action through both gradient-based and gradient-free methods. They provide a regret guarantee for linear MDPs and demonstrate empirical performance improvements in continuous control tasks by integrating their PETS approach with established RL algorithms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Integrating RL theory with deep RL is a fascinating and crucial direction for advancing the RL community. The application of theoretically proven techniques to real-world problems, with demonstrated success, is especially compelling. This made the paper both exciting and highly engaging to read. I believe this type of work will make a significant contribution to the community."
            },
            "weaknesses": {
                "value": "- My main concern is the technical novelty. The proof of Theorem 1 seems very similar to the analysis in Ishfaq et al., 2024. What is the main technical challenge or novelty in this work compared to Ishfaq et al., 2024, particularly in proving Theorem 1? A detailed discussion of these technical challenges in the main paper would be very helpful.\n\n- Regarding the experiments, I have several comments:\n    1. Generally, a strong exploration strategy is particularly beneficial in sparse reward settings. However, it appears that all the experiments in this paper were conducted in dense reward environments. Do you believe your algorithm would still demonstrate improved performance in sparse reward settings?\n\n    2. The paper lacks an analysis of computational costs. The proposed method appears to be computationally intensive due to the large number of posterior samples and gradient ascent steps, especially in the Cheetah environment where $n=50$, likely resulting in considerable computational overhead. Including a comparison of computational costs with baseline algorithms would be helpful for understanding the contribution of your work.\n\n\n    3. I wonder if the proposed method might be sensitive to the exploration probability $\\epsilon$, given that it varies considerably across different environments. Is your algorithm robust to changes in $\\epsilon$?"
            },
            "questions": {
                "value": "- In Theorem C.10, how large is the condition number $\\kappa$ typically, or in the worst case? \n\n\n- Since the proposed method uses parallel models, it relates closely to ensemble-based algorithms, such as Bootstrapped DQN (Osband et al., 2016, [1]). Additionally, recent work on linear ensembles for bandits (Janz et al., 2024 [2]), published in NeurIPS 2024, is also relevant. Comparing your approach with these ensemble-based algorithms would enhance the paper.\n\n    [1] Osband et al., Deep Exploration via Bootstrapped DQN, NeurIPS 2016.\n    [2] Janz et al., Ensemble sampling for linear bandits: small ensembles suffice, NeurIPS 2024.\n\nI will consider raising the score once my concerns and questions have been addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper discusses how to apply Langevin Monte Carlo (LMC) based Thompson sampling algorithm for continuous action space RL tasks. Relying on the result of Ishfaq et al 2024, the authors derive a regret bound for continuous action setting and also provide some experiments. The paper further explores both gradient based and gradient free methods for finding approximate optimal action."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The work is nice in the sense that it extends LMC-LSVI from Ishfaq et al 2024a to continuous action setting which is an important problem setting. It also shows the efficacy of the method through some thorough experiment.\n\n2. The proposal of parallel posterior sampling is a promising way to mitigate the issue of correlated sampling in vanilla LMC."
            },
            "weaknesses": {
                "value": "1. My main complaint for this work is that it doesn\u2019t situate the contribution of the paper relative to other highly related papers such as Ishfaq et al 2024a and Ishfaq et al 2024b. In line 70-74, it introduces the idea of using LMC for TS as if it\u2019s the first work to do so. Proposition C.1, Definition C.2, Lemma C.5,  Lemma C.6, Lemma C.7, Lemma C.8, Lemma C.9 all are essentially verbatim to different lemmas used in the proof of Ishfaq et al 2024a. With this respect the novelty of the algorithmic contribution and theoretical analysis are very limited in this paper.\n\n2. Practical $\\epsilon$ Exploring Thompson Sampling (PETS) directly borrows ideas/methods from two existing works $\\epsilon$-TS [Jin et al 2023] and LMC-LSVI of Ishfaq et al 2024a. As such the novelty in terms of algorithmic design seems limited. Despite borrowing significant chunk of Ishfaq 2024a for algorithm design, the paper fails to properly acknowledge it in their method section, especially in Section 3.1. \n\n3. Also, I think all the assumptions made in the theorem statements such as L-smoothness and Polyak Lojasiewicz inequality in Theorem 3.1 should be clearly defined in the main paper. \n\n4. Missing related work: I think in Section 5, around Line 486, the authors should discuss other provably efficient randomized exploration methods such as Ishfaq et al 2021, Russo 2019, Xiong et al 2022 etc. \n\n5. Missing code: Given the empirical aspect of the work, I think the authors should share the code even during the reviewing process. Could you please share the code base anonymously?\n\n6. Mujoco experiments: for mujoco experiments in Fig3, it was run for 150k or 100k time steps whereas the standard is to run for 1million step. Could you explain why this is the case? With this low number of steps, the result interpretation can be unfair. Also, can you consider stronger baseline such as DSAC-T (Duan et al 2023)?\u2028\n\n\nIshfaq, Haque, et al. \"Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo.\"\u00a0The Twelfth International Conference on Learning Representations. 2024a\n\nIshfaq, Haque, et al. \"More Efficient Randomized Exploration for Reinforcement Learning via Approximate Sampling\"\u00a0The 1st Reinforcement Learning Conference. 2024b\n\nIshfaq, Haque, et al. \"Randomized exploration in reinforcement learning with general value function approximation.\"\u00a0International Conference on Machine Learning. PMLR, 2021.\n\nDaniel Russo. Worst-case regret bounds for exploration via randomized value functions. 2019\n\nXiong et al. Near-optimal randomized exploration for tabular Markov decision processes 2022\n\nDuan et al. DSAC-T: Distributional Soft Actor-Critic with Three Refinements, 2023"
            },
            "questions": {
                "value": "1. In line 303, it says \u201cFor optimizations, we use the Adam optimizer in all cases.\u201d But in Line 23 of Algorithm 1, the update rule is standard SGLD update. Could you please clarify how exactly Adam is used here? Also, do you use Adam LMCDQN based adaptive bias term as explained in Ishfaq et al 2024a?\n\n2. Could you comment on how the idea of maintaining n parallel Markov chains is different from the optimistic sampling scheme mentioned in Remark 4.3 and Appendix D.1 in Ishfaq et al. 2024a?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "PETS introduces a new exploration strategy for RL in continuous control tasks, leveraging TS while addressing its challenges in high-dimensional environments. In traditional bandit problems, TS has proven effective, but scaling it to RL with continuous actions is complex because the posterior distribution is intractable in most cases. To address the intractability of posterior sampling in TS, PETS uses Langevin Monte Carlo, which is a method for generating samples from complex distributions. However, naive application of LMC often results in highly correlated samples, which limits the diversity of explored actions. PETS mitigates this by running parallel Markov chains to obtain a wider range of posterior samples, improving exploration. Once the posterior sample is obtained, the next challenge in continuous action spaces is to select the optimal action under the sampled model of the action-value function. PETS explores both gradient-based and gradient-free approaches to optimize action selection. The theoretical analysis in PETS shows that gradient-based optimization achieves regret bounds comparable to the best-known results for discrete settings, making it a viable choice for continuous control. PETS was tested with popular RL algorithms like Policy Optimization with Model Planning (POMP), Model-Based Policy Optimization (MBPO), and Soft Actor-Critic (SAC), demonstrating improved exploration and stability without requiring major modifications to the underlying algorithms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "PETS enhances exploration in RL by maintaining multiple posterior samples and using them to explore a wider range of potential actions. This parallel ensemble approach helps in achieving better exploration of the state-action space. PETS has been shown to outperform state-of-the-art RL algorithms in complex continuous control tasks, such as Walker2d, Ant, and Humanoid, achieving higher returns and more stable learning curves. Its exploration mechanism is particularly beneficial for environments with high-dimensional and continuous action spaces. PETS leverages Thompson Sampling principles to minimize regret. Theoretical analysis shows that it matches the best-known regret bounds in certain settings, such as in linear Markov Decision Processes (MDPs) with continuous actions, ensuring its long-term learning efficiency."
            },
            "weaknesses": {
                "value": "PETS maintains multiple posterior samples and performs parallel exploration, which can increase the computational burden, especially when scaling to high-dimensional environments. The need to maintain and update multiple models or samples can lead to higher memory and CPU/GPU requirements, particularly when dealing with large state and action spaces. While PETS uses Thompson Sampling to balance exploration and exploitation, it might still struggle in environments with sparse rewards or deceptive reward structures. The method may over-explore in certain cases, leading to inefficient learning when exploitation would yield better results. Similarly, the $\\epsilon$-Exploring Thompson Sampling ($\\epsilon$-TS) might lead to suboptimal performance when $\\epsilon$ is not well-calibrated."
            },
            "questions": {
                "value": "PETS has demonstrated success in linear Markov Decision Processes (MDPs). Do you have an idea of its performance in highly non-linear or non-stationary environments ?\nThe algorithm\u2019s theoretical guarantees for regret minimization are often more straightforward in linear settings, what is the bottleneck in non-linear settings ?\nAre there specific problem types or domains where PETS is particularly effective?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}