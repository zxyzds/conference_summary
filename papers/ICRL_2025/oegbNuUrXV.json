{
    "id": "oegbNuUrXV",
    "title": "Generalizable Dynamic Radiance Field in Egocentric View",
    "abstract": "We present a novel framework for generalizable dynamic radiance field in first-person view. Our approach can predict the neural representation of physical world conditioned on a sequence of egocentric observations without test-time training. To this end, we propose a data-driven algorithm that updates an explicit 3D representation by aggregating features from given videos and the 3D representation itself in an implicit 4D-aware mechanism. Specifically, we use a contracted triplane as the 3D representation, propose a 4D-aware transformer module to aggregate features and introduce a temporal-based 3D constraint to achieve better multiview consistency. We train the proposed model with large-scale diverse videos in self-supervised manner. We demonstrate the effectiveness of our approach by showing top results in novel view synthesis on dynamic scene datasets. Besides, we also validate the cross-scene ability of our method on various datasets. Furthermore, we observe that our approach emerges capabilities for geometry and semantic learning. We hope our approach can provide preliminary understanding of the physical world in first-person view and help ease future research in computer vision, computer graphics and robotics.",
    "keywords": [
        "generalized dynamic view synthesis",
        "NeRF",
        "computer vision"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=oegbNuUrXV",
    "pdf_link": "https://openreview.net/pdf?id=oegbNuUrXV",
    "comments": [
        {
            "summary": {
                "value": "The paper describes an approach for novel view rendering given dynamic egocentric video inputs. The approach is feedforward without optimization at inference time, and is trained to generalize to unseen scenes. The approach uses a triplane as scene representation, which is subsequently updated with a transformer to incorporate the input frames as well as camera parameters. Qualitative and quantitative evaluation provides evidence of the performance and generalizability of the approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Significance: As the authors rightly point out, existing solutions for dynamic NVS generally can not generalize to novel scenes, limiting their practical use scenarios. Therefore, a solution for generalizable, dynamic NVS given first-person video frames can be significant. \n\nOriginality: Though none of the building blocks are new, the technical solution of a triplane representation for dynamic NVS and the refinement of the representation with frame features through a \"4D-aware transformer\" is sound and novel. \n\nClarity: Though missing some details (explained in the next section), the paper is overall well-written, conveys the main ideas well, and is easy to follow.  \n\nQuality: The results are generally consistent with the authors' claim. It is particularly helpful that authors make clear distinctions between dynamic and static scene components, in-domain and out-of-domain samples in the experiments, and also conduct ample ablation studies."
            },
            "weaknesses": {
                "value": "I think a major weakness is in the evaluation of the approach, particularly regarding generalization. \n\nResults in Sec.4.1.1 are not quite helpful since the approach is behind a few competing methods and the testing scenes are already seen during training. While no per-scene optimization is needed at inference time, it's possible the network can memorize the scenes to some extent via training. \n\nResults in Sec.4.1.2 provide some insights regarding generalization, but are very limited. Table 2 only shows comparisons with single-view methods on a single dataset. To demonstrate generalization capability, the authors should consider: \n* Analyzing results across more diverse testing datasets and scenes;\n* Comparison with other multi-view approaches (even if they're static or optimization-based);\n\nAlso regarding experiments, it would be valuable to show:\n* more distinct views from input views to better understand the limitations\n* results and comparisons regarding first-person vs other types of samples. \n\nThe paper does not have any analysis nor comparisons regarding latency and speed. This would make clear the efficiency advantage over optimization-based approaches. \n\nSome closely related literature on Gaussian Splatting (GS) is not mentioned, namely\n* 4D GS: \n    - 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering\n    - Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis\n    - Motion-aware 3D Gaussian Splatting for Efficient Dynamic Scene Reconstruction\n    - 3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis\n    - Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos\n* Feedforward GS: \n    - MVSGaussian, GS-LRM, MVSplat, etc. \n\nLast but not least, the paper lacks clarity on some key concepts that should be better defined, e.g. \n* How is the initial triplane learned/initialized?\n* The concept of feature \"similarity\" is used in a few places but is not properly defined. \n* \"epipolar feature\" appears to be simply projected pixel features. If so, the use of term \"epipolar\" is uncommon and confusing.  \n* \"time bias\" on L200 is not defined."
            },
            "questions": {
                "value": "Please see the suggestions made in the section above, regarding results, analysis, and clarity. Most critically, I think a more thorough experimental analysis of the generalization capability can significantly improve the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a generalizable dynamic radiance filed in an ego-centric view. Differing from the common NeRF literature, which requires test-time optimization and object centric-view, the method predicts a neural representation from ego-centric image sequence without test-time training. For this purpose, the paper newly proposes a 4D-aware transformer consisting of a View-Attention Module, Axis-Attention Module, and Plane-Attention Module. Using the method, the paper trains a model on a number of training datasets (NVIDIA Dynamic Scenes, EPIC Fields, and nuScenes) and tests on nuScene (test) and RealEstate 10K. However, the performance does not outperform previous related works, and the method has a number of questions in terms of generalization and dynamic representation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "S1. The paper proposes a 4D-aware transformer consisting of a View-Attention Module, an Axis-Attention Module, and a Plane-Attention Module."
            },
            "weaknesses": {
                "value": "W1. The design philosophy of the proposed framework for generalization.\n\n* The paper conducts generalization experiments by training the triplane on several training datasets (NVIDIA Dynamic Scenes, EPIC Fields, and nuScenes) and testing on nuScene (test) and RealEstate 10K.\nHowever, it is questionable whether the proposed framework is suitable for dynamic radiance field generalization.\nOriginally, the learnable triplane aimed to learn three feature planes to embed the target scene context and its temporal change.\nHowever, if the target training set gets diverse, the target of the learnable triplane is unclear. \nIn the current training scenario, what is the learning goal of the learnable triplane, and what do they learn?\n\n* Also, it would be great to discuss whether the current framework is suitable for dynamic radiance field generalizatoin. \nThe current frameworks seem unsuitable for radiance field generalization to handle totally unseen and out-of-distribution domain data, such as mountains, caves, or endoscopes. \n\nW2. Performance improvement is not significant.\n\n* In novel view generation in both the seen and unseen domains, the proposed method doesn't outperform the pseudo-generalized methods MonoNeRF Tian et al. (2023) and PGDVS Zhao et al. (2024).\n\n* Also, the paper uses nuScene (test) and RealEstate10K datasets as unseen datasets. However, in terms of domain gap, the training set already includes nuScene (training) and common indoor and outdoor scenes. So, the network is already aware of similar structures, such as the common load scenario and indoor building scenario. The domain gap between the training and testing (unseen) dataset is quite small. If the method is truly generalizable, it should be tested with totally out-of-distribution data.\n\nW3. Novelty in terms of generalization and dynamics representation\n\n* The paper insists that it proposed a generalizable dynamic radiance field estimation framework. For this purpose, the paper proposes a new 4D-aware transformer consisting of a View-Attention Module, an Axis-Attention Module, and a Plane-Attention Module. However, their strength in terms of generalization and dynamics representation is unclear.\nIt would be great to prove the proposed 4D-aware transformer's effectiveness in both generalization and dynamics representation by comparing it to previous methods. For instance, compared to previous dynamic content embedding methods (Li et al. (2023); Tian et al. (2023)), is the View-Attention Module superior to representing dynamic content?"
            },
            "questions": {
                "value": "Please answer the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a framework for creating generalizable dynamic radiance fields from first-person (egocentric) views, enabling view synthesis of dynamic scenes without test-time training. Using a 4D-aware transformer with dedicated attention modules for understanding temporal, spatial information with regard to the camera parameters, the model aggregates features from monocular videos to form a 3D triplane representation, achieving robust generalization across diverse, unseen scenes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper introduces a 4D transformer, that can be generally used in view synthesis.\n- This paper conducted extensive experiments on various datasets, to demonstrate the effectiveness and generalizability of the proposed method."
            },
            "weaknesses": {
                "value": "- The methodology part is not straightforward to understand.\n- The methodology is not aligned to the title or motivation of this paper, focusing on 'egocentric views'\n- Experimental results are far from 'comparable' to the previous approaches. I understand that previous algorithms optimize view synthesis scene-wise, so the state-of-the-art performance is not expected. However, showing mid-low performance on PSNR and SSIM metrics in table 3, while calling it 'on-par' is not agreeable.\n- This paper includes shallow ablation study. Only module-wise plug-in-plug-out ablation is not enough to fully demonstrate the motivation and effectiveness of the suggested modules. This is where the authors can truly argue that their 4D transformer is actually valid in understanding 4D scene information, even though it lacks performance compared to previous algorithms with scene-specific training.\n- Overall qualitative results are not curated well to present the effectiveness of the proposed method."
            },
            "questions": {
                "value": "- Can you explain what exactly the suggested attention modules in 4D transformer are trying to 'learn'? For example, what is 'axis-attention', implicitly? What does it mean, and how does it help to understand 4D scene?\n- Can you explain how the suggested modules are specifically designed for 'egocentric' views?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a generalizable dynamic radiance field by using attention across view, axis and plane."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Propose a attention-based attention to improve triplane for generalizable task."
            },
            "weaknesses": {
                "value": "The generalizability is tested on only one dataset. While the main focus is on generalizability, the primary results table (Table 1) does not reflect this aspect."
            },
            "questions": {
                "value": "Could the algorithm be evaluated on additional datasets to assess its generalizability?\n\nAdditionally, in Table 1, it states, \"Our model is trained on scenes from NVIDIA, EPIC, Plenoptic, and nuScenes (train set)\" (Line 370). Why does the performance remain limited compared to other methods?\n\nMissing reference: \nDynPoint: Dynamic Neural Point For View Synthesis\nNeuPhysics: Editable Neural Geometry and Physics from Monocular Videos"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the generalizable model for dynamic scene rendering. It uses triplane as scene representation, and proposes view-attention, axis-attention and plane-attention modules to optimize the triplane features. Experimental results on RealEstate10K and Nvidia Dynamic Scene datasets show the proposed method achieves comparable results without any finetuning on dynamic scenes compared to other methods that all require per-scene optimization."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "It could achieve good dynamic scene rendering performance without flow and depth supervision.\nIt achieves better generalization ability compared to MonoNeRF and PGDVS."
            },
            "weaknesses": {
                "value": "After carefully reading the method section, I'm still confusing how the proposed method processes dynamic scenes over time. Line 139 says that the model could render novel views at target time $t_1$, but does not mention how $t_1$ is determined and introduced into the model. Figure 1  does not present how the model could generate novel views of dynamic scenes at different timestamps either.\n\nBesides, since this paper uses 3D triplane representation without a time axis and uses all the video frames to train the 3D representation, the time-variant motion features may be mixed into one triplane representation, which is quite confusing to use the model to render novel views at different timestamps.\n\nThe paper claimed that it could learn semantic information with the proposed pipeline, but it only tests its image encoder performance on image classification tasks, which is somehow weak. Semantic learning of dynamic scenes should be demonstrated by challenging semantic tasks like segmentation, tracking, completion, and generalization in Semantic Flow [1] paper.\n\nThe paper uses many non-egocentric view datasets for evaluation. It is weird to conduct experiments on these dataset as the goal of the paper is to build generalizable dynamic radiance fields in egocentric view. I want to know why.\n\nReference:\n[1] Tian, Fengrui, Yueqi Duan, Angtian Wang, Jianfei Guo, and Shaoyi Du, \"Semantic Flow: Learning Semantic Fields of Dynamic Scenes from Monocular Videos\", ICLR 2024. https://arxiv.org/abs/2404.05163"
            },
            "questions": {
                "value": "Some citation formats in this paper seem wrong. Please double-check citation formats."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}