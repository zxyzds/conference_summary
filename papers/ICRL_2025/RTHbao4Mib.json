{
    "id": "RTHbao4Mib",
    "title": "Large Language Models Often Say One Thing and Do Another",
    "abstract": "As large language models (LLMs) increasingly become central to various applications and interact with diverse user populations, ensuring their reliable and consistent performance is becoming more important. This paper explores a critical issue in assessing the reliability of LLMs: the consistency between their words and deeds. To quantitatively explore this consistency, we developed a novel evaluation benchmark, the Words and Deeds Consistency Test (WDCT), which establishes a strict correspondence between word-based and deed-based questions across different domains, including opinion versus action, non-ethical value versus action, ethical value versus action, and theory versus application. The evaluation results reveal a widespread inconsistency between words and deeds across LLMs and domains. Subsequently, we conducted experiments with either word alignment or deed alignment to observe their impact on the other aspect. The experiment results indicate that alignment only on words or deeds poorly and unpredictably influences the other aspect. This supports our hypothesis that the underlying knowledge guiding LLMs' choices of words or deeds is not contained within a unified space.",
    "keywords": [
        "corpus creation",
        "benchmarking",
        "language resources",
        "NLP datasets",
        "evaluation",
        "metrics",
        "consistency"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=RTHbao4Mib",
    "pdf_link": "https://openreview.net/pdf?id=RTHbao4Mib",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses the issue of inconsistencies in the texts generated by LLMs, formalized as the difference between the LLMs' \"words\" (i.e., replies to direct questions about the LLMs' views) and \"deeds\" (i.e., replies to questions about the preferred action in situations that manifest the LLMs' views). The authors create a new benchmark of matched word and deed questions and use it to evaluate a series of LLMs. They find evidence for rampant inconsistencies between the LLMs' words and deeds."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The problem that the paper addresses (i.e., inconsistencies between the LLMs' words and deeds) has been studied before, but a better understanding of the underlying issues is still lacking. In this context, the paper proposes a very valuable resource, both for future research on inconsistencies in LLMs and for evaluating the values encoded in LLMs. The analyses that the authors conduct using their benchmark are methodologically sound; the authors did a great job in testing the robustness of their findings."
            },
            "weaknesses": {
                "value": "The biggest weakness of the paper is that it completely ignores prior work on inconsistencies between the words and deeds of LLMs, claiming that it is \"the first [study] to systematically evaluate the consistency of responses from prominent LLMs based on the words and deeds\" (p. 10); while prior studies do not use the terms \"words\" and \"deeds\", they are still very similar in terms of motivation and overall findings, so I think it is critical that the authors add a comprehensive discussion and comparison before the paper can be published. There are two lines of work that I deem particularly relevant:\n\n1) There has been a lot of work on the inconsistencies in the political values that LLMs express when asked in a multiple-choice setup (\"words\") versus more real-world use cases (\"deeds\"). For example, [R\u00f6ttger et al. (2024)](https://aclanthology.org/2024.acl-long.816/) compare the behavior of LLMs when directly asked for their political values, and the values they implicitly support when asked to write texts about political issues, finding inconsistencies very similar to the ones described in this paper. There have been various other papers looking at the same question (e.g., [Moore et al., 2024](https://arxiv.org/abs/2407.02996)), none of which the authors cite.\n\n2) The difference between the LLMs' \"words\" and \"deeds\" is related to the distinction between the implicit and explicit behavior of LLMs (e.g., [Bai et al., 2024](https://arxiv.org/abs/2402.04105), [Hofmann et al., 2024](https://www.nature.com/articles/s41586-024-07856-5)) -- another line of work that the authors should acknowledge. For example, Hofmann et al. (2024) show that the divergence between the explicit and the implicit behavior of LLMs is bigger for aligned than for unaligned models, which is exactly in line with the findings that the authors present (p. 7)."
            },
            "questions": {
                "value": "Minor points:\n- Table 1, caption: \"a aligned -> \"an aligned\"\n- Table 3: The abbreviation \"IFT\" is never introduced. Did you mean \"SFT\"?\n- 421-422: \"to generalize to generalize\" -> \"to generalize\"\n- Section 4.2, 2: I think Figure 5 does not clearly support your claim. For example, the bar for D2 is as high as the one for S1 with both models. You either need to provide a more detailed analysis or weaken your claim.\n- Table 5: What is shown in the first column?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses a critical issue within LLMs \u2014 the inconsistency between their expressed words (statements or opinions) and their actions (behavioral outputs). To quantify this inconsistency across various domains, the authors introduce a novel evaluation benchmark, the Words and Deeds Consistency Test (WDCT). Through the WDCT, the study reveals substantial discrepancies between what LLMs say and do, emphasizing the challenges in achieving consistent alignment, especially when alignment is performed only on words or deeds independently. The paper makes a valuable contribution by suggesting a new direction for alignment research in LLMs."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Relevance and Importance:\nThe problem tackled is highly relevant. As LLMs are increasingly embedded in real-world applications where consistency between communication and behavior is essential, identifying and addressing this issue has meaningful implications for enhancing user trust and model reliability, particularly as LLMs are now frequently deployed as autonomous agents.\n\n- Novel Evaluation Benchmark (WDCT):\nThe WDCT is a significant contribution, offering a well-structured approach to measure alignment between words and deeds across multiple domains (e.g., opinion, ethical values, theory). This benchmark provides a robust foundation for future alignment studies in LLMs.\n\n- Systematic Evaluation Experiment:\nThe authors pose three clear research questions and provide a detailed experimental framework to evaluate whether separate alignment or common knowledge generalization methods could improve consistency between words and deeds."
            },
            "weaknesses": {
                "value": "- Ambiguity in the Underlying Hypothesis:\nWhile the paper proposes that the inconsistency arises due to the separation of knowledge guiding words and deeds, this hypothesis remains underexplored. The authors could strengthen the paper by providing more in-depth analysis or empirical evidence that supports this claim. For example, conducting case studies to trace the model's reasoning paths, could illustrate where the divergence occurs."
            },
            "questions": {
                "value": "I have no questions for the authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "1. This work proposes a novel benchmark called the WDCT to measure the inconsistency between words and actions. This benchmark contains test pairs of word-based and action-based questions across various domains to measure the alignment between models' statements and their behaviors.\n\n2. The paper introduces the issue of large language models (LLMs) being inconsistent between what they say (words) and what they do (deeds). This inconsistency is observed across different domains such as opinions, non-ethical values, and theories. The authors argue that this gap reduces trust in LLMs.\n\n3. The authors introduce two metrics, the Consistency Score (CS) and the Probability Consistency Score (PCS), to evaluate the alignment between words and deeds. These metrics help quantify how well models maintain consistency across different tasks.\n\n4. Conducted extensive experiments on multiple state-of-the-art LLMs, including GPT-4, Vicuna, and LLaMA, to evaluate their consistency using the WDCT. The results show a significant misalignment between words and deeds across models and domains, particularly in non-ethical value contexts.\n\nI am willing to increase my score during the rebuttal phase if the authors address my concerns."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces two new metrics, Consistency Score (CS) and Probability Consistency Score (PCS), which provide a quantitative way to measure how well LLMs maintain consistency between their words and deeds. These metrics offer valuable tools for evaluating model alignment.\n\n2. Conduct thorough experiments on various LLMs, including GPT-4, Vicuna, and LLaMA, across various domains like opinions, non-ethical values, and theory. The experiments reveal that all the tested models exhibit the issue of inconsistency between what they say and what they do across all the domains.\n\n3. This work shows that commonly used methods such as COT, alignment and paraphrasing do not solve this issue."
            },
            "weaknesses": {
                "value": "1. The paper looks a little bit rough in places. For example, Table 4 and Figure 8 are poorly formatted, extending beyond the usual layout boundaries, which affects readability and professionalism.\n\n2. While the paper effectively identifies the issue of inconsistency between words and deeds in LLMs, it does not provide any in-depth analysis or reasoning behind why this inconsistency occurs. This leaves a gap in understanding the reason cause of the problem.\n\n3.  Although some alignment techniques are explored, they are not shown to fully resolve the inconsistency, and the paper does not provide clear insights for overcoming this challenge.\n\n4. The results rely on somewhat outdated models like LLaMA 2. Given that more advanced versions like LLaMA 3 and LLaMA 3.1 are now available, it would have been better to test these newer models to assess whether they exhibit similar issues or show improvements in consistency.\n\n5. This work lacks ablation studies. Like studying which components of the models or alignment techniques contribute most to the word-action inconsistency. This would help clarify whether the problem is primarily due to data, model architecture, or training techniques."
            },
            "questions": {
                "value": "1. In Table 3, both LLaMA 2 and Mistral base models show relatively low average Consistency Scores (CS), whereas ChatGLM3 base has a noticeably higher CS. Does this suggest some fundamental differences in the architecture, training methodology, or other factors that set ChatGLM3 apart from the other two models?\n\n2. In Figure 5, the consistency rate for the second epoch suddenly drops from over 50% in epoch 1 to 0% in epoch 2. Is there anything unusual about this drastic drop? Could it be related to the specific alignment process, or does this indicate a possible issue in the model's training or evaluation setup?\n\n3.  Given that LLaMA 2 is somewhat outdated, would it be possible to try the newer LLaMA 3.1 Instruct model? It would be interesting to see if the consistency issues observed in LLaMA 2 are still present in LLaMA 3.1 instruct.\n\n4. Have you analyzed how sensitive the consistency scores are to the prompts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces an evaluation benchmark to assess the consistency between the verbal outputs of language models and fact-based realities that goes by the Words and Deeds Consistency Test (WDCT).\n\nThis benchmark ensures the correspondence between word-based and fact-based questions in various domains. Evaluations with the GPT, Llama and Mistral-based models revealed weaknesses, potentially due to model tuning problems.\n\nThis research highlights the importance of assessing the reliability of LLMs by measuring the alignment between words and actions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The contribution has its merits. A detailed list follows:\n\n- The writing is clear, fluent and understandable. However, the organisation of the introduction should definitely be reconsidered (comment on weak points)\n\n- The authors address an important issue in evaluating words and deeds using LLMs.\n\n- The datasets introduced in the paper include several domain topics to ensure better generalisability (however, see weaknesses).\n\nThe habitual evaluation of LLMs is very extensive and detailed."
            },
            "weaknesses": {
                "value": "Although the contribution has its myths, there are many points that need to be clarified and improved:\n\n**Introduction**\n\n- RQs are described (this is a good thing). However, these should be dealt with in the course of the contribution and perhaps discursively introduced in the intro. The \u2018answers\u2019 delivered by the authors in the intro to lines 49,53,73 should be revised (this is not a note on the content but on the style \u2018To answer...\u2019 is not a proper formula to be repeated three times).\n\n- The summary of contributions on line 82 makes sense but why use line 49 to line 77 to answer the RQs and then repeat lines 82-90?\n\n**Section 2 and 3**\n\n- The experimental setting is described in a sufficient and comprehensible manner, even if it is sometimes difficult to follow the discourse due to the positioning of the figures.\n\n- The evaluation is made on a limited dataset (1325 examples). This is not a major problem, but it does undermine the scientific soundness of the paper. For example, look at the well-known QA tasks (SIQA, PIQA) or the mathematicians (GSM8K). all of these have significantly more instances than 1k. It would be interesting to observe these phenomena on a larger scale (e.g. 4k).\n\n- The construction of the examples could be biased by the constructor (an LLM constructs, to be precise does augmentation, of the examples on which it is prompted) \n\n\n**Section 4 and 5**\n\nThe fact that the authors decided to organise the section into Discoveries and Discovery Discussions is not wrong as many papers do this. However, I do not think it is the best way. Indeed, the contribution shows a lot of analysis but it is really limiting to organise the presentation of results in this way.\n\nFurthermore, it is difficult to read the comments of the experiments in the findings. For example, see \u2018Exp. 1\u2019 on line 326. I understand that the authors wanted to make the work more discursive, but they should pay more attention to style. An example could be constructing subsections by structuring the discourse by placing the RQ next to the answer. The title of the section should describe the analysis (e.g. line 334 should be a subsection called \u2018inconsistency\u2019 or something similar).\n\n\n\nI would strongly advise the authors to revise the structure of the paper as in this condition it is really a pity to leave it as it is."
            },
            "questions": {
                "value": "- What is the role of the consistency score? As a non-multiple-choices question answer, your score should be able to capture all possible generations. Did you test and or ablation test for this?\n\n- How much impact do systemic demonstrations in SFT have on DPO? \n\n- What are the limits of the transferability of your study? What if another LLM was used instead of GPT-4 to build the samples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}