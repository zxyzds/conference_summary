{
    "id": "SNsdlEp3Ne",
    "title": "Efficient Text-driven Human Motion Generation via Latent Consistency Training",
    "abstract": "Consistency models excel at few-step inference in generative tasks across various scenarios, but typically rely on pre-trained diffusion model distillation, involving additional training costs and performance limitations. \nIn this paper, we propose a motion latent consistency training framework that learns directly from data rather than distillation for efficient and text-controllable human motion generation.\nFor representation optimization, we design a motion autoencoder with quantization constraints that enable concise and bounded motion latent representations.\nFocusing on conditional generation, we construct a classifier-free guidance (CFG) format with an additional unconditional loss function that extends the CFG technique from the inference phase to the training phase for conditionally guided consistency training.\nWe further propose a clustering guidance module to provide additional references to the solution distribution at minimal query cost.\nBy combining these enhancements, we achieve stable and consistent training in non-pixel modality and latent representation spaces for the first time.\nExperiments in benchmarks demonstrate that our method significantly outperforms traditional consistency distillation methods with reduced training cost, and enhances the consistency model to perform comparably to state-of-the-art models with lower inference cost.\nOur code will be open source.",
    "keywords": [
        "motion generation",
        "diffusion",
        "consistency training"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=SNsdlEp3Ne",
    "pdf_link": "https://openreview.net/pdf?id=SNsdlEp3Ne",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a novel framework for efficient text-to-motion synthesis, which requires one third of inference time compared to the existing state-of-the art motion synthesis work, MoMask. Different from existing latent consistency model, e.g., MotionLCM, the current framework gains insight from consistency training in pixel-based space, and introduce a bounded quantization method which converts continuous motions to the discrete value ranging from -1 to 1 like pixels. Meanwhile, consistency constraints are involved in both training and inference, potentially go beyond the pre-trained diffusion model. Quantitative evaluations showed that the proposed method performed comparable to existing state-of-the-art methods while being faster two time than MoMask, and being the same fast as latests motion consistency model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**1. Technically novel**. This work proposes a novel scheme of motion latent consistency models. I like the idea of how the author tailored the motion representation as a pixel-like representation using the bounded quantization. Different from existing motion consistency model, MotionLCM, which tried to distill knowledge from pre-trained diffusion model and thus is limited to the performance of these diffusion models,  the current work incorporate consistency constraints in both training and inference. Meanwhile, other novel techniques such as CFG in consistency training to enhance the semantic matchness. Overall, this work proposes a fundamentally different framework for efficient text-to-motion synthesis. However, since my expertise is not on the theoretical side, I may not be able to validate the theories described in the paper.   \n**2. Comprehensive evaluations**. I appreciate the authors effort in conducting a series of experiments for baseline comparisons, ablation analysis. From table 4-8, we can observe the effect of token number, quantization level, and clustering counts.    \n**3. Outperforming quantitative evaluations**. From table 2-3, the proposed method achieves comparable performance (i.e., FID, RPrecision) comparing to non-consistency model, which being two times faster. Compared to existing motion consistency mode, i.e., MotionLCM, the proposed method costs the same time will performs better on motion quality and semantic matchness."
            },
            "weaknesses": {
                "value": "**1. Presentation need improvement**. The presentation is kind of messy in this paper. Many symbols in Figure 1 & 2 are corrupted, which makes them hard to understand. Without this two figures, it's troublesome for the readers to understand the main logic and points in the proposed framework. Though there are detailed descriptions in text, understanding the whole picture is still challenging, especially for connecting all the pieces together.    \n**2. Qualitative results look mediocre**. Although the quantitative results show the effectiveness of the proposed consistency model, while in the visualizations the difference are rather subtle. BTW, I think it's necessary to visually compare to motionLCM as well, as it's the most related work. In addition, from the visual results, we can observe several artifacts such as foot sliding, motionless movement, etc. It also seemed that the authors use different length for MoMask and other diffusion based method, which may not be fair.   \n**3. Limited significance**. The current work aims to address the efficiency of text-to-motion that, however, not seems not an important problem any more. Unlike images/videos, motions are low-dimensional data. After the work of MLD & MoMask, the time cost for motion synthesis is not that unacceptable. Though this work proposes a more lightweight inference scheme, the time cost is not substantially different, only from 0.118 to 0.031. For me, the significance of this work is questionable."
            },
            "questions": {
                "value": "Please refer to the weakness section. The presentation and figures are yet to be ready. I also hope the authors could properly justify the significancy of this work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a motion diffusion model with consistency regularization in latent space. This paper encode the motion into a latent space with quantization. They also simulate the conditional generation and design a similar loss for training. They also introduce a cluster center guidance to map text center to motion center."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper shows low inference cost for motion synthesis, and is competitive with the prevailing MotionLCM.\n2. This paper presents good results on KIT and HumanML3D, and conducts many ablation study."
            },
            "weaknesses": {
                "value": "1. The quantization of latent code is widely used and the cross-attention for text and motion is common. These two modules are not highly related to the consistency training. This can also be reflected in Figure 3.\n2. In Table 2 and Table 3, the motions synthesized by the proposed method show results than the real data in Top-k R-precision, and this should be explained.\n3. According to Table 3, the Modality is sacrificed during the consistency training.\n4. Some contents are not self-contained. For example, in Eq. 4, why \\mathcal{S}=\\epsilon is not explained. In Eq. 5, the \\hat{\\mathcal{S}} is not explained. In line 266, x_{\\epsilon} and x_0 can be confusing."
            },
            "questions": {
                "value": "The definition of each symbol should be given clearly for easy read."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Overall, considering the success and advantages of consistency training, this paper extends consistency training from the pixel space to the latent space for the first time. Meanwhile, this paper also correspondingly involves some designs such as over the training-time CFG."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Based on the importance of consistency training, I believe that this paper addresses an important problem of extending consistency training to the latent space.\n\n2. The paper does achieve a quite efficient inference as claimed."
            },
            "weaknesses": {
                "value": "(see the questions part for more details.)"
            },
            "questions": {
                "value": "Overall, I believe that this paper currently is roughly on the borderline level. Below are my concerns.\n\n1. Fisrtly, I believe that Figure 1 needs to be adjusted to also involve \"traditional\" consistency training as (c) and the proposed latent consistency training as (d). This is to more clearly compare the proposed method and the existing consistency training pipeline. The current drawing can confuse the readers on the advantage of consistency training and the advantage of the proposed method itself.\n\n2. I suggest the authors to more explicitly point out the challenge of extending consistency learning from the pixel space to the latent space. The current manuscript seems to focus more on how this is done in the proposed method but not why this is challenging, leading to difficulty in better understanding the contribution of this work.\n\n3. I believe that the sensitivity of the quantization level l across different datasets is important to be measured. Specifically, I am curious that whether the quantization level would cause different level of information loss among different datasets.\n\n4. W.r.t Sec. 4.3, the authors seem to cluster text into different clusters and provide motion clustering guidance. I am wondering if this could lead the effectiveness of the guidance, for text near and far from the center of the cluster, to be different.\n\n5. W.r.t. table 1 and 2, I also have the following suggestions:\n\n(1) It is suggested for the model training time for those methods that need to train the parent model (e.g., distillation-based methods) to be also reported to better show the advantage of the proposed method.\n\n(2) It is suggested to combine table 1 and 2 to give readers a better feeling over the performance vs the efficiency."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}