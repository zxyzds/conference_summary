{
    "id": "GQhlM0Mavg",
    "title": "Exploring the Link Between Out-of-Distribution Detection and Conformal Prediction with Illustrations of Its Benefits",
    "abstract": "Research on Out-Of-Distribution (OOD) detection focuses mainly on building scores that efficiently distinguish OOD data from In Distribution (ID) data. \n    On the other hand, Conformal Prediction (CP) uses non-conformity scores to construct prediction sets with probabilistic coverage guarantees. In other words, the former designs scores, while the latter designs probabilistic guarantees based on scores. Therefore, we claim that these two fields might be naturally intertwined. \n    This work advocates for cross-fertilization between OOD and CP by formalizing their link and emphasizing two benefits of using them jointly.\n    First, we show that in standard OOD benchmark settings, evaluation metrics can be overly optimistic due to the test dataset's finite sample size.\n    Based on the work of (Bates et al, 2022), we define new *conformal AUROC* and *conformal FRP@TPR$\\beta$* metrics, \n    which are corrections that provide probabilistic conservativeness guarantees on the variability of these metrics.\n    We show the effect of these corrections on two reference OOD and anomaly detection benchmarks, OpenOOD (Yang et al, 2022) and ADBench (Han et al. 2022). \n    Second, we explore using OOD scores as non-conformity scores and show that they can improve the efficiency of the prediction sets obtained with CP.",
    "keywords": [
        "Out-of-distribution detection",
        "Conformal Prediction",
        "benchmark",
        "nonconformity scores"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We show that Out-of-distribution detection and Conformal Prediction are naturally intertwinded and emphasize some benefits of this link.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=GQhlM0Mavg",
    "pdf_link": "https://openreview.net/pdf?id=GQhlM0Mavg",
    "comments": [
        {
            "summary": {
                "value": "The paper examines the relationship between conformal prediction (CP) and out-of-distribution (OOD) detection. It approaches the OOD detection problem from a conformal prediction perspective and, inspired by CP, proposes several modified metrics for OOD evaluation, testing them on various OOD benchmarks. Additionally, the paper constructs prediction sets for classification tasks, drawing on techniques from OOD detection."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper is well-structured, well-written, and easy to read.\n\n* The paper introduces modified metrics for OOD evaluation that are more robust when considered from a hypothesis testing perspective.\n\n* The OOD detection problem is significant for ML research, making this topic relevant to the ICLR community.\n\n* The exploration of the link between OOD and CP is informative."
            },
            "weaknesses": {
                "value": "* The main insight of the paper is that FPR@\u03b2 can be interpreted as a p-value within a specific statistical hypothesis testing framework. By building on the work of [1], the paper proposes a corrected estimator for this metric to improve robustness. While this approach is interesting, I believe it is insufficient to warrant publication. Firstly, much of the technical foundation relies on [1], and the corrected metric for OOD detection performs similarly to the classical version. Therefore, I do not find the current version sufficiently novel for publication.\n\n* Computing the proposed metric for OOD detection requires access to extra validation sets.\n\nMinor Comments:\nLine 22: FPR? (not FRP)\n\n[1]Stephen Bates, Emmanuel Cand\u00e8s, Lihua Lei, Yaniv Romano, and Matteo Sesia. Testing for Outliers\nwith Conformal p-values, May 2022. URL http://arxiv.org/abs/2104.08279."
            },
            "questions": {
                "value": "1. Is there a scenario where classical metrics like FPR@\u03b2 or AUROC might fail for OOD evaluation, but your conformal metrics accurately capture the evaluation? How do the experimental results in Section 4 contribute to safer OOD evaluation?\n\n2. When using OOD scores as nonconformity scores in CP, can you still obtain theoretical guarantees for the prediction sets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the relationship between Out-of-Distribution (OOD) detection and Conformal Prediction (CP). They apply conformal prediction methods from prior work by Bates et al. (2022) to the task of Out-of-Distribution (OOD) detection, which is to distinguish in-distribution (ID) data from OOD data. They introduce metrics conformal AUROC and conformal FPR@TPR95 which provide conservative probabilistic guarantees in OOD detection tasks, particularly relevant for safety-critical applications. The authors then empirically validate the performance of these metrics on common OOD benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper presents an application by adapting the conformal prediction methods from Bates et al. (2022) to OOD detection."
            },
            "weaknesses": {
                "value": "(1) The methodology largely follows the framework established by Bates et al. (2022) in the conformal prediction literature, with limited novel contributions specific to OOD detection. The primary innovation appears to be an application of existing CP methods to OOD tasks rather than a new approach.\n\n(2) While the idea of combining OOD detection with CP is interesting, the approach itself and the resulting metrics are relatively straightforward extensions and may not represent a significant advancement in either OOD or CP methodologies.\n\nminors:\n(1) Line 135: In Equation (2), n_val is not defined and weird, should be probability. Similarly, n_val is used as probability in Line 296, Equation (10). Other places, it is used as the number of data points. \n \n(2) A lot of places, for example, section 4.3.1, tau and t are used in mix to refer to the same thing."
            },
            "questions": {
                "value": "Given the significant overlap with Bates et al. (2022) 's methods, what specific contributions does this paper make beyond adapting CP to OOD tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the connection between Out-of-Distribution (OOD) detection scores and the non-conformity scores in Conformal Prediction (CP), showing the potential for cross-fertilization between these methods. Non-conformity scores offer probabilistic interpretation and correction for OOD scores, while OOD scores can enhance the efficiency of prediction sets obtained through CP."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "\u2022\tOOD scores can be unreliable, and employing new statistical developments to improve their reliability is a promising direction. Using the uncertainty estimates from CP appears to be a robust approach to address OOD score unreliability.\n\n\u2022\tThe paper targets a practical problem, effectively demonstrating key concepts through comprehensive case studies.\n\n\u2022\tBy linking OOD detection with CP, this work facilitates cross-fertilization that benefits both machine learning and statistics. The authors provide a good introduction to using CP for interpreting and correcting OOD scores."
            },
            "weaknesses": {
                "value": "\u2022\tNon-conformity scores in CP require an evaluation dataset, and it reliability depends on the choice of this dataset. This introduces a practical challenge in implementing this statistically sound solution.\n\n\u2022\tThe paper presents limited innovation, as much of the work is based on existing research."
            },
            "questions": {
                "value": "I think that there are two types of uncertainties for a model predicting a new test instance:\n\n1.\tPrediction Confidence: Assuming the instance is within the In-distribution, this type of uncertainty relates to the strength of evidence supporting the prediction.\n\n2.\tIn-Distribution Uncertainty: As discussed in this paper, this refers to uncertainty regarding whether the instance truly belongs in the In-distribution.\n\nI would appreciate further discussion from the authors on the distinctions and connections between these two types of uncertainties. Can they be quantified into a single metric?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors ask themselves, and answer positively, to the question whether CP can be beneficial to OOD, and whether the vice versa holds as well. They also validate empirically their findings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper makes an extremely interesting parallel between OOD and CP. In addition, I found very interesting the marginal guarantees that the authors were able to find."
            },
            "weaknesses": {
                "value": "CP is a method for *uncertainty representation*, not uncertainty *quantification*. Indeed, CP *represents* uncertainty via the conformal prediction region. It does not quantify it: there is no real value attached to any kind of predictive uncertainty (e.g. aleatoric or epistemic, AU and EU, respectively). Some claim that the diameter of the conformal prediction region quantifies the uncertainty, but even in that case, it is unable to distinguish between AU and EU. Indeed, the diameter is a positive function of both: it increases as both increase, and hence it cannot be used to distinguish between the two [1]. Please add this clarification in the camera-ready version of the manuscript.\n\nThe simplest CP technique is (arguably) transductive CP, not split CP.\n\nShouldn't $n_\\text{val}$ in (2) and in (10) be substituted by $P$? In particular, CP guarantees hold for all exchangeable countably additive probabilities $P$ on the space $\\mathcal{Y}$ of outputs.\n\nHow does the proposed method relate to the subsequent work by Kaur [2]?\n\n---\n\n[1] https://arxiv.org/abs/2302.09656\n\n[2] https://arxiv.org/abs/2302.11019"
            },
            "questions": {
                "value": "See Weaknesses. In addition, Typo at line 69: eas (2023). Shouldn't it be Easa?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}