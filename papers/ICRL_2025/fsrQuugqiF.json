{
    "id": "fsrQuugqiF",
    "title": "Tensor-Var: Variational Data Assimilation in Tensor Product Feature Space",
    "abstract": "Variational data assimilation estimates the dynamical system states by minimizing cost function that fits the numerical models with observational data. The widely used method, four-dimensional variational assimilation (4D-Var), has two primary limitations: (1) computationally demanding for complex nonlinear systems; and (2) relying on state-observation mappings, which are often impractical. Recently, deep learning (DL) has been used as a more expressive class of efficient model approximators to address these challenges. However, integrating such models into 4D-Var remains challenging due to their inherent nonlinearities and the lack of theoretical guarantees for consistency in assimilation results. In this paper, we propose \\textit{Tensor-Var} to address these challenges using kernel Conditional Mean Embedding (CME). Tensor-Var characterizes system dynamics and state-observation mappings as linear operators in a feature space, enabling a more efficient linear 4D-Var framework. Our method seamlessly integrates CME with 4D-Var, offering theoretical guarantees of consistent assimilation results between the original and feature space. To improve CME scalability, we use deep kernel features that map data into a finite-dimensional feature space, utilizing the expressiveness of deep learning. Experiments on chaotic systems and global weather forecasting demonstrate that Tensor-Var outperforms operational and DL hybrid methods 4D-Var baselines in terms of accuracy while achieving efficiency comparable to the static 3D-Var method.",
    "keywords": [
        "Variational Data Assimilation",
        "Dynamical System",
        "Weather Forecasting",
        "Representation Learning"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=fsrQuugqiF",
    "pdf_link": "https://openreview.net/pdf?id=fsrQuugqiF",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes an innovative Tensor-Var method, which integrates kernel conditional mean embedding with 4D-Var data assimilation to address computational efficiency and convergence issues in the optimization of nonlinear dynamical systems."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The Tensor-Var method is proposed, applying kernel conditional mean embedding to 4D-Var data assimilation, significantly improving the optimization efficiency of nonlinear dynamical systems."
            },
            "weaknesses": {
                "value": "Although the method shows remarkable performance on simulated data, its validation on real-world datasets is relatively limited. Additional tests in practical application scenarios could be supplemented."
            },
            "questions": {
                "value": "see Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Tensor-Var, a novel data assimilation (DA) framework that uses Conditional Mean Embeddings (CME) in conjunction with deep learning based nonlinear feature maps (\"deep features\", DFs) to linearize observed nonlinear dynamics in feature space, addressing issues of the traditional 4D-Var data assimilation problem. Tensor-Var achieves both competitive accuracy and efficiency compared to 4D-Var and other baselines in chaotic dynamics and numerical weather prediction benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The authors use known and established methods in the field of deep learning and nonlinear dynamics (CME, DFs, delay embeddings, overcomplete autoencoders) to substantially improve over existing DA methods.\n- The paper is generally well-written\n- The authors clearly motivate each and every modification to the final employed algorithm they propose and provide plenty of theoretical justifications"
            },
            "weaknesses": {
                "value": "- A lot of compartments of the approach have deep roots in dynamical systems theory (DST), which the authors do not seem to touch upon:\n1. using a history of observations to better model the state of the underlying dynamical system is connected to the method of delay embeddings, often used for attractor/state space reconstruction in nonlinear dynamics [1, 2]. This field has ample literature, including (optimal) heuristics to create these embeddings, from choosing time lag between history time stamps to number of lags/dimension of the embedding [3]. This literature might provide better ways of finding an optimal representation of the underlying system state and might a reasonable alternative to costly cross-validation to find the optimal history length. All this could be discussed when introducing the history method in section 3.1.\n2. Mapping nonlinear dynamics in (infinite) higher dimensional spaces with the aim to linearize the dynamics in this space is the core idea of Koopman Operator theory [4]. However, I am not familiar with KKL-observer theory (which the authors address in the paper). These fields might be strongly connected. Anyhow, at least a brief connection to this large field of research is missing in the manuscript.\n\n- As the authors point out in their limitations: The feasibility of the framework in real-world applications is questionable due to the ground-truth state data demand. However, a more elaborate discussion on how Tensor-Var yields any improvement in this data setting compared to traditional DA methods would be welcome to guide future research.\n\nMinor Details:\n- ll. 173-174: \u201cEquation equation 1\u201d\n- ll. 356-357: \u201cglobal optimal\u201d should be \u201cglobal optimum\u201d\n- Author Contributions and Acknowledgments on p. 10 seem to contain the sample text of the ICLR template.\n\nReferences: \n\n[1] Sauer, Tim, James A. Yorke, and Martin Casdagli. \"Embedology.\" Journal of statistical Physics 65 (1991): 579-616.\n\n[2] Kr\u00e4mer, Kai-Hauke, et al. \"A unified and automated approach to attractor reconstruction.\" New Journal of Physics 23.3 (2021): 033017.\n\n[3] Kantz, Holger, and Thomas Schreiber. Nonlinear time series analysis. Cambridge university press, 2003.\n\n[4] Brunton, Steven L., et al. \"Modern Koopman theory for dynamical systems.\" arXiv preprint arXiv:2102.12086 (2021)."
            },
            "questions": {
                "value": "- Is the entire loss objective of Tensor-Var really convex if the DFs are jointly learned with the linear operators/CMEs? Since the feature maps are parameterized by nonlinear NNs, the optimization should still be non-convex, no?\n- Is there an intuitive reason why the NRMSE distributions of Latent 4D-Var are a lot more spread compared to Latent 3D-Var and Tensor-Var?\n- Can the authors comment on the optimization of loss (7)? Learning the inverse feature map $\\phi^\\dagger_{\\theta'_s}$ in this form comes down to an overcomplete autoencoder, which, when trained with a simple reconstruction loss, can often fall short in learning useful features and may easily overfit. Did the authors face any problems when training this architecture? And how does the regularization weight w influence the performance of Tensor-Var?\n- A linear approximation of nonlinear dynamics can only be approximate, and hence, especially in chaotic systems, longer roll-outs will lead to qualitative and quantitative differences in the learned linear dynamics compared to the true dynamics (i.e. a linear model of nonlinear dynamics can not reproduce the \u201cclimate\u201d or long-term behavior of the underlying system). Can the authors comment on this in the context of their DA approach?\n\nI'm happy to increase my score if the authors appropriately address my comments and questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents *Tensor-Var*, an efficient 4D-Var computational framework. The key component of the framework are the linear representations of the dynamical systems and state-observation mappings obtained by kernel Conditional Mean Embedding. These transform the nonlinear state and observation models into a linear, convex framework, alleviating the need of nonlinear optimization. The framework is benchmarked against competitive variational DA baselines on two chaotic systems and a numerical weather prediction task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- *Originality*: This work stands out as original as it avoids the DL trend and looks for an approximated solution which preserves theoretical guarantees. \n- *Quality & Clarity*: Thorough background as well as theoretical analysis of the framework is provided; proving the existence of linear dynamics with consistent convergence between the original and feature space solutions.\n- *Significance*: The work addresses a core task, forecasting of dynamical systems, and provides improvement in efficiency and accuracy over evaluated settings."
            },
            "weaknesses": {
                "value": "- *Baseline choices*: The authors describe many DL approaches suggested for the task and it is unclear why the performance of the chaotic system is evaluated only with respect to Frerix et al., 2021, neglecting the more recent attempts (e.g. Bocquet et al. 2024) and avoiding the Latent-x-Var baseline considered in the NWP task. \n- *Efficiency claims*: The framework is compared to variational approaches which rely on GPU acceleration via JAX (Bradbury et al., 2018) however runtime was evaluated on CPU making the comparison misleading and unfair. To address this the authors can either run each framework on most suitable hardware or state this evaluation limitation in the main text. A final alternative is to provide a GPU compatible *Tensor-Var* implementation.\n- *Evaluation on synthetic data*: As mentioned by the authors, showcasing performance on simulated data is a limitation of the current work. It makes it hard to evaluate the applicability and performance over real world applications. \n- *Poorly written*: The paper is hard to read, it contains many grammatical errors, unclear sentences, and many repetitions. For example, lines 164-166 repeat definition mentioned a few times and contain grammatical inconsistencies, Lines 270-271 contain errors, and the entire conclusion section can be improved.\n(**minor; remaining paragraphs from the template on page 10)."
            },
            "questions": {
                "value": "In light of the above weaknesses, it will be valuable if the authors would:\n(1) clarify baseline choices as well as runtime evaluation;\n(2) showcase a more realistic application;\n(3) proofread the manuscript to improve readability."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper utilizes the CME method to improve the efficiency of 4D-Var; Several toy scenarios are used to study the assimilation ability of the TENSOR-VAR in chaotic systems and global weather forecasting. Observations are simulated by sampling the dataset, rather than using true observations, for the assimilation of the background field. Ultimately, this work achieves a toy assimilation model with limited significance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The article thoroughly discusses how the Conditional Mean Embedding (CME) method can further improve the efficiency of 4D-Var. It presents the mathematical principles of the method clearly, albeit with some redundancy, and employs several toy scenarios to compare various assimilation approaches."
            },
            "weaknesses": {
                "value": "1. The resolution is too low, and it seems that all observations are located at grid points (If not, please correct it), which greatly simplifies the problem.\n\n2. Real observations are not used in assimilation:\n\n(1) Real observations are often not randomly distributed and typically exhibit some certain patterns.\n(2) The ERA5 dataset is already a reanalysis dataset (incorporating numerous observations and numerical simulations). Using random samples from ERA5 to simulate assimilation means that the model assimilates only one type of data, which differs significantly from reality. This article does not address or explain the assimilation of multiple sources of observations.\n(3) A key aspect of assimilation is real-time processing (unlike data fusion), and the experimental design of this paper does not explore online operational scenarios. \n\nIn summary, I believe this work is still far from real assimilation applications. The research method and experimental design are relatively detached from real-world conditions, which significantly reduces the study's relevance and value.\n\n3. The paper includes an excessive amount of mathematical derivations that appear 'sophisticated' but are redundant and unhelpful. \nIn summary, I believe this work is still far from practical assimilation applications. The research methods and experimental design are relatively detached from real-world conditions, which significantly diminishes the study\u2019s relevance and value. It artificially creates barriers to understanding, making it highly unsuitable for dissemination in physics fields like atmospheric science, complex systems, and physics."
            },
            "questions": {
                "value": "In weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}