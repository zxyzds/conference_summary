{
    "id": "ufhrQG5xie",
    "title": "POIL: Preference Optimization for Imitation Learning",
    "abstract": "Imitation learning (IL) enables agents to learn policies by mimicking expert demonstrations. \nWhile online IL methods require interaction with the environment, which is costly, risky, or impractical, offline IL allows agents to learn solely from expert datasets without any interaction with the environment.\nIn this paper, we propose Preference Optimization for Imitation Learning (POIL), a novel approach inspired by preference optimization techniques in large language model alignment. \nPOIL eliminates the need for adversarial training and reference models by directly comparing the agent's actions to expert actions using a preference-based loss function. \nWe evaluate POIL on MuJoCo control tasks under two challenging settings: learning from a single expert demonstration and training with different dataset sizes (100\\%, 10\\%, 5\\%, and 2\\%) from the D4RL benchmark.\nOur experiments show that POIL consistently delivers superior or competitive performance against state-of-the-art methods in the past, including Behavioral Cloning (BC), IQ-Learn, DMIL, and O-DICE, especially in data-scarce scenarios, such as using one expert trajectory or as little as 2\\% of the full expert dataset. \nThese results demonstrate that POIL enhances data efficiency and stability in offline imitation learning, making it a promising solution for applications where environment interaction is infeasible and expert data is limited.",
    "keywords": [
        "Offline Imitation Learning",
        "Preference-based Reinforcement Learning",
        "Large Language Model Alignment",
        "Data Efficiency"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "POIL is a novel offline imitation learning method inspired by preference optimization in language models. It delivers superior or competitive performance against SOTA methods in Mujoco task by eliminating adversarial training and reference models.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ufhrQG5xie",
    "pdf_link": "https://openreview.net/pdf?id=ufhrQG5xie",
    "comments": [
        {
            "summary": {
                "value": "This paper considers a practical offline imitation learning setting, aiming at learning the agent\u2019s policy from limited demonstration without environmental interactions. The proposed method is called Preference Optimization for Imitation Learning (POIL), which compares the agent\u2019s actions with the expert\u2019s actions and computes the preference loss for updating the policy parameters. The empirical results on MuJoCo show consistently superior performances compared to several offline imitation learning baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tPOIL adapts the preference optimization techniques from large language models, eliminating the need for preference datasets, a discriminator, or a preference model. The learning process is simplified, avoiding adversarial training instability. This work exemplifies the successful adaptation of DPO-like alignment methods in LLMs to control problems in reinforcement learning and imitation learning. \n2.\tPOIL is simple and effective and shows superior performance in MuJoCo tasks compared to several offline imitation learning methods. POIL also performs better than other preference optimization methods in MuJoCo tasks."
            },
            "weaknesses": {
                "value": "1.\tFrom Section 2, we know that SPIN is the most closely related to the proposed POIL, however, the POIL objective is directly adapted from CPO instead of SPIN, which is strange and may cause confusion. I believe CPO is chosen because it avoids using a preference model. In this sense, maybe CPO is the most related work to POIL? \n2.\tSPIN can generate its training data and refine itself by distinguishing between current and previous outputs, continuously updating its reference model. In Section 3.2, Equation (3) is explained to maximize the divergence between the agent\u2019s current behavior and its previous, which tries to link POIL to SPIN. However, it is not clear how this goal has been achieved."
            },
            "questions": {
                "value": "1.\tHow does equation (3) maximize the divergence between the agent\u2019s current and previous behaviors?\n2.\tIt seems POIL works quite well when \\lambda=0 in Table 1, and POIL is sensitive to different \\lambda in Figure 3. Does it mean that there is no need to introduce the BC regularization to the POIL loss?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduce an imitation learning method called POIL, a DPO-like method, that compares agent actions (negative example) with expert actions (positive example). It evaluates POIL on 3 Mujoco tasks: halfcheetah, hopper, and walker2D."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- POIL is an interesting application of DPO like methods to robotics.\n- the results demonstrate sample-efficiency gains compared to a few IRL and one BC baseline on Mujoco environments\n- the ablations comparing various DPO like methods in robotics is interesting"
            },
            "weaknesses": {
                "value": "- This work is missing comparisons to (or atleast discussions in related work comparing to) sample-efficient BC approaches that can work with one or very few demonstrations like ROT [1] and MCNN [2].\n- A comparison (or discussion about) CPL [3] and other baselines in the CPL paper, another RLHF method for robotics, is also missing. \n- The evaluation environments are very simple vector-observation mujoco environments. It would be helpful to extend to either more environments from D4RL that testing stitching like the ant maze environment, or more complicated image-based environments like Atari, or more dexterous robotics environments like Robosuite and Adroit.\n- the return tabulated for different methods is not normalized --- this makes it hard to determine its performance between random and expert and hard to compare with other papers.\n\n\n[1] S Haldar, et al, Watch and match: Supercharging imitation with regularized optimal transport, CoRL 22\n\n[2] K Sridhar, et al, Memory-Consistent Neural Networks for Imitation Learning, ICLR 24\n\n[3] J Hejna, et al, Contrastive Preference Learning: Learning from Human Feedback without RL, ICLR 24"
            },
            "questions": {
                "value": "Please see weaknesses.\nAlso, have the authors investigated a provable upper bound on the sub-optimality gap for POIL, maybe building on guarantees for DPO-like methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors consider framing offline IL as a preference learning problem. Specifically, they generate actions from the learner on states from the expert demonstrations and try to raise the relative probability of the expert actions to the learner actions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "-  A generally clear exposition of the proposed method."
            },
            "weaknesses": {
                "value": "Apologies for LaTex failing to render properly below -- I spent some time playing around with things to no avail.\n\nI am fairly confident the proposed method is provably equivalent to a noisy version of behavioral cloning. To see this, first note that for policies in the exponential family (e.g. Gaussians), we can always write $\\pi(a|s) = \\exp(f(s, a)) / \\sum_{a'} \\exp(f(s, a'))$. Then, the likelihood gradient can be expanded to $\\nabla_{\\theta} \\log \\pi_{\\theta}(a_E|s) = \\nabla_{\\theta} f_{\\theta}(s, a_E) - \\nabla_{\\theta} \\log \\sum_{a'} \\exp(f_{\\theta}(s, a')) = \\nabla_{\\theta} f_{\\theta}(s, a_E)  - \\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot|s)}[\\nabla_{\\theta} f_{\\theta}(s, a)]$. Observe that if we ignore the $\\log \\sigma$ in the POIL loss for a moment, the BC gradient is simply the \"infinite sample\" estimate of POIL loss. Put differently: MLE in exponential families already includes a \"negative gradient,\" there is no need to add one in explicitly.\n\nFor the specific case of Gaussians, for which the sufficient statistics / moments ($f_{\\theta}$ in the above notation) are the mean and variance, the negative gradient term is basically computing the mean by sampling from the policy. I don't see how this can provide any value compared to straight MLE / BC where we just use the fact we know the mean because we know the policy.\n\nOf course, one might then ask about the effect of the $\\log \\sigma$. If one recalls the original MaxEnt IRL / DPO derivations, this term in the loss is meant to ensure closeness to the prior / reference policy. However, the POIL loss does not include any regularization to the prior (i.e. there is no $\\pi_{ref}$ in the denominator), so this is at best providing entropy regularization (i.e. bumping up the variance for a Gaussian policy). This could be done without any samples from the policy, which again begs the question of what we're getting out of this more complex procedure compared to BC."
            },
            "questions": {
                "value": "1. Could you please add standard error bars to all the plots / tables in the paper?\n\n2. Could you test out your method on something other than the three easiest Mujoco environments?\n\n3. Could you try the \"linear\" version of the loss I discuss in the weaknesses section? Maybe try tacking on a variance bonus?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes the use of techniques from the preference learning literature for offline imitation learning. The paper utilizes recent techniques from the offline preference-learning literature such as DPO/SPIN/CPO (where the policy's log probability of an action is treated as the reward of the action) and proposes to utilize the combination of a DPO-style loss and a BC loss to prevent overfitting. The authors show surprisingly good results, especially in the low-data regime, where POIL outperforms state-of-the-art model-free offline imitation learning algorithms including IQ-Learn, DMIL, and DICE variants."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The experimental section is very well ablated, with strong performance of the proposed method. I am quite surprised that POIL does well in the low-data regime, especially considering most offline preference learning algorithms are very data hungry in the LLM finetuning context (albeit with larger models come larger dataset necessities). The ablation studies of the proposed method were also pretty exhaustive, where the authors ablated over the impact of the preference temperature parameter and the BC loss they use in practice."
            },
            "weaknesses": {
                "value": "There is no explicit theoretical justification in this work, but this is minor to me. I think DPO and its variants have strong theory as is when it comes to solving the KL-regularized RL problem. I feel like with less than 1 expert trajectory (which some offline imitation learning methods look into, albeit with additional suboptimal data) the method fails, but maybe this is not necessary in real-world settings.\n\nI am curious as to whether an RLHF-centric approach to this problem can be good to compare to (e.g. in LLMs there is the PPO w/ RM vs. DPO debate), where one trains a reward model on policy data (low reward) vs expert data (high reward) as the agent trains. In some sense, I feel like this method is similar to DAC, which is outperformed by IQ-Learn anyway, so maybe this is unnecessary."
            },
            "questions": {
                "value": "No questions from my perspective for now."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce a novel algorithm for offline IL, inspired by research from the RLHF literature. The algorithm is quite simple, but demonstrates strong performance against a range of strong baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "I see this paper as strong on several fronts. It is clearly written and motivated. There are many references to related work. The method is well-explained. There is a solid set of experiments with good baselines and ablations; as far as I can tell, the experimental setup is sound as well.\n\nThe final algorithm presented is relatively simple, and I appreciate that the authors do not try to obfuscate this fact. And yet, its performance is apparently quite strong, which should be of significance to the IL community."
            },
            "weaknesses": {
                "value": "The only criticism that I can produce is that it would be nice to know that this method scales to tasks that are more realistic than the MuJoCo benchmark environments, but the authors have already performed a set of experiments that should be considered sound in this particular research topic (fundamental IL algorithms).\n\nHaving tried similar approaches in the past, I was surprised that this method worked as well as it did. But, as shown in the authors' ablations, the scaling factor \u03b2 is crucial, and in general should be significantly < 1. The authors mention \"A smaller \u03b2 value tends to smooth the preference function, which leads to more stable gradients and improved training dynamics\", but do not say/show more about this (most results only show the final return); it would be nice to have more exploration of this."
            },
            "questions": {
                "value": "- Considering parallels to the LLM literature, I believe that BC is equivalent to supervised fine-tuning (SFT)? An example is in RPO [1]. But it appears that Xu et al. (2024), already cited in the paper, also does this.\n- For readers outside of the offline IL literature, how can one determine how long to train for? Are trained policies simply evaluated on the environment, or is it possible to perform some sort of validation?\n- As mentioned in the weaknesses, an analysis of how the learned and expert policies change over time could be a good empirical investigation of \u03b2's influence on the optimisation process.\n\n[1] Liu, Z., Lu, M., Zhang, S., Liu, B., Guo, H., Yang, Y., ... & Wang, Z. (2024). Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer. arXiv preprint arXiv:2405.16436."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}