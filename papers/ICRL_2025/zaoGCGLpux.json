{
    "id": "zaoGCGLpux",
    "title": "RETHINK MAXIMUM STATE ENTROPY",
    "abstract": "In the absence of specific tasks or extrinsic reward signals, a key objective for an agent is the efficient exploration of its environment. A widely adopted strategy to achieve this is maximizing state entropy, which encourages the agent to uniformly explore the entire state space. Most existing approaches for maximum state entropy (MaxEnt) are rooted in two foundational approaches, which were proposed by Hazan and Liu \\& Abbeel, respectively. However, a unified perspective on these methods is lacking within the community.\n\nIn this paper, we analyze these two foundational approaches within a unified framework and demonstrate that both methods share the same reward function when employing the $k$NN density estimator. We also show that the $\\eta$-based policy sampling method proposed by Hazan is unnecessary and that the primary distinction between the two lies in the frequency with which the locally stationary reward function is updated.  Building on this analysis, we introduce MaxEnt-(V)eritas, which combines the most effective components of both methods: iteratively updating the reward function as defined by Liu \\& Abbeel, and training the agent until convergence before updating the reward functions, akin to the procedure used by Hazan. We prove that MaxEnt-V is an efficient $\\varepsilon$-optimal algorithm for maximizing state entropy, where the tolerance $\\varepsilon$ decreases as the number of iterations increases. Empirical validation in three Mujoco environments shows that MaxEnt-Veritas significantly outperforms the two MaxEnt frameworks in terms of both state coverage and state entropy maximization, with sound explanations for these results.",
    "keywords": [
        "Reinforcement Learning for Exploration",
        "Maximum Entropy",
        "Intrinsic Rewards"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "Simpliset Maximum State Entropy Ever.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zaoGCGLpux",
    "pdf_link": "https://openreview.net/pdf?id=zaoGCGLpux",
    "comments": [
        {
            "comment": {
                "value": "Crucially, we do not believe it is appropriate to reject a paper solely because its claims and methods are perceived as too simple or lacking complex modifications, regardless of its correctness. The Heliocentrism, for instance, should not be dismissed simply because it is conceptually simpler or only slightly modifies the center while removing epicycles from the Geocentrism.\n\n[1] Mirco Mutti, Lorenzo Pratissoli, and Marcello Restelli. Task-agnostic exploration via policy gradient of a non-parametric state entropy estimate.\n\n[2] Alfr\u00e9d R\u00e9nyi. On measures of entropy and information. 1961.\n\n[3] Principe, Jose C. Information theoretic learning: Renyi's entropy and kernel perspectives. Springer Science & Business Media, 2010."
            }
        },
        {
            "comment": {
                "value": "We appreciate your detailed response, but we humbly disagree that the contribution of this paper is limited. This paper is the first to bridge two of the most important tracks on maximum state entropy (MaxEnt), helping to clarify the disorganized literature within this community. This work help prevent further misinterpretations, such as those made by the reviewer, in fundamentally misreading previous works including MaxEnt-H, MaxEnt-LA, and MEPOL [1], which have led to conclusions based on factual errors. We elaborate on these points in detail as follows.\n\n**Weaknesses:**\n\nWe believe you misinterpret the relationship between MEPOL and MaxEnts discussed in this paper. MEPOL is a particularly misleading work for readers seeking to understand MaxEnt, as it claims to improve MaxEnt-H by introducing kNN. Works like MaxEnt-LA cite it, but in fact, these works only reference MEPOL in passing, acknowledging its early connection to kNN, without ever directly comparing it or discussing it in detail. This has led some to assume a strong connection, which is, in fact, incorrect. Let us now address MEPOL in more detail.\n\nFrom a motivational perspective, MEPOL seeks to maximize the entropy of the average state distribution $H\\left(\\frac{1}{T}\\sum_{t=1}^{T}d_t^{\\pi}\\right)$, defined over episodes (note the index $t$ within an episode), while MaxEnt aims to directly maximize the state distribution $H(d^{\\pi})$. Theoretically, MEPOL reduces the general MaxEnt problem to a finite-horizon setting, which requires a fixed hyperparameter (episode length) $T$ for each experiment. Consequently, MEPOL has to remove the default termination rules for safe learning in Mujoco to maintain a fixed horizon length, which leads to illegal poses. In contrast, MaxEnt methods allow for varying episode lengths in each run.\n\nMore specifically, given the definition $d_t^{\\pi}(s) = P(s_t = s | \\pi)$ and an episode $\\tau = \\{s_1, s_2, \\dots, s_T\\}$, MEPOL treats the $T$ different random variables in the episode, with a fixed hyperparameter $T$, and estimates the corresponding distributions $\\{d_1, d_2, \\dots, d_T\\}$. As a result, the agent is encouraged to explore novel states $s_t$ compared to the distribution $d_t$. Intuitively, MEPOL partly encourages exploration of novel episodes or trajectories, while MaxEnt focuses on exploring novel states. This is why we argue that MEPOL targets a trajectory-wise state entropy objective, placing it in a fundamentally different class.\n\nFrom an information-theoretic perspective, MEPOL establishes a projection from the state space to a space defined by the average probabilities within trajectories. This differs significantly from state entropy $H(d(s))$. The design of such projections generally results in fundamentally different information-theoretic metrics and properties. Further details on these information-theoretic definitions can be found in the [2, 3].\n\n\n\nFrom an implementation perspective, MEPOL fundamentally does not present the algorithmic ideas used in Liu et al. or even MaxEnt-H. The key component of MaxEnt-LA is a non-stationary intrinsic reward function, which can be integrated into any RL algorithm (such as Q-learning, DQN, SAC, etc.) by substituting or augmenting the extrinsic rewards, much like the Approximation Oracles in MaxEnt-H. In contrast, the core of MEPOL is an implicit expression of the gradient of the trajectory-wise state entropy defined by itself, constrained by a KL divergence, within the framework of TRPO. Therefore, there is no need to discuss $\\delta$, as we cannot identify any algorithmic relationship between MEPOL and the MaxEnt framework, despite their sharing a similar high-level intuition. Overall, it is more reasonable to consider MEPOL as another type without further discussion in the paper.\n\n\n\n\nCLARITY and QUALITY:\n1. We recommend that the reviewer set aside any prior misconceptions about MEPOL and read MaxEnt-H, MaxEnt-LA, and our paper in sequence. We believe some misunderstandings arise from the confusing notions in previous works. For instance, in MaxEnt-H, the symbols $t$ and $T$ are used both for training iterations and as indices in the rollout, while in MEPOL, they are used exclusively for the rollout. We have taken care to reorganize these elements for clarity.\n2. We will present them in a concise manner."
            }
        },
        {
            "summary": {
                "value": "This paper provides a stronger proof for learning a policy that achieves maximum state-entropy, by using a simpler and more adaptive policy-sampling strategy. It further claims to unify two popular approaches to maximum-entropy RL. Empirical results suggest that this improved sampling strategy leads to more exploratory and uniform policies."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors do a good job of explaining the shortcomings of the bounds provided by Hazan. The thorough description of why small \u03b7 leads to unrealistic T was very helpful for intuition. The proof result, if I understand it, is a significant improvement in that regard. I generally thought the mathematical writing in this paper was strong."
            },
            "weaknesses": {
                "value": "I understand this is largely a theoretical paper, but nevertheless the experimental section of this paper is not very well-described or thorough. That\u2019s the main reason for my \u201cweak reject\u201d \u2013 if the authors sufficiently address these concerns, I\u2019ll gladly raise it.\n\nFrom the plots, it appears that the experiments are run with one seed \u2013 if so, this isn\u2019t acceptable, but can be easily rectified.\n\nI\u2019m very confused how you are using the NGU bonus, which is history-dependent, while not using a memory-based (e.g. LSTM) policy. Can you explain? The NGU bonus as I understand it doesn\u2019t make much sense without recurrence. I also found it a bit unfair to criticize MaxEnt-LA for using a non-stationary reward when you use one here (of almost the same form, even).\n\nMany details are missing for experiment configuration as well. How much training data is gathered each epoch? How much training is done per epoch? Was there a concrete measurement for the \u201cepsilon-accurate stopping criteria\u201d in the empirical results, or was it based on the number of training steps (it's hard for me to understand what \"train to convergence\" means in the high-dimensional setting)? What does an \u201cepoch\u201d mean for MaxEnt-LA? What\u2019s the scale of the NGU bonus? Are the projection functions consistent across methods (the random projections for each method need to have the same parameterization for the comparison to be fair)?\n\nHow can there be a slight downwards tick for the blue line for Ant for \u201ctotal unique states visited during training\u201d?\n\nTo clarify, in the appendix when you describe projecting down the states to a 7-dimensional space, that\u2019s only for entropy calculation, correct? And not projecting down for control as well?\n\nI would also encourage the authors to choose a different name for their method. \u201cMaxEnt-Veritas\u201d seems to imply that the authors view their method as the \u201cone final and true MaxEnt\u201d method, while also implying that there was something false about prior work. The main difference between your method and Hazan is an improved policy-selection scheme, and so I don\u2019t think this method is any more \u201ctruthful\u201d than theirs, possibly just more efficient. And I\u2019m sure someone (maybe even you) will at some point improve upon this method as well. I similarly think the title is not as informative about the methodology as it could be."
            },
            "questions": {
                "value": "Largely the empirical questions above, which is what my review currently hinges on. In brief:\n\n* Can you run with more seeds?\n* Can you include much more information on experimental configuration in your paper? Especially, can you describe what an \"epoch\" means, and if epsilon-accurate stopping is used, how that works in practice? Can you clarify whether projection is only for entropy computation or control as well? I should in theory be able to recreate your experimental protocol from your paper, but at the moment that's the not the case.\n* As I understand, the NGU bonus as applied in the original paper (computed using previous states seen throughout a given trajectory) seems incorrect for a non-recurrent method. Would you share your thoughts on this? And can you comment on the non-stationarity introduced by NGU in relationship to the criticisms of MaxEnt-LA?\n* Can you explain the slight downward tick in the blue line, since that doesn't seem possible given the plot's description?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors study the two algorithms by Hazan (MaxEnt-H) and by Liu \\& Abbel (MaxEnt-LA), which both compute a mixture of policies to maximize the (discounted) state entropy of an agent evolving in an infinite-time MDP. A priori, the reward function they maximize is different, the weights of the mixture are different, and the number of policy updates are different before updating the intrinsic reward estimate. The authors nevertheless prove that (1) the intrinsic rewards both algorithms optimize are proportional to each other when computed using kNN, (2) the parameter $\\eta$ for computing the mixture in MaxEnt-H is unnecessary, and a uniform mixture as in MaxEnt-LA is sufficient (3) it is better to completely optimize the policy before updating the estimate of the intrinsic reward function, as advocated by Hazan in MaxEnt-H. Based on these three observations, the authors introduce a new algorithm (MaxEnt-V) by combining the methods from Hazan and Liu \\& Abbel. They remove the unnecessary steps from Hazan, and compute the mixture as in Liu \\& Abbel, but optimize the policy as in Hazan before updating the intrinsic reward estimate. The algorithm is $\\epsilon$-optimal, meaning that the agent is at most suboptimal by $\\epsilon$, and under some assumptions $\\epsilon$ decreases by $(B + \\beta \\ln T) / T$ where $T$ is the number of iterations and where $B$ and $\\beta$ are constant. In practice it performs at least as well as the algorithms from Hazan and from Liu \\& Abbel.\n\nWarning: I must apologize beforehand for not having checked the demonstrations in the appendix."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is well written and easy to follow.\n2. The problem addressed is interesting to the community. \n3. The algorithm the authors propose is well motivated, with theoretical guarantees, and tested on several problems, where it also outperforms the alternative methods."
            },
            "weaknesses": {
                "value": "1. I would appreciate if the authors could clarify if their algorithm is always to be favoured compared to that of Hazan and Liu \\& Abbel, or if there may be configurations in which their method would fail and where the others would not.\n2. Paragraph line 313 to line 317 is pretty unclear to me. Could the authors explicitly provide the order of $\\epsilon$ when $\\eta \\rightarrow 0$. Also, the bound of Hazan seems to be exponentially decreasing in T, wich a priori looks better than the bound of MaxEnt-V.\n3. In Figure 4, results are represented in term of epochs. To my understanding, an epoch is a step $t$ in algorithm 1. Then, as in MaxEnt-H and MaxEnt-V, and epoch fully optimizes the policy, I suppose it is also much longer in terms of wall-time, compared to MaxEnt-LA. How do the figures look like as a function of the wall-time?\n4. I do not agree with line 486, stating that the algorithms maximizing the action entropy lack the capability of exploring in absence of extrinsic reward. To me, without extrinsic rewards, these methods aim to explore the action space uniformly, which does not guarantee uniform state exploration. In opposition MaxEnt-H, MaxEnt-LA, and MaxEnt-V aim to explore the state space uniformly, which does not guarantee action space exploration. Both approaches have different intrinsic motivation (i.e., exploration objective), and it is possible to construct examples for which uniform action exploration outperforms uniform state exploration, and vice versa.\n5. In paragraph line 508, there is an important distinction to make between parametric methods. Some are explicitly based on the entropy. The intrinsic motivation is to maximize the entropy of some distribution, which is typically approximated with a neural density estimator. Other methods are based on the uncertainty of some model. The intrinsic motivation is to take actions for which a parametric model over states and/or rewards provides different outcomes compared to the MDP realization. The distinction is particularly important in the current work as the first class of algorithms optimizes the same objective as the method the authors presented (and may have been added in the experiments). I think in particular that the related work should include [1, 2, 3], and probably other, more recent, works.\n\n[1] Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., & Salakhutdinov, R. (2019). Efficient exploration via state marginal matching. arXiv preprint arXiv:1906.05274.\n\n[2] Guo, Z. D., Azar, M. G., Saade, A., Thakoor, S., Piot, B., Pires, B. A., ... & Munos, R. (2021). Geometric entropic exploration. arXiv preprint arXiv:2101.02055.\n\n[3] Islam, R., Ahmed, Z., & Precup, D. (2019). Marginalized state distribution entropy regularization in policy optimization. arXiv preprint arXiv:1912.05128."
            },
            "questions": {
                "value": "1. I cannot see the reward functions in the three algorithms that are stationary and those that are not. In particular the sentence line 183 confuses me.\n2. Line 195 to line 199, might their be a confusion between MaxEnt-V and MaxEnt-H?\n3. Line 488, I suppose the entropy function is concave and not convex."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper first presents two established approaches to tackle the maximum state entropy maximization problem in reinforcement learning. Then, they first show how the two algorithmic schemes fundamentally aim to solve the same formal problem and present limitations of both schemes. Crucially, they identify that the policy sampling schedule is sub-optimal for one, while the policy update strategy is sub-optimal for the other one. Towards bridging the two schemes in a unified manner and overcoming these issues they propose a new algorithm, provide a theoretical analysis of its finite-time sub-optimality, and provide experimental comparisons with the previously mentioned existing algorithms."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "CLARITY:\n- The paper presents a clear and detailed comparison between two existing RL algorithms for state entropy maximization. \n- The intentions of the paper are clearly specified from the abstract and followed along the paper in a clear and coherent manner.\n\nQUALITY:\n- After a brief check of the proof, the deviation from the original analysis of Hazan et al. of Theorem 1 (pag. 14 of the paper) seems correct and well explained.\n\nORIGINALITY:\n- Although maybe common in other areas, I am not aware of other works in this context leveraging the Euler-Mascheroni constant to analyze the telescoping sum as done in the proof of Theorem 1.\n\nSIGNIFICANCE:\n- The problem tackled within the paper, namely state entropy maximization, is a fundamental problem for RL as it tackles from first principles the issue of exploration in RL. As a consequence, investigation in this direction is highly relevant for RL and beyond.\n- Theorem 1 closes a gap between theory and practice in the choice of policy sampling schedule."
            },
            "weaknesses": {
                "value": "ORIGINALITY and SIGNIFICANCE:\nUnfortunately, the paper seems to suffer a quite fundamental issue in terms of originality and significance because of the following points holding together:\n1) The work fundamentally aims to 'build a unified perspective' on  the maximum state entropy problem by bridging two established schemes, namely the algorithms presented by Hazan et al. (MaxEnt, here renamed MaxEnt-H) and by Liu et al. (APT, here renamed MaxEnt-LA).\n2) Crucially, the work by Liu et al. cites (and seems to build on) work [1]. This work fundamentally already presents the algorithmic ideas used in Liu et al. (namely the non-parametric entropy estimate to scale to non-tabular domains) in the context of the maximum state entropy problem presented by Hazan et al. (Sec. 4 of [1]). Moreover, it proposes an algorithm, named MEPOL, that seems to be nearly analogous to the one proposed by the authors (MaxEnt-Veritas) as a subcase. In particular, by choosing a high value of $\\delta$ in MEPOL, it seems that the policy update scheme corresponds to the one in MaxEnt-Veritas (as in Hazan et al.), while using the policy sampling scheme as in Liu et al.\n3) The authors cite [1] both in the Introduction and Related Works section, where they claim that [1] 'focuses on maximizing trajectory-wise state entropy' which is a 'fundamentally different objective'. Although this is the case for later works of the same author that alike the mentioned work by Jain et al. optimize trajectory-wise state entropy, this does not seem to be the case in [1], where the notion of entropy in MEPOL is not trajectory-wise.\n\nAs a consequence, it seems to me that the authors aimed to bridge two works that were already deeply (historically and formally) connected by a misinterpreted well-established reference. Although the submitted paper brings a new theoretical result (Theorem 1) by a slight modification of the analysis in Hazan et al., this seems very limited in terms of contribution and novelty compared with what the authors claim (e.g., in the abstract), which was arguably already achieved in large part. \n\nCLARITY and QUALITY:\n- I believe that the first 5 pages of the paper can be significantly sharpened in their presentation, which currently seems loose and slightly hard to follow at points.\n- I would suggest to present Propositions 2 and 3 not as propositions as they seem simple calculations based on existing theorems and could be integrated within the text to improve the flow of the paper.\n\n\n[1] Mirco Mutti, Lorenzo Pratissoli, and Marcello Restelli. Task-agnostic exploration via policy gradient of a non-parametric state entropy estimate."
            },
            "questions": {
                "value": "Did I misinterpret something within points 1-3 above that renders the conclusion fundamentally wrong?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses maximum state entropy exploration in MDPs without rewards. First, it analyzes and compare previous approaches to state entropy maximization, namely a Frank-Wolfe algorithm (like MaxEnt, Hazan et al. 2019) and a policy optimization algorithm (like APT, Liu & Abbeel, 2021). The paper shows that the two approaches share the same reward function when the entropy is estimated via kNN. Then, the paper proposes a new algorithm, called MaxEnt-V, incorporating the best of both approaches. The method is demonstrated to lead to approximately optimal policies and empirically validated in a set of Mujoco environments."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper provides a unifying perspective on the literature of maximum state entropy;\n- The paper proposes an interesting implementation change for maximum state entropy algorithms, which prescribes to freeze the intrinsic reward for multiple policy optimization steps (like in a Frank-Wolfe routine);\n- The paper makes an effort to provide theoretical ground to the proposed method."
            },
            "weaknesses": {
                "value": "EVALUATION\n\nThis paper provides some fresh ideas, mostly that the previous approaches for maximum state entropy can be connected into a \"unifying\" algorithm and that the entropy intrinsic reward shall be frozen for multiple policy optimization steps, which bears some resemblance to the target network trick for deep Q-learning. Unfortunately, in my opinion the paper fails to provide convincing evidence that the frozen reward trick gives empirical benefit to justify widespread adoption. More broadly, I think the paper falls short of the technical quality required to be accepted at ICLR. I provide below a summary of what I believe are the main weaknesses of the paper and further comments.\n\nMAJOR WEAKNESSES\n\n1) Misunderstandings of the literature\n   1. The paper presents MaxEnt-H and MaxEnt-LA as two competing approaches for maximum state entropy. Whereas different design choices can be extracted on an abstract level, the authors seem to misunderstand the purpose of the two papers. Hazan et al. were the first to introduce the problem of maximum state entropy in MDPs. They show the latter problem is not hopeless, despite being non-convex in the policy parameters, by providing a provably efficient algorithm. While an implementation and experiments are provided, the algorithm is mostly a theoretical and analytical tool. Instead, Liu & Abbeel's paper falls into a stream of practical methods for maximum state entropy, where the main advancements were given by the use of kNN entropy estimators in place of state densities (this technique has been introduced by Mutti et al., 2021 not by Liu & Abbeel as stated in the manuscript) and learning representations.\n   2. The paper suggests that MaxEnt-LA trains a mixture with uniform parameters. I am not sure this is the case. From my understanding, it does sample uniformly from a replay buffer, which may include transitions coming from previous policies, but it does that only to perform updates on the *last* policy. Indeed, the output is the last policy network, which is then fine-tuned with external reward, and not a mixture. There seems to be an important gap here.\n   3. The paper says that another stream of work (Mutti et al 2021, Jain et al 2024) optimize the trajectory-wise entropy. Perhaps this distinction shall be clarified. From my understanding, the objective of Mutti et al 2021 is the same of Eq. 2, although the entropy reward is not decomposed in state terms as in Liu & Abbeel 2021. \n\n2) Some claims look subjective and lack strong support\n   1. \"Tuning $\\eta$ is unnecessary in MaxEnt-H\". Since the purpose of tuning $\\eta$ comes from the analysis in Hazan et al., 2019, the paper should show that similar convergence guarantees  can be obtained with a uniform mixture to support this claim. Showing that $\\eta$ is always small and tuning is unnecessary in practice is not enough.\n   2. \"Freezing the reward is better\". Theorem 1 only provides an upper bound on the sub-optimality. It is rather weak to say that freezing the reward is better because it leads to a smaller upper bound. Maybe the upper bounds are just not tight, and the analysis would say nothing about which one (freezing the reward or changing it at any step) is better.\n   3. Theorem 1. I have some concerns on the validity of this result. First, the statement assumes that $H_{kNN}$ is smooth and bounded, which does not seem to be the case by staring at Eq. 6. Can the authors provide more details on when those conditions are met? Moreover, what does it mean to assume access to estimation oracle (as in lien 310)? Note that the kNN entropy estimators are biased, does that mean $\\epsilon_0$ error on the biased estimate or the true entropy?\n\n3) The empirical analysis looks very far from the standards of the community\n   - Are the curves in the figures reported on a single run? Some details seem to be missing on how the experiments are conducted to meet some statistical significance (e.g., look at https://ojs.aaai.org/index.php/AAAI/article/view/11694 and https://proceedings.neurips.cc/paper_files/paper/2021/hash/f514cec81cb148559cf475e7426eed5e-Abstract.html)\n   - Previous papers, especially Hazan et al. 2019 and Liu & Abbeel 2021 have public implementations. To claim MaxEnt-V gives benefits over them, it would be better to compare its performance with the official implementations of MaxEnt-H and MaxEnt-LA.\n\nOTHER COMMENTS\n\nThe literature of maximum state entropy could be presented better, especially considering that this manuscript builds over them. The first papers on this problem have been Hazan et al 2019; Lee et al 2019, Mutti & Restelli 2020), which presented algorithms requiring state density estimation, which is mostly impractical in high dimensions. To overcome this issue, Mutti et al 2021 proposed to use kNN entropy estimators (Singh et al 2003 and others) to guide policy optimization without explicit state density estimation. Mutti et al 2021 compute kNN distances on the state features, which is not suited for images. Liu & Abbeel 2021 coupled kNN estimators with contrastive representations (together with various other implementation changes, such as state-based rewards, actor-critic architecture with replay buffers). Other representations have been proposed by subsequent works, such as Seo et al 2021, Yarats et al 2021. Several other works followed on both practical methodologies and theoretical analysis of maximum state entropy. Some relevant references that does not seem to be mentioned:\n- Lee et al., Efficient exploration via state marginal matching, 2019; \n- Mutti & Restelli, An intrinsically-motivated approach for learning highly exploring and fast mixing policies, 2020; \n- Guo et al., Geometric entropic exploration, 2021;\n- Liu & Abbeel, Aps: Active pretraining with successor features, 2021;\n- Mutti et al., Unsupervised reinforcement learning in multiple environments, 2022;\n- Mutti et al., The importance of non-Markovianity in maximum state entropy exploration, 2022;\n- Mutti, Unsupervised reinforcement learning via state entropy maximization, 2023; \n- Yang & Spaan, CEM: Constrained entropy maximization for task-agnostic safe exploration, 2023; \n- Zisselman et al., Explore to generalize in zero-shot rl, 2023;\n- Zamboni et al., How to explore with belief: State entropy maximization in pomdps, 2024;\n- Zamboni et al., The limits of pure exploration in POMDPs: When the observation entropy is enough, 2024.\n\nDespite my concerns expressed above, I think the idea of the frozen rewards to improve maximum state entropy approaches is very interesting and worth studying. Perhaps the authors could think of restructuring the paper and their analysis to focus on the empirical benefit that this trick may provide and the (stability) issues that may arise from chasing the non-stationary reward.\n\nMINOR\n- Perhaps clarify the meaning of MaxEnt-H and MaxEnt-LA earlier in the text\n- l.48 \"While these importance sampling-based methods\" -> what does that mean?\n- Eq. 2 min -> max\n- Algorithm 2, line 2: how many states are sampled?"
            },
            "questions": {
                "value": "I mostly do not have direct questions. Some are reported in the comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}