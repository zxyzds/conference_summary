{
    "id": "ULGbw2URE3",
    "title": "L3Ms \u2014 Lagrange Large Language Models",
    "abstract": "Supervised fine-tuning (SFT) and alignment of large language models (LLMs) are key steps in providing a good user experience. However, the concept of an appropriate alignment is inherently application-dependent, and current methods often rely on heuristic choices to drive the optimization. In this work, we formulate SFT and alignment as a constrained optimization problem, where the LLM is trained on a task while being required to meet application-specific requirements, without resorting to heuristics. To solve this, we propose Lagrange large language models (L3Ms), which employ logarithmic barriers to enforce the constraints. This approach allows for the customization of L3Ms across diverse applications while avoiding heuristic-driven processes. We demonstrate experimentally the versatility and efficacy of L3Ms in achieving tailored alignments for various applications.",
    "keywords": [
        "LLM",
        "alignment"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ULGbw2URE3",
    "pdf_link": "https://openreview.net/pdf?id=ULGbw2URE3",
    "comments": [
        {
            "summary": {
                "value": "This paper presents Lagrange large language models (L3Ms), which formulate supervised fine-tuning and alignment of large language models as a constrained optimization problem. L3Ms use logarithmic barriers to enforce constraints, allowing for customization across diverse applications without relying on heuristics. The paper demonstrates the versatility and efficacy of L3Ms in achieving tailored alignments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The formulation of SFT and alignment as a constrained optimization problem and the use of logarithmic barriers in L3Ms is a novel approach that has the potential to improve the customization of LLMs for different applications.\n\nThe paper provides a solid theoretical foundation for the proposed method, including discussions on constraint types, Lagrange multipliers, and the connection to the KKT conditions.\n\nThe experimental results on length-constrained L3Ms and helpful and harmless (HH) L3Ms show that the proposed method can effectively customize LLMs according to different constraints while maintaining performance on the original task."
            },
            "weaknesses": {
                "value": "The experiments mainly consider length constraints and helpful and harmless preferences. It would be interesting to see how the proposed method performs on a more diverse set of tasks, such as computational and reasoning tasks, and whether it can optimize for task-related metrics specific to those tasks.\n\nThe paper primarily focuses on perplexity as an evaluation metric. It would be beneficial to include other metrics to provide a more comprehensive assessment of the performance of L3Ms.Are there any plans to incorporate other evaluation metrics in addition to perplexity to better evaluate the performance of L3Ms? If so, which metrics are being considered and why?\nWill the authors explore the application of L3Ms to a wider range of tasks, such as computational and reasoning tasks? How would the method be adapted to optimize for task-related metrics specific to these tasks?"
            },
            "questions": {
                "value": "-Are there any plans to incorporate other evaluation metrics in addition to perplexity to better evaluate the performance of L3Ms?\n\n-Will the authors explore the application of L3Ms to a wider range of tasks, such as computational and reasoning tasks? How would the method be adapted to optimize for task-related metrics specific to these tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel approach to the supervised fine-tuning (SFT) and alignment of large language models (LLMs), formulating it as a constrained optimization problem. The authors propose Lagrange large language models (L3Ms), which use logarithmic barriers to enforce application-specific requirements, eliminating the need for heuristic-driven processes. This approach allows for the customization of L3Ms across diverse applications. The paper demonstrates the versatility and efficacy of L3Ms through experimental results, showing their ability to achieve tailored alignments for various applications. This work contributes a new perspective and methodology to the fine-tuning and alignment of LLMs, enhancing their adaptability and effectiveness across different applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper innovatively formulates SFT and alignment as a constrained optimization problem, offering a fresh perspective that avoids traditional heuristic methods.\n2. L3Ms, with their logarithmic barriers, demonstrate technical excellence and adaptability across diverse applications, ensuring robust performance.\n3. By providing a systematic approach to fine-tuning LLMs, this research addresses a critical need and offers practical benefits for both academic and industrial use."
            },
            "weaknesses": {
                "value": "1. The paper's experimental section may lack comprehensive evaluations, which could limit the full demonstration of L3Ms' capabilities and robustness across a wide range of scenarios.\n2. The choice of datasets used in the experiments appears to be restricted, potentially affecting the generalizability of the findings and the ability to assess the method's effectiveness on varied data types.\n3. Utilizing LLMs with only 7 billion parameters might restrict the scalability and performance of L3Ms, as larger models are often necessary for handling complex tasks and achieving state-of-the-art results.\n4. Writing Issues: The paper has some writing issues, including incorrect formula numbering, which can cause confusion and disrupt the flow of the paper. Improving the clarity and accuracy of the writing would enhance the overall quality and readability of the paper."
            },
            "questions": {
                "value": "See the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces a new method for fine-tuning and aligning large language models (LLMs) by treating these processes as a constrained optimization problem. Instead of relying on heuristic methods, the authors propose using Lagrange Large Language Models (L3Ms) that incorporate constraints directly into the optimization through logarithmic barriers. This approach allows models to maintain performance on primary tasks while being customized to meet specific user or application-related preferences. The paper provides experimental evidence showing that L3Ms are effective."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Clear problem statement. This work discusses the shortcomings of previous works on constrained RLHF and proposes corresponding improvements to solve it, which seems convincing to me.\n- Good optimization problem conversion. I find it quite interesting and instructive how this work converts a general target of balancing task performance and multiple specific requirements into a solvable object function of Lagrangian and introduces the necessary components of the logarithmic barrier to make it practically work."
            },
            "weaknesses": {
                "value": "- My main concern is with the experimental results in Figure 3, where MM appears to achieve a better performance frontier than L3M. Although L3M results in lower perplexity (PPL), it's unclear how significantly this affects real task performance, as the improvement seems marginal. Could the authors provide further discussion on this?\n- The constraints used in the experiments are somewhat limited in terms of variety. It's unclear how the proposed algorithms would perform in scenarios with more complex or demanding constraints.\n- The experimental setups seem relatively simple, which may not fully capture the real-world performance of the fine-tuned LLMs."
            },
            "questions": {
                "value": "- Can authors provided more discussions on computation overheads introduced in this work in comparison to the general SFT and other related works (https://openreview.net/forum?id=TyFrPOKYXw, https://openreview.net/forum?id=gkfUvn0fLU)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper suggests to model SFT and alignment into one constrained optimization problem and proposes a method called L3M, which is to optimize such constrained optimization problems via approximate Lagrangian."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper suggests to formulate SFT and alignment into one constrained optimization problem and optimize its approximate Lagrangian."
            },
            "weaknesses": {
                "value": "- The core objective Eq.(10) $\\mathcal{G} _ \\mu(\\theta)=L(\\theta)+\\frac{1}{k}\\sum _ {i=1}^k\\mathcal{B} _ {\\mu,\\mu^2}(C _ i(\\theta))$ is equivalent to the lowerbound of multi-objective RLHF objective with induced reward models and a SFT term. Given a reward model $r _ i(x,y)$, a monotonically increasing function $f _ 1(x)=\\mathcal{B} _ {\\mu,\\mu^2}(x)$, and a monotonically decreasing function $f _ 2(x)=b _ i-x$, we can induce a new reward model $r _ i'(x,y)=-f _ 1(f _ 2(r(x,y)))=-\\mathcal{B} _ {\\mu,\\mu^2}(b _ i-r _ i(x,y))$. As $\\mathcal{B} _ {\\mu,\\mu^2}(x)$ is convex,\n$$\n\\begin{split}\n\\min &\\mathbb{E} _ {x,y}[-r _ i'(x,y)]\\\\\\\\\n=&\\mathbb{E} _ {x,y}[\\mathcal{B} _ {\\mu,\\mu^2}(b _ i-r(x,y))] \\\\\\\\\n\\geq&\\mathcal{B} _ {\\mu,\\mu^2}(\\mathbb{E} _ {x,y}[b _ i-r(x,y)])\\\\\\\\\n=&\\mathcal{B} _ {\\mu,\\mu^2}(C _ i(\\theta)),\n\\end{split}\n$$\nwhere the inequality holds by jensen\u2019s inequality. It can be extend into multi-objective scenario with a SFT term $L(\\theta)$:\n$$\n\\begin{split}\n\\min &L(\\theta)+\\mathbb{E} _ {x,y}\\left[\\frac{1}{k}\\sum _ {i=1}^k-r _ i'(x,y)\\right]\\\\\\\\\n=&L(\\theta)+\\frac{1}{k}\\sum _ {i=1}^k\\mathbb{E} _ {x,y}[-r _ i'(x,y)]\\\\\\\\\n\\geq&L(\\theta)+\\frac{1}{k}\\sum _ {i=1}^k\\mathcal{B} _ {\\mu,\\mu^2}(C _ i(\\theta))=\\mathcal{G} _ \\mu(\\theta),\n\\end{split}\n$$\nwhich is Eq.(10) of the paper.\n\n- For baselines, as Line 355-358 writed: \u201cL3Ms have the same time complexity as traditional approaches as we combine the SFT and alignment stages into one\u201d. Traditional SFT+alignment, having the same training budget with L3M, should be considered as a baseline.\n\n- For benchmarks, more instruction following tasks should be evaluated to demonstrate the generalization of L3M across diverse distribution.\n\n- For evaluations, 1) ppl (perplexity) can not directly reflect the instruction following ability of an aligned LM. 2) As reward hacking is a considerable problem in RLHF, the score of reward models which are involved into L3M training is not a valid metric. Other metrics should be considered to evaluate the alignment."
            },
            "questions": {
                "value": "- Why the log barrier parameter $\\mu$ should be decayed during training? (Lines 715-716) \n\n- As the log barrier parameter $\\mu$ is a hyper-parameter of L3M, what the relationship between its value and L3M performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}