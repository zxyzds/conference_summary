{
    "id": "CU8CNDw6Vv",
    "title": "Reassessing the Validity of Spurious Correlations Benchmarks",
    "abstract": "Neural networks can fail when the data contains spurious correlations. To understand this phenomenon, researchers have proposed numerous spurious correlations benchmarks upon which to evaluate mitigation methods. However, we observe that these benchmarks exhibit substantial disagreement, with the best methods on one benchmark performing poorly on another. We explore this disagreement, and examine benchmark validity by defining three desiderata that a benchmark should satisfy in order to meaningfully evaluate methods. Our results have implications for both benchmarks and mitigations: we find that certain benchmarks are not meaningful measures of method performance, and that several methods are not sufficiently robust for widespread use. We present a simple recipe for practitioners to choose methods using the _most similar_ benchmark to their given problem.",
    "keywords": [
        "benchmarking",
        "evaluation",
        "spurious correlations"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We systematically explore disagreement between spurious correlations benchmarks and examine their validity, finding that certain benchmarks are not meaningful evaluations of spurious correlations mitigation method performance.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=CU8CNDw6Vv",
    "pdf_link": "https://openreview.net/pdf?id=CU8CNDw6Vv",
    "comments": [
        {
            "comment": {
                "value": "Thank you for your considered and thoughtful review - we\u2019re pleased you think that our paper addresses a critical gap, and that it could lead to improved benchmark selection practices. We\u2019ve uploaded a new version addressing the typo you mention, and respond to your other comments below:\n\n### Re weaknesses\n* Thank you for highlighting these papers. We are happy to add extended discussion of new advancements in the field that are not covered by the methods we evaluate, and we will explore the feasibility of adding these methods into section 4.\n\n### Re question 2\n* You are correct that M_ERM was trained using Adam, but we don\u2019t see these as conflicting. The model was trained to minimize the mean loss over the training samples (i.e., the empirical risk) using gradient-based optimization with an adaptive per-parameter learning rate (i.e., Adam). Whether using SGD, Adam, or RMSProp, as long as the gradient is w.r.t. the mean loss over the training set, we\u2019d still consider this to be ERM. If you think this distinction needs further clarification, we\u2019d be happy to update accordingly.\n\n### Re question 3\n* Thank you for the wonderful question! The short answer is: yes, to a certain extent, we think model architecture should influence benchmark validity. To repeat the trivial example we use in the paper, if we were to choose a single-channel, grayscale, CNN architecture, then benchmarks where color is the spurious feature would be fairly useless.\n* More broadly, a core motivation in introducing our measure of task difficulty due to spurious correlation, K, is the idea that the strength of the spurious correlation is necessarily dependent on the model perceiving the correlation. That said, _in practice_, we imagine that many large image classification models would produce similar K for each benchmark\u2014as a result of similar training data, and only small differences in architecture\u2014and would be unlikely to substantially change our conclusions re benchmark suitability. \n* We chose the two model architectures considered, ResNet-50 and BERT base, as they remain the most frequent models considered in spurious correlations research, forming the typical basis upon which conclusions are drawn as to which methods perform best. As the research community continues to develop new models, repeatedly evaluating the utility of current benchmarks (particularly w.r.t. ERM Failure) is likely to be necessary. \n* We think this might make a nice addition to our discussion section, and thank you again for raising this question!"
            }
        },
        {},
        {
            "title": {
                "value": "Thank you for your feedback"
            },
            "comment": {
                "value": "Thank you for your detailed and constructive feedback, and we apologize for giving you the impression that our submission had been rushed. We hope that we can assure you that this was not the case. We\u2019ve just uploaded a new version addressing some of your feedback, including significantly larger figures. To respond to a few of your comments in turn: \n\n### Re general weakness 1\n* The benchmarks we considered currently only cover classification settings, but they are not limited to computer vision: two NLP benchmarks, CivilComments and MultiNLI, are already included in our analysis.\n* More broadly, however, we agree that spurious correlations are likely to continue to be a problem in more general settings and newer domains, including both text and image generation. The set of benchmarks we analyze is intended to cover the majority of present research into mitigating spurious correlations.\n* We\u2019d be happy to supplement our list if you have specific suggestions of existing benchmarks that we\u2019ve neglected.\n\n### Re general weakness 2\n* We discuss the reliance of our work, and all benchmarks within, on available (and high-quality) attributes on lines 521\u2013530, and we\u2019d be happy to expand this section if you think anything is missing.\n* We would like to stress that our \u201cpractical guide\u201d in section 5 is intended for practitioners faced with a choice between competing methods for mitigating spurious correlations. In reality, a researcher without access to a group-annotated test or audit set is unlikely to be considering this suite of methods in the first place.\n* While an evaluation of benchmarks without group annotations would be a great next step, we don\u2019t consider it within the scope of our contribution here. \n\n### Re general weakness 3\n* If you have any specific recommendations for what you\u2019d like to see, in terms of experiments, or to improve the presentation of section 5, we\u2019d be more than happy to revise accordingly.\n* To help clarify our results, we compare three approaches in section 5 and table 2. (1) Picking the best method according to performance averaged over all benchmarks. This produces GroupDRO. (2) Picking the best method according to performance averaged over only valid benchmarks, according to section 3. This produces ReSample. (3) Picking the best method _on the closest benchmark according to K_. This produces a different method per test dataset, and often improves performance compared with (1) and (2), as noted on lines 477\u2013481.\n* Table 2 currently covers 9 datasets, including Dollar Street to test the applicability of our approach to a held-out starting dataset, i.e., one not previously considered in our work. Do you have additional tests you\u2019d like to see here?\n\n### Re weakness 6\n* The caption for Figure 1 appears to be correct. In Figure 1, the legend on the color bar reads: \u201cRank (higher better)\u201d. DFR is the best performing method on Waterbirds (rank 19), while the second worst on NICO++ (rank 2).\n* We have now clarified this in an updated caption."
            }
        },
        {
            "summary": {
                "value": "The paper investigates the validity and consistency of benchmarks used for evaluating methods that mitigate spurious correlations in machine learning models. Recognizing that current benchmarks often produce conflicting results\u2014with certain methods performing well on one benchmark but poorly on others\u2014the authors aim to understand the root of these disagreements. They propose three key desiderata for a valid spurious correlation benchmark: ERM (Empirical Risk Minimization) Failure, Discriminative Power, and Convergent Validity. To assess a benchmark\u2019s validity, they introduce a model-dependent measure, the Bayes Factor (K), which quantifies task difficulty due to spurious correlation. Through an empirical study across multiple benchmarks, the paper identifies benchmarks that meet the proposed validity criteria and highlights methods that demonstrate robustness across varying benchmarks. Additionally, they offer practical recommendations for practitioners to choose benchmarks and methods tailored to their specific dataset characteristics, advocating for a systematic approach to benchmark selection in real-world applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- **Originality**: The paper presents a novel approach to evaluating spurious correlation benchmarks by proposing three validity criteria\u2014ERM Failure, Discriminative Power, and Convergent Validity.\n- **Quality**: The study is well-executed, with a thorough empirical analysis to assess the proposed validity criteria. The use of the Bayes Factor as a measure of task difficulty provides a quantifiable metric, helping to identify benchmark inconsistencies.\n- **Clarity**: Definitions of key concepts, such as the three validity criteria, are well-explained. The practical recommendations provide actionable insights for researchers and practitioners selecting benchmarks.\n- **Significance**: By focusing on the quality of benchmarks themselves, the paper addresses a critical gap in spurious correlation research. The findings could lead to improved benchmark selection practices, which are essential for evaluating and developing robust models across diverse domains."
            },
            "weaknesses": {
                "value": "The methods discussed in the paper currently omit some recent state-of-the-art algorithms and techniques in spurious correlation research published before July 1, 2024, which would strengthen both the related work and Section 4. For instance, \n- Wang et al. \"On the Effect of Key Factors in Spurious Correlation.\" AISTATS 2024.\n- Yang et al. \"Identifying Spurious Biases Early in Training through the Lens of Simplicity Bias.\" AISTATS 2024.\n- Lin et al. \"Spurious Feature Diversification Improves Out-of-distribution Generalization.\" ICLR 2024.\n- Deng et al. \"Robust Learning with Progressive Data Expansion Against Spurious Correlation.\" NeurIPS 2023.\n    \nIncluding these and potentially other relevant studies would make the paper more up-to-date. Even if not directly compared in Section 4, these works should at least be cited and discussed to reflect the current advancements in the field."
            },
            "questions": {
                "value": "1. There is an unresolved comment left in Line 242: \u201cCan we make x axis bigger? Too hard to read even zooming.\u201d This appears to have been unintentionally included in the submitted version and should be removed.\n2. In lines 312-313, the paper states that $M_{ERM}$ is trained using ERM, while in lines 866-867, it is mentioned that $M_{ERM}$ is trained using the Adam optimizer. Could the authors confirm which training method was used and clarify any potential discrepancies?\n3. Given the emphasis on benchmark selection, do the authors have insights into how the choice of model architecture might impact the validity of a benchmark? Are certain models more or less suitable for assessing spurious correlation benchmarks under the proposed criteria of ERM Failure, Discriminative Power, and Convergent Validity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper assesses the quality of spurious correlation benchmarks and methods. The paper first develops three criteria desired for spurious correlation benchmarks and checks whether these are satisfied by some commonly used benchmarks. They then check which methods perform well across different benchmarks, and develop a new recommendation for choosing which method to use for a given dataset and model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The results provide insights both about which benchmarks are good indicators of mitigating spurious correlations and which methods are robust across different benchmarks, which can be useful to a variety of practitioners."
            },
            "weaknesses": {
                "value": "1. Calculating K requires two full training runs (one with ERM, one with reweighting). This is extremely resource-intensive, and the empirical results do not seem to show a significant enough improvement to warrant such a cost.\n\n2. The variety of spurious correlation benchmarks is a problem that has been addressed in previous work (Joshi et al., 2023; Yang et al., 2023). A more detailed comparison of the observations in this work versus those in previous work would be appreciated.\n\n3. Some parts of the paper could be reorganized for clarity. A few specific points\n- unnecessary comments on line 242\n- lack of a dedicated related works section that puts the paper in the context of existing research (see previous comment)\n- the discussion section jumps between many topics that are only loosely related to each other and the main paper, making it hard to follow"
            },
            "questions": {
                "value": "I wonder how some datasets can have negative K (i.e. reweighting decreases performance)? This seems indicative of some confounding factors other than the identified spurious correlation, which may hinder the validity of the experimental results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper takes a critical look at a set of popular Spurious Correlation Benchmarks (SCBs), and shows that they often disagree with one another. The authors set out three desiderata that they think SCBs should exhibit based on the performance of different methods on the worse group accuracy. Specifically they claim a good SCB should exhibit a failure case of vanilla ERM, have strong Discriminative Power and Convergent Validity. The paper then evaluates how well these desiderata are satisfied by the set of SCB in question. The authors introduce a metric \u201cK\u201d to measure the difficulty of SCBs due to spurious correlations, After establishing a subset of the SCBs that satisfy the three desiderata, common domain generalisation approaches are assessed on this subset. Finally it is recommended practitioner also assess methods on these data sets or on data set similar in term of \u201cK\u201d to their data set of interest. In the Discussion section the authors discuss some weakness of their work and make some general recommendations for which SCB to use."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The papers main finding that Spurious Correlation Benchmarks (SCBs) often disagree with one another is interesting and definitely of interest.\n\nThe experiments performed are sound and presented in a clear and manner.\n\nThe prose of the paper are of good quality and in general it is easy to read and understand."
            },
            "weaknesses": {
                "value": "The biggest weakness of the paper is it is incorrectly titled, and the abstract is misleading. Spurious Correlations can be present outside of data sets with subpopulations shifts, however only subpopulation data sets and approaches have been considered in this work.  While the authors note this in the discussion section, I find this to still be insufficient. In its current state I think the work would be much better titled \u201cREASSESSING THE VALIDITY OF SUBPOPULATION SHIFT BENCHMARKS\u201d. With this title and a little rewriting to narrow the focus to these data sets and mitigation strategies I think the paper would be much better.\n\n\u201cSpurious Correlations\u201d or \u201cshortcuts\u201d are typically defined as decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios (Geirhos R et al. 2020). This phenomena only requires a distribution shift between test and train environments. The link between group performance and spurious correlations is critically missing from the paper. \n\nThis has the following issues:\n1)The assumption of having access to group information limits the usefulness of the desiderata to subpopulation shift benchmarks. \n2) How data set are grouped into subpopulation would likely have a large impact on these desiderata, how robust the desiderata are to the merging or splitting of groups has not been explored. I would suspect that the desiderata would be very sensitive hence more detail here seems necessary.\n3) There is no explanation of the different groups for the data sets in question, no detail on how the groups were selected. Or how to select useful groups when they have not be provided\n4) Many (possibly all) of the mitigation strategies require group labels. Many Spurious Correlation mitigation strategies that don\u2019t require group labels have not be considered, Feature Sieve, Deep feature reweighing, or the ensemble approach of (Teney et al 2022b) to name just a few.\n\nAll in all this paper just focuses on subpopulation shift benchmarks, hence the title and abstract and introduction should reflect that, and the effect of quality of the sub population labels should be explored.\n\nThe recipe for practitioners comparing mitigation methods on similar data sets in terms of K (Line 416-420), assumes access to data of the test domain to compute K. This requires the domain shift is known at train time.  This limits the usefulness of the approach as it assumes one has access to \u201cclean\u201d test data but insufficient to train on directly. \n\n**Typos:**\n\nLine 242: - author comment left in\n\nLine 140: ANother\n\nLine 102: correctionS - should be single\n\n**Refs**\n\nGeirhos R, Jacobsen JH, Michaelis C, Zemel R, Brendel W, Bethge M, Wichmann FA. Shortcut learning in deep neural networks. Nature Machine Intelligence. 2020 Nov;2(11):665-73.\n\nHermann KL, Mobahi H, Fel T, Mozer MC. On the foundations of shortcut learning. arXiv preprint arXiv:2310.16228. 2023 Oct 24.\n\nDamien Teney, Maxime Peyrard, and Ehsan Abbasnejad. Predicting is not understanding: Recognizing and addressing underspecification in machine learning. In European Conference on Computer Vision, pp. 458\u2013476. Springer, 2022b."
            },
            "questions": {
                "value": "**Questions**\n\nWhat are the groups for the data set you consider?\nHow would extend your desiderata to setting where you did not have group labels?\nHow sensitive are your desiderata to the merging of groups or splitting of groups?\n\n\n**Suggestions**\n\nThis paper focuses on subpopulation shift benchmarks, hence the title and abstract and introduction should reflect that, and the effect of quality of the sub population labels should be explored.\n\nIn its current state I think this work would be much better titled \u201cREASSESSING THE VALIDITY OF SUBPOPULATION SHIFT BENCHMARKS\u201d. With this title and a little rewriting to narrow the focus to these data sets and mitigation strategies I think the paper would be much better.\n\nThe 3.4.1 1. Is commonly referred to as \u201cpredictivity\u201d and 2, and 3 are know as the \u201cavailability\u201d Hermann et al. 2024 It\u2019s also not clear to me what the different between 2 and 3 are."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates the problem of spurious correlations and the fact that results are inconsistent across benchmarks. It demonstrates that the top-performing methods on one benchmark may perform poorly on another, revealing significant benchmark disagreements. In particular, the authors show that some methods while achieving best on some benchmarks they perform among the bottom 3 on other benchmarks. To address this, the authors propose three desiderata for a valid benchmark: ERM Failure, Discriminative Power, and Convergent Validity. Their analysis shows that many benchmarks and mitigation methods fail to meet these criteria, questioning their effectiveness. The paper also provides guidance for selecting appropriate benchmarks based on specific tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper's objectives and goals are clearly articulated.\n\n2. The problem addressed is a longstanding challenge in machine learning, as defining spurious correlations and constructing relevant attributes is difficult. This paper delves deeply into the reasoning behind these challenges and explores the properties that datasets should possess to qualify for evaluating spurious correlations."
            },
            "weaknesses": {
                "value": "The paper seems to have been rushed for the submission. There are several errors, mistakes, typos, in addition to a comment left out by the authors regarding one of their figures that reads \"Can we make x axis bigger? To hard to read even zooming\" in line 242.\n\nI will list below a non-exhaustive list:\n\n1. Line 242 \"Can we make x axis .. \".\n\n2. Stay consistent \"Figure\" vs \"fig\" vs \"Fig\". It should always be capitalised \"F\" but at least stay consistent on the abbreviation or not.\n\n3. Similarly to the above, also Appendix X, Figure Y, Table Z, Equation T all need to have the first letter capitalized.\n\n4. Lines 202, 204 are missing extra spaces @ \"Citybirdsshould\" and \"(AvP)has. There are a number of these\n\n5. Figures are poorly presented. Figure 1 for instance, is hard to read (particularly figure 2). Make it bigger or change the presentation. The text is too small.\n\n6. Caption of Figure 1 seems wrong. It reads \"best method on Waterbirds (DFR) is the second worst on NICO++\". DFR performs 19 (worst) on Waterbirds and second best on NICO++ according to Figure 2b.\n\n7. \"of its\" > \"to its\" @ line 92.\n\n8. Figure 4 is very poorly presented, xlabel, ylabels, and legends are all small.\n\n9. Lots of white space in Figure 5. You can make it better. and enlarge the plots.\n\n\nGeneral weakness:\n\n1. The paper focuses on image classification, which is to some extent an outdated setup and less exciting compared to newer domains.\n\n2. All datasets require attributes, which is less realistic in real-world scenarios. The paper aims to provide a practical guide for practitioners deploying their models, but it is unlikely to encounter a test dataset where the attributes are known a priori.\n\n3. The most interesting experiments are those presented in Table 2, as they provide evidence that filtering benchmarks based on the proposed desiderata helps in capturing and measuring spurious correlations. However, more experiments are needed. The details of this experiment section are poorly presented, and the rationale for selecting particular methods (GroupDRO and ReSample) is not adequately explained. It would also be valuable to investigate the same experiments using a different set of starting datasets to assess the impact of applying the desiderata on final performance."
            },
            "questions": {
                "value": "See above. I believe the paper is not yet ready for publication. The presentation is poor and the paper still needs to conduct a few experiments justifying the proposed desiderata in addition to better justify the experiments when attributes are actually required."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}