{
    "id": "J8LYjgi7nH",
    "title": "Free-MoE: Tuning-Free Mixture-of-Experts Purifying LLMs to Thrive across Any Field",
    "abstract": "The Mixture-of-Experts (MoE) framework efficiently scales large language models (LLMs) by selectively activating expert subnetworks, reducing computational costs. However, current MoE methods are costly in computation and include additional expert modules that require extra training data for tuning, leading to instability in the optimization process. To address these issues, we introduce Free-MoE, a tuning-free MoE method that leverages pre-trained LLMs' inherent ability to generalize across a wide range of tasks and domains. Free-MoE dynamically activates experts based on specific domains, achieves improvements while 1) requiring no extra model parameters and 2) being completely tuning-free. Specifically, we design the DOWP Alg., a Domain-Oriented Weight Purification Algorithm that purifies the weights in hidden layers and selects the optimal domain-specific experts of domain-specific experts in the hidden layers of the LLM to optimize activation decisions. The activated DSS-Experts, Domain-Specific Subnetwork Experts,can thereby concentrate on specialized task generation, outperforming the corresponding original model. Moreover, Free-MoE incorporates a multi-level trainable router that activates only the most relevant subnetworks during task, effectively minimizing unnecessary inference computations.  Comprehensive evaluations reveals that the DOWP Algorithm consistently achieves general performance gains of 2% to 3%, reaching up to 6.8% across datasets like MMLU, HumanEval, GSM8K, and etc. Additionally, when integrated into \\model~framework, our method demonstrates a cumulative improvement of 1.11% in average.  Findings indicate that Free-MoE not only enhances overall computational efficiency but improves the model\u2019s adaptability across any field that encompassed in contemporary language generation model benchmarks, and can be seamlessly applied to any transformer-based LLMs. Code for this project will be released in reachable future.",
    "keywords": [
        "Mixture of Experts",
        "Pretrained LLMs"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=J8LYjgi7nH",
    "pdf_link": "https://openreview.net/pdf?id=J8LYjgi7nH",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces Free-MoE to enhance the efficiency and adaptability of non-MoE LLMs across various tasks and domains without requiring additional tuning. Free-MOE propose a Domain-Oriented Weight Purification (DOWP) Algorithm to generate domain specific experts for each linear layer. Then Free-MoE propose a multi-level router to select optimal experts for each input instance during inference. This approach achieves performance improvements of 2% to 3% across multiple datasets without tuning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors conducted detailed experiments to verify the effectiveness of their algorithm and provided a comprehensive ablation study. The results indicate that Free-MoE can further improve model performance on specific tasks."
            },
            "weaknesses": {
                "value": "1. Lacking specific implementation details. Including: How many experts will Free-MoE produce in total? Does storing these experts require additional memory overhead, and if so, how large is the memory cost?\n\n  2. A more comprehensive evaluation of the approach is lacking. While the experts identified for one specific task may improve performance on that specific task, how much does this affect performance on other tasks? Is the performance drop on other tasks acceptable?\n\n  3. There is no hardware performance evaluation and analysis. It seems that each expert generated by Free-MoE has an unstructured sparse pattern. In this case, it is hard to accelerate the inference speed, and a special system design is needed to reduce the computation overhead. For each task, can you provide the inference speed and summarize the FLOPS results in the experiment part?"
            },
            "questions": {
                "value": "What is the difference between the proposed Free-MoE and unstructured weight pruning? I recommended the authors add the discussion in the related work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a tuning-free method to convert a non-MoE LLM into a MoE LLM to reduce computational overhead. Specifically, for each linear layer, Free-MoE uses a calibration dataset containing N tasks to generate M task-specific **sparse weight matrices** as experts based on the original **dense matrix**. The authors then train a multi-level router that first selects the set of task-specific experts for each input sample and subsequently identifies the most relevant experts within that set. Experiments show that on models smaller than 13B, Free-MoE can improve performance on specific tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. On more challenging datasets such as MMLU, GSM8K, and MathQA, Free-MoE demonstrates good algorithmic performance, showing that this method has the potential for further exploration."
            },
            "weaknesses": {
                "value": "1. The writing of this paper needs to be checked very carefully, especially the logic and terminology. For example:\n\n    - Logic: The logic of abstract is hard to follow. In the first sentence, the authors mentioned that the MoE canreduce the computation cost. However, in the second sentence, the authors mentioned MoE methods are costly in computation. The statements are quite conflict.\n\n    - Logic: Again in the abstract. In the second sentence, the authors mentioned that the current MoE methods have three issues require extra training data which can lead to instability in the optimization process. However, why more training data can lead to instability in training? It seems that it is not necessary to mention this issue, because the Free-MoE does not solve the instability issue.\n\n    - Terminology: In line 21, the authors define \"domain-specific experts\" and in line 23, the authors define \"Domain-Specific Subnetwork Experts\". Is there any difference between these two terms?\n\n2. The equations in this paper need detailed explaination. The equations are hard to understand now.\n\n    - For equation 1, how can we solve this estimation regarding the datasets and tasks?\n\n    - For equation 2, the embedding of each sentence is a matrix with different sizes. How to convert the embedding matrix into embedding vector?\n\n    - For equation 4, the size of weight matrix and the input activation are always different. Why can we use the same index to select numbers in both matrices? What does P_{i,j} stand for? \n\n3. Larger models are more likely to encounter excessive computational demands, yet the authors only conducted experiments on models below 13B, lacking tests on larger models like LLaMA3-70B and Qwen2-72B.\n\n4. Some tiny writing issues:\n\n    - In line 21, \"selects the optimal domain-specific experts of domain-specific experts in the hidden layers\", it seems that the authors repeat the \"domain-specific experts\"."
            },
            "questions": {
                "value": "1. How many experts will be generated by the proposed method for each task?\n\n  2. What is the density of each generated expert on different tasks? Are there any new findings? For example, do math datasets need more dense weight than other datasets, such as law datasets in MMLU?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Free-MoE, a tuning-free mixture-of-experts (MoE) method that harnesses the inherent generalization abilities of pre-trained large language models (LLMs) to adapt across diverse tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(1) The method seems to be interesting and novel, as it effectively leverages the inherent MoE structure from pre-trained dense LLMs. This approach intuitively simplifies MoE fine-tuning and training while stabilizing the optimization process.   \n\n(2) Extensive experiments are conducted across various models, demonstrating the method\u2019s effectiveness and scalability."
            },
            "weaknesses": {
                "value": "(1) While the motivation and main idea are clear, the implementation details in Algorithm 1 are somewhat confusing:  \n      (a) What does Perf mean? It's unclear whether Perf is a specific performance metric or an abstract indicator for evaluating expert effectiveness.   \n     (b) How is the weight matrix scaled by alpha? Is this scaling achieved through random search, or are heuristic methods applied to determine alpha?    \n    (c) What is the meaning of Patch? The reference to an element in the reduced matrix as a patch adds to the confusion\u2014could this term imply a specific structure or operation within the matrix?    \n(2) The paper doesn\u2019t explicitly discuss efficiency drawbacks. From my understanding, the method seems to rely on maintaining indices to store expert-related information across coarse to fine-grained features, potentially causing efficiency degradation during inference.   \n(3) It's not immediately clear why performance would improve with only DOWP, as fewer parameters are utilized during inference, which would typically suggest a decrease rather than an improvement in performance."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This article introduces a method to reduce inference costs and improve large language model (LLM) performance by activating only a subset of its parameters based on task-specific data. The method first classifies the input embedding vector into a sub-domain, then refines the relevant parameters to create a domain-specific subnetwork using the Domain-Oriented Weight Purification (DOWP) algorithm. This tailored subnetwork processes the input embedding vector to generate the final output."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Enhance LLM performance on specific tasks while simultaneously reducing inference costs.\n- Introduce a new LLM architecture that leverages the Mixture-of-Experts concept."
            },
            "weaknesses": {
                "value": "- The clarity of the proposed method's description could be improved; please refer to the questions section for details.\n- The claims lack accuracy. For instance, the authors describe the method as \"tuning-free,\" yet it still requires \"training\" for the router.\n- The notations are inconsistent in Sections 3.1 and 3.2:\n  - The interchangeable use of \"domain\" and \"dataset\" in Section 3.1 causes confusion, making it unclear whether $n$ refers to the number of datasets or domains. Additionally, it is unclear whether $D_A$ in the third paragraph of Section 3.1 refers to a domain or a dataset. Furthermore, the meaning behind $S$ and $D_{j_i}$ remains undefined.\n  - The definition of $Y$ presented in Line 250 is missing.\n  - The notation $k$ has multiple meanings in the paper. it denotes the number of patches in Line 265 but represents the number of clusters in previous paragraphs. Furthermore, in Algorithm 1, $k$ appears in `kmeans_clustering` without prior definition.\n  - In Section 3.1, it is unclear if \"feature matrix $X$\" refers to the activation embedding vector or the self-attention head.\n  - In Section 3.2, $X$ represents input tokens, while in the previous section, $X$ is used to denote the feature matrix.\n-  Several typos appear in the paper:\n   - In the first paragraph of page 5, \u201cthe Euclidean distance between $F_{\\mathcal{T}}$\u201d not $F_T$.\n   - In the 3rd paragraph of page 5, \u201cIn passing, **the** scaling\u201d, not \u201cThe\u201d.\n   - In the 5th paragraph of page 7, \u201cThese purified **experts** **are** dynamically activated\u201d, not \u201cexpert in\u201d.\n   - In the 5th paragraph of page 7, \u201cWhole inference procedure is shown in **Figure 3**.\u201d It should not be \u201c2\u201d based on the context."
            },
            "questions": {
                "value": "- In the abstract and introduction sections, the authors report performance gains of 2% to 3% and 1.11%. Could they clarify which model these improvements are based on?\n- Could the authors explain which MoE addresses communication overhead in distributed systems in Lines 76-78? I find it difficult to see the connection between the first part of this paragraph, which discusses various expert foundation models.\n- Could the authors explain the difference between \u201cmain knowledge domain\u201d and \u201cdomain\u201d mentioned in Lines 192-195?\n- Since the authors claim that Free-MoE enhances overall computational efficiency, could the authors provide an inference time comparison?\n- Could authors clarify the difference between DOWP and Free-MoE?\n- Could the authors elaborate on the rationale behind classifying a domain into sub-domains? The reviewer thinks the motivation is unclear.\n- Please address the following questions regarding Fig. 2:\n    - In Step 1, why is there an \u201cXOR\u201d operator before the hidden layer?\n    - In Step 1, what does \u201csort by row\u201d mean? Is it relevant to the sorting process of importance score $\\mathcal{M}_{ij}$?\n    - In Step 2, why is there a dashed arrow from the hidden layer to the output? It seems contradictory to Fig. 3, where the output comes from the path through DSS-Expert only.\n- The pseudo-code and figure expression seem inconsistent. In Step 2 of Algorithm 1, it seems that the patches with importance scores greater than $\\theta$ are taken. However, Fig. 2 does not align with this algorithm. Could the authors explain this clearly?\n- The details regarding the K-means algorithm are missing:\n  - Did the authors use one token or one batch for each domain in their experiments?\n  - Could the authors explain the initialization strategy used in the K-means algorithm?\n- In inference mode, when forming the DSS expert, the authors mention in Fig. 3 that the DSS experts are dynamically formed. Could the authors clarify whether this requires iteratively finding the best threshold for each token or for each batch?\n- After performing K-means clustering and finding each sub-domain centroid, should this centroid be cached and reused when inferencing?\n- The author only conducts experiments on the datasets that are already used to train routers and perform K-means clustering. Can this method be applied to other datasets that also fall in the primary domain (e.g., \u201cgeneral\u201d, \u201ccode\u201d, \u201cmath\u201d)?\n- What is the relationship between this work and \u201cInterpretability of LLMs Mechanism\u201d mentioned in Section 2.2?\n\n**Writing Suggestions**\n- The reviewer suggests changing the term \u201cdomain-specific experts\u201d in the abstract to \u201cthe optimal experts among domain-specific experts\u201d for enhanced clarity.\n- The reviewer suggests removing redundant sentences:\n  - In Lines 241-243, the two sentences \u201cthe task T will be assigned to this subdomain\u201d and \u201cSubsequently, task will be further distributed based on the characteristics of the subdomain\u201d seem to have the same meaning. \n  - In Line 267, \u201dThis sorting process helps identify the least important patches for purification.\u201d seems redundant."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}