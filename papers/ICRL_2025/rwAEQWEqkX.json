{
    "id": "rwAEQWEqkX",
    "title": "Contrastive Localized Language-Image Pre-Training",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect image inputs for language interactions. The success of CLIP as a vision-language foundation model relies on aligning web-crawled noisy text annotations at image levels. Nevertheless, such criteria may become insufficient for downstream tasks in need of fine-grained vision representations, especially when region-level understanding is demanding for MLLMs. In this paper, we improve the localization capability of CLIP with several advances. We propose a pre-training method called Contrastive Localized Language-Image Pre-training (CLOC) by complementing CLIP with region-text contrastive loss and modules. We formulate a new concept, promptable embeddings, of which the encoder produces image embeddings easy to transform into region representations given spatial hints. To support large-scale pre-training, we design a visually-enriched and spatially-localized captioning framework to effectively generate region-text pseudo-labels at scale. By scaling up to billions of annotated images, CLOC enables high-quality regional embeddings for image region recognition and retrieval tasks, and can be a drop-in replacement of CLIP to enhance MLLMs, especially on referring and grounding tasks.",
    "keywords": [
        "pre-training",
        "CLIP",
        "multimodal"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=rwAEQWEqkX",
    "pdf_link": "https://openreview.net/pdf?id=rwAEQWEqkX",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses a critical limitation of CLIP by introducing CLOC (Contrastive Localized Language-Image Pre-training), which enhances CLIP's capability to understand region-level semantics while maintaining its strong image-level performance. The key contributions include: (1) a novel concept of \"promptable embeddings\" that allows easy transformation of image embeddings into region representations, (2) a lightweight Prompter module that effectively extracts region features without compromising CLIP's original capabilities, and (3) a scalable pseudo-labeling pipeline (VESL) that generates high-quality region-text annotations. The approach is validated through extensive experiments across 31 evaluation tasks, showing particular effectiveness in MLLM applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Strong practical value:\n    - Addresses a real limitation in CLIP that affects its use in MLLMs\n    - Provides a drop-in replacement solution without compromising original capabilities\n    - Demonstrates consistent improvements across various downstream tasks\n2. Technical novelty:\n    - Introduces an elegant concept of \"promptable embeddings\"\n    - Develops a lightweight and effective Prompter module (though it reminds me of prompt encoder of SAM[^1])\n    - Proposes a scalable approach to generate region-text annotations\n3. Comprehensive evaluation:\n    - Extensive experiments across 31 tasks\n    - Thorough ablation studies\n    - Strong performance improvements in MLLM applications\n\n[^1]: Kirillov, Alexander, et al. \"Segment anything.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
            },
            "weaknesses": {
                "value": "1. Insufficient comparison with related works:\n    - Missing comparison with DenseCLIP[^1] and similar region-aware CLIP variants [^2, ^3, ^4, ^5]\n    - While RegionCLIP[^6] is mentioned, the differentiation from other similar approaches is not clearly articulated\n    - Limited discussion of trade-offs compared to existing solutions\n2. Missing implementation details:\n    - The region sampling strategy (how 4 regions per image are selected) is not specified\n    - The rationale for setting \u03bb as the ratio of images with region labels in the mini-batch is not explained\n    - The impact of batch composition (ratio of images with/without region labels) on training is not discussed\n    - The threshold (0.9) for similar text filtering lacks justification\n    - The definition and extraction method of \"leaf entities\" in NER is unclear\n    - Computational overhead analysis of the Prompter compared to RoI pooling is missing\n3. Limited analysis of scalability:\n    - The saturation of performance at 300M images for region tasks needs more investigation\n    - The impact of the threshold for similar text filtering on training time is not analyzed (in addition to (10) in table 2)\n\n[^1]: Rao, Yongming, et al. \"Denseclip: Language-guided dense prediction with context-aware prompting.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.  \n[^2]: Gu, Xiuye, et al. \"Open-vocabulary object detection via vision and language knowledge distillation.\" arXiv preprint arXiv:2104.13921 (2021).  \n[^3]: Zhou, Xingyi, et al. \"Detecting twenty-thousand classes using image-level supervision.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.  \n[^4]: Feng, Chengjian, et al. \"Promptdet: Towards open-vocabulary detection using uncurated images.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.  \n[^5]: Lin, Jiayi, and Shaogang Gong. \"Gridclip: One-stage object detection by grid-level clip representation learning.\" arXiv preprint arXiv:2303.09252 (2023).  \n[^6]: Zhong, Yiwu, et al. \"Regionclip: Region-based language-image pretraining.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022."
            },
            "questions": {
                "value": "1. Region Sampling Strategy\n    - How are the 4 regions per image selected during training?\n    - Is there any consideration for overlap between regions?\n    - How does the sampling strategy affect the model's performance?\n2. Training Implementation\n    - What is the rationale behind setting \u03bb as the ratio of images with region labels?\n    - How do you handle the batch composition in terms of images with/without region labels?\n    - Have you experimented with different similar text filtering thresholds besides 0.9?\n3. Scalability and Efficiency\n    - Why does the performance saturate at 300M images for region tasks but continue to improve for MLLM tasks?\n    - How does the computational cost of Prompter compare to traditional RoI pooling approaches?\n    - What is the memory overhead of storing region embeddings compared to traditional CLIP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Contrastive image-text pretraining has achieved significant success in recent years; however, low-quality annotations and the limitations of contrastive learning alone remain challenges. This work aims to enhance CLIP by incorporating fine-grained annotations, enabling the model to scale to billions of data points and demonstrating substantial improvements over the CLIP baseline.\n\nCLOC can be viewed as a practical approach to enhancing dataset quality, with a particular focus on fine-grained grounding details."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The data refinement process is scalable to billions of samples, demonstrating its practical utility in vision-language pretraining.\n\n2. The approach is intuitive and straightforward, leading to improved grounding capabilities.\n\n3. The architecture is easily adaptable for downstream grounding tasks, enhancing versatility.\n\n4. This method effectively handles large-scale datasets (billions-level in this work), with comprehensive experiments conducted despite significant computational demands.\n\n5. In addition to widely used recaption, this work also include region phrase candidates extraction and open-vocabulary detection with extracted phrases."
            },
            "weaknesses": {
                "value": "1. Visually-enriched and spatially-localized models, such as QWEN, rely on robust training to achieve high-quality outputs. These models contribute essential knowledge to the process, with the quality of grounding pseudo-labels also largely dependent on these off-the-shelf models.\n\n2. Significant work has already been done to improve dataset captions, as seen in studies like *Improving Multimodal Datasets with Image Captioning* by Thao et al. (NeurIPS 2023). Integrating object grounding information into vision-language pretraining is also an established concept, exemplified by X-VLM (Zeng et al., ICML 2021). Although CLOC\u2019s approach to grounding differs somewhat from X-VLM, these differences are minor and do not detract from the overall contribution."
            },
            "questions": {
                "value": "1. What explains the significant improvement that CLOC achieves over LLaVAW and MME-C? In contrast, GQA, which heavily relies on precise object grounding, shows only limited improvement."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper enhances the region-level representation in CLIP and introduces a new concept termed \"promptable embeddings.\" The region-level representation is a critical task in this domain. The experimental results appear promising."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper addresses a significant challenge: enhancing the region-level representation of CLIP, which is crucial for various applications, including multi-modal large language models (MLLMs).\n2. The experimental results presented in this paper demonstrate promising performance across both image-level and region-level tasks."
            },
            "weaknesses": {
                "value": "Several related works are not adequately addressed in the manuscript, such as 'UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding' (ECCV 2024)."
            },
            "questions": {
                "value": "Will the collected fine-grained region-text annotations be made publicly available? This aspect is highlighted as one of the main contributions of the study."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes to combine ITC with region text prediction. The authors show that a data labelling pipeline at scale for region proposal combined with a image-text dataset leads to model outperforming CLIP baseline for MMLLM among other tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The author's model is well engineered to take latency into context while adding additional blocks.\nThe data engine uses synthetic captions rather than alt-text to generate data for the box predictor which is lesser explored.\nThe authors show improvement in multiple tasks over CLIP."
            },
            "weaknesses": {
                "value": "This paper would have been strong if other works did not explore better pretraining over CLIP for open vocabulary computer vision tasks. However, that is not true anymore. This work ignores all the advancement in the field of pretraining image-text representation models.\nSeveral works in combining SSL with ITC exist and show better localization performance e.g. MaskCLIP and SILC. Moreover, BRAVE at ECCV2024 showed that these works also improve MMLLM performance. \nMoreover, the primary idea of this work has also been explored in \"LocCa: Visual Pretraining with Location-aware Captioners\" recently.\n\nWhile I commend the authors for doing an extensive study of their method vs CLIP, we have a different landscape today. It is not fair to claim large improvements without putting other improvements from the field in context. Moreover, while the authors improve upon CLIP, their performance metrics on imagenet or LVIS fall short compared to other works from the field. Therefore I unfortunately have to recommend a rejection for this paper."
            },
            "questions": {
                "value": "No questions, the work is very easy to read. I commend the authors for that."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}