{
    "id": "eLLBILFRsA",
    "title": "UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation",
    "abstract": "We present UniDetox, a universally applicable method designed to mitigate toxicity across various large language models (LLMs).\nPrevious detoxification methods are typically model-specific, addressing only individual models or model families, and require careful hyperparameter tuning due to the trade-off between detoxification efficacy and language modeling performance. \nIn contrast, UniDetox provides a detoxification technique that can be universally applied to a wide range of LLMs without the need for separate model-specific tuning. \nSpecifically, we propose a novel and efficient dataset distillation technique for detoxification using contrastive decoding. \nThis approach distills detoxifying representations in the form of synthetic text data, enabling universal detoxification of any LLM through fine-tuning with the distilled text. \nOur experiments demonstrate that the detoxifying text distilled from GPT-2 can effectively detoxify larger models, including OPT, Falcon, and LLaMA-2. \nFurthermore, UniDetox eliminates the need for separate hyperparameter tuning for each model, as a single hyperparameter configuration can be seamlessly applied across different models. \nAdditionally, analysis of the detoxifying text reveals a reduction in politically biased content, providing insights into the attributes necessary for effective detoxification of LLMs.",
    "keywords": [
        "Large Language Models",
        "Detoxification",
        "Safety",
        "Fairness"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We present UniDetox, a universally applicable method designed to mitigate toxicity across various LLMs, leveraging dataset distillation.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=eLLBILFRsA",
    "pdf_link": "https://openreview.net/pdf?id=eLLBILFRsA",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces UniDetox, a universal detoxification method for LLMs that avoids model-specific tuning by generating detoxifying text via dataset distillation with contrastive decoding. This distilled text derived from GPT-2 can detoxify other models like LLaMA-2 without additional hyperparameter adjustments. Experiments validate UniDetox's ability to mitigate toxicity and reduce political bias consistently across models. The paper also discusses insights into the features necessary for detoxification."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Proposes a universally applicable detoxification approach, addressing limitations of existing model-specific methods by utilizing dataset distillation.\n- Provides a theoretical basis by aligning detoxification with the direction opposite to a \"toxic vector\" in model parameter space (Sec. 2.2, Eq. 5)\n- Reduces the need for model-specific hyperparameter tuning, as evidenced by successful application to different models with consistent results with a single hyperparameter configuration."
            },
            "weaknesses": {
                "value": "- The paper lacks a comparison with alternative alignment algorithms such as DPO or other detoxification baselines using task arithmetic between a base model and a tuned model, such as [1][2].\n- Although UNIDETOX is free from downstream model-specific hyperparameter tuning, results are sensitive to the selected learning rate and \u03b1 parameters (Section 3.4). The need for different UNIDETOX configurations impacting performance lacks sufficient justification or analysis.\n- UniDetox demonstrates a drop in diversity metrics compared to other methods. A broader evaluation on other LM tasks would help validate its capability to preserve language quality.\n\n[1] https://arxiv.org/abs/2403.08994\n[2] https://arxiv.org/abs/2405.13967"
            },
            "questions": {
                "value": "- Would the authors provide examples of the distilled detoxifying text for tuning? This would help clarify the properties and coherence of the distilled data."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "UniDetox proposes a framework for the detoxification of Language Models via a three-step process.\n* Step 1 : Create a Toxic Model\n* Step 2 : Distill Detoxifying Text\n* Step 3 : Finetune on Detoxifying Text\n\nIt is valuable in that it proposes a way to detoxify LLMs in an efficient manner and also show that the steps till Step 2, could be independent of the actual model. \nThis is very favorable as models are getting larger and detoxifying them with common methods like task vectors can get computationally expensive (since they involve training another model of identical specs)"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Definitely an interesting idea combining the core principles of task arithmetic and dataset distillation (i.e  showing that fine-tuning on data from a specific distribution also pivots a model\u2019s representation internally in that direction and applying it to the toxicity setting) \n* The idea that detoxifying text can be independent of the final model and can come from a smaller model is also favorable\n* Their baseline approach also contains the straightforward \u201cprompt\u201d variants which is a useful data point for comparison as models get more powerful."
            },
            "weaknesses": {
                "value": "I would like to list the following weakness fully ensuring the authors that I am not unreasonable and am completely open to increasing my score if these are addressed/answered satisfactorily.\n \n* The related section definitely lacks coverage of methods that are available for detoxification. Few of them include : Parameter-Efficient Detoxification with Contrastive Decoding, etc\n* There is a lack in comparison with more recent methods. An efficient method to benchmark against would be the cited LM Steering work(Han et al)\n* Most work in Detoxification uses the Perspective API in their evaluations, but it has been omitted here without any explanation. \n* An example of what the generated\u201cdetoxifying\u201d text looks like would help. Is this just \u201cnon-toxic\u201d text ? Or does it have other interesting lexical properties? Some more analysis would be welcome here( even if in the appendix)"
            },
            "questions": {
                "value": "* Very curious to know why recent methods weren\u2019t benchmarked against?\n* Is there an obvious reason the Perspective API wasn\u2019t used ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes an approach to detoxify LLMs via fine-tuning (UniDetox), with the novelty of using dataset-distilled exemplars. The authors propose to distill datasets using contrastive decoding. An analysis based in fine-tuning gradients is provided, showing that the proposed fine-tuning strategy is indeed moving the model weights in the opposite direction of the toxicity vector. UniDetox requires (1) fine-tuning a base model (B) to be toxic, obtaining model (T), (2) obtaining distilled text with contrastive decoding according to (B-T) and (3) fine-tuning a target model with such data. The authors claim that the obtained data and fine-tuning parameters are universal (i.e. can be applied to any LLM). \nExperiments show better toxicity mitigation than DExperts (Liu et al. 2021) on the source GPT2 model, as well as on other LLMs that are fine-tuned with distilled data using GPT2. Additionally, perplexity and n-gram metrics are provided as proxies for fluency."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Originality:**\n\n* Using dataset distillation in the setting of LLMs in an actionable way is original, to the best of my knowledge.\n* The use of contrastive decoding is also an interesting aspect of this work.\n\n**Clarity:**\n\n* The paper is well written, with clear language. The mathematical notation and formulation is also clear and easy to follow. The figures are also self-explanatory and provide good insights on the work.\n\n**Significance:**\n\n* Toxicity mitigation is an extremely important topic, this work can be of interest to the community. However, see weaknesses and questions for suggestions on how to increase the impact."
            },
            "weaknesses": {
                "value": "**Quality:**\n\n* The experimental setup can be improved to convince the community about the validity of the proposed method. I suggest the authors to include some zer/few-shot metrics (eg. MMLU) to understand how the model abilities are impacted. I also have concerns about the perplexity reported and the performance on OOD data (see questions).\n* Ablations:\n  * The proposed method has an important hyper-parameter $\\lambda$. I missed an ablation study on $\\lambda$, showing the impact of this choice in the final performance. \n  * The amount of data produced via distillation is of great importance. Can the authors discuss about the data required for UniDetox to be effective? Additionally, I would be really curious in seeing some of the distilled sentences produced by UniDetox, as well as examples generated by the detoxified model. Such qualitative results can help the reader understand the method.\n\n**Clarity:**\n\n* I kindly ask the authors to provide the steps followed to derive the Taylor approximation in Eq. 5 (from $s(x)$ in Eq. 1.\n\n**Significance:**\n\n* The evaluation proposed (methods compared) and metrics (ppl, n-gram) are too weak given the current abilities of LLMs and the thorough evaluations many works provide nowadays."
            },
            "questions": {
                "value": "* In Section 3.1, I encourage the authors to include some metric related to zero/few-shot abilities. For example, the overall score in MMLU seems a good candidate.\n\n* Results in Table 1. I have some concerns about the methods used for comparison. Although DExperts was a successful method when it was proposed, several methods have shown superior performance in recent years. For example, [SuauICML24] proposes a method to reduce toxicity without any inference cost and not requiring fine-tuning, showing superior performance compared to DExperts or pre-prompting. Similarly, [PozzobonEMNLP23] propose an efficient method for toxicity mitigation.  I encourage the authors to consider comparing with some newer method than DExperts, this work's imact could strongly benefit from that.\nI also suggest adding these recent methods in the Related Work.\n\n> [SuauICML24] Suau, X., Delobelle, P., Metcalf, K., Joulin, A., Apostoloff, N., Zappella, L., & Rodr\u00edguez, P. (2024). Whispering experts: Neural interventions for toxicity mitigation in language models. ICML 2024.\n\n> [PozzobonEMNLP23] Pozzobon, Luiza, et al. \"Goodtriever: Adaptive toxicity mitigation with retrieval-augmented models.\" EMNLP 2023 Findings.\n\n* Results in Table 1. Could the authors comment on the fact that PPL goes down from 17.28 to 12.23 when using UniDetox? To me, this behavior is counterintuitive, since the PPL of a LLM rarely goes down when one intervenes on the LLM. In this case, fine-tuning on distilled data is an intervention on the original model. It is surprising that fine-tuning on a small synthetic dataset brings the LLM perplexity to 12.23 (which would be the PPL of a much larger/stronger LLM).\nI encourage the authors to provide details about the PPL evaluation, and a conclusive justification of why such PPLs are strongly decreasing. \n\n  * Additinally, in Table 2, UniDetox reduces PPL for OPT, Falcon and increases PPL for LLama2. Conversely, using lr=1e-5, the PPL strongly increases for OPT, Falcon and decreases for LLama2. This hints that the fine-tuning parameters and/or distilled data are not _universal_ but rather well suited for the combinations of parameters and models chosen.\n\n* Results in Table 1. Why is OOD toxicity reduction stronger than ID reduction? I would have expected a fine-tuning based approach like UniDetox to be much more effective on ID. Could the authors comment on this aspect? \n\n* Results in Table 1. For the safety preprompt to be effective, have the authors considered evaluating an instruction-tuned model such as gemma-2-2b-it? I believe safety preprompts are much better designed for instruction tuned models than for decoder only (and arguably older) models like GPT2. \n\n**Comments**:\n\n* This work has some interesting proposals such as using dataset distillation to address model alignment. However, the _universality_ claims are only validated using a small number of models and a non-comprehensive set of metrics. I find very confusing the fact that PPL strongly decreases when the intervention is applied. Moreover, details about the choice of $\\lambda$ and the data produced are lacking. Considering all the above, I cannot recommend this paper for acceptance as is, however, I am open to reconsider my score upon rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The research presents a novel and universally applicable detoxification approach to a wide range of language models without the prior understanding or fine-tuning of the specific hyperparameters and architectures. The work introduces an efficient data distillation method by incorporating contrastive decoding, which helps to identify the toxicity vector within the initial parameter space and generate detoxifying text sets. Later, the language model is fine-tuned on the distilled datasets to mitigate the toxicity in the base language model. The approach was evaluated and analyzed based on various baseline methods and UniDetox to understand the technique's effectiveness. Overall, the method, UniDetox, introduces a novel data distillation algorithm with contrastive decoding to significantly mitigate the level of toxicity in the base language models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The work provides a novel approach and insights into the following points \n1. Novel dataset distillation method using contrastive decoding\n2. Complete evaluation across a range of commonly used detoxification methods from the references \n3. Significant results based on the PT and EMT metrics across different benchmarks\n\nAs for the paper, the strengths are\n1. Well-structured and written paper\n2. Well-styled and formatted paper\n3. Thoughtful introduction and rigorous reasoning"
            },
            "weaknesses": {
                "value": "1. The size of the model might mitigate UniDetox's effectiveness since the ratio of parameter space and the volume of detoxifying text might be increased significantly, and no further preventive techniques could be introduced.\n2. Compared to other methods, UniDetox still requires model fine-tuning to mitigate the toxicity from the base models, making this approach's computational intensity a further challenge or question to understand.\n3. Limited to the text-only language models, as the emerging traction of multi-modal language models, how this approach further mitigates toxicity with a vast data modality remains challenging to this technique."
            },
            "questions": {
                "value": "The following points suggest further investigation and evaluation of this method's potential weaknesses or limitations.\n1. Does this method apply to the scope beyond the text-only model? Can this approach apply to the multimodal language models as well?\n2. What are the costs regarding the computational resources, time required, or other considerations?\n3. What is the impact of the larger parameters space, like using larger models from 14B to even 405B models, on the small set of detoxifying text?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}