{
    "id": "j0sq9r3HFv",
    "title": "Automated Parameter Extraction for Biologically Realistic Neural Networks: An Initial Exploration with Large Language Models",
    "abstract": "In computational neuroscience, extracting parameters for constructing biologically realistic neural models is a resource-intensive task that requires continuous updates as new research emerges. This paper explores utilizing large language models (LLMs) in automating parameter extraction from scientific literature for biologically realistic neural models. We utilized open-source LLMs via Ollama to construct KGs, capturing parameters such as neuron morphology, synapse dynamics, and receptor properties. SNNBuilder \\cite{Gutierrez2022}, a framework for building spiking neural network (SNN) models, serves as a key validation example for our framework. However, the methodology we outline here can extend beyond SNNs and could applied to systematic modelling of the brain.By experimenting with different prompting strategies\u2014general extraction, in-context hints, and masked prompting\u2014we evaluated the ability of LLMs to autonomously extract relevant data and organize it within an expert-base or data-driven ontology, as well as to infer missing information for neural model construction. Additionally, we implemented retrieval-augmented generation (RAG) via LangChain to further improve the accuracy of parameter extraction through leveraging external knowledge sources. Analysis of the the generated KGs, demonstrated that LLMs, when guided by targeted prompts, can enhance the data-to-model process, paving the way for more efficient parameter extraction and model construction in computational neuroscience.",
    "keywords": [
        "Large Language Models",
        "Knowledge Graphs",
        "Computational neuroscience",
        "Neural model construction"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=j0sq9r3HFv",
    "pdf_link": "https://openreview.net/pdf?id=j0sq9r3HFv",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates the use of large language models (LLMs) to automate parameter extraction from scientific literature for constructing biologically realistic neural models, capturing details like neuron morphology, synapse dynamics, and receptor properties."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "***1.*** Using LLMs to extract parameters for building SNNs is an interesting topic."
            },
            "weaknesses": {
                "value": "***1.*** The paper lacks more experimental results and does not provide a detailed analysis of the findings.\n\n***2.*** The paper has an unusual format and does not appear to follow the original template.\n\n***3.*** The phrasing throughout the paper is unclear.\n\n***4.*** The authors need to conduct extensive restructuring and experimental analysis."
            },
            "questions": {
                "value": "Please refer to the \"Weaknesses\" section for further details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the use of open-source LLMs to automatically extract parameters from scientific literature for building biologically realistic neural models, particularly SNNs. By employing various prompting strategies and retrieval-augmented generation, the authors successfully constructed knowledge graphs that capture essential neural attributes, enhancing the efficiency of model parameterization."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The intuition is good."
            },
            "weaknesses": {
                "value": "1. Formatting Issues: The paper's format is problematic, with the main text appearing excessively narrow, which does not adhere to standard formatting guidelines.\n\n2. Insufficient Detail: The paper is notably short, providing an overly simplistic description of the methodologies employed. Additionally, the experimental section is minimal, lacking comprehensive analysis and validation.\n\n3. Project-Oriented: The content resembles a preliminary project report rather than a fully developed research paper. It falls significantly below the standards expected for publication in venues like ICLR, both in terms of depth and rigor."
            },
            "questions": {
                "value": "1. Could you provide a more detailed description of your method?\n\n2. Could you give more experimental results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work documents an initial exploration of the use of LLMs (such as GPT4o and LLama3.1) in conjunction with different prompting strategies (such as RAG) to automatize the extraction of computational neuroscience relevant parameters (in particular for SNN) into Knowledge Graphs (KG) from relevant literature."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The project aims to tackle relevant research questions.\n- The use of open source LLMs ensures wider applicability of the method.\n- The conclusions does acknowledge some limitations of the work."
            },
            "weaknesses": {
                "value": "* 1\\. The main claims/contributions in the abstract are hard to relate to what experiments and results are reported in the paper:\n   - the \u201caccuracy of parameter extraction\u201d was not evaluated?\n   - the \u201cdata-to-model\u201d process was not evaluated?\n   - the \u201cSNNBuilder\u201d was not used in any evaluation?\n   - it's unclear how the characterization of the graphs using the  Leiden Modularity relates to any of the claims\n   - => the suitability of the generated graphs for comp. neuroscience model generation/configuration was therefore neither qualitatively nor quantitatively shown anywhere\n* 2\\. Many methodological details are unclear and missing, and therefore it is unclear how to interpret the results, or how to reproduce the results? (see Questions).\n* 3\\. Many formulations are hard to properly understand, as they are presumably imprecise or incorrectly using technical terms, among others:\n   - \u201c... promoting strategies \u2026\u201d => prompting strategies? (line 58-59)\n   - \u201c... one-shot promoting \u2026\u201d => prompting? (line 82-83)\n   - Spelling errors \u201cnda\u201c and \u201csparge\u201c?  (line 169-171)\n   - Unclear what is meant by \u201c parameterization of brain models\u201d here? (line 34)\n   - Unclear what is meant by \u201cagumenting external data\u201d here? (line 44-45)\n   - Unclear what \u201cbrain data\u201d is referring to? (line 51)\n   - \u201cStructurally, this loss is a critical limitation in this approach,\u2026\u201d => the loss of structure? (line 202-203)\n* 4\\. The structuring of the paper is confusing and hard to follow:\n   - The \u201cPreliminaries\u201d section mostly describes experiments and methods unrelated to the rest of the paper, and presumably serves as explanation as to why a different approach is introduced later. Possibly something for the Appendix?\n   - Some results are first described in the Discussion, and additional experiments are introduced in the Discussion.\n* 5\\. Figure 3 is mostly unreadable, and readable parts mostly reflect non neuroscience related terms:\n   - E.g. 3.c: \u201cKeep\u201d, \u201cTo\u201d, \u201cIt\u201d, \u201cAssuming\u201d, \u201cBased\u201d, \u201cName:\u201d, \u201cNot\u201d, \u201c\\\u201d, \u201cdon\u2019t\u201d, \u2026\n* 6\\. The related literatur section is missing methodologically more closely realted papers."
            },
            "questions": {
                "value": "1) Which paper was used as source for Table 1?\n2) How was the hand authored ontology in section 3.1 derived?\n3) How are the Figure 1 and the Ontology from 3.1 related?\n4) What are the exact equations for \u201cLeiden Modularity\u201c?\n5) What for / where was the \u201cSNNBuilder\u201d actually used?\n6) What are the \u201cdifferent parts of the ontology from 3.1\u201d, and were they actually used? (line 286)\n7) What is the corpus of papers? (line 301)\n8) What other corpus of papers? (line 351)\n9) How does evaluating \u201cLeiden Modularity\u201d support the claims in the abstract?\n10) How does evaluating \u201cAverage Degree Centrality\u201d support the claims in the abstract?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the feasibility of using large language models (LLMs) to automatically extract the parameters required to build biologically realistic neural networks from scientific literature. The authors evaluated the ability of LLMs to extract parameters such as neuronal morphology, synaptic dynamics, and receptor properties through different prompting strategies (general extraction, contextual prompts, and masked prompts), and used retrieval-augmented generation (RAG) technology to improve the extraction accuracy. The main verification case of the study is the SNNBuilder framework, which is used to build spiking neural networks (SNNs). The results show that LLMs can effectively promote data extraction and organization in neural model construction under specific prompts, providing an efficient and automated parameter extraction method."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The author focuses on two core areas in the paper:\n1. Using LLM to automatically build and update knowledge graphs from unstructured data\n2. Organizing complex experimental data in neuroscience and converting them into computable models\n\nI think both areas are very interesting and have practical application value.\n\nFor 1, knowledge graphs are difficult to be expanded to a larger scale, updated and deployed in real time due to their high construction and maintenance costs. However, graph databases provide a reliable way to store and reason about complex information. On the one hand, knowledge graphs can benefit from LLM's powerful natural language parsing capabilities, and on the other hand, LLM can benefit from knowledge graphs' reliable information storage methods to alleviate hallucinations.\n\nFor 2, there is a large GAP between current AI research and neuroscience research: researchers in the two fields find it difficult to communicate using the same academic language. For example, a large number of experimental results produced in neuroscience research are difficult to be converted into clear and reusable computational models in the field of machine learning; on the other hand, advanced machine learning models (such as LLM, LVM) lack biological interpretability, and it is difficult to find corresponding experimental evidence in neuroscience. Bridging this GAP in any way will help the two fields communicate and benefit each other."
            },
            "weaknesses": {
                "value": "As I mentioned in the strengths section, this paper focuses on interesting problems. But I think it is a simple technical report rather than an academic paper. It is not appropriate to publish it at ICLR. (Btw I think this paper uses the iclr template incorrectly, and the margins of the main text are incorrect.) \n\nThe specific areas that can be improved are as follows:\n1. Lack of novelty in the method: The author did not do any further work except proposing three prompts.\n2. Lack of experiments: There are many technical details in designing computational models in the field of neuroscience using LLM. The authors did not conduct/show these experiments.\n\nI will elaborate on these weaknesses in the Question."
            },
            "questions": {
                "value": "1.Does the SNN model (entity, relation, entity triple) extracted using LLM strictly follow the prompt? Does the author's method in Section 4 effectively alleviate the problems pointed out in Section 3? :\"This result suggests that the LLM with the Graph RAG KG generation approach\ncan struggle to maintain the intended multi-level hierarchy of the ontology when generating the graph, mainly when using the Base Prompt alone.\"\n\n2.How to measure the quality of knowledge graph extracted by LLM? According to Figure 3, there are a large number of meaningless entities in the generated graph, such as \"while, to, the, here, i\". Such a graph is far from meeting the standards of practical applications.\n\n3.An important problem of knowledge graph is entity disambiguation, but it is not shown in the author's research. The author used the well-known open source framework GraphRAG/Neo4j. It is obvious that the existing methods are not good enough, but the author did not make more contributions.\n\n4.Are the SNN models generated by LLM biologically correct and meaningful to biology? For the machine learning community, can these models be used or even lead to better machine learning models? This paper lacks any measurement of the effectiveness of these models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}