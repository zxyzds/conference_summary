{
    "id": "b5CEjE4zyL",
    "title": "Neural Networks Decoded: Targeted and Robust Analysis of Neural Network Decisions via Causal Explanations and Reasoning",
    "abstract": "Despite their success and widespread adoption, the opaque nature of deep neural networks (DNNs) continues to hinder trust, especially in critical applications. Current interpretability solutions often yield inconsistent or oversimplified explanations, or require model changes that compromise performance. In this work, we introduce TRACER, a novel method grounded in causal inference theory designed to estimate the causal dynamics underpinning DNN decisions without altering their architecture or compromising their performance. Our approach systematically intervenes on input features to observe how specific changes propagate through the network, affecting internal activations and final outputs. Based on this analysis, we determine the importance of individual features, and construct a high-level causal map by grouping functionally similar layers into cohesive causal nodes, providing a structured and interpretable view of how different parts of the network influence the decisions. TRACER further enhances explainability by generating counterfactuals that reveal possible model biases and offer contrastive explanations for misclassifications. Through comprehensive evaluations across diverse datasets, we demonstrate TRACER's effectiveness over existing methods and show its potential for creating highly compressed yet accurate models, illustrating its dual versatility in both understanding and optimizing DNNs.",
    "keywords": [
        "AI Explainability",
        "Causal Discovery",
        "Neural Network Optimization"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=b5CEjE4zyL",
    "pdf_link": "https://openreview.net/pdf?id=b5CEjE4zyL",
    "comments": [
        {
            "summary": {
                "value": "This paper (TRACER) aims to analyze the causal dynamics of deep neural network (DNN) decisions without altering their architecture. By intervening on input features, TRACER tries to identify feature importance and constructs a causal map."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- aims to  estimate the causal mechanisms underpinning DNN decisions\n\n- aims to provide explanations for correct and misclassified samples"
            },
            "weaknesses": {
                "value": "-  The paper is poorly structured and difficult to follow, with several instances of sloppiness. For instance, in lines 157-160, it says the first level of PCH is Association; However, it is typically Abduction, which involves inferring exogenous noises.\n\n\n>Association: We extract dependency structures from the DNN activations and outputs $P(Y^{(i)}| X)$\nwhere $X$ and $Y^{(i)}$ represent the input and the $i$-th layer\u2019s output variables, respectively;\n\nIs $Y^{(i)}$ exogenous noise?  **Please clarify their interpretation of the Association level and how it relates to abduction**\n\n- A clear description of causal variables is missing. \n  >line (204-213) : Assuming b to be causally independent (e.g., binary mask), all input\nfeatures, before and after interventions, can be considered exogenous variables in the causal map.\n\nHow? **Please justify.**\n\n- Many definitions and theorems lack adequate motivation. For instance, the reasoning behind the Average Causal Effect presented in that specific form is unclear. **Please provide more context or motivation for key definitions and theorems, particularly the Average Causal Effect.**\n\n- >*Upon obtaining the similarity measures, we establish causality by grouping layers based on their CKA values* line (246-251)...causal explanation depends on the grouping based on predetermined threshold $\\epsilon$. \n\nDifferent thresholds will give different causal structures or explanations for the DNN. The consistency of the explanations is not discussed.\n**How does the choice of threshold affect the stability and reliability of causal explanations?**\n\n- There is no clear explanation for selecting $\\epsilon$, which is a critical hyperparameter. **How do you select or tune the \n parameter? Please  include any empirical studies or theoretical justifications for your choice**\n\n- Figure 1 does not adequately illustrate the methodology or framework.\n\n-  misses relevant literature, such as:\n> - *Neural network attributions: A causal perspective*, Aditya Chattopadhyay, Piyushi Manupriya, Anirban Sarkar, and Vineeth N Balasubramanian. \n> - *Towards learning and explaining indirect causal effects in neural networks*,  Abbaavaram Gowtham Reddy, Saketh Bachu, Harsh Nilesh Pathak, Ben Godfrey, Vineeth N. Balasubramanian, V Varshaneya, and Satya Narayanan Kar. \n\n**A comparison with these works is necessary..**"
            },
            "questions": {
                "value": "Please see the weaknesses..."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose a novel method, called TRACER, to investigate the causal dynamics inside DNNs by intervening on input features. They further use TRACER to generate counterfactuals to improve explainability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well organized and well written.\n    \n- The proposed method is technically sound.\n    \n- The problem is of great importance and of interest to the community"
            },
            "weaknesses": {
                "value": "- As we know that DNNs are strong correlation learners, the learned links/parameters between neurons/layers might contain lots of strong spurious connections, rather than causal connections. In this case, we cannot distinguish them even though intervening on input features and monitoring the changes on intermediate outputs. Thus, I guess that it might be difficult to learn a true causal graph.\n    \n- Many details in the experimental part are missing, e.g., what interventions are performed in each specific experiment? What specific regularization metric is chosen for the counterfactual analysis? etc."
            },
            "questions": {
                "value": "- It seems not to show a counterfactual generation for a misclassified ImageNet sample? I want to see the quality of such a bit complicated counterfactual.\n    \n- It is well known GANs have the mode collapse problem. Does it occur in your experiments? If so, how to deal with it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes TRACER, a method for generating \"explanations\" of neural network behavior with methods from causal inference. TRACER groups neural network layers into \"groups\" based on their similarity, and then computes a causal effect score for each input dimension in terms of how that feature affects the intermediate network layers and the final output. This is done by making a series of interventions to the input and observing how that affects downstream layer groups and the network output. The authors claim that the method generates more sensible saliency maps than existing methods, as measured by a \"reliability score\" that they define. They also claim that their method can be used to compress an MNIST classifier by over 99% without harming accuracy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper is well-presented with high-quality figures\n* The research direction, of using causal inference for explainability, is an interesting and important one."
            },
            "weaknesses": {
                "value": "* The paper uses a lot of vague and flowery language that makes me worry that it may be substantially LLM-written.\n* The authors claim that they can reduce the size of an MNIST classifier by 99.42% without substantially harming model performance. This seems too good to be true, and their method for doing this not very clearly described.\n\nOverall, I am having a hard time assessing the method and claims of the paper, so will give a score of 5 for now with low confidence."
            },
            "questions": {
                "value": "* You say that you use a pre-trained AlexNet model for MNIST classification, however AlexNet was an ImageNet model. Did you fine-tune it with a new head to classify MNIST digits? If so, why isn't this described anywhere in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper present method grounded in causal theory to estimate the causal link between the various layers of neural networks. This technique is used for both the explanation of the model and its optimization."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The use of the causal paradigm for building an explanability tool ensures it has interesting groundings. \nThe related work section is thorough.\nA various number of considerably unique experiments are considered."
            },
            "weaknesses": {
                "value": "1. The first major weakness concerns the writing of the paper.\n\n1.1 I see the \u00ab\u00a0proofs\u00a0\u00bb in section A of the appendices as explanations as to why the propositions and theorems presented in the main paper make sense and are coherent, and not formal nor rigorous proofs (especially A.1).\n\n1.2 In section 4.1, no explanation is given about what P is in practice; the proportion p that is used. The layer grouping is explained in Section 3, but no details regarding the inputs themselves are provided. How are the most important features found? Many parts of the approach and the experiments lack clear explanations of the various steps, leading to many unanswered questions (see the Questions section) and questions found in the present section. Figure 1 does not give much information about the presented approach. How are the counterfactuals generated in practice, and how are they used? How is the feature contribution made?\n\n1.3 There are many inconsistencies or errors in the notation and the writing itself, see ** Typos and such**.\n\n2. \u00ab\u00a0Causality\u00a0\u00bb is central in the article, yet I see no causality in the proposed approach.\n\n2.1 For example, let\u2019s consider the way the various layers in the network are compared via Centered Kernel Alignments. This \u00ab\u00a0distance measure\u00a0\u00bb, or similarity measure, does not tells us anything about causality, simply about resemblance. \n\n2.2 The relationship is considered or examined between the various components of a single level (e.g. feature level), which truly undermine the scope of the analysis.\n\n3.1 The approach is based on the fact that the considered networks are straightforward in their nature; the output of a layer is considered to be the input of the next layer, but what if more sophisticated architecture is considered, such as U-nets? How would the approach work in the case of recurrent networks? Or networks with parallel components, such as attention heads?\n\n3.2  I am conceptually unsure of the approach, in that it only considers huge perturbations made by a single layer. What about if the whole network is a composition of many small perturbations, such as in Variational autoencoders?\n\n3.3 It is said that if a group of layers is similar, they can be regrouped in a single transformation $g_{i,j} = fj \u25e6 fj\u22121 \u25e6 . . . \u25e6 fi = fi$, but how seeing this part of the network as a single transformation helps to reduce the number of parameters? Simply replacing $ fj \u25e6 fj\u22121 \u25e6 . . . \u25e6 fi$ by $fi$ simply cannot be done when the dimensions of the various layers aren\u2019t the same.\n\n4. The last concern is about the contributions of the paper. Line 532\u00a0: \u00ab\u00a0Through our foundational principles and findings, we have ascertained that by producing intuitive, human-interpretable explanations, TRACER offers outstanding transparency to neural networks [...]\u00a0\u00bb The paper proposes a way to compute how big of a transformation is applied at each step of the network, and to generate counterfactual, so I would not say so.\n\n4.1 Concerning the originality of the approach: The use of counterfactual is not a contribution, for the same procedure could be used with a different model\n\n\n\n**Typos and such**\n\n-Line 89\u00a0: rior \u2192 Prior\n\n-Line 96\u00a0: explainabibilty \u2192 explainability\n\n-Line 161\u00a0: What does do() mean?\n-Line 172\u00a0: Based on the notation only, it seems like it is the masked input that is in $\\{0,1\\}^d$, where, I guess, it is implied that it is the mask that is in $\\{0,1\\}^d$.\n\n-Line 175\u00a0: Please define the \u00ab\u00a0|=\u00a0\u00bb operator.\n\n-Definition 2\u00a0: In both C2 and C3, the notation is wrong, for you consider applying masks of dimension different than d to set of examples of dimension d. For example, in C3, instead of \u00ab\u00a0for all M\u2019 $\\subset$ M\u00a0\u00bb, you probably meant \u00ab\u00a0For all M\u2019 such that |M\u2019|\u2081 < |M|\u2081\u00a0: \u2026\u00a0\u00bb.\n\n-Line 231\u00a0: Please provide citations for \u00ab\u00a0 \u2026 prevalent approach for quantifying the\nsimilarity between high-dimensional embeddings.\u00a0\u00bb\n\n-Line 231\u00a0: Notations are introduced for talking about the inner components of neural networks at too many various places in the article; there should be a section where this notation is properly introduced. Please state explicitly the dimensions of $f_i$ and $f_j$. It is not obvious only looking at the notation whether $K_i$ is a matrix of size $nxn$ or else.\n\n-Line 252\u00a0: The composition increases the layer number at certain places (line 252, 259) but decreases at other places (line 259, 266). Please be coherent with the mathematical notation.\n\n-Line 197\u00a0: The usage of \u00ab\u00a0nodes\u00a0\u00bb in \u00ab\u00a0causal nodes\u00a0\u00bb refers to the concept of layer in this work, but \u00ab\u00a0node\u00a0\u00bb typically refer to a single neuron of a layer. It wold be less ubiquitous to use another term for talking about \u00ab\u00a0causal nodes\u00a0\u00bb.\n\n-Line 359\u00a0: when citing MNIST, please consider the following indication: http://citebay.com/how-to-cite/mnist/. \n\n-Line 378\u00a0: The hyperparameters that were used should be found in the appendices.\n\n-Line 408\u00a0: This letter P (with this exact calligraphy) is already used for Probability.\n\n-Table 1\u00a0: C1, C2 and C3 already refers to concept in Definition 2. Please use distinct notation.\n\n-Line 512\u00a0: \u00ab\u00a0In this study, we focused our evaluations of TRACER on white-box neural network.\u00a0\u00bb I consider neither AlexNet nor ResNet-50 to be white-box models."
            },
            "questions": {
                "value": "1. Section 3.2.1\u00a0: Why consider a single unique value to replace a subset of $x$, and why would it be different than say 0?\n\n2. Line 241\u00a0: \u00ab\u00a0Flexibility: It accommodates various kernel functions, such as linear or Gaussian, enabling flexibility based on specific requirements of the analysis\u00a0\u00bb. Please explain in which situation it would be preferable to have a Gaussian kernel instead of a linear one. Why this current choice of kernel?\n\n3. Line 287\u00a0: Why consider the many neurons of a layer as realizations of a random variable? To which probability are associated?\n\n4. Line 292\u00a0: How does $P (g\u2032i(x) | do(X = x\u2032))$ and $P (g\u2032i(x) | do(X = x))$ are computed in practice?\n\n5. Line 408\u00a0: How does P is found in practice and what does P actually outputs? A mask?\n\n6. Table 1\u00a0: How is the simpler model created?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}