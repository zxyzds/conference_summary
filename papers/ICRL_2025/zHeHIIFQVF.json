{
    "id": "zHeHIIFQVF",
    "title": "Train once and generalize: Zero-shot quantum state preparation with RL",
    "abstract": "Quantum state preparation is an essential cornerstone of quantum information science and quantum algorithms. Notwithstanding worst-case hardness results, designing efficient and scalable methods for approximate state preparation on near-term quantum devices remains a significant challenge. In this work, we present a deep reinforcement learning approach to quantum state preparation that allows for the zero-shot preparation of any state at a fixed system size. We scale significantly beyond previous works by designing a novel reward function with provable guarantees. In our experiments on stabilizer states up to nine qubits, we achieve generalization to unseen states by training on less than $10^{-3}$\\% of the state space. We prepare target states with varying degrees of entanglement content and obtain insight into the quantum dynamics generated by our trained agent. Benchmarking shows our model produces stabilizer circuits up to $60$\\% shorter than existing algorithms, setting a new state of the art. To our knowledge, this is the first work to prepare arbitrary stabilizer states on more than two qubits without re-training.",
    "keywords": [
        "Quantum State Preparation",
        "Deep Reinforcement Learning",
        "Zero-shot Inference",
        "Off-the-shelf Algorithms",
        "Generalization"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "Reinforcement learning framework for zero-shot quantum state preparation.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zHeHIIFQVF",
    "pdf_link": "https://openreview.net/pdf?id=zHeHIIFQVF",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a zero-shot quantum state preparation approach. RL has been used.\nA moving goal post reward function has been designed. Training is performed on less than 0.0001% of the\nstate space and then generalized."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Upto 60% shorter circuit depths are obtained.\n2. Generalized to more than 2 qubits without re-training."
            },
            "weaknesses": {
                "value": "1. Proof of Prop. 2 of convergence is not completely convincing. Conditions \nunder which it is valid need rigorous statement.\n2. No guarantee that the potential function will be strictly convex. No consideration\nis given to this.\n3. It is not clear why retraining is not required. How general is this is not stated."
            },
            "questions": {
                "value": "1. Why retraining is not required needs further explanation and study?\n2. Why was the 9 qubit agent not trained till convergence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors employ reinforcement learning (RL) to the task of quantum state preparation (QSP) on physically relevant states. Additionally, their method enables zero-shot preparation for any target state when the system size is fixed. \nNumerical results for systems with up to $9$ qubits show that their method generates circuits up to $60$% shorter than the baseline methods evaluated in the study."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper does a reasonable job introducing the quantum state preparation task and, from the presented difficulties of the domain, manages to derive its adaptations to standard RL employed to tackle this domain. The adaptations presented are sound and straight-forward. The results give a nice guideline for future research in that direction."
            },
            "weaknesses": {
                "value": "- The presentation of the paper is somewhat convoluted and challenging to follow; clearer descriptions would be helpful.\n  - The contributions could be better highlighted, such as the introduction of the moving goalpost reward (MGR) for the QSP task.\n  - The methodology and results, presented through plots, are introduced quite late in the paper. Providing clearer scenario descriptions of the experiments would improve readability.\n  - A clearer contrast with related work would be helpful.\n  - Figure legends could be enlarged for improved readability.\n  - To aid understanding of the MGR function, a figure illustrating its workings would be valuable.\n\n- The paper appears to misinterpret the QSP definition by assuming access to the target state $|\\psi\\rangle$. Typically, in QSP, one has access only to a succinct (classical) description of the target state, not the circuit that implements it. However, in this approach, the authors start each episode with $|\\psi\\rangle$ as the initial state and then find a circuit to prepare $|\\mathbf{0}\\rangle^{\\otimes n}$ for zero-shot state preparation. This approach may not be physically feasible in a lab; if the circuit for $|\\psi\\rangle$ is known, the rationale for using RL (or any method) to find a circuit that prepares $|\\mathbf{0}\\rangle^{\\otimes n}$ and then inverts it is unclear. It might be argued that the given $|\\psi\\rangle$ circuit is too deep or not NISQ-friendly, necessitating a more compact architecture, though this would still be costly in a real lab setting.\n\n- Insufficient comparison with relevant state-of-the-art methods for the quantum state preparation task:\n  - RL methods: [1-4]\n  - ML methods: [5,6]\n  - SAT-based methods: [7,8]\n\n- The algorithm's performance under common noise models, such as state preparation and measurement (SPAM), bit flip, phase flip, depolarizing noise, and thermal relaxation is not evaluated. These noise sources are standard in quantum computing.\n\n- Several claims in the paper lack theoretical or numerical support. Ablation studies providing numerical evidence would help support these claims:\n  - Line 306-308: \"However, the cumulative reward obtained ..... terminating the episode.\"\n  - Line 803-806: \"Finally, for the linear-connectivity agents, .... faster with this term.\"\n\n- The paper lacks open-source code and a reproducibility statement.\n\n\n[1] Fosel, Thomas, et al. \"Quantum circuit optimization with deep reinforcement learning\", arXiv:2103.07585 (2021)\\\n[2] Patel, Yash J., et al. \"Curriculum reinforcement learning for quantum architecture search under hardware errors\",  ICLR (2024)\\\n[3] Zen, Remmy, et al. \"Quantum Circuit Discovery for Fault-Tolerant Logical State Preparation with Reinforcement Learning\", arXiv:2402.17761 (2024)\\\n[4] Kremer, David, et al. \"Practical and efficient quantum circuit synthesis and transpiling with Reinforcement\nLearning\", arXiv:2405.13196 (2024)\\\n[5] Wang, Hanrui, et al. \"Quantumnas: Noise-adaptive search for robust quantum circuits.\" 2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)\\\n[6] Wu, Wenjie, et al. \"Quantumdarts: differentiable quantum architecture search for variational quantum algorithms\", ICML (2023)\\\n[7] Peham, Tom, et al. \"Depth-Optimal Synthesis of Clifford Circuits with SAT Solvers\", QCE (2023)\\\n[8] Schneider, Sarah, et al. \"A SAT Encoding for Optimal Clifford Circuit Synthesis\", ASPDAC (2023)"
            },
            "questions": {
                "value": "- see above.\n\n- HSH gate in the gateset:\n  - What is the rationale behind including HSH gate in the gateset $G$ (action space)? Because in principle the agent should learn the symmetry on $S$ w.r.t. to $X$ and $Z$ operations by itself. I believe this inductive bias is introduced during the learning process while the whole point of using RL is not to have any kind of bias.\n\n  - Also HSH gate has three single qubit gates. In Table 1, do you count it as one gate for all the methods? Do you use the same gateset for all the methods for a fair comparison?\n\n  - Can you provide an ablation study with and without HSH gate to see how the number of gates differ between these two settings for your method?\n\n- Why is there no circuit size numbers for $9$-qubit DRL (local gates) in Table 1?\n\n- In Fig. 3(d), $n=9$, can you please explain the behaviour of the circuit size for depth $t=1, 2$ signficantly larger for your method compared to other two baselines?\n\n- How does the algorithm perform for the QSP task when the target states which are complex and physically relevant but are not classically simulable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This study presents a zero-shot reinforcement learning (RL) algorithm that incorporates a novel reward function specifically designed for quantum state preparation (QSP). The authors concentrate on stabilizer state preparation tasks involving up to 9 qubits. Notably, the proposed algorithm demonstrates strong generalization capabilities on unseen data, utilizing a training dataset that is 0.001% smaller than the total state space. Furthermore, the quantum circuits generated by this algorithm achieve a circuit size reduction of over 60% compared to existing methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The authors introduce a novel reward function that is inspired by the intrinsic nature of the QSP problem, integrating it into the RL framework to enhance performance in terms of circuit size reduction compared to existing algorithms.\n2. The proposed algorithm demonstrates zero-shot generalization, significantly decreasing computational complexity compared to traditional RL algorithms, which require retraining for each new target state.\n3. The authors provide a theoretical analysis of the generalization capabilities of the proposed algorithm."
            },
            "weaknesses": {
                "value": "1. As someone who is not well-versed in RL algorithms, I found it somewhat challenging to follow the overall algorithms framework. The authors present the general RL algorithms separately before introducing their specific formulation for QSP tasks along with the novel reward function. It would enhance clarity if the authors summarized the algorithm in a table format.\n2. The authors primarily use the total number of gates in the circuit, including both single-qubit and two-qubit gates, as the main evaluation metric for circuit size. However, two-qubit gates are more challenging to implement in practical devices and are more susceptible to noise. It would be beneficial to separately compare the counts of single-qubit and two-qubit gates in the circuits generated by the proposed algorithm against those produced by existing algorithms.\n3. Typos: Line 256: \"a the target\"."
            },
            "questions": {
                "value": "The questions are included in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This manuscript studied the problem of quantum state preparation (QSP). While theoretical lower bounds for the required operations have been established, efficiently approximating quantum states on near-term quantum computers remains an open question. The authors proposed a reinforcement learning (RL) algorithm for QSP, focusing on the preparation of stabilizer states\u2014 a set of states of significant practical interest\u2014using Clifford gates. The authors introduced a unified target state by replacing $|\\psi\\rangle$ with $|0\\rangle$ and approximating backwards, referred as \"Zero-shot\" in the manuscript. Additionally, the authors proposed a moving-goalpost reward (MGR) function that aligns the maximum cumulative reward with the highest final fidelity. On the experimental side, the authors conducted experiments with up to 9 qubits, with and without connectivity restrictions, achieving better performance in terms of gate count compared to referenced algorithms. Theoretically, they proved a loose lower bound on the probability of generalization success."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The problem studied is highly relevant to the quantum computing community. The manuscript presented a RL algorithm to efficiently prepare stabilizer state, which is valuable for future research.\n\n2.  The numerical results is promising compared to conventional algiorthm, even with connectivity constraints, showing the potential utility on near-term quantum computers."
            },
            "weaknesses": {
                "value": "1. The nanuscript lack discussing about relevant literatures. The \"Zero-shot\" trick is not new; amany existing works have adopted it in reinforcement learning for quantum circuit synthesis and compiling, with QSP being only a subset [1,2]. \n\n[1] Zhang, Yuan-Hang, et al. \"Topological quantum compiling with reinforcement learning.\" Physical Review Letters 125.17 (2020): 170501.\n[2] Qiuhao, Chen, et al. \"Efficient and practical quantum compiler towards multi-qubit systems with deep reinforcement learning.\" Quantum Science and Technology (2024).\n\n2. The claim in the abstract that \"To our knowledge, this is the first work to prepare arbitrary stabilizer states on more than two qubits without re-training\" is questionable. A recent work [3] also proposed a reinforcement learning algorithm for Clifford circuit synthesis.\n\n[3] Kremer, David, et al. \"Practical and efficient quantum circuit synthesis and transpiling with Reinforcement Learning.\" arXiv preprint arXiv:2405.13196 (2024).\n\n3. The authors established a loose lower bound for the probability of generalization success (as indicated in the last equation of the main text). In Table 2, the probability decreases with qubit size, raising concerns about the generalizability of the proposed algorithm. Additionally, in Line 517, the authors stated, \"We did not train the 9-qubit agent to convergence...,\" which raises further concerns about the trainability of the proposed RL algorithm."
            },
            "questions": {
                "value": "1. The term \"Zero-shot\" is ambiguous. Typically, \"zero-shot\" refers to training without class labels. In the manuscript, the authors seem to be referring to the unified target trick, which should be clarified.\n\n2. In the introduction, the authors claim that their RL method is applicable to \"arbitrary states.\" However, the states studied are specifically stabilizer states. The authors should either correct this claim or provide results for additional categories of states.\n\n3. How are the experimental results related to the theoretical lower bound of generalization? The authors should consider displaying the success probability achieved during their experiments.\n\n4, What are the definitions of $q$ and $N$ in Lines 517-518? Additionally, the last equation should be numbered for clarity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The manuscript presents a deep reinforcement learning approach for quantum state preparation (QSP) . The authors design a novel reward function with guarantees that significantly scales beyond previous works, allowing for generalization to unseen states. They demonstrate their method on stabilizer states up to nine qubits, achieving state-of-the-art performance by preparing target states with varying degrees of entanglement."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This manuscript introduces a novel novel reward function with provable guarantees, which adds a level of theoretical robustness to the approach.\n- The proposed method's success on stabilizer states, which are crucial for quantum error correction and other quantum information processes, suggests its applicability.\n- The paper demonstrates that their method generates circuits that are up to 60% shorter than those produced by existing algorithms in some cases, which is an improvement in efficiency."
            },
            "weaknesses": {
                "value": "- I believe the term \"zero-shot\" in the title of this paper is misleading.Zero-shot learning (ZSL) is a problem setup in deep learning where, at test time, a learner observes samples from classes which were **not** observed during training, and needs to predict the class that they belong to. However, in the context of this paper, training is conducted on a set $\\mathcal S$ of $n$-qubit states. From my perspective, this setup aligns more with traditional supervised learning rather than what is commonly referred to as \"zero-shot\" learning.\n- The authors claim that \"The key idea that allows us to obtain zero-shot inference is the following: a sequence of actions starting in state |\u03c8\u27e9 leading to the state |0\u27e9 yields the inverse of a circuit preparing |\u03c8\u27e9 starting from |0\u27e9\". However, I believe this concept is not original to this paper. For example, a similar idea was proposed in [1]. Yet, the authors have failed to cite it.\n- I doubt the authors overstated their results. It's important to distinguish between the preparation of arbitrary quantum states and the preparation of arbitrary stabilizer states. The latter is a more specialized and arguably less challenging problem compared to the general case of arbitrary state preparation. Stabilizer states, due to their specific properties and structure, may be more amenable to efficient preparation methods, which might not directly translate to the broader and more complex task of preparing any arbitrary quantum state.\n\n[1] Huang, Hsin-Yuan, et al. \"Learning shallow quantum circuits.\" *Proceedings of the 56th Annual ACM Symposium on Theory of Computing*. 2024."
            },
            "questions": {
                "value": "Please see the \"Weaknesses\" above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work utilizes a reinforcement learning PPO agent to solve the quantum state preparation problem. The key idea involves a potential based reward that is based on goalposts - earlier fidelity penalties are weighted less to encourage exploration and the preparation of partially suboptimal but intermediately necessary states. Training is conducted on stabilizer circuits, which can be completely classically simulated and only involve the H, CNOT and phase gate variants, the placement of which are used as actions. To reduce the set of possible end-target states the RL agent trains to reverse the state preparation, starting from the target state towards the |0> state. After training a one-shot model is evaluated on a study of random target states and a set of states relevant for quantum error correction is shown to yield circuits preparing valid states with fewer gates than analytical models. An analysis on the entangling entropy is given empirically and a upper bound is approximated via Monte-Carlo estimations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The ideas of this paper are presented understandable and mostly concise, the format is clean and the structure is logical. The background is explained very extensively. While I think the application of RL for stabilizer states is has fundamental issues, see below, I find the concept of reverse construction for a fixed end-state an interesting idea. I also think the extra effort for the study of entangling entropy and the attempt at a generalization bound, albeit quite loosely estimated, deserve praise and should be more common in empirical QRL papers."
            },
            "weaknesses": {
                "value": "Nonetheless, I think this paper has three major issues:\n\n- While fundamental explanations are generally good, the first 5 1/2 pages are simply explanation of fundamental QC / RL basics (excluding Figure 2). The first original section starts at the bottom of page 6, which is not efficient use of space. Much of Chapter 2 could be formulated much more concise or moved to an appendix. In general the information and result density is rather light for a A* publication with 10 pages. On the other hand the actual RL algorithm realization, and the training details should be made much more clear. From what I understand the observation is the whole stabilizer tableau (flattened probably?) and the actions are discrete choices over the set of all applicable gate-actions - or gate-target pairs for two-qubit gates. I am not clear if the addition of g_i in L343 is included somehow, or how often the actions are chosen. Once for each qubit for every layer? Some of the training details like hyperparameters are in the appendix, but are critical information for a self-contained paper and should be moved to the main-paper. (E.g. the training episodes, the max-T curriculum for increasing n, the fidelity relaxation and the fact that the otherwise-case of the reward was not even used as described in the paper.)\n- The goalpost potential-reward as part of the contribution feels quite over-engineered and I\u2019d argue this work is lacking a critical evaluation in the RL context. Similarly, while the reverse generation thought is interesting, without a comparison to the forward-generation training (zero to target) I find it hard to judge any significance. A study of the generation with a simpler fidelity based reward as is found in the literature (even the ones cited here, K\u00f6lle et al., Gabor et al., Wu et al.) would have helped, as would a test with a simpler, fixed step-based regularizer instead of the goalpost. A direct comparison of \u2018backward\u2019 vs. \u2018forward\u2019 generation training would also help support the claims made here.\n- Finally, I think the RL application for stabilizer circuits in particular has fundamental issues. While they are indeed used in QEC, the point and the reason for that is, that they are completely and very efficiently classically simulated. (The abstract of Aaronson & Gottesman mentions thousands of qubits.) In other words, any efficiency gain w.r.t. circuit size, generalization ability or else, could also be classically brute-forced. Without application to circuits with unsimulatable quantum-effects (including rotation gates, e.g.) efficient one-shot generalization is an interesting insight, but lacking overall impact in terms of QC relevancy.\n---\nMinor Issues:\n- L256 Typo: parameters: [a] the target \u2026\n- Inconsistent use of \u2018Fig.\u2019 and \u2018Figure\u2019 references."
            },
            "questions": {
                "value": "- This max { } function notation is confusing to me, L323 (e.g.) uses max{a : b}, L324 uses max{a | b}? How is this supposed to be read and understood?\n- Why was n=9 picked as the benchmark and not something higher, since stabilizer circuits can be so efficiently computed? L517 mentions that the he 9-qubit agent was not trained to convergence, why is that?\n- How were the observation and action choices implemented in detail?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}