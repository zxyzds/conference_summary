{
    "id": "4ndvumlZak",
    "title": "Closing the Gap between Neural Networks for Approximate and Rigorous Logical Reasoning",
    "abstract": "Despite the historical successes of neural networks, the rigour of logical reasoning is still beyond their reach. Though this is consistent with the dual-process model of the mind, which separates the model for rigorous reasoning from that for approximate associative thinking, a systematic explanation is still missing in the literature. We review syllogistic reasoning and its irreplaceable role in logic and human rationality, show existing neural networks cannot reach the rigour of syllogistic reasoning, and propose features that neural networks for rigorous reasoning should and should not have. (1) They should not use combination tables: We reduce syllogistic relations into part-whole relations, and translate the criterion of rigorous syllogistic reasoning into a deterministic process of Euler diagram construction in vector space. Then, we survey recent neural architectures (Siamese Masked Autoencoder) for reasoning part-whole relations in object completion and degrade the task into reconstructing Euler diagrams for syllogistic reasoning. We dissect Euler Net (EN), the Siamese (Masked) Autoencoder for syllogistic reasoning, and report three experiments, showing that EN, utilising a pre-designed combination table, cannot reach 100\\% accuracy for testing data without restriction. As Transformer's Key-Query-Value structure is a combination table, we conclude that LLMs and Foundation Models built upon Transformers cannot reach the rigour of syllogistic reasoning. (2) They should use non-vector embedding as computational building blocks: Transformer's oversmoothing prevents any neural architecture built upon them from reaching the rigour of syllogistic reasoning. We prove, however, that in the setting of part-whole relations, if neural networks use non-vector embedding as computational building blocks, they will not oversmooth. This work suggests a new way to close the gap between neural networks for approximate and rigorous logical reasoning.",
    "keywords": [
        "neural reasoning",
        "syllogistic reasoning",
        "Euler diagram",
        "composition tables",
        "rigorous reasoning"
    ],
    "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)",
    "TLDR": "this paper analyses features of neural networks for rigorous reasoning should and should not have.",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4ndvumlZak",
    "pdf_link": "https://openreview.net/pdf?id=4ndvumlZak",
    "comments": [
        {
            "comment": {
                "value": ">With the right data, any finite problem can be solved by training NNs by just learning a direct input-output map.\n>Limited depth models can solve any finite problem; hence they can solve the\u00a0finite\u00a0problem of syllogistic reasoning.\n\nNo. Here, we clearly argued that there is no right data to reach the rigour of symbolic level of syllogistic reasoning. We have explained this. It seems you neglected it. Again, taking two simplest syllogistic reasoning types: \n\n\u201call Greeks are humans. All humans are mortal. Therefore, ALL Greeks are mortal\u201d\n\n \u201call Greeks are humans. All humans are mortal. Therefore, SOME Greeks are mortal\u201d\n\nWhat is the direct input-output map? \n\n\n> That's not a formal definition.\u00a0\n\nThe formal definition of oversmoothing is abstracted into all output embedddings are coincided and described in the supplementary material."
            }
        },
        {
            "comment": {
                "value": "None of my concerns are properly addressed, so I'm keeping my score. \n\nTo be clear, I don't disagree much with the _opinion_ on the feasibility of learning general reasoning. The point is that this is not a debate on this problem. I act as a reviewer of the paper, which I do not believe is in a state that is ready for publication, as I believe the evidence presented is not strong enough and is not clearly described. \n\n> No. The problem is finite and appears simple, but cannot be solved by both weaker and complex architectures (without qualitative extensions).\n\nWith the right data, any finite problem can be solved by training NNs by just learning a direct input-output map.\n\n> Oversoomthing is defined in line 502-503 and also in line 507 again -- outputs converge to the same feature embedding.\n\nThat's not a formal definition. What is the notion of convergence? What are the outputs? What is the process with which they converge? As far as I know, oversmoothing converges in the depth, meaning limited-depth models do not oversmooth. Limited depth models can solve any finite problem; hence they can solve the _finite_ problem of syllogistic reasoning."
            }
        },
        {
            "comment": {
                "value": "We hope you have read our arguments and explanation. At this stage, we win the debates. Do you have further questions or anything hard to accept?"
            }
        },
        {
            "comment": {
                "value": "Our argument tells as long as Transformers and RNNs use (1) vector embeddings for concepts and (2) composition tables for mapping premises and conclusions, they will not reach the rigorr of the symbolic level of syllogistic reasoning. This is independent of what training data you use \u2013 the example that we gave in the last response shows there are no consistent training data for logical conclusion and logical consistent conclusion.  \n \n\u201cexisting expressivity results in my review on these architectures that go far beyond the complexity of this problem\u201d. \nExactly, they are beyond the complexity of syllogistic reasoning. That is the reason we choose syllogistic reasoning to explore whether and how traditional NN (including Transformers and RNNs) can reach rigorous reasoning. If they cannot for simple syllogistic reasoning, as the micro-world for rational reasoning, they will not for other rational reasoning. \n\nSyllogistic reasoning is special \u2013 it dominated the research of logic for 2000 years, and the research of rationality in psychology for over 100 years (till today, it is still not completely solved). Solving neural syllogistic reasoning is the first step to solve more complex reasoning problems. \n\n> This is just how you setup your NNs and data. Natural language inference and models tackling this is the task of judging whether a reasoning step holds and allows for multiple reasoning steps to be valid.\n\nNo. Syllogistic reasoning is atomic. We cannot break it into multiple steps, they cannot either. The current methods (using training data) cannot determine there is no model (unsatisfiability), and thus cannot separate satisfiable reasoning from valid reasoning and reach the rigor of symbolic level of logic reasoning. \n\n>. Up to you. \n\nNo, it is up to whether we want to be alchemists or modern engineers.\n\n>The problem you study is finite and can be solved by much weaker architectures than ones that exhibit oversmoothing (infinite / deep transformers or GNNs). The condition (which again is not well-defined in the paper) of oversmoothing is thus much too strong to claim impossibility results.\n\nNo. The problem is finite and appears simple, but cannot be solved by both weaker and complex architectures (without qualitative extensions). Oversoomthing is defined in line 502-503 and also in line 507 again -- outputs converge to the same feature embedding. \n\nNo matter whether the condition of oversoomthing is strong or not, our proof (the rigorous logic deduction) guarantees the results."
            }
        },
        {
            "comment": {
                "value": "> it (NN) is impossible to reach the rigour of the symbolic level of syllogistic reasoning.\n\nTransformers and RNNs are sufficiently powerful to represent logical reasoning **for fixed sizes of inputs** (which as far as I can tell is the problem you're studying in your paper). I shared existing expressivity results in my review on these architectures that go far beyond the complexity of this problem. Whether they will actually learn this is something else - that depends on your data and training setup (as I shared in the references in the comment). \n\n> However, both are valid syllogistic reasoning.\n\nThis is just how you setup your NNs and data. Natural language inference and models tackling this is the task of judging whether a reasoning step holds and allows for multiple reasoning steps to be valid. \n\n> We do not and shall not consider engineering aspects of existing neural architectures\n\nUp to you. The problem you study is finite and can be solved by much weaker architectures than ones that exhibit oversmoothing (infinite / deep transformers or GNNs). The condition (which again is not well-defined in the paper) of oversmoothing is thus much too strong to claim impossibility results."
            }
        },
        {
            "comment": {
                "value": "You totally misunderstand our argument. We do not argue it (NN) is impossible to learn syllogistic reasoning. What we argue is: it (NN) is impossible to reach the rigour of the symbolic level of syllogistic reasoning. \n\nThe symbolic level rigour means: for any syllogistic reasoning (two premises and one conclusion), the NN shall determine whether the three statements are satisfiable or unsatisfiable. \n\nFor a simple supervised NN, if it is confident for the reasoning \u201call Greeks are humans. All humans are mortal. Therefore, all Greeks are mortal\u201d, it will not be confident for the weaker reasoning  \u201call Greeks are humans. All humans are mortal. Therefore, SOME Greeks are mortal\u201d.  However, both are valid syllogistic reasoning. \n\nTo deduce the velocity to escape the earth gravity is approx 11km pro second, we do not need to examine the mechanics of bikes or cars (why they cannot). However, with this theoretical result, we can design rockets to escape the earth gravity and fly to the moon. \n\nThe proof is a complete logical deduction. We do not and shall not consider engineering aspects of existing neural architectures.   With this theoretical result, we conclude (after negating the theorem) that one condition for NN to avoid oversmoothing (which is well-defined in the literature and we also referenced) is to use non-vector embedding. This is a pre-condition to reach the rigour (symbolic level) of logical reasoning (this level is much higher than the level discussed in the papers you listed). Our conclusion is consistent with the other research that we have referenced."
            }
        },
        {
            "comment": {
                "value": "It is well known that NNs/transformers fail to reason out of distribution, see eg [1, 2, 3]. I don't debate this. My comment is on how this paper attempts to show that it is impossible to learn syllogistic reasoning, which I believe is poorly explained, incomplete and lacks evidence. \n\n> These single green circles are different from the standard inputs (two circles). \n\nYes, since they're OOD. It will act like the distribution it is trained on - this is not surprising at all. \n\n> Line 357: new randomly generated test data have different distributions from the training data.\n\nOf course I get that, but nowhere it is specified _how_ this distribution is different. \n\n> The theorem is solidly proved using region-based spatial logic. The proof shall be independent of model architectures.\n\nThe proof really needs more than this. There are hidden assumptions on what these neural architectures are and that they oversmooth (which again is undefined). \n\nI increased my confidence to reflect the rebuttal. \n\n[1] Zhang, Honghua, et al. \"On the paradox of learning to reason from data.\" Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. 2023.\n\n[2] Berglund, Lukas, et al. \"The Reversal Curse: LLMs trained on \u201cA is B\u201d fail to learn \u201cB is A\u201d.\" The Twelfth International Conference on Learning Representations.\n\n[3] Mirzadeh, Iman, et al. \"Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models.\" arXiv preprint arXiv:2410.05229 (2024)."
            }
        },
        {
            "comment": {
                "value": "The takeaway is as follows: \n\nCurrent deep-learning systems cannot and will not reach the rigour of logical reasoning, no matter what kinds of and how much training data we use. To achieve the rigour of logical reasoning, traditional neural networks shall do qualitative extensions, namely, to promote vector embedding to non-vector embedding. \n\nThe \u2018sketched proof\u2019 does not prove transformers cannot do syllogistic reasoning. \n\nLLMs work very well, in terms of language communication, but this does not follow that they can reason well. see the reference below. \n\nEvelina Fedorenko, Steven T. Piantadosi, and Edward A. F. Gibson (2024). Language is primarily a tool for communication rather than thought. In Nature.\n\nThis paper takes syllogistic reasoning as the micro-world of rationality and shows current deep-learning systems cannot and will not reach the rigour of syllogistic reasoning. \n\nSiamese architectures are used for object recognition and for syllogistic reasoning. In both cases, they achieve excellent results. However, The phenomena described in Section 4.1 raise the problem -- These single green circles are different from the standard inputs (two circles). Surprising is that the well-trained Euler-Net may automatically complete a single green circle into standard inputs. For object recognition, this is a great capability \u2013 it can recognise objects by observing its partial image (we do not say, partial images are out-of-distribution inputs). But, for reasoning, this capability shall not be allowed, because the neural networks shall not add new premises.  \n\nLine 357: new randomly generated test data have different distributions from the training data.\n\nLine 359: The motivation is to let Euler Net improve its performance by itself. It is not difficult to create an image with two circles, given two centre points and two radii. \n\nThe theorem is solidly proved using region-based spatial logic. The proof shall be independent of model architectures."
            }
        },
        {
            "comment": {
                "value": "We are not sure whether we correctly understand your question.  One-hot representation reduces the amount of training data, compared with using image representation."
            }
        },
        {
            "comment": {
                "value": "Yes. The phenomena described in Section 4.1 are entirely different from the out-of-distribution scenarios. These single green circles are different from the standard inputs (two circles); in this way, they are out-of-distribution and will perform incorrectly, as we expect. Surprising is that the well-trained Euler-Net may automatically complete a single green circle into standard inputs. For object recognition, this is a great capability \u2013 it can recognise objects by observing its partial image (we do not say, partial images are out-of-distribution inputs). But, for reasoning, this capability shall not be allowed, because the neural networks shall not add new premises. \n\nIf using (non-)vector or vector feature embeddings and the output embeddings oversmooth, then the converged output embedding must be a single vector feature embedding (a point). Or, put it this way: if feature embeddings are spheres with radii >=0, and output embeddings oversmooth, then their radii = 0. This means if we restrict radii > 0, oversmoothing will not happen. \n\nAfter researchers promote vector embeddings into spheres and introduce the method of reasoning using model construction, neural models achieve rigorous syllogistic reasoning without training data, see, Sphere Neural-Networks for Rational Reasoning https://arxiv.org/abs/2403.15297"
            }
        },
        {
            "comment": {
                "value": "Promoting traditional vector embeddings into manifold embedding is the first step. The second step is to introduce the method of reasoning as model construction, see, Sphere Neural-Networks for Rational Reasoning https://arxiv.org/abs/2403.15297\n\nHere, we show the limitations of (1) the vector representation, and (2) the method of reasoning through combination tables. Both prevent neural networks from achieving rigorous reasoning, which goes beyond the statistic metrics -- more data experiments will not help.  Three statements being unsatisfiable (contradictory) is a topic of possibility, not probability -- no training data for deciding unsatisfiability."
            }
        },
        {
            "summary": {
                "value": "This paper proposes a task that converts syllogism into subset relations and then generates an image dataset that visualizes the subset relations and evaluates neural networks. The authors show in their experiments that although Euler Networks can learn part-whole relations between two entities, it cannot learn complex combinations of these relations, resulting in a lack of validity in the equivalent syllogism reasoning. Furthermore, the authors hypothesized that NNs should use one-hot representation to acquire the rigorous reasoning ability."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper presents an important question that the community really cares about.\n- The author shows the equivalence between syllogism reasoning and part-whole relations, and converted reasoning task into a visual prediction problem, which is interesting to me."
            },
            "weaknesses": {
                "value": "- This paper still lacks enough experiments to support the authors' claims. Why would a one-hot representation save neural nets in reasoning soundness issues?\n- The presentation of this paper could be further improved. The structure of it now looks more like a technical report. It lacks of figures and charts to present the experimental results.\n- The discuss is high-level, while the technical detail or insufficiency of the compared methods are not discussed enough."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors highlight the limitations of neural networks, including large language models (LLMs), in achieving rigorous syllogistic reasoning, which is essential for logic and human rationality. They argue that these networks should avoid combination tables and instead use non-vector embeddings to prevent oversmoothing. The paper reviews the Siamese Masked Autoencoder and presents experiments demonstrating that models relying on combination tables cannot attain 100% accuracy in syllogistic tasks. However, using non-vector embeddings as computational building blocks can help neural networks avoid oversmoothing. This work aims to bridge the gap between neural networks for approximate and rigorous logical reasoning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The authors substantiate their claims with experimental results, showcasing the shortcomings of existing models, such as the Siamese Masked Autoencoder, in achieving high accuracy in syllogistic reasoning.\n- The paper opens avenues for further exploration, encouraging researchers to develop architectures that can effectively address rigorous reasoning tasks."
            },
            "weaknesses": {
                "value": "The authors claim three main contributions, and there are corresponding weaknesses for each:\n\n   - **Contribution 1:** The authors conduct an experiment in Section 4. However, the experiments in Sections 4.1 and 4.2 appear to primarily test neural models' performance on out-of-distribution inputs. The poor performance of neural models on out-of-distribution inputs is already well-documented, which limits the novelty of this contribution.\n\n   - **Contribution 2:** The use of combination tables is discussed in Section 4.3, but this section is confusing. For example, the authors state that the combination table only generates the conclusion \"all V are U\" is not enough, since it misses the conclusion \u201csome V are U.\u201d However, the statement \"all V are U\" clearly describes a part-whole relationship, and \"some V are U\" can be derived from \"all V are U.\" The authors did not explain why this senario is worse.\n   \n   - **Contribution 3:** The authors discuss this in Section 5 (lines 502-519), but the proof is unclear. For example, it's unclear how the two theorems prove \"using non-vector feature embedding to avoid oversmoothing\". Additionally there lacks empirical studies to support it."
            },
            "questions": {
                "value": "1. Are the phenomena described in Section 4.1 distinct from typical out-of-distribution scenarios?\n\n2. In Section 5 (lines 502-519), what is the relationship between using (non-)vector feature embeddings and output embeddings being points?\n\n3. Given that symbolic approaches are effective for syllogistic reasoning, why is it necessary for neural models to also support rigorous reasoning? In Section 2.1 (line 181), the authors argue that \"symbolic approaches neither explain how symbols emerge from our neural minds nor capture the ways humans reason in daily life.\" Can neural models genuinely achieve these objectives?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper discusses the \"dual-process\" theory of mind, highlighting the distinction between fast, intuitive thinking  and slower, more deliberate thinking. It conclude that LLMs and\nFoundation Models built upon Transformers cannot reach the rigour of syllogistic\nreasoning. \nThe article proposes a method of transforming syllogistic relationships into \"part-whole relationships\" and suggests using non-vector embeddings instead of traditional vector embeddings to avoid the problem of \"oversmoothing.\" Oversmoothing can cause the outputs of neural networks to converge to similar embeddings, thereby affecting the accuracy of reasoning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper attempts to analyze and study the reasoning capabilities of transformers, which is of great value. Additionally, the methods proposed in this paper possess certain innovative and theoretical significance."
            },
            "weaknesses": {
                "value": "1. This work lacks experimental validation and seems to be not fully complete.\n\n2. The article is not clearly written. The abstract and introduction are somewhat verbose, and the key innovations and objectives are not clearly defined."
            },
            "questions": {
                "value": "In fact, enhancing the inference capabilities of neural networks is a very challenging task. Will merely changing traditional vector embeddings yield significant improvements, or can it lead to substantial advancements?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No Ethics Concerns."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors study whether current neural networks can perform robust syllogistic reasoning via Euler diagrams, showing that they fail in very specific aspects, and conclude with arguments stating that neural networks need to go beyond vector embeddings to solve rigorous reasoning."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is fairly well written, with some clear figures."
            },
            "weaknesses": {
                "value": "I found it hard to follow what the contributions of this paper are. There are a few results that seem simple, arbitrary, poorly explained, and relevant only to a single network architecture. It is not clear to me what I should take home from these experiments. \n\nThe 'sketched proof' which is supposed to prove that transformers cannot do syllogistic reasoning also falls short: It assumes that they oversmooth, which only happens for transformers with many layers (the theoretical results are for the infinite-depth setting). If this happened consistently in practical transformer models, there is no chance LLMs could work as well as they do (as also Dovonon 2024 argues and shows, which is cited). \n\nTogether, this paper only provides meagre evidence for the infeasibility of syllogistic reasoning. Then the authors argue that different concept embeddings are needed, but do not compare (either theoretically or empirically) to the vector case, except for referring quickly to related work."
            },
            "questions": {
                "value": "- What is the motivation for specifically studying this Siamese Masked Autoencoder model? I suppose that this model does not use specific embeddings for each object (unlike models in object-centric learning, involving eg slot attention [1] or the method specific for this task as cited [2])\n- Line 357: \"We fed new randomly generated test data' How is this data different?\n- Line 359: What's the motivation for Euler Net version 2? The description of this method is extremely difficult to follow and incomplete. How does a model 'generate' input images?\n- 4.1, first paragraph. This lacks in details. Furthermore, it's well known that standard NNs are not adversarially robust. This connection is missing. \n- 4.2: I did not understand the point of this experiment. Of course a model will not be able to say anything meaningful about incorrect input data that we never defined how to respond to, especially if it's not designed for out of distribution detection. \n- Line 428: This blanket statement is highly overclaiming these results. This is about misspecification - not a lack of learning capability. \n- 4.3: It is not clear to me how these combination tables are defined from a neural network point of view. Furthermore, this result again comes from the design of the neural network. If it's allowed to output multiple answers (for instance like an LLM would be able to), it may give all syllogistic conclusions. \n- 479 \"More powerful than vanilla RNN, LSTM\": From a theoretical perspective, this is hard to claim. RNNs (with unbounded time) are Turing Complete [3]. Similar results exist for Transformers, but these require an infinite 'scratchpad / chain of thought' [4]. I suppose this 'powerful' refers to an empirical interpretation, but this should be clarified. \n- Theorem 1 is unclear and informal, and does not properly state its assumptions. What is oversmoothing? Output embeddings? \"will be points\"? Of course output embeddings are points. What are the assumptions on the model architecture? A quick look at the proof did not help me understand these questions. This certainly doesn't constitute a 'rigorous proof\" (Line 531)\n- Similarly for Theorem 2, I have no idea what \"If the output embeddings are not points\" would mean. \n\n[1] Locatello, Francesco, et al. \"Object-centric learning with slot attention.\" Advances in neural information processing systems 33 (2020): 11525-11538.\n\n[2] Wang, Duo, Mateja Jamnik, and Pietro Lio. \"Abstract diagrammatic reasoning with multiplex graph networks.\" arXiv preprint arXiv:2006.11197 (2020).\n\n[3] Nowak, Franz, et al. \"On the representational capacity of recurrent neural language models.\" arXiv preprint arXiv:2310.12942 (2023).\n\n[4] Lena Strobl, William Merrill, Gail Weiss, David Chiang, Dana Angluin; What Formal Languages Can Transformers Express? A Survey. Transactions of the Association for Computational Linguistics 2024; 12 543\u2013561."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}