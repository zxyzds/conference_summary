{
    "id": "qmqRdxQcMA",
    "title": "Enhancing Cost Efficiency in Active Learning with Candidate Set Query",
    "abstract": "This paper introduces a cost-efficient active learning (AL) framework for classification, featuring a novel query design called candidate set query. Unlike traditional AL queries requiring the oracle to examine all possible classes, our method narrows down the set of candidate classes likely to include the ground-truth class, significantly reducing the search space and labeling cost. Moreover, we leverage conformal prediction to dynamically generate small yet reliable candidate sets, adapting to model enhancement over successive AL rounds. To this end, we introduce an acquisition function designed to prioritize data points that offer high information gain at lower cost. Empirical evaluations on CIFAR-10, CIFAR-100, and ImageNet64x64 demonstrate the effectiveness and scalability of our framework. Notably, it reduces labeling cost by 42% on ImageNet64x64.",
    "keywords": [
        "Active Learning",
        "Conformal Prediction",
        "Label efficient learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "This paper presents a cost-efficient active learning framework for classification featuring a novel query design.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=qmqRdxQcMA",
    "pdf_link": "https://openreview.net/pdf?id=qmqRdxQcMA",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes an active-learning framework called Candidate Set Query (CSQ) to reduce the size of possible classes during annotation process, minimizing the search space and lowering the labeling cost at the same time. Furthermore, the authors leverage conformal prediction to produce accurate candidate sets, and introduce an acquisition function that exploits data points with high information gain."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper introduces a novel approach called Candidate Set Query (CSQ), which effectively reduces labeling costs by narrowing down the candidate classes presented to annotators, thereby minimizing annotation time.\n2. The proposed method leverages conformal prediction to dynamically produce accurate candidate labels based on a cost-efficient data acquisition function. This function prioritizes samples with high information gain, leading to greater efficiency and reduced labeling costs.\n3. The framework demonstrates strong performance across multiple image recognition datasets, consistently outperforming baseline methods by a significant margin.\n4. The authors provide comprehensive ablation studies to thoroughly validate the effectiveness of all components."
            },
            "weaknesses": {
                "value": "1. The rationale behind the cost-efficient acquisition function in Eq. (8) needs to be further explained. Additional motivation and explanation for this function are recommended.\n2. As shown in Fig. 9a, the performance is sensitive to the hyperparameter d. Providing guidelines for setting this parameter to an appropriate range on different datasets would be beneficial.\n3. In realistic scenarios, the samples with high uncertainty waiting to be annotated can be divided into two groups based on their probability distributions on categories, high confidence on several specific classes or low confidence on almost all classes. The proposed framework might only be suitable for the former case. As for the latter one, an intuitive solution is to sift out all candidate labels with low prediction probabilities, such as less than 0.1 * 1/C where C is the number of categories. So it\u2019s suggested to conduct more experiments to evaluate the performance of this approach under this scenario.\n4. A smaller candidate label set means model have higher certainty for samples, which seems to be against the motivation of selecting the most uncertain samples in active learning. Despite the proposed method minimizing the labeling cost by narrowing down the label space, the information gain is limited. It\u2019s suggested to plot the graph of how performance varies with the number of queried samples.\n5. There is a minor formatting issue in Fig. 5c, where certain data points from the top-1 prediction fall outside the scale range of the graph.\n6. Line 274 contains a typo: \u201ccalcuclate\u201d should be corrected to \u201ccalculate.\u201d"
            },
            "questions": {
                "value": "Please refer to the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a cost-efficient active learning strategy using conformal predictions. Instead of letting the annotator choose from all possible labels, a candidate set of fewer labels is given. The paper uses a log order expected cost function and shows the improved efficiency in terms of actual labeling cost."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The content of the paper is well presented.\n2. The paper studies the cost of AL query in a more realistic way and proposes a solution for reducing the cost by candidate set query. \n3. The candidate set is formed by conformal prediction and the candidate labels are related to the expected information gain with cost considerations."
            },
            "weaknesses": {
                "value": "The proposed method still depends on the conformal prediction and the calibration set to determine the confidence level. It is a realistic solution however not guaranteed to be theoretically sound. The convergence can not be obtained in a proper label complexity analysis. Similarly, the labeling cost assumption in Theorem 3.1 is only a rough approximation."
            },
            "questions": {
                "value": "1. Is random selection effective for the calibration set?\n2. Is it possible that the candidate set obtained from eq(5) is empty?\n3. Is there any guarantee that using the quantile from the previous round would work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the Candidate Set Query (CSQ) framework which aims to improve the cost efficiency in active learning (AL) tasks. CSQ reduces the size of the set of candidate classes which reduces the search space, leveraging conformal prediction to dynamically adjust the optimal candidate set size across successive AL rounds. The paper also proposes an acquisition function that considers the ratio of information gain vs labeling cost to help prioritize samples to label. The author then benchmarked this framework across multiple datasets and empirically demonstrate the reduction in labeling costs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The motivation for this paper is clear, and the paper proposes a novel framework of high significance \n2. The paper presents a solid theoretical framework that is thoroughly explained and mostly straightforward to follow\n3. The framework is benchmarked across 3 well-known datasets, empirically demonstrating the effectiveness of the method \n4. Thorough ablations studies were conducted to highlight the significance of each component of the framework"
            },
            "weaknesses": {
                "value": "1. The benchmarks are conducted on very similar datasets (CIFAR-10, CIFAR-100, and ImageNet64x64 are all image classification datasets), and also only compares against a small number of baseline AL methods. It is unclear if the results will generalize well across different datasets and domains, and if more advanced underlying AL acquisition methods are used\n2. The paper does not consider the implication of real-world datasets, such as those containing label noise, imbalance classes etc might impact the performance of CSQ. More experiments could help identify CSQ\u2019s robustness when it comes to noisy annotations, as it could lead to inefficient training and candidate sets with lower quality\n3. CSQ relies on several hyperparameters, however there is limited justification on how to properly optimize the hyperparameter $d$, making it difficult to apply the method in new datasets"
            },
            "questions": {
                "value": "1. Have the authors considered any special handling of outlier or anomalous datapoints, which tend to have have high uncertainty, and could lead to inefficient construction of the candidate set, acquisition and labeling\n2. Have the authors considered the risk of overfitting and drift of confidence scores across successive AL rounds and how CSQ could incorporate some steps to address it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors proposed a cost-efficient active learning (AL) framework for classification, featuring a novel query design called candidate set query (CSQ). CSQ narrows down the set of candidate classes likely to include the ground-truth class and leverages conformal prediction to dynamically generate small yet reliable candidate sets. Experiments on several datasets demonstrate the proposed CSQ is effective."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed CSQ present the annotator with an image and a narrowed set of candidate classes that are likely to include the ground-truth class, which reduces labeling cost by minimizing the search space the annotator needs to explore. The various modules in the entire paradigm are relatively mature."
            },
            "weaknesses": {
                "value": "1. The explanation of \"empirical quantile\" in Section 3.2, titled \"Candidate Set Construction from Conformal Prediction,\" is not very clear. Could the authors provide further clarification, specifically regarding the insights that Conformal Prediction offers for method design and its role in the proposed approach?\n\n2. Is Table 1 in the experiments derived from the cost-efficient acquisition function?\n\n3. Does the Active Learning (AL) approach include corresponding real-world datasets? Would it be possible to include more experimental results using real-world datasets?"
            },
            "questions": {
                "value": "Please see the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}