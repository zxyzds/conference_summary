{
    "id": "2U8owdruSQ",
    "title": "Has the Deep Neural Network learned the Stochastic Process? An Evaluation Viewpoint",
    "abstract": "This paper presents the first systematic study of evaluating Deep Neural Networks (DNNs) designed to forecast the evolution of stochastic complex systems. We show that traditional evaluation methods, such as threshold-based classification metrics and error-based scoring rules, assess a DNN's ability to replicate the observed ground truth (Observed-GT) but fail to measure the DNN's learning of the underlying stochastic process. To address this gap, we propose a new property called *Fidelity to Stochastic Process (F2SP)*, representing the DNN's ability to predict the *Statistic-GT*\u2014the ground truth of the stochastic process\u2014and introduce an evaluation metric that exclusively assesses fidelity to Statistic-GT. We formalize F2SP within a stochastic framework and establish criteria for validly measuring it. We demonstrate that the Expected Calibration Error (ECE) satisfies the necessary conditions for evaluating fidelity to Statistic-GT. Empirical experiments on synthetic datasets\u2014including wildfire, host-pathogen, and stock market models\u2014show that ECE exclusively measures F2SP. We further extend our study to real-world wildfire data, highlighting the limitations of conventional evaluation and discussing the practical utility of incorporating F2SP into model assessment. This work offers a new perspective on evaluating DNNs in stochastic complex systems by emphasizing the importance of capturing the underlying stochastic process.",
    "keywords": [
        "evaluation",
        "deep neural network",
        "stochasticity",
        "complex systems",
        "forecasting"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "A novel evaluation criterion to assess whether DNNs modeling stochastic complex systems have learnt the underlying stochastic process",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=2U8owdruSQ",
    "pdf_link": "https://openreview.net/pdf?id=2U8owdruSQ",
    "comments": [
        {
            "summary": {
                "value": "the metric of expected calibration error is introduced and studied as a way to capture fidelity of a learned representation to an underlying stochastic process (rather than a single realization of that process, as with typical metrics like AUC or MSE)."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Great paper, wonderfully practical and insightful; I've been looking for something like this for 5+ years! Nice eval on real-world data.\nI started writing a thing I would like to you add and then discovered it was already in the paper (long horizon behaviour)"
            },
            "weaknesses": {
                "value": "While overall the paper is very clear, some of the captions and explanations of the experiments/insights from them and how they tie to the figures could be improved. \nSome specifics:\n- first\u00a0fig should say what you\u00a0mean by realization, and F2R and F2SP should be bolded (not ital) to make them easy to find in the text. Observed GT should be explained a bit more, or maybe it would be enough to move the sentence currently after F2R to be the second sentence of the paragraph.\n\u00a0- Fig 5 is unclear to me. What is the data, what is S-level, why is it \"good\" that the 20 vs 10 lines are far apart? All of this should be clear from the caption\n - the clarity wanes a bit as the paper goes on, and it's a bit confusing that you call it ECE vs. F2SP vs Statistic-GP. Do these different namings really serve something? It could be a lot more clear if you just have one naming."
            },
            "questions": {
                "value": "- I don't understand the second part of the critical question, \"is it encountering different stochastic\u00a0behaviours\" (different from what)? how is the \"differentness\" relevant?\n- While it's pretty clear to me how to use this immediately in my work, I think anyone who wasn't already aware they wanted exactly this might struggle. Could you provide something like a \"practical users guide\" for non-domain experts?\n - if the clarity of the plots can be improved, the naming of the stat/metric you're introducing, and improve it's \"usability\" to the community, I would be happy to upgrade my score. You've done great work and this would bring the paper to the level it deserves."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a study evaluating deep neural networks (DNNs) within stochastic complex systems, emphasizing the importance of Expected Calibration Error (ECE) in measuring fidelity to stochastic processes. The findings are validated through multiple experiments and comparisons."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The topic of evaluating DNNs within stochastic complex systems is both intriguing and important.\n\nIn the primary evaluations, the author conducted experiments across various settings, including different DNN architectures, comparisons with multiple evaluation metrics, and diverse simulation tasks.\n\nThe main text clearly explains the difference between ECE in classical assessment and stochastic process settings."
            },
            "weaknesses": {
                "value": "The paper is somewhat difficult to follow. For example, providing a brief introduction to the structure of each section would enhance clarity, particularly in Sections 2 and 3. Additionally, it is difficult to grasp the main messages conveyed by the table in Figure 2(b). Furthermore, in lines 229\u2013240, the macro-level concept is introduced abruptly, which may disrupt the clarity and readability of the main text.\n\nThe main findings' practical applicability appears limited. In real-world scenarios, data generally provides only a single observed outcome centered on observable ground truth (line 117). Since the primary evaluation is simulation-based, the controlled stochasticity falls short of capturing real-world complexity. The Statistic-GT is basically derived by normalizing the frequency of target state occurrences across multiple Monte Carlo simulations.\n\nMinors:\n\nM1. The original text for the abbreviation RV is not given.\n\nM2. In Table 1, what about the possibility of recovery in the Host-Pathogen problem?\n\nM3. In line 152, maybe consider using an alternative symbol for Moore neighborhood, instead of $\\mathcal{N}$ (normally representing Gaussian distribution)."
            },
            "questions": {
                "value": "A question arises regarding Figure 1: Can ECE be an effective metric for measuring F2R compared to other available metrics? Figure 1 suggests that the answer may be *no*.\n\nAn important indicator that ECE is a reliable measure is its diagonal pattern, showing low scores only when training and test S-Levels align, as illustrated in Figure 4. Could the authors provide theoretical insights to support this indicator?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a study on evaluating deep neural networks designed to forecast the evolution of stochastic complex systems. The authors identify a gap in traditional evaluation methods\u2014such as threshold-based classification metrics and error-based scoring rules\u2014which focus on a model's ability to replicate observed ground truth but fail to assess how well the model has learned the underlying stochastic process. To address this issue, they introduce a new property called Fidelity to Stochastic Process, representing the DNN's ability to predict the statistical ground truth of the stochastic process.\n\nThe paper proposes using the Expected Calibration Error (ECE) as an evaluation metric that satisfies the necessary conditions for assessing fidelity to statistical ground truth. This work underscores the importance of capturing the underlying stochastic processes in deep neural networks  evaluations for complex systems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper makes a significant contribution by introducing the concept of Fidelity to Stochastic Process (F2SP), a novel evaluation criterion specifically designed to assess a DNN's ability to learn the underlying stochastic interactions in complex systems.\n\nThe authors provide a rigorous formalization of F2SP within a stochastic framework, establishing clear criteria for its valid measurement. The use of Expected Calibration Error (ECE) as an evaluation metric is well-justified."
            },
            "weaknesses": {
                "value": "I found it hard to read the paper because there was a lack of consistency in the acronyms, the authors would redefine them in several parts of the text again and again. I addressed my comments on text in the questions section. \n\nIn the tables, the best neural networks based on each criterion are not highlighted, which makes it difficult to the reader to infer and correlate the arguments in the text. I addressed my comments on text in the questions section. \n\nThe focus of the paper is primarily on binary or discrete prediction tasks, leaving out regression tasks where the definition of calibration is more complex. While the authors acknowledge this and suggest it as an area for future work, the current scope limits the immediate applicability of the findings to a broader range of problems involving continuous outcomes.\n\nAdditionally, the use of the NDWS dataset, which is restricted to next-day predictions, prevents the assessment of ECE over longer time horizons, which are common in many complex systems. Could you elaborate on how future work might address this limitation? \n\nThe paper highlights the lack of open-source complex system datasets as a barrier to broader validation. Are there any ongoing initiatives or plans to develop, collect, or standardize such datasets?"
            },
            "questions": {
                "value": "L50: Is --> is (lowercase)\nFig1: no need to write the whole name, you can use acronyms because they're already defined in the text, however MSE is not defined at this point.\nL88: fidelity to realization --> F2R (it was already defined previously, so you can use the acronym)\nL99: the notation of the dimension of the real vector O_t is confusing, what is (R^n)^(H x W), is n = H x W? If so, make that explicit.\nTable 1: some rows end with full stop, other don't. Please make it consistent. Either all with or all without.\nI find it odd to place Figures in columns as Figure 1 (which has a large top white margin) and Figure 3. I would suggest column figures into one row figure with multiple subfigures as you did with Figure 2. \nL201: Isn't the indicator variable already defined as B_t in L99? Why defining again with different notation?\nL298: MSE already defined in text previously, no need to write the whole name again.\nL516: ECE already defined in text previously, no need to write the whole name again.\nTable 2 and Table 7: highlight the best performing DNNs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work offers a new perspective on evaluating DNNs in stochastic complex systems by emphasizing  the importance of capturing underlying the stochastic process. Traditional evaluation methods assess the DNN\u2019s ability to replicate the observed ground truth but fail to measure the DNN\u2019s learning of the underlying stochastic process. This paper proposes a new property called Fidelity to Stochastic Process, representing the DNN\u2019s ability to predict the ground truth of the stochastic process, and introduces an evaluation metric that exclusively assesses fidelity to  the ground truth of the stochastic process. The Expected Calibration Error is used to evaluate the fidelity to ground truth of statistic process. Empirical experiments on synthetic datasets (including wildfire, host-pathogen, and stock market models) and real-world wildfire data are used to show the measurement of fidelity to stochastic process by Expected Calibration Error."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper offers a new perspective on evaluating DNNs by considering DNNs as stochastic processes and uses a widely used criteria in Bayesian Deep Learning application to assess the fidelity to stochastic process. This work clearly explains the Expected Calibration Error is used to assess DNN modes in three synthetic cases and one real world case."
            },
            "weaknesses": {
                "value": "This paper is well organized and well written, several minor issues should be addressed: (1) The explaination of figures is not sufficient, e.g., in Figure 2 (1), the label for x-axis is not specified (I guess it is time?), either add a label or explain it in the captions. Same problems also exist in Figure 4. (2) This work examines ECE on three synthetic environments (forest fire, host-pathogen and stock market models) and a real world wildfire spread dataset. I can tell that these datasets are all multivariate either for classification or regression. Maybe due to the limit of pages, the authors didn't include the experiments on images. I suggest the authors add some discussions or comments in the paper."
            },
            "questions": {
                "value": "As mentioned in the \"Weaknesses\" part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel stochasticity-compatible evaluation strategy for assessing existing models in the context of complex systems. The author justifies the Expected Calibration Error (ECE) as suitable for assessing the model fidelity of stochastic systems through both simulation environments and real-world data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Evaluating model fidelity on the stochastic system is significant and has wide applications.\n2. The paper is well-motivated and both the dataset and experiments are thorough."
            },
            "weaknesses": {
                "value": "1. Although the author attempts to explain the difference between their work and ECE in deep learning in Lines 282-288, it appears to me this work is still a direct application of using ECE to evaluate the model performance on a stochastic system. The author is encouraged to discuss more in-depth about the distinction between ECE in the proposed method (stochasticity comes from evolving in the environment, aka, Statistic-GT) and ECE in previous works (stochasticity comes from the output distribution).\n2. In Lines 243-244, the author claims that Statistic-GT is more stable than classification-based metrics, but I could not find any evidence related to calculating ECE on Statistic-GT is less sensitive to the system variance than MSE. Is there any theoretical support for using ECE over MSE on stochastic systems with different noise levels and could the author clarify it a bit more?"
            },
            "questions": {
                "value": "I'm curious about how the author evaluates ECE at time $t$ based on Statistic-GT $P_{t}$. Do we have to simulate it again from $t=0$ for $N$ times or we can sample states from $t-1$ and go forward $N$ times (the system is Markov)? Can we still apply ECE on Statistic-GT when the system is not Markov?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}