{
    "id": "FyMjfDQ9RO",
    "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
    "abstract": "Syllables are compositional units of spoken language that play a crucial role in human speech perception and production. However, current neural speech representations lack structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised model that regresses features on syllabic segments distilled from a teacher model which is an exponential moving average of the model in training. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) syllabic units better suited for lexical and syntactic understanding. We also train token-to-speech generative models with our syllabic units and show that fully intelligible speech can be reconstructed from these tokens. Lastly, we observe that categorical perception, a linguistic phenomenon of speech perception, emerges naturally in our model, making the embedding space more categorical and sparse than previous self-supervised learning approaches. Together, we present a novel self-supervised approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
    "keywords": [
        "Self-supervised learning",
        "speech representation",
        "spoken language model",
        "syllable discovery",
        "speech segmentation",
        "speech tokenization"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We present a novel self-supervised approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=FyMjfDQ9RO",
    "pdf_link": "https://openreview.net/pdf?id=FyMjfDQ9RO",
    "comments": [
        {
            "summary": {
                "value": "The paper presents an innovative self-supervised learning (SSL) method that converts speech into syllable-based embeddings. The authors employ a range of evaluation metrics\u2014such as syllable detection and discovery, speech intelligibility, coding efficiency, sWUGGY, and sBLIMP\u2014to demonstrate the effectiveness of their approach. The approach provides a linear-time syllable segmentation algorithm and efficient speech tokenization with an average of 4.27 tokens per second."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The framework is well-motivated, particularly due to the efficiency of its tokenization algorithm, which helps manage the exponentially increasing compute costs associated with transformer-based models in downstream tasks."
            },
            "weaknesses": {
                "value": "While the approach is well-motivated as an efficient alternative for speech tokenization, achieving an average rate of 4.27 tokens per second, the evaluation metrics used don\u2019t fully justify the applicability of these tokens for down stream tasks, as shown in Table 9. It would be beneficial for the authors to moderate some claims, such as:\n\n\t\u2022\tthat syllabic units are better suited for lexical and syntactic understanding\n\t\u2022\tand that these units are better suited for SLU\n\nInstead, the focus could remain on highlighting the promising initial results regarding the efficiency of the speech tokenization and interpretability of the tokens, as further work is needed before demonstrating the superiority of syllable-based tokens in downstream tasks.\n\nI would suggest that the authors add more details in Section 3.1 and be more comprehensive. \n\nAlso, I would strongly suggest that the authors simplify some of the very long sentences throughout the paper, e.g., \n\n\"The target segment labels are continuous embeddings averaged across frames within each segment that are found by an unsupervised segmentation algorithm\""
            },
            "questions": {
                "value": "- The model was trained to explicitly detect syllables. Wouldn't it perform the best using the proposed metric for syllable detection and discovery? Would metric be a biased one?\n\n- On the author's observation that the articulatory reconstruction and intelligibility increase with finer clustering granularity,  Does that indicates that speech requires more fine-grained representation to performs well in spoken language understanding? \n\n- The author emphasized that other SSL tokens lack structure. Though, other SSL might not have syllable-based structure, but they have sub-phonemic structure"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the problem of tokenizing speech waveforms into discrete units that are suitable for tasks such as speech language modeling. Current approaches to this problem typically cluster (e.g. with K-means) the intermediate representations of a self-supervised transformer (such as HuBERT or WavLM), then run-length encode these clusterings to derive discrete tokens for training a language model over speech units. Two problems with this approach are that 1) the K-means units have been found to represent phonetic/sub-phonetic information and thus have a very high temporal rate, which makes Transformer-based speech language models difficult to scale from a computational perspective and 2) the units do not capture higher-level linguistic abstractions (e.g. words or morphemes) which could make even higher level abstractions (such as semantics) more difficult to learn, preventing current speech LMs from unlocking emergent abilities such as in-context learning.\n\nThe paper builds upon a previous approach from the literature, namely SD-HuBERT. The paper uses SD-HuBERT to extract syllable-like segmentations of speech waveforms (without the need for ground-truth annotations). It then fine-tunes an SD_HuBERT model with a teacher-student knowledge distillation objective, where the teacher is an exponential moving average of the student model whose outputs are average pooled features within each syllable-like segment.\n\nThe paper presents experimental results showing state-of-the-art performance on syllable segmentation and clustering, and demonstrates that the learned representations exhibit categorical discrimination (\"lest\" vs. \"rest\") that mirrors that of humans. It also shows the units can be used to resynthesize intelligible speech and when used to train a speech unit language model achieve strong performance on standard metrics (sWUGGY/sBLIMP)"
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "-The paper investigates an important and timely topic, namely speech tokenization focused on learning linguistically-motivated large granularity units.\n\n-The proposed method for learning the units is simple and effective.\n\n-The experiments are broad, covering categorical perception to syllable segmentation to resynthesis to spoken language understanding tasks with speech LMs\n\n-The experimental results are strong on all tasks evaluated."
            },
            "weaknesses": {
                "value": "The main weakness from my perspective is that I would have liked to see a more in-depth analysis of the resynthesis results in terms of naturalness. It is expected that when moving from low-level acoustic units to higher level syllable-like units, we may lose a lot of the low-level details that are unnecessary for higher level understanding but are needed to represent highly naturalistic speech. However, when building speech LMs we often want to re-synthesize their outputs so they may be played back to the user (e.g. when using the speech LM as a dialog agent) so it is important to understand how well the proposed units work for that scenario."
            },
            "questions": {
                "value": "Did the authors conduct any naturalness evaluation of the resynthesized speech from the proposed units? How did it compare to existing approaches (such as using HuBERT K-means units)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Sylber, a self-supervised knowledge distillation and bootstrapping method to learn syllable-level speech features. With better features, a faster segmentation algorithm with O(n) complexity can be utilized (compared to O(n^2) algorithms). Better syllable features also improve downstream performance on speech understanding and codec efficiency."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors presented extensive experiments to demonstrate the strengths of Sylber, covering syllable segmentation, spoken language understanding, and audio codec.\n2. The propose learning approach is intuitive and having linear time segmentation algorithm would also greatly facilitate utilization of syllable level features into downstream tasks such as spoken language modeling with syllable level tokens.\n3. Authors also presented interesting qualitative analysis in Section 4 connecting syllable representations to the categorical perception in rhymed syllable pairs."
            },
            "weaknesses": {
                "value": "1. Some details and ablation studies are missing:\n    1. The proposed method regresses non-speech frames to zero. What model / method was used to determine whether a frame is speech or non-speech?\n    2. The authors claim that better features motivate the design of the linear-time greedy segmentation algorithm. For SD-HuBERT and Sylber, how much does that segmentation algorithm affect the performance? It would be clear if the authors can report Table 1 results with all combinations of (SSL feature, segmentation algorithm).\n2. The authors should move the slightly negative results in A.3 to the main paper. It is crucial to discuss not just the advantages but also the caveats of the proposed methods."
            },
            "questions": {
                "value": "1. In Figure 2, does the heat map show syllable level or frame level similarity matrix for Sylber?\n2. How robust is the proposed method with respect to the quality of the initial segmentation (currently segmentation from SDHuBERT) and the initial features (currently SDHuBERT) respectively? If phone segmentation instead of syllable segmentation is provided, does the model learn phone level representation instead?\n3. See other questions in the Weakness section above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Sylber, a self-supervised learning method for extracting syllabic representations from speech. To achieve this goal, the authors propose a self-distillation framework with an unsupervised syllable segmentation algorithm. Sylber surpasses some prior speech tokenizers on multiple benchmarks, including syllable detection, speech resynthesis, and spoken language modeling (understanding). Besides, this paper introduces a discriminability index to measure whether speech embeddings align with categorical perception."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The proposed Sylber approach successfully bridges the self-distillation method in speech self-supervised learning (SSL) with syllable segmentation/discovery.\n2) Sylber demonstrates strong syllable segmentation and discovery capabilities through comprehensive experimental results.\n3) The discriminability index and Figure 3 clearly show Sylber's categorical perception capability, which aligns with human perception."
            },
            "weaknesses": {
                "value": "1) **Training effort:**  \nIt is not clearly stated in the main text, but according to Appendix A.1.4, Sylber requires two-stage training with 1.15M and 500k updates, respectively, even though the model was trained on the 960 hours LibriSpeech. Note that this does not include the pre-training costs of HuBERT and SDHuBERT. Moreover, the performance gain in the second stage of training is not significant (Table 6). E.g., the R-value only improved from 75.6 to 75.9. Thus, the need for the second stage of training is questioned.\n2) **Model scalability:**  \nThe unstable training process and sensitivity to hyperparameters are known problems of EMA-based self-distillation, like data2vec. This fact might lead to challenges in scaling the model. Furthermore, larger models increase the training costs, not to mention the required 1.65M updates in the original Sylber model.\n3) **Data scalability:**  \nA commonly known fact between speech datasets is the significant domain differences. According to the experimental results, all models were optimized for LibriSpeech, a relatively clean speech corpus. However, it is unclear whether the segmentation algorithm works in noisier conditions like conversational speech (e.g., Switchboard). Additionally, the segmentation algorithm might need extra design for multilingual speech. Hence, scaling the training data might be difficult, casting doubts about Sylber's real-world applications.\n4) **Complicated resynthesis approach:**  \nCompared to prior studies [2,3], this paper's speech resynthesis (token-to-speech) method is significantly more complicated. It involves a CFM model to generate low-level articulatory features for the Articulatory Encodec. The complicated resynthesis method might introduce more uncertainties to the evaluation results. Also, the resynthesis intelligibility and quality are not significantly better than HuBERT with K-means clustering (Table 2).\n5) **Actual inference efficiency:**  \nThe authors only reported the complexity of syllable segmentation methods. However, a more accurate and practical evaluation method is to include the inference time required to extract syllable boundaries. E.g., latency and real-time factors (RTF). \n6) **Insignificant SLU improvements:**  \nThe SLU results in Table 4 do not show great improvements over GSLM (less than 3% relative difference). Some prior methods, like NAST [4] and SpeechTokenizer [5], are not reported for comparison. Besides, the dataset for training uLMs is not clearly stated, which is important since uLM performance highly depends on the amount of data used [6].\n7) **The necessity of syllabic tokens:**  \nIn addition to the scalability issues previously raised, another question is whether syllabic tokens are necessary in real-world use cases. First, one of the main purposes of developing better speech tokenizers is to advance applications like ASR, TTS, and spoken LM. Nevertheless, the authors only presented a minor improvement in SLU. So, it is unclear whether Sylber helps downstream tasks. Second, because of the loss of fine-grained information (Table 9), tokens extracted from Sylber require incorporating separate speech encoders/tokenizers for more complex problems, which other SSL-based tokenizers or neural audio codecs could address. Perhaps providing convincing reasons and experimental evidence helps justify the need for syllabic tokens.\n\n[1] Chang, Heng-Jui, Alexander H. Liu, and James Glass. \"Self-supervised fine-tuning for improved content representations by speaker-invariant clustering.\" arXiv preprint arXiv:2305.11072 (2023).  \n[2] Lakhotia, Kushal, et al. \"On generative spoken language modeling from raw audio.\" Transactions of the Association for Computational Linguistics 9 (2021): 1336-1354.  \n[3] Nguyen, Tu Anh, et al. \"Expresso: A benchmark and analysis of discrete expressive speech resynthesis.\" arXiv preprint arXiv:2308.05725 (2023).  \n[4] Messica, Shoval, and Yossi Adi. \"NAST: Noise Aware Speech Tokenization for Speech Language Models.\" arXiv preprint arXiv:2406.11037 (2024).  \n[5] Zhang, Xin, et al. \"Speechtokenizer: Unified speech tokenizer for speech large language models.\" arXiv preprint arXiv:2308.16692 (2023).\n[6] Borsos, Zal\u00e1n, et al. \"Audiolm: a language modeling approach to audio generation.(2022).\" arXiv preprint arXiv:2209.03143 (2022)."
            },
            "questions": {
                "value": "1) Explain the differences between the discriminability index and phonetic ABX [1,2]. How are these metrics correlated?\n2) How many and what kind of GPUs were used to train/fine-tune each model?\n3) Why did Table 2 not consider HuBERT with 500 K-means clusters and the continuous representations? Note that 500 clusters or even 2k are commonly used in prior studies [3].\n4) Because Sylber was trained with audio in 5-second segments, how does this affect the downstream performance with long utterances (syllable detection and speech resynthesis)?\n\n[1] Schatz, Thomas. ABX-discriminability measures and applications. Diss. Universit\u00e9 Paris 6 (UPMC), 2016.  \n[2] Nguyen, Tu Anh, et al. \"The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language modeling.\" arXiv preprint arXiv:2011.11588 (2020).  \n[3] Maiti, Soumi, et al. \"VoxtLM: Unified Decoder-Only Models for Consolidating Speech Recognition, Synthesis and Speech, Text Continuation Tasks.\" ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}