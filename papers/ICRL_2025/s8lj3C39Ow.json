{
    "id": "s8lj3C39Ow",
    "title": "Strengthening Federated Learning: Surrogate Data-Guided Aggregation for Robust Backdoor Defense",
    "abstract": "Backdoor attacks in federated learning (FL) have garnered significant attention due to their destructive potential. Current advanced backdoor defense strategies typically involve calculating predefined metrics related to local models and modifying the server's aggregation rule accordingly. However, these metrics may exhibit biases due to the inclusion of malicious models in the calculation, leading to defense failures. To address this issue, we propose a novel backdoor defense method in FL named $\\textit{Su}$rrogate $\\textit{D}$ata-guided $\\textit{A}$ggregation (SuDA). SuDA independently evaluates local models using surrogate data, thereby mitigating the influence of malicious models. Specifically, it constructs a surrogate dataset composed of pure noise, which is shared between the server and clients. By leveraging this shared surrogate data, clients train their models using both the shared and local data, while the server reconstructs potential triggers for each local model to identify backdoors, facilitating the filtering of backdoored models before aggregation. To ensure the generalizability of local models across both local and surrogate data, SuDA aligns local data with surrogate data in the representation space, supported by theoretical analysis. Comprehensive experiments demonstrate the substantial superiority of SuDA over previous works.",
    "keywords": [
        "Federated Learning",
        "Backdoor Attacks",
        "Generative Learning"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We propose a Surrogate Data-Guided Aggregation (SuDA) method in federated learning to defend against backdoor attacks.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=s8lj3C39Ow",
    "pdf_link": "https://openreview.net/pdf?id=s8lj3C39Ow",
    "comments": [
        {
            "summary": {
                "value": "The paper aims to design a defense method against backdoor attack under the federated learning setting. Without calculating predefined metrics related to local models and modifying the server\u2019s aggregation rule accordingly, the paper proposes SuDA to evaluate the local models based on the surrogate data, which is constructed by pure noise. Besides, SuDA aligns local data with surrogate data in the representation space to ensure the generalizability of local models. Several experiments are conducted to demonstrate the performance of SuDA."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper propose a backdoor defense method that is independent of local models.\nTheoretical analysis are provided for the generalizability of local models."
            },
            "weaknesses": {
                "value": "* The referenced methods are outdated - most of the referenced papers are before 2022. Plenty of recent studies focus on FL backdoor defense ([1-4] listed below are just a small subset). The authors should discuss and compare with more SOTA methods.\n* The overview of SuDA in 4.1 referred equations that are not introduced previously, which may cause confusion.\n* Observation 4.1 is provided without any empirical results validating it.\n* Eqn 3 cannot cover all types of triggers. For example, it cannot represent the trigger that only change a small region in bottom right corner, as all pixels in x_posion will be changed\n\n[1] Zhu, Chengcheng, et al. \"ADFL: Defending backdoor attacks in federated learning via adversarial distillation.\" Computers & Security 132 (2023): 103366.\n[2] Kumari, Kavita, et al. \"Baybfed: Bayesian backdoor defense for federated learning.\" 2023 IEEE Symposium on Security and Privacy (SP). IEEE, 2023.\n[3] Wang, Shuaiqi, et al. \"Towards a defense against federated backdoor attacks under continuous training.\" arXiv preprint arXiv:2205.11736 (2022).\n[4] Nguyen, Thuy Dung, et al. \"Iba: Towards irreversible backdoor attacks in federated learning.\" Advances in Neural Information Processing Systems 36 (2024)."
            },
            "questions": {
                "value": "* Why the surrogate dataset is generated by Style-GAN rather than a more simpler model. Is there a specific reason?\n* Why the surrogate data share the same labels with training data? Can they have total different label set?\n* What is the definition of \\alpha in 5.1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a new Federated Learning (FL) backdoor defense. Traditional defense mechanisms often rely on empirical metrics that can be skewed by malicious client contributions, leading to ineffective defenses. The proposed Surrogate Data-Guided Aggregation for Robust Backdoor Defense (SuDA) addresses this by using surrogate data shared between the server and clients. SuDA employs feature alignment techniques to minimize distributional shifts between local and surrogate data, aiming to improve model generalization and robustness against backdoor attacks. Empirical results show that SuDA is effective on standard datasets (CIFAR-10, FMNIST, SVHN) and in adaptive attack scenarios."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The surrogate data-based approach is flexible and could be adapted to different FL architectures (e.g., centralized or decentralized) as long as clients and servers can communicate surrogate data.\n* The proposed defense mechanism is resilient to different types of backdoor patterns, and its design makes it less dependent on assumptions about specific attack triggers."
            },
            "weaknesses": {
                "value": "* Training with surrogate noise data could lead to over-regularization or reduced performance, especially in cases where the model requires specialized training on specific, non-noise-related features.\n* The inclusion of surrogate data and trigger reconstruction introduces additional computational and communication overhead, which may limit scalability, particularly in resource-constrained FL setups.\n* SuDA assumes that backdoor patterns can be identified and reconstructed using the surrogate data approach. However, adaptive attackers could vary the backdoor trigger dynamically, evading this detection method.\n* The defense\u2019s effectiveness relies on hyperparameters such as the alignment parameter for feature distribution. Tuning may be complex and environment-dependent.\n* Using surrogate noise data in model training could potentially degrade the performance of benign models, particularly when training on highly specialized or sensitive datasets."
            },
            "questions": {
                "value": "* Could SuDA be adapted to settings with non-image data, like natural language or time-series data?\n* How well does SuDA handle backdoor attacks that dynamically change over time rather than using a fixed trigger pattern?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces SuDA, a defense mechanism against backdoor attacks in Federated Learning. SuDA employs a surrogate dataset composed of pure noise to independently evaluate local models, effectively mitigating the risk of traditional defense metrics being manipulated by malicious clients. By training local models on both real and surrogate data and aligning their feature distributions, SuDA reconstructs potential triggers and identifies backdoored models before aggregation. Experiments conducted on three image datasets (CIFAR-10, FMNIST, and SVHN) across various attack scenarios demonstrate SuDA's effectiveness, particularly in settings with a high proportion of malicious clients."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- SuDA presents a new approach by using surrogate noise data to independently assess local models, reducing the impact of malicious models on defense metrics.\n- The paper provides a theoretical analysis connecting the model's generalization performance with distribution shift, justifying the use of feature distribution alignment between real and surrogate data.\n- Comprehensive experiments explore different hyperparameters and attack scenarios, validating SuDA\u2019s efficacy across diverse conditions."
            },
            "weaknesses": {
                "value": "- The evaluation only includes fixed-trigger attacks, limiting insight into SuDA's robustness against more advanced adaptive attacks like 3DFed[1], A3FL[2], and IBA[3] (optimized-trigger attacks). How might SuDA handle these more sophisticated attacks?\n- Several surrogate generation methods are examined, but potential limitations of using pure noise data as a surrogate are not discussed. Could a more representative surrogate dataset (while maintaining privacy) enhance defense performance in certain cases?\n- The comparison lacks state-of-the-art defenses such as DeepSight[4], FLDetector[5], FLIP[6], and BackdoorIndicator[7], making it difficult to evaluate SuDA\u2019s relative performance and computational efficiency."
            },
            "questions": {
                "value": "- Have you explored other distance metrics beyond the $L_1$ norm for measuring trigger size? How might different metrics (e.g., $L_0$, $L_2$, or $L_{\\infty}$ norms) impact SuDA\u2019s effectiveness?\n- What is SuDA's sensitivity to the hyperparameter $\\lambda = 0$ in Table 15? Were any automated methods for tuning this parameter considered?\n- The paper mentions that SuDA can detect backdoor models even with a small number of malicious clients. Could you elaborate on the minimum number required for effective detection?\n- How does SuDA address non-iid data distributions among clients? Are there any assumptions or strategies for handling this challenge within the defense framework?\n- The experimental setup diverges from previous studies in backdoor attack/defense research (DBA, FLIP[6], A3FL[2], and IBA[3]) which use 100 clients and 4 attackers with non-iid data ($\\alpha = 0.5$). How would SuDA perform under these standard conditions?\n- What is the value of $n_k$ in the aggregation phase when clients use surrogate data?\n\n**References:**\n\n[1]. Li, Haoyang, et al. \"3DFed: Adaptive and extensible framework for covert backdoor attack in federated learning.\" 2023 IEEE Symposium on Security and Privacy (SP). IEEE, 2023.\n\n[2]. Zhang, Hangfan, et al. \"A3FL: Adversarially adaptive backdoor attacks to federated learning.\" Advances in Neural Information Processing Systems 36 (2024).\n\n[3]. Nguyen, Thuy Dung, et al. \"IBA: Towards irreversible backdoor attacks in federated learning.\" Advances in Neural Information Processing Systems 36 (2024).\n\n[4]. Rieger, Phillip, et al. \"Deepsight: Mitigating backdoor attacks in federated learning through deep model inspection.\" arXiv preprint arXiv:2201.00763 (2022).\n\n[5]. Zhang, Zaixi, et al. \"Fldetector: Defending federated learning against model poisoning attacks via detecting malicious clients.\" Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022.\n\n[6]. Zhang, Kaiyuan, et al. \"FLIP: A provable defense framework for backdoor mitigation in federated learning.\" International Conference on Learning Representations (2023).\n\n[7]. Li, Songze, and Yanbo Dai. \"BackdoorIndicator: Leveraging OOD Data for Proactive Backdoor Detection in Federated Learning.\" arXiv preprint arXiv:2405.20862 (2024)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a method called \u201csurrogate data-guided aggregation\u201d for learning federated learning models in the presence of attackers that are trying to backdoor the model. The main idea of SuDA is to provide a surrogate dataset that is held by the central server. However, unlike most surrogate datasets, this one has nothing to do with the target (private) client data distributions---it is drawn from a noise distribution that is independent of the data. Then, the clients train their models locally on an objective that optimizes performance both on the true training data and the surrogate data. To prevent this from affecting model quality, the local clients align their representations on real data to the representations on noise. Then, the central server attempts to filter out triggered (adversarial) models that have been backdoored by reconstructing a trigger for each target class. The authors demonstrate that SuDA outperforms several defenses against trigger-based backdoor attacks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall I think this paper is interesting. I liked a few things about it: \n\n-\tThe problem is important (and very heavily studied)\n-\tThe approach is creative, I liked the use of a completely noisy surrogate dataset\n-\tThe representation alignment idea was also nice to make training on noisy data not harm model utility too much"
            },
            "weaknesses": {
                "value": "I want to clarify that I generally liked the paper. I do have some concerns that I think would need to be addressed for an ICLR publication though. \n\n1) The theoretical contributions didn't seem very strong. Can you state Theorem 4.3 more precisely? What do you mean by \u201celicits the bounded statistical robustness\u201d? Do you mean there exists a lower bound on statistical robustness? Is the lower bound greater than zero? \n\nMoreover, I didn\u2019t understand why I should care about the theoretical result, in the sense that I would expect statistical robustness to have a nonzero lower bound for almost any reasonable classifier and dataset (since it\u2019s an expectation over the distribution), regardless of whether it is trained with SuDA or not. Can you explain whether this theorem is providing unexpected insights? For instance, if you train a classifier without special defenses, and there are adversarial clients trying to backdoor the model, does the resulting model typically have zero statistical robustness? \n\n2) The baselines in the evaluation seem a bit outdated. The most recent one is FLAME from 2022. I would have liked to see some more recent competitive baselines included, such as: \n-\tS. Wang et al, \u201cTowards a defense against federated backdoor attacks\u2026\u201d (2023)\n-\tK. Kumari et al, \u201cBayBFed: \u2026\u201d (2023)\n-\tT. Krauss et al, \u201cMESAS: \u2026\u201d (2023)\nThe paper also seems to be missing a discussion of more recent FL defenses (and attacks). I\u2019m a little worried that the evaluation and related work aren't deep enough for a FL backdoor paper. \n\n3) If you only need random noise for the surrogate data, why do you pass it through a GAN instead of just drawing from a Gaussian and using that as the surrogate dataset? Also why do you instantiate this data as pure noise rather than some public dataset from a somewhat-related domain? Your evaluation examines part of this question, in the sense of using pure noise and not participating in training, but it would be better to also evaluate the effect of using pure noise (not passing it through a GAN), but still participating in training. (As well as possibly using public data for the surrogate training dataset). Of course I understand this won\u2019t always be possible for all FL problems. \n\n4) Smaller issues: \n\n* Just to clarify---I don\u2019t think the paper explicitly says this, but if a model is filtered out (because a small trigger is found), that model is not included in the subsequent aggregation, right? Can you please explicitly explain in the paper what happens after model filtering? \n\n* The notation was a bit confusing. Y_t is listed as the target label, but t is also used as the round in FL."
            },
            "questions": {
                "value": "1) Could you clarify the significance of the theoretical result? \n\n2) Can you provide results from more recent defense baselines? (and update your discussion of related work accordingly, there are a lot more papers that I didn't mention)\n\n3) Can you explain why you construct the surrogate data by passing noise through an untrained GAN? Is there a simpler way of doing this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}