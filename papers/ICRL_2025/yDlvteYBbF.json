{
    "id": "yDlvteYBbF",
    "title": "Differentiable Distance Between Hierarchically-Structured Data",
    "abstract": "Many machine learning algorithms solving various problems are available for\nmetric spaces. While there are plenty of distances for vector spaces, much\nless exists for structured data (rooted heterogeneous trees) stored in popular\nformats like JSON, XML, ProtoBuffer, MessagePack, etc. This paper\nintroduces the Hierarchically-structured Tree Distance (HTD) designed\nespecially for these data. The HTD distance is modular with differentiable\nparameters weighting the importance of different sub-spaces. This allows\nthe distance to be tailored to a given dataset and task, such as classification,\nclustering, and anomaly detection. The extensive experimental comparison\nshows that distance-based algorithms with the proposed HTD distance\nare competitive to state-of-the-art methods based on neural networks with\norders of magnitude more parameters. Furthermore, we show that HTD is\nmore suited to analyze heterogeneous Graph Neural Networks than Tree\nMover\u2019s Distance.",
    "keywords": [
        "Distance",
        "Distance function",
        "Tree-structured data",
        "Heterogenous Graphs",
        "JSONs",
        "Multiple Instance Learning"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=yDlvteYBbF",
    "pdf_link": "https://openreview.net/pdf?id=yDlvteYBbF",
    "comments": [
        {
            "title": {
                "value": "Answers"
            },
            "comment": {
                "value": "The goal of our work is to define a distance on the space of hierarchically-structured trees. We thrived to have proper distance, since it is important for the theory behind many popular algorithms  and for the mathematical analysis.  The construction is on the end relatively straightforward, but we not certain it is obvious including the complexity.\n\nThe experiments were intended to demonstrate the qualities of the distance when used inside common machine learning algorithms. We were pleasantly surprised how well it worked, as it is frequently better than state of the art methods with orders of magnitude more parameters (Table 3 and Table 6b). The experiments were not meant to propose a new state of the art on tested problems, the goal was to show that the proposed distance is not only mathematically sound, but also have good properties on practical problems.\n\n**I would like to hear the authors discussi the related works on Hyperbolic spaces for dealing with hierarchical data and Hierarchical clustering, where data also have latent tree structure. The goal here is to compare different trees and assign a loss to each so that lower loss means better tree for the dataset. The approaches there are also differentiable so how do you compare with them?**\n\nWith all the respect, we do not understand how the work on hyperbolic spaces relates to the definition of a distance on HS-trees. Let us go in more details into individual works you have mentioned.\n\n* Nickel et al. \"Poincar\u00e9 embeddings for learning hierarchical representations\" learns embedding, but if we are not mistaken, the paper does not prove that the result is a distance in the original space of HS-trees.  Contrary,  the distance proposed in our work has parameters and we guarantee that for all their values in feasible range the requirements on distance (triangular inequality) are satisfied.\n\n* \"Hyperbolic graph neural networks\" of Nickel et al. and later the works of Chami et al. \"Hyperbolic graph convolutional neural networks\" proposes to adapt graph neural networks to use hyperbolic (Poincare and Lorenthz embedding), but again it is not clear how is this related to the definition of a distance on HS-trees.\n\n* \"From Trees to Continuous Embeddings and Back: Hyperbolic Hierarchical Clustering\" and \"Gradient-based hierarchical clustering using continuous representations of trees in hyperbolic space\" both proposed a clustering method improving about linkage clustering. But they are not about distances between samples to be clustered, as is our work. We admit we could use Hyperbolic Hierarchical Clustering instead of single-linkage clustering, but single-linkage clustering was meant only for demonstration. The input to Hyperbolic Hierarchical Clustering would on the end be the same distance matrix as to single-linkage clustering, computed by our method. \n\nWith respect to this, can you be little bit more specific how exactly should we compare to the algorithms you have mentioned? \n\n**Is there any hope to prove something about the quality of the metric found?**\n\n What do you mean exactly by the quality of the metric? The quality is likely application specific. The properties of the metric are mostly influence by a distance on bags, which is discussed in the paper. If for a given problem bags are always sets and never multi-sets, one can use cheaper set-distances instead of expensive distances on multi-sets. Besides, we have found important for practice to weight contributions of sub-trees. But this weighting is similar to weighting of coordinates in Euclidean spaces, as is done in Mahalanobis  distance with diagonal weight (covariance) matrix."
            }
        },
        {
            "summary": {
                "value": "The paper studies hierarichically structured data and introduces a tree-distance with differentiable parameters weighting the importance of different subspaces. The paper presents experimental evidence that their approach achieves similar performance to SOTA methods based on neural networks while having orders of magnitude fewer parameters, and also has some benefits for heterogeneous Graph Neural Networks compared to prior methods.\n\nThe paper is motivated by the fact that there are many structured data formats such as JSON/XML/Protobuffer but not a good way of defining a reasonable notion of distance between them, which is in contrast with what happens when we deal with more standard objects like vectors in Euclidean space.\n\nThis paper proposes a particular distance called HTD distance, which exploits the recursive nature of the previously-mentioned data formats. The ultimate goal is to have a modular construction by combining potentially different metrics on different levels of the given tree. HTD has weight parameters, which control importance on different parts, is differentiable, and requires orders of magnitude fewer parameters than neural networks with similar guarantees (based on experiments). The authors perform a series of experiments with supervised learning, ianomaly detection, heterogenous GNNs,  clustering and UMAP for visualization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+the paper studies a natural problem on hierarchies which is how to define suitable metrics that are differentiable and modular. \n\n+the authors present some natural candidate and apply it to different types of hierarchical data\n\n+the authors present experimental results showcasing properties of their proposed metrics and benefits over prior methods."
            },
            "weaknesses": {
                "value": "-the theory is very straightforward in this paper. In fact, the two theorems stated as Th1 and Th2 could be obserations or propositions as they follow from the basic definitions.\n\n-there have been recently approaches to define differentiable objectives suitable for doing optimization over trees and hierarchies, especially to deal with problems on relational data coming from networks (e.g. facebook or other social networks) with the goal of performing hierarchical clustering. The first such works were 1) Nickel et al. \"Poincar\u00e9 embeddings for learning hierarchical representations\" and later 2) \"Hyperbolic graph neural networks\" of Nickel et al. and later the works of 3) Chami et al. \"Hyperbolic graph convolutional neural networks\" and 4) \"From Trees to Continuous Embeddings and Back: Hyperbolic Hierarchical Clustering\" and of 5) Monath et al. \"Gradient-based hierarchical clustering using continuous representations of trees in hyperbolic space\" have dealt with similar questions. I am surprised the authors do not cite such works as the problem of optimization over trees was addressed using differentiable methods in all of these works.\n\n-omission of discussion for use of hyperbolic techniques and hyperbolic spaces in the present paper which is known to be suitable for hierarchical relations, much more than euclidean spaces."
            },
            "questions": {
                "value": "-Please I would like to hear the authors discussi the related works on Hyperbolic spaces for dealing with hierarchical data and Hierarchical clustering, where data also have latent tree structure. The goal here is to compare different trees and assign a loss to each so that lower loss means better tree for the dataset. The approaches there are also differentiable so how do you compare with them?\n\n-For your metrics, is there any hope to prove something about the quality of the metric found?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents the Hierarchically-structured Tree Distance (HTD), a novel metric for structured data in formats like JSON and XML. Designed for rooted heterogeneous trees, HTD is modular and differentiable, allowing it to adapt to tasks like classification, clustering, and anomaly detection. Experiments show that HTD-based algorithms perform competitively with neural network methods while using far fewer parameters and are more effective for analyzing heterogeneous Graph Neural Networks than the Tree Mover\u2019s Distance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is overall well-written and easy to follow.\n2. This paper demonstrates theoretical superiority, as shown in Table 1."
            },
            "weaknesses": {
                "value": "1. The authors demonstrate the effectiveness mainly on distance-based tasks, which shows better performance compare to other distance-based method but does not appear comparable to GNN classifiers.\n\n2. I am also concerned about the contribution and scope of this paper; however, I acknowledge that I am not an expert and am open to other opinions.\n\n3. Although some limitations are mentioned in the submission (e.g., in the caption of Table 4), there is no comprehensive discussion of the proposed method's limitations."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the Hierarchically Structured Tree Distance (HTD), a metric designed to measure distances between tree-structured data commonly stored in formats like JSON and XML. HTD effectively represents message passing in heterogeneous graph neural networks (GNNs). Experimental results show that this distance metric is capable of addressing various machine learning tasks, including classification, visualization, and clustering."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-written.\n- The proposed HTD generalizes the tree mover\u2019s distance, making it applicable to both heterogeneous graphs and tree-structured data.\n- Extensive experiments conducted across multiple tasks\u2014classification, clustering, and anomaly detection\u2014show HTD's superiority over state-of-the-art methods for tree-structured data."
            },
            "weaknesses": {
                "value": "I have several concerns regarding the novelty of the proposed distance:\n- HTD appears to be a straightforward extension of the tree mover\u2019s distance, replacing the optimal transport (OT) distance with the Hausdorff and Chamfer distances, which may introduce cheaper computational complexity.\n- It is unclear why HTD outperforms the tree mover\u2019s distance on homogeneous graph datasets, such as MUTAG and BZR, as shown in Table 3.\n- Even on heterogeneous datasets, it seems feasible to apply the tree mover\u2019s distance by constructing separate computational trees for each node type in the graph. So we can improve the performance of tree mover's distance on heterogeneous datasets like MUTAG and BZR.\n- Similar to the tree mover\u2019s distance, HTD does not meet the criteria for defining a valid kernel for tree-structured data, as it is not conditionally negative definite.\n- Including standard deviations (STD) in the results of Tables 3 and 4 would be beneficial, as the variances are large; for instance, the STD of accuracy values for MUTAG and BZR might be around 5."
            },
            "questions": {
                "value": "See the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces hierarchically-structured tree distance (HTD) between HS-Trees."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors introduce a new distance for hierarchically-structured trees based on Leaves, Bags, and Dicts."
            },
            "weaknesses": {
                "value": "- The motivation and explanation of the use of HS-trees and their distance are unclear in the introduction. The authors first say that the properties of HT-Trees are used in existing work. Then, in the next paragraph, the authors mention that the distance on HS-Trees has been studied very little. Among the properties used in previous work, were there HS-Tress distance?\n- In the introduction and background, it\u2019s unclear if the term HS-Trees is from previous work or if the term is first given by the authors. In addition, the background of HS-Trees, sample, and schema are very confusing. It would be clearer if the authors could provide an example of what they are here. Moreover, it\u2019s unclear how bags are assumed to be \u201cpermutation invariant; therefore, the position has to be encoded through position encoding\u201d and how the \u201cuniversal approximation theorem for HS-Trees has been proved in Pevny & Kovarik (2019).\u201d\n- The authors mention the motivation of the work is \u201cmeasuring the distance between samples emerging from popular data storage formats (e.g., JSON, XML, and ProtoBuffer)\u201d. However, in the experiment section, there is no application in such data format. It is misleading to use the data storage formats as motivation in the introduction, but there is no related experiment\n- In Section 2.1, it is unclear if the authors are trying to claim that GNNs are \u201chierarchically-structure data\u201d. In the context of GNN, it\u2019s hard to see what sample, schema, and even HS-Trees are.\n- The authors claim that the proposed distance is differentiable, however, there is no theoretical proof\n- The term Leaves is commonly used in tree structure data. It will be important to differentiate the difference between common tree structures and HS trees. In addition, it\u2019s unclear how and why the definition of Leaves in Section 3. 1 is outside of the scope\n- There is no proof for Theorem 1. The justification right after Theorem 1 is hard to follow.\n- It is hard to link the relation between Eq. (5) and the distance in Table 2, even with the brief introduction in Appendix B. A simple proof or derivation could have helped to understand the connection.\n- It\u2019s unclear what the relation between HTD and TMD is.  A simple proof or derivation could have helped to understand the connection.\n- The README.md files in the provided link to the code are not sufficient to reproduce the HTD and experimental results.\n- The experimental setup is unclear in the main texts. It\u2019s unclear what the actual classification tasks and anomaly detection are. Also, while Appendix A includes the implementation details, there is no reference in the main text link to the appendix. In addition, in Appendix A, the authors claim that the experiments are repeated five times, and there is no variance or standard deviation reported in the performance in the main text.\n- It\u2019s unclear what the colors represent in Figure 3. It\u2019s unclear why and what \u201c\u2014\u201d represents in Tables 3-5.\n- The paper needs more proofreading: i) additional ) in line 029, ii) line 250 missing a period and there is an additional ), iii) notation is very hard to follow; a notation is given with confusing comma\n\n## Minor\n- There if no reference and introduction when the Mutagenesis dataset is first mentioned in line 317"
            },
            "questions": {
                "value": "- Only until Figure 2 the authors demonstrate examples of Schema. However, the sample 1 and sample 2 are essentially trees, it still remains unclear how HS-Trees to GNN in Section 2.1. Could the author provide an illustrative figure to show the relation?\n- The font size is a bit bigger than usual?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}