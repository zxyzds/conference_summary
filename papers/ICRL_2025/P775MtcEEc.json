{
    "id": "P775MtcEEc",
    "title": "Distributional Sobolev reinforcement learning",
    "abstract": "Distributional reinforcement learning (DRL) is a framework for learning a complete distribution over returns, rather than merely estimating expectations. In this paper, we further expand DRL by estimating a distribution over the gradient of the state-action value function, in addition to its scalar value. We refer to this method as Distributional Sobolev training. Inspired by Stochastic Value Gradients (SVG), we achieve this by leveraging a one-step world model of the reward and transition distributions implemented using a conditional Variational Autoencoder (cVAE). Our approach is sampled-based and relies on Maximum Mean Discrepancy (MMD) to instantiate the distributional Bellman operator. We first showcase the method on a toy supervised learning problem. We then validate our algorithm in several Mujoco/Brax environments.",
    "keywords": [
        "Reinforcement learning",
        "distributional reinforcement learnng",
        "Sobolev training of neural networks"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We extend distributional RL to model uncertainty over the gradient of the random returns.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=P775MtcEEc",
    "pdf_link": "https://openreview.net/pdf?id=P775MtcEEc",
    "comments": [
        {
            "summary": {
                "value": "The paper meticulously formulates a new model-based actor-critic method for distributional RL. The proposed method utilizes a conditional VAE as their world model, that learns to predict both the next state and reward, conditioned on the current state and action. This model, trained along with the policy and critic networks, allows computing the MMD loss for the critic, and introduces uncertainty in the action gradients (of the returns) along with the returns themselves. The authors elaborate the required mathematical background to explain their method and design choices, and show the advantage of the distributional approach and the usage of gradient information in a toy problem. Combined with RL, the proposed method is tested in five mujoco environments, showing a certain superiority over other approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Mathematical background and notations: Comprehensive and helps to explain the proposed method. I am not sure which ones are novel though, if some of them, it is worth noting.\n\n2. The extension to distributional RL is done elegantly, using a rather simple world model, which allows using other types of models.\n\n3. Experiments seem extensive in terms of benchmarks, although see my notes in the weaknesses section."
            },
            "weaknesses": {
                "value": "### Experiments:\n1.  My major concern with this work is the empirical performance of the proposed method; It performs very similar to DDPG with MMD (only minor improvements) and in most tested environments, vanilla DDPG performs the best. Considering that the toy problem does not involve RL, I think that the method is not justified enough. I suggest looking for other environments that show the value of such method, maybe ones that are more stochastic?\n\n2. Lack of baselines: although the method is meant for the distributional setting, mean-based methods could be applied to the same environments, and there are many that should be compared, if not for direct comparison it helps for understanding the scale of performance improvement.\n\n### Related Work:\nThis section seems very limited. Since the introduction describes some, I would move this section to the beginning of the paper, and maybe merge with the introduction section.\n\n### Clarity:\nFor me, it is not clear enough what is novel in this method; The usage of the gradient info has been done, hence the method extends this approach to distributional RL?\nThe same applies for the mathematical analysis. I think it should be clearer -- either differ between existing and new/ add a statement of the paper's contribution.\n\n### Minor Comments:\n1. line 449: Deterministic Deep Policy Gradient -> Deep Deterministic Policy Gradient.\n\n2. line 456: both variants of\nDSDPG, using action gradient and DSDPG using state-action\" -- rephrase"
            },
            "questions": {
                "value": "1. What are the paper's contributions?\n\n2. Is it possible to extend your method to support other actor-critic methods? (e.g., PPO, A2C, etc.)\n\n3. In your experiments, it seems that you evaluate the learning curve, while eventually most methods perform in par after enough interaction. Have you tried limiting the data (to show better sample complexity)?\n\n4. One of the advantages of using a model-based method, given an accurate enough model, is the ability to train without direct interaction (only from the world model) have you tried it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes \u201cDistributional Sobolev RL\u201d to extend distributional RL by modeling both the return and gradient distributions of the state-action value function. Using a one-step world model with a conditional Variational Autoencoder (cVAE) and Maximum Mean Discrepancy (MMD) for the Bellman operator, the approach is validated on toy tasks and Mujoco/Brax environments."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Learning the gradient in addition to the distribution is an interesting idea."
            },
            "weaknesses": {
                "value": "I failed to understand this paper and am quite confused about the proposed approach. We should check if it is a personal difficulty of me or if other reviewers also feel confused. If the latter, this indicates room for significant improvements in the paper.\n\nI have listed my questions in the Questions section below.  These questions prevent me from understanding the proposed algorithm. In addition, Section 3.1 has few intuitive explanation. for example, why is eq (16) proposed? The paper states, \"Depending on the applications Eq. 11 or 16 may make more sense \". This may need further clarification (when should we prefer 11 or 16?). Since I couldn\u2019t follow Section 3.1, I was unable to understand Section 3.2 as well.\n\nOverall, I find this paper lacks rigor and has limited (theoretical) justification. The definitions in section 3.1 are not rigorous enough. \n\nThe related work section is not organized well. The authors should put most related works here instead of in the introduction. Also, many prior works appear to be missed.\n\nMinor issue: Figure 1 appear somewhat sloppy: the right side is cut off, and so $Z^{next}$ is incomplete."
            },
            "questions": {
                "value": "- The $f$ in eq (12) is not bold, which took me a while.\n- In Line 178, how do you assume the action-gradient $x^\\text{action}$ exists? First, I believe you need some differentiable conditions; second, if actions are discrete, how is the gradient defined in this case?\n- Similarly, what if transition is discrete? Then is $\\frac{\\partial }{\\partial s} x^{\\text{return}}$ well defined?\n- Similarly, is $\\frac{\\partial }{\\partial a} R(s,a)$ well defined given that $a$ may be discrete?\n- In Line 191, what does \"gradient computations of the bootstrapped target\" mean? Why do you assume it is differentiable?\n- In eq (16), the notation $\\nabla_s$ is unclear since there is no $s$ on the right. This is confusing.\n- SImilarly, $\\nabla_a$ is also unclear in eq (16). It is also in eq (11).\n- Estimating the return distribution and the gradient seem orthogonal. Could you explain why you want to merge them in this paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a so-called Distributional Sobelev Deterministic Policy gradient, where the proposed method uses a distributional critic loss while incorporating the gradient of a random variable. Particularly, in RL scenarios, the authors also leverage a differential world model of the environment in order to infer gradients from observations. They also conducted some experiments on the toy supervised learning tasks and several Mujoco envs."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The proposed algorithm seems to incorporate different kinds of methods, such as Sobolev learning and conditional VAE."
            },
            "weaknesses": {
                "value": "**A poor motivation**. In the introduction part, it seems that the authors try to motivate the paper to improve the (stability of) policy gradient algorithm by incorporating the gradient information in the policy optimization. This is a very straightforward and even trivial idea. Also, the authors tried to develop a distributional version of algorithms that is orthogonal to the existing algorithm and has already been studied before, such as [1]. It was very confusing to understand the motivation behind it, but I ended up being frustrated. \n\n**No methodological contribution**. Distributional RL and Sobolev are already known methods, and I do not think it is meaningful to do such kind of combination research. Also, the proposed method is constrained. For example, the paper directly limits the scope to a deterministic policy gradient method starting from Eq.(2). The gradient to be incorporated into the training requires the differentiability of the environment. To this end, the authors proposed a world model by CVAE, but it is not practical for real and potentially complicated environments.\n\n**Poor Writing**. It is very hard to understand what the authors want to express. Many expressions are very inaccurate, and notation usage is sloppy. For example, what is the likelihood estimation issue in Line 215? How to define the derivative of a random variable over a in Eq.8? Note that the return Z is a random variable conditionally over the policy on s and a. I am confused to understand this point. The reference of Czarnecki et al., 2017 is also not complete, making me feel the writing of this paper is very non-professional. Z^S in Eq. 11 is not consistent with Z_S, which occurs many times. \n\n**The experiments are trivial and limited**. It is not clear if it is necessary to have experiments on supervised learning tasks since the focus of this paper is RL. The empirical improvement in Mujoco is insignificant, making me feel that the proposed method is even not sound.\n\n\n[1] Distributed Distributional Deterministic Policy Gradients (ICLR 2018)"
            },
            "questions": {
                "value": "Please see Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel approach to distributional reinforcement learning, Distributional Sobolev RL, which models both the distribution of random returns and the action-gradients. The paper also empirically demonstrate the effectiveness of proposed algorithm. While incorporating Sobolev training is an interesting and promising idea, I have some concerns regarding the current manuscript."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The motivation is clear, and the idea of learning distribution over action gradients is good."
            },
            "weaknesses": {
                "value": "- **Experiments**: There are several issues that need to be clarified in the toy experiments. For example, the red and blue dashed lines in Figures 3 and 4 are confusing. In Figure 1, it is difficult to distinguish the blue and green lines. For MuJoCo experiments, I have big concerns about the **fairness comparison**. The authors only present the performance results after $0.25\\times10^6$ training steps, while the default setting is  $1\\times10^6$. The potential cause I guess is that DSDPG is computationally cost, but this adjustment seems to benefit DSDPG since it is a model-based algorithm utilizing pre-trained transition information, which helps faster convergence. In contrast, DDPG is a model-free algorithm that is not sample-efficient, particularly in the early stages.  Furthermore, using actor-critic-based methods to demonstrate the advantage of incorporating gradient information into distributional modeling may not be ideal since the decision-making is based on the actor, while the critic (return distribution) is primarily used to train the actor network.\n\n- **Random state-action Sobolev return**: First, It is unclear whether Sobolev returns are actually used for decision-making. Second, Equation (15) is confusing, and it appears to be missing a term involving $\\frac{\\partial s'}{\\partial s}\\frac{\\partial s}{\\partial a} x$.  Third, as the Sobolev return is defined as the derivative of the return with respect to the state, I have a concern regarding the handling of complex state spaces, such as pixel-based states in Atari games. The computational burden will significantly increase as the dimension of the state grows. Since the current experiments are implemented on Mujoco, where the state and action are vectors, I am curious about how well DSRL will perform in more complex environments.\n\n  \n\n- **Discussion with other methods:**  The discussion of related methods is limited. It would be beneficial to expand on this, especially regarding the connections to existing distributional RL approaches (QR-DQN, C51). The use of MMD minimization for learning the distributional Bellman operator is inspired by MMD-DQN [1], but this connection should be discussed in more detail. Additionally, tuning the bandwidth parameter $h$ in the multiquadratic kernel is crucial since kernel-based methods are sensitive to bandwidth selection.\n\n[1] Distributional Reinforcement Learning via Moment Matching. AAAI 2021."
            },
            "questions": {
                "value": "Q1: I have concerns regarding sample-based distributional RL. While some works utilize samples to represent return distributions, these methods are not mainstream. One significant drawback is the computational inefficiency associated with using generative models to represent return distributions. Furthermore, sample-based methods involving maximum likelihood estimation and specific distributional parameterization may suffer from model misspecification, making it difficult to capture the distributional Bellman equation.\n\nQ2: Have the authors considered using diffusion models to model transition probabilities, given their impressive empirical performance?\n\nQ3: Have the authors considered implementing Distributional Sobolev RL on top of value-based RL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}