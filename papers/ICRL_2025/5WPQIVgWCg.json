{
    "id": "5WPQIVgWCg",
    "title": "Satisficing Exploration in Bandit Optimization",
    "abstract": "Motivated by the concept of satisficing in decision-making, we consider the problem of satisficing exploration in bandit optimization. In this setting, the learner aims at finding a satisficing arm whose mean reward exceeds a certain threshold. The performance is measured by satisficing regret, which is the cumulative deficit of the chosen arm's mean reward compared to the threshold. We propose SELECT, a general algorithmic template for Satisficing Exploration via LowEr Confidence bound Testing, that attains constant satisficing regret for a wide variety of bandit optimization problems in the realizable case (i.e., whenever a satisficing arm exists). Specifically, given a class of bandit optimization problems and a corresponding learning oracle with sub-linear (standard) regret upper bound, SELECT iteratively makes use of the oracle to identify a potential satisficing arm. Then, it collects data samples from this arm, and continuously compares the lower confidence bound of the identified arm's mean reward against the threshold value to determine if it is a satisficing arm. As a complement, SELECT also enjoys the same (standard) regret guarantee as the oracle in the non-realizable case. Finally, we conduct numerical experiments to validate the performance of SELECT for several popular bandit optimization settings.",
    "keywords": [
        "Online learning",
        "Bandits",
        "Satisficing",
        "Regret"
    ],
    "primary_area": "learning theory",
    "TLDR": "We propose SELECT, a general algorithm that informs satisficing decision-making with constant loss, for a wide variety of multi-armed bandit problems.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5WPQIVgWCg",
    "pdf_link": "https://openreview.net/pdf?id=5WPQIVgWCg",
    "comments": [
        {
            "summary": {
                "value": "The paper tackles the problem of satisficing exploration in multi-armed bandits. Satisficing problem here represents finding an option which is above a preset threshold. The paper proposes a novel method SELECT which utilizes any existing bandit method with sub-linear regret guarantees and utilizes the same sample path trajectories to further provide a constant satisficing regret framework. \n\nThe method implementation is split into three parts: \n1. Shadowing the sub-linear regret method's trajectory for a set number of rounds\n2. Forced sampling of selected arm \n3. Comparing the lower confidence bound of the selected arm with the threshold value.\n\nThe paper provides regret guarantees based on the difference between the highest mean among all the options and the threshold value (denoted in the paper as $\\Delta_S*$). The paper also provides a matching lower bound (up-to-logarithmic factors) to validate the performance of SELECT. \n\nThe paper further goes on to provide examples of how SELECT can be used with different bandit frameworks, making the method quite applicable to a large set of setups. This is supplemented with experiments for the same, further strengthening the case of SELECT."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The following would contribute to the strengths of the paper:\n- **Clear Writing**: The paper is well written, precise, and to-the-point. \n\n- **Justified Problem Setup**: The paper clearly explains the justification of the problem setup, literature surround it and solution of the problem with theoretical and experimental backing. \n\n- **Innovative Umbrella Solutions**: The novel proposed method SELECT can be appended to any sub-linear regret method and can provide constant satisfying performance guarantee to the respective application. This makes the algorithm quite applicable to a lot of varied problem setup. \n\n- **Theoretical performance guarantees**: The paper provides theoretical proof on both the regret upper-bound and shows the tightness to the fundamental lower-bound on the best performance possible on the satisficing problem. \n\n- **Example distinct setups**: The paper provides example problem setup in finite-armed bandits, concave bandits and Lipschitz bandits.  \n\n- **Experiments**: The paper provides a synthetic implementation for all the example setups and showcases the promise of SELECT method."
            },
            "weaknesses": {
                "value": "There are very few obvious loopholes to the paper. Overall the paper is a complete work. A few paragraphs on the potential future works and possible extensions would be a good addition."
            },
            "questions": {
                "value": "Nothing to add here"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces SELECT, an algorithmic framework designed for satisficing exploration in bandit optimization. The primary objective of SELECT is to frequently identify arms with mean rewards exceeding a specified threshold, with its performance evaluated through satisficing regret, which measures the cumulative deficit of the chosen arm's mean reward compared to this threshold. SELECT operates by leveraging a learning oracle that provides a sub-linear regret upper bound. It iteratively identifies potential satisficing arms, collects data samples, and monitors the LCB of the arm\u2019s mean reward against the threshold to determine if it qualifies as a satisficing arm. The algorithm guarantees constant satisficing regret in scenarios where a satisficing arm exists (realizable case) and matches the standard regret of the oracle in non-realizable situations. The framework is successfully instantiated across various bandit settings. Numerical experiments validate the efficacy of SELECT, demonstrating its ability to achieve constant regret."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper successfully integrates standard bandit optimization algorithms into a new framework, achieving better results and providing a more direct application of bandit algorithms to the satisficing exploration problem.\n- The proposed SELECT employs a unique approach by utilizing a learning oracle for bandit optimization, enabling it to sample candidate arms and monitor performance efficiently.\n- The paper establishes that SELECT achieves a constant satisficing regret in realizable cases, independent of the satisficing gap, and inversely related to the exceeding gap. This feature allows it to maintain performance even when the satisficing gap varies."
            },
            "weaknesses": {
                "value": "- The main weakness is that the algorithm imposes stringent conditions (Condition 1) on the oracle algorithm, requiring sublinear regret for all time steps t. Most algorithms, including all oracles referenced in Section 5, only achieve sublinear regret when t is sufficiently large. If alpha approaches 1 when t is small, the theoretical regret bound could become excessively large.\n- The algorithm involves hyperparameters that rely on the oracle algorithms. In scenarios where oracles are unavailable or when the sublinear oracles have unclear parameters, extending theoretical conclusions becomes challenging.\n- The first step of each phase necessitates a bandit algorithm, which is crucial. However, the paper lacks a general discussion on how to select the appropriate oracle algorithm.\n- While Remark 2 highlights the novelty of each step, an ablation study demonstrating the impact of each component would strengthen the paper. Currently, the experimental results do not robustly support the conclusions, as SELECT only outperforms all baselines in 3 out of 6 settings."
            },
            "questions": {
                "value": "- There appears to be an inconsistency in the paper regarding the baseline references to \"Hajiabolhassan \\& Ortner (2023)\" in line 427 and \"Michel et al. (2023)\". Clarification is needed.\n- The time horizon T may not be large enough for UCB-based algorithms. For instance, in Figure 3(b), as T further increases,  SELECT appears to perform worse than the other algorithms.\n- The experimental results further indicate that Condition 1 is overly strict, as the oracles struggle to satisfy it. For example, in Figures 4(a) and 4(b), the regret appears to converge to T for Uniform UCB."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the problem of satisficing exploration, inspired by the concept of satisficing in decision-making. The authors propose an algorithmic framework called SELECT, which leverages a learning oracle with sub-linear regret guarantees to iteratively identify and test potential satisficing arms. SELECT achieves constant regret in the realizable cases, and it maintains the original regret bound in the non-realizable cases. Finally, numerical experiments are conducted to demonstrate the algorithm's performance across various bandit settings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. It is commendable that the proposed algorithm can be applied to any learning oracle with sub-linear regret guarantees, making it adaptable to different bandit models.\n2. The three-step algorithm design is well-constructed and clearly explained. Step 3 (LCB Tests) is particularly impressive. Roughly speaking, in the realizable cases, a certain round will continue indefinitely, while in the non-realizable cases, the algorithm will proceed round by round."
            },
            "weaknesses": {
                "value": "1. While the three-step design in each round is great, each round of SELECT runs independently, resembling the doubling trick, which is often criticized. In the non-realizable case, this may lead to suboptimal theoretical and practical performance.\n2. The result that constant regret can be achieved in the realizable case is not surprising, given prior research like Garivier et al. (2019). Additionally, the study on the lower bound feels insufficient. For instance, in the case of finite-armed bandits, the lower bounds are limited to two-armed bandits. I encourage the authors to explore the lower bounds further.\n3. A minor issue to note is the terminology \"satisficing exploration.\" In the bandit literature, exploration refers to selecting arms with uncertain rewards to gather information, as opposed to exploitation, where arms are selected to maximize immediate rewards based on current knowledge. In this problem, there is indeed a tradeoff between exploration and exploitation. I believe the thresholding bandits problem (in the pure exploration setting) is a better model of satisficing exploration. The authors might consider clarifying this or adopting different terminology."
            },
            "questions": {
                "value": "1. For Condition 1, what happens when $\\alpha < 1/2$?\n2. Regarding the numerical results for finite-armed bandits, which algorithm is used as the learning oracle? Could you explain why SELECT outperforms Uniform UCB in Figure 4b?\n3. Can $\\gamma_i$ be multiplied by a constant? If so, are the empirical results sensitive to the choice of constant for both realizable and non-realizable cases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}