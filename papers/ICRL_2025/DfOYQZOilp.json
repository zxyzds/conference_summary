{
    "id": "DfOYQZOilp",
    "title": "Jump-teaching: Ultra Robust and Efficient Learning with Noisy Labels",
    "abstract": "Sample selection is the most straightforward technique to combat noisy labels, aiming to prevent mislabeled samples from degrading the robustness of neural networks. However, compounding selection bias and redundant selection operations have always remained challenging in robustness and efficiency. To mitigate selection bias, existing methods utilize disagreement in partner networks or additional forward propagation in a single network. For selection operations, they involve dataset-wise modeling or batch-wise ranking. Any of the above methods yields sub-optimal performance. In this work, we propose $\\textit{Jump-teaching}$, a novel framework for optimizing the typical workflow of sample selection.  Firstly, Jump-teaching is the $\\textit{first}$ work to discover significant disagreements within a single network between different training iterations. Based on this discovery, we propose a jump-manner strategy for model updating to bridge the disagreements. We further illustrate its effectiveness from the perspective of error flow. \nSecondly, Jump-teaching designs a lightweight plugin to simplify selection operations. It creates a detailed yet simple loss distribution on an auxiliary encoding space, which helps select clean samples more effectively. In the experiments, Jump-teaching not only outperforms state-of-the-art works in terms of robustness, but also reduces peak memory usage by $0.46\\times$ and boosts training speed by up to $2.53\\times$. Notably, existing methods can also benefit from the integration with our framework.",
    "keywords": [
        "learning with noisy labels",
        "machine learning",
        "classification"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "A novel framework for combating noisy labels.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=DfOYQZOilp",
    "pdf_link": "https://openreview.net/pdf?id=DfOYQZOilp",
    "comments": [
        {
            "summary": {
                "value": "The authors introduce the concept of identifying significant disagreements within a single neural network across different training iterations. This discovery leads to the proposal of a \"jump-manner\" strategy for model updates, effectively bridging the gaps caused by these disagreements. Jump-Teaching simplifies the sample selection process through a lightweight plugin that generates a clear loss distribution in an auxiliary encoding space. This approach enhances the ability to select clean samples more effectively, addressing selection bias and redundancy. The framework demonstrates substantial improvements in both robustness and efficiency compared to state-of-the-art methods, specifically reducing peak memory usage by 46% and increasing training speed by up to 253%. The paper highlights that current methods can benefit from integrating with the Jump-Teaching framework, suggesting that it enhances the overall approach to learning with noisy labels."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The method achieves improved performance under high noise rates on CIFAR datasets.\n2. The method demonstrates better computational and storage efficiencies during testing."
            },
            "weaknesses": {
                "value": "1. The experiments conducted on CIFAR-10 with 90% symmetric noise lack meaningful insight, as this setting results in random labels for each sample, effectively reducing the task to an unsupervised learning scenario.\n2. The presentation needs improvement. Suggested changes include:\n   - The methodology section incorporates experimental analysis (Figure 4), making it difficult to discern insights related to debiasing.\n   - The connection among the four subsections in Section 3.2 is unclear.\n   - The framework presented in Figure 1 contains excessive details that are not explained in the introduction; these should either be removed or relocated to the methodology section.\n3. The authors claim that \u201cJump-Teaching is the first work to discover significant disagreements within a single network between different training iterations.\u201d However, the concept of leveraging disagreements across different training iterations has been previously studied (see [1]).\n\n[1] Self-Filtering: A Noise-Aware Sample Selection for Label Noise with Confidence Penalization, ECCV 2022."
            },
            "questions": {
                "value": "1. As indicated in Table 5, the accuracy under 90% symmetric noise on CIFAR-10 exceeds 75%, which corresponds to random labels for the training samples. This scenario can be classified as an unsupervised learning task rather than weakly supervised learning. We need to reconsider the implications of generalization in learning with noisy labels using semi-supervised learning methods, given the lack of supervision.\n2. It seems unreasonable to separate the updates of the neural network parameters in steps 9-10. Combining \\( L^{BCE} \\) and \\( L^{CE} \\) and updating the neural network with respect to the total loss could be more efficient."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes \"Jump-teaching,\" a novel framework for robust and efficient learning with noisy labels. By introducing a jump-update strategy and a Semantic Loss Decomposition plugin, the method reduces sample selection bias and enhances efficiency. Experiments show Jump-teaching improves performance over state-of-the-art methods, particularly under high noise conditions, with notable gains in memory efficiency and processing speed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Proposes an innovative jump-update strategy that significantly reduces selection bias in single-network training.  \n- Semantic Loss Decomposition provides a lightweight yet effective way to distinguish clean and noisy samples.  \n- Empirically validated with improved accuracy, efficiency, and robustness across various noise levels and datasets."
            },
            "weaknesses": {
                "value": "- I disagree with the claim that this work is the first to identify disagreements across different iterations within a single network. Prior studies [1][2] have leveraged these disagreements to distinguish clean samples from corrupted data in training sets. I recommend that the authors revise this claim and include comparisons with these two works.\n\n- In Figure 2, the authors introduce the IoU metric to measure disagreements. Although an explanation of IoU is provided in the appendix, could the authors illustrate what range of IoU values is considered preferable? Because I notice that the IoU value of Jump-update is between the values of self-update and cross-update, the performance of Jump-update is the best (see Figure 2(c,d)).\n\n- Property 1.   (1) I have a question regarding the assumption in Property 1, namely that $N_A$ equals $N_{iterations}$. From past experience, the model often generates biased selections initially, then gradually corrects this bias as performance improves, given moderate noise rates (10%, 20%). Therefore, error accumulation may not persist in later iterations.  (2) Additionally, the results in Figure 4(a) do not align well with the conclusion of Property 1. The highest test accuracy occurs at r = 50% rather than r = 10%. \n\n- There are some concerns regarding whether the jump-update is a more effective strategy for selecting clean samples. (1) In Table 4, at typical noise ratios (e.g., CIFAR-10/100 sym. 50%, CIFAR-100 asym. 40%), J-Co-teaching does not outperform standard Co-teaching (2 networks). (2) While non-trivial improvements are observed in Table 1, these gains do not carry over to a semi-supervised learning setting (see Table 5). In some settings, J-DivideMix is worse than DivideMix.\n\n- The compared methods in Table 1 are outdated. Comparing with more recent works is necessary; for example, ProMix (IJCAI'23).\n\n\n[1] Late Stopping: Avoiding Confidently Learning from Mislabeled Examples. ICCV'23  \n[2] Self-Filtering: A Noise-Aware Sample Selection for Label Noise with Confidence Penalization. ECCV'22  \n[3] ProMix: Combating Label Noise via Maximizing Clean Sample Utility. IJCAI'23"
            },
            "questions": {
                "value": "see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "To mitigate compounding selection bias and redundant selection operations in existing methods, the authors of this paper propose a novel framework for optimizing the typical workflow of sample selection, called Jump-teaching. Jump-teaching focuses on discovering significant disagreements within a single network between different training iterations by employing a jump-manner strategy for model updating to bridge the disagreements. Besides, Jump-teaching designs a lightweight plugin to simplify selection operations to help select clean samples more effectively. Finally, experimental results on synthetic and real-world noisy datasets, demonstrate the robustness of Jump-teaching."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This idea and motivation for discovering significant similarities within a single network between different training iterations are interesting and fascinating.\n\n2. This paper has carried out a lot of formula derivation and proved the effectiveness of the proposed method from the theoretical knowledge level.\n\n3. Figures 1, 2, and 3 in this paper simply and clearly express the main ideas and innovations of the paper."
            },
            "weaknesses": {
                "value": "1. Authors need to further provide the results of J-Co-teaching and J-DivideMix on Clothing1M in Table 3 to prove the reliable performance in real scenarios.\n\n2. There are errors with the experimental results data. J-Co-teaching does not achieve optimal performance under some settings in Table 4, such as Sym-0.5 of CIFAR-10 and Sym-0.5 and Asym-0.4 of CIFAR100.\n\n3. The content of the semantic loss decomposition part of this paper is not strongly related to the main motivation of this paper, Jump-update Strategy, and is more like an auxiliary trick.\n\n4. There is no clear definition of I_{detection} in Eq. (8).\n\n5. The names of the citation methods are not uniform, such as ' JoCoR ' in the relevant work section and ' JoCor ' in the experimental section.\n\n6. It is recommended to compare the proposed method with 2024 SOTAs."
            },
            "questions": {
                "value": "See above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes the Jump-Teaching methodology for learning with noisy labels. \n\nSpecifically, it investigates an efficient approach that requires only a single network. \n\nTo achieve this, the authors introduce two key techniques: a Jump-update Strategy to mitigate selection bias and Semantic Loss Decomposition to simplify the selection operation. \n\nThe effectiveness of the proposed approach is demonstrated through experiments on three benchmark datasets: CIFAR-10, CIFAR-100, and Clothing-1M."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Research on sample selection methods using a single network to reduce computational costs is an interesting research topic."
            },
            "weaknesses": {
                "value": "The academic novelty of this approach appears limited. It is unclear whether updating the model based on selections from the previous step offers any theoretical advantage over the naive approach of updating the model at every iteration. Additionally, is there theoretical support that using data from the previous step effectively addresses the noisy label problem? Moreover, extracting useful information from models at different training epochs has already been extensively explored in the literature. A seminal work in this area, for example, is Snapshot Ensembles: Train 1, get M for free (ICLR \u201917).\n\n\nThe experimental results are not convincingly state-of-the-art. In particular, several recent relevant papers (a, b, c) are missing from the references. Their result tables show significantly better performance on CIFAR-10 and CIFAR-100 compared to the results presented in this work.\n \n(a) Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels (NeurIPS\u201921)\n\n(b) DISC: Learning From Noisy Labels via Dynamic Instance-Specific Selection and Correction (CVPR\u201923)\n\n(c) Sample-wise Label Confidence Incorporation for Learning with Noisy Labels (ICCV\u201923)"
            },
            "questions": {
                "value": "It is not clear why using data selection results from previous iterations for model updates would be beneficial. Specifically, why would the set of samples selected by the model in the previous step yield better results than the samples selected in the current step? Is the main advantage simply that it avoids sequential updates, thereby reducing the amplification of error accumulation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}