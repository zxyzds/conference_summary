{
    "id": "1Nwsqw0sTm",
    "title": "Open-Vocabulary Object Detection for Incomparable Spaces",
    "abstract": "In open-vocabulary object detection (OVDet), specifying the object of interest at inference time opens up powerful possibilities, allowing users to define new categories without retraining the model. These objects can be identified through text descriptions, image examples, or a combination of both. However, visual and textual data, while complementary, encode different data types, making direct comparison or alignment challenging. Naive fusion approaches often lead to misaligned predictions, particularly when one modality is ambiguous or incomplete. In this work, we propose an approach for OVDet that aligns relational structures across these incomparable spaces, ensuring optimal correspondence between visual and textual inputs. This shift from feature fusion to relational alignment bridges the gap between these spaces, enabling robust detection even when input from one modality is weak.  Our evaluation on the challenging datasets demonstrates that our model sets a new benchmark in detecting rare objects, outperforming existing OVDet models. Additionally, we show that our multi-modal classifiers outperform single-modality models and even surpass fully-supervised detectors.",
    "keywords": [
        "Multimodal learning",
        "object detection"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1Nwsqw0sTm",
    "pdf_link": "https://openreview.net/pdf?id=1Nwsqw0sTm",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses the challenges of open-vocabulary object detection (OVDet), where the goal is to detect objects at inference time that were not seen during training. The authors propose an approach called VOCAL (Vocabulary Alignment Classifier), which integrates visual and textual embeddings by aligning both feature-level and relational structures across these two modalities. This method aims to bridge the gap between visual and textual data, enabling robust detection even when input from one modality is weak or ambiguous."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. It combines textual descriptions and visual examples to identify objects, leveraging the strengths of both modalities to improve detection accuracy.\n2. Instead of simple feature fusion, VOCAL focuses on aligning the contextual relationships between objects in text and images, which is a novel way to handle the misalignment problem in heterogeneous data. The model can adapt to new categories or unseen objects without retraining, which is a significant advantage in dynamic environments where new objects frequently appear.\n3. The evaluation shows that the model outperforms existing OVDet models, setting new benchmarks in detecting rare objects."
            },
            "weaknesses": {
                "value": "1. The method involves complex alignment mechanisms that could be computationally expensive and may require substantial resources for training and inference.\n2. The performance of VOCAL heavily relies on the quality of the text and image embeddings. If the embeddings are not representative, the alignment may not be effective.\n3. While the model can adapt to new categories, the scalability to a very large number of categories or extremely rare objects is not explicitly discussed and could be a challenge.\n4. Although the paper mentions cross-dataset transfer, the generalization of the model to datasets outside of the trained domain is a potential concern that may require further validation."
            },
            "questions": {
                "value": "Pls see the weeknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduce an approach for open-vocabulary object detection (OVDet) that aligns relational structures across visual and textual data to enhance the detection of objects, especially unseen or rare objects. The authors propose a model called VOCAL (Vocabulary Alignment Classifier) that shifts from feature fusion to relational alignment, bridging the gap between visual and textual inputs. VOCAL leverages both text descriptions and image examples to identify objects, addressing limitations such as lexical ambiguity, lack of visual specificity, and unknown class names. The evaluation on challenging datasets shows that VOCAL outperforms existing OVDet models and even surpasses fully-supervised detectors in detecting rare objects."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper presents a sophisticated method for OVDet that focuses on relational alignment between visual and textual data, which is a novel approach in the field.\n\n2. VOCAL demonstrates superior performance in detecting rare objects and outperforms existing OVDet models, which is a significant achievement.\n\n3. The model demonstrates a new benchmark in detecting rare objects and outperforms existing OVDet models, which is a substantial achievement."
            },
            "weaknesses": {
                "value": "1.  The approach may be more complex and computationally intensive than simpler fusion methods, which could be a limitation in resource-constrained environments.\n\n2. The introduction of the Image and Text Encoder results in a detection process that requires more computation, and fairness compared to other OVDet methods needs to be considered.\n\n3. Some related OVDet methods are missing. For example, Distilling DETR with Visual-Linguistic Knowledge for Open-Vocabulary Object Detection ICCV 2023."
            },
            "questions": {
                "value": "See Weaknesses. My major concern is introducing much more complexity compared with previous methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel open-vocabulary detection method that utilizes both textual and visual classifiers, integrating them through feature-level alignment and relational alignment. The author conducts experiments on LVIS to demonstrate its performance on novel categories."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The feature-level alignment and relational alignment for fusing textual and visual classifiers is very interesting.\n2. The weighted contextual embeddings and prototype discovery respectively optimize the methods for constructing textual and visual classifiers."
            },
            "weaknesses": {
                "value": "1.\tThe method\u2019s pipeline is similar to MMOVD[1], with only minor improvements made to the construction and fusion of classifiers. Overall, the novelty might be quite limited.\n2.\tThe method is similar to MMOVD, but lacks critical experiments comparing it with MMOVD, such as evaluations using IN-LVIS as extra data on the LVIS dataset, and MMOVD\u2019s evaluations on cross-dataset transfer detection.\n3.\tThere are missing experiments that prove the effectiveness of the method. 1) Lack of experiments demonstrating that weighted contextual embeddings improve the performance of a text-based classifier compared to simply averaging; 2) Lack of experiments showing that using feature-level alignment and relational alignment is more effective compared to naive fusion strategies like addition.\n4.\tThe comparison experiments between V-CLS and V-Mean are not reasonable. V-CLS, compared to V-Mean, uses both the prototype discovery strategy and additional transformer blocks as the Visual Aggregator. This setup does not validate the effectiveness of the prototype discovery strategy. According to MMOVD[1], using a Visual Aggregator already performs better than directly averaging various visual embeddings. V-CLS should be compared with a Visual Aggregator that does not use the prototype discovery strategy.\n5.\tThere is a lack of hyperparameter analysis for $\\lambda$ and $\\alpha$.\n6.\tResults of open vocabulary object detection evaluations on the COCO dataset are missing.\n\n[1] Multi-Modal Classifiers for Open-Vocabulary Object Detection, ICML 2023"
            },
            "questions": {
                "value": "1. Figure 2 attempts to demonstrate the model\u2019s effectiveness in detecting rare categories, but the examples provided belong to either the frequent or common categories, which does not prove the model\u2019s capability in detecting rare categories. For instance, \u2018knife\u2019, \u2018skateboard\u2019, \u2018belt\u2019, \u2018pillow\u2019, and \u2018bicycle\u2019 are all frequent categories, while \u2018rhinoceros\u2019, \u2018goose\u2019, \u2018kiwi\u2019, and \u2018gull\u2019 belong to common categories. \n2. Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}