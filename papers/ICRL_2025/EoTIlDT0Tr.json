{
    "id": "EoTIlDT0Tr",
    "title": "$\\mathcal{X}^2$-DFD: A framework for e$\\mathcal{X}$plainable and e$\\mathcal{X}$tendable Deepfake Detection",
    "abstract": "Detecting deepfakes (*i.e.*, AI-generated content with malicious intent) has become an important task. Most existing detection methods provide only real/fake predictions without offering human-comprehensible explanations. Recent studies leveraging multimodal large-language models (MLLMs) for deepfake detection have shown improvements in explainability. However, the performance of pre-trained MLLMs (*e.g.*, LLaVA) remains limited due to a lack of understanding of their capabilities for this task and strategies to enhance them. In this work, we empirically assess the strengths and weaknesses of MLLMs specifically in deepfake detection via forgery-related feature analysis. Building on these assessments, we propose a novel framework called $\\mathcal{X}^2$-DFD, consisting of three core modules. \nThe first module, *Model Feature Assessment (MFA)*, measures the detection capabilities of forgery-related features intrinsic to MLLMs, and gives a descending ranking of these features. \nThe second module, *Strong Feature Strengthening (SFS)*, enhances the detection and explanation capabilities by fine-tuning the MLLM on a dataset constructed based on the top-ranked features. \nThe third module, *Weak Feature Supplementing (WFS)*, improves the fine-tuned MLLM's capabilities on lower-ranked features by integrating external dedicated deepfake detectors. \nTo verify the effectiveness of this framework, we further present a practical implementation, where an automated forger-related feature generation, evaluation, and ranking procedure is designed for *MFA* module; an automated generation procedure of the fine-tuning dataset containing real and fake images with explanations based on top-ranked features is developed for *SFS* model; an external conventional deepfake detector focusing on blending artifact, which corresponds to a low detection capability in the pre-trained MLLM, is integrated for *WFS* module. \nExperimental results show that the proposed implementation enhances overall detection performance compared to pre-trained MLLMs, while providing more convincing explanations. \nMore encouragingly, our framework is designed to be plug-and-play, allowing it to seamlessly integrate with more advanced MLLMs and external detectors, leading to continual improvement and extension to face the challenges of rapidly evolving deepfake technologies.",
    "keywords": [
        "Deepfake Detection; Multimodal Large Language Models; Media Forensics"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We propose an explainable and extendable framework to enhance deepfake detection via multimodal large-language models.",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=EoTIlDT0Tr",
    "pdf_link": "https://openreview.net/pdf?id=EoTIlDT0Tr",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a new method for deepfake detection based on MLLMs. The method enhances the deepfake detection capabilities of MLLMs by ranking forgery-related features, strengthening strong features and augmenting weak features. In addition, the paper raises the explainability of MLLMs' inference through fine-tuning. Evaluations on multiple datasets prove the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe paper proposes Model Feature Assessment(MFA), Strong Feature Strengthening (SFS) and Weak Feature Supplementing (WFS) to enhance the detection capabilities of MLLMs, contributing to MLLM-based algorithms on deepfake detection.\n\n2.\tThe paper points out the limitations of pretrained MLLMs and introduces a pipeline for constructing datasets based on more effective forgery features.\n\n3.\tThe experiments provide different protocols and a wide range of evaluation to prove the effectiveness of the method."
            },
            "weaknesses": {
                "value": "1.\tThe authors claim that human-annotated VQA data (zhang et al, 2024) may not be ideal as standard answers for fine-tuning MLLMs. Instead, they propose using MLLM-generated data to fine-tune MLLMs. This raises an important question: if all answers are expressed in human's natural language, how can MLLM-generated data establish a more 'standard' answer?\n\n2.\tThe generated questions lack reliability due to the absence of a human verification process to ensure the accuracy of these fake features. Additionally, the authors use data generated by MLLMs to evaluate the MLLMs themselves, which raises concerns about the validity of this approach. This circular logic is confusing and calls into question the robustness of the evaluation.\n\n3.\tThe authors claim to study the intrinsic capabilities of MLLMs on detection. However, Strong Feature Strengthening (SFS) and Weak Feature Supplementing (WFS) seem to be a process of dataset construction based on more effective features rated by the ranking in Sec. 3.3, thus becoming less pervasive at how it improves the explainability of MLLMs. More analyses are expected to verify the contributions of the procedure to the enhancement of MLLMs.\n\n4.\tThe explanation ability of X^2-DFD should also be quantitatively evaluated in the main paper. The authors mention conducting a human study to assess explanation performance in Fig. 7, but they only verbally claim that their model is superior and provide a single qualitative example in the appendix. This lacks reliability and ignores the main contribution of explainability."
            },
            "questions": {
                "value": "1.\tIn Figure 1, I wonder why of X^2-DFD predicts a lower fake probability on the second picture compared with a pretrained MLLM. Meanwhile, the response of X^2-DFD considers the image completely fake, which is contrary to the predicted probability.\n\n2.\tFigure2 is too small, the numbers are barely visible.\n\n3.\tThere are undefined and inconsistent expressions. In Sec. 4.2.3 and Figure 5, The abbreviation of WCS is undefined in the aforementioned method. In Sec. 5.4 and Tab. 3, GCS is never seen before.\n\n4.\tMore details on the implementation of fine-tuning MLLMs should be provided. Which part of parameters are trained? The dataset information should be detailed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors use a multi-modal large model to enhance the interpretability of face forgery detection. To construct an effective fine-tuning dataset, they propose first ranking various forgery features and then building a corresponding VQA dataset based on the top-ranked features. Additionally, the detection results from smaller models are fed into the LLM as extra guidance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The recognition effect has been greatly improved, especially for cross-dataset scenarios.\n2. It is the first exploration to design a novel MLLMs-based framework for explainable forgery detection.\n3. The experiments are thorough and the comparisons are comprehensive, effectively demonstrating the validity of the proposed method.\n4. The paper is well-organized and easy to understand."
            },
            "weaknesses": {
                "value": "1. It is hard to believe the simple fine-tuning of multi-modal large models typically can achieve such satisfactory results. The public source code can help prove its reproducibility.\n2. Directly using the prediction probabilities of the smaller model as text input for the LLM seems unreasonable, as LLMs lack numerical reasoning capabilities.\n3. Since Pretrained LLMs lack fundamental knowledge about true/false classification, ranking based on relevance generated by Pretrained LLMs may not be meaningful. It may be that random selection of other questions has a similar effect. Can the author provide further analysis?\n4. Authors only select the specific LLM and MLLM models in the framework. How to ensure the generality and reasonability of the conclusion?"
            },
            "questions": {
                "value": "1. Only the LLaVa model is chosen as the typical MLLM. Do most MLLMs contain similar properties? The author should point this out.\n2. How can the commonality of the definition of forgery features be ensured? Authors utilized the GPT-4o to generate a questions list, how about other LLM models? Different LLM models may generate different question lists to extract forgery-related features.\n3. In TABLE 2. Why the proposed method is inferior to comparison methods with fsgan and inswap generation models? Please give a deeper theoretical analysis.\n4. In TABLE 3, the GCS concept was not explained clearly, and the ablation study settings are somewhat confusing.\n5. To ensure the integrity of the experiment, authors also could provide in-domain results in the FF++ dataset."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes X2-DFD, a novel framework that utilizes Multimodal Large Language Models (MLLMs) for explainable and extendable DeepFake Detection. The basic idea of the paper is quite interesting. And the paper seems to the first to systematically assess the inherent capabilities of MLLMs specifically in deepfake detection, which is valuable."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.The paper first systematically assesses the inherent capabilities of MLLMs specifically in deepfake detection, and has found that MLLMs have varying discriminating capabilities on different forgery features.\n2.It proposes a novel approach to fine-tune the MLLM to make it better adaptive with the deepfake detection task. \n3.Besides MLLM, it integrates external dedicated detectors (EDDs) to fill the gap where MLLMs show limitations."
            },
            "weaknesses": {
                "value": "Although interesting to see MLLM being applied in improving Deepfake detection performance, I still wonder if the interpretability brought by  MLLM is reliable. Therefore I have two major concerns:\n1.\tThe explainability of large language models (LLMs) like GPT, BERT, and similar models is a complex and evolving area. Generally, LLMs are not inherently transparent or easily interpretable due to their massive size and the intricacy of their neural architectures. So, how do you ensure the explainability brought by LLMs is \u201creal\u201d explainability instead of merely providing some texts describing why an image is fake or not. \n2.\tBased on my first point, how would you evaluate the explainability of the proposed method? There seems to be a lack of metric or ground-truth involved in your research. I wonder if human evaluation is fairly enough to evaluate the explainability since human subjects involved in the experiments also have no real knowledge about how the fake data was created and why it is fake. By the way, the details of the human experiment are not provided. \nIn my opinion, it will be more promising to integrate the knowledge about the deepfake creation and the true difference between real and fake data into the detection stage in order to improve the detector\u2019s explainability. Nevertheless, I still think it is a good paper which provides a good trial on solving deepfake detection with MLLMs."
            },
            "questions": {
                "value": "The extendability of the proposed method should be further explained. Most efforts of the paper are on the explainability. I do not clearly see what you mean by \u201cextendable\u201d."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a novel framework called X2-DFD, consisting of three core modules (i.e., Model Feature Assessment (MFA), Strong Feature Strengthening (SFS), and Weak Feature Supplementing (WFS)) for explainable deepfake detection. The results on several benchmark datasets demonstrate its effectiveness. However, the overall story and solution is interesting but with too much human workload and tricks while without enough technical contribution."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper adopts MLLMs for explainable deepfake detection. The cross-dataset performance seems good. The structure of the paper is clear."
            },
            "weaknesses": {
                "value": "1. The cross-manipulation performance presented in Table 2 is suboptimal despite using a trainable MLLM and a non-trainable GPT-4. Given the extensive parameters and pre-trained data leveraged, the proposed method cannot achieve SOTA performance. The capability of handling unseen attacks limits its applications in real-world scenarios.\n\n2. X2-DFD\u2019s explainablity is unsatisfactory for some samples, such as the left bottom sample in Figure 1, which outputs vague statements like \u201c...with an unusual layout and unnatural skin tone...\u201d. This description is oversimplified. It cannot provide sufficient information to users and offers less explanatory power than traditional methods like Grad-CAM (which can provide heatmaps). Is this a common issue across samples, and how is explainablity assessed (e.g., through user studies)?\n\n3. The authors ignore prior work using LLMs for deepfake detection and over-claim their contribution (Lines 100-103)(r1,r2). The summary on Lines 067-075 misrepresents previous work [r1] by claiming\u201c...fail to provide intuitive and convincing explanations behind predictions.\u201d\n\n[r1] FakeBench: Probing Explainable Fake Image Detection via Large Multimodal Models\n[r2]Common Sense Reasoning for Deep Fake Detection\n\n4. The authors do not present experiments on low-quality images or post-processed images.\n\n5. The ablation study is insufficient. What would happen if WFS were applied to Strong Features or SFS to Weak Features? Or if both Strong and Weak Features used the same module?\n\n6. The idea of using an external model (EDD in this paper) to support LLMs is not novel.\n\n[r3] LISA: Large Language Instructed Segmentation Assistant\n[r4] AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models\n\n7. The reason for choosing LLaVA & GPT-4 is unclear. What would happen with different MLLMs, or different parameter configurations (e.g., LLaVA-13B/7B or GPT-3)? X2-DFD\u2019s sensitivity to various MLLMs remains unexplored.\n\n8. Writing issues:\n(1) Citation inconsistencies, e.g., the reference in L136 is missing. ArXiv references, especially in L598-601 and L619-621, follow inconsistent formats.\n(2) Missing percentage symbols in metrics reported on L175-180.\n(3) Metric explanations occupy excessive space.\n(4) Table 4 lacks clarity regarding which metrics are presented.\n\n\n9 .During the reasoning process, the paper utilizes an external reasoning engine for analysis. However, if the results from this engine are subpar, it could significantly impact the model's performance. Unfortunately, the paper does not discuss this in detail\uff08Especially, the training details of external expert models are not mentioned\uff09. Additionally, in Table 3's ablation study, the minimal performance difference between the EDD only and final models suggests limited gains. Furthermore, using an untrained MLLM as a baseline for comparison seems unreasonable.\n\n10. In the strong feature data selection phase, the paper considers only one metric, and similarly, it evaluates performance using just that metric. This lack of additional metrics related to accuracy (ACC) or natural language processing (NLP) interpretability may limit the comprehensiveness of the results.\n\n11.The paper lacks a discussion on how common large models would perform in comparative experiments after fine-tuning, such as the effects of switching to different LLM base models, which is worth exploring further."
            },
            "questions": {
                "value": "see weakness for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}