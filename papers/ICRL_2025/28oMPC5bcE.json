{
    "id": "28oMPC5bcE",
    "title": "UNComp: Uncertainty-Aware Long-context Compressor for Efficient Large Language Model Inference",
    "abstract": "Deploying large language models (LLMs) is challenging due to their high memory and computational demands, especially during long-context inference. While key-value (KV) caching accelerates inference by reusing previously computed keys and values, it also introduces significant memory overhead. Existing KV cache compression methods\u2014such as eviction and merging\u2014typically compress the KV cache after it is generated and overlook the eviction of hidden states, failing to improve the speed of the prefilling stage. Additionally, applying a uniform compression rate across different attention heads can harm crucial retrieval heads in needle-in-a-haystack tasks due to excessive compression. In this paper, we propose UNComp, an uncertainty-aware compression scheme that leverages matrix entropy to estimate model uncertainty across layers and heads at the token sequence level. By grouping layers and heads based on their uncertainty, UNComp adaptively compresses both the hidden states and the KV cache. Our method achieves a 1.6x speedup in the prefilling stage and reduces the KV cache to 4.74% of its original size, resulting in a 6.4x increase in throughput and a 1.4x speedup in inference with only a 1.41% performance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms the full-size KV cache even when compressed to 9.38% of its original size. Our approach offers an efficient, training-free Grouped-Query Attention paradigm that can be seamlessly integrated into existing KV cache schemes.",
    "keywords": [
        "KV Cache",
        "GQA",
        "Matrix entropy",
        "Uncertainty",
        "Efficient Inference"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=28oMPC5bcE",
    "pdf_link": "https://openreview.net/pdf?id=28oMPC5bcE",
    "comments": [
        {
            "summary": {
                "value": "This paper focuses on the high latency and memory cost associated with long-context LLM inference by proposing UNComp, a training-free method that combines model compression and KV cache compression with a matrix entropy-based, dynamically allocated compression ratio. Specifically, the approach involves identifying similar layers and heads through offline search for compression, while using SnapKV with dynamic compression ratio of the KV cache with an approximated dynamic window size during inference. This paper test their method on the LongBench and NIAH benchmarks across four LLMs (Llama-2-7B/13B, Llama-3-8B, Mistral-7B-v0.1). Results indicate that UNComp offers slight improvements over baselines such as SnapKV, PyramidKV, and CHAI at the same compression ratio, although performance loss occurs when applying model compression."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper focuses on a practical and relevant topic.\n- The proposed matrix entropy-based method with dynamically allocated sparsity is intuitive."
            },
            "weaknesses": {
                "value": "1. The proposed approach is relatively straightforward and could be viewed as a combination of existing methods. For instance, the model compression component can function independently, yet no necessary baselines are provided for comparison in this area.\n2. The paper lacks sufficient ablation studies and analysis to demonstrate the contribution of each module in the proposed method. Specifically:\n  - The improvement over PyramidKV appears to mainly derive from the dynamic approximated window size selection based on matrix entropy. However, there is no ablation study examining the effect of applying dynamic approximated window sizes to PyramidKV, or the performance impact of applying PyramidKV\u2019s dynamic sparse ratio within UNComp. A comparison of dynamic sparsity ratios between this method and PyramidKV is also missing.\n  - There is no analysis of the dynamic approximated window size across different layers and heads.\n3. The experiments are limited to LongBench and NIAH with a maximum context length of 32k, with no results for other state-of-the-art long-context benchmarks or longer contexts, such as RULER[1] and InfiniteBench[2].\n\n[1] RULER: What\u2019s the Real Context Size of Your Long-Context Language Models?  \n[2] InfiniteBench: Extending Long Context Evaluation Beyond 100K Tokens, ACL 2024."
            },
            "questions": {
                "value": "- **Q1**: Do you have any analysis of the dynamic sparsity ratio compared with PyramidKV?\n- **Q2**: Do you have any analysis of the dynamic approximated window size across different layers and heads?\n- **Q3**: Do you have results for other long-context benchmarks and longer context windows, such as RULER[1] and InfiniteBench[2]?\n- **Q4**: Typo corrections needed for quotation marks, e.g., #390, #511, #713-715. And incorrect references, e.g., in Figure 3's legend, \"Sec 4.2\" might be \"Sec 3.2\" (#294, #298)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces UNComp, an uncertainty-aware compression method designed to address memory and computational challenges associated with large language models (LLMs) during long-context inference. UNComp uses matrix entropy to estimate model uncertainty, applying selective compression across layers and attention heads based on these uncertainty levels. This approach preserves crucial information while enhancing efficiency in both memory and computational requirements."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper introduces a \u001dmatrix entropy to quantify the amount of information in each layer across the token sequence, which is then effectively applied for compression.\n- Using the metric at both the layer and head levels, the authors propose customized inter-layer and inter-head compression strategies, allowing for a more targeted approach to model compression.\n- The method undergoes extensive evaluation on diverse benchmarks, consistently delivering superior performance at comparable compression ratios."
            },
            "weaknesses": {
                "value": "* While Figure 3 aims to illustrate the overall workflow of the proposed method, it presents too much information at once, which makes it difficult to follow. One suggestion to improve readability is to break it down into subfigures or add step-by-step numbering to guide the reader through each part of the process. This adjustment would make the method\u2019s workflow clearer and easier to understand.\n* An essential aspect of evaluating compression methods is understanding the trade-off between accuracy and throughput (or latency). However, this paper separates these metrics: Table 1 presents only accuracy, while Table 3 focuses solely on latency, making it challenging to assess the accuracy-throughput balance across different methods at a glance. Adding a combined table or figure that displays both accuracy and throughput would better support comparisons of this trade-off.\n* The paper primarily addresses end-to-end accuracy and latency but lacks an analysis of the compression ratio at each layer or head level within a single model (e.g., Llama3-8B-Instruct). Including this breakdown would provide greater insight into the internal dynamics and behavior of the model when applying the proposed method.\n* Although the authors claim that the proposed method achieves faster performance than CHAI despite a lower compression ratio, the reasons for this improvement are not sufficiently explained. Offering more details on which specific aspects of the method contribute to greater hardware efficiency and speed, beyond just compression ratio, would make this claim more convincing."
            },
            "questions": {
                "value": "Please see the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents UNComp, an innovative uncertainty-aware compression scheme designed to enhance the efficiency of large language models (LLMs) during long-context inference."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The writing is clear and easy to follow.\n2. The source code is provided."
            },
            "weaknesses": {
                "value": "1. The number of groups seems to have little impact on performance, and sometimes fewer groups even yield better results. So why the complex design? However, if a uniform compression rate is applied, it feels like the paper doesn't contribute anything new.\n2. Different layers have varying levels of attention to tokens, so \"using the attention scores of the current layer to predict the tokens to be evicted in the next layer\" may pose significant issues.\n3. Lack of some baselines:  streamingLLM[1],Quest[2],doublesparse[3] \n\n[1] Efficient Streaming Language Models with Attention Sinks https://arxiv.org/abs/2309.17453\n[2] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference https://arxiv.org/abs/2406.10774\n[3] Post-Training Sparse Attention with Double Sparsity https://arxiv.org/abs/2408.07092"
            },
            "questions": {
                "value": "1. This method has many hyperparameters; how did you select them?\n2. If different heads retain a different number of tokens, does it affect parallel computation? If padding is used, how can true acceleration be achieved?\n3. Why does a deeper layer necessarily retain fewer tokens? From the picture, it appears that the effective rank may fluctuate."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper finds that:\n1. higher-layer tokens gather more information, and a small number of tokens can represent the entire sequence\n2. For heads on the same layer, those with a higher effective rank should evict fewer tokens because this head is more informative\n3. Tokens of the same head in different layers gradually share information as the layers deepen, while tokens of different heads do not share information as the layers deepen. \n\nTherefore, based on the matrix entropy and effective rank, the KV cache and hidden states is compressed with training-free method, which achieves a compression rate of 4.74%, with a throughput increase of 6.4\u00d7 and a 1.4\u00d7 inference speedup in a single batch, incurring only a 1.41% performance loss."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The concept of matrix entropy and effective rank is novel and useful for determining the token redundancy."
            },
            "weaknesses": {
                "value": "The compression is based on the calculation of attention score and related accumulation, which introduces the onlince cost."
            },
            "questions": {
                "value": "The simplication of the calculation of importance score may be the fulture major direction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}