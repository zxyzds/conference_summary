{
    "id": "5xmXUwDxep",
    "title": "Manifold Constraint Reduces Exposure Bias in Accelerated Diffusion Sampling",
    "abstract": "Diffusion models have demonstrated significant potential for generating high-quality images, audio, and videos. However, their iterative inference process entails substantial computational costs, limiting practical applications. Recently, researchers have introduced accelerated sampling methods that enable diffusion models to generate samples with far fewer timesteps than those used during training. Nonetheless, as the number of sampling steps decreases, the prediction errors significantly degrade the quality of generated outputs. Additionally, the inherent exposure bias in diffusion models causes errors to propagate and amplify, further introducing non-negligible inaccuracies in inference. To address these challenges, we leverage a manifold hypothesis to explore the exposure bias problem in depth. Based on this geometric perspective, we propose a manifold constraint that effectively reduces exposure bias during accelerated sampling of diffusion models. Notably, our method involves no additional training and requires only minimal hyperparameter tuning. Extensive experiments on high-resolution datasets demonstrate the effectiveness of our approach, achieving a FID score of 15.60 with 10-step SDXL on MS-COCO, surpassing the baseline by a reduction of 2.57 in FID.",
    "keywords": [
        "Diffusion Models",
        "Exposure Bias"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5xmXUwDxep",
    "pdf_link": "https://openreview.net/pdf?id=5xmXUwDxep",
    "comments": [
        {
            "summary": {
                "value": "To narrow the gap between the training of the sampling phase of diffusion models, the authors analyze the diffusion processes from the view of the manifold. They propose to compute the statics of all intermediate diffusion variables and calibrate the sampling process based on the computed statics (variance, mean, \\etc). Experiments show that the proposed method can reduce the sampling steps while maintain the generation quality."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed method improves sample quality without adding substantial computational overhead.\n2.Unlike some prior methods, this approach does not require model retraining or intensive hyperparameter tuning.\n3. The manifold constraint method shows improved performance across various high-resolution datasets, achieving better FID scores with fewer sampling steps.\n4. The paper provides both theoretical analysis and empirical evidence to support its method, including experiments on multiple tasks like image and video generation."
            },
            "weaknesses": {
                "value": "1. The proposed method needs to be verified on more diffusion schedulers, such as DPMSolver, PNDM.\n\n2. Some ODE-based diffusion models such as rectified flow and consistency models can reduce the sampling steps to two or even one. The proposed method focuses on accelerating the sampling process but is not compared with these fast-sampling diffusion models.  The authors are encouraged to apply their methods to more recent diffusion models (\\ie SD3, FLUX) to show their priority and general ability."
            },
            "questions": {
                "value": "See the weakness above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed.",
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "- This paper proposes a method for improving the performance in accelerated diffusion sampling algorithms\n- The paper identifies the exposure bias in accelerated diffusion sampling \n- The method applies manifold constraint for reducing the exposure bias that occurs in accelerated sampling.\n-  The paper presents evaluations of the methods showing improvements over the baseline.\n- A discussion on the geometric view of the exposure bias is presented."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well written\n- The method is evaluated on multiple diffusion models trained on different datasets\n- The method is simple yet effective. It does not require any further training and the additional computations are marginal.\n- The approach shows an improvement over the baselines in most cases"
            },
            "weaknesses": {
                "value": "- The derivation of section 4.2 is relatively weak, many assumptions and loose steps need to be either refined or omitted.\n  - In section 4.2, you assume that $E[\\epsilon_\\theta^t]=0$, but this is not always the case.\n  - In Equation (12) the authors utilize the fact that $\\frac{1}{n}\\sum_{i=1}^n \\hat{x}_i \\approx 0$, which does not always hold.\n  - Equation (15) does not hold, an expectation value is required for it to be true\n- The evaluations include only comparison to DDIM, even though there are many accelerated samplers that achieve much better results [1,2,3], adding them to the tables is very important for evaluating the method.\n\n[1] GENIE: Higher-Order Denoising Diffusion Solvers\n\n[2] DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps\n\n[3] DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models."
            },
            "questions": {
                "value": "- Figure 3: the x-axis title is not clear, what do you mean by denoising steps if the sampling steps are given in the legend? and in general the figure needs to be explained properly\n- In section 4.2, you assume that $E[\\epsilon_\\theta^t]=0$, but this is not always the case.\n- Equation 15 does not make any sense without expectation value."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper views the issue of the quality degradation during accelerating diffusion model\u2019s inference on the perspective of the explore bias. The authors point out that explore bias is an important reason that contributes to the loss of image quality when reducing inference steps. The noisy prediction will direct to inaccurate manifolds and thus the errors would be accumulated and amplified. A manifold constraint is proposed to curate this bias and thus lead to better image quality when significantly reducing inference steps."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed method is well supported by a series of proofs with some assumptions.\n2. The quantitative results look very promising, and largely outperforms the alternative.E.g.,  in Table 3, MCDO with 4 steps has better results than DDIM with 5 steps.\n3. The method is well motivated and the writing logically reasonable and flows well."
            },
            "weaknesses": {
                "value": "1. Assumption 3 is a very important part in deriving the manifold constraint. I think it is too strong. I understand it's motivated by Equation (17), however \\hat{x}_0 could have a different distribution of x_0 which is inaccessible during inference. Could you elaborate more?\n2. There are some details not explained well for some key equations/explanations. I added those in the questions below."
            },
            "questions": {
                "value": "1. In table4, why do fewer steps have lower FID?\n2. Can you add more details of how to get equation 11 and 16 in the appendix?\n3. I didn\u2019t understand why \\epslon_t is equal to \\sqrt{n} when n is large in L261? Can you explain?\n4. Can you elaborate on L271? In my understanding, in fig3, var(x_t) decreases faster as reducing steps, thus potentially making d(.) larger than r_t in later steps. But figure 3 is only on one sample, how to generalize this observation?\n5. The qualitative examples look a bit over-saturated, it would be helpful if you can also show an oracle results (e.g., 1000 steps) on the side for comparison."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an approach to enhance the efficiency and accuracy of diffusion models by addressing the issue of exposure bias in accelerated sampling. Leveraging a manifold hypothesis, the authors introduce a manifold constraint that reduces error accumulation during sampling without requiring additional training or extensive tuning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The use of the manifold constraint is an interesting idea, which addresses exposure bias without adding additional training costs during sampling.\n2. The use of well-chosen visualizations enhances the readability of the method section, conveying key information and clarifying the approach.\n3. The paper is technically clear and well-organized, with the proposed method thoroughly explained."
            },
            "weaknesses": {
                "value": "1. The term \"denoising observations\" requires a clear and precise definition, as its current use lacks specificity. A more rigorous description would help readers to understand and improve the technical clarity of the paper.\n2. The pre-computation process still requires a full denoising sequence (e.g., 1000 steps), which incurs substantial computational cost, especially when applying the proposed method to new datasets or domains. It is suggested that potential strategies for reducing this computational cost be discussed or that an analysis of the trade-offs between the number of steps in pre-computation and the method's performance be provided.\n3. The number of samples and their diversity will influence the resulting approach $v_t$. However, the experiments simply set the sample number to 20 without discussing the diversity of prompts or other characteristics of the samples. Including these details would be valuable for readers to have a better understanding, and provide insights for the community. It is recommended to conduct an ablation study on the impact of sample number and diversity on the performance of the proposed method, or to provide more details on how you selected the 20 samples used in the experiments."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "From the manifold hypothesis, the paper proposes a method to reduce Exposure Bias by adjusting the variance of $x_t$ at each step to match the variance of $q_t$ made by the forward diffusion process. The paper demonstrates that this approach yields FID gains across various model and datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "They validated the method in various datasets and methods. \n\nIt is good to see that the method proposed from this work can be combined with the method from previous literature, i.e., DDIM-MCDO^\\dagger."
            },
            "weaknesses": {
                "value": "**1. strong assumption**\n\nAssuming that the manifold can be understood solely by considering variance is too strong an assumption. It might be better to tone it down to suggest that this approximation is sufficient for performance improvement."
            },
            "questions": {
                "value": "**Q1**: Using the statistics of the data directly in generation\u2014could this be seen as an FID \"hack\"? For example, I\u2019m curious how the FID would change if the mean and variance of the generated data w/o MCDO were adjusted to match \\( q_0 \\).\n\n**Q2**: Does this method can improve other ODE solvers like DPM-solver++ [1] or PNDM [2]? I know that DDIM performs poorly when the NFE is below 50. I also want to see the results where NFE is around 50.\n\n[1]: DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models ([Arxiv](https://arxiv.org/abs/2211.01095))\n[2]: Pseudo Numerical Methods for Diffusion Models on Manifolds ([ICLR22](https://arxiv.org/abs/2202.09778))\n\n**Q3**: (Minor) When comparing performance, I recommend plotting FID on the y-axis and NFE on the x-axis for clarity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the exposure bias of accelerated diffusion model sampling from geometry perspective. The authors extend the previous manifold constraint theory with more detailed description of pixel variance, claiming that both exposure bias and truncation error account for the performance degradation. To this end, the authors propose to pre-calculate the reference pixel variance, which serves as a correction during inference. Such method achieves a training-free and easy-to-implement solution to performance degradation. Comprehensive experiments confirm the efficacy of the proposed algorithm."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well structured and easy to follow. Most of the derivation is clear.\n\n- The paper extends the previous manifold constraint strategy with deeper study of the pixel variance and the consequent exposure bias, providing a novel perspective of geometric technique in diffusion models.\n\n- The proposed algorithm is overall both efficient and effective, inference time cost barely increases.\n\n- Quantitative experiments are convincing and extensive."
            },
            "weaknesses": {
                "value": "- There seem some theoretical flaws in the draft, harming the soundness:\n  1. Eq. (15) is wrong, when $x$ and $y$ are orthogonal, one can only deduce that $|x+y|^2=|x|^2+|y|^2$. Besides, what is the definition of $x_0$ and $\\epsilon_t$? $\\epsilon_t$ and $x_0$ may not be orthogonal if no further assumption are made.\n  2. I cannot understand the relation between Eq. (18) and analytical form of $Var(x^{(t)}_0)$ and $Var(x_t)$, $Var(\\cdot)$ is supposed to be the pixel variance as claimed in Eq. (12) which is a scalar.\n  3. Eq. (16) and Eq. (19) are similar, but the authors conclude differently. If minimizing right hand side in Eq. (19) could lead to the distance reduction, then so could minimizing right hand side in Eq. (16) be. They are all the **lower bounds** of distance of samples to manifold.\n  4. If the authors insist that nonzero $|x_0|$ affects the derivation, then (1) since $\\hat{x_t}\\in\\mathcal{M}_t$, one can simply choose $x_0=0$ (which is reasonable since the authors have already assumed zero mean in L346), or (2) move the term with $|x_0|$ outside the absolute value using $|a+b-c|\\geqslant|b-c|-|a|$, which is similar to the form of Eq. (19). The authors should make further clarification.\n  5. Why assume zero mean in L346? $\\hat{x}_0^{(t)}$ is the denoise observation at timestep $t$, somewhat a data sample with no noise. Then why is the case? The authors could calculate the mean to confirm the reasonability.\n\n- Figs. 4 and 6 only employ 64 samples, which seems inconvincing.\n\n- Visualization in Fig. 7 fails to be photorealistic with obvious color shift artifact, which is also the case in Fig. 12."
            },
            "questions": {
                "value": "- It is intuitive that applying MCDO with larger NFEs or better sampler will achieve weaker improvements. I am curious about the comparison on better sampler like Heun or DPM-Solver. There is also no discussion about applicability on high-order samplers.\n\n- MCDO is proposed for manifold constraint to relieve exposure bias, how will the efficacy vary if different CFG scales are set? Larger CFG scale may also lead to severe exposure bias."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}