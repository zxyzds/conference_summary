{
    "id": "pbJMPo4HwR",
    "title": "Learning Monotonic Attention in Transducer for Streaming Generation",
    "abstract": "Streaming generation models are increasingly utilized across various fields, with the Transducer architecture being particularly popular in industrial applications. However, its input-synchronous decoding mechanism presents challenges in tasks requiring non-monotonic alignments, such as simultaneous translation, leading to suboptimal performance in these contexts. In this research, we address this issue by tightly integrating Transducer's decoding with the history of input stream via a learnable monotonic attention mechanism. Our approach leverages the forward-backward algorithm to infer the posterior probability of alignments between the predictor states and input timestamps, which is then used to estimate the context representations of monotonic attention in training. This allows Transducer models to adaptively adjust the scope of attention based on their predictions, avoiding the need to enumerate the exponentially large alignment space. Extensive experiments demonstrate that our MonoAttn-Transducer significantly enhances the handling of non-monotonic alignments in streaming generation, offering a robust solution for Transducer-based frameworks to tackle more complex streaming generation tasks. Codes are publicly available in supplementary materials.",
    "keywords": [
        "streaming generation",
        "simultaneous translation",
        "Transducer"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=pbJMPo4HwR",
    "pdf_link": "https://openreview.net/pdf?id=pbJMPo4HwR",
    "comments": [
        {
            "summary": {
                "value": "This paper discusses the challenges faced by Transducer-based streaming generation models, particularly in tasks requiring non-monotonic alignments, such as simultaneous translation. These challenges arise from the input-synchronous decoding mechanism of Transducers, which can result in suboptimal performance. To address this, the authors propose integrating a learnable monotonic attention mechanism within the Transducer architecture. This mechanism uses a forward-backward algorithm to calculate the posterior probability of alignments between predictor states and input timestamps. Consequently, it allows for the estimation of context representations of monotonic attention during training, enabling the model to adaptively adjust its attention scope based on predictions. This innovative approach eliminates the need to explore the exponentially large alignment space. Experiments reveal that the proposed MonoAttn-Transducer significantly improves performance in streaming generation tasks dealing with non-monotonic alignments. The codes for this study are available in the supplementary materials."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors propose a learnable monotonic attention mechanism within the Transducer architecture to solve the non-monotonic alignments in streaming generation.\n\n2. The propsoed method is effectiveness and easy to reproduce."
            },
            "weaknesses": {
                "value": "1. I am unable to discern the difference between the proposed monotonic attention mechanism and the cross-attention mechanism.\n\n2. Furthermore, the authors should compare the cross-attention approach (Liu et al., 2021; Tang et al., 2023) with other methods that utilize input history."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes MonoAttn-Transducer, a novel approach to enhance Transducer models with learnable monotonic attention for streaming generation tasks. The key contributions are:\n\n- A method to integrate monotonic attention into Transducer's architecture while maintaining its efficient training through the forward-backward algorithm.\n- A training algorithm that uses posterior alignment probabilities to estimate context representations for monotonic attention.\n- A chunk synchronization mechanism to bridge the gap between training and inference.\n- Extensive experiments demonstrating improved performance on simultaneous translation tasks."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Technical Innovation:\n\n- The proposed method cleverly solves the exponential state space problem by using posterior alignments to estimate context representations\n- The solution maintains the same computational complexity as vanilla Transducer\n- The chunk synchronization mechanism shows thoughtful consideration of practical deployment\n\nTheoretical Foundation:\n\n- The approach is well-grounded in probability theory and previous work on Transducers\n- The mathematical derivations are sound and clearly explained\n- The relationship between prior and posterior alignments is well-analyzed\n\nEmpirical Results:\n\n- Comprehensive experiments on MuST-C dataset\n- Strong improvements over baseline Transducer (+0.75-1.0 BLEU, +0.95-2.06 COMET)\n- Thorough ablation studies and analysis\n- Competitive performance against SOTA methods"
            },
            "weaknesses": {
                "value": "Limited Experimental Scope:\n\n- Experiments focus only on speech-to-text translation.\n- Only two language pairs (En->De, En->Es) are tested.\n- No experiments on other streaming generation tasks like ASR or TTS.\n\nTraining Efficiency:\n\n- The paper doesn't discuss training time comparison with baseline Transducer.\n- Memory usage analysis could be more detailed.\n- No discussion of potential overhead from posterior alignment calculation.\n\n\nAlgorithmic Limitations:\n\n- The method still requires chunk-based processing.\n- The impact of chunk size on performance could be better theoretically explained.\n- The choice of diagonal prior distribution seems somewhat arbitrary."
            },
            "questions": {
                "value": "/"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper describes an extension for Transducers for tasks where the input and output are not monotonically aligned, such as speech translation. The method is evaluated on two languages of the MuST-C benchmark and it is compared to existing methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper addresses an interesting setting which has practical implications\n* The method is described in detail and a large portion of the paper is dedicated to this description."
            },
            "weaknesses": {
                "value": "* The method is fairly complex, requiring a range of steps in addition to Transducer training as shown in Algorithm 1 (which is already complex).\n* The description of the method is sometimes not very clear. It would be helpful to provide intuition in addition to equations.\n* The evaluation was done on languages with relatively similar word order: En-Es and En-De differ in their word order but much less so, then say En-Chinese or English and any non-European language. There are speech translation benchmarks which enable these settings, e.g., FLEURS and which would provide more interesting results.\n* It seems a bit surprising that the BLEU improvements for En-Es and En-De between Transducer and the new method seem fairly similar across most chunk sizes (about 1 BLEU, Table 2). I would have expected En-De to benefit more the new method than En-Es given that the word order of En-De is more different."
            },
            "questions": {
                "value": "* Did you consider evaluating on other settings than En-De/En-Es where word order is more different?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a solution to the challenges faced by Transducer-based streaming generation models in handling non-monotonic alignments, particularly in simultaneous translation tasks. The authors introduce a learnable monotonic attention mechanism that integrates with Transducer decoding. Utilizing the forward-backward algorithm, they infer alignment probabilities between predictor states and input timestamps, allowing adaptive adjustments in attention scope. Experimental results show significant performance improvements of the MonoAttn-Transducer, offering a robust method for complex streaming generation tasks. Their code is publicly available."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper improves the Transducer architecture, making it more suitable for streaming generation tasks.  \n2. The experiments demonstrate that the proposed method achieves commendable performance in the Speech-to-text Simultaneous Translation task."
            },
            "weaknesses": {
                "value": "1. The experiments in this paper only provide results on the Speech-to-Text Simultaneous Translation task, and do not validate the method on other streaming generation tasks.  \n2. Under lower latency conditions (about 1000 ms) for the EnDe and EnEs tasks, the performance of MA-T appears to be inferior to that of CAAT, another transducer-based method. This may indicate that the proposed method in this paper is not highly effective.  \n3. The proposed method in this paper shares a very similar model structure with both CAAT (Liu et al. 2021) and  MILK (Arivazhagan et al 2019), which raises concerns about the lack of novelty in the article.\n\n**Reference**\n\n[1]Dan Liu, Mengge Du, Xiaoxi Li, Ya Li, and Enhong Chen. Cross attention augmented transducer networks for simultaneous translation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.\n\n[2]Naveen Arivazhagan, Colin Cherry, Wolfgang Macherey, Chung-Cheng Chiu, Semih Yavuz, Ruoming Pang, Wei Li, and Colin Raffel. Monotonic infinite lookback attention for simultaneous machine translation. In Anna Korhonen, David Traum, and Llu\u00b4\u0131s Marquez (eds.), ` Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.2019."
            },
            "questions": {
                "value": "1. After incorporating a unidirectional encoder, the model structures of MA-T and CAAT are very similar. What are the specific differences between the two? How does MA-T manage to perform attention computations while ensuring that Memory Overload remains O(1)?  \n2. Using AL as a latency metric may not accurately assess the phenomenon of over-generation. Could you provide alternative latency metrics, such as LAAL or LAAL-CA?  \n3. Regarding the comparison between MA-T and TAED, could you provide more experimental results to demonstrate that your method truly achieves better performance than MA-T? From the perspective of AL and BLEU, while TAED has a larger computational overhead, it does achieve higher translation quality at a lower latency and the overhead is still acceptable."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}