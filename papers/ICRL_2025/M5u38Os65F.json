{
    "id": "M5u38Os65F",
    "title": "Variance Reduced Distributed Non-Convex Optimization Using Matrix Stepsizes",
    "abstract": "Matrix-stepsized gradient descent algorithms have been shown to have superior performance in non-convex optimization problems compared to their scalar counterparts. The det-CGD algorithm, as introduced by Li et al. (2023), leverages matrix stepsizes to perform compressed gradient descent for non-convex objectives and matrix smooth problems in a federated manner. The authors establish the algorithm\u2019s convergence to a neighborhood of a weighted stationarity point under a convex condition for the symmetric and positive-definite matrix stepsize. In this paper, we propose two variance-reduced versions of the det-CGD algorithm, incorporating MARINA and DASHA methods. Notably, we establish theoretically and empirically, that det-MARINA and det-DASHA outperform MARINA, DASHA and the distributed det-CGD algorithms in terms of iteration and communication complexities.",
    "keywords": [
        "Federated Learning",
        "Non-Convex Optimization",
        "Optimization"
    ],
    "primary_area": "optimization",
    "TLDR": "This paper proposes two variance reduced matrix step-sized federated learning algorithms for non-convex objectives.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=M5u38Os65F",
    "pdf_link": "https://openreview.net/pdf?id=M5u38Os65F",
    "comments": [
        {
            "summary": {
                "value": "This paper provided two novel federated learning optimization methods based on variance-reduced matrix stepsizes. The theoretical analysis shows the communication complexity of proposed methods are better than existing methods, which is also validated by numerical experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea of using matrix stepsize and variance-reduction to improve the communication complexity in federated learning is interesting. The thermotical guarantees have been established."
            },
            "weaknesses": {
                "value": "Please see the part of questions."
            },
            "questions": {
                "value": "I have several questions about Section 4.2.\n\na) Line 310 says we can relax the problem to certain linear subspace and achieve Corollary 1. Is there any theoretical result to bound the approximation error of such relax?  \n\nb) In the expression of equation (6), we require achieving matrices ${\\bf W}^{1/2}$, ${\\bf L}^{-1}$, and several eigenvalues. Is the computation of these quantity expensive in practice? \n\nc) Line 394 says we can estimate ${\\bf L}_i$ by gradient method (Wang et al., 2022). I have not detailed checked the paper of Want et al. (2022). Can you summarize their main results and ideas (e.g., the complexity and the algorithm design)?\n\nI have also some questions about the comparisons with related work.\n\nd) I think more detailed introduction for MARINA and DASHA is necessary, which is useful to help the readers understand why the authors focus on the comparisons with these two methods.\n\ne) The iteration with matrix stepsize looks more expensive than classical first-order methods. I think the comparison with classical first-order methods (e.g., distributed GD, SGD, SARAH...) should be involved, including the discussion on the computational cost."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The work \"Variance reduced distributed nonconvex optimization using matrix stepsizes\" proposes decentralized variance reduced schemes with matrix step-sizes. The incorporation of variance reductions allows for convergence to arbitrary precision as opposed to the original work Li et. al (2024b). The authors provide theoretical guarantees for their algorithms and demonstrate their performance empirically."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The proposed schemes address an issue that makes the distributed version of det-CGD less desirable. This is achieved via the incorporation of a variance reduction mechanisms that aids in controlling the variance introduced by the compression.\n2) The work is well written and the contribution is clear."
            },
            "weaknesses": {
                "value": "1) The novelty of this work is limited, as it seems combine a known algorithm (detCGD) with a given problem with two known solutions to the problem (proposed in MARINA and DASHA). It is unclear to this reviewer what is the main challenge when combining these. I suggest the authors highlight which technical challenges need to be addressed to incorporate variance reduction into detCGD, and why these significantly differ from the case in which the step-size is a scalar."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes matrix-stepsize versions, namely det-MARINA and det-DASHA, of distributed variance-reduction algorithms MARINA and DASHA. The main idea was that the scalar stepsize can be replaced by a matrix stepsize similar to second-order (Newton) methods. The authors derive convergence guarantees to stationarity in the sense that the gradient squared norm vanishes with rate $O(1/k)$ where $k$ is the algorithms' iterate. Numerical experiments show some improvement of the proposed methods compared to MARINA and DASHA."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is generally well-written with sufficient background. The motivation is also clear and natural: matrix stepsize is better than scalar stepsize. Convergence analysis is provided."
            },
            "weaknesses": {
                "value": "Since the proposed methods are matrix-stepsize adaptations of MARINA and DASHA, we can expect them to retain the general properties observed in the original MARINA and DASHA schemes. To my understanding, both MARINA and DASHA also converge to stationarity and, unlike det-CGD, do not exhibit a restrictive convergence neighborhood. Thus, these new algorithms would naturally inherit this characteristic, which makes this advantage less novel in my view. From a theoretical standpoint, it would be helpful to directly compare Theorems 1 and 2 with the convergence properties of MARINA and DASHA (see specific questions below). Additionally, the Lipschitz matrix seems impractical in most cases, whereas the Lipschitz constant is typically more accessible; therefore, including the Lipschitz matrix may not be essential. While the experiments do show improvements, the gains appear relatively modest."
            },
            "questions": {
                "value": "(1) Regarding the optimal matrix stepsize in the optimization problem in line 304, the authors minimize the RHS of (5) with respect to D that aims to makes the bound as small as possible. However, the LHS of (5) also contains D. How could you tell that minimizing the RHS yields the optimal improvement?\n\n(2) When comparing det-MARINA and and MARINA, in Corollary 3 for example, they say this iteration complexity of det-MARINA is better than MARINA. Please elaborate this claim more. For example, their gradient norm are not the same, how can we compare them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "the paper considers matrix step size for federated learning, combined with some variance reduction methods to achieve exact convergence"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "variance reduced method together with matrix step size shows convergence"
            },
            "weaknesses": {
                "value": "only considered nonconvex case"
            },
            "questions": {
                "value": "1. What is the challenge in analysis of combining variance reduction with matrix step size?\n2. How does the rate compare with existing literature?\n3. Why one should take the trouble of using matrix step size?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}