{
    "id": "hYd6BCZTzg",
    "title": "Revisit Self-Debugging with Self-Generated Tests for Code Generation",
    "abstract": "Large language models (LLMs) have shown significant advancements in code generation, but still face challenges on tasks beyond their basic capabilities. Recently, the notion of self-debugging has been proposed to boost the performance of code generation by leveraging execution feedback from tests. Despite its promise, the availability of high-quality tests in real-world scenarios is limited. In this context, self-debugging with self-generated tests is a promising solution but lacks a full exploration of its limitations and practical potential. Therefore, we investigate its efficacy on diverse programming problems. To deepen our understanding, we propose two distinct paradigms for the process: post-execution and in-execution self-debugging. Within the scope of self-contained Python programming tasks, we find that post-execution self-debugging struggles on basic problems but shows potential for improvement on competitive ones, due to the bias introduced by evaluation on self-generated tests. On the other hand, in-execution self-debugging enables LLMs to mitigate the bias by solely leveraging intermediate states during execution, thereby enhancing code generation.",
    "keywords": [
        "self-debugging",
        "code generation",
        "code reasoning",
        "large language models"
    ],
    "primary_area": "generative models",
    "TLDR": "We study the efficacy of self-debugging with self-generated tests on programming tasks and offer several insights based on the findings.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=hYd6BCZTzg",
    "pdf_link": "https://openreview.net/pdf?id=hYd6BCZTzg",
    "comments": [
        {
            "summary": {
                "value": "This paper explores self-debugging in large language models (LLMs) for code generation, focusing on the use of self-generated tests. They define two paradigms in the execution-then-feedback process: post-execution and in-execution self-debugging. Experimental results across both basic and competitive code generation tasks reveal key insights. First, post-execution self-debugging struggles with simpler tasks. Second, self-generated test biases can cause inconsistencies across problem levels. Finally, in-execution self-debugging, which utilizes intermediate runtime data, consistently outperforms post-execution methods, demonstrating its promise for advancing self-debugging capabilities in LLMs"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- **Analysis:** The study effectively compares two distinct self-debugging paradigms: post-execution and in-execution, providing valuable insights into their strengths and weaknesses.\n- **In-Depth Analysis:** The paper delves into the limitations of post-execution self-debugging, particularly its performance degradation on simpler tasks, and offers potential explanations for this phenomenon.\n- **Promising Results:** The research highlights the significant potential of in-execution self-debugging, which utilizes runtime information to enhance accuracy and reliability, especially in complex code generation scenarios"
            },
            "weaknesses": {
                "value": "- **Limited Evaluation Scope:** The paper's evaluation is constrained to a relatively small set of models, potentially limiting the generalizability of the findings.\n- **Novelty Concerns:** While the paper presents a novel application of self-debugging to LLMs, the core idea of leveraging runtime information for debugging is not entirely new and has been explored in previous work, as cited in [1].\n- **Inconsistent Evaluation Setup:** The paper's experimental setup may introduce bias due to the different prompts used for post-execution and in-execution self-debugging. A more consistent approach would involve using the same prompts for both methods to ensure a fair comparison.\n\n[1] Ni, Ansong, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin. \"NExT: Teaching Large Language Models to Reason about Code Execution.\"\u00a0arXiv preprint arXiv:2404.14662\u00a0(2024)."
            },
            "questions": {
                "value": "Questions\n\n1. **Trace Length:** How will you handle extremely long traces, such as those generated by deeply nested loops? Will you implement strategies to truncate or summarize traces while preserving essential information?\n2. **Consistent Evaluation:** Why is there a discrepancy in the evaluation setup for post-execution and in-execution self-debugging? For instance, let LLMs analyze input-output pairs for post-execution to decide whether it is correct and explain, which is the same as in-execution.\n3. **Comparison to NExT:** How does the proposed in-execution self-debugging approach differ from NExT [1]?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides an investigation into the useage of self-generated tests specifically for code generation. Specifically the paper defines two types of self-debug executions: post-execution and in-execution. The paper evaluates using two different LLMs: GPT-4o and LLaMA-3-70B-Instruct across three self-contained coding benchmarks. The results show that self-debugging techniques (in particular post-execution) struggles with basic problems while in-exeuction self-debugging can effective alleviate some biases by providing the intermediate executions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- focus an important problem of studying code generation using LLMs\n- study design and settings are clear and relevant\n- definitions provided in the paper provides a nice guideline for future research\n- text is written clearly and easy to read"
            },
            "weaknesses": {
                "value": "- **Limited number of study LLMs**:\n  -  The paper aims to provide a detailed investigation to evaluate the slef-debugging capabilities of LLMs by using generated tests\n  -  However, the paper only evaluates two different LLMs: GPT-4o and LLaMA-3-70B-Instruct\n  -  How would the findings highlighted in the paper translate to other LLMs, for example LLMs that are not instruction-tuned or LLMs that are  specifically fine-tuned for code\n  - Given the landscape of current SOTA LLMs, evaluate only on a few LLMs is not enough to showcase the investigation findings \n- **Lack of more comprehensive study settings can be applied**:\n  - The authors evaluate some basic settings with respect to code generation\n  - Given the setup of the paper to study existing settings without providing any new technique or improvement, I would expect a more systematic settings\n  - For example, the authors only use greedy sampling, what if we study the effect of sampling (for example in the classic self-debug study paper: Olausson et al. 2024) this is a key finding: \n  - For example, what if we vary the number self-generated tests we produce? what if we use the tests from a larger LLM to debug the smaller LLM?\n  - I believe the paper lack more interesting findings that can be obtained with more detailed experiments. \n- **Evaluation on more realistic coding benchmarks**\n  - The authors evaluate on some basic coding benchmarks including HumanEval, MBPP, and LiveCodeBench. \n  - However, these are self-contained benchmarks that are not in fact realistic \n  - I am not sure if the findings in this paper would translate to more realistic repository level coding benchmarks\n  - This limitation is not neccesarily the fault of the authors as it may be difficult to perform very extensive evaluation on those benchmarks, however I would still like to see some prelimeary experiments if possible and discussion \n- **Limited direction and discussion for future work**:\n  - While I really like the insight of additional bias (i.e., giving more detail may not be good) in the findings and discussions, I find the discussion section after the findings rather weak.\n  - I believe the paper would benefit a lot from either a small technical contribution of testing some new techniques given the findings or offer more in terms of future discussion to improve future work along this direction \n\n**References:** \nOlausson, Theo X., et al. \"Is Self-Repair a Silver Bullet for Code Generation?.\" The Twelfth International Conference on Learning Representations. 2023."
            },
            "questions": {
                "value": "- Can the authors elaborate a bit more on how the findings in this work can lead to future improvements along this direction apart from Section 5.\n- Can the authors comment on how the findings might differ when applied on more realistic benchmarks say for example for repository level coding?\n- Did the authors evaluate the effect of changing the number self-generated tests and how that can affect performance?\n- I am also curious how the authors deal with executions with loops in terms of basic blocks (in terms of how the intermediate states is provided to the LLM as feedback), would appreciate if more technical discussion can be provided"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper dives into the problem of self-debugging based on self-generated tests. It studies different programming tasks and finds that post-execution can help contest-level programming, while post-execution can enable LLMs to mitigate the bias that comes from wrong test cases by recognizing faulty feedbacks and improve performance. The experiment results demonstrate promising improvements for both basic and competitive tasks by focusing on the intermediate states."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper is presented very clearly and comprehensively. The motivation is reasonable: as many LLM frameworks are using self-generated tests, it is a question that whether these test cases are truly valid and accurate. And the paper answers the question well by conducting experiments on several popular simple Python programming tasks: HumanEval, MBPP, and LiveCodeBench. The results are interesting: while most of the tests are generated correctly, it is hard to guarantee that a full test suite for a question is totally correct (accuracy only 59.15% even with GPT-4o). This leads to self-debugging bias when the test cases are not fully correct. \n\nThe paper also tries different categories of execution information: in-execution and post-execution information. And they find that the latter will help offset the bias from inaccurate self-generated tests. This paper dives into the bias of post-execution information and even find that sometimes though the tests are wrong, the labels of pass or not could still help by simply adding more rounds of debugging. \n\nThese findings of the paper are interesting and could inspire research on test generation."
            },
            "weaknesses": {
                "value": "An important findings from this paper is that the in-execution state can help reduce the bias of wrong self-generated tests. However, the section of in-execution is short and not very detailed, which I think need more details:\n1. A case study of how the bias is offset by in-execution information.\n2. Since the in-execution helps offset the bias from wrong self-generated tests, it could be better presented in a comparison figure with post-execution information. This can show that although in-execution does not significantly improve the original performance, it does not cause harm and at least sometimes help.\n3.  Though this paper provides many interesting findings, the technical side is relatively weak. It would be great if the author can provide some potential solutions on improving test generation accuracy.\n4. The iteration number of the experiments is limited, which might not fully support the authors' findings."
            },
            "questions": {
                "value": "1. Can you give a case study of how the bias is offset by in-execution information?\n2. Can you add the performance of more iterations of debugging and the trends?\n3. Can you discuss some potential improvement on test generation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper looks at \"self-debugging\", i.e. when the LLM is tasked with debugging and repairing programs it has generated that do not pass the tests.\nThis is a problem that has received much attention in the literature in the last year or two; the key novelty here is:\n- Looking at the impact of having the tests themselves be synthesized by the model (which risks leading it astray during debugging, if the tests are spurious)\n- The second set of experiments, when the authors evaluate what they call \"in-execution\" repair. In the usual setting (which the authors call \"post-execution\"), the LLM is simply told which test failed (and, possibly, the manner in which it failed); in the \"in-execution\" setting, the LLM is instead given access to what is basically a partial execution trace of the program, that it can then compare to the specification."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I think this paper makes for a nice addition to the literature.\nTo be honest I don't think there's much in the \"post-execution\" part that hasn't been done already, but it's nice to see such experiments repeated with newer models and a newer dataset (LiveCodeBench), and there are some details about the experimental setup (e.g. the test synthesis) that probably haven't been done exactly the same way before.\nMore importantly, the discussion thereof sets up nicely for the \"in-execution\" experiments, and I think the authors do a great job of analysis the relative strengths and weaknesses of each approach.\nGenerally speaking, I am also relatively confident that the experiments were done correctly (which is more than can be said for most work in this area), and I am happy with both the model selection (one open and one API-based) as well as the datasets (I'm glad to see that the authors not only bring on board a more modern benchmark in LiveCodeBench but that they also use the less satured \"Plus\" versions of HumanEval and MBPP).\n\nAs a result, I think people in the code generation community would benefit from seeing this paper presented at ICLR; my concerns are relatively minor and could certainly be addressed in time for camera-ready."
            },
            "weaknesses": {
                "value": "- If I understand the method correctly, the way you are generating the trace for the \"in-execution\" method requires treating dynamic loops as one atomic unit. For example, if you have for loop which an unknown number of iterations, you can't statically insert a breakpoint halfway through it, so instead you have to break before or after. I would like to see this limitation discussed in the paper; do you, for example, have any thoughts on whether this hurts performance in practice, e.g. if the bug often lays within such a loop?\n- I think the paper could do a better job emphasising that a +X.Y% increase in pass rate after 3 iterations of self-debugging does NOT mean that's what you want to do in practice, since it is compared against a baseline with 1 sample - not a baseline with the same budget as you used up during self-debugging. I apreciate that the point of this paper is not to chase numbers on these benchmarks but to actually do some cool science so we can understand *why* these things work the way they do, but again I am worried that a less familiar reader might mistakenly see this as a raw performance increase (neglecting the cost).\n- Finally, I would encourage you to double check your references list. I was confused why you kept citing \"Chen (2023)\" when I was pretty sure that self-debugging paper was at ICLR 2024; indeed your references list it as ICLR 2023, which I do not believe to be correct. Probably worth double checking that you don't have any more mistakes in there."
            },
            "questions": {
                "value": "- 44-46: \"Reflexion [...] evaluates [the code] with hidden oracle tests.\" - what do you mean by this, exactly? Is there another way of evaluating *the final* code than with the hidden oracle tests? Do you mean that they evaluate the code *before repair* with the hidden oracle tests (rather than with model-generated ones)?\n- 109-110: \"Self-debugging does not require increasing the sample budget, making it a cost-effective solution...\" Self-debugging does of course require drawing more samples from the model, and there has been prior work showing that this is not always made up for by an increase in pass rates. I know you know this, but someone who doesn't might jump to the wrong conclusions here.\n- In table 1, how can the accuracy drop from 2 to 3 iterations if you are using *oracle* tests to determine correctness? Wouldn't the passing programs from 3 iterations be a superset of those passing after 2 iterations in this case?\n- 310-312: It would be nice if you could give some examples of these program contracts (maybe in an appendix). I'd be curious to see the details; in particular it's not clear whether you're just checking for type mismatches or if you have more fine-grained constraints.\n- 407-409: I understand you probably don't want to open this can of worms in the paper but for the record my takeaway from your results is that data contamination is likely the reason why things look so different on LiveCodeBench in comparison to HumanEval and MBPP (which most models have at this point most certainly been trained on, knowingly or not)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}