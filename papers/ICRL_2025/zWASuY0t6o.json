{
    "id": "zWASuY0t6o",
    "title": "Focus On This, Not That! Steering LLMs With Adaptive Feature Specification",
    "abstract": "Despite the success of Instruction Tuning (IT) in training large language models (LLMs) to perform arbitrary user-specified tasks, these models often still leverage spurious or biased features learned from their training data, leading to undesired behaviours when deploying them in new contexts. In this work, we introduce *Focus Instruction Tuning* (FIT), which trains LLMs to condition their responses by ''focusing on'' specific features whilst ignoring others, leading to different behaviours based on which features are specified. Across several experimental settings, we show that focus-tuned models can be adaptively steered by focusing on different features at inference-time, such as (a) improving robustness by focusing on task-causal features and ignoring spurious features, and (b) mitigating bias by ignoring demographic categories. Furthermore, FIT can steer behaviour in new contexts, generalising under distribution shift and to new unseen features at inference time, thereby facilitating more robust, fair, and explainable LLM applications in real-world environments.",
    "keywords": [
        "instruction tuning",
        "LLMs",
        "spurious correlations",
        "robustness",
        "distribution shift",
        "bias"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We introduce a method to trains LLMs to adaptively condition their task behaviours based on specified features.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zWASuY0t6o",
    "pdf_link": "https://openreview.net/pdf?id=zWASuY0t6o",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces Focus Instruction Tuning, a finetuning strategy for focusing LLMs on specific features.  The method is applied to classification settings with spurious correlations.  Results shown on three tasks indicate for potential to mitigate bias over traditional finetuning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed work targets improving robustness to spurious correlations. Improving robustness to spurious correlations is an important problem  that has been well studied in classification problems and is also important to study for more recent LLMs.  \n- The paper is overall well-written.  I appreciate the authors have highlighted the key takeaways one act experiment."
            },
            "weaknesses": {
                "value": "- The experiments for the paper consider older tasks including NLI and sentiment.  These datasets are old, and there are already many existing approaches for reducing bias in these datasets which the authors have not considered in this work.  I recommend the authors to compare against existing approaches for example: https://aclanthology.org/2020.acl-main.769/ and https://aclanthology.org/2022.acl-long.190.pdf.  Further the author could consider more recent benchmark tasks that the models are typically evaluated on as even the included QA dataset is a few years old (2022).\n- The proposed method is limited to classification in the experiments, however there are also many tasks these LLMs are capable of including general QA, MCQ, generation, etc. which may have bias and also important to study.\n- The evaluations that are done do not seems to be consistent with prior works as they separate based on the label for computing accuracy.  Could the authors also include the full accuracy on the datasets for comparison with prior work?"
            },
            "questions": {
                "value": "- Can the authors clarify on the difference between FIT and IT? More specifically how is the $I_{focus}$ included in the data? Is this included for the IT comparison, or is the data left out? I believe including this data as part of $x$ would be a necessary comparison as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose an instruction fine-tuning method called Focus Instruction Tuning (FIT), designed to guide large language models (LLMs) to prioritize causal features while disregarding spurious ones. Specifically, the authors construct an instruction fine-tuning dataset based on Equations (1) and (2), categorizing the desired output labels into four groups. The concurrence rate between spurious features and labels is adjusted at various levels to control the difficulty of the test set. In the experiments, the authors evaluate the model\u2019s focus instruction capability, denoted as $\\hat{y}\\sim p_{\\theta}(y|I,I_{text},x)$, representing the probability of predicting the label given the focus instruction. The metric $A_{focus}$ is used to compare different methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "I have several concerns about this paper:\n\n1. It appears that both training and evaluation require prior knowledge of spurious features (relative to a given model), which may be challenging in practical applications.\n\n2. This approach primarily enhances instruction-following in a narrow domain \u2014 specifically, where the model learns to \"focus on X while ignoring Y\" \u2014 rather than truly mitigating spurious correlations. After training, the model still relies on \"focus and ignore\" instructions to control its output. \n\n  According to my experiment, part of the spurious correlation problem originates from the supervised fine-tuning (SFT) data itself, highlighting the importance of data quality and diversity, as emphasized in technical reports for models like LLaMA and Nemotron. For instance, Nemotron achieves this by applying a Cartesian product over task diversity (e.g., open Q&A, writing), topic diversity (e.g., STEM, humanities), and instruction diversity (e.g., JSON output, yes-or-no answers). FIT seems to add only a single dimension of variation (\"focus and ignore\") to the SFT data.\n\n3. While FIT demonstrates an improvement in the model's ability to follow \"focus and ignore\" instructions, this is expected given that the model is specifically trained on such patterns. However, it is unclear how this affects other instruction-following abilities \u2014 do they remain stable, or do they degrade? Can author demonstrate that?"
            },
            "questions": {
                "value": "See my comments on the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Focus Instruction Tuning (FIT), a method designed to guide Large Language Models (LLMs) in emphasizing specific features or disregarding others during task execution. FIT addresses limitations in traditional Instruction Tuning (IT) by training LLMs to avoid spurious correlations and biases, thereby enhancing model robustness and fairness. This approach enables adaptive focus on task-relevant features, leading to improved performance in scenarios involving distributional shifts and unfamiliar contexts. Empirical evaluations across multiple NLP tasks demonstrate FIT's effectiveness in managing various spurious and causal features and in mitigating biases."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **Originality**: FIT's feature-based focus approach offers flexibility in controlling LLM responses based on specified attributes, advancing traditional instruction tuning techniques. However, this technique in my opinion can be viewed as a variant of context distillation, which limits the originality of this paper. \n2. **Quality**: The experiments are comprehensive, encompassing various LLM models, datasets, and evaluation conditions. The robustness of FIT against spurious correlations and distribution shifts is well-documented, establishing the method's adaptability and efficacy.\n3. **Clarity**: The methodology is detailed and clear, with illustrative examples and visualizations that support understanding of FIT\u2019s impact. However, further contextualization of feature selection strategies could enhance comprehension.\n4. **Significance**: FIT\u2019s potential to mitigate biases and enable feature-based focus presents meaningful advancements in applying LLMs in ethically sensitive areas, such as fair NLP applications and explainable AI."
            },
            "weaknesses": {
                "value": "1. **Overlap with Context Distillation**: FIT shares similarities with context distillation methods, potentially limiting its distinctiveness and raises the question of where FIT truly diverges from established context distillation approaches. For instance, studies such as Snell et al. (2022) on learning by distilling context (https://arxiv.org/abs/2209.15189) explore related concepts. I first saw context distillation in Anthropic\u2019s first paper: https://arxiv.org/abs/2112.00861 So I think it\u2019s probably fairly well-known at this point. Additional comparisons with established context distillation methods could clarify FIT\u2019s unique contributions. The authors may benefit from clarifying these distinctions to strengthen the paper's contributions and make its novel aspects more apparent.\n2. **Dependence on Human Guidance**: FIT relies on human intervention to determine which features are task-relevant or spurious, which may not be feasible in highly dynamic or uncertain domains. Automation in identifying these attributes would improve applicability.\n3. **Generalization Challenges**: The model's ability to generalize across unseen, highly overlapping features is noted as a limitation. More discussion on addressing this would help clarify FIT\u2019s potential for broader generalization."
            },
            "questions": {
                "value": "1. Could the authors provide more insight into automating feature selection for focus tuning? How does FIT handle feature ambiguity in complex datasets?\n2. How does FIT compare empirically with existing context distillation methods, and what unique improvements does it provide over these methods?\n3. Are there additional strategies the authors could explore to enhance FIT\u2019s generalization on overlapping features, especially in high-dimensional feature spaces?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Motivated by the observation that instruction-tuned LLMs tend to overly rely on spurious features, this paper proposes a new method, FIT, for instruction tuning LLMs while focusing on or ignoring certain features in the input. The method is a simple, intuitive modification to standard instruction tuning to include an additional focus instruction, such as \"Direct your attention solely to X\" or \"Exclude Y entirely from your evaluation.\" Models trained with FIT can then be flexibly guided to focus on or ignore certain features at inference time, by modifying the prompt in a similar way. The authors evaluate FIT on three models across three datasets, presenting promising results for reducing over-reliance on spurious correlations and mitigating bias involving demographic attributes. The authors also show that models trained with FIT can also generalize to focus prompts other than those in the training data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- FIT is a simple, intuitive, and effective idea.\n- The paper is well-written and clear. The diagram in fig 1 and the remaining figures are clearly presented.\n- Over the three models and three evaluations presented, FIT appears to be effective (though see my questions below), showing an improvement over the standard IT baseline.\n- The paper presents reasonably thorough evaluations. Manipulating the extent of the spurious correlation, and which label it is correlated with, is a particularly nice addition.\n- Fine-tuning to actively focus on the spurious feature and ignore the causal feature provides an interesting control and highlights the flexibility of the method.\n- The ablations in section 5 demonstrate that the model does not overfit to the specific focus prompts used during training."
            },
            "weaknesses": {
                "value": "There are a number of weaknesses that at the very least warrant some discussion.\n1. Training using FIT requires access to ground-truth information about which features are spurious and which are causal, or group attributes in a fairness setting. These are often unavailable, or undesirable to collect. How do the authors expect FIT to perform if, for example, trained on a dataset where y_spurious is available, but tested in a different setting where unavailable? It would be helpful if the authors could discuss this in their limitations section.\n2. The work is restricted to classification settings, rather than open-ended generation. This is a reasonable assumption but also significantly limits the applicability of the approach. This should be discussed in the limitations section, if not earlier.\n3. While the evaluations are reasonably comprehensive, the paper would benefit greatly from an explicit test of out-of-distribution generalization w.r.t. training domain. For example, how well would a model trained to follow instructions on SS be able to follow instructions on SMNLI without training?\n\nTo my eyes, this paper presents a flexible method for specifying which features should be used and which ignored at inference time. Yet, the authors consistently frame the paper in terms of spurious or causal features. In practice, it is often unclear whether a feature is actually casual. I wonder if the paper might benefit from framing similar to \"core vs. spurious\", or \"intended vs. unintended\", rather than \"spurious vs. causal\"?\n\n**Minor**: The final section of the related work, on refocusing LLMs, is a little confusing in light of the broader arc of the paper. After reading, I half expected the authors to introduce something akin to LlamaGuard, rather than a new instruction fine-tuning method. I wonder if this could be relegated to a broader related work section later in the paper, for the sake of narrative clarity."
            },
            "questions": {
                "value": "1. I find it surprising that certain models able to follow Focus(C) + Ignore(S) instructions even under vanilla SFT, across all test conditions on the SS dataset (e.g. fig. 2, Mistral, SFT)? Could the authors elaborate on this, and possibly temper the claims in lines 360--362 as a result?\n2. Similarly, could the authors elaborate how FIT can improve accuracy on the null focus instruction set (e.g. fig 4., Mistral, SFT vs. FIT)? I would have imagined there should be no change here?\n3. Could the authors clarify at which stage FIT is applied? Is it used in place of existing instruction tuning, or a second stage after standard instruction tuning? As a broader question, do the authors have any intuition about a) whether FIT could be applied to downstream chat tuning, or b) whether chat tuning might undo some of the success of FIT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}