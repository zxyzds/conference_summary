{
    "id": "8GhwePP7vA",
    "title": "Feature Matching Intervention: Leveraging Observational Data for Causal Representation Learning",
    "abstract": "A major challenge in causal inference from observational data is the absence of perfect interventions, making it difficult to distinguish causal features from spurious ones. We propose an innovative approach, Feature Matching Intervention (FMI), which uses a matching procedure to mimic perfect interventions. We define causal latent graphs, extending structural causal models to latent feature space, providing a framework that connects FMI with causal graph learning. Our feature matching procedure emulates perfect interventions within these causal latent graphs. Theoretical results demonstrate that FMI exhibits strong out-of-distribution (OOD) generalizability. Experiments further highlight FMI's superior performance in effectively identifying causal features solely from observational data.",
    "keywords": [
        "Causal representation learning",
        "Observational data",
        "Out-of-distribution generalization"
    ],
    "primary_area": "causal reasoning",
    "TLDR": "We propose a novel method, Feature Matching Intervention (FMI), which uses a matching procedure to mimic perfect interventions",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8GhwePP7vA",
    "pdf_link": "https://openreview.net/pdf?id=8GhwePP7vA",
    "comments": [
        {
            "summary": {
                "value": "This paper studies settings where images are created from spurious and true features, the true features being invariant across environments. Using a single environment, and under the assumption that\u00a0_only_\u00a0the spurious feature is used for minimising the risk, the authors propose a scheme to create a new dataset (or batch) that simulates interventions on the spurious features. Another model can be trained on this dataset (batch) that then is independent of the spurious feature and only uses the true feature for the task at hand. The authors also introduce a test for the assumption that only the spurious features are used for the task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall the idea is a simple one and quite interesting. I do have some issues with the experiments and the test for the assumptions. I think these points could be a lot stronger and should clearly show when the FMI method works and when it doesn't.\n\n- The method is simple and sound when the assumptions of the method hold. The assumptions are\u00a0_mostly_\u00a0clear.\n- The paper is mostly clear, although certain areas could be improved (see below)"
            },
            "weaknesses": {
                "value": "The main weakness of the work is that it only works if the model only uses the spurious feature in the training environment. This is quite a strong assumption and thus should be main and centre in the work. In my opinion, in most cases, it seems likely that a model trained on a single environment in this setting will learn from a\u00a0_mixture_\u00a0of spurious and true features (with varying strengths). In this case, applying FMI can also\u00a0_hurt_\u00a0performance as the signal from the true feature can be lost in the matching process. Furthermore, I'm not sure if the test in Section 5.2 will pick up this case, as Y|Z may differ in the validation environment even in the case that both spurious and true features are used. A thorough analysis of this case will greatly improve the work. It would be of interest to see how sensitive the test is and how much performance is lost if the training results in a mixture of true and spurious features. I would encourage the authors to discuss if this is the case, and include experiments that show if performance drops or not (for example when colour noise in Section 6.2 is higher than the label noise), and to show how trustworthy their proposed test is at finding these cases.\n\nA second weakness is that the procedure requires training a neural network to convergence at every training step."
            },
            "questions": {
                "value": "- What is the resultant added cost in your experiments as you require training a network to convergence at every step?\n- Is it not possible to just train two neural networks to convergence instead of training a new one to convergence at every training step?\n- L312: I'm not sure how Assumption 3 implies that\u00a0Zspu\u00a0is the feature learned in the training environment? Surely this depends on how correlated the spurious feature is with the label in the training environment? As far as I can tell, there is no assumption about the training environment at all.\n- L345: Related to the first weakness: This property may still hold if\u00a0_both_\u00a0the spurious feature and the true feature are used. I think it may be more correct to say that if\u00a0Y|Ze\u00a0and\u00a0Y|Ze0\u00a0are the same then you can be sure that\u00a0Z\u00a0is the true feature.\n- The experiments in 6.1 an 6.3 are not very informative. There is no clear information from what I can see about the level of correlations between the spurious features and labels.\n- Section 6.2: The Colored MNIST setting does not read clearly to me at all. I have a few questions about this:\n- There are 3 environments (0.1, 0.2, 0.9) it sounds like two are used as the training environment and one is used as a testing environment. Are the two that are used as a training environment mixed? If so, why is this done? Why not train on 0.1 and then test on 0.9 and so on? This is not commented on at all. This seems like an odd choice and makes the experiment quite unclear.\n- It seems that the 0.1 and 0.9 setting are the same (as they have the same correlation between label and spurious feature), is this correct?\n- The performance drop when 0.1 is the test environment is a bit worrying. I completely see why training on (mixture of) 0.1 and 0.2 would result in the spurious feature being used, and the performance on the 0.9 environment improves when FMI is used. I'm not sure I believe the claim in L418 that the performance drop in the 0.1 env is due to subsampling. It seems that an equally reasonable explanation could be that training on 0.9 and 0.2 together results in a classifier that uses\u00a0_both_\u00a0Ztrue\u00a0and\u00a0Zspu. Matching would thus result in a drop in performance. This should be tested thoroughly to see if this is the case, and to see if the test (Section 5.2) actually spots when this is the case.\n- The plot in figure 5 is unclear to me. What is environment 0 and environment 1? Why are there two plots given that you are testing how similar Y|f are in two different environments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper is concerned with representation learning with the aim of finding invariant representations that provide good out of distribution behavior (and can be considered causal under suitable definitions). To achieve this the authors provide a matching scheme which matches on the prognostic score. The authors provide an intuitive and simple realization of the approach, adapting the standard minibatch learning scheme with a subsampling procedure that aims to provide balance and, as a result, control for unobserved confounding. A set of experimental results is provided demonstrating the relative performance of the proposed approach with respect to variants of empirical and invariant risk minimization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The authors examine an interesting and compelling problem. \n* The proposed solution is simple and intuitive; the idea of using matching for this problem holds appeal given both it's relative simplicity and robustness against a broad array of underlying data generating processes.\n* Empirical results indicate the proposed approach holds promise."
            },
            "weaknesses": {
                "value": "While a reader well familiar with this area understands the connections between distribution shift, invariance, and causal inference, it is not made clear within the introduction and problem setting. I would strongly suggest that the authors rewrite these sections making each connection much more explicit. In particular, it should be very explicit what the definition of a causal feature is in this work. \n\nThe proposed method, as I understand it is more akin to matching on the prognostic score (Hansen, 2008), rather than more standard matching (e.g., the Stuart paper cited), in that matches are constructed using the _outcomes_ rather than matching covariates with respect to _treatment status_. This should be clarified in the paper. Toward this end, in the problem setup it is stated that these results easily extend to additional outcome types, however it is not immediately clear to me that this should be the case since matching on real valued and multi-valued treatments entails a more nuanced procedure. \n\nSubsampling to make proportions match is reasonable, but also likely introduces issues when there is large distribution skew. \n\nIt's not clear to me how equation 5 achieves balance, or why we should think of this as matching in the standard set? Typically we would find matched pairs where $\\hat{f}$ is as close as possible, while this doesn't seem to be doing any explicit matching? \n\nAssumption three is incredibly strong, and it is not clear to me how likely this is to hold for any realistic dataset (see below for  a question regarding this). Toward that end, it's not clear to me how substantial the theory is that is provided here. If we are placing strong, and difficult to meet, assumptions on the available data the risk here is that the results serve more as a proof of existence, rather than a general theorem that can be leaned upon in practice. \n\nThe highlighting scheme in the results table is confusing. I think the authors meant to bold the best performing method in each setting, rather than just the settings where the algorithm performs well? \n\nBen B. Hansen, The prognostic analogue of the propensity score, Biometrika, Volume 95, Issue 2, June 2008, Pages 481\u2013488, https://doi.org/10.1093/biomet/asn004"
            },
            "questions": {
                "value": "Can the authors explain how equation 5 is achieving balance here? It's not clear to me that the procedure as described would appropriately control for confounding.\n\nWhy is the matching done with respect to batches? It would seem that this would result in poor entailed balance properties? \n\nAs I mentioned above Assumption three is incredibly strong, and it is not clear to me how likely this is to hold for any realistic dataset (unless I am misreading it. To be clear, all variables are intervened at all levels? Only one intervention has to be present for each variable? Are they perfect interventions? \n\nIs assumption 4 the observed support?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Feature Matching Intervention (FMI), an approach for mitigating spurious correlations using a feature-matching procedure to mimic perfect interventions on spurious features. The authors provide theoretical guarantees for the proposed method's out-of-distribution (OOD) generalization under specific assumptions and propose a validation approach to assess whether spurious features are being learned in the training environment. Experimental results on synthetic and semi-synthetic datasets, including Colored MNIST and WaterBirds, demonstrate that the proposed method outperforms baseline methods, especially in scenarios with strong spurious correlations in the training data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The theoretical analysis of the OOD generalizability of the proposed method is rigorous, and the derivation procedure is clear and easy to follow.\n\n2. The experiments demonstrate that the proposed method outperforms baselines in identifying causal features, especially in the presence of spurious correlations."
            },
            "weaknesses": {
                "value": "1. __Single Environment Claim__: Although the authors claim that the proposed method can mitigate spurious correlations using data from a single training environment, Assumptions 2 and 3 appear to imply the need for multiple environments when deriving the theoretical guarantees. Additionally, the empirical studies on Colored MNIST utilize two training environments, which seems inconsistent with this claim. It would be beneficial for the authors to conduct experiments using a single training environment and evaluate the method's performance on both synthetic and semi-synthetic datasets.\n\n2. Assumption 1 appears to be more of an intuitive conjecture, lacking formal theoretical support.\n\n3. __Missing Related Work__: Some relevant related works have been omitted. First, the concept of reweighting to mimic perfect interventions on spurious features for improving distributional robustness has been discussed in [1] and [2]. Additionally, there is a body of work focused on improving group distributional robustness based on the understanding that ERM tends to learn spurious correlations ([3], [4], [5]). The proposed method seems to share similarities with these works. It would be helpful if the authors could discuss the novelty of their approach and how it fills a gap compared to these existing works.\n\n4. __Subsampling and Overfitting Concerns__: The authors use subsampling to remove the dependence between the label and spurious features. However, spurious correlations often occur in highly imbalanced data distributions, and subsampling in such cases could lead to dropping a substantial portion of the data from majority groups. This may increase the risk of overfitting, especially if the remaining dataset is small. It would be great if the authors could address how they mitigate the risk of overfitting in this scenario.\n\n5. __Validation Environment Concerns__: When assessing whether spurious features are learned in the training environment, the authors propose using a validation environment. This appears to contradict the single-training-environment assumption. One of the benefits of the single-environment setting is the reduced requirement for environment labels or predefined environment divisions. However, if a validation environment is required, this benefit is lost. Furthermore, the validity of the test may depend on the level of distributional shift between the training and validation environments. If the shift is minimal, the test might incorrectly conclude that ERM has learned the causal feature. Clarification on these points would be great.\n\n6. __Experimental Setup for WaterBirds Dataset__: Could the authors provide more details regarding the experimental setup for the WaterBirds dataset? \n\n7. __Discussion on Poor Performance in Heterogeneous Training Environments__: The experimental results on Colored MNIST indicate that FMI performs poorly when the training environments are highly heterogeneous. Specifically, when training environments are (0.2, 0.9) or (0.1, 0.9) and the test environment is (0.1) or (0.2), the performance degrades. A detailed discussion on the reasons behind this poor performance and potential ways to address it would be helpful.\n\n8. Minor typo: in line 245, $i, j \\in \\\\{1,2\\\\}$ should be $i, j \\in \\\\{0,1\\\\}$?\n\n\n\n[1] Makar, Maggie, et al. \"Causally motivated shortcut removal using auxiliary labels.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.\n[2] Veitch, Victor, et al. \"Counterfactual invariance to spurious correlations in text classification.\" Advances in neural information processing systems 34 (2021): 16196-16208.\n[3] Liu, Evan Z., et al. \"Just train twice: Improving group robustness without training group information.\" International Conference on Machine Learning. PMLR, 2021.\n[4] Kirichenko, Polina, Pavel Izmailov, and Andrew Gordon Wilson. \"Last layer re-training is sufficient for robustness to spurious correlations.\" arXiv preprint arXiv:2204.02937 (2022).\n[5] Yang, Yu, et al. \"Identifying spurious biases early in training through the lens of simplicity bias.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2024."
            },
            "questions": {
                "value": "Please see the questions in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose Feature Matching Intervention (FMI), which uses a matching procedure to mimic perfect interventions. They define causal latent graphs, extending structural causal models to latent feature space, providing a framework that connects FMI with causal graph learning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The procedure emulates perfect interventions within causal latent graphs. Theoretical results demonstrate that FMI exhibits strong out-of-distribution (OOD) generalizability. Experiments further highlight FMI\u2019s superior performance in effectively identifying causal features solely from observational data."
            },
            "weaknesses": {
                "value": "Please refer to questions."
            },
            "questions": {
                "value": "page 3, line 147. ''Thus, identifiability becomes an issue here. However, since our goal is to learn $f\\phi$, this concern is not relevant.'' Is the goal to identify $\\phi$ here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}