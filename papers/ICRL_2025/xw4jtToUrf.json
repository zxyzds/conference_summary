{
    "id": "xw4jtToUrf",
    "title": "Investigating Online RL in World Models",
    "abstract": "Over the past decade, online reinforcement learning (RL) has made drastic improvements in a number of settings, such as video games and robotics. However, despite these successes, the impact of RL on many *real-world* problems has remained limited. Underlying this fact is that, in many settings, we are unable to learn in an online fashion due to excessive cost and safety requirements or lack of an accurate simulator. \nIn principle, foundation world models trained on large-scale uncurated offline data such as internet videos and other modalities could provide a training paradigm for generalist AI agents which alleviates the need for task specific simulation environments. \nUnfortunately, training inside world models is usually studied in the context of offline RL, where popular datasets have a biased structure. This necessitates short roll-outs or other severely limiting mechanisms to prevent model exploitation. \nHere we probe under what circumstances full roll-out training inside world models is possible *without* any penalties.\nWe find that on a non-adversarial offline dataset simply ensembling over a large number of independently trained world models is sufficient to ensure transfer to the real world, even for datasets that are orders of magnitude smaller than is common in offline RL. Interestingly, more sophisticated methods for level selection provide no advantage and standard offline RL methods underperform in this setting.",
    "keywords": [
        "World models",
        "Domain Randomization",
        "Offline RL"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We train a set of different world models on a dataset of mixed expertise and use them as levels to train an RL agent",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=xw4jtToUrf",
    "pdf_link": "https://openreview.net/pdf?id=xw4jtToUrf",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a novel approach to online reinforcement learning. It is different from the typical approach to online interactions directly with the underlying environment (sim or real). Instead, this work studies RL methods within learned world models, attempting to mitigate common pitfalls of offline RL (reward exploitation, skewed datasets, etc.) and avoid the costly and sometimes infeasible samplings directly from the environments.\n\nUnder the settings where authors collected a more uniformly distributed dataset (in terms of state/action coverage compared to D4RL), this work trained PPO from an ensemble of independently trained world models, using Domain Randomization techniques (DR) and Unsupervised environment design (UED) methods. On the curated dataset, the experiment results suggest significant improvements over offline RL methods (CQL & SACn).\n\nOverall, this work is novel and the results presented in the paper offers new insights to training RL agents purely from world models. If there are more experimental evidence from different tasks or environments to support the the claim that \"full roll-out training inside world models is possible\", and clarify the questions I have below, I would be willing to raise the score."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Originality:\n- While the approach of using an ensemble of world models to reduce over-fitting and exploitation has been previously studied for model-based RL methods, this work differentiates itself by training entirely within the world model and on full-length roll-outs without any penalty terms inside learnt models.\n\n2. Quality:\n- The experiments are well-designed with clear assumptions and comparisons against multiple baselines. The use of different data scales and detailed ablation studies on the components of their method provides a thorough validation of their claims.\n\n3. Clarity:\n- The paper is well-organized with informative figures and tables. Especially figure 9 and 10, they explained how the curated dataset differs from the d4rl dataset. The writing is generally easy to follow. \n\n4. Significance:\n- The proposed assumptions, settings, and methods are valuable to the RL research community as it shows preliminary positive results on smaller scale world models, which could potentially serve as basis for training RL agents within larger and more capable world models on more complex tasks."
            },
            "weaknesses": {
                "value": "1. While the paper presents results from multiple tasks (pendulum, half-cheetah, cartpole, hopper), there is a lack of extensive testing across a wider variety of environments and tasks. This raises some questions about the robustness and generalizability of the proposed method beyond the tested scenarios. Perhaps some tasks such as ant-maze or robot arm manipulation ones.\n\n2. The ensembles of world models seems essential to the proposed method. A sweep over the numbers of world models in the ensemble v.s. tasks' performance could reveal more information on how many world models are needed to achieve certain level of task performance.\n\n3. Minor typos:\n- Line 131: This setting Furthermore\n- Line 509: the type (of) increasingly available large-scale datasets"
            },
            "questions": {
                "value": "1. \"Each of the baselines is tuned by doing a grid search of the ranges documented in their respective papers:\n- Is the grid search conducted over their original dataset and transferred to the curated dataset? Or is the grid search done on the curated dataset?\n\n2. What would happen if the proposed method is trained and evaluated on the d4rl datasets? In the current paper, we see baseline methods do not perform as well as the proposed method under the paper's settings. The authors argued d4rl dataset is biased towards the baseline methods and provided visualizations. However, it would still be interesting to see how it impacts the proposed method.\n\n3. \"We open source all our code and data to facilitate further work in this exciting direction.\"\n- I couldn't find a link to the code repo, or find any supplementary materials."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores the potential of using uncurated offline data to train world models that can serve as a training ground for reinforcement learning. The primary goal is to enable the transfer of learned policies from these world models to the real world, thereby reducing the reliance on task-specific simulation environments. The authors demonstrate that by ensembling multiple independently trained world models, they can achieve robust transfer to the real world, even when the offline datasets are much smaller than those typically used in offline RL."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper presents a novel approach for training RL agents using world models derived from large-scale, uncurated offline data. By employing an ensemble of world models trained on the same dataset and leveraging them to create learning curricula through the Unsupervised Environment Design method, this work introduces a fresh perspective to RL."
            },
            "weaknesses": {
                "value": "- In my view, the main issue with this paper is that it somewhat exaggerates the contributions of the proposed method. The title, \"Investigating Online RL in World Models,\" is slightly misleading, as the study addresses an offline RL problem. Additionally, the term \"world models\" could be confusing. While the paper discusses many existing visual world model approaches (such as Ha and Schmidhuber's NIPS 2018 paper and recent interactive video generation studies), it does not actually work with visual data, instead focusing on fully observable MDPs in low-dimensional state spaces. I suggest the authors consider replacing the term \"world models\" to more accurately reflect the context.\n\n- The organization of the paper is also disjointed, with an imbalanced structure. For example, Chapter 2 uses considerable space for background information, while the methods section in Chapter 3 is relatively brief. This structure makes it challenging for readers to fully grasp the paper's core contributions.\n\n- In methodology, the authors treat world models trained on offline data of varying quality as different levels in an unsupervised environment design approach. I recommend that the authors discuss the motivation and rationale for this choice, explaining why this training method would lead to a robust and transferable policy.\n\n- While the paper outlines an ambitious story, it lacks sufficient experimental support: \n(1) The authors claim to use the D4RL dataset but do not provide comprehensive experimental results across different tasks. The focus on the relatively simple Hopper task is insufficient to support their claim.\n(2) The paper lacks comparisons with recent offline RL methods, as well as more detailed model analysis, such as examining the impact of the number of world models --- The authors mention training 100 world models in line 77, which seems excessive for a simple Hopper task and introduces considerable training overhead, which raises questions about the method's practical use in real-world applications."
            },
            "questions": {
                "value": "- Could the authors provide additional experiments on visual offline RL tasks to demonstrate the world model's generalizability in open-world or visual environments?\n- I strongly suggest that the authors further discuss the impact of the number of world models on the experimental results and clarify the necessity of using 100 world models.\n- If possible, please refine the methods section to more clearly highlight the core contributions of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper combined Unsupervised Environment Discovery (UED) and world model ensembles to provide a method for offline model-based RL that is sample-efficient. It treats different world models as levels within the PLR curriculum method in UED. It evaluates the method on vector-based cartpole, hopper, half cheetah, and pendulum tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- the paper performs an interesting combination of UED and world models, considering each world model as a level in UED\n- the method doesn't rely on online tuning in the environment (and uses held out world models to tune hyperparameters)\n- the results demonstrate sample-efficiency gains compared to CQL and a vanilla world model baseline"
            },
            "weaknesses": {
                "value": "- The differences with other model-based offline RL methods like MOPO [1], MBPO [2], Planning with Diffusion [3] is not clear, especially since the world model in the experiments in this paper, unlike David Ha's work, is just a simple MLP. \n- It would be helpful to compare with other offline MBRL methods like the ones above as well as world model based approaches like IRIS [4] and Dreamer [5], particularly on more challenging environments with image observations than the vector observation based  locomotion environments. Detailing the differences with these world model based methods would also be useful\n- Other papers to discuss in the related work include sample-efficient BC approaches that can work with very few demonstrations like ROT [6] and MCNN [7]. In summary, this work could use more comparisons (or atleast discussions comparing) to other works on offline MBRL, world models, and sample-efficient behavior cloning as well as evaluations in more challenging environments with image observations.\n\nMinor comments:\n- the return plotted for different methods is not normalized --- this makes it hard to determine its performance between random and expert and hard to compare with other papers\n- confusingly, \"inside world model\" is referred to as \"simulation\" and \"in simulation\" is referred to as \"real world\"\n- Appendix A.3 is empty\n\n[1] T Yu, et al, MOPO: Model-based Offline Policy Optimization, NeurIPS 20\n\n[2] M Janner, et al, When to Trust Your Model: Model-Based Policy Optimization, NeurIPS 19\n\n[3] M Janner, et al, Planning with diffusion for flexible behavior synthesis, ICML 22\n\n[4] V Micheli, et al, Transformers are Sample-Efficient World Models, ICLR 23\n\n[5] D Hafner, et al, Mastering Diverse Domains through World Models \n\n[6] S Haldar, et al, Watch and match: Supercharging imitation with regularized optimal transport, CoRL 22\n\n[7] K Sridhar, et al, Memory-Consistent Neural Networks for Imitation Learning, ICLR 24"
            },
            "questions": {
                "value": "Please see weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates online reinforcement learning directly within world models without conservative constraints typically used in offline RL and proposes to combine ensemble world models and prioritized level replay to tackle this problem."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The motivation to perform online RL directly in world models is timely and relevant."
            },
            "weaknesses": {
                "value": "1. The method is confusing. What is the $\\delta$ in Equation (5)? Why the Equation (5) approximates the regret? The right side of Figure 1 is difficult to understand. I suggest adding pseudocode to clarify the algorithm.\n\n2. The paper structure and writing need significant improvement. For example, the introduction lacks a clear statement of contributions. The relationship between contextual MDP subsection in Preliminaries and the reset of paper is unclear. The Preliminaries section is too long.\n\n3. The dataset construction method appears similar to D4RL. Both record the encountered transitions from randomness to expertise during training. Why the authors claim that D4RL is adversarial in data coverage (line 55) and CQL and TD3+BC does not inform this (line 263)? The data coverage is a common concern when building offline RL datasets, and previous benchmarks typically offer choices with various data coverage (D4RL and Atari DQN-Replay[1]).\n \n4. The checkpoint frequency seems uniform based on Figure 2, despite claims of heuristic selection. At the 5th ckpt, the agent has basically converged. The distribution perhaps change little after the subsequent sampling. Should the sampling frequency be increased between the first few ckpts?\n\n5. The world model architecture (simple MLP with current state and action as input) seems overly simplistic compared to state-of-the-art approaches like RSSM, Transformer, or diffusion models. Since the authors assumes that learning is done within a generalist world model (line 58 and 87), the experiment results of MLP with no historical trajectory are not convincing. I even suspect that using a single SOTA architecture world model can already solve this problem.\n\n6. Based on Figures 4-6, the DR methods perform well, and the proposed PLR methods do not show clear advantages.\n\n7. Judging from the rendering results of the `ref` and `cite` commands, this paper does not use the ICLR 2025 template!\n\n[1] Agarwal R, Schuurmans D, Norouzi M. An optimistic perspective on offline reinforcement learning. International conference on machine learning. PMLR, 2020: 104-114."
            },
            "questions": {
                "value": "1. What is the size of the collected dataset in terms of transitions?\n2. Could you provide pseudocode for the algorithm to aid understanding?\n3. Why were existing implementations of CQL[1] and SAC_N[2] not used?\n4. There are numerous typos that hinder readability. Such as incorrect citation format (line 51 and 53), \"This setting Furthermore, they showed ...\" (line 131), \"Therefore, the policy\u2019s minimizes ...\" (line 136), \"Prioritized Level Replay as described in with an ...\" (line 305) and so on.\n5. The abstract states that \"training inside world models is usually studied in the context of offline RL.\" However, many online RL algorithms (e.g., MBPO, Dreamer, TD-MPC, IRIS, TWM, STORM) train agents entirely with imagined trajectories (i.e., within world model). On the contrary, most of the offline model-based RL seems to be based on the Dyna framework, that is, using both offline datasets and world model imagination trajectories to train the agent.\n\n[1] https://github.com/young-geng/JaxCQL\n\n[2] https://github.com/Howuhh/sac-n-jax"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses the challenge of training online RL models inside world models. For this they rely on an ensemble of world models to train agents in an online fashion, without any offline penalties. The proposed method is evaluated on robotics tasks including Cartpole, Halfcheetah and Hopper."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper investigates an interesting question, that is, whether RL agents can be trained online inside world models, without the need for constraints required in offline RL."
            },
            "weaknesses": {
                "value": "The presentation of this paper can be improved. For example, Figure 1 is confusing and lacks a detailed figure caption to explain what\u2019s going on. Dataset distributions of offline RL datasets are criticised in the Discussion section, which feels out of place (also as this section comes after Related Work). Instead of bringing these results after the main experiments, they could be seen as a motivation before the main experiments (which would require experiments that support the motivation).\n\nIt is unclear where the proposed method starts and ends, or what the authors consider as their own method. Instead, 6 different variations (PLR, PLR_PVL, DR, DR_STEP, DR_PROB) of ensemble world models are tested against a single world model (WM). \n\nIn the current form, the paper lacks convincing empirical evidence. The main experiments are conducted on toy tasks like Cartpole, Pendulum, Acrobot, Hopper and Halfcheetah. Across most environments (Figure 4, 6, 7), the ensemble methods (PLR, DR, PLR_PVL, DR_STEP) tend to learn faster than a single model (WM) but reach the same level of performance. Furthermore, there is a lack of offline RL baselines (only CQL is provided). Also, there\u2019s a lack of ablations on components of their method. For example, what is the effect of the size of the ensemble? \n\nThe proposed method relies on an ensemble of world models. Training an ensemble of world models is feasible in the toy tasks considered currently, but raises questions on scalability of the proposed methods to more complex environments, such as visual domains. Consequently, reporting information on the additional cost of training and during evaluation of those models would benefit the paper."
            },
            "questions": {
                "value": "- What is the size of the world model ensemble used in your experiments? What is the effect of the number of world models? \n- What is the additional cost of training and evaluating using an ensemble? \n- How does your method transfer to other benchmarks (e.g., Atari, Procgen, Meta-World, etc.) with regard to efficiency at training and inference time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}