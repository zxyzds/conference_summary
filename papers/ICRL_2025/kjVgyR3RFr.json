{
    "id": "kjVgyR3RFr",
    "title": "Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models",
    "abstract": "Despite the rapid progress and outstanding performance of Large Vision-Language Models (LVLMs) in recent years, LVLMs have been plagued by the issue of hallucination, i.e., LVLMs tend to generate responses that are inconsistent with the corresponding visual inputs. To evaluate the degree of hallucination in LVLMs, previous works have proposed a series of benchmarks featuring different types of tasks and evaluation metrics. However, we find that the quality of the existing hallucination benchmarks varies, with some suffering from problems, e.g., inconsistent evaluation results under repeated tests, and misalignment with human evaluation. To this end, we propose a Hallucination benchmark Quality Measurement framework (HQM), which leverages various indicators to assess the reliability and validity of existing hallucination benchmarks separately. Specifically, for reliability we explore test-retest reliability and parallel-forms reliability, while for validity we examine criterion validity and coverage of hallucination types. Furthermore, we construct a High-Quality Hallucination Benchmark (HQH) for LVLMs, which demonstrates superior reliability and validity under our HQM framework. We conduct an extensive evaluation of over 10 representative LVLMs, including GPT-4o and Gemini-1.5-Pro, to provide an in-depth analysis of the hallucination issues in existing models. Our benchmark is publicly available at https://github.com/HQHBench/HQHBench.",
    "keywords": [
        "Large Vision-Language Models",
        "Hallucination",
        "Benchmark"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kjVgyR3RFr",
    "pdf_link": "https://openreview.net/pdf?id=kjVgyR3RFr",
    "comments": [
        {
            "summary": {
                "value": "This paper rethinks the reliability and validity of existing hallucination evaluation benchmarks, and therefore proposes test-retest and parallel testing methods for quantitative evaluation. Besides, this paper also curates a High-Quality Hallucination Benchmark (HQH) for comprehensively evaluating existing LVLMs hallucination degree."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Strengthens\n1.\tThis paper investigates the instability of the evaluation metric of existing hallucination benchmarks, and contribute a new high-quality benchmark for the community.\n2.\tThe writing quality is good and easy to follow."
            },
            "weaknesses": {
                "value": "1. The overall technical contribution is limited. The primary contributions of this paper include that existing evaluation metrics of hallucination benchmarks are instable under multi-fold validation, and curates a high-quality hallucination benchmark with \u201challucination rate\u201d as the evaluation metric.\n2. The motivation and method are inconsistent. The major motivation of this paper lies in that the evaluation metrics of existing hallucination benchmarks are insufficient. But the proposed solution is to curating a new high-quality benchmark and calculate \u201challucination rate\u201d by altering the GPT-scores and GPT-based binary choices. The proposed multi-fold validation criterions are not employed for HQH evaluation.\n3. The details of proposed designs are unclear. First, is it reasonable to use the validation set of Visual Genome dataset? It can incurs data leakage problem as existing LVLMs are using validation sets of open-source datasets for their training. Under this circumstance, test set is more suitable for establishing a benchmark. Second, what are the key differences between asking a LLM to directly give a score and judging whether the answer includes hallucination? I cannot figure out their intrinsic differences here."
            },
            "questions": {
                "value": "1. I'm wondering whether it is reasonable to use the validation set of Visual Genome dataset. It can incurs data leakage problem as existing LVLMs are using validation sets of open-source datasets for their training.\n2. What are the key differences between asking a LLM to directly give a score and judging whether the answer includes hallucination? I cannot figure out their intrinsic differences here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes to look at multi-modal hallucination benchmarking from a novel perspective, i.e. the property of existing benchmarks. Further, on top of these properties, the paper presents a newly curated hallucination benchmark, providing more reliable and trustworthy evaluation of LVLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The work systematically studies the reliability and robustness for existing LVLM benchmarks. It provides quantitative evidences, characterizing various benchmarks' weakness in reliability (yes-and-no) and validity (free-form).\n- The curated HQHBench effectively leverage existing annotations from Visual Genome, and have demonstrated that such a benchmark provides both reliable and valid signal for LVLM evaluation.\n\nI also appreciate that the authors also managed to provide an anonymous data hosting the actual benchmark, which greatly increase the credibility of this work and the speed of the community to benefit from it."
            },
            "weaknesses": {
                "value": "One challenge I have is that some efforts of (in-)validating existing benchmarks are already creating new benchmarks that come with certain properties:\n- we can combine POPE and \"POPE-rewrite\" as a benchmark that is robust against this parallel-form check, because it has been saturated with parallel forms.\n- By utilizing the latest models, like GPT4o/o1, Claude 3.5 etc, validity may improve as well.\n\nEssentially, the effort of curating a new benchmark is orthogonal to improving existing benchmarks. Even more I'd say they are additive to some extend, as this is a more efficient way to merge community efforts for a more comprehensive benchmark. I'd like to see authors' discussion on this point.\n\nAnother issue is that the HQHBench is curated with the bless of Visual Genome annotation, therefore hallucination judgement can be done in a binary format against image information provided in the annotation. However, there are still image information not covered by the annotation. Such a missing-annotation scenario may cause a rich response capturing not-in-annotation fact be judged as hallucination. How can we address such scenarios?"
            },
            "questions": {
                "value": "- The test-retest protocol is a bit unclear. When we re-run the benchmark, do we require the model being evaluated to re-generate the responses? If not, why is there difference for Yes-or-No benchmarks? If yes, how do we distinguish whether the randomness comes from the benchmarking judge, or the models being evaluated?\n- Visual Genome dataset's annotation is known to be noisy. How does this benchmark mitigate this issue?\n- Can you provide an example of what POPE has been re-written into? Since POPE itself contains random/adversarial settings that prevents a degraded all-Yes model to pass, it already introduces parallel forms."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors first evaluate the quality of current hallucination benchmarks, then propose a new benchmark HQHBench"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Please refer to Questions"
            },
            "weaknesses": {
                "value": "Please refer to Questions"
            },
            "questions": {
                "value": "### Strength\n1. The paper is well-written and easy to follow\n2. The proposed evaluation method is proper.\n3. The proposed benchmark seems to work well on the evaluation.\n\n### Weakness\n1. My main concern is the limited novelty. As the authors introduced in Sec.3, introducing psychometrics into AI is not a new idea, and the proposed metrics (Test-retest Reliability, Parallel-forms Reliability, Criterion Validity) are also proposed by previous works. So the evaluation part seems to evaluate the current benchmarks with an off-the-shelf idea, with limited novelty. \n\n2. Following 1, the evaluation is a general idea that is unrelated to the Hallucination problem. We could apply it to other multi-modal benchmarks and have some similar conclusions. So I think it hardly provides some insight about the following study about hallucination.\n\n3. Similarly, the proposed HQHBench also seems like a common MLLM capability evaluation benchmark (for example, a free-form version of MMBench or SeedBench), rather than a hallucination evaluation benchmark. **If we treat all the wrong answers as hallucinations, we should not view hallucination as an independent research topic.**\n\n4. Last, I have a concern about the evaluation quality of HQHBench, the GPT-3.5 seems not capable enough to understand the complex image with only a short caption and phrases with bbox. \n\n5. Besides, the MLLMs evaluated in the paper are quite out-of-date. There are many capable new models, such as LLaVA-Next/OneVision, Qwen2-VL, InternVL2, NVLM, etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors first conduct a comparison between hallucination benchmarks on test-retest reliability, parallel-forms reliability, and criterion validity. Based on their findings, the authors construct the High-quality Hallucination Benchmark (HQH), which demonstrates reliability and validity."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The authors innovatively start with a benchmark on hallucination benchmarks.\n- The experimental results are extensive.\n- The code is provided. I appreciate the authors doing that."
            },
            "weaknesses": {
                "value": "- About test-retest reliability:\n  - The overall idea is the testify to the consistency of evaluation results over different random seeds, which, however, involves two factors, including the randomness of the models and the metrics. It is unreasonable to attribute it directly to the metrics.\n- About parallel-form reliability:\n  - During the construction of HQH, to improve validity, the authors use free-form VQA, and to improve test-retest reliability, the authors prompt GPT to only conduct binary classification.\n  - I wonder how HQH improves its parallel-form reliability, even compared with the free-form counterparts like MMHal and GAVIE.\n- About the evaluation metric of HQH:\n  - In Figure 4, we see that the authors provide a lot of textual information about the image for GPT. Do you still give the original image to GPT? If so, why did you choose to provide this textual information explicitly to GPT judgment?\n- About insight for mitigation:\n  - Indeed, this paper proposes a better hallucination benchmark. However, it seems that it mainly focuses on the benchmark construction, while cannot provide us more insight into how to relieve LVLM hallucination.\n  - Based on the more human-aligned evaluation results in Table 5, can you give us observations or insights that we cannot derive from the less human-aligned benchmarks?\n\n- Overall, I think it is an interesting paper. However, there are still unclarified details and analyses requiring revision."
            },
            "questions": {
                "value": "- About instruction collection:\n\n  - Do you use and filter the original instructions of the VG dataset or create new ones?\n  - During the filtering process of instructions, is the hallucination type the main concern or have you done anything to maintain reliability and validity?\n  - I think a more detailed explanation of the instruction collection procedure of the HQH dataset is interesting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}