{
    "id": "QyVLJ7EnAC",
    "title": "Model-Free Offline Reinforcement Learning with Enhanced Robustness",
    "abstract": "Offline reinforcement learning (RL) has gained considerable attention for its ability to learn policies from pre-collected data without real-time interaction, which makes it particularly useful for high-risk applications. However, due to its reliance on offline datasets, existing works inevitably introduce assumptions to ensure effective learning, which, however, often lead to a trade-off between robustness to model mismatch and scalability to large environments. In this paper, we enhance both aspects with a novel double-pessimism principle, which conservatively estimates performance and accounts for both limited data and potential model mismatches, two major reasons for the previous trade-off. We then propose a universal, model-free algorithm to learn an optimal policy that is robust to potential environment mismatches, which enhances robustness in a scalable manner. Furthermore, we provide a sample complexity analysis of our algorithm when the mismatch is modeled by the $l_\\alpha$-norm, which also theoretically demonstrates the efficiency of our method. Extensive experiments further demonstrate that our approach significantly improves robustness in a more scalable manner than existing methods.",
    "keywords": [
        "offline RL",
        "robust",
        "scalability",
        "model-free"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=QyVLJ7EnAC",
    "pdf_link": "https://openreview.net/pdf?id=QyVLJ7EnAC",
    "comments": [
        {
            "summary": {
                "value": "This work enhances offline RL by developing a scalable, robust model-free algorithm that optimizes worst-case performance within an uncertainty set. Using a double-pessimism principle to handle data uncertainty and model mismatch, the method adapts across settings without model estimation. Through theoretical analysis the author demonstrates the algorithm\u2019s near-optimal data efficiency, advancing robustness and scalability in offline RL."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces a \"double-pessimism principle\" that uniquely addresses both model mismatch and limited dataset coverage within a model-free framework\n2. The authors provide a rigorous theoretical foundation, including sample complexity analysis and convergence guarantees, which enhances the paper's quality"
            },
            "weaknesses": {
                "value": "1. The paper lacks sufficient experiments to demonstrate the effectiveness of the proposed method. For instance, there is no comparison with model-based baselines, and the evaluation environment is relatively simple.\n\n2. There are no ablation experiments analyzing the functionality of the proposed \"double-pessimism principle.\""
            },
            "questions": {
                "value": "1. As mentioned above, have the authors evaluated how the proposed appraoch performs in any of the D4RL environments, which are widely used in offline RL research?\n2. Are there any strategies for fine-tuning parameters for specific tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors proposed a novel double-pessimism principle framework for offline reinforcement learning, which conservatively estimates the performance of offline policy learning and accounts for both the pessimism caused by limited data and potential model mismatch. Compared with previous methods that usually leverage model-based approaches, the authors proposed a model free algorithm to learn the optimal policy that is robust to potential environment mismatches, which enhances robustness in a scalable manner. The authors also provide sample complexity analysis of the algorithm and extensive experiments to further demonstrate that the approach is robust and scalable."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The authors present a scalable and robust model-free algorithm to quantify both the uncertainty of model mismatch and limited dataset; the way of constructing the model mismatch pessimistic allows the authors to derive a model-free algorithms;\n- The authors present the sample complexity result for both finite horizon case and infinite discounted case. \n- The authors demonstrate that with the double pessimism principle, empirically we can learn better policies compared with single pessimistic based algorithms."
            },
            "weaknesses": {
                "value": "-My central question about the paper is that in practise how we can justify the uncertainty set in real case, and how we can estimate the parameters of the uncertainty set instead of directly assigning some upper hyperparameter."
            },
            "questions": {
                "value": "- In some environments such as mujoco, how can we identify what kind of uncertainty set we should use here, and how can we estimate the uncertainty set hyperparameters to ensure the policy we learn is under the real settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a model-free offline RL algorithm using a \"double-pessimism principle\" to enhance robustness against model mismatches and scalability, effectively handling uncertainties from limited data and environment shifts."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The paper is well-structured.  \n2. The double-pessimism principle effectively tackles uncertainties from both data limitations and environment mismatch, adding a valuable perspective to offline RL.  \n3. By avoiding transition model estimation, the algorithm shows strong scalability potential for complex environments.  \n4. The paper provides a detailed sample complexity analysis and shows performance improvements on benchmark tasks, supporting the method's effectiveness."
            },
            "weaknesses": {
                "value": "1. While the paper shows strong results, additional tests on more diverse, high-dimensional environments could provide a clearer picture of scalability, such as d4rl.\n2. Comparing other offline RL algorithms, such as CQL, IQL, etc., will further enhance the contribution of this work."
            },
            "questions": {
                "value": "n/a"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a new approach for offline reinforcement learning that emphasizes both robustness to model mismatch and limited dataset. The paper provides theoretical results regarding robust value estimation, sub-optimality gap, and sample efficiency. The empirical results also support the superiority of the proposed double-pessimism approach, compared to the single-pessimism approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper proposed the first offline model-free algorithm with a robustness guarantee.\n- The empirical results on toy environments show the proposed double-pessimism approach outperforms the single-pessimism baseline."
            },
            "weaknesses": {
                "value": "- The proposed method has a higher sample complexity than the baseline (Shi & Chi, 2022) regarding gamma.\n- Adding another pessimistic term might lead to overly conservative policies, especially when the model mismatch problem is not significant.\n- The source code is not provided, making it difficult to assess the reproducibility of the results.\n- It would be more practical if the authors could provide a real-world example where model mismatch is likely to occur and construct the uncertainty set based on that scenario. While the mismatch modeled by the \\ell_\\alpha norm is suitable for theoretical guarantees, it is unclear if real-world perturbations would follow this type."
            },
            "questions": {
                "value": "- In Figure 2, can the authors provide the standard deviations or confidence intervals for the y-axis of each method? The envelopes in Figure 1 were informative, but Figure 2 does not provide such."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel approach to offline reinforcement learning (RL) by introducing a double-pessimism principle. This principle aims to enhance robustness and scalability by conservatively estimating performance, accounting for both limited data and potential model mismatches. The authors propose a model-free algorithm that is robust to environment mismatches and provide a rigorous sample complexity analysis for a specific case of mismatch modeling using the $l_\\alpha$-norm. The paper also includes some experiments demonstrating the approach's effectiveness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The introduction of the double-pessimism principle is a significant contribution to offline RL. By addressing both data limitations and model mismatches, the approach provides a more comprehensive framework for robust policy learning.\n- The paper offers a detailed sample complexity analysis for the $l_\\alpha$-norm model, which theoretically demonstrates the efficiency of the proposed method. This analysis is crucial for understanding the algorithm's performance and provides a solid foundation for its claims.\n- Some experiments conducted in both simulated and real environments showcase the practical applicability and robustness of the proposed algorithm. The results indicate that the double-pessimism approach outperforms existing methods in handling model uncertainty."
            },
            "weaknesses": {
                "value": "- The paper's focus on perturbations over transition probabilities is a limitation. In real-world applications, perturbations can occur in various forms, not just in transition probabilities. This narrow focus may hinder the practical applicability of the approach in more complex environments where other types of perturbations are present.\n- While the sample complexity analysis is rigorous, it is limited to the $l_\\alpha$-norm model. This restricts the generalizability of the theoretical findings to other types of perturbations or uncertainty models. A broader analysis covering more general cases would strengthen the paper's contributions.\n- For general perturbations, the approach may require solving large optimization problems for each update. This could significantly affect learning efficiency, especially in large-scale or real-time applications. The paper would benefit from a discussion on how to mitigate these computational challenges."
            },
            "questions": {
                "value": "- Can you provide more details on the algorithm's implementation efficiency, particularly regarding the $\\kappa$ calculation? What is its computational complexity?\n- Does the current definition of perturbations ensure that their impacts are not cascading\u2014meaning they don't affect future steps?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}