{
    "id": "EVK0sQHVCd",
    "title": "How and how well do diffusion models improve adversarial robustness?",
    "abstract": "Recent findings suggest that diffusion models significantly enhance empirical adversarial robustness. While some intuitive explanations have been proposed, the precise mechanisms underlying these improvements remain unclear. In this work, we systematically investigate how and how well do diffusion models improve adversarial robustness. First, we observe that diffusion models intriguingly increase\u2014rather than decrease\u2014the $\\ell_p$ distances to clean samples. This is the opposite of what was believed previously. Second, we find that the purified images are heavily influenced by the internal randomness of diffusion models. To properly evaluate the robustness of systems with inherent randomness, we introduce the concept of fuzzy adversarial robustness, and find that empirically a substantial fraction of adversarial examples are fuzzy in nature. Finally, by leveraging a hyperspherical cap model of adversarial regions, we show that diffusion models increase robustness by dramatically compressing the image space. Our findings provide novel insights into the mechanisms behind the robustness improvements of diffusion-model-based purification and offer guidance for the development of more efficient adversarial purification systems.",
    "keywords": [
        "diffusion models",
        "adversarial purification",
        "robustness"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=EVK0sQHVCd",
    "pdf_link": "https://openreview.net/pdf?id=EVK0sQHVCd",
    "comments": [
        {
            "summary": {
                "value": "This paper provides theoretical and empirical studies on the mechanism of diffusion-based adversarial purification methods. First, based on empirical studies on the behavior of diffusion-based purification models, it is suggested that the purification generally increases the $\\ell_p$ distance of the input sample to the clean sample, and that the purification results are affected by randomness in diffusion models. Second, the concept of fuzzy robustness and the corresponding evaluation method are proposed for diffusion-based purification methods. Third, a hyperspherical cap model is introduced to depict the adversarial regions. Finally, it is argued that the robustness brought by diffusion-based purification is due to the compressing of image space."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The observation of the increasing $\\ell_p$ distance to the clean sample produced by diffusion-based purification is interesting.\n- The definition of fuzzy adversarial robustness is meaningful and the proposed CFR curve can be a practical evaluation metric for stochastic defense methods.\n- The figures clearly demonstrate the ideas of the paper."
            },
            "weaknesses": {
                "value": "- The description of the previous theoretical result in (Nie et al., 2022) is unfaithful.\n  - In Section 3.1, it is stated that the previous explanation of the robustness of diffusion-based purification methods lies in the \"shrinkage of the adversarial attack\", or more specifically, the decreasing $\\ell_p$ distance to the clean sample after purification.\n  - Nonetheless, Theorem 3.1 in (Nie et al., 2022) only suggests that the divergence between the clean data distribution and adversarial sample distribution can be decreased by the *forward diffusion process*. It is not assumed that the purified sample obtained by the *diffusion-denoising purification process* is closer to the clean sample under $\\ell_p$ distance.\n  - From my perspective, while the purified sample is expected to lie on the non-adversarial manifold, its $\\ell_p$ distance to the clean sample is not important as long as the semantic information is preserved.\n- The hyperspherical cap model is not well supported.\n  - Crossing the critical threshold along an adversarial direction can be a necessary condition for adversarial samples, but it is insufficient. Specifically, Assumption 2 is valid locally given the continuity of the model, but there can be an upper bound on radius $\\hat{\\gamma}(x_0, \\eta)$ determined by $x_0$ and the direction $\\eta$ for this assumption to hold. In other words, the sample may cross the boundary again as the radius exceeds $\\hat{\\gamma}(x_0, \\eta)$. An example of such a decision boundary is depicted in Figure 1 of [1].\n  - Therefore, unless a non-trivial uniform upper bound $\\hat{\\gamma}$ is derived, it cannot be claimed that crossing the critical threshold is a sufficient condition for adversarial examples within an $\\ell_2$ neighborhood with a certain radius.\n  - It is also assumed that \"the classification boundaries are locally linear\" (Lines 375-376), which is not supported by valid theoretical or empirical evidence. While the decision boundaries depicted in the 2D projection in Figure 4(b,c) appear to be linear, they are likely non-linear in the high-dimensional input space. Stronger evidence or proper references are required to claim this point.\n- The causal relationship between the compression of image space and the improved robustness is not well explained.\n  - Section 6 has validated that diffusion-based purification can compress the image space, but it's still not apparent how it contributes to the robustness. For example, while the magnitude of the critical threshold can be reduced due to the compression, the purified sample $f(x_0)$ may also lie closer to the decision boundary, which cannot explain the improved robustness.\n  - It is better to clarify whether the compression effect is the sole contributor to the robustness of diffusion-based purification models.\n- The little-o notation in Line 717 is inappropriate since two real numbers are compared here instead of two growing functions. The \"considerably larger\" change of logit should be better defined.\n\n[1] Kim, Hoki, Woojin Lee, and Jaewook Lee. \"Understanding catastrophic overfitting in single-step adversarial training.\" AAAI 2021."
            },
            "questions": {
                "value": "- If image space compressing is sufficient for improving the robustness, are conventional image compressing methods like JPEG effective for adversarial defense?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This is an analytical work to investigate how diffusion-based purification (DBP) improve the adversarial robustness. With pilot experiments, this work finds that diffusion models increase the lp distances to clean samples and the purified images are heavily influenced by the internal randomness of diffusion models. Furthermore, this work introduces the concept of fuzzy adversarial robustness and hyper-spherical cap model of adversarial regions, and gives an explanation on how DBP works."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tOverall, this paper is well-written, with clear organizations and illustrations.\n2.\tThe finds in this work are reasonable, with sufficient explanations and experimental supports."
            },
            "weaknesses": {
                "value": "My major concern on this work is about the contribution of this work:\n\n1)\tIt is not surprising that diffusion-based purification will guide the adversarial images to a place with a larger distance to the original images. As the adversarial perturbation is usually small and the corrupted process (adding Gaussian noise) will make it hard to reverse it to the original image (information on the original image is missing). According to [1,2], the reversion process of diffusion models is to make the generated image has a higher probability to be close to the original image distribution, which does not mean this image should be closer to the original image. Below are some suggested experiments to make this work more comprehensive:\n\n- a)\tBesides traditional l_p distance, the metrics for generative models (e.g., FID score, Inception score) should also be used to evaluate the distance. In such latent space, the conclusion might be different.\n\n- b)\tLines 171-172. I think this experiment is important and should not be hidden.\n\n- c)\tEarly denoiser-based defense [3] has been shown to be ineffective while diffusion-based purification is effective. It is important to show if early denoiser-based defense has the similar behavior (enlarging the lp distance) to diffusion models. If they are similar, the findings in this work may not explain why they have different defense performance.\n\n2)\tThe proposed fuzzy adversarial robustness (a new framework in evaluation) is similar to [1] while the discussion on it is missing.\n3)\tThis work claims that the findings offer guidance for the development of more efficient adversarial purification (AP) systems, but no deeper discussions and experiments are provided. If this work can give a prototype method of more efficient AP with the findings, I could have improved my score.\n\nLine 450: $\\textbf{x}_0$\uff0c line 269: $\\textbf{x}^{`}$\n\n[1] Xiao C, Chen Z, Jin K, et al. Densepure: Understanding diffusion models towards adversarial robustness[J]. arXiv preprint arXiv:2211.00322, 2022.\n\n[2] Chen H, Dong Y, Wang Z, et al. Robust classification via a single diffusion model[J]. arXiv preprint arXiv:2305.15241, 2023.\n\n[3] Liao F, Liang M, Dong Y, et al. Defense against adversarial attacks using high-level representation guided denoiser[C] CVPR. 2018: 1778-1787.\n\n[4] DiffHammer: Rethinking the Robustness of Diffusion-Based Adversarial Purification. NeurIPS, 2024"
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "As mentioned in the title, this paper aims to investigate how and how well diffusion models improve adversarial robustness. Specifically, this paper first demonstrates that diffusion models push the purified images away from clean images, which challenges the previous belief (i.e., diffusion models will push adversarial images closer to clean images).  Then, this paper shows that the randomness largely influences the robustness of diffusion models. Based on this, this paper introduces a new concept called 'fuzzy adversarial examples', which considers the probability of adversarial attack fooling the system and proposes a new robust evaluation metric called 'cumulative fuzzy robustness (CFR)' to evaluate the robustness of fuzzy adversarial examples. Lastly, this paper uses a hyperspherical cap model to show that diffusion models improve robustness by shrinking the image space."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. This paper is very well-written and easy to follow.\n\n2. This paper provides sufficient insights (as mentioned in the summary) on diffusion-based adversarial purification, which can inspire researchers in this area to develop more advanced defense methods. More importantly, this paper closes an important research gap on how diffusion-based adversarial purification actually works. I would also like to hear other reviewers' opinions on the contributions of this paper.\n\n3. The concept of fuzzy adversarial examples is novel and intuitive. Based on this, the proposed cumulative fuzzy robustness (CFR) is a more suitable evaluation metric to examine the robustness of a system with inherent robustness (especially for diffusion-based adversarial purifications)."
            },
            "weaknesses": {
                "value": "1. While the observation that the diffusion model further pushes adversarial examples away from clean examples is intriguing, this paper lacks a theoretical explanation of why this occurs. However, this is only a minor weakness, as including a theoretical explanation is not always possible.\n\n2. Fuzzy robustness evaluation is only performed on DiffPure, which is a bit outdated. Authors are encouraged to evaluate more recent diffusion-based adversarial purification methods to make results more convincible (e.g., [1] [2]).\n\n[1] DensePure: Understanding Diffusion Models Towards Adversarial Robustness, ICLR 2023.\n[2] Robust Evaluation of Diffusion-Based Adversarial Purification, ICCV 2023."
            },
            "questions": {
                "value": "1. What is the PGD+EOT result on ImageNet? \n\n2. Just curious\u2014do you think the conclusions in this paper would hold up if diffusion models are constructed in latent space instead of pixel space? And why?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examined the properties of diffusion models used in defensed against adversarial attacks. The authors showed that diffusion models do not shrinkage the distance of the transformed images toward clean images. Instead, diffusion models force the images to a compressed latent space, thus enhancing adversarial robustness. Altogether, this paper provides an explanation as to why diffusion models improve empirical adversarial robustness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper made an interesting observation on adversarial defenses based on diffusion models, where the purified images have larger l2 distance to the clean images than the adverasrial images. This suggests that diffusion models do not increase robustness by simply removing noise.  \n- The paper is well-written and motivated, providing clear empirical evidence and concrete discussion as to how diffusion models work on improving adversarial robustness."
            },
            "weaknesses": {
                "value": "- In Figure 1, the authors show that the l_2 distance to the original clean images increase after diffusion purification. However, the authors do not consider the effect of the sampling process of diffusion models, e.g., deterministic vs random, or more advanced sampling method [1]. It is encouraged to validate the observation on more sampling methods of diffusion models.  \n- It might be good to have some practical discussions on how the observations/understanding from this paper could contribute to the development of future defenses, or even improving existing diffusion model based defenses.  \n\n[1] Elucidating the design space of diffusion-based generative models. 2022."
            },
            "questions": {
                "value": "- Could the authors evaluate the one-shot denoising approach as used by [1] and validate if the claim that l2 distance between purified images and clean images decrease as compared to that between adversarial images and clean images?  \n- As indicated by [1], one reason why diffusion model could provided adversarial robustness is that the denoised images can be well classified by the base classifier. Therefore, would it possible that the actual distance decrease under a perceptual based losses [2]?  \n\n[1] Certified adversarial robustness for free. Carlini et al, 2023.  \n[2] Perceptual losses for real-time style transfer and super-resolution. Johnson et al, 2016."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents several observations and explanations for DiffPure, mainly summarized as follows:\n\n- Diffusion models increase\u2014rather than decrease\u2014the \u2113p distances to clean samples.\n- The concept of fuzzy adversarial robustness is introduced.\n- A hyperspherical cap model of adversarial regions is proposed.\n- It is shown that diffusion models increase adversarial robustness by compressing the image space."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- By measuring the distance between denoised examples and adversarial images, this paper shows that DiffPure actually increases the distance between denoised images and adversarial examples, rather than decreasing it, which contradicts the previous claim that \"DiffPure increases robustness by decreasing the perturbation budget.\" Although I believe **Carlini et al., Xiao et al., and Nie et al. have never made this claim**, I still consider this experiment insightful, as many people agree with this idea.\n\n- I strongly appreciate the authors' experiments demonstrating that \"randomness in diffusion determines the purification direction but not the randomness in the input.\" I believe this is insightful for diffusion researchers."
            },
            "weaknesses": {
                "value": "This paper contains several ambiguities, overclaims, and misunderstandings of previous work (see both Weakness and Questions sections), but these can likely be quickly addressed during the rebuttal phase. I would consider raising my score if the authors address these issues.\n\n- I disagree with the authors' claim that \"a model simply encoding every clean image as the prior mode may not generalize well\" and \"the results above indicate that diffusion models are ineffective in removing small perturbations.\" A large distance between diffusion-purified images and real images does not support this claim. Imagine an image of a panda; the direction of hair growth in pandas can vary, so even if the hair direction changes, the image remains realistic. Such subtle changes often occur after diffusion denoising, causing an increase in \u2113p distance, but not due to poor generalization.\n\n- In Fig. 2 (a) and (b), the term \"correlation\" (e.g., 0.2368, 0.9954 in the paper) is not clearly defined. I'm still unsure of the x and y axes or what is meant by \"correlation\" here (covariance? cosine similarity? normalized correlation?).\n\n- \"Some treated randomness as a bug and proposed methods to cancel its effect in evaluation (cite Carlini et al.).\" Carlini never stated this. What Carlini means is that randomness may obscure gradients, making evaluations challenging, but Carlini does not claim that \"randomness is a bug\" or that randomness inherently negates robustness.\n\n- The authors' definition of \"fuzzy adversarial examples\" is already extensively discussed in the context of certified robustness via randomized smoothing. You should cite at least [1, 2], as their definition \\( g(x)_c = \\text{Pr}(f(x) = c) \\) closely aligns with yours. While I recognize the differences between your definition and theirs, citing these works is essential for academic integrity.\n\n---\n\n**References**  \n[1] Cohen, Jeremy, Elan Rosenfeld, and Zico Kolter. \"Certified adversarial robustness via randomized smoothing.\" International Conference on Machine Learning. PMLR, 2019.  \n[2] Salman, Hadi, et al. \"Provably robust deep learning via adversarially trained smoothed classifiers.\" Advances in Neural Information Processing Systems 32 (2019)."
            },
            "questions": {
                "value": "- I'm unclear about Fig. 1(c). The x-axis is labeled as the diffusion noise level, but what is the y-axis? Is it the distance between noisy examples and the original example, or between denoised examples and the original example? Additionally, why does the image become clearer rather than blurrier as \\( t \\) increases from 50 to 200? Are these images noisy examples or denoised ones? I can't fully understand this figure, and as a result, I'm unable to accurately interpret lines 162-201.\n\n- \"We removed the forward process by only performing denoising and still observed that the distances to clean samples increased.\" Could you please include this figure? In my opinion, diffusion will produce collapsed images if the input noise level does not align with the diffusion network's noise level condition. I\u2019m really curious to see the result of this experiment.\n\n- \"Under these assumptions, one can mathematically show that the adversarial regions should form a hyperspherical cap when considering the \\( \\ell_2 \\) neighborhood.\" In addition to including the proof in the appendix, I strongly recommend that the authors provide some intuition in the main paper to help readers understand why this holds. Could the authors give further intuition on this result during the rebuttal?\n\n- In Fig. 5(b), does the y-axis represent \"variance explained\" as the eigenvalue? What is meant by \"explained\" here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}