{
    "id": "0ydseYDKRi",
    "title": "Beyond The Rainbow: High Performance Deep Reinforcement Learning On A Desktop PC",
    "abstract": "Rainbow Deep Q-Network (DQN) demonstrated combining multiple independent enhancements could significantly boost a reinforcement learning (RL) agent\u2019s performance. In this paper, we present \"Beyond The Rainbow'\" (BTR), a novel algorithm that integrates six improvements from across the RL literature to Rainbow DQN, establishing a new state-of-the-art for RL using a desktop PC, with a human-normalized interquartile mean (IQM) of 7.4 on Atari-60. Beyond Atari, we demonstrate BTR's capability to handle complex 3D games, successfully training agents to play Super Mario Galaxy, Mario Kart, and Mortal Kombat with minimal algorithmic changes. Designing BTR with computational efficiency in mind, agents can be trained using a desktop PC on 200 million Atari frames within 12 hours. Additionally, we conduct detailed ablation studies of each component, analyzing the performance and impact using numerous measures.",
    "keywords": [
        "Reinforcement Learning",
        "Computational Efficiency",
        "High Performance",
        "Atari",
        "Value-Based",
        "DQN",
        "Rainbow DQN",
        "BeyondTheRainbow"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We create a high-performance Deep Reinforcement Learning algorithm capable of solving even modern games on a desktop PC.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=0ydseYDKRi",
    "pdf_link": "https://openreview.net/pdf?id=0ydseYDKRi",
    "comments": [
        {
            "comment": {
                "value": "We thank you for understanding our core motivation for this work and appreciating what it achieved. \n\n**Error Bars**\n\nIn this work, we computed the error bars using the evaluation episode variance rather than as proposed in [1] with the seed variance (for the mean evaluation performance). Using [1], for the Atari-5, we trained BTR across 3 seeds, finding IQM scores of 7.77, 8.22, and 7.86. However, due to time and computing limitations, we only trained BTR for 1 seed on Atari-60 and ProcGen, meaning we can\u2019t use [1] for error bars. If the reviewer agrees, we will train BTR for Atari-60 and ProcGen with three seeds, allowing us to update the error bars. However, due to the computational and time requirements of these experiments, we could not provide these results within the discussion period though they would be ready before the camera-ready version deadline.\n\nRegarding Figure 2, we did not intend to claim that we achieved state-of-the-art performance compared to prior work. Instead, we claim that we achieved similar performance as the prior state-of-the-art with significantly fewer resources, in particular, walltime and learnable parameters. We will change our wording on Line 083 from \u201coutperformed\u201d to \u201csimilar\u201d and Line 274 from \u201cexceed\u201d to \u201csimilar\u201d. Does this address the reviewer's concern? \n\n**Comparison of prior work on Wii Games**\n\nWhen demonstrating BTR in these new environments, we aimed to showcase BTR\u2019s capability in games that are not normally considered in RL research. Therefore, we didn\u2019t compare to the prior works shown in Figure 1 that achieved significantly lower scores than BTR. If the reviewer believes this is a crucial comparison to include, we are willing to train a Rainbow DQN agent for the Wii games to include in Figure 3. \n\n**Ablation Study**\n\nWe are happy to use human-normalized performance to measure each ablation\u2019s performance and will update Figure 4. Regarding the regression procedure, we are not making claims about the performance of all Atari games from these ablations, rather, we used Atari-5 as a benchmark to better discriminate each ablation\u2019s performance without needing to train on all Atari 60 environments to find similar results. Does this answer the reviewer\u2019s concern?\n\n**Dormant Neurons**\n\nWe really appreciate this comment as we ourselves are skeptical of dormant neurons as a measurement. However, we included it as it has become a popular measure in the literature [1,2,3]. Additionally, in Appendix F, we plot a histogram of activations showcasing the range of activations used by the agent presenting a better overview of the agent\u2019s activations. If you feel these results should not be included, we will remove them at your request.\n\n**Claims of the state-of-the-art on a desktop PC**\n\nWe recognise that this is a difficult claim to make; however, after surveying the literature, we believe, to the best of our knowledge, that it is true. Part of the reason we make this claim is that few algorithms achieve greater performance than BTR, and those that do, in their reported training requirements, make it clear that it would not be possible to train their agent on a desktop PC. In Table 3, we survey three top-performing algorithms, listing their claimed resources used and identifying several reasons for preventing them from being a fair comparison to a desktop PC: the required resources with numerous CPUs / GPUs, excessive training time over 5 days or more recently GPU VRAM to train a model (commercial GPU have significantly less VRAM than research-grade GPUs). Is the reviewer aware of any training algorithm that could reasonably compete with our claim? \n\n[1] Sokar, Ghada, et al. \"The dormant neuron phenomenon in deep reinforcement learning.\" International Conference on Machine Learning. PMLR, 2023.\n\n[2] Xu, Guowei, et al. \"Drm: Mastering visual reinforcement learning through dormant ratio minimization.\" arXiv preprint arXiv:2310.19668 (2023).\n\n[3] Qin, Haoyuan, et al. \"The Dormant Neuron Phenomenon in Multi-Agent Reinforcement Learning Value Factorization.\" The Thirty-eighth Annual Conference on Neural Information Processing Systems."
            }
        },
        {
            "comment": {
                "value": "Thank you for this detailed review with productive and interesting questions.\n\n**Computation and using hardware accelerated environments**\n\nWe considered using hardware-accelerated environments, which provide several advantages to researchers, particularly in parallelisation and training speed. However, it is not clear how to hardware-accelerate many real-world problems, e.g., Wii games tested in this work. As we are interested in expanding and investigating RL in a broad variety of challenging environments, we focused on classical CPU-based environments. \n\nWe also want to clarify that 12 hours is the time to train an agent for a single environment.\n\n**Different Domains**\n\nYes, as stated in Section 4.2 (lines 285 to 287), we use the same algorithm hyperparameters for all domains (Atari, Procgen and Wii games). The only thing we changed is the input resolution (140x114 from 84x84) as Dolphin Emulator\u2019s native resolution is much higher, and the number of parallel environments as we found Dolphin was too memory-expensive for more than 4 on a single 64Gb desktop. To counteract this, we took 16 steps (in all 4 environments) before performing an update, which gives us a similar effect to taking 64 steps in parallel, meaning that no additional tuning is required. \n\n**Large Shaded Areas on Plots**\n\nThe shaded areas are the standard deviation across all evaluation episodes and seeds. For example, with 100 evaluation episodes and 3 seeds, the shaded area is the standard deviation over all 300 episodes. This tends to be high when performing evaluations due to the surprisingly stochastic nature of atari games (i.e., sticky-actions, no-op starts and long episode lengths), increasing the score variance in a single episode.\n\nBTR\u2019s IQM on Atari-5 for each seed is 7.77, 8.22, and 7.86\n\n**Rainbow with Vectorization**\n\nAdding vectorization to Rainbow is certainly possible, however, it would require extensive tuning to provide a fair and unbiased comparison as BTR\u2019s parameters were tuned to use vectorization (e.g., learning rate, batch size and how often to replace the target network). We are, however, still happy to provide an untuned comparison if you believe this is important. \n\n**Comparison to \u201cSimplifying Deep Temporal Difference Learning\u201d (PQN)**\n\nWe think this is a really interesting comparison, and thank you for raising it. Sadly, PQN does not report their full results for 200M frames and importantly uses the life information parameter that alters the environment to provide more regular termination signals, which has been found to significantly improve training. This makes it an unfair comparison to previous works that do not use it. In Appendix J, we evaluate BTR with and without life information so we can provide a fair comparison. See the table below. Additionally, we note that even without life information, at 200 million, BTR still achieves a higher IQM performance at 7.42 than PQN for 400 million with 3.86 across the Atari-5 environments.\n\nAtari-5 IQM and per-game Scores. For individual games, Human-Normalized scores are reported, with the raw score in brackets. \n\n| Game           | BTR (with life info, 200M frames) | PQN (with life info, 400M frames) |\n|----------------|-----------------------------------|------------------------------------|\n| Inter-Quartile Mean | **14.02** | 3.86|\n| BattleZone | **13.53** (473,580)| 1.51 (54,791)  |\n| DoubleDunk  | **14** (23.0) | 6.03 (-0.92) |\n| NameThisGame  | **4.59** (28,710) | 3.18 (20,603) |\n| Phoenix | **89.95** (583,788) | 38.79 (252,173)|\n| Qbert | **14.54** (193,428) | 2.37 (31,716)|\n| Walltime (A100) | 22 Hours (PyTorch (non-compiled) + gymnasium async) | **2 Hours** (JAX + envpool) |\n\nBeyond performance, the largest difference is in wall time for several reasons. First, we use Gymnasium async rather than EnvPool, which is 1.6x faster [1]. Second, we do not use PyTorch compile or cudagraphs, which have both recently been shown to significantly speed up reinforcement learning code which are the equivalent of `jax.jit` [2]. Incorporating them should reduce BTR\u2019s walltime. Third, using a large replay buffer will mean that off-policy algorithms like BTR will be slower from repeatedly accessing RAM than on-policy algorithms like PQN or PPO that can be stored in GPU VRAM. On-policy and off-policy algorithms tradeoff sample efficiency and walltime, which we believe the performance impact is worthwhile for BTR. \n\n\nIt is noteworthy that some of BTR\u2019s improvements could be combined with PQN to improve performance, particularly in relation to the neural network architecture. \nWe agree that a comparison against PPO in terms of performance and walltime would be interesting, and we will add one in the appendices of our paper.\n\n[1] Weng, Jiayi, et al. \"Envpool: A highly parallel reinforcement learning environment execution engine.\" Advances in Neural Information Processing Systems 35 (2022): 22409-22421.\n\n[2] https://github.com/pytorch-labs/LeanRL"
            }
        },
        {
            "comment": {
                "value": "Thank you for your diligent and detailed response, which summarizes the paper\u2019s strengths and raises productive criticisms.\n\n**Details of Adaptive Maxpooling**\n\nThanks for spotting this. We will update the paper to clarify Adaptive Maxpooling and how it works. \n\nAdaptive Maxpooling is identical to the standard maxpooling layer, except the output shape is an argument with the kernel size and stride automatically adjusting for different input resolutions, enabling support for different resolutions with no algorithmic change or needed learnable parameters. Our code simply uses the PyTorch Adaptive Maxpooling 2D layer with a maxpool size of (6, 6) as a drop-in replacement for standard maxpooling.\n\n**Citations and comparison against other work**\n\nIn an earlier draft, we included a comparison against Schmidt and Schmied, 2021 in Figure 4, however, after some investigation found that they used a \u201clife information signal\u201d when evaluating their results. As demonstrated in Appendix J, this significantly impacts performance and is not recommended for evaluation by Machado et al., 2018 [1]. Because of this, we decided not to include these results in the main paper as we believed it would confuse readers as to the reason for the performance difference, however, we did still include a comparison in Appendix A.\n\nWith regard to your requested citations, we think these are strong relevant additions to the paper and have added them in Sections 3.1 and 3.2, respectively.\n\n**Adam\u2019s Epilson Formula**\n\nThe parameter `1.95e-5` for Adam\u2019s epsilon comes from Schmidt and Schmied, 2021. We will add a reference to this. While the value was correct, we apologize for a minor mistake in our formula, which should have been `0.005 / batch_size`, as opposed to `0.05 / batch_size`. \n\n**BTR\u2019s hyperparameters on different domains**\n\nFor new domains, e.g., the Wii games, we did not do any further hyperparameter tuning, keeping the same set across Atari, ProcGen, and Wii games. We wouldn\u2019t be surprised if further improvements could be made to individual environments with further tuning, but we aim to produce a single algorithm that works across many domains, which is in line with DQN\u2019s original motivation. In terms of the most important parameters, we found the learning rate, discount rate, and N (from N-Step TD learning) to be the most significant. \n\n**Using A High-End Desktop PC**\n\nWe agree about specifying desktop components and will specify them in the introduction. We would like to note that research servers with an Nvidia A100, a four-year-old GPU model, cost over \\\\$10,000 each, compared to an equivalent high-end desktop used in this research, which costs less than \\\\$5,000 in total. Additionally, with future consumer graphics cards, we anticipate the cost of building similar desktops will decrease, making this research more accessible.\n\n**Ethical issue with regards to using Wii Games**\n\nOn the copyright-based and emulator question for using Wii-based games, we view this as similar to the use of Atari games in RL research, where the ROMs are similarly under Atari's copyright, though they are not viewed as an ethical issue for research. Likewise, we have bought the ROMs for the respective games used and are not distributing the ROMs with the research, so we do not believe this is an issue. If the reviewer is unsatisfied with this response, could they clarify in what way they believe there is an ethical issue here that does not exist for RL research using the Atari games? Furthermore, we have discussed using the Dolphin Emulator with the developers, who have cleared us for use in research.\n\n[1] Machado, Marlos C., et al. \"Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents.\" Journal of Artificial Intelligence Research 61 (2018): 523-562."
            }
        },
        {},
        {
            "comment": {
                "value": "Thank you for your insightful review, which succinctly outlined our paper\u2019s contributions, results, and analysis.\n\nIn answer to where the novelty in this work comes from, we identify three sources:\n  - The process of choosing, integrating, and testing components together is a time-consuming process that requires diligence. In addition to the six improvements included in BTR, several more promising extensions were considered and investigated, though dismissed (see Appendix I for more detail). In publishing the paper, we hope this will prevent future researchers from recomputing the same tests conducted in this paper. While integrating these components, we found it necessary to re-tune hyperparameters, especially when using vectorization with components designed to be used with a single actor (e.g., batch size, learning rate, and frequency of updating the target network).\n- We analyze the components across several measures beyond performance, demonstrating their different and sometimes competing effects on action gaps, policy churn, and observational noise. Such analyses have been completed previously, though not across as many measures or as varied improvements.\n- We demonstrate BTR beyond the standard testing suite to Wii games, showcasing BTR\u2019s continued strength as an algorithm outside of standard research testing environments. This includes games with far more graphically intensive and challenging domains than what any other algorithm (with similar compute resources) has been shown to handle.\n\nOn the challenge of integrating these components, we found it technically complex to simultaneously support Impala, Spectral Normalization, Maxpooling, Dueling Networks, Noisy Networks, and IQN, which has never before been attempted. An additional complexity was implementing a replay buffer that supports vectorization, N-Step TD Learning and Prioritization in a memory-efficient way. This approach stores each frame individually rather than all four frames together, reducing memory overhead from 28GB to 7GB in total for a million Atari observations. To help future researchers and hobbyists, we open-source our code.\n\nWe are happy to further elaborate on any of these at your request."
            }
        },
        {
            "comment": {
                "value": "We thank all the reviewers for their responses and the time and effort spent on the reviews, recognising the value of the paper\u2019s motivations and empirical results. Furthermore, we appreciate the insights and constructive criticism that we will use to continue improving the paper. \n\nTowards the end of the discussion period, we will provide an updated PDF of the paper, along with a summary of the changes made."
            }
        },
        {
            "summary": {
                "value": "This paper introduces Beyond The Rainbow (BTR), a novel reinforcement learning (RL) algorithm that enhances Rainbow DQN by integrating six key improvements. The BTR algorithm is computationally efficient, capable of training powerful agents on a standard desktop computer within a short time. Experimental results show that BTR outperforms state-of-the-art RL algorithms on both the Atari-60 and Procgen benchmarks. Additionally, BTR can handle training agents for challenging levels in complex, modern games. Finally, this paper includes a comprehensive ablation study to analyze the performance and impact of each component within the BTR algorithm."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper is well-written and well-organized. The ideas are clear and could be easily understood\n2. The experiments are comprehensive and the results are strong. As shown in Section 4, the proposed BTR algorithm could greatly outperform state-of-the-art baselines in two classic benchmarks and handle three hard and complex modern games with a desktop PC.\n3. The paper includes extensive ablation studies and experimental data. Section 5 presents a detailed analysis of the performance and impact of each component of the BTR algorithm, providing readers with insights into the sources of the algorithm's performance gains. Additionally, the authors include complete experimental results and settings in the appendix, helping to clarify any potential confusion or misunderstanding for readers."
            },
            "weaknesses": {
                "value": "1. The BTR integrates six improvements from existing RL literature to Rainbow DQN. While the algorithm demonstrates strong performance, its novelty might appear limited.  Could you further clarify the novelty of this work? Or specifically, could you briefly discuss if there is any challenges in integrating these existing improvements into the BTR algorithm?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a variant of Rainbow that adds further architectural and algorithmic improvements to improve not only the agent's score but also to increase its training speed to around 3x what has been previously reported, while running it on top-notch consumer hardware. Finally the authors also show that their improved version of rainbow can deal with modern games with complex graphics and physics."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The presentation is overall clear, the methodology is sound, and the results are compelling. Both extensive use of ablations, and the connection to other important metrics related to pathologies in Deep RL algorithms are an example that more papers should follow.\nThe appendices are also data rich, showing ablations' performances on each of ALE's 60 games, and even having one appendix about things that were tried but did not lead to improvements in performance, which may help others not repeat the experiments."
            },
            "weaknesses": {
                "value": "1. Adaptive Maxpooling is never defined. It's not a common layer in reinforcement learning and it's never defined in the paper, in fact skimming (Schmidt and Schmied, 2021) that layer is also not defined, I believe this is the only seious weakness in the paper's presentation, but still I believe it is a serious weakness (though hopefully the authors can fix it and so I can increase their grade).\n2. There are at least 2 relevant citations missing, \"Spectral Normalisation for Deep Reinforcement Learning: An Optimisation Perspective\" when talking about Spectral Normalisation, and \"On the consistency of hyper-parameter selection in value-based deep reinforcement learning\" when talking about the need for tuning Deep RL hyperparameters and the benefits of using layer norm between dense layers.\n3. I believe it's slightly misleading to not specify \"a high-end PC\" when talking about the kind of machine that can run the algorithm in 12 hours (4090 RTXs are quite expensive, and i9s are Intel's high-end consumer line)\n4. I believe a more direct comparison with Schmidt and Schmied, 2021 is warranted, given its foundational importance to the paper.\n5. Using only 3 seeds while having a large increase in the number of tuned hyperparameters weakens the validity of the results as explained in \"Empirical Design in Reinforcement Learning\", though at the same time the analysis of metrics beyond simply the score and the extensive use of ablations help."
            },
            "questions": {
                "value": "1. What exactly is adaptive maxpooling? Would it be possible to add a description of it with either an equation, pseudo-code, or diagram?\n2. Where did the formula 0.05/batch_size for Adam's epsilon come from?\n3. The final algorithm has a considerable number of hyperparameters, would it be possible to discuss a bit which ones are the most important to tune should someone try to apply this algorithm to a new domain?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I am just slightly worried about the use of somewhat modern Nintendo games as RL environments through the use of emulators, is the use of emulators for research legal?"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper combines several different RL improvements to a single algorithm, with a focus on high performance with reasonable computational requirements. In doing so, they find that their approach achieves a new SoTA (not including recurrent methods), while being able to be run on a desktop machine in under a day.\n\nThey analyse the factors that led to this performance in detail through several ablations.\n\nOverall, this paper makes rainbow/dqn-type methods more accessible to non-industry labs"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper gives RL researchers a way to do pretty well in atari without expending too significant computational resources.\n- They perform ablations on their individual changes to identify what helps and what has the most effect on performance/walltime. This is quite useful.\n\nI am not giving this a lower score because I think making RL more accessible is worthwhile, and this paper takes a step towards this, and further analyses many of these independent components to see what their effect is. I am not giving a higher score because I think the paper's significance does not warrant it."
            },
            "weaknesses": {
                "value": "- To me it is unclear if 12 hours is for all games or just 1.\n- I wonder how this fits in with the recent trend of hardware-accelerated RL (see e.g., Stoix/PureJaxRL/Gymnax and all of the hardware-accelerated environments). Does that line of work better achieve the goal of making RL more accessible? In that setting, the environment is often run entirely on the GPU, leading to hundreds or thousands of times speedups."
            },
            "questions": {
                "value": "- The 12 hour number/other timings, is that the total time it takes to train BTR on a single game or on all 57 games?\n- It seems like you made quite a few hyperparameter choices (e.g. how often to update, etc.) Do you use the same values for each domain?\n- What is the shaded area in the plots? If it is standard deviation it seems like the proposed BTR algorithm is very inconsistent across seeds. Could you elaborate please/maybe provide results for individual seeds?\n- Figure 3 does show that you can apply your approach to other games, which is great. I would really like to see some point of comparison, however, to act as a reference point. For instance, run vanilla PPO or DQN or Rainbow as a baseline.\n- Why is fig 4 using raw score as the y-axis, as opposed to e.g. normalised?\n- Figure 4 is somewhat hard to follow as there are so many lines and it seems like most of them overlap quite a lot.\n- Is it feasible to run rainbow with vectorisation? This is not that crucial, it just seems like something obvious to run given figure 5, where vectorisation is the main speedup factor.\n- Table 2: Would be nice to have another method, e.g. rainbow or DQN to act as a reference point.\n- One recent work that seems to have a similar purpose is \"Simplifying Deep Temporal Difference Learning\" (https://arxiv.org/pdf/2407.04811). It seems like they use vectorisation as well to achieve large speedups. More importantly, however, is that they primarily use JAX---which is becoming increasingly common in RL, and is reducing computational load significantly/making RL more accessible to compute-poor labs/institutions. Could you please comment on a few things\n\t- How does this paper's score compare to yours?\n\t- How does the walltime compare to yours?\n\t- What do you see as the benefits/disadvantages of this hardware accelerated paradigm compared to the more classic approach you are taking?\n- I know it is not usual in these types of papers but I would really appreciate a PPO comparison, both in terms of walltime and performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors present Beyond the Rainbow (BTR), an algorithm combining advances in Q-learning based approaches to Atari developed since Rainbow. \n\nThe authors train their agent on Atari, Procgen and 3 games which aren't well-established benchmarks in RL (Super Mario Galaxy, Mario Kart and Mortal Kombat). They run ablations on their method and demonstrate that the Impala architecture contributes the most to their method's performance. They also demonstrate that vectorization of the environment is key to the faster runtime of their algorithm."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper has a number of positive points:\n- The core idea of trying to achieve strong performance using Q-learning on a desktop PC has significant merit and would constitute an interesting contribution\n- The introduction of new games to evaluate on is interesting and the games chosen would make good potential benchmarks.\n- The paper is easy to follow and clearly written"
            },
            "weaknesses": {
                "value": "I slightly feel for the authors of this paper. I would like to be able to commend the paper on its empirical results, or the performance on the new environments, but the results are not presented scientifically enough for me to do that, and so I can't recommend acceptance at this venue.\n\nTo make my objections more concrete:\n- In Figure 2, the authors claim that their method outperforms the red baseline, but this is plainly not the case from the plot. The error bars so significantly overlap the red line there is no way this result is significant. \n- The authors do not aggregate their results in accordance with recommended practice [1]. Although they use the inter-quartile mean, they do not provide bootstrapped confidence intervals to estimate errors and do not seem to provide errors in their baseline results. This issue appears in Figures 1 and 2. As far as I know, the authors do not state what the error bars in Figures 1 and 2. If the plotted error bars are standard \n- While the evaluation of their method on new games is nice, I can't take any information away from this without even a semblance of a baseline. Training an RL policy on Wii games has no intrinsic scientific value -- it is only by contextualisation of a baseline that this would be a compelling result. Similarly, the authors provide no error bars in this domain.\n- Figure 4 again because of the way the results were processed provides almost no information. Atari-5 [2] provides a way to estimate the median given performance on those 5 games. But this is only after the application of a regression procedure. Without the application of this summary metric, it is just not clear what to take away from these results. This figure does not even present human normalised scores, as is standard. This Figure should therefore be replaced by a plot of the regressed median for Atari-5 with bootstrapped confidence intervals. The authors can use rliable [1] for this.\n- Again, the analysis in Section 5.2 *should* be compelling and interesting reading, but it's just not done thoroughly enough. Figure 6 is presented without error bars and so are the results in Table 2 and the IQM in Table 3. It's just not possible to believe the authors' conclusions on their work without any estimates of error. \n- Additionally, the authors use dormancy [3], but set a threshold of 0.1. Although resetting dormant neurons was shown to improve performance, neurons with a small activation are not in themselves a problem! A neuron followed by a ReLU activation that always outputs 0 is not learning, which clearly constitutes a problem, but a neuron that outputs a small value is still perfectly plastic. The dormancy results therefore also aren't a proxy for any form of plasticity. \n- The authors make multiple claims about their method being \"state-of-the-art for a desktop PC\" (or similar). These should be removed from the paper as they are just impossible to verify. Even as an expert, I do not know the hardware that every paper ran experiments on and whether it would be possible to run it on a desktop PC, and it is not a claim that can be clearly backed-up. I note that the authors did not do all their experimentation on a desktop PC, but only claim that their method can run on one effectively."
            },
            "questions": {
                "value": "See weaknesses.\n\n\nOverall, this work is just not good enough in its current format. I recommend that the authors fix the presentation of the results, especially adding error bars and effective aggregation using a tool like rliable. Given the significant problems with every figure and table in the main body of the paper, this work is not good enough for this venue in its current form and would require wholesale changes to fix that.\n\n[1] Deep Reinforcement Learning at the Edge of the Statistical Precipice. Agarwal et al. Neurips 2021.\n\n[2] Aitchison, Matthew, Penny Sweetser, and Marcus Hutter. \"Atari-5: Distilling the arcade learning environment down to five games.\" International Conference on Machine Learning. PMLR, 2023.\n\n[3] Sokar, Ghada, et al. \"The dormant neuron phenomenon in deep reinforcement learning.\" International Conference on Machine Learning. PMLR, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}