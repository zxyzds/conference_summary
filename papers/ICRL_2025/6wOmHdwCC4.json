{
    "id": "6wOmHdwCC4",
    "title": "Divergence-enhanced Knowledge-guided Context Optimization for Visual-Language Prompt Tuning",
    "abstract": "Prompt tuning vision-language models like CLIP has shown great potential in learning transferable representations for various downstream tasks. The main issue is how to mitigate the over-fitting problem on downstream tasks with limited training samples. While knowledge-guided context optimization (Yao et al.,2023; 2024) has been proposed by constructing consistency constraints to handle catastrophic forgetting in the pre-trained backbone, it also introduces a potential bias toward pre-training. This paper proposes a novel and simple Divergence-enhanced Knowledge-guided Prompt Tuning (DeKg) method to address this issue. The key insight is that the bias toward pre-training can be alleviated by encouraging the independence between the learnable and the crafted prompt. Specifically, DeKg employs the Hilbert-Schmidt Independence Criterion (HSIC) to regularize the learnable prompts, thereby reducing their dependence on prior general knowledge, and enabling divergence induced by target knowledge. Comprehensive evaluations demonstrate that DeKg serves as a plug-and-play module can seamlessly integrate with existing knowledge-guided methods and achieves superior performance in three challenging benchmarks.",
    "keywords": [
        "visual-language prompt tuning;few-shot learning;zero-shot learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6wOmHdwCC4",
    "pdf_link": "https://openreview.net/pdf?id=6wOmHdwCC4",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a simple yet effective knowledge-based prompt tuning method that leverages the Hilbert-Schmidt Independence Criterion (HSIC) to regularize learnable prompts. By reducing the reliance on prior general knowledge, this approach enables the prompts to better align with task-specific knowledge. The method is versatile and can be easily integrated into other frameworks. When applied to the TCP method, it demonstrates superior performance across most datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper is well-organized and easy to follow. Figure 2 effectively illustrates the main idea by clarifying the roles of each loss function: the \\( L_{CE} \\) loss enforces alignment between text and vision embeddings, the \\( L_{kg} \\) loss encourages the learnable prompts to align closely with the CLIP textual embeddings, and the core \\( L_{HSIC} \\) loss ensures independence within the learnable prompt embeddings.\n\n2. The experiments are comprehensive, covering base-to-new generalization, cross-dataset generalization, and few-shot classification. The proposed DeKgTCP method achieves superior results across most datasets."
            },
            "weaknesses": {
                "value": "1. Why was the proposed method applied to KgCoOp and TCP rather than other state-of-the-art methods, such as PromptSRC, which performs even better than KgCoOp? Is it more challenging to integrate with PromptSRC, or are the results less effective? Providing additional clarification on this choice would enhance the paper.\n\n2. Figure 4 provides an insight into how the proposed method balances dependence and independence; however, the paper lacks further analysis on this. Expanding on this point would strengthen the reader\u2019s understanding of the method's underlying mechanics."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tackles the inherent issue of knowledge-guided context optimization, which overly biases general knowledge in pre-training. It proposes a novel HISC-based regularization method, DeKg, for encouraging independence between the learnable and the crafted prompts. Extensive experiments demonstrate the superiority of the proposed method in three challenging benchmarks:"
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+Using the Hilbert-Schmidt Independence Criterion (HSIC) is an interesting topic for encouraging independence between learnable and crafted prompts, which can boost performance in the seen classes.\n\n+Evaluation shows the effectiveness of the proposed method.\n\n+The proposed DeKg integrates seamlessly with existing knowledge-guided methods."
            },
            "weaknesses": {
                "value": "-As shown in Figure 1, the proposed DeKg obtains a higher performance than the performance of CoOp for base classes and the zero-shot CLIP for new classes. However, the Hilbert-Schmidt Independence Criterion (HSIC) contained in DeKg is a constraint between the learnable and crafted prompts without injecting additional information. Why can the proposed DeKg obtain a better performance?\n\n-L221: The proposed L_{kd} involves two terms: intra-class relations and inter-class relations. Moreover, the author claims that  penalizing\nL_{kd} encourages both intra-class and inter-class independence. Furthermore, the intra-class consistency is formulated between w_i and w_{i}^{clip}, which is the same as the L_{kg}. In other words, the proposed HSIC has contained the knowledge consistency L_{kg}. Therefore, the final objective of Eq.(5) should not contain L_{kg} because L_{kd} has been constrained by the intra-class consistency. However, the results in Table 4 are inconclusive with the above conclusion. Even more unfortunate, L_{kd} performs worse than L_{kg}. Why?\n\n-It is recommended to provide a code.\n\n-Since the proposed HSIC is model-independent, it is suggested that the module's generalization and plug-and-play be verified using more CoOp-based methods."
            },
            "questions": {
                "value": "Please see #Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel method called Divergence-enhanced Knowledge-guided Prompt Tuning (DeKg), which employs Hilbert-Schmidt Independence Criterion (HSIC) regularization to maintain a degree of independence between the learnable prompts and pre-trained knowledge, addressing the bias problem caused by over-reliance on pre-trained knowledge. Built upon knowledge-guided context optimization, DeKg introduces an independence constraint, enabling learnable prompts to retain consistency with general knowledge while capturing task-specific features, thus achieving a better balance between base and novel classes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper addresses the inherent bias issue in knowledge-guided context optimization by introducing a novel Hilbert-Schmidt Independence Criterion (HSIC)-based regularization that encourages independence between learnable and crafted prompts. \n2. DeKg integrates with existing methods, enhancing class-specific prompt distinction without increasing model complexity."
            },
            "weaknesses": {
                "value": "1. The motivation of using HSIC as the constrain is not clearly elaborated. Further analysis of your motivation will be insightful. \n2. One of the proposed loss: $L_{kg}$ is already applied in existing methods, such as KgCoOp and PromptSRC, which weakens the novelty of overall method."
            },
            "questions": {
                "value": "1. As mentioned in weakness1, more analysis of your motivation concerning why you choose Hilbert-Schmidt Independence Criterion would be insightful.\n2. The experiment of domain generalization seems to be missed in your paper, which is conducted by most prompt tuning method. Could you provide Dekg\u2019s performance on this setting.\n3. In Tabel 4, it is obvious that the $L_{kg}$ works well in novel, while $L_{kd}$ performs well in base. Dose that mean these two losses are strongly coupled and your proposed $L_{kd}$ is not recommended to use independently? More analysis on above question would be insightful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to adapt HSIC as an extra regularization term, which achieves a better trade-off between the performance on base and new classes. The experiments show the effectiveness of this regularization on varies experiment settings. The paper is well written and easy to understand."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. I think this paper has a reasonable motivation to maximize the independence between learnable prompt and manual prompt. \n2. This paper has a very extensive experiment analysis on varies clip model adaptation task and show good results."
            },
            "weaknesses": {
                "value": "1. More analysis is needed to discuss why HISC is chosen as the metrics to measure the prompt independence. Other methods like information bottleneck can do that too. \n2. L_kd and L_kg seems to be a pair of totally contradictive losses. I wonder if this will cause the model to be difficult to converge. It would be better to provide more analysis on how the loss weight of these two losses affect the model convergence. \n3. More performance comparison and analysis on other state-of-the-art prompt tuning method like:\nYubin, et.al, Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models"
            },
            "questions": {
                "value": "Please refer to the concern in the Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}