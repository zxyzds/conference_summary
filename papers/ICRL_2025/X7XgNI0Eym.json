{
    "id": "X7XgNI0Eym",
    "title": "MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis",
    "abstract": "Recent works in volume rendering, \\textit{e.g.} NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. \nRendering on top of an explicit representation, the vanilla 3DGS and its variants deliver real-time efficiency by optimizing the parametric model with single-view supervision per iteration during training which is adopted from NeRF. Consequently, certain views are overfitted, leading to unsatisfying appearance in novel-view synthesis and imprecise 3D geometries.\nTo solve aforementioned problems, we propose a new 3DGS optimization method embodying four key novel contributions:\n1) We transform the conventional single-view training paradigm into a multi-view training strategy. With our proposed multi-view regulation, 3D Gaussian attributes are further optimized without overfitting certain training views. As a general solution, we improve the overall accuracy in a variety of scenarios and different Gaussian variants. \n2) Inspired by the benefit introduced by additional views, we further propose a cross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure concerning different resolutions.\n3) Built on top of our multi-view regulated training, we further propose a cross-ray densification strategy, densifying more Gaussian kernels in the ray-intersect regions from a selection of views. \n4) By further investigating the densification strategy, we found that the effect of densification should be enhanced when certain views are distinct dramatically.\nAs a solution, we propose a novel multi-view augmented densification strategy, where 3D Gaussians are encouraged to get densified to a sufficient number accordingly, resulting in improved reconstruction accuracy.\nWe conduct extensive experiments to demonstrate that our proposed method is capable of improving novel view synthesis of the Gaussian-based explicit representation methods about 1 dB PSNR for various tasks. \\href{https://mvgs666.github.io/}{\\textcolor{magenta}{Codes are available.}}",
    "keywords": [
        "3D Reconstruction; Gaussian Splatting; Neural Radiance Feild"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=X7XgNI0Eym",
    "pdf_link": "https://openreview.net/pdf?id=X7XgNI0Eym",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a series of improvements to the 3D Gaussian Splatting (3DGS) framework, aiming to address challenges in rendering quality and overfitting by incorporating multi-view regulation techniques. The motivation is reasonable and the rendering quality is impressive."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+: Thanks for providing the code. The results are reproducible and good. \n+: The motivation makes sense since an explicit and discrete primitive like gaussian tends to overfit and get stuck in local minima easily. Multi-view constraints would be beneficial to solve these.\n+: The method is thoroughly evaluated in various settings."
            },
            "weaknesses": {
                "value": "-: The overhead has increased significantly. Although the total number of iterations remains at 30k, each iteration now involves multiple forward passes, causing the overall training time to multiply. Specifically, training a single scene now takes 2\u20133 hours, compared to 3DGS's 20\u201330 minutes. This raises concerns about the method\u2019s practicality, as one of Gaussian Splatting\u2019s key advantages is its efficiency in both training and rendering. Additionally, the output PLY file size is several times larger than other methods, reaching over 1GB for many scenes, which may further hinder its usability.\n\nI suggest the author consider two further improvements:\n\n(1 )Attempt to reduce the final count of Gaussians to demonstrate that the improvements come from better Gaussian placement rather than excessive densification.\n(2) Test the effect of using the AVG instead of SUM for multi-view loss aggregation."
            },
            "questions": {
                "value": "Please refer to weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed four main modules to improve the 3DGS model, namely, the multi-view regulated training, cross-intrinsic guidance, cross-ray densification and multi-view augmented densification. This paper tried to impose the constraints from multiple views and multiple resolutions simultaneously, densify more points in the central area and densify more points for distinct views."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper has conducted experiments with different baselines, different benchmarks and different tasks.\n2. This paper investigated the impact of multi-view constraints and proposed many strategies to mitigate the overfitting problem in 3DGS in terms of loss function, training strategy and densification strategy."
            },
            "weaknesses": {
                "value": "1. I still don't understand the relationship of these four main contributions well, such as , you were inspired by the multi-view constraints and proposed the cross-intrinsic guidance, so what's the relationship of these two contributions? Do they have to work together to see a performance gain?\n2. I am still a little confused about the effectiveness of the multi-view regulated training. Considering from the perspective of gradient backpropagation, this multi-view regulated training is similar to the way that losses from multiple iterations are accumulated into one iteration. Are there any more designs, such as these multiple views?\n3. How does the training efficiency compare to existing methods? Perhaps using two samples to show the ablation results is not convincing enough, and it would be better to put Table 5 in the main paper."
            },
            "questions": {
                "value": "see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces an approach for novel view synthesis using 3D Gaussian Splatting (3DGS) with multi-view regularization, achieving an approximate 1 PSNR improvement over existing methods on various datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Advantages\nNovelty: The paper proposes three key strategies to enhance the rendering quality of 3DGS:\nA cross-intrinsic guidance scheme that employs a coarse-to-fine training procedure.\nA cross-ray densification strategy that densifies Gaussian kernels in regions where rays intersect, improving details in specific views.\nA multi-view augmented densification strategy to further optimize Gaussian density based on view discrepancies."
            },
            "weaknesses": {
                "value": "Drawbacks\nWhile the figures are clear, the writing quality could be improved for readability.\nLines 93\u201396 states: \u201cWe first propose a multi-view regulated training strategy that can be easily adapted to existing single-view supervised 3DGS frameworks and their variants, optimized for a large variety of tasks, where NVS and geometric precision can be consistently improved.\u201d However, there are no experiments specifically demonstrating improvements in geometric precision.\nThe overall pipeline is more complex and larger than standard 3DGS. It is unclear if this increase is due to the larger network or the proposed modules that enhance 3DGS."
            },
            "questions": {
                "value": "Among the proposed cross-intrinsic guidance, cross-ray densification, and multi-view augmented densification strategies, which component significantly improves the quality? Have ablation studies been conducted to isolate and measure each component\u2019s impact?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work propose to use more views in each training iteration of 3DGS. The experiments suggest that this simple startegy consistently leads to better quality. In addition, a new Gaussians densification method is proposed which densifies more on region with larger multi-view losses. A multi-resolution training stategy is also proposed to use more views with lower resolution at the beginning of training. These additional training strategies can further improve the quality."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The core message of this paper is clear: using more training views in each iteration can improve quality. The effectiveness of this is evaluated by extensive experiments on various 3DGS variant and various tasks."
            },
            "weaknesses": {
                "value": "- The paper writing need many improvement. There are many technical details sections are hard to understand. See Questions section for details.\n- The training time and memory consumption may increase significantly by using more views in each iteration. The tradeoff on this aspect is not discussed. The results tables should disclose the additional training cost comparing to the other.\n- When comparing to the other method with more training iterations, is all the scheduler hyperparameters scaled accordingly? For instances, `position_lr_max_steps` for lr annealing, `densification_interval, opacity_reset_interval, densify_until_iter` for adaptive Gaussians. When training with the baseline of 8x longer schedule, are the learning rate downscale by 1/8 as well?"
            },
            "questions": {
                "value": "Sec.3.1:\n- How the multi-view are sampled in each iteration? Is it just uniform sample from the training set?\n\nSec.3.2:\n- L249: is confusion.\n    - Which of the $k$-th layer is set to 8 when saying \"the set $s_k$ as 8\"? I can guess it means {1, 2, 4, 8}. Then the correct writing should be S = {s^{k-1} | k=1...4} and s=2.\n    - Seems that $c_k', f_k'$ are the scaled principle point and focal length and $c_k, f_k$ are the source one. Then we do not need the layer index for the source principle point and focal length as there are all the same.\n- In Figure 2, it seems that different resolution of the images are employed in each training iteration. However, from the main text, it turn out to be that a single resolution is used in each iteration.\n- What is the implementation details for the schedule of multi-resolution training? How many iterations are trained in each of the image scale? How is the multi-resolution training and the original training schedule couple together? Do we start to do densification, pruning, and sh degree increment in the coarser resolution training?\n\nSec.3.3:\n- What is the sliding window size in Sec.3.3 for finding the patches with high loss?\n- Four rays are casted from the high-loss patches. How the rays from different images can intersect? As rays are infinite thin line in 3D, two rays from different cameras may can hardly intersect. From figure 2, I guess the image patch frustum is casted instead of just rays. Then the following question is why the intersection of the frustrum form cuboid? Isn't it the intersection form 3D polygon?\n- The Gaussians in the high-loss region is densified \"to a certain amount\". How are these Gaussians actually density? Each duplicated by two?\n\nSec.3.4:\nI have read several times but it still hard for me to understand this section.\nIn original 3DGS, the viewspace gradient is accumulated for hundreds of iterations and at some interval, the Gaussians with gradient above the threshold $\\beta$ are densified.\nIs it that the only difference in this work is that the threshold is adaptive choiced between $\\beta$ and $0.5\\beta$ based on camera positions?\n$\\hat{\\beta}$ seems to be a global threshold as in the original 3DGS but from Eq.4, $\\hat{\\beta}$ is depend on the distance of a pair of cameras. For each Gaussian, do we accumulate the viewspace gradient for each pairs of the cameras instead of just a global viewspace gradient now?\n\n\nIn L513: It is still unclear to me why the performance degrade when there are too much training views in each iteration. How much is too much? Why using too much views is analogous to using a region of views?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}