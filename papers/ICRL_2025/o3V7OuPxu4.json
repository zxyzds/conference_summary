{
    "id": "o3V7OuPxu4",
    "title": "StarCraft II Arena: Evaluating LLMs in Strategic Planning, Real-Time Decision Making, and Adaptability",
    "abstract": "StarCraft II plays an important role in developing AI agents for real-time strategic reasoning due to its complex nature. However, people usually draw conclusions of how competent their agents are according to the level of the built-in agents in StarCraft II which they can win in terms of the final success rate. Little intermediate quantitative information is considered while human-in-the-loop analysis is time inefficient, which results in inadequate reflection of the true strategic reasoning ability. In this work, we propose StarCraft II Arena, a well-designed benchmark for evaluating the strategic planning, real-time decision-making, and adaptability capabilities of large language models (LLMs) agents. We introduce using fine-grained capability metrics, allowing for targeted capture and analysis of specific capability, and further propose a detailed decision trace to enhance the understanding of LLM behavior. We demonstrate the utility of such a benchmark by evaluating several state-of-the-art LLMs in various setups. Our results reveal distinct performances in long-term strategy development, real-time decision-making, and adapting to environmental changes. Such results show that the StarCraft II Arena offers a deeper insight into the decision-making process of LLMs and has the potential to become a challenging and comprehensive benchmark for strategic reasoning.",
    "keywords": [
        "benchmark evaluation",
        "large language model",
        "LLM-based agent",
        "strategic reasoning",
        "real-time decision-making."
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "A benchmark for evaluating large language models in StarCraft II, focusing on strategic planning, real-time decision-making, and adaptability using fine-grained capability metrics and decision trace analysis.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=o3V7OuPxu4",
    "pdf_link": "https://openreview.net/pdf?id=o3V7OuPxu4",
    "comments": [
        {
            "summary": {
                "value": "The paper presents StarCraft II as a benchmark for evaluating reasoning capabilities of LLMs. The paper also presents a set of metrics for evaluating these models, which are based on the domain itself (e.g., amount of resources the play collects). Finally, the paper presents results of several LLMs on this benchmark."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The idea of having a challenging benchmark for reasoning with LLMs is interesting and appealing. I also think that computer games can be a good benchmark for this type of evaluation. The choice of a real-time strategy game is particularly good, given the response latency of these systems."
            },
            "weaknesses": {
                "value": "My main concern with the paper is that it is only half-baked in the sense that the presentation should be improved substantially before being accepted for publication.\n\nMy concerns with presentation range from the low level to the high level.\n\n**Low-level concerns**\n\n- The paper has a dangling sentence in line 241. It seems that the explanation of Table 2 was inadvertently commented out from the paper.\n- It is not clear what Equation 1 is trying to convey. What is its role in the paper? What is the parameter $\\tau$ that is passed as a parameter but not used in the equation?\n- The paragraph starting in line 137 discusses the need for communication between the agents, but this is not discussed in the paper. I can see SCII having this need if agents play in a team, but this doesn't seem to be the setting evaluated in the paper.\n- In line 106, the paper states that early RL algorithms learn through trial and error. Isn\u2019t this the case for current RL algorithms too?\n- Some of the tables are not referred to in the text. For example, I am not sure when I should read Table 3, and I don\u2019t think the paper discusses it at all.\n\n**High-level concerns**\n\nOne of the key points I was looking for in the paper was the input-output system for the benchmark. Is the LLM receiving images or text as input? How is the input defined? How much time do they have to reason? What happens when they timeout? The text mentions that smaller models perform better than larger models in the micromanagement part of the game. Is this simply because they are able to respond quicker? These are some of the key questions the authors would need to address in the paper. This is what the readers will be after when understanding whether SCII is a suitable benchmark for LLM evaluation.\n\nOverall, the paper is only half-baked, as the authors need to fix all these presentation issues and adjust the discussion of their results and benchmark. There is clearly a system running, as the paper also includes tables of results. However, it is not possible to understand how this system works as the description is missing."
            },
            "questions": {
                "value": "Please see the questions in the \"high-level concerns\" section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents StarCraft II Arena as a benchmark for evaluating LLMs in strategic planning and decision-making capabilities. While it introduces fine-grained metrics and decision tracking mechanisms, the paper needs substantial clarification on its novel contributions and experimental methodology."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Well-structured evaluation framework with fine-grained metrics\n2. Comprehensive testing across multiple LLM models\n3. Interesting decision tracking system for behavior analysis"
            },
            "weaknesses": {
                "value": "1. Insufficient experimental details:\n- Build-in AI difficulty level not specified\n- Race and map selection criteria not documented\n- Limited experimental scope (10 games per model)\n2. Theoretical foundation needs strengthening:\n- Limited discussion of why LLMs are suitable for StarCraft II\n- Insufficient comparison between LLM agents and traditional RL agents\n- Unclear theoretical justification for chosen metrics\n3. Literature positioning:\n- Need more thorough comparison with existing StarCraft II benchmarks\n- Better contextualization of contributions needed\n- Clearer differentiation from existing approaches required"
            },
            "questions": {
                "value": "1. Can you elaborate on the key differences between this work and existing StarCraft II benchmarks, particularly \"Large Language Models Play StarCraft II\"? What are the novel contributions that advance the field?\n2. The paper would benefit from a deeper discussion of why LLMs are suitable for StarCraft II:\n- What unique capabilities do LLMs bring compared to traditional RL approaches?\n- How does the decision-making process differ between LLM agents and RL agents?\n- What insights have you gained about LLM capabilities through this work?\n3. Experimental details requiring clarification:\n- What was the difficulty level of the built-in AI?\n- How were races and maps selected?\n- Why was 10 games chosen as the sample size? Why not do more experiments and test open-soruce model\n- What statistical analyses support the conclusions?\n4. Theoretical foundation:\n- How do the proposed metrics relate to fundamental LLM capabilities?\n- What theoretical guarantees or limitations exist for the evaluation framework?\n- How generalizable is this benchmark to other strategic decision-making tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a benchmark called Starcraft II Arena, which aims to evaluate the decision-making, planning, and adaptability of large language models (LLMs) within a strategic gaming environment. While traditional benchmarks for Starcraft II assess agents based on a single overall metric\u2014win rates against built-in opponents\u2014the authors argue that a more detailed analysis of LLM performance is necessary. The main contribution of this paper is a methodology designed for an in-depth evaluation of the capabilities of LLM agents."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The key idea of this paper, introducing a benchmark for a more in-depth multi-agent evaluation of LLMs, is significant and useful."
            },
            "weaknesses": {
                "value": "Overall, the paper lacks clarity and depth in describing both the technical implementation and practical contributions.\n\n### **Major comments**\n\n1. Unclear contribution: The paper does not effectively justify why this benchmark must exist as a standalone contribution rather than an addition to existing Starcraft II resources. The contribution seems limited to a collection of scripts and metrics, which could likely be integrated into the existing environment without creating a separate benchmark.\n\n2. Lack of implementation details: Key technical aspects of the implementation are insufficiently described, making it hard to understand the benchmark's novelty and how it's technically realized. Several things are not clear, such as:\n   - Integration: How are LLM agents integrated with StarCraft II? How can users use the benchmark? Does the benchmark use a custom API or an interface for this?\n   - Decision Tracking: How is decision-making tracked and analyzed? While Table 3 provides a decision trajectory, details of how this is analyzed and used are missing.\n   - Computational Requirements: What hardware/software is necessary to run this benchmark effectively? This information is critical for usability but is absent.\n   - Opponents: Are the LLMs evaluated with built-in agents or newly introduced opponents? The fact that agents are evaluated against built-in agents in Starcraft II is mentioned as a limitation, but it is unclear whether the authors change this in their benchmark.\n\n3. Incomplete metric information: The metrics lack context. For instance, while Appendix A.1 outlines the metrics, there are no defined ranges, leaving the reader unsure of how to interpret scores. For example, how should a Real-Time Decision score of 21.12 versus 37.51 in Table 4 be interpreted? Similarly, terms such as \u201ceffective\u201d actions in EPM or \u201ccollected vespene\u201d are not unexplained, reducing the metrics\u2019 interpretability (how do we know that these are the right metrics to assess decision-making and planning?).\n \n4. Missing benchmark discussion and limitations: A discussion about future development and limitations of the benchmark is missing, which limits the reader's understanding of the benchmark's intended scope and future extensions.\n\n5. Figure 2 indicates a large variance. Why are there no error bars in the tables?\n6. It's important to have the prompt included in the appendix or supplement. Was it possibly in a supplement that I cannot access?\n\n### **Minor comments** (These did not affect my score)\n- Abstract: Lines 016-019 are a bit difficult to understand; consider rephrasing\n- Figure 2: It\u2019s unclear what this Figure is meant to convey, and the Figure lacks labeled y-axes.\n- In Section 4.3, line 367 states \"Definitions and methods for these metrics will be further detailed in the figure 4.3.\" This seems to refer to a table, possibly Table 3, rather than a figure. \n- In Table 3, \"OBSERVERtgreater\" should probably be \"OBSERVER.\"\n- Lines 323 + 350 state that screenshots illustrating decision traces will be provided in the appendix, but these are not included\n- I don't understand what is meant when the authors state that civilization and the other games are not \"strategic and tactical\" in Table 1. Additionally, Werewolf is clearly an imperfect information game. The authors should reconsider this table because I believe many of the entries are inaccurate.\n- Why is the score in Table 4 unnormalized? It's an incomprehensible number as it stands."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "summary: The paper presents StarCraft II Arena as a benchmark for evaluating LLMs in strategic planning and decision-making capabilities. While it introduces fine-grained metrics and decision tracking mechanisms, the work needs substantial clarification regarding its novel contributions, metric selection justification, and experimental methodology."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. Well-structured evaluation framework with proposed fine-grained metrics\n2. Comprehensive testing across multiple LLM models\n3. Detailed decision tracking system for behavior analysis\n4. Clear visualization of experimental results\n5. Systematic approach to evaluating different aspects of LLM capabilities"
            },
            "weaknesses": {
                "value": "1. Metric Selection and Justification:\n- APM/EPM metrics appear borrowed from traditional StarCraft II evaluation without clear justification of their relevance to LLM agents\n- No discussion of how these metrics specifically reflect LLM decision-making capabilities\n- Missing analysis of whether traditional StarCraft II performance metrics are appropriate for language models\n2. Experimental Design Limitations:\n- Build-in AI difficulty level not specified\n- Race and map selection criteria not documented\n- Limited experimental scope (10 games per model)\n- Absence of LLM vs LLM experiments\n- No evaluation against human players\n- Limited testing of open-source models, reducing reproducibility\n3. Theoretical Foundation and Novelty:\n- Limited discussion of why LLMs are suitable for StarCraft II\n- Insufficient comparison between LLM agents and traditional RL agents\n- Evaluation metrics show significant overlap with existing work\n- Need clearer articulation of novel contributions\n- Better contextualization within existing literature required"
            },
            "questions": {
                "value": "1. Evaluation Metrics:\n- How do APM and EPM meaningfully evaluate LLM agent performance when their decision-making process is fundamentally different from traditional agents?\n- What is the relationship between these metrics and LLM reasoning capabilities?\n- Have you considered developing metrics specifically designed for language model evaluation?\n2. Comparative Analysis:\n- Could you clarify the key methodological differences between your work and \"Large Language Models Play StarCraft II\"?\n- What novel insights does your evaluation framework provide that weren't captured in previous work?\n- How does your fine-grained capability analysis advance the field beyond existing benchmarks?\n3. Experimental Design:\n- Why weren't LLM vs LLM experiments included in the evaluation?\n- What prevented the inclusion of human player comparisons?\n- Could you explain the decision to limit testing of open-source models?\n- How would the results differ with head-to-head language model competitions?\n4. Implementation Details:\n- How are LLM actions translated into game commands?\n- What are the time constraints for model inference?\n- What specific prompt engineering techniques were used?\n- What is the role of temperature and other sampling parameters?\n5. Theoretical Framework:\n- How does the decision tracking system account for the unique characteristics of language model reasoning?\n- What theoretical justification supports the choice of strategic planning metrics?\n- How do you ensure the evaluation framework captures the full range of LLM capabilities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}