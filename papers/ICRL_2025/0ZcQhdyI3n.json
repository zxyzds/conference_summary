{
    "id": "0ZcQhdyI3n",
    "title": "LSH Tells You What To Discard: An Adaptive Locality-Sensitive Strategy for KV Cache Compression",
    "abstract": "Transformer-based large language models (LLMs) use the key-value (KV) cache to significantly accelerate inference by storing the key and value embeddings of past tokens. However, this cache consumes significant GPU memory. In this work, we introduce LSH-E, an algorithm that uses locality-sensitive hashing (LSH) to compress the KV cache. LSH-E quickly locates tokens in the cache that are cosine dissimilar to the current query token. This is achieved by computing the Hamming distance between binarized Gaussian projections of the current token query and cached token keys, with a projection length much smaller than the embedding dimension. We maintain a lightweight binary structure in GPU memory to facilitate these calculations. Unlike existing compression strategies that compute attention to determine token retention, LSH-E makes these decisions pre-attention, thereby reducing computational costs. Additionally, LSH-E is dynamic -- at every decoding step, the key and value of the current token replace the embeddings of a token expected to produce the lowest attention score. We demonstrate that LSH-E can compress the KV cache by 30\\%-70\\% while maintaining high performance across reasoning, multiple-choice, and long-context retrieval tasks.",
    "keywords": [
        "kv cache",
        "locality-sensitive hashing",
        "compression"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We develop a token eviction strategy that uses locality-sensitive hashing to locate low-attention tokens without computing attention.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=0ZcQhdyI3n",
    "pdf_link": "https://openreview.net/pdf?id=0ZcQhdyI3n",
    "comments": [
        {
            "summary": {
                "value": "The idea is to reduce KV Cache by evicting and permanently dropping tokens at each position in the query. The heuristic used is to evict the lowest attention scored keys ( which is essentially similar to H2O / Scissorhands which preserve top attention scored keys). The difference is to use LSH to do a approximate score ranking to avoid SoftMax for exact computation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Uses LSH to approximate attention computation for eviction (if you compare to h2o / scissorhands)"
            },
            "weaknesses": {
                "value": "- Novelty: The novelty is limited.\n- H2O / Scissorhands are known to not perform well on longbenchmark. Can we see some results on longbenchmark like passage retrieval datasets ?\n- Missing baselines --only baseline used is L2 norm. \n-\u00a0Limited evaluation. can we get more results on longbenchmark at different budgets with standard baselines."
            },
            "questions": {
                "value": "see questions above,"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents new methods to accelerate inference of auto-regressive transformers used in most modern-day decoder-based LLM architectures. Indeed, the main drawback of existing systems is the size of the \"KV  Cache\" or Key-Value Cache which is used during the attention mechanism. To speed up the attention calculation, most systems have a cache which remembers the keys and values of commonly used tokens, to avoid recomputing it for each token decoding.  However ,such a cache, for it to be performant at inference time, must scale quadratically with the sequence length, and linear in number of layers and attention heads. \n\n(Authors: please explain why for the uninformed reader -- this is stated in the intro, but without explanation)\n\nIn this paper, the authors present an LSH based method to evict far-away key tokens. Indeed, suppose we have an LSH which gets a binary encoding of any vector using random hyperplane projection method (SIMHASH). \nThen, we can first pre-process and compute the hamming distance between query token and all key tokens, and evict the farthest one, as this is the one least likely to affect the overall attention soft-max operation.\n\nThey implement this simple scheme and provide a range of quality vs cache size metrics comparing with one other KV-cache called L2-Dropout Cache, which drops the keys based on their magnitudes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Studies an important problem of much significance in todays LLM era. \n\nPresents a simple yet elegant approach\n\nDoes good evaluations on a range of use-cases"
            },
            "weaknesses": {
                "value": "Why is there no timing experiment, since that will be one key benefit of caching.\n\nWhy only restrict to attention-free cache policies and specifically only compare with the L2-dropout baseline?\n\nConceptually, what is the key difference with Reformer? I have not read that paper but you mention in passing that it is using LSH and simhash also. Is which cells to evaluate vs what to evict the only difference between Reformer and your work? If so, worth comparing with Reformer also in plots?\n\nWhat is the rationale of the policy? Why can't a token just evicted become relevant again? I guess is there some language-based \"locality of reference\"?\n\nDo ablation of the hardcoded bits, i.e., you mention you hard-cache the first few and last few tokens. What is the contribution of this to your overall success metrics?\n\nThe eviction policy is not clearly understandable in how it aggregates the hamming distances over time steps. Is it only based on the most recent time step, or some more complex rule?"
            },
            "questions": {
                "value": "Line 52: \"However, this L2 dropout strategy only performs well on\nlong-context retrieval tasks. It is specialized to retain only those tokens with the highest attention\" -- be more specific. Why is this?\n\nLine 57: \"wide variety of tasks?\" -- how do you define this?\n\nLine 145: Formally for our setup, distd(x, y) cos \u03b8x,y, here it is more a measure of cosine similarity than distance. Misleading, perhaps?\n\nLine 419: did you mean \"LSH dimension does significantly impact performance\" --> does not?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces LSH-E, an algorithm for compressing the key-value (KV) cache in large language models (LLMs) using locality-sensitive hashing (LSH). Despite the availability of prior work\u2014including KDEformer, Hyperattention, SubGen, and QJL\u2014that similarly utilizes LSH for efficient attention and memory management, these related efforts are not cited here. LSH-E leverages Hamming distance calculations in a binary space following a Quantized Johnson-Lindenstrauss (JL) transform (SimHash) to identify and evict tokens with low relevance to the current query, resulting in memory savings. This pre-attention approach provides a lightweight, GPU-efficient solution for long-context tasks, although its effectiveness ultimately depends on the algorithm\u2019s CUDA implementation efficiency."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The use of theoretical approaches such as SimHash, a highly efficient hashing method, is a valuable aspect of this work, contributing to both the effectiveness and scalability of the proposed method."
            },
            "weaknesses": {
                "value": "- The term \"novel\" should not be used for LSH in this context, as it is not a new approach and has appeared in prior work. Specifically, the methods used in KDEformer, Hyperattention, QJL, and SubGen demonstrate significant overlap, yet these works are not cited here, despite their relevance.\n\n- The experimental setup lacks comprehensiveness; comparisons with alternative methods like H2O, SubGen, and other established baselines should be included to provide a more robust evaluation.\n\n- The datasets used in the experiments are not sufficiently large for evaluating performance in long-context scenarios. Given that these methods target long-sequence processing, experiments should ideally use token sizes over 50,000. LongBench or other large-scale datasets would be more appropriate for a thorough evaluation.\n\n- Additionally, runtime metrics should be reported to assess the efficiency of token generation and to substantiate the computational benefits claimed in the paper.\n\nKDEformer : https://proceedings.mlr.press/v202/zandieh23a.html\nHyperAttention : https://arxiv.org/abs/2310.05869\nSubGen :  https://arxiv.org/abs/2402.06082\nQJL : https://arxiv.org/abs/2406.03482"
            },
            "questions": {
                "value": "- Could you provide a plot showing the distortion error introduced by LSH compression across different levels of compression? Specifically, how does the approximation quality change as more tokens are evicted or as the quantization parameters are adjusted?\n\n- Given that LSH-E\u2019s efficiency largely depends on its CUDA implementation, can you elaborate on any specific optimizations made within the CUDA code?\n\n- Could you clarify how LSH-E handles multi-head attention? Specifically, is each head processed separately with its own LSH compression, or is there a shared mechanism across heads?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method that uses LSH to perform kv cache eviction. The provided experiments show that the proposed method outperforms the baseline."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Strong Points\n----\nS1. The problem of the paper is well-motivated. \n\nS2. The proposed algorithm is simple and clear with illustrative example.\n\nS3. The proposed method outperforms the baseline L2."
            },
            "weaknesses": {
                "value": "Weak Points\n----\nW1. Important related studies and baselines are missing:\nSinghania, P., Singh, S., He, S., Feizi, S., & Bhatele, A. (2024). Loki: Low-Rank Keys for Efficient Sparse Attention. arXiv preprint arXiv:2406.02542.\nTang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., & Han, S. (2024). Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference. arXiv preprint arXiv:2406.10774.\n\nW2. The key measures of the targeted task should be have more accurate inference with lower memory footprint and latency. I do not agree with the methodology of not comparing with other \"non attention-free\" methods.\n\nW3. The presentation of experiments need to be improved: Lack of discussions and intuitions in the experiment analysis. For example, why does LSH-E outperform Full in Figure 4a; why does LSH-E become worse than L2 after 50% cache budget in Figure 4b? We have many subsubsections in the experiments, but most contents in those are barely text illustration of the figure and result while no discussion of why we would have those results.\n\nW4. The execution time of the proposed system is missing.\n\nW5. The discussion of the error introduced by the LSH is not included. I wonder what if we use cosine similarity to evict the cache instead of LSH, how will be the accuracy, latency, and memory usage?\n\nW6. In the supplementary materials, we see more experiments with more baselines that are better than L2. I wonder the reason why the authors do not include them.\n\n\nPresentation\n----\nP1. Line 180 \"heavy hitters' -> ``heavy hitters''\nP2. The axis captions of the figures are too small to be seen."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a KV cache compression method based on LSH and shows that LSH-E can achieve good downstream performance on various downstream tasks with a 30%- 70% compression ratio."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper applies novel LSH methods to KV cache problems. The motivations and reasons why LSH can produce a good performance are well discussed. Besides this, a static compression rate of 30% - 70% is also helpful for many LLM serving systems, given the accuracy is preserved."
            },
            "weaknesses": {
                "value": "1. There is no comparison with other static KV compression baselines, including H2O, streamingLLM, and SnapKV. If this problem is solved, I will raise my score.\n2. Only the memory compression ratio is shown. I will ask for the wall clock speedups (latency or throughput)."
            },
            "questions": {
                "value": "Besides the problems mentioned in Weakness,\n1 Does this method work well with quantization (KIVI, AWQ)?\n2 How long does LSH-E increase first token latency?\n\nThese two questions can be left for future work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}