{
    "id": "vtCkb4KJxr",
    "title": "Adaptive Threshold Sampling for Fast Noisy Submodular Maximization",
    "abstract": "We address the problem of submodular maximization where objective function $f:2^U\\to\\mathbb{R}_{\\geq 0}$ can only be accessed through i.i.d noisy queries. This problem arises in many applications including influence maximization, diverse recommendation systems, and large-scale facility location optimization. We propose an efficient adaptive sampling strategy, called Confident Sample (CS), that is inspired by algorithms for best-arm-identification in multi-armed bandit, which significantly improves sample efficiency. We integrate CS into existing approximation algorithms for submodular maximization, resulting in algorithms with approximation guarantees arbitrarily close to the standard value oracle setting that are highly sample-efficient. We propose and analyze sample-efficient algorithms for monotone submodular maximization with cardinality and matroid constraints, as well as unconstrained non-monotone submodular maximization. Our theoretical analysis is complemented by empirical evaluation on real instances, demonstrating the superior sample efficiency of our proposed algorithm relative to alternative approaches.",
    "keywords": [
        "submodular",
        "multi-armed bandit",
        "bandit feedback",
        "best-arm identification",
        "combinatorial optimization"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vtCkb4KJxr",
    "pdf_link": "https://openreview.net/pdf?id=vtCkb4KJxr",
    "comments": [
        {
            "summary": {
                "value": "The topic of this paper is the problem of submodular maximization:\ngiven a ground set $U$ and a an oracle access to a\nsubmodular objective function $f: U \\to \\mathbb{R}$,\nour task is to find $S\\subseteq U$ with the highest value of $f(S)$.     \nThey study also the classical constrained versions where\nwe maximize $f(S)$ subject to cardinality and matroid constraints.\n\nThe authors study submodular maximization in the setting,\nwhere\nexact evaluation of $f(S)$ is not possible \nand one can obtain only noisy estimate. In particular, they assume\nthe oracle returns a noisy estimate which is unbiased and \nis $R$-subgaussian. \nThis setting was already studied before, e.g. by Singla et al. '15, who\nachieved almost the same approximation guarantees as in the classical\nnon-noisy setting and provided bounds on the number of the performed\nnoisy queries.      \nAuthors provide theoretical bounds which they compare to previous\nworks and they also present an empirical comparison.\n\nThe main difference between their work and Singla et al. '15\nis that their algorithm is based on a faster implementation\nof the classical greedy algorithm by Badanidiyuru and Vondrak which, instead\nof comparing the marginal gain of the elements to each other, it compares\ntheir marginal gain to a threshold chosen during algorithm's runtime.\nTherefore, instead of a quadratic dependence on the gap $\\Delta_{max}$\nbetween the two top marginal gains, they have a quadratic dependence on the\ngap from the threshold (their parameter $\\phi$)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The problem is important and their setting seems relevant and well motivated.\nThey achieve improvements in sample complexity over previous works at least in\nsome settings. They have a better running time as well."
            },
            "weaknesses": {
                "value": "The results are somewhat difficult to appreciate for me,\ndespite that they provide more than a page long comparison with the\nprevious works in Appendix discussing the differences in the long formulas.\nFor example, it is not clear to me why is it better to have dependence\non $\\phi$ instead of $\\Delta_{max}$.\nIs $\\phi$ always smaller than $\\Delta_{max}$?\n\nI am not an expert in the field. While I see that the authors do achieve\nan improvement over the previous works, I do not see its significance.\nTherefore my rating."
            },
            "questions": {
                "value": "* Can you comment on the weakness above?\n\n* Can you say that your algorithm has a better sample complexity\nthan the previous works always?\n\n* Knowing something about the properties of the input instance, how can\nyou estimate the value of $\\phi$ without running your algorithm?\nI am asking this to understand whether your bounds can be used to predict your algorithm's performance. Note that, for example, $\\Delta_{max}$ can be\nestimated in advance based on the properties of the input instance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the constrained submodular maximization under noise problem that arises in many applications in AI and ML Communities. The key algorithm of this work is Confident Sample (CS), which is inspired by algorithms for best-arm-identification in multi-armed bandit. The CS algorithm can then be integrated into many existing approximation algorithms for submodular maximization under constraints. The authors show that the integrated algorithms take fewer samples on both theoretical and practical sides."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This work studies an interesting and meaningful research problem in the AI \u200b\u200band ML community. The paper is well-written and structured. \n- The core algorithm, Confident Sample (CS), has been shown to be effective in estimating the expectation of the objective submodular function in Gause distributions. The theoretical analysis is natural and reliable. However, the techniques for proving them are pretty elementary."
            },
            "weaknesses": {
                "value": "- Except for CS, the remaining algorithms are not new. The author's main contribution is how to apply CS algorithm to existing algorithms and the corresponding theoretical analysis.\n- The idea of the CS algorithm is not new. It existed in previous algorithms (For example, in Alg 2 in [Mat]). \n- It is natural to apply CS algorithms to existing algorithms, but it is not difficult to derive theoretical bounds.\n\n======================\n\nRef. \n\n[Mat] Matthew Fahrbach, Vahab S. Mirrokni, Morteza Zadimoghaddam: Submodular Maximization with Nearly Optimal Approximation, Adaptivity and Query Complexity. SODA 2019: 255-273."
            },
            "questions": {
                "value": "1. What are the differences (ideas, theoretical analysis) between CS and Alg 2 in [Mat]?\n2. What are the challenges in getting better theoretical bounds when applying CS algorithms to existing algorithms?\n3. Does the CS algorithm work well with other distributions?\n4. Can the CS algorithm be applied to the Minimum Cost Submodular Cover (MCSC) problem? The paper's contribution would be better if it is possible to apply CS to MCSC with better theoretical bounds."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies submodular maximization in a noisy model. Formally, the goal is the same as in the standard monotone submodular maximization with cardinality or matroid constraints, but the algorithm has access to the function via a noisy oracle. In particular, given a set $S$ and element $x$, the algorithm can query $(S,x)$ to the oracle and receives a random (unbiased) estimator of the marginal value of $x$ with respect to $S$. The authors assume that such estimator is subgaussian (of parameter $R$ that is known).\n\nThe paper's contribution is to provide tight approximation results for cardinality and matroid constraints by adapting known techniques with a sample efficient estimation procedure called Confident Sample (CS)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Submolar maximization is a relevant topic for the NeurIPS/ICML/ICLR audience, given its vast applicability in ML. The model of noisy queries is natural and well-motivated. The sample complexity bounds are not trivial (as the authors explain, using Hoeffding would already yield some results)."
            },
            "weaknesses": {
                "value": "- The technical contribution is moderate. In the end, the paper's contribution lies in rewriting concentration bounds, parameterized by a notion of gap. Threshold-based algorithms are well-known in the submodular literature. \n- The sample complexity bounds are pretty involved. Many parameters are entailed, and a clear picture is difficult to get."
            },
            "questions": {
                "value": "Do you have any tightness results on the sample complexity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces the Confident Sample (CS) algorithm to maximize submodular functions under noisy conditions efficiently. By leveraging insights from multi-armed bandit algorithms, CS reduces the number of noisy queries required to approximate submodular functions, making it applicable to diverse optimization tasks such as influence maximization and recommendation systems. Theoretical analysis and empirical results demonstrate the effectiveness of CS in achieving competitive approximation guarantees with significantly improved sample efficiency compared to traditional methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper proposes the Confident Sample (CS) algorithm, which effectively reduces the number of noisy queries by dynamically adjusting the sample size according to the level of uncertainty. This adaptive approach contrasts with traditional fixed-precision methods, offering substantial improvements in sample efficiency. The work's theoretical contributions are robust, providing guarantees on both approximation quality and sample complexity, making it a competitive alternative to existing methods like ExpGreedy. These theoretical insights are further supported by empirical evaluations on real-world datasets, where the proposed algorithms demonstrate superior sample efficiency, highlighting the practical relevance of the approach."
            },
            "weaknesses": {
                "value": "I do not have significant negative comments regarding the contributions of this paper. My only concern is whether the topic aligns well with ICLR's scope, as I have not come across purely theoretical work on submodular maximization algorithms published at ICLR before. While submodular maximization is indeed a crucial problem in machine learning, ICLR, to my knowledge, tends to focus more on areas related to deep learning and neural networks. Therefore, would it be more appropriate to consider submitting this paper to venues like NeurIPS or ICML, which might better align with its focus?"
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}