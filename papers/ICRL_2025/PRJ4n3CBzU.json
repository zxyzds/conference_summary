{
    "id": "PRJ4n3CBzU",
    "title": "AttackQA: Development and Adoption of a Dataset for Assisting Cybersecurity Operations using Fine-tuned and Open-Source LLMs",
    "abstract": "Retrieval-augmented generation (RAG) on specialized domain datasets has shown improved performance when large language models (LLMs) are fine-tuned for generating responses to user queries. In this study, we develop a cybersecurity question-answering (Q\\&A) dataset, called AttackQA, and employ it to build a RAG-based Q\\&A system designed for analysts in security operations centers. The dataset comprises 25,335 Q\\&A pairs, accompanied by rationales to facilitate fine-tuning and evaluation. 80\\% of the dataset was generated with help of a lightweight open-source LLM (LLama 3 8B), which produced over 1100 tokens per second with full 16-bit precision on specialized hardware. To ensure dataset quality, we fine-tuned LLama 3 70B to detect and reject low-quality Q\\&A pairs. In using the dataset for RAG, we demonstrate that fine-tuning open-source embeddings and LLMs can yield superior accuracy compared to OpenAI's state-of-the-art proprietary embedding and LLM (GPT-4o). Furthermore, we use Llama 3.1 405B as a judge to evaluate answer correctness, enabling the creation of a fully open-source, high-speed RAG and evaluation pipeline with an associated benchmark.",
    "keywords": [
        "LLM",
        "Large Language Models",
        "Generative AI",
        "Dataset",
        "Cyber",
        "Security",
        "MITRE",
        "Chatbot",
        "Judging",
        "Quality",
        "Control",
        "Open source",
        "RAG",
        "Retrieval",
        "Augmented",
        "Generation"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "A new dataset to train or tune models to gain cybersecurity expertise, along with an evaluation of fine-tuned open-source models that outperform proprietary LLMs that are over 100x as large on the dataset",
    "creation_date": "2024-09-21",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PRJ4n3CBzU",
    "pdf_link": "https://openreview.net/pdf?id=PRJ4n3CBzU",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a detailed process for dataset construction and model capability enhancement, centred around the topic of cybersecurity Q&A. The paper describes the experimental details to illustrate its reliability, and gives specific experimental results with case studies. When the fine-tuned models based on AttackQA are integrated into related RAG pipeline, it can effectively improve the model performance in cybersecurity domain."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Build high-quality, reliable dataset AttackQA for cybersecurity community\n- Plans for dataset and benchmark open-source \n- In addition to the dataset generation, a significant amount of work has been done to illustrate feasible application scenarios for AttackQA, enriching the hierarchy of the whole paper"
            },
            "weaknesses": {
                "value": "- The discussion of related datasets is missing in the Related Work section\n- Lack of ethics review as a cybersecurity domain paper that might raise ethical concerns"
            },
            "questions": {
                "value": "I appreciate the author's efforts throughout the paper, this work has a clear contribution and recursive structure, my main concern is the limitations of the contribution. Specifically, all of evaluations are based on AttackQA dataset, despite the author splits it to training set and the evaluation set, but they are also generated in the same source and have similar distribution. Then it is not surprising that the improvement in experimental results is achieved on the test set after fine-tune the model, the evaluation experiments can only show that the AttackQA fine-tuned model performs better on AttackQA after the model has been integrated into the RAG pipeline, but cannot show that the model has improved its overall performance in the cybersecurity domain. This paper is an excellent technical report, containing detailed parameters and specific results, but the overall contribution is not sufficient to support it as a research paper.\n\nIn addition, as a security-related paper that might raise ethical concerns, having a relevant discussion indicates that due diligence has been made to minimise potential harm has become a common practice, hope the authors can provide this information.\n\n\n*minors*:\n- Unify the terms. `The MITRE ATT&CK knowledge base` and `The MITRE ATT&CK@ knowledge base`\n- Line173-175, the use of inverted commas is not standardised, it is recommended to present the dataset example by a separate text box rather than plain text"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This article used MITRE ATT&CK, which\u00a0is a knowledge base of adversary tactics and techniques, to generate a dataset on cybersecurity, named AttackQA, but lacked a discussion addressing the harmfulness of the dataset."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces AttackQA, a cybersecurity Q&A dataset comprising 25,335 Q&A pairs with rationales, generated using the open-source LLM Llama-3 8B. A larger Llama-3 70B model was fine-tuned to improve dataset quality by filtering out low-quality pairs. The dataset was used to fine-tune open-source embeddings and LLMs, demonstrating better accuracy compared to OpenAI's GPT-4o. This work aims to support analysts in security operations centers by enhancing the efficiency and effectiveness of cybersecurity operations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Main strengths:\n+ Reduce both the cost and complexity of the system by utilizing open-source LLMs and embedding models.\n+ Fine-tuning open-source LLM and embedded models achieves better performance than using proprietary models."
            },
            "weaknesses": {
                "value": "Main weaknesses:\n-  Since AttackQA mainly focuses on textual knowledge, it cannot address the challenges of rapid response to cybersecurity incidents. In practice, Security Operations Centers (SoCs) also need to check  various tools' result to investigate the incidents.\n- The experimental results may not be universally applicable, as AttackQA relies on a single dataset named MITRE ATT&CK\u00ae .\n- The methodology for selecting techniques at each stage is not explained in sufficient detail.\n- The experiments lack a comparison with existing solutions in this field.\n- Partial grammatical errors and incomplete legends, such as the first paragraph in the conclusion section and  in the legend of Figure 2."
            },
            "questions": {
                "value": "1.  What is the performance of the existing Q&A systems for network security datasets, and  what advantages does AttackQA provide?\n\n2. Given the abundance of cybersecurity datasets, why did the author choose MITRE ATT&CK\u00ae .  If AttackQA were trained on a different  dataset, would the performance vary significantly? How can AttackQA be scaled to other datasets?\n\n3. How did the authors choose the LLM models through the entire process? What are the criteria and reasons for choosing LLM for different stages and tasks?\n\n4. Could the authors elaborate on the specific techniques and strategies employed during the fine-tuning of the Llama 3 8B and Llama 3 70B models? What were the main challenges encountered, and how were they addressed?\n\n5. In section 3.1, the paper claims each Q&A pair is derived from a single document because answers do not require information from multiple documents. Why? I question the validity of this assertion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a dataset, AttackQA,\ndesigned to assist analysts in Security Operations Centers (SOCs)\nwith timely and accurate answers to cybersecurity-related questions.\nThe dataset is created using the MITRE ATT&CK knowledge base\nand features final 25,335 question-answer (Q&A) pairs fine-tuned to\nimprove retrieval-augmented generation (RAG) pipelines.\nThe proposed system is intended to streamline SOC workflows,\nenabling analysts to quickly access high-quality,\ncontextualized information, thereby addressing key challenges in cybersecurity operations,\nsuch as knowledge gaps and slow response times.\n\nThe reviewer acknowledges the relevance of the dataset and the potential impact of a fast,\nreliable RAG pipeline that can be deployed using accessible,\nopen-source technology. The reviewer thinks this work is a valuable addition to the cybersecurity\nfield by providing a high-quality, open-source tool for cybersecurity-specific question answering.\n\nHowever, the paper does not explain how this dataset could be useful for the cybersecurity research community. The reviewer recommends adding a discussion and/or subsection to discuss specific research applications and practical uses of AttackQA for cybersecurity researchers. This addition would help readers realize the dataset\u2019s value and relevance for research in cybersecurity."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Domain-Specific Dataset\n\nThe reviewer notes the paper\u2019s primary strength in creating a reasonably large,\ncybersecurity-specific Q&A dataset that\nleverages the MITRE ATT&CK knowledge base, a widely adapted resource in the cybersecurity community.\nThe paper has included detailed\ndescriptions of the dataset generation process,\nincluding both human-generated and LLM-generated Q&A pairs,\nand a quality control process.\nThe review believes this structured approach increases confidence in the dataset's\nquality and applicability for SOC needs.\n\nHowever, the reviewer wonders about the representativeness\nof the Q&A pairs, particularly how well they reflect real SOC inquiries.\nThe authors might elaborate on how they determined which types of questions\nwould be most beneficial to SOC analysts.\n\n2. Quality Control Mechanism\n\nThe reviewer appreciates the paper\u2019s attention to quality control,\nwhich involves fine-tuning LLMs (e.g., Llama 3 70B) to identify and\nfilter low-quality Q&A pairs.\n\nThe reviewer wonders if the paper could provide more\ninsight into how the quality control model was trained and\nthe rationale behind choosing G-Eval metric for\nquality acceptance.\n\n\n3. Model Deployment with High-Throughput Performance\n\nThe paper highlights that AttackQA models achieve high token generation speeds\nusing specialized hardware, such as the SambaNova Cloud,\nwhich is particularly beneficial for SOC environments where latency is critical.\n\nThe reviewer wonders if the paper has considered testing the model\non consumer-grade hardware. Including performance benchmarks on less specialized\nhardware could make the paper more broadly applicable to organizations with limited resources."
            },
            "weaknesses": {
                "value": "1. Limited Validation in Real-World SOC Settings\n\nThe reviewer observes that the dataset and RAG pipeline have not yet been validated\nin an operational SOC environment,\nlimiting the understanding of AttackQA\u2019s real-world effectiveness.\n\nThe reviewer suggests that a small-scale deployment or pilot study could greatly enhance\nthe paper\u2019s credibility.\nEven preliminary data on SOC analysts\u2019 feedback or performance improvements in\nsimulated SOC scenarios would substantiate the system's value.\n\n\n\n2. Dependence on Specialized Hardware\n\nThe reported performance relies on access to high-performance hardware,\nsuch as the SambaNova Cloud, which may not be available to all SOCs.\nThis could limit the scalability and accessibility of the proposed solution.\n\nTo increase accessibility, the reviewer recommends that the authors benchmark AttackQA on\nwidely available hardware, such as NVIDIA consumer GPUs,\nand report these results as part of the analysis.\nThis would demonstrate the adaptability of the pipeline to a range of environments.\n\n\n3. Scope of RAG Framework\n\nThe reviewer notes that while the RAG framework is effective,\nit may not fully address the complexity of real SOC inquiries that\nrequire multi-hop reasoning or cross-document synthesis.\nAttackQA, in its current form, may be limited in answering such complex questions accurately.\n\nThe reviewer recommends that the authors may consider extending the framework to\ninclude multi-hop reasoning or explore alternative retrieval models that can\nhandle cross-document synthesis. This could make the model more adaptable to complex,\nreal-world SOC questions.\n\n4. Error Analysis and Failure Case Discussion\n\nThe reviewer finds that the paper lacks an analysis of common failure cases,\nwhich could help in understanding potential limitations of AttackQA and\nprovide directions for improvement.\n\nThe reviewer suggests conducting an error analysis to identify and\ncategorize common failure cases, such as instances where the model\u2019s answers are incomplete,\nincorrect, or hallucinated.\nOutlining these challenges and discussing potential improvements would strengthen the paper.\n\n5. Practicality and Utility in the Cybersecurity Research community\n\nThe paper does not discuss how AttackQA might be used by researchers,\nleaving questions about its broader utility in the cybersecurity field.\nNote that the reviewer is focusing on the cybersecurity research community, not its application in the industry.\n\n- How could this dataset be useful for the cybersecurity research community?\n- In what ways can researchers use this dataset for open science or further studies?\n- What specific research applications and practical uses does AttackQA offer for cybersecurity researchers?\n- How does the dataset add value and relevance to research in cybersecurity?"
            },
            "questions": {
                "value": "1. How were the Q&A pairs chosen to ensure that they reflect the most relevant types of\nquestions SOC analysts would encounter?\n\n2. Could the paper provide more details on the specific metrics, thresholds,\nand methods used in the quality control process to filter Q&A pairs?\n\n3. In what types of scenarios or question types does AttackQA outperform GPT-4o,\nand could the paper elaborate on the specific strengths in these contexts?\n\n4. Has the model\u2019s performance been tested on consumer-grade hardware,\nand if so, could these results be included?\n\n5. Did the paper do any user studies or\ntests with SOC analysts to gather real-world feedback\non the tool's performance and utility?\n\n6. Could the paper discuss any potential performance\ntrade-offs or adjustments when deploying AttackQA on hardware with limited resources?\n\n7. Did the paper consider testing AttackQA\u2019s performance on\nmore complex, multi-step SOC questions?\n If so, what were the findings, and/or is this an area of future exploration?\n\n9. Could the paper share preliminary findings on common\nerrors or failure cases, and what strategies might address these limitations?\n\n10. How could this dataset be useful for the cybersecurity research community?\n\n11. In what ways can researchers use this dataset for open science or further studies?\n\n12.  What specific research applications and practical uses does AttackQA offer for cybersecurity researchers?\n\n13. How does the dataset add value and relevance to research in cybersecurity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper curated a dataset named AttackQA based on  MITRE ATT&CK knowledge base and fintune the LLM and embedding model to show that the performance of QA and retrieval can be improved by fine-tuning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "+ It takes some efforts to curate the dataset\n+ The security dataset is needed"
            },
            "weaknesses": {
                "value": "- The paper is a bit hard to follow for readers unfamiliar with the cybersecurity\nI would say, for a machine learning conference, the paper should be able to be understood by general audience, instead of only readers who already have a strong background of cybersecurity. For example, in the related work section, the paper could expand on giving a detailed example of one entry of MITRE ATT&CK dataset since this work is based on that. The same for your dataset. You should provided detailed examples for your dataset, instead of leaving them in appendix. Although the paper mentions the  attack technique \u2019T1562.001' as example in the main body, it is hard for general readers to understand what is this for (I have some bg for cybersecurity but I also have no idea before searching for it). I would suggest replacing it with a more common CWE example such as SQL injection or integer overflow.\n\n- The paper writing could be improved.\nI think the paper writing could be enhanced if the authors put more efforts on that. For example, there are many double quotes like this \u2019T1562.001\u2019. Please check how to properly use that in latex. Some figures like Fig4 is too small to see the texts on that. Moreover, the table alignment for some tables make it painful to read, especially Tab9. \n\n- Irrelevant contribution about decoding speed.\nThis paper claims the high decoding speed by SambaNova Cloud platform, and mentions it a lot in paper. However, I don't think using a cloud platform can be deemed as contribution. When we refer to inference efficiency, we usually refer it to serving framework such as vllm, sglang, or attention mechanism such as flash attention 2, or decoding strategy such as lookup decoding. If you really want to show the overhead is small and this system can be real-time, you should at least apply these commonly used techniques since they are open-sourced. \n\n- Unclear technique details\nDue to the missing example, I found section 3.1-3.4 hard to understand. For example, what is your manual heuristics? why do you choose Llama-3-8B instead of gpt-4o or others? what is the instruction prompting? The missing of these details make readers hard to follow.\n\n- Missing insights of model choice\nThe paper choose Llama-3 family for the whole work, while not even mentioning some sota open-sourced model such as Qwen-2.5-72B. There is no insights of such model prefernce. Also, in Section 3.5.2, the paper mentions that the Llama-3-70B(before finetuning) can outperform GPT-4o without any explanation nor detailed examples. It has no insights for the community. \n\n- Missing training details\nIn section 4, a lot of training details for RAG for embedding model are missing. \n\n- Choice of judgement model without justification\nAgain, this works chooses Llama-3 family (Llama-3-405B) for judgement. Typically, for the evaluation part, it needs justification for why such a method could suit. It won't be a problem if you pick gpt-4o since many LLM papers are doing that. It is commonly known as SOTA and able to judge. You CAN justify that gpt-4o is not accurate if you could provide detailed explanation and examples. Otherwise, it is not convincing to pick a 405B model for judgement as it is too large to run locally and unclear whether it is the SOTA method for judgement."
            },
            "questions": {
                "value": "See comments above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}