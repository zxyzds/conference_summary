{
    "id": "vcJiPLeC48",
    "title": "Gradient-free training of recurrent neural networks",
    "abstract": "Recurrent neural networks are a successful neural architecture for many time-dependent problems, including time series analysis, forecasting, and modeling of dynamical systems. Training such networks with backpropagation through time is a notoriously difficult problem because their loss gradients tend to explode or vanish. In this contribution, we introduce a computational approach to construct all weights and biases of a recurrent neural network without using gradient-based methods. The approach is based on a combination of random feature networks and Koopman operator theory for dynamical systems. The hidden parameters of a single recurrent block are sampled at random, while the outer weights are constructed using extended dynamic mode decomposition. This approach alleviates all problems with backpropagation commonly related to recurrent networks. The connection to Koopman operator theory also allows us to start using results in this area to analyze recurrent neural networks. In computational experiments on time series, forecasting for chaotic dynamical systems, and control problems, as well as on weather data, we observe that the training time and forecasting accuracy of the recurrent neural networks we construct are improved when compared to commonly used gradient-based methods.",
    "keywords": [
        "recurrent neural networks",
        "koopman operator",
        "random feature networks"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "We construct all parameters of recurrent neural networks using random features and Koopman operator theory, without any iterative optimization.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vcJiPLeC48",
    "pdf_link": "https://openreview.net/pdf?id=vcJiPLeC48",
    "comments": [
        {
            "summary": {
                "value": "-  The paper proposes a training method for modeling recurrent neural networks without the use of gradient-based methods, such as backpropagation through time (BPTT), which suffers from exploding and vanishing gradients, mostly occurring in a system with chaotic dynamics. Building on concepts from random feature models, such as reservoir computing (echo-state networks), the paper proposes the random sampling of weights (W\u00ad\u00ad\u00ad and b) in the RNN from a data-driven distribution. In addition the paper employs Koopman operator theory to find the outer weights of the RNN model, which map the current state to the next state. The Koopman operator theory maps the finite nonlinear transformation matrices (outer weights) to a linear infinite-dimensional space, where the extended dynamic mode decomposition (EDMD) method is used to find a finite-dimensional approximation of the Koopman operator. \n-  For model validation, they show some computational experiments comprising simple ODEs, such as the Van Der Pol Oscillator, chaotic dynamics (Lorenz and Rossler systems), and real-world examples involving weather data.\n- Paper reports results from these computational experiments in the form of training time and error (MSE/KL Divergence). When compared to other models, such as an LSTM, ESN (echo-state network), and shPLRNN (state of the art backpropagation-based RNN), the proposed model (Sampled-RNN) achieves comparable performance, in terms of MSE and KL Divergence, and a faster training time."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Interesting connection to Koopman operator.\n- Interesting topic of trying to circumvent gradient based training."
            },
            "weaknesses": {
                "value": "- Sampling procedure not clearly explained. (E.g. 'As we stick to networks with one hidden layer in this paper, we ignore the multilayer sampling here and direct the reader to Bolager et al. (2023) for the full sample and construction procedure for an arbitrary number of hidden layers.')\n- Paper presentation generally not clear. Hard to follow. Illustrative example: equation 1 is referred to before presented. Paper requires excessive \u2018detective work\u2019 to understand what they did and/or are talking about.\n- How sampling approach differs from ESNs seems unclear, and a minor innovation at best.\n- Although interesting, connection to Koopman operator theory does not seem novel."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an alternative strategy to backpropagation through time (BPTT) by using a combination of Koopman operator theory and random feature networks instead of gradient-based techniques. This novel method avoids vanishing and exploding gradient problems, and outperforms BPTT in terms of training time and accuracy in a series of empirical comparisons, including time series, forecasting, control, and weather problems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Proof seems to be sound and theoretically correct given the assumptions stated by the authors, however i am not an very well versed with koopman operator theory and thus have provided a lower confidence score.\n2. Their method (called sampled RNN) takes significantly less time to train than alternative state-of-the-art models such as ESNs, shPLRNNs, and LSTMs. Predictions from this model capture patterns in toy experiments as well as real-world data (like weather forecasting), and outperform current models (especially on problems with long horizons)."
            },
            "weaknesses": {
                "value": "1. There is little motivation for the problem in the introduction, and the paper jumps right into formalisms. \n2. The weights and biases need to be sampled from a data-dependent probability distribution, however it's unclear how feasible this is?\n3. This method does not converge for controlled systems."
            },
            "questions": {
                "value": "1. \u201cFor completeness we have added sigma_hx as an arbitrary activation function. We choose to set sigma_hx as the identity function to let us solve for the last linear layer\u2026 other activation functions such as the logit are possible as well\u201d. However this is not shown in the paper. Can the authors provide clarification on the effect of non-linear activations here?\n2. In the weather task, the sampled RNN has a similar MSE to shPLRNNs and LSTMs for 1-day forecasting, but higher than both alternative models for week-long forecasting. However, MSE is higher than ESNs for the Rosseler system task. Can the authors provide some clarification on this?\n4. Can the authors provide clarifications/results on how performance for chaotic systems might change based on how many samples are drawn for the sampled RNN (and the influence on dimensionality of the hidden layer)? \n5. The authors state: \u201cThe complexity of solving this system depends cubically on the minimum number of neurons and the number of data points (respectively, time steps). This means if both the network and the number of data points grow together, the computational time and memory demands for training grow too quickly. For BPTT, the memory requirements are mostly because many gradients must be stored for one update pass\u201d. \u2014 can the authors provide a comparison of # of neurons vs # of time steps?\n6. Can the authors provide details on how they infer the \u201cdata-dependent probability distribution\u201d (especially for the weather data)? \n5. Additionally, can the authors expand on how weights and biases are sampled from the specific, data-dependent probability distribution (again for weather data) and how other parameters are computed using a sequence of linear equations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a novel way to construct recurrent neural networks. Their approach is two stepped, where they first generates hidden weights and biases according to a data-dependent distribution, and then construct read-out parameters by approximating the dimensional Koopman operator with dynamic mode decomposition."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Training recurrent neural networks is notoriously hard. I am partial to the authors approach of fitting an RNN to a a dynamical system by a smart sampling of hidden parameters and Koopman operator based modeling. Particularly, I am drawn to the use of Koopman operator in the context of RNNs. \n\nThe paper is very well written and is a nice read, and the results comparing their gradient-free approach to trained models are impressive."
            },
            "weaknesses": {
                "value": "It is unclear how much the Koopman operator and EDMD components contributed to model performance. Since the hidden weights initialisation schema is an application of previous work (Bolager et al. (2023)), I see the Koopman related work as the main conceptual innovation. However, based on the presented results it is hard to disentangle where improvements come from and I am not fully convinced of the added value of EDMD. I suggest the authors include two additional experiments: a. setting hidden weights randomly and learning read-out with EDMD and b. setting hidden weights based on Bolager et al. (2023) and learning read-out without EDMD. \n\nI am also not keen on the name and think it over-promises. Being able to train general recurrent neural networks without gradient descent or BPTT is an extremely ambitious goal, which this paper does not fulfils. As the authors explain in the (very much appreciated) limitation section, their approach does not immediately extend to RNN tasks relating to computer vision or NLP, thus the paper results are mostly regard dimensional dynamical system. I still believe their results are impressive, but the language, and specifically the title, needs to be toned down."
            },
            "questions": {
                "value": "The impact of exploding and vanishing gradients is not made clear in this manuscript. I understand their model outperforms LSTMs and other, gradient based models based on numerical values. Could the authors make the impact of EVGP more apparent?\n\nCan the authors include more real-world datasets? As of now, all but 1 result is simulated, and additional examples would significantly strengthen the manuscript."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The MS explores an alternative method for fitting recurrent neural networks, based on Koopman operator theory.  A single-layer neural network with *random weights* is used to map the state (and possibly a control input) to a higher-dimensional space, in which the dynamics are assumed to be linear.  Since the state is assumed fully observed, the low-D state at consecutive time steps can be mapped to the high-D state, and the state matrix fit with the normal equations (and similarly for the input matrix when there is a control input).  The map back to the low-dimensional state and the output matrix can be found likewise.  The intuition from Koopman operator theory is that there exists a set, possibly infinite-dimensional, of measurement functions that evolve linearly in time.  The authors shows that this method yields computationally cheap and highly accurate models of simple nonlinear systems."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed approach fits simple nonlinear systems with high accuracy and very little computational time compared with using BPTT in RNNs.  This makes the approach a very appealing alternative for modeling and controlling such systems.  To my knowledge, the approach is novel (but see below), and open up a new perpective on random networks."
            },
            "weaknesses": {
                "value": "From the point of view of implementation, the manuscript's random RNNs are a fairly minor variation on echo-state networks.  Indeed, there is a large literature on ESNs and reservoir computing, and I am surprised that this variation has not previously been explored.  Can the authors confirm this?\n\nThe introduction of Koopman operator theory does provide a nice intuition about why a linear dynamical system should exist in a higher-dimensional space.  But it seems that the trade-off it enables---linearity for higher dimension---limits the scope of application of this approach: As the authors note, the matrix inversion operation (in the normal equations) is expensive, and it is cubic precisely the dimension of the (large) measurement space.\n\nMy understanding of the theory is that, in general, not only is this space infinite-dimensional, but also there is no way to bound the number of required dimensions.  That is, M could be arbitrarily large.  Perhaps this is addressed in the proof in Appendix B, which I did not read closely.  Can the authors provide more insight here?\n\nFinally, I found the paper somewhat hard to read.  This could be my fault, but there seem to be notational issues.  For example, near l. 139 the authors write h_t = F(h_{t-1}), and then later in the same paragraph, h_t' = F(h_t).  Is the idea that, in the first instance, h_t is the model state; where in the second instance, h_t is the *true* state (and therefore F(h_t) need not be equal to h_{t+1})?"
            },
            "questions": {
                "value": "The authors compare against ESNs on some problems and LSTM on others.  Is there any reason for these choices?\n\nCan the authors provide more intuition about the weight initialization, i.e., Eq. 4 and the equation for the distribution p_H?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}