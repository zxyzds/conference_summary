{
    "id": "i99hYFGpWl",
    "title": "Capsule Network Projectors are Equivariant and Invariant Learners",
    "abstract": "Learning invariant representations has been the longstanding approach to self-supervised learning. However, recently progress has been made in preserving equivariant properties in representations, yet do so with highly prescribed architectures. In this work, we propose an invariant-equivariant self-supervised architecture that employs Capsule Networks (CapsNets) which have been shown to capture equivariance with respect to novel viewpoints. We demonstrate that the use of CapsNets in equivariant self-supervised architectures achieves improved downstream performance on equivariant tasks with higher efficiency and fewer network parameters. To accommodate the architectural changes of CapsNets, we introduce a new objective function based on entropy minimisation. This approach which we name CapsIE (Capsule Invariant Equivariant Network) achieves state-of-the-art performance across all invariant and equivariant downstream tasks on the 3DIEBench dataset, while outperforming supervised baselines. Our results demonstrate the ability of CapsNets to learn complex and generalised representations for large-scale, multi-task datasets compared to previous CapsNet benchmarks.",
    "keywords": [
        "Capsule Networks",
        "Equivariant",
        "Invariant",
        "Self Supervised"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "Capsule Networks in place of MLP projectors can help learn equivariant and invariant representations in self supervised training without needing separate networks.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=i99hYFGpWl",
    "pdf_link": "https://openreview.net/pdf?id=i99hYFGpWl",
    "comments": [
        {
            "summary": {
                "value": "The proposed CapsIE method leverages self-supervised CapsNets to learn equivariant and invariant representations, achieving state-of-the-art performance in downstream benchmarks. It shows significant improvements in self-supervised rotation and color prediction and outperforms supervised baselines. CapsIE also showcases the implicit strengths of its capsule architecture through experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This is an intriguing work that empowers the inherent equivariance characteristics of capsule networks more effectively.\n2. The significance of the proposed method has been demonstrated using more complex benchmarks compared to those traditionally used for validation in existing capsule network research."
            },
            "weaknesses": {
                "value": "1. The citation format is incorrect. Most citations should be within parentheses, but in all instances, they are presented without them.\n2. While the methodology indeed enhances the equivariance of the original capsule network, there are still inherent limitations in the loss function that restrict its extension to real-world benchmarks. Real-world benchmarks often do not contain only a single (needed) equivariant factor; e.g., if there are five factors simultaneously, how would the loss function or the proposed self-supervised learning method handle such cases? Please discuss potential approaches for handling multiple equivariance factors simultaneously, or propose experiments to test the method's performance in such scenarios.\n3. How should the current loss function be modified when trying to learn equivariance beyond those explicitly represented as scalars, such as color and rotation? Without prior human knowledge of answers, how can implicit yet useful equivariant factors present in benchmarks be learned? Please discuss potential modifications to the proposed approach for learning more complex equivariances, or suggest experiments to explore the method's ability to discover implicit equivariance factors without explicit supervision."
            },
            "questions": {
                "value": "Please address the concerns mentioned above in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces CapsIE (Capsule Invariant Equivariant Network), a self-supervised architecture for learning both invariant and equivariant representations. CapsIE leverages Capsule Networks (CapsNets), an architecture that was introduced around 2017 and is considered somewhat outdated. The method incorporates a new entropy minimization objective function, claiming improved efficiency and downstream task performance with fewer parameters. Evaluations on the 3DIEBench dataset report state-of-the-art results, showing improvements over both previous self-supervised and supervised baselines. However, the experimental focus remains on a single dataset and task."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Equivariant self-supervised learning is an interesting topic and is not systematically investigated in the literature.\n\n- The entropy minimization objective introduced in the paper is novel. It is an interesting approach for enhancing the representation learning capacity of Capsule Networks.\n\n- Promising initial results on the 3DIEBench dataset."
            },
            "weaknesses": {
                "value": "- My first main concern is that the proposed method relies heavily on Capsule Networks, which have been shown in prior research to be generally less robust and scalable compared to CNNs for a wide range of tasks (see [A]).\n\n- My second main concern is the limited scope of experiments. The experiments are confined to a single dataset (3DIEBench). This narrow experimental scope significantly limits the impact of the results and raises concerns about the practical applicability of CapsIE.\n\n- As mentioned in the paper, given the high computational cost associated with CapsNets, the practicality of deploying this method at scale remains questionable.\n\nBased on these weaknesses (and limitations), I believe that the authors need to further develop the paper to make it suitable for publication in a top-tier conference like ICLR.\n\n[A] Gu, Jindong, Volker Tresp, and Han Hu. \"Capsule network is not more robust than convolutional network.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021."
            },
            "questions": {
                "value": "See Weaknesses!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a self-supervised invariant equivariant approach based on Capsule Networks (Sabour et al., 2017). More specifically, the paper proposes to replace the project head, typically implemented by MLPs, by a capsule-based component based on SRCaps (Hahn et al. 2019). In doing so, the invariant (the activations) and equivariant (the pose matrices) aspects of the output of the CapsNet are considered \u00a0for learning the new representation.\n\nThe proposed method is evaluated on the 3DIEBench dataset, where it pushes the performance previously obtained by the state-of-the-art. Moreover, additional experiments on image in colour prediction tasks show the equivariant capabilities of the proposed method might go beyond than what it is trained for."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The proposed method is sound. it is very clear how the characteristics of the inner-workings and representations of capsule networks match the invariance/equivariance goals targeted by the proposed method.\n    \n- The different design decisions made throughout the paper, in both the proposed method and conducted experiments, are clear and well motivated.\n    \n- The proposed method outperforms existing methods in the benchmark related to the considered 3DIEBench dataset.\n    \n- The paper has a good balance between verbal and formal presentation.\n    \n- As part of the validation of the proposed method, the paper includes an ablation study. This type of study is always helpful to assess the effect of the different parameters that define the proposed method."
            },
            "weaknesses": {
                "value": "- There is significant repetition between the contents of the Introduction (Sec.1) and Related Work (Sec.6) sections around the topics of equivariance and invariance. A revision/re-organization of this content may help improve the clarity/flow of the paper and help make additional room for extending the experimentation and/or discussion of already present experiments. Moreover, moving the description/introduction of Capsule Networks from Sec. 6.2. to an earlier section in the paper, would assist readers unfamiliar with this type of architecture. \n    \n- While the claim/goals of the paper regarding invariance and equivariance are general, the reported improvements are mostly constrained to experiments within the considered benchmark (orientation and location features from 3DIEBench). While the experiments on the colour prediction task (Sec. 5.2) are a good step towards the assessment of the general capabilities of the proposed method towards concurrently preserving invariance/equivariance, it is still too limited as to serve as grounds for generalization. The validation of the proposed method would strengthen by an extended evaluation considering other predictions problems (e.g. segmentation/localization of specific parts of the objects of interest, Skeleton-based pose estimation, in-painting, input reconstruction, etc. ) to assess the capabilities of the proposed method towards joint invariance/equivariance.\n    \n- While having an ablation study is desirable, it is currently missing an evaluation of the effect of the $\\lambda$ weights applied to the loss functions that are part of the proposed method."
            },
            "questions": {
                "value": "- The results reported in Table.2 for the Color Prediction problem based on the Encoder Representation of SIE are significantly low. May you elaborate on why this is the case?\n    \n- May you indicate how the $\\lambda$ weights for the loss functions introduced in Sec. 3.2 were selected?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}