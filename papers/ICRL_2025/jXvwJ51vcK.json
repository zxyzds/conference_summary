{
    "id": "jXvwJ51vcK",
    "title": "Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation",
    "abstract": "Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to segment novel categories with minimal annotated support samples. While existing FS-PCS methods have shown promise, they primarily focus on unimodal point cloud inputs, overlooking the potential benefits of leveraging multimodal information. In this paper, we address this gap by introducing a cost-free multimodal FS-PCS setup, utilizing textual labels and the potentially available 2D image modality. Under this easy-to-achieve setup, we present the MultiModal Few-Shot SegNet (MM-FSS), a model effectively harnessing complementary information from multiple modalities. MM-FSS employs a shared backbone with two heads to extract intermodal and unimodal visual features, and a pretrained text encoder to generate text embeddings. To fully exploit the multimodal information, we propose a Multimodal Correlation Fusion (MCF) module to generate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module to refine the correlations using text-aware semantic guidance. Additionally, we propose a simple yet effective Test-time Adaptive Cross-modal Calibration (TACC) technique to mitigate training bias, further improving generalization. Experimental results on S3DIS and ScanNet datasets demonstrate significant performance improvements achieved by our method. The efficacy of our approach indicates the benefits of leveraging commonly-ignored free modalities for FS-PCS, providing valuable insights for future research. The code will be released.",
    "keywords": [
        "few-shot 3D point cloud semantic segmentation",
        "multimodality"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=jXvwJ51vcK",
    "pdf_link": "https://openreview.net/pdf?id=jXvwJ51vcK",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates the few-shot 3D point cloud semantic segmentation task. It proposes a cost-free multimodal FS-PCS setup. Based on this setup, the authors introduce MM-FSS to utilize the multimodal information to help with few-shot learning. The model includes MCS to generate multimodal correlations and MSF to refine the correlations by textual guidance. To mitigate training bias, the authors also propose a TACC technique to adaptively calibrate predictions at test time. The experiments demonstrate its effectiveness."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "+ The paper is well-written and easy to follow, with clear motivations for each proposed design.\n+ It introduces the first multimodal few-shot 3D segmentation setting, which is cost-free and doesn\u2019t increase labeling effort, consistent with standard few-shot tasks. The idea of leveraging available free modalities to help few-shot learning could provide valuable insights for the field.\n+ The proposed model is novel and well-justified. The MCS and MSF modules use different modalities by exploiting cross-modal visual correlations and textual semantics, respectively. The TACC module is parameter-free and used at test time to calibrate predictions adaptively based on support samples.\n+ The performance gains are substantial, and the experiments are thorough. Detailed ablation studies and visualizations are provided and effectively validate the proposed design choices and justify their motivations."
            },
            "weaknesses": {
                "value": "Certain paragraphs could be more concise. For example, the paragraph (lines 235-240) explains the two training steps. Then the paragraph (lines 259-266) writes about these two steps again. These two parts could be merged to improve conciseness."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the Few-Shot 3D Point Cloud Semantic Segmentation task by using multimodality information. Specifically, it uses 2D image modality to  extract additional intermodal features apart from the unimodal visual features. It also uses text embedding to refine the multimodal correlations generated by the proposed Multimodal Correlation Fusion module. Moreover, it presents the Test-time Adaptive Cross-modal Calibration technique to improve the generalization capability. The experimental results show that the proposed methods can achieve higher performance than previous works."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The writing is good, making this paper easy to follow.\n\n- The performance is good.\n\n- The experiments are adequate.\n\n- The introduction of the multimodality information is useful."
            },
            "weaknesses": {
                "value": "- How the design of the MSF module can achieve the improvement for the correlation. Please give more explanations."
            },
            "questions": {
                "value": "Please answer the question in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes a multimodal few-shot segnet (MM-FSS) to leverage multimodal information from textual labels and 2D images. Specifically, they propose multimodal correlation fusion (MCF) to fuse the multimodal correlations and enhance the fused features by multimodal semantic fusion (MSF) with textual information. Besides, the authors proposed test-time adaptive cross-modal calibration (TACC) to calibrate the predictions during test time. Experimental results on both S3DIS and ScanNet show the superior performance of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. It makes sense to use complementary information from multimodal inputs for point cloud segmentation.\n2. The ablation studies on ScanNet prove the effectiveness of the proposed models, including MCF, MSF, and TACC.\n3. The features from 2D images are learned during the pre-training step. In this way, the proposed method does not rely on 2D inputs during inference.\n4. The proposed MM-FSS achieves promising performance on two benchmark datasets compared to the state-of-the-art methods."
            },
            "weaknesses": {
                "value": "1. The idea of using multimodal information for 3D point cloud segmentation is not new. As noticed by the authors, such an idea has been explored for fully supervised or more challenging tasks like open-vocabulary segmentation. It is unclear why the prior few-shot methods only use unimodal information. It would be better to provide more discussions on the difficulties or challenges when using multimodal features for few-shot segmentation.\n\n2. The claim about \u201ccost-free\u201d is ambiguous as \u201ccost\u201d may mean many things, such as training cost. I don't think it is necessary to claim the \"cost-free\"."
            },
            "questions": {
                "value": "Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In the paper, the MultiModal Few-Shot SegNet was proposed, which segments novel categories with minimal annotated support samples utilizing textual labels and the potentially available 2D image modality.  The main contributions are as follows: 1) To incorporate the textual modality of category names and the 2D image modality in few-shot 3D learning scenarios without additional annotation cost; 2) A new model, MM-FSS, was introduced, and information from different modalities was utilized. Experiments validated the value of the proposed method and the efficacy of the proposed method across different few-shot settings."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Based on the learning method, a large number of annotations are required for the identification and segmentation of new target categories in the point cloud, which is very time-consuming and laborious. The paper is of great significance to reduce the data annotation process through the combination of text and images.The MM-FSS model exhibits superior generalization capabilities, especially in few-shot settings, providing valuable insights for future research directions in the field."
            },
            "weaknesses": {
                "value": "1. The paper's description of the 2D image modality's integration into the multi-modal FS-PCS setup could be more detailed, providing clearer insights into how the 2D images are leveraged to enhance the model's performance without additional annotation costs.\n2. The article does not offer a detailed analysis of the performance enhancements specifically attributable to the multi-modal information fusion during the modeling process, making it challenging to assess the individual impact of textual and 2D image modalities on few-shot segmentation performance.\n3. The existing FS-PCS datasets are generally on a small scale, the practicability and effectiveness of the method in this paper need to be further verified in the future."
            },
            "questions": {
                "value": "1. In Line 110\uff0cii should be iii.\n2. Whether there is a comparison of the computational complexity and computational efficiency of different algorithms\uff1f\n3. The significance of the important regions depicted in Figure 3 is unclear. The authors should provide additional details to enhance reader understanding.\n4. The authors input multiple images into the VLM; how do they determine the viewpoints for the RGB images fed into the VLM?\n5. The authors demonstrate a notable performance increase by introducing textual information into the query. I am curious if there is potential to also incorporate textual or other types of information into the support set to achieve further gains.\n6. The selection of support appears somewhat random. If the authors could provide some theoretical guidance in the appendix regarding this selection process, I believe it would greatly enhance the significance of this work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}