{
    "id": "6QBHdrt8nX",
    "title": "SafetyAnalyst: Interpretable, transparent, and steerable LLM safety moderation",
    "abstract": "The ideal LLM content moderation system would be both structurally interpretable (so its decisions can be explained to users) and steerable (to reflect a community's values or align to safety standards).  However, current systems fall short on both of these dimensions. To address this gap, we present SafetyAnalyst, a novel LLM safety moderation framework. Given a prompt, SafetyAnalyst creates a structured ``harm-benefit tree,'' which identifies 1) the actions that could be taken if a compliant response were provided, 2) the harmful and beneficial effects of those actions (along with their likelihood, severity, and immediacy), and 3) the stakeholders that would be impacted by those effects.  It then aggregates this structured representation into a harmfulness score based on a parameterized set of safety preferences, which can be transparently aligned to particular values. Using extensive harm-benefit features generated by SOTA LLMs on 19k prompts, we fine-tuned an open-weight LM to specialize in generating harm-benefit trees through symbolic knowledge distillation. On a comprehensive set of prompt safety benchmarks, we show that our system (average F1=0.75) outperforms existing LLM safety moderation systems (average F1$<$0.72) on prompt harmfulness classification, while offering the additional advantages of interpretability and steerability.",
    "keywords": [
        "AI safety",
        "AI ethics",
        "LLM content moderation",
        "interpretability",
        "pluralistic alignment"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6QBHdrt8nX",
    "pdf_link": "https://openreview.net/pdf?id=6QBHdrt8nX",
    "comments": [
        {
            "title": {
                "value": "Clarification of potential misunderstandings"
            },
            "comment": {
                "value": "We sincerely thank Reviewer Hbfe for their detailed feedback and for highlighting areas where our explanations can be improved. However, there appear to have been several misunderstandings, which we address below.\n\n1. The reviewer asked: _\u201cA smaller student LM (by the way what kind of LM are you using here? BERT-based?)\u201d_. Our base student LM is Llama-3.1-8B-Instruct, which we state multiple times in the main text:\n   - Line 83: \u201cvia supervised fine-tuning of Llama-3.1-8B-Instruct\u201d\n   - Line 126: \u201cwe fine-tuned an open-weight LM (Llama-3.1-8B-Instruct) to specialize\u201d\n   - Line 207: \u201cLM (Llama-3.1-7B-Instruct) to specialize in the tasks\u201d (typo).\n\n2. The reviewer stated: _\u201cSafetyReporter was never mentioned before Table 1\u201d_. We introduced SafetyReporter in Figure 1, both in the figure and its captions: _\u201ctwo specialist models \u2014 one to generate harms and one to generate benefits (together named SAFETYREPORTER)\u201d_. Regarding _\u201cWhat is the difference between SafetyReporter and SafetyAnalyst?\u201d_, please refer to Figure 1\u2019s captions and the paragraph below Table 1 in Section 2.2.\n\n3. The reviewer claimed: _\u201cAll the paper claims made in the method section refer to a table in the appendix\u201d_. We request clarification on _\u201cthe method section\u201d_, as our manuscript lacks a section named _\u201cMethods\u201d_. Please specify which of the two appendix tables (Tables 4 and 5) is being referenced.\n\n4. The reviewer wrote: _\u201cIn Section 2.3, the authors claim that they propose a new aggregation algorithm. I have the feeling that this is just a mere multiplication between some predefined weights. Are these weights somewhat learned? Are you also constantly updating $f$, $g$, and $h$ when $W$ and $\\gamma$ are updated?\u201d_ Section 2.3 clarifies that the feature weights are fitted to a given label distribution by maximum-likelihood estimation, meaning they are optimized, not predefined or constantly updated. We will revise this section for better clarity.\n\n5. The reviewer asked: _\u201cWho's the teacher and who's the student model in Table 2?\u201d_ In Line 209, we note _\u201cteacher models (SOTA LLMs)\u201d_, referring to those listed in Lines 76-77: _\u201cSOTA LLMs (GPT-4o, Gemini-1.5-Pro, Llama-3.1-70B-Instruct, Llama-3.1-405BTurbo, and Claude-3.5-Sonnet)\u201d_. Lines 214-215 specify the students as _\u201cThe two student models that specialize in harm and benefit feature generation are collectively named \u2018SAFETYREPORTER.\u2019\u201d_\n\n6. The reviewer wrote: _\u201cIt makes little sense to say that a model's performance is $F1 \\geq 84.7$. Is the $F1 = 84.7$ as what it is shown in the table? What are you trying to say in Lines 290-292?\u201d_ Aggregation models trained on harm-benefit trees generated by all models achieved high classification performance, measured by $F1$, AUPRC, and AUROC, reported in Table 2. We request that the reviewer please clarify why \u201cit makes little sense to say that the model\u2019s performance is $F1\u226584.7$\u201d\u2014we used this number since it is the lowest $F1$ among all models reported in Table 2, so the $F1$ scores in Table 2 are all $\u226584.7$.\n\n7. The reviewer asked: _\u201cWhy is the conclusion a brief paragraph? Make it a section where you summarize your paper and future works.\u201d_ The Discussion section includes the Conclusion subsection, summarizing our findings and future works. We can rename this section to _\u201cConclusion\u201d_ if preferred.\n\n8. The reviewer wrote: _\u201cThe experiments feel cut short where there is no clear connection between the harm-benefit-trees and the performances. Where do harm-benefit-trees come into play here. Do the authors really need these trees?\u201d_ Sections 2.3 and 2.4 detail how the features in the harm-benefit tree are aggregated numerically and translated into a safety label that was used for evaluation in our experiments (i.e., the harm-benefit tree serve as the input to the aggregation algorithm, which outputs a harmfulness score that is then converted into a binary label). Nonetheless, we agree that further studies showing the usefulness of different features in the harm-benefit trees would be compelling, so we are working on supplementing the manuscript with ablations of different types of features in the trees (e.g., actions, effects, etc.).\n\n9. The reviewer wrote: _\u201cIn the appendix Table 5, it is not clear what the authors are measuring here to assess the agreement with human annotators.\u201d_ Lines 814-816 explain: _\u201cTo obtain the agreement rates, we computed the proportion of positive ratings (e.g., very plausible, somewhat plausible, and reasonable) among all positive and negative ratings.\u201d_ We will move this explanation to the table captions for clarity.\n\nWe hope this clarification resolves the misunderstandings and respectfully ask Reviewer Hbfe to reconsider their assessment in light of this explanation while we work on revising the manuscript. Once we have updated the manuscript, we will comment again with a point-by-point response to all reviewers\u2019 comments. Meanwhile, we appreciate your time and effort in revisiting the above points."
            }
        },
        {
            "summary": {
                "value": "The authors introduce SafetyAnalyst, a language model solution to LLM content moderation. Critically, SafetyAnalyst is both interpretable and tunable to reflect different safety values while achieving SOTA performance on prompt safety benchmarks. The authors achieve this by training two open-weight LM to identify trees of stakeholders, actions, and effects from text prompts. One focuses on harms and the other on benefits. The model output is tunable to different safety values via a parameterized feature aggregation algorithm for calculating harmfulness. The output is interpretable due to the generated harm-benefit trees which consist of chains of stakeholders to actions to effects."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors introduce a novel interpretable moderation system which achieves SOTA performance in a dense field. SafetyReporter provides structured output which helps humans understand the harmfulness of a prompt.\nThe paper presents a wealth of both benchmark datasets and models. This provides clear evidence that SafetyReporter is strong in a wide variety of content moderation.\nThe SafetyAnalyst framework is clearly explained. The authors make great use of graphics explaining the overall framework and the role of the language model. Additionally, the discussion of value alignment displays the flexibility of the system."
            },
            "weaknesses": {
                "value": "In the related work section there is room for more discussion on the differences between existing content moderation systems. Both how the baseline models differ from each other and how they differ from SafetyReporter and SafetyAnalyst as a whole. Discussion beyond \u201c[existing content moderation systems] internal decision mechanisms are challenging to interpret\u201d would strengthen the authors\u2019 claims about the importance of interpretability in content moderation.\nMore experiments on the steerability of the content moderation system would be beneficial. As the paper stands, the authors do a good job explaining how to align the system with a dataset. However, experimentation about how alignment would make the system more effective for specific tasks is missing. For example, the authors could align the aggregation weights of SafetyAnalyst to a held-out portion of a benchmark dataset and look at how performance improves."
            },
            "questions": {
                "value": "Overall the paper is strong however a few areas could use clarification.\nIn the limitation section the authors discuss the tradeoff between interpretability and inference time. Quantifying the difference in inference time between baselines, SafetyAnalyst, and LLMs would be beneficial to weigh the value of this tradeoff. \nAdditionally, more insight on the implications of GPT4 outperforming both SafetyAnalyst and baselines would be helpful.\nIn the evaluation results section the authors\u2019 mention that SafetyReporter was not aligned to any of the benchmark datasets. How does performance differ if alignment is done? Additionally, I would be interested in how aggregation feature weights change from baseline to baseline."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The SAFETYANALYST framework is a system for moderating content that aims to be both interpretable and adaptable to specific values. It uses harm-benefit trees to evaluate actions and their effects, producing a harmfulness score that aligns with safety preferences, and it outperforms existing systems on prompt safety benchmarks. They have considered various stakeholders and compared the results with five well-known LLM solutions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The use of both harm-benefit trees and symbolic knowledge distillation is the key element in this research."
            },
            "weaknesses": {
                "value": "The idea is promising and the paper is focusing on a major issue of AI Safety that is scientifically sounds. Still, there is room to improve it to reach a high level of originality, quality, clarity, and significance. The following comments can be addressed to improve the paper from all mentioned aspects:\n1- The literature review of the paper could be improved, there are various papers on LLM safety. For example, \"SafeLLM\", \"TrustLLM\", etc and most of them focus on the same issue. For example, SafeLLM goes even deeper and focuses on Domain-specific or in-context safety.\n2- The author must provide results regarding computation complexity and delay in response time in the provided model."
            },
            "questions": {
                "value": "1- How the proposed solution is robust again jail breaking and prompt injection.\n2- Considering the problem of in-context reward hacking, how the proposed method could help us to avoid such issues?\n3- I think more in depth research is needed to gain a proper novelty and originality. For example, one may consider formation of concepts in LLMs and try to fix the issue in the that level considering research like this: https://www.anthropic.com/research/mapping-mind-language-model"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces SafetyAnalyst -- although it's not clear why there is also a SafetyReporter as a contribution to the paper -- that produces a harm-benefit-tree and aggregates its features mathematically to accomodate different safety preferences. The authors tackle the current limitations of LLM-based moderation systems in flagging possibly harmful prompts in presence of OOD samples which leads these classification systems astray. The authors claim that their SafetyAnalyst satisfies the two LLM moderation desiderata, i.e., interpretability and steerability, although the experimental section does not necessarily provide further details on the support (or not) of these desiderata."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* The scenario looks interesting and challenging and the harm-benefit tree approach where one has LLM teachers such that a smaller student LM (by the way what kind of LM are you using here? BERT-based?) model learns how to \"moderate\".\n* The paper is very suitable for industrial applications rather than being theory-intensive."
            },
            "weaknesses": {
                "value": "1. It is difficult to interpret the numbers in Table 1. What do they mean? Also SafetyReporter was never mentioned before Table 1. What is the difference between SafetyReporter and SafetyAnalyst?\n\n2. The paper feels as if it was written in haste, where the true insights of the paper are missed for some reason. For example, I would've included a list of contributions right after the introduction section to give the reader a brief summary of what they should expect from the paper.\n\n4. It's unfortunate that all the paper claims made in the method section refer to a table in the appendix.\n\n5. In Section 2.3, the authors claim that they propose a new aggregation algorithm. I have the feeling that this is just a mere multiplication between some predefined weights. Are these weights somewhat learned? Are you also constantly updating $f$, $g$, and $h$ when $W$ and $\\gamma$ are updated? This section needs heavy revision.\n\n6. Who's the teacher and who's the student model in Table 2?\n\n7. I makes little sense to say that a model's performance is $F_1 \\geq 84.7$. Is the $F_1 = 84.7$ as what it is shown in the table? What are you trying to say in lines 290-292?\n\n8. Although, as per ICLR reviewer's guidelines, not being SoTA for an approach is not reason for rejection, not having at least similar performances isn't great. There's a 6.2 point difference in terms of F1 scores (GPT-4 has 81.6 and SafetyAnalyst has 75.4). This would be justifiable if SafetyAnalyst were more interpretable than the black-box GPT-4. I fail to see why one would prefer SafetyAnalyst rather than GPT-4o.\n\n9. Table 3 reports averages of F1 scores on all datasets. Stating that an average score is better than the other doesn't say much. In fact, if you look at WildGuard and SafetyReporter, the former is constantly better than the latter on each dataset. It just has a performance drop in SORRY-Bench which makes the average F1 plummet. This is a classic scenario where averages aren't trustworthy and, in this scenario, make SafetyAnalyst the second-best performing method after GPT-4, when, instead, WildGuard should be. Here, I would suggest the authors to perform a Friedman test [1] with a post-hoc Bonferroni-Dunn test to assess whether SafetyReporter is better than the rest of the SoTA models without considering GPT-4. Here, one has to use multiple runs over the same dataset -- say 10+ -- to have several F1 scores for each dataset and then perform the Friedman test to tell whether the overall F1 averages are different among the SoTA methods and SafetyReporter. Then, one performs the Bonferroni-Dunn test where the control group is SafetyReporter and assess whether its average F1 score is statistically and significantly different from the rest. For each of the other SoTA methods, except GPT-4, this test gives a p-value which shows us if the method is \"better\" than the other. Using a p-value of $0.05$ would be sufficient. I expect that SafetyReporter is better than all but WildGuard, which undermines the paper's claim that ''**SafetyReporter outperforms existing LLM safety moderation systems on prompt harmfulness classification.**''\n\n10. Why is the conclusion a brief paragraph? Make it a section where you summarize your paper and future works.\n\n11. The experiments feel cut short where there is no clear connection between the harm-benefit-trees and the performances. Where do harm-benefit-trees come into play here. Do the authors really need these trees? If so, show it somehow. If there is a lack of space, I'd argue that Figure 2 should be rethought. Too much unsupported detail.\n\n12. In the appendix Table 5, it is not clear what the authors are measuring here to assess the agreement with human annotators. Are the authors measuring Cohen's kappa?\n\n\n[1] Friedman M. The use of ranks to avoid the assumption of normality implicit in the analysis of variance. Journal of the american statistical association. 1937 Dec 1;32(200):675-701."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}