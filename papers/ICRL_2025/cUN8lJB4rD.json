{
    "id": "cUN8lJB4rD",
    "title": "Tight Time Complexities in Parallel Stochastic Optimization with Arbitrary Computation Dynamics",
    "abstract": "In distributed stochastic optimization, where parallel and asynchronous methods are employed, we establish optimal time complexities under virtually any computation behavior of workers/devices/CPUs/GPUs, capturing potential disconnections due to hardware and network delays, time-varying computation powers, and any possible fluctuations and trends of computation speeds. These real-world scenarios are formalized by our new universal computation model. Leveraging this model and new proof techniques, we discover tight lower bounds that apply to virtually all synchronous and asynchronous methods, including Minibatch SGD, Asynchronous SGD (Recht et al., 2011), and Picky SGD (Cohen et al., 2021). We show that these lower bounds, up to constant factors, are matched by the optimal Rennala SGD and Malenia SGD methods (Tyurin & Richt\u00e1rik, 2023).",
    "keywords": [
        "nonconvex optimization",
        "lower bounds",
        "parallel methods",
        "asynchronous methods",
        "convex optimization"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-15",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cUN8lJB4rD",
    "pdf_link": "https://openreview.net/pdf?id=cUN8lJB4rD",
    "comments": [
        {
            "title": {
                "value": "Official Comment by Authors (Part 2)"
            },
            "comment": {
                "value": "We now respond to the main concern \"Lastly and most importantly, I do not think this paper has sufficient delta from earlier work\":\n\n> Not having any further examples beyond 6.5 for the heterogeneous setting does not help the cause.\n\nWe agree, so we\u2019ve added Example 6.6 to the paper for the heterogeneous setting.\n\n> Since the problem setup is quite complicated, it is important for the authors to justify that the added generality actually leads to some meaningful implications.\n\nIn total, we have five distinct examples in both homogeneous and heterogeneous settings, through which we can derive explicit formulas for the optimal time complexity. Examples 5.4 and 6.5 are included to demonstrate the recovery of the previous result [1]. The purpose of three additional scenarios, Examples 5.5, 5.6, and 6.6 (new), is to offer justification and insights into the obtained time complexities.\n\nThe primary challenge is that explicit and elegant formulas (which the reviewer expects) can only be obtained for a limited set of scenarios. However, using Theorems 5.3 and 6.4, we can find the optimal time complexities numerically! Note that we\u2019ve added Figures 1 and 2 that explain the difference between the fixed computation model and the universal computation model. The fixed computation model is very limited and captures constant computation powers (see Fig 1). The universal computation model is rich and includes virtually all possible computation scenarios (see Fig. 2 (a), Fig. 2 (b), Fig. 2 (c)). The delta between the computation models is huge. We also provide numerical computations of the time complexities and attach a jupyter notebook that reproduces the plots and the calculations (see supplementary material).\n\nNote that there was some skepticism in the literature (see \u201cOther delay models,\u201d p. 3 in [6], \u201cRelated Works,\u201d p.2 in [7]) that Rennala SGD and Malenia SGD are optimal in other regimes, and it was an open problem that we solved. The previous works [6,7] are on the same page that the fixed computation model is limited.\n\n> And there seem to be a major reuse of proof techniques between the two papers. The proof of Theorem 5.1 up to equation (25) is almost a line-by-line reproduction of the existing analysis in [1]. While I am in no way suggesting any ill intent from the authors, I want to hear from the authors what are the main technical difficulties going from [1] to their proofs.\n\nNote that this is the standard \"proof opening\" used in all papers that develop lower bounds in the nonconvex setting (see [1,2,3,4,5] and many more other papers). However, after the opening, the proof is completely different. \n1) Note that all lower bounds in stochastic optimization ultimately reduce to the concentration analysis of the sum of random variables. [1] approach this by analyzing the sum $\\sum_{i=1}^T \\min_{j \\in [n]} \\tau_j \\eta_{ij},$ where $\\eta_j$ are i.i.d. geometric random variables. In our case, we cannot directly apply this reduction anymore because the computation powers vary over time. Therefore, we found a non-trivial modification: we have to reduce the problem to the concentration analysis of the sum of indicators: $\\sum_{j=1}^T \\mathbb{I} [\\eta_j > \\frac{1}{p}]$ and investigate this sum. (we added this clarifications to the revision)\n2) In the heterogeneous setting, the construction and allocation of copies of the \"bad\" functions are ''dynamic\" and change through time. We have never seen such a construction in the literature, including [1-5].\n3) The universal computation model itself is an independent and non-trivial contribution that significantly generalizes the fixed computation model [1]. Introducing the new computation model, developing new proof techniques, and integrating the new computation model with these proof techniques are all non-trivial tasks.\n\nHence, we believe the discrepancy between the proof techniques is substantial and non-trivial.\n\n---\n\nOverall, we agree with many comments by the reviewer and have already added the corresponding fixes/clarifications/plots to the paper and added an additional example in the heterogeneous. Unfortunately, it is very difficult to get exact formulas in most cases, but we explain that it is always possible numerically. We added new figures and clarified the proof techniques that highlight a huge gap between the previous and the new approach. We kindly ask the reviewer to read our comments and changes in the paper and reevaluate our paper if the reviewer believes we have resolved all concerns. Please ask us more questions. \n\n---"
            }
        },
        {
            "title": {
                "value": "Official Comment by Authors (Part 1)"
            },
            "comment": {
                "value": "Thank you for the suggestions and comments!\n\n> In terms of organization, I suggest that the examples should be moved forward. Currently, the statements of Theorem 5.1 and 6.1 are very long and hard to digest.\n\nNote that before Theorem 5.1, we provided an informal version exactly to facilitate understanding of the main results.\n\n> Having the examples immediately following these theorems would be appreciated.\n\nIndeed, we do not provide examples just after Theorem 5.1, but on the next page, after the non-less important discussion explaining that Rennala SGD is an optimal method, one can find our examples.\n\n> Also, since Them 5.1 and 6.1 are implicit due to the generality of the setting, it could be useful to make some plots for a specific problem instance ...\n\nWe agree! Please see the revision of our paper. We've added Figures 1 and 2.\n\n> For equations (12), (13) and (15), I think the minimum is always at $m = 1$ and therefore redundant?\n\nThis is not true in almost all cases. Let us explain this part and consider (12):\n$$\\bar{t} = \\min\\limits_{m \\in [n]} \\left(\\frac{1}{m}\\sum\\limits_{i=1}^m v_{\\pi_i}\\right)^{-1} \\left(\\frac{L \\Delta}{\\varepsilon} + \\frac{L \\Delta \\sigma^2}{m \\varepsilon^2}\\right).$$\nAssume that the performances $v_i = v$ are equal. Then\n$$ \\bar{t} = \\min\\limits_{m \\in [n]} \\left(\\frac{1}{m}\\sum\\limits_{i=1}^m v\\right)^{-1} \\left(\\frac{L \\Delta}{\\varepsilon} + \\frac{L \\Delta \\sigma^2}{m \\varepsilon^2}\\right) = \\frac{1}{v} \\min\\limits_{m \\in [n]} \\left(\\frac{L \\Delta}{\\varepsilon} + \\frac{L \\Delta \\sigma^2}{m \\varepsilon^2}\\right) = \\frac{1}{v} \\left(\\frac{L \\Delta}{\\varepsilon} + \\frac{L \\Delta \\sigma^2}{n \\varepsilon^2}\\right),$$ \nwhere the minimum is attained with $m = n,$ which is reasonable because $m$ is an ''effective number'' of workers that contribute to the optimization process [1]. \n\nNext, let computation performances vary according to $v_i = \\frac{1}{\\sqrt{i}},$ then\n$$ \\bar{t} = \\min\\limits_{m \\in [n]} \\left(\\frac{1}{m}\\sum\\limits_{i=1}^m \\frac{1}{\\sqrt{i}}\\right)^{-1} \\left(\\frac{L \\Delta}{\\varepsilon} + \\frac{L \\Delta \\sigma^2}{m \\varepsilon^2}\\right) = \\Theta \\left(\\min\\limits_{m \\in [n]} \\sqrt{m} \\left(\\frac{L \\Delta}{\\varepsilon} + \\frac{L \\Delta \\sigma^2}{m \\varepsilon^2}\\right) \\right) = \\Theta \\left(\\min\\limits_{m \\in [n]} \\left(\\frac{L \\Delta \\sqrt{m}}{\\varepsilon} + \\frac{L \\Delta \\sigma^2}{\\sqrt{m} \\varepsilon^2}\\right)\\right) = \\Theta \\left( \\max[\\frac{\\sigma L \\Delta}{\\varepsilon^{3/2}},\\frac{L \\Delta \\sigma^2}{n \\varepsilon^2}] \\right)$$ where the minimum is attained with $m = \\min[ \\left\\lceil \\frac{\\sigma^2}{\\varepsilon} \\right\\rceil, n ]$.\n\nIn total, we have demonstrated at least two cases where $m \\neq 1$ in practical scenarios. \n\n> The proof sketch in Section 7 is not helpful at all. In particular, I want to see the \"worst case function\" being spelled out. ...\n\nWe use the standard \"worst case function\" from the nonconvex world [2], which was used in numerous papers that investigate lower bounds. See [3,4,5,1]. Following all these papers, we hide the construction in the appendix (Section D) because it is somewhat well-known and, unfortunately, takes a lot of place. We believe repeating the construction of this function in the main parts of every paper is redundant. The best description one can find in the original paper [2]. But while the construction has a non-trivial structure, it should be noted that the worst-case function is essentially a variation of Nesterov's chain function (see the celebrated \"Lectures on Convex Optimization\" book by Yu. Nesterov).\n\n> Also, on line 486, I think you meant \"first coordinate.\" The random variables should be defined earlier in that paragraph.\n\nWe fixed this part. Thank you. Please check the changes in blue. Note that we moved \"Proof Techniques\" to the appendix to give space for Figures 1 and 2.\n\n---\n\n[1]: A. Tyurin, P. Richtarik. Optimal Time Complexities of Parallel Stochastic Optimization Methods Under a Fixed Computation Model,\n\n[2]: Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points i. Mathematical Programming, 184(1):71\u2013120, 2020.\n\n[3]: Xinmeng Huang, Yiming Chen, Wotao Yin, and Kun Yuan. Lower bounds and nearly optimal algorithms in distributed learning with communication compression. Advances in Neural Information Processing Systems, 2022.\n\n[4]: Yucheng Lu and Christopher De Sa. Optimal complexity in decentralized training. In International Conference on Machine Learning, pp. 7111\u20137123. PMLR, 2021.\n\n[5]: Kumar Kshitij Patel, Lingxiao Wang, Blake Woodworth, Brian Bullins, Nathan Srebro. Towards Optimal Communication Complexity in Distributed Non-Convex Optimization. Advances in Neural Information Processing Systems, 2022.\n\n[6]: R. Islamov et al. AsGrad: A sharp unified analysis of asynchronous-SGD algorithms AISTATS 2024\n\n[7]: A. Attia et al. Faster Stochastic Optimization with Arbitrary Delays via Asynchronous Mini-Batching"
            }
        },
        {
            "title": {
                "value": "Official Comment by Authors"
            },
            "comment": {
                "value": "Thank you for the questions and comments!\n\n> The theoretical results, while comprehensive, may be too complex to be applied to practical algorithms. This complexity could limit their usability for practitioners who require simpler and more accessible tools for system design and analysis. Consequently, the tight lower bounds for time complexities are primarily of theoretical interest and provide limited insight into the potential performance improvements achievable with parallel stochastic optimization methods.\n\nActually, our results have very important applications for practitioners. First of all, these theoretical results say that Rennala SGD and Malenia SGD are optimal methods under arbitrary computation dynamics. Before that, it was only known for the fixed computation model [1]. There was some skepticism in the literature (see \u201cOther delay models,\u201d p. 3 in [3], \u201cRelated Works,\u201d p.2 in [2]) that Rennala SGD and Malenia SGD are optimal in other regimes, and it was an open problem that we solved. Next, as we noted in Lines 352-354 and Lines 473-476, these methods achieve the optimal time complexity without knowing $\\{V_i\\}.$ They automatically work in the optimal regimes, and practitioners can safely use Rennala SGD-like and Malenia SGD-like algorithms regardless of the theory. \n\nMoreover, in the supplementary material, we've added an implementation that computes the optimal time complexities numerically, which may help practitioners.\n\nRegarding the complexity of the time complexities, it might simply be fundamentally infeasible to derive simpler formulas.\n\n> The paper does not provide sufficient discussion on the limitations of the universal computation model. Are there any properties in parallel or distributed systems that fall outside the scope of this universal model?\n\nThis is a good question. To be honest, we truly believe that our computation model covers almost all possible scenarios, and there are no obvious limitations right now. We've added new plots to the paper: please see Figure 2, where one can see that our new model is versatile. However, one possible limitation is that our model assumes that computation performances are statistically independent of the randomness that we get when calculating stochastic gradients. We've just added this note to the \"Conclusion\" section (please check the revision).\n\n> Does the computation power characterize server\u2019s computation speed?\n\n> In Methods 3 and 4, is the communication time between the workers and the server taken into account? In practical systems, it may take a significantly longer amount of time for a worker to transmit a gradient than to compute it. How does the universal computation model account for this?\n\nThank you for the thoughtful questions.\n\nWe work with $n$ workers/nodes/servers, which have different computation powers. If you are asking about a central server that aggregates and orchestrates the workers, then we do not take its performance into account in this work because, in many scenarios, the computation of stochastic gradients is the main bottleneck.\n\nTaking into account the communication time is undoubtedly a logical direction for future research that deserves separate investigation. We feel that even our current findings are complex and require notable effort and a separate paper. In the case of the communication bottleneck, one can introduce a \"universal *communication* model\" using our construction and investigate this scenario also, but it is a non-trivial next research step.\n\n> Line 209: \n\nThere is a typo. Thank you. We have to fix this part to $O \\in \\mathcal{O}(f)$ without $\\mathcal{D}.$ We've just fixed it (please check the revision).\n\n---\n\nWe hope we have addressed all potential concerns and questions raised by the reviewer and added the required changes to the paper (please check the revision). If you feel this is the case, we kindly ask you to consider reevaluating our work.\n\n---\n\n[1]: Optimal Time Complexities of Parallel Stochastic Optimization Methods Under a Fixed Computation Model, A. Tyurin et al.\n\n[2]: AsGrad: A sharp unified analysis of asynchronous-SGD algorithms, R. Islamov et al.\n\n[3]: Faster Stochastic Optimization with Arbitrary Delays via Asynchronous Mini-Batching, A. Attia et al."
            }
        },
        {
            "title": {
                "value": "Official Comment by Authors"
            },
            "comment": {
                "value": "Thank you! Let us respond to the questions:\n\n> The paper lacks numerical results and experiments, which would strengthen its contributions.\n\nOur main contribution is related to lower bounds. We do not develop new methods. The optimal Rennala SGD and Malenia SGD methods have already been successfully tested in [1]. However, we've just added numerical illustrations and computation of the optimal time complexities to the paper in Figure 2.\n\n> Do you also consider a model for communication bandwidth? From what I understand, when a worker computes a stochastic gradient, it is broadcast to all other workers. Do you account for situations where there is a communication bandwidth bottleneck or where, due to stragglers, communication may completely fail, preventing the message from being broadcast?\n\nThank you for the good question. Indeed, this is a natural future research question that requires independent work. We believe that even the current results are non-trivial and need a lot of effort and a full paper to investigate. One can use our universal computation model and develop a \"universal *communication* model.\"\n\n\\[1\\]: Alexander Tyurin and Peter Richtarik. Optimal time complexities of parallel stochastic optimization \u00b4\nmethods under a fixed computation model. Advances in Neural Information Processing Systems,\n2023"
            }
        },
        {
            "title": {
                "value": "Official Comment by Authors"
            },
            "comment": {
                "value": "Thank you for the very positive review! We now respond to the weaknesses and questions: \n\n> See my question about [3] below.\n\nTheir lower bound can not capture simple scenarios when, for instance, worker 1 is the fastest in the first 10 seconds, then worker 1 is the slowest in the second 10 seconds. Their scenario can not capture fluctuation in computation times. Note that we've added Figures 1 and 2 to the paper to visualize the difference.\n\nThere are many places where the proofs are different: \n1) Note that all lower bounds in stochastic optimization ultimately reduce to the concentration analysis of the sum of random variables. [1] approach this by analyzing the sum $\\sum_{i=1}^T \\min_{j \\in [n]} \\tau_j \\eta_{ij},$ where $\\eta_j$ are i.i.d. geometric random variables. In our case, we cannot directly apply this reduction anymore because the computation powers vary over time. Therefore, we found a non-trivial modification: we have to reduce the problem to the concentration analysis of the sum of indicators: $\\sum_{j=1}^T \\mathbb{I} [\\eta_j > \\frac{1}{p}]$ and investigate this sum.\n2) In the heterogeneous setting, the construction and allocation of copies of the \"bad\" functions are ''dynamic\" and change through time.\n3) The universal computation model itself is an independent and non-trivial contribution that significantly generalizes the fixed computation model [1]. Introducing the new computation model, developing new proof techniques, and integrating the new computation model with these proof techniques are all non-trivial tasks.\n\n\n> See my question about the graph-oracle framework [4]. Adding a comparison in the appendix would make the paper even more exhaustive.\n\n[3] have already shown that the graph oracle framework gets weaker lower bound compared to our approach and approach by [3]. See Section M and Table 3 in [3].\n\n> Since the paper does not consider data heterogeneity, it can not recover the homogeneous lower bound...\n\nThat is right. We consider the homogeneous setting ($f_i = f$) and the heterogeneous setting ($f_i \\neq f$), which are clearly far apart. There should be some intermediate regimes between the two settings. One interesting future research question is too choose the right similarity measure (first-order, second-order, ...) and obtain a lower bound.\n\n> I am unsure if I understand the difference between Theorems 6.1 and 6.2...\n\nBecause the random functions in Theorems 6.2 depend on the randomness of the stochastic oracles ($\\xi$ in (8)). This inner/max player can not choose the worst function because the inner/max player does not control the randomness in (8).\n\n> Can the authors comment on what Theorem 6.2 (or 6.1) implies for the partial participation setting in Federated learning (as studied in papers such as [1], [2]), where sampling from a meta distribution of users is standard?\n\nGood question! We haven't investigated it deeply, but one can get results for the meta-learning by taking $n \\to \\infty.$ Under some reasonable assumptions, every continuous measure is the limit of discrete measures. So, if you solve $\\min_x E_{\\eta}{[f_{\\eta}(x)]},$ you can find a sequence $\\eta_n$ with discrete distribution such that $\\eta_n \\to \\eta.$ Then apply our result for $\\eta_n,$ take $n \\to \\infty$ in (18), and get the result. We've tried to provide a non-strict explanation. \n\n> Can the authors highlight in their proof sketches or at least in the appendix why the techniques used in the heterogeneous setting can not recover the homogeneous results...\n\nBy the construction in the heterogeneous setting $f = \\frac{1}{n} \\sum_{i=1}^S h_i$ (29), where $h_i$ depends only on a subblock $x_i$ of $x = [x_1, x_2, \\dots, x_S]$ (see (28)). At every time $t$, worker 1 has access only to $h_1, \\dots, h_{k_t},$ worker 2 has access only to $h_{k_t + 1}, \\dots,$ and so on. It means that the workers optimize independent problems because $\\{h_i\\}$ depend on independent subblocks $x_i.$ So, roughly, the second-order heter of some worker $j$ is $\\|\\| \\nabla^2 f(x) - \\nabla^2 f_j(x) \\|\\|^2 \\approx \\|\\| \\nabla^2 f(x) \\|\\|^2$ because $\\sum_{i} h_i$ has non-zero values only in a small number of coordinates of the vector $x.$"
            }
        },
        {
            "summary": {
                "value": "The paper provides new lower bounds for parallel optimization where the workers can have arbitrary delays. While this setting has been studied in many previous works, and there are known optimal analyses of algorithms such as asynchronous SGD, the paper studies a more general computation model where the delays are not only arbitrary but can also evolve in a structured or unstructured manner over time. Under this model, the paper provides tight lower bounds for many problems of interest, such as periodic delay patterns (common in cross-device federated learning), random device outages, etc. The paper studies homogeneous and heterogeneous distributed setups, matching the best-known upper bounds (up to log factors) in both settings. Overall, the paper closes important gaps in parallel optimization and I support accepting the paper."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. **Closes significant gaps**: The paper closes critical gaps in parallel optimization by providing lower bounds that match the convergence rates for existing algorithms, meaning they are tight. I like that the paper makes these connections explicit, providing relevant corollaries for upper bounds where needed. \n2. **Good writing**: The paper's writing is clear and rigorously describes all results. In notationally heavy parts, such as while introducing the computation model, the paper provides the rough intuition for each term/unit/state, which is very helpful. \n3. **Exhaustive coverage**: Another good thing is that the paper considers both homogeneous and heterogeneous settings relevant to applications like federated learning, which is uncommon in much of the literature on asynchronous optimization, which focuses on data center settings. Overall, the paper's coverage is exhaustive, with both non-convex and convex results."
            },
            "weaknesses": {
                "value": "There are no significant weaknesses in this paper, but I think the following may help improve the writing and exposition:\n1. See my question about [[3]](https://arxiv.org/abs/2305.12387) below.\n2. See my question about the graph-oracle framework [[4]](https://arxiv.org/abs/1805.10222). Adding a comparison in the appendix would make the paper even more exhaustive.\n3. Since the paper does not consider data heterogeneity, it can not recover the homogeneous lower bound. For instance, in the fixed computational power model, (19) can not recover (12). From reading the proof, I raise this issue because the heterogeneity across the workers (in the lower bound) looks pretty adversarial, and it is unclear to me what practical settings will have such heterogeneity. Theoretically, I understand there is no reason the lower bound would not use the full power of the adversary. This is why, theoretically, I think the fully heterogeneous setting is a bit too pessimistic. For instance, if we restrict the heterogeneity to disallow arbitrary division of data blocks ($h_j$'s), the lower bound should be worse (i.e., go down)."
            },
            "questions": {
                "value": "1. I am unsure if I understand the difference between Theorems 6.1 and 6.2. Why does it matter if the functions are randomized v/s not? The inner/max player can always put all their weight on the worst functions for a given algorithm.\n2. Can the authors comment on what Theorem 6.2 (or 6.1) implies for the partial participation setting in Federated learning (as studied in papers such as [[1]](https://arxiv.org/abs/2008.03606), [[2]](https://openreview.net/forum?id=SNElc7QmMDe)), where sampling from a meta distribution of users is standard?\n3. Could the authors provide a detailed comparison against [[3]](https://arxiv.org/abs/2305.12387) regarding the main difference in the lower bound proof techniques while highlighting which delayed feedback settings can not be captured by their lower bound? It also seems that much notational/computational heavy loading was already done in this published paper.\n4. One benefit of the graph oracle framework [[4]](https://arxiv.org/abs/1805.10222) is that it is easier to describe the information flow between different oracle queries between different time steps and agents, compared to using the states like the authors do in their computational model. Could the authors describe what can be captured in their model that can not be captured in the graph oracle model? What do their lower bounds say about existing gaps in the graph oracle framework?\n5. Can the authors highlight in their proof sketches or at least in the appendix why the techniques used in the heterogeneous setting can not recover the homogeneous results under some notion of data heterogeneity? For instance, second or first-order heterogeneity notions combined [[2]](https://openreview.net/forum?id=SNElc7QmMDe) could help interpolate between these two settings (given the hard instances are similar to the usual zero-distributed lower bounds). Or is there any other restriction on the adversary that would make the lower-bound construction less pessimistic?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies distributed stochastic optimization in a parallel and asynchronous setting. In the literature, there is an assumption that all workers operate at a stable and uniform speed, which is unrealistic in practice. This work extends the framework to account for arbitrary computational capacities of workers, capturing their instability and time-varying nature. It then analyzes this generalized framework by deriving both lower and upper bounds."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "-The paper considers a realistic scenario for analyzing distributed stochastic gradient descent by introducing a universal computation model. This model is general and can capture unstable, time-varying random workers.\n\n- It defines a class of algorithms within this new framework and derives tight lower bounds achievable by optimal algorithms.\n\n- The paper also connects previously developed algorithms for distributed stochastic gradient descent, showing that they remain optimal in this new setting."
            },
            "weaknesses": {
                "value": "The paper lacks numerical results and experiments, which would strengthen its contributions."
            },
            "questions": {
                "value": "Do you also consider a model for communication bandwidth? From what I understand, when a worker computes a stochastic gradient, it is broadcast to all other workers. Do you account for situations where there is a communication bandwidth bottleneck or where, due to stragglers, communication may completely fail, preventing the message from being broadcast?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper analyzes the time and oracle complexities of distrusted stochastic gradient descent. To approach this problem, the authors proposed a general computation model where the stochastic gradient is accessed through a set of workers that have different computational speed. In contrast to the classical oracle complexity results for SGD, this work considers an additional level of complexity that the time spent for each oracle access may be different. Then, the authors proved lower bound for the time complexity under the proposed setting and then provided two algorithms that match the lower bound."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper offers a very detailed analysis for distributed SGD \n- I agree with the author's claim that their proposed computation model is in some sense \"universal\" because it greatly extends the model considered by the previous works such as [1].\n- For the proposed setting, this paper offers matching lower and upper bound, thus provides a complete narrative for this problem.\n\nI appreciate that the author does a good job explaining all of the mathematically technically and despite the verbosity required for the very general setting, the definitions in this paper do not take major effort to follow.\n\nThe introduction is direct and concise and I immediately know what kind of results I should expect to see in the technical sections.\n\n[1] Tyurin A, Richt\u00e1rik P. Optimal time complexities of parallel stochastic optimization methods under a fixed computation model. Advances in Neural Information Processing Systems. 2024."
            },
            "weaknesses": {
                "value": "In terms of organization, I suggest that the examples should be moved forward. Currently, the statements of Theorem 5.1 and 6.1 are very long and hard to digest. Having the examples immediately following these theorems would be appreciated. Also, since Them 5.1 and 6.1 are implicit due to the generality of the setting, it could be useful to make some plots for a specific problem instance (say for the fixed computation model) so it is easier to parse the implications of these results.\n\nFor equations (12) (13) and (15), I think the minimum is always at $m=1$ and therefore redundant?\n\nThe proof sketch in Section 7 is not helpful at all. In particular, I want to see the \"worst case function\" being spelled out. From reading the full proof, this step is highly nontrivial and I would like to see some intuition. Also, on line 486, I think you meant \"first coordinate.\" The random variables $\\eta_k$ should be defined earlier in that paragraph. Lastly, I don't understand Section 7.2 at all even after multiple re-reads, in particular, I am left with a feeling with that something important was left out between lines 517 and 522.\n\nLastly and most importantly, **I do not think this paper has sufficient delta from earlier work [1].** \n- From what I can tell, Theorems 6.4 and A.2 from [1] correspond exactly to Examples 5.4 and 6.5. Since the problem setup is quite complicated, it is important for the authors to justify that the added generality actually leads to some meaningful implications. Not having any further examples beyond 6.5 for the heterogeneous setting does not help the cause.\n- And there seem to be a major reuse of proof techniques between the two papers. The proof of Theorem 5.1 up to equation (25) is almost a line-by-line reproduction of the existing analysis in [1]. While I am in no way suggesting any ill intent from the authors, I want to hear from the authors what are the main technical difficulties going from [1] to their proofs.\n\nIn light of these observations, I am hesitant to recommend  an accept because this paper seems to a straight extension of an earlier work [1] that added a lot of mathematical complexity but without too much of new insights. I think the amount of new contribution is insufficient for a new conference paper but could be worthwhile as a journal submission. So I will give score of **5** for the time-being."
            },
            "questions": {
                "value": "I am happy to hear from the author's response to my main concern that how this work adds significant value over [1]. If the authors convince me to change my mind during the rebuttal process, then I expect the authors to incorporate those discussion into the final version of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses the challenges and establishes optimal time complexities for distributed stochastic optimization methods that utilize parallel and asynchronous computation. The authors introduce a universal computation model that captures the real-world scenarios of fluctuating computation speeds, hardware and network delays, and other irregularities often encountered in parallel computing environments. They apply this model to demonstrate tight lower bounds for both synchronous and asynchronous optimization methods, and highlight that these bounds are closely matched by the Rennala SGD and Malenia SGD methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper proposes a new computation model that more realistically simulates the computational irregularities of distributed systems, which advances over previous models that assumed stable and uniform computation speeds.\n2. The analysis encompasses a broad range of stochastic optimization methods and considers both homogeneous and heterogeneous computing environments."
            },
            "weaknesses": {
                "value": "1. The theoretical results, while comprehensive, may be too complex to be applied to practical algorithms. This complexity could limit their usability for practitioners who require simpler and more accessible tools for system design and analysis. Consequently, the tight lower bounds for time complexities are primarily of theoretical interest and provide limited insight into the potential performance improvements achievable with parallel stochastic optimization methods.\n2. The paper does not provide sufficient discussion on the limitations of the universal computation model. Are there any properties in parallel or distributed systems that fall outside the scope of this universal model?"
            },
            "questions": {
                "value": "1. Does the computation power characterize server\u2019s computation speed? \n2. In Methods 3 and 4, is the communication time between the workers and the server taken into account? In practical systems, it may take a significantly longer amount of time for a worker to transmit a gradient than to compute it. How does the universal computation model account for this?\n3. Line 209: What does $\\mathcal{D}$ mean?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}