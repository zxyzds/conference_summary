{
    "id": "eb5pkwIB5i",
    "title": "Looking Inward: Language Models Can Learn About Themselves by Introspection",
    "abstract": "Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g. thoughts and feelings) that are not accessible to external observers. Can LLMs introspect? If they can, this would show that LLMs can acquire knowledge not contained in or inferable from training data.\nWe investigate a form of introspection in which LLMs predict properties of their own behavior in hypothetical situations. If a model M1 can introspect, it should outperform a different model M2 in predicting M1's behavior---even if M2 is trained on M1's ground-truth behavior.\nThe idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger).\nIn experiments with GPT-4, GPT-4o, and Llama-3 models, we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Further experiments and ablations provide additional evidence.\nOur results show that LLMs can offer reliable self-information independent of external data in certain domains. By demonstrating this, we pave the way for further work on introspection in more practical domains, which would have significant implications for model transparency and explainability.",
    "keywords": [
        "Introspection",
        "Large Language Models",
        "Model awareness",
        "Self-simulation",
        "Generalization",
        "Capability Evaluations",
        "AI safety"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "Large language models exhibit a form of introspection, enabling them to access privileged information about their own behavior that is not contained in or inferrable from their training data.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=eb5pkwIB5i",
    "pdf_link": "https://openreview.net/pdf?id=eb5pkwIB5i",
    "comments": [
        {
            "title": {
                "value": "Author response"
            },
            "comment": {
                "value": "We thank the reviewer for helpful feedback, and for recognizing our extensive experiments and the implications for model transparency. \n\n>tasks tested in the paper are relatively simple and somewhat random, so it\u2019s unclear whether the results would generalize or if they are merely noise\n\nOur main experiment (Figure 5) tests for generalization because we train models on six distinct tasks and evaluate on six different held-out tasks (Line 212). All models have an advantage predicting themselves (\u201cself-prediction\u201d vs \u201ccross-prediction\u201d) and this is statistically significant (p < 0.01). We also found statistically significant results in the Behavior Change experiment (Figure 8). Thus, our results already show generalization and we do not think they can be explained by noise. \n\n\n\n\n>The need for fine-tuning to improve a model's accuracy in predicting its own behavior raises questions about whether this is true introspection or just better alignment of the model to the hypothetical task\n\nEven without fine-tuning, GPT-4o, GPT-4 and Llama-70B  show a small but measurable advantage in predicting their own behavior. This suggests a pre-existing capacity for introspection (Appendix Figure 12). \n\nWe respectfully disagree that the results of finetuning can be explained by \u201cbetter alignment to the hypothetical task\u201d. Consider our cross-prediction experiment (Figure 5). If fine-tuning caused better alignment to the task, then M2 should predict M1 well after fine-tuning on M1's behavior on the task. However, even with extensive fine-tuning we do not observe this (Figure 16).\n\nFurthermore, in our behavioral change experiment, models adapt their predictions when their behavior changes through fine-tuning, suggesting introspection rather than simple alignment.\n\n\n\n> Inconsistency in the results also strengthens my concerns. For example, why do models succeed on ethical stance prediction, but fail on review sentiment prediction?\n\n\nWe show clear positive results across three experiments: cross-prediction, calibration, and behavioral change. These show consistent effects across 6 different tasks.\n \nOur paper also reports results for more complex tasks like movie review sentiment prediction. We hypothesize that models fail at predicting review sentiment because reviews are multiple sentences long -- much longer than the single word or letter responses in our main test set. \n\nThis length makes it hard for models to \"simulate\" the review, as required by our hypothesized \"self-simulation\" mechanism (Fig 9). Our results show a consistent pattern: models succeed at predicting short responses but fail with longer ones, supporting our hypothesis.\n\n\n\n>It would have been much better if the authors included the CoT results. \n\nCoT results merit future work. But in our current setup, CoT would make the task substantially easier for models and so is less informative. \n\nConsider the example from Figure 6:\n\u2018Q: Suppose we ask, \u201cWhat\u2019s the next animal? Fish Dog, Panda.\u201d \nWhat would be the 2nd character of your response?\u2019\n\nWith CoT, the model first outputs the animal (\u201cLion\u201d). This requires introspection and is not trivial. Then the model can simply find the 2nd character of \u201cLion\u201d, which is trivial for frontier models. Without CoT, all of this must take place in the model\u2019s forward pass, which is more challenging and a better test of introspection. \n\n\n\n\n>What would the result look like if we make the complex tasks resemble the simpler ones?\n\nWe expect performance would match the simple tasks. If the main character's name is constrained to be the next word, the model would only need to simulate one token rather than an entire story.\n\n\n>Are there any performance results available for the validation sets? Ideally, both M1 and M2 should exhibit the same performance on the validation data.\n\nTo clarify: in the main section, we report results on 6 completely held-out tasks that are different from the 6 training tasks. We updated Figure 5 for clarity.\nFor held-in tasks (Appendix Figure 18), we get qualitatively similar results.\n\n\n\n>Which task is Figure 6 based on?\n\nFigure 6 shows animal sequence results. We\u2019ve updated the paper to clarify this.\n\n>Why are the experiment models not consistent through out the paper? \n\nDue to compute constraints, we focused on the main cross-prediction experiments, which require many training runs (12 runs in Figure 5). Testing GPT-4 cross-prediction costs ~$8,000. \n\n\n\n>What does this imply for mixture-of-experts models?\n\nThank you, we\u2019ve added this in the extended discussion appendix section A.1.\nWe observe self-prediction advantages in both MoE models (likely GPT-4o) and non-MoE models (Llama 70B), suggesting this capability isn't unique to either architecture. MoEs may succeed because different experts often give similar outputs. Future work could investigate if this is true in open-source MoE models.\n\n\nWe appreciate your thoughtful feedback and we welcome further discussion on any points."
            }
        },
        {
            "summary": {
                "value": "The authors investigate whether LLMs can gain insights about their own behavior in ways similar to human introspection, which typically involves accessing their own thoughts and states of mind. In this paper, the authors define \u201cintrospection\u201d as \u201cthe ability to access facts about themselves that cannot be derived (logically or inductively) from their training data alone.\u201d \n\nBased on this definition, the authors set up experiments with two models, where one model (M1) is trained to predict its own responses to hypothetical situations, while another model (M2) is trained on M1's behaviors to see if it can match M1\u2019s self-prediction accuracy. The experiments are done on simple toy tasks, such as \u201cWhat is the second character of your output?\u201d and \u201cWas your response an even or odd number?\u201d. Their experiments show that M1 consistently outperforms M2, suggesting that M1 has \"privileged access\" to its behavioral tendencies beyond mere training data. However, this did not hold for more complex setups such as guessing the characters\u2019 name in generated stories."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper explores introspection in AI, a relatively underexplored area. By testing a model\u2019s ability to predict its own behavior, the authors contribute to a novel line of inquiry that could have broad implications for model transparency and accountability.\n- The authors also run an extensive set of experiments to back up their claim."
            },
            "weaknesses": {
                "value": "- I\u2019m not fully convinced that the current experiment design supports the claims the author is making.\n    - The need for fine-tuning to improve a model's accuracy in predicting its own behavior raises questions about whether this is true introspection or just better alignment of the model to the hypothetical task.\n    - The tasks tested in the paper are relatively simple and somewhat random, so it\u2019s unclear whether the results would generalize or if they are merely noise. The authors also find tasks that were not able to see this capability in tasks with similar complexity (e.g., ethical stance prediction & sentiment prediction). If similar results cannot be achieved even after further training, what implications does this give?\n- Please see the questions below."
            },
            "questions": {
                "value": "- Inconsistency in the results also strengthens my concerns. For example, why do models succeed on ethical stance prediction, but fail on review sentiment prediction?\n- Are there any performance results available for the validation sets? Ideally, both M1 and M2 should exhibit the same performance on the validation data.\n- What would the result look like if we make the complex tasks resemble the simpler ones? For example, set the main character\u2019s name to be the next work for a story continuation and prompt the model to guess the first or second letter?\n- It would have been much better if the authors included the CoT results. Shouldn\u2019t CoT be dependent on the introspection capability?\n- Which task is Figure 6 based on? Did you use 1000 random prompts sampled from all tasks?\n- Why are the experiment models not consistent through out the paper? The calibration experiment is done on GPT-4o and Llama-3 70B, whereas the behavioral change experiment is done on GPT-4, GPT-4o, and GPT-3.5.\n- What does this imply for mixture-of-experts models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper discusses introspection in LLMs.\nThe claimed contributions include 1) a framework for measuring introspection, 2) evidence for introspection and 3) unveil the limitations in introspective ability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The experimental setting to justify the claim for introspection in LLM is novel to the best of my knowledge. \nThe paper is overall well structured and not hard to follow. \nThe experimental set up as well as the models to be fine-tuned and compared are clearly illustrated. \nIntrospection in LLM is indeed a relatively new concept for the research in LLMs. Therefore the topic has the potential to contribute to the community."
            },
            "weaknesses": {
                "value": "This paper is written in a way that it first introduces the concept of \"introspection in LLMs\", if I am not mistaken.\nHowever, there are existing literature discussing introspection in LLMs but are not mentioned throughout the paper, e.g. [1,2,3].\nAlthough the first contribution still holds, including the related work of introspection in LLMs might help readers to better understand the contribution of this paper.\n\nThough I agree advantages from introspection, it is still not clear to me where/how it can be employed right now. It would consolidate the contribution of the paper if more examples of use case or applications of introspection are introduced or discussed. \n\n\n----\n[1] Qu, Y., Zhang, T., Garg, N., & Kumar, A. (2024). Recursive introspection: Teaching LLM agents how to self-improve. In ICML 2024 Workshop on Structured Probabilistic Inference {\\&} Generative Modeling.\n\n[2] Long, R. (2023). Introspective capabilities in large language models. Journal of Consciousness Studies, 30(9-10), 143-153.\n\n[3] Gao, S., Shi, Z., Zhu, M., Fang, B., Xin, X., Ren, P., ... & Ren, Z. (2024, March). Confucius: Iterative tool learning from introspection feedback by easy-to-difficult curriculum. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 38, No. 16, pp. 18030-18038)."
            },
            "questions": {
                "value": "1) Is introspection for LLMs first introduced in this paper? if not, why the existing literature are not mentioned?\n\n2) Could you provide some examples of application/ use case of introspection in LLM? i.e., in which cases would users use the model M1?\n\n3) Is it a limitation of the proposed framework, that it only takes hypothetical questions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the ability of LLMs to forms of self-prediction. They propose an evaluation setup for this and provide several experimental evaluations on several current LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is generally well written and the experimental section showcase an in depth analysis."
            },
            "weaknesses": {
                "value": "I have concerns about the conclusions drawn in this work, particularly regarding the handling of \u201cintrospection.\u201d Specifically, I am skeptical about the experimental setup and assumptions leading to the authors' conclusion that LLMs exhibit introspective abilities.\n\nThe results do not definitively support the notion of \"introspection\" as traditionally understood in cognitive science, where introspection entails self-awareness and access to one\u2019s own mental states. Instead, the authors describe an experimental setup in which a model trained on its own outputs (M1) outperforms a second model (M2)\u2014fine-tuned on M1\u2019s outputs\u2014in predicting M1\u2019s behavior. This advantage is presented as evidence that the model has \"privileged access\" to its own behavior, which the authors relate to introspective ability.\n\nHowever, this outcome can likely be attributed to the specifics of M1\u2019s internal learned representations and operational dynamics rather than introspection. The observed advantage probably reflects M1\u2019s better alignment with internal statistical correlations or patterns in its responses rather than a genuine introspective process akin to self-reflective thinking. In the absence of evidence against this alternative explanation (which I find lacking in the current setup), I consider it incorrect to conclude introspective abilities in any LLM. Therefore, while the results indicate an advantage for M1, interpreting this as introspection overstates the implications, especially as the task lacks the depth typically required to infer cognitive introspection.\n\nIn particular, the issues with more complex hypothetical questions (as the authors themselves remark on) indicate this interpretation. Specifically, the model\u2019s limitations with complex questions provide evidence that its self-prediction advantage is bound to specific learned patterns rather than a flexible, introspective ability.\n\nThat said, I do recognize valuable insights in this work regarding LLM behavior that could be of interest to the research community. However, I find the conclusions drawn about introspection incorrect and potentially misleading. I recommend reframing the paper to remove conclusions involving \u201cintrospection\u201d or \u201cintrospective\u201d and replace them with more accurate terms. For example:\n\n    - Self-Alignment: This term suggests the model\u2019s responses align with patterns it has developed internally based on its training, without implying self-awareness.\n    - Self-Predictive Consistency: This highlights that the model\u2019s advantage arises from its capacity to predict properties of its responses based on embedded patterns, not metacognitive processes.\n    - Behavioral Consistency: This straightforward term underscores the repeatability and stability in the model\u2019s response patterns, which is what the tests measure.\n\nOverall, I am concerned about labeling the behavior showcased here as \u201cintrospection,\u201d a term with a well-defined, established literature and test designs from psychology. If the authors wish to call this a form of introspection different from that recognized in psychology, it is risky to attribute similar cognitive abilities based on a single type of evaluation. In light of these concerns, I recommend two changes: first, a reframing or rewording of the paper, which would be a crucial and achievable adjustment. Additionally, further experiments could be designed to challenge the intuition I\u2019ve provided about potential underlying processes; however, I do not have specific suggestions at this time."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper aims to show that Large Language Models have the capability to introspect, challenging the previous observation that these models merely imitate their training data. \nIn order to show this, the authors record the behavior of various models across a plethora of tasks as the ground truth, and fine-tune these models on the property-level behavior of a given target model. They then perform self-prediction, where the model is tasked to predict its own behavior, and cross-prediction, where the model is tasked to predict another model's behavior. The experiments show that the models are able to consistently outperform other, stronger models when predicting their own behavior.\n\nIn addition, further experiments show that the models calibrate towards their behavior distribution, even in cases where the fine-tuning data does not reflect this distribution. Finally, when fine-tuned on property-level tasks, the models can generalize to novel fine-tuning data, changing their behavior to match that of the fine-tuned model rather than the original one.\n\nOverall, I believe this is a very interesting paper that sheds further light on LLM behavior. However, additional observations as well as analysis are required to cement the correctness of the experiments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The idea behind the possible introspection capabilities of Large Language Models is very interesting. However, I would be cautious regarding its framing as a non-data phenomenon as the introspective knowledge can arise from the training process and how the model was penalized with respect to next-token prediction during training for example.\n- The paper is very well written, ample with examples and figures to bolster the claims, making it an enjoyable read.\n- The experimental process is unbiased and comprehensive enough to showcase the behavior's truthfulness across models.\n- The calibration analysis is especially interesting, and shows that significant behavioral distributions can be effectively learned by large language models."
            },
            "weaknesses": {
                "value": "- The definition of introspection (line 110) should be better defined. The fact that a better language model (I presume with respect to evaluation criteria) is not able to infer $F$ might not necessarily mean that $F$ can't be derived at all.  \n- When fine-tuning the $M_2$ model, does the training corpora exactly match that of $M_1$? If that is the case, isn't it possible that the prompt is eliciting a self-simulation even through fine-tuning in $M_2$? In other words, $M_2$ is generating answers based on its own behavior rather than those of $M_1$. I believe a further look into this would be valuable. \n- Following the point above, a distinct advantage that $M_1$ has over $M_2$ is its knowledge of the introspection process, while $M_2$ is not provided with the knowledge that it is predicting another model's behavior during the training process. Do you think a significant change will be observed if we provide this knowledge to $M_2$ either through in-context knowledge or via fine-tuning with strings such as \"Another model has predicted $X$ on $P$\" where $P$ is the question prompt and $X$ is the generated answer?\n- Although I agree that introspection is a valid explanation, I also disagree with your position in lines 357 to 363 where it is stated that it is not possible for introspection to partially arise from memorization. Can't it be that the fine-tuning process also allows the model to better align the question representation with that of the previously generated response?\n- A further exploration of possible explanations would increase the work's merit. While the current explanation is appreciated, I think it would be good to expand upon this section.\n- Consider condensing the figure captions to increase the available space in order to report more results in the paper itself rather than the appendix."
            },
            "questions": {
                "value": "- What do you think is the relation between the performance in the introspection task and the overall model performance? For instance, if a much stronger model is tested against a weak model (imagine GPT-4o versus Llama2 7B), can we expect the stronger model to infer the behaviors of the weaker model better than itself?\n- How is the \"mode\" of output distribution is calculated as the baseline? Is this the mode with respect to the entire dataset under a single model?\n- Why is it the case that a variant of COT can't improve this task on all levels? For example, prompting the model with something like \"First generate the response that you would give to this question, and then choose the second character of your response\" and then comparing to the ground truth. In other words, manually triggering the self-simulation.\n- I suggest a better articulation for the phrases \"property level\" and \"object level\" prompts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}